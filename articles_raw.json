{"all_articles": [{"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351241", "id": "tag:google.com,2005:reader/item/0000000338b94926", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Posterior Sampling for Large Scale Reinforcement Learning. (arXiv:1711.07979v1 [cs.LG])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07979"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07979", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c6e65c62\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c6e65c62&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Posterior sampling for reinforcement learning (PSRL) is a popular algorithm \nfor learning to control an unknown Markov decision process (MDP). PSRL \nmaintains a distribution over MDP parameters and in an episodic fashion samples \nMDP parameters, computes the optimal policy for them and executes it. A special \ncase of PSRL is where at the end of each episode the MDP resets to the initial \nstate distribution. Extensions of this idea to general MDPs without state \nresetting has so far produced non-practical algorithms and in some cases buggy \ntheoretical analysis. This is due to the difficulty of analyzing regret under \nepisode switching schedules that depend on random variables of the true \nunderlying model. We propose a solution to this problem that involves using a \ndeterministic, model-independent episode switching schedule, and establish a \nBayes regret bound under mild assumptions. Our algorithm termed deterministic \nschedule PSRL (DS-PSRL) is efficient in terms of time, sample, and space \ncomplexity. Our result is more generally applicable to continuous state action \nproblems. We demonstrate how this algorithm is well suited for sequential \nrecommendation problems such as points of interest (POI). We derive a general \nprocedure for parameterizing the underlying MDPs, to create action condition \ndynamics from passive data, that do not contain actions. We prove that such \nparameterization satisfies the assumptions of our analysis. \n</p>"}, "author": "Georgios Theocharous, Zheng Wen, Yasin Abbasi-Yadkori, Nikos Vlassis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351240", "id": "tag:google.com,2005:reader/item/0000000338b9492f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Relating Input Concepts to Convolutional Neural Network Decisions. (arXiv:1711.08006v1 [cs.LG])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08006"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08006", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many current methods to interpret convolutional neural networks (CNNs) use \nvisualization techniques and words to highlight concepts of the input seemingly \nrelevant to a CNN's decision. The methods hypothesize that the recognition of \nthese concepts are instrumental in the decision a CNN reaches, but the nature \nof this relationship has not been well explored. To address this gap, this \npaper examines the quality of a concept's recognition by a CNN and the degree \nto which the recognitions are associated with CNN decisions. The study \nconsiders a CNN trained for scene recognition over the ADE20k dataset. It uses \na novel approach to find and score the strength of minimally distributed \nrepresentations of input concepts (defined by objects in scene images) across \nlate stage feature maps. Subsequent analysis finds evidence that concept \nrecognition impacts decision making. Strong recognition of concepts \nfrequently-occurring in few scenes are indicative of correct decisions, but \nrecognizing concepts common to many scenes may mislead the network. \n</p>"}, "author": "Ning Xie, Md Kamruzzaman Sarker, Derek Doran, Pascal Hitzler, Michael Raymer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351239", "id": "tag:google.com,2005:reader/item/0000000338b94938", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Recurrent Relational Networks for Complex Relational Reasoning. (arXiv:1711.08028v1 [cs.AI])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08028"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08028", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Humans possess an ability to abstractly reason about objects and their \ninteractions, an ability not shared with state-of-the-art deep learning models. \nRelational networks, introduced by Santoro et al. (2017), add the capacity for \nrelational reasoning to deep neural networks, but are limited in the complexity \nof the reasoning tasks they can address. We introduce recurrent relational \nnetworks which increase the suite of solvable tasks to those that require an \norder of magnitude more steps of relational reasoning. We use recurrent \nrelational networks to solve Sudoku puzzles and achieve state-of-the-art \nresults by solving 96.6% of the hardest Sudoku puzzles, where relational \nnetworks fail to solve any. We also apply our model to the BaBi textual QA \ndataset solving 19/20 tasks which is competitive with state-of-the-art sparse \ndifferentiable neural computers. The recurrent relational network is a general \npurpose module that can augment any neural network model with the capacity to \ndo many-step relational reasoning. \n</p>"}, "author": "Rasmus Berg Palm, Ulrich Paquet, Ole Winther", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351238", "id": "tag:google.com,2005:reader/item/0000000338b9493c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Deterministic Policy Optimization by Combining Pathwise and Score Function Estimators for Discrete Action Spaces. (arXiv:1711.08068v1 [cs.AI])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08068"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08068", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Policy optimization methods have shown great promise in solving complex \nreinforcement and imitation learning tasks. While model-free methods are \nbroadly applicable, they often require many samples to optimize complex \npolicies. Model-based methods greatly improve sample-efficiency but at the cost \nof poor generalization, requiring a carefully handcrafted model of the system \ndynamics for each task. Recently, hybrid methods have been successful in \ntrading off applicability for improved sample-complexity. However, these have \nbeen limited to continuous action spaces. In this work, we present a new hybrid \nmethod based on an approximation of the dynamics as an expectation over the \nnext state under the current policy. This relaxation allows us to derive a \nnovel hybrid policy gradient estimator, combining score function and pathwise \nderivative estimators, that is applicable to discrete action spaces. We show \nsignificant gains in sample complexity, ranging between $1.7$ and $25\\times$, \nwhen learning parameterized policies on Cart Pole, Acrobot, Mountain Car and \nHand Mass. Our method is applicable to both discrete and continuous action \nspaces, when competing pathwise methods are limited to the latter. \n</p>"}, "author": "Daniel Levy, Stefano Ermon", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351237", "id": "tag:google.com,2005:reader/item/0000000338b94945", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Robust Stackelberg Equilibria in Extensive-Form Games and Extension to Limited Lookahead. (arXiv:1711.08080v1 [cs.GT])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08080"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08080", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stackelberg equilibria have become increasingly important as a solution \nconcept in computational game theory, largely inspired by practical problems \nsuch as security settings. In practice, however, there is typically uncertainty \nregarding the model about the opponent. This paper is, to our knowledge, the \nfirst to investigate Stackelberg equilibria under uncertainty in extensive-form \ngames, one of the broadest classes of game. We introduce robust Stackelberg \nequilibria, where the uncertainty is about the opponent's payoffs, as well as \nones where the opponent has limited lookahead and the uncertainty is about the \nopponent's node evaluation function. We develop a new mixed-integer program for \nthe deterministic limited-lookahead setting. We then extend the program to the \nrobust setting for Stackelberg equilibrium under unlimited and under limited \nlookahead by the opponent. We show that for the specific case of interval \nuncertainty about the opponent's payoffs (or about the opponent's node \nevaluations in the case of limited lookahead), robust Stackelberg equilibria \ncan be computed with a mixed-integer program that is of the same asymptotic \nsize as that for the deterministic setting. \n</p>"}, "author": "Christian Kroer, Gabriele Farina, Tuomas Sandholm", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351236", "id": "tag:google.com,2005:reader/item/0000000338b9494b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Asymmetric Action Abstractions for Multi-Unit Control in Adversarial Real-Time Games. (arXiv:1711.08101v1 [cs.AI])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08101"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08101", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Action abstractions restrict the number of legal actions available during \nsearch in multi-unit real-time adversarial games, thus allowing algorithms to \nfocus their search on a set of promising actions. Optimal strategies derived \nfrom un-abstracted spaces are guaranteed to be no worse than optimal strategies \nderived from action-abstracted spaces. In practice, however, due to real-time \nconstraints and the state space size, one is only able to derive good \nstrategies in un-abstracted spaces in small-scale games. In this paper we \nintroduce search algorithms that use an action abstraction scheme we call \nasymmetric abstraction. Asymmetric abstractions retain the un-abstracted \nspaces' theoretical advantage over regularly abstracted spaces while still \nallowing the search algorithms to derive effective strategies, even in \nlarge-scale games. Empirical results on combat scenarios that arise in a \nreal-time strategy game show that our search algorithms are able to \nsubstantially outperform state-of-the-art approaches. \n</p>"}, "author": "Rubens O. Moraes, Levi H. S. Lelis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351235", "id": "tag:google.com,2005:reader/item/0000000338b94955", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Multiagent Simple Temporal Problem: The Arc-Consistency Approach. (arXiv:1711.08151v1 [cs.MA])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08151"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08151", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Simple Temporal Problem (STP) is a fundamental temporal reasoning problem \nand has recently been extended to the Multiagent Simple Temporal Problem \n(MaSTP). In this paper we present a novel approach that is based on enforcing \narc-consistency (AC) on the input (multiagent) simple temporal network. We show \nthat the AC-based approach is sufficient for solving both the STP and MaSTP and \nprovide efficient algorithms for them. As our AC-based approach does not impose \nnew constraints between agents, it does not violate the privacy of the agents \nand is superior to the state-of-the-art approach to MaSTP. Empirical \nevaluations on diverse benchmark datasets also show that our AC-based \nalgorithms for STP and MaSTP are significantly more efficient than existing \napproaches. \n</p>"}, "author": "Shufeng Kong, Jae Hee Lee, Sanjiang Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351234", "id": "tag:google.com,2005:reader/item/0000000338b9495c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "An influence-based fast preceding questionnaire model for elderly assessments. (arXiv:1711.08228v1 [cs.AI])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08228"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08228", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>To improve the efficiency of elderly assessments, an influence-based fast \npreceding questionnaire model (FPQM) is proposed. Compared with traditional \nassessments, the FPQM optimizes questionnaires by reordering their attributes. \nThe values of low-ranking attributes can be predicted by the values of the \nhigh-ranking attributes. Therefore, the number of attributes can be reduced \nwithout redesigning the questionnaires. A new function for calculating the \ninfluence of the attributes is proposed based on probability theory. Reordering \nand reducing algorithms are given based on the attributes' influences. The \nmodel is verified through a practical application. The practice in an \nelderly-care company shows that the FPQM can reduce the number of attributes by \n90.56% with a prediction accuracy of 98.39%. Compared with other methods, such \nas the Expert Knowledge, Rough Set and C4.5 methods, the FPQM achieves the best \nperformance. In addition, the FPQM can also be applied to other questionnaires. \n</p>"}, "author": "Tong Mo, Rong Zhang, Weiping Li, Jingbo Zhang, Zhonghai Wu, Wei Tan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351233", "id": "tag:google.com,2005:reader/item/0000000338b94967", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "The Stochastic Firefighter Problem. (arXiv:1711.08237v1 [cs.SY])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08237"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08237", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The dynamics of infectious diseases spread is crucial in determining their \nrisk and offering ways to contain them. We study sequential vaccination of \nindividuals in networks. In the original (deterministic) version of the \nFirefighter problem, a fire breaks out at some node of a given graph. At each \ntime step, b nodes can be protected by a firefighter and then the fire spreads \nto all unprotected neighbors of the nodes on fire. The process ends when the \nfire can no longer spread. We extend the Firefighter problem to a probabilistic \nsetting, where the infection is stochastic. We devise a simple policy that only \nvaccinates neighbors of infected nodes and is optimal on regular trees and on \ngeneral graphs for a sufficiently large budget. We derive methods for \ncalculating upper and lower bounds of the expected number of infected \nindividuals, as well as provide estimates on the budget needed for containment \nin expectation. We calculate these explicitly on trees, d-dimensional grids, \nand Erd\\H{o}s R\\'{e}nyi graphs. Finally, we construct a state-dependent budget \nallocation strategy and demonstrate its superiority over constant budget \nallocation on real networks following a first order acquaintance vaccination \npolicy. \n</p>"}, "author": "Guy Tennenholtz, Constantine Caramanis, Shie Mannor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351232", "id": "tag:google.com,2005:reader/item/0000000338b9496e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "High-dimensional Motion Planning using Latent Variable Models via Approximate Inference. (arXiv:1711.08275v1 [cs.RO])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08275"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08275", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c6e65eb2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c6e65eb2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this work, we present an efficient framework to generate a motion \ntrajectory of a robot that has a high degree of freedom (e.g., a humanoid \nrobot). High-dimensionality of the robot configuration space often leads to \ndifficulties in utilizing the widely-used motion planning algorithms because \nthe volume of the decision space increases exponentially with the number of \ndimensions. To handle complications arising from the large decision space, and \nto solve a corresponding motion planning problem efficiently, two key concepts \nwere adopted. First, the Gaussian process latent variable model (GP-LVM) was \nutilized for low-dimensional representation of the original configuration \nspace. Second, an approximate inference algorithm was used, exploiting through \nthe duality between control and estimation, to explore the decision space and \nto compute a high-quality motion trajectory of the robot. Utilizing the GP-LVM \nand the duality between control and estimation, we construct a fully \nprobabilistic generative model with which we transform a high-dimensional \nmotion planning problem into a tractable inference problem. Finally, we compute \nthe optimal motion trajectory via an approximate inference algorithm based on a \nvariant of the particle filter. Numerical experiments are presented to \ndemonstrate the applicability and validity of the proposed approach. The \nresulting motions can be viewed in the supplemental video. ( \nhttps://youtu.be/DsZ1afN6pTY ) \n</p>"}, "author": "Jung-Su Ha, Hyeok-Joo Chae, Han-Lim Choi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351231", "id": "tag:google.com,2005:reader/item/0000000338b94975", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Systems, Actors and Agents: Operation in a multicomponent environment. (arXiv:1711.08319v1 [cs.MA])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08319"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08319", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multi-agent approach has become popular in computer science and technology. \nHowever, the conventional models of multi-agent and multicomponent systems \nimplicitly or explicitly assume existence of absolute time or even do not \ninclude time in the set of defining parameters. At the same time, it is proved \ntheoretically and validated experimentally that there are different times and \ntime scales in a variety of real systems - physical, chemical, biological, \nsocial, informational, etc. Thus, the goal of this work is construction of a \nmulti-agent multicomponent system models with concurrency of processes and \ndiversity of actions. To achieve this goal, a mathematical system actor model \nis elaborated and its properties are studied. \n</p>"}, "author": "Mark Burgin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351230", "id": "tag:google.com,2005:reader/item/0000000338b9497c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A correlational analysis of multiagent sensorimotor interactions: clustering autonomous and controllable entities. (arXiv:1711.08333v1 [cs.AI])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08333"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08333", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A first step to reach Theory of Mind (ToM) abilities (attribution of beliefs \nto others) in synthetic agents through sensorimotor interactions, would be to \ntag sensory data with agent typology and action intentions: autonomous agent X \nmoved an object under the box. We propose a dual arm robotic setup in which ToM \ncould be probed. We then discuss what measures can be extracted from \nsensorimotor interaction data (based on a correlation analysis) in the proposed \nsetup that allow to distinguish self than other and other/inanimate from \nother/active with intentions. We finally discuss what elements are missing in \ncurrent cognitive architectures to be able to acquire ToM abilities in \nsynthetic agents from sensorimotor interactions, bottom-up from reactive agent \ninteraction behaviors and top-down from the optimization of social behaviour \nand cooperation. \n</p>"}, "author": "M. S&#xe1;nchez-Fibla, C. Moulin-Frier, X. Arsiwalla, P. Verschure", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351229", "id": "tag:google.com,2005:reader/item/0000000338b94984", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Allocation Problems in Ride-Sharing Platforms: Online Matching with Offline Reusable Resources. (arXiv:1711.08345v1 [cs.AI])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08345"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08345", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Bipartite matching markets pair agents on one side of a market with agents, \nitems, or contracts on the opposing side. Prior work addresses online bipartite \nmatching markets, where agents arrive over time and are dynamically matched to \na known set of disposable resources. In this paper, we propose a new model, \nOnline Matching with (offline) Reusable Resources under Known Adversarial \nDistributions (OM-RR-KAD), in which resources on the offline side are reusable \ninstead of disposable; that is, once matched, resources become available again \nat some point in the future. We show that our model is tractable by presenting \nan LP-based adaptive algorithm that achieves an online competitive ratio of 1/2 \n- eps for any given eps greater than 0. We also show that no non-adaptive \nalgorithm can achieve a ratio of 1/2 + o(1) based on the same benchmark LP. \nThrough a data-driven analysis on a massive openly-available dataset, we show \nour model is robust enough to capture the application of taxi dispatching \nservices and ride-sharing systems. We also present heuristics that perform well \nin practice. \n</p>"}, "author": "John P Dickerson, Karthik A Sankararaman, Aravind Srinivasan, Pan Xu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511446017351", "timestampUsec": "1511446017351228", "id": "tag:google.com,2005:reader/item/0000000338b9498a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Building Machines that Learn and Think for Themselves: Commentary on Lake et al., Behavioral and Brain Sciences, 2017. (arXiv:1711.08378v1 [cs.AI])", "published": 1511446018, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08378"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08378", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We agree with Lake and colleagues on their list of key ingredients for \nbuilding humanlike intelligence, including the idea that model-based reasoning \nis essential. However, we favor an approach that centers on one additional \ningredient: autonomy. In particular, we aim toward agents that can both build \nand exploit their own internal models, with minimal human hand-engineering. We \nbelieve an approach centered on autonomous learning has the greatest chance of \nsuccess as we scale toward real-world complexity, tackling domains for which \nready-made formal models are not available. Here we survey several important \nexamples of the progress that has been made toward building autonomous agents \nwith humanlike abilities, and highlight some outstanding challenges. \n</p>"}, "author": "M. Botvinick, D.G.T. Barrett, P. Battaglia, N. de Freitas, D. Kumaran, J. Z Leibo, T. Lillicrap, J. Modayil, S. Mohamed, N.C. Rabinowitz, D. J. Rezende, A. Santoro, T. Schaul, C. Summerfield, G. Wayne, T. Weber, D. Wierstra, S. Legg, D. Hassabis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787738", "id": "tag:google.com,2005:reader/item/00000003388e938d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "The Riemannian Geometry of Deep Generative Models. (arXiv:1711.08014v1 [cs.LG])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08014"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08014", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep generative models learn a mapping from a low dimensional latent space to \na high-dimensional data space. Under certain regularity conditions, these \nmodels parameterize nonlinear manifolds in the data space. In this paper, we \ninvestigate the Riemannian geometry of these generated manifolds. First, we \ndevelop efficient algorithms for computing geodesic curves, which provide an \nintrinsic notion of distance between points on the manifold. Second, we develop \nan algorithm for parallel translation of a tangent vector along a path on the \nmanifold. We show how parallel translation can be used to generate analogies, \ni.e., to transport a change in one data point into a semantically similar \nchange of another data point. Our experiments on real image data show that the \nmanifolds learned by deep generative models, while nonlinear, are surprisingly \nclose to zero curvature. The practical implication is that linear paths in the \nlatent space closely approximate geodesics on the generated manifold. However, \nfurther investigation into this phenomenon is warranted, to identify if there \nare other architectures or datasets where curvature plays a more prominent \nrole. We believe that exploring the Riemannian geometry of deep generative \nmodels, using the tools developed in this paper, will be an important step in \nunderstanding the high-dimensional, nonlinear spaces these models learn. \n</p>"}, "author": "Hang Shao, Abhishek Kumar, P. Thomas Fletcher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787737", "id": "tag:google.com,2005:reader/item/00000003388e93a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Disagreement-based combinatorial pure exploration: Efficient algorithms and an analysis with localization. (arXiv:1711.08018v1 [stat.ML])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08018"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08018", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We design new algorithms for the combinatorial pure exploration problem in \nthe multi-arm bandit framework. In this problem, we are given K distributions \nand a collection of subsets $\\mathcal{V} \\subset 2^K$ of these distributions, \nand we would like to find the subset $v \\in \\mathcal{V}$ that has largest \ncumulative mean, while collecting, in a sequential fashion, as few samples from \nthe distributions as possible. We study both the fixed budget and fixed \nconfidence settings, and our algorithms essentially achieve state-of-the-art \nperformance in all settings, improving on previous guarantees for structures \nlike matchings and submatrices that have large augmenting sets. Moreover, our \nalgorithms can be implemented efficiently whenever the decision set V admits \nlinear optimization. Our analysis involves precise concentration-of-measure \narguments and a new algorithm for linear programming with exponentially many \nconstraints. \n</p>"}, "author": "Tongyi Cao, Akshay Krishnamurthy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787736", "id": "tag:google.com,2005:reader/item/00000003388e93ac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "The Doctor Just Won't Accept That!. (arXiv:1711.08037v1 [stat.ML])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08037"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08037", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Calls to arms to build interpretable models express a well-founded discomfort \nwith machine learning. Should a software agent that does not even know what a \nloan is decide who qualifies for one? Indeed, we ought to be cautious about \ninjecting machine learning (or anything else, for that matter) into \napplications where there may be a significant risk of causing social harm. \nHowever, claims that stakeholders \"just won't accept that!\" do not provide a \nsufficient foundation for a proposed field of study. For the field of \ninterpretable machine learning to advance, we must ask the following questions: \nWhat precisely won't various stakeholders accept? What do they want? Are these \ndesiderata reasonable? Are they feasible? In order to answer these questions, \nwe'll have to give real-world problems and their respective stakeholders \ngreater consideration. \n</p>"}, "author": "Zachary C. Lipton", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787735", "id": "tag:google.com,2005:reader/item/00000003388e93b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "\"I know it when I see it\". Visualization and Intuitive Interpretability. (arXiv:1711.08042v1 [stat.ML])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08042"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08042", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Most research on the interpretability of machine learning systems focuses on \nthe development of a more rigorous notion of interpretability. I suggest that a \nbetter understanding of the deficiencies of the intuitive notion of \ninterpretability is needed as well. I show that visualization enables but also \nimpedes intuitive interpretability, as it presupposes two levels of technical \npre-interpretation: dimensionality reduction and regularization. Furthermore, I \nargue that the use of positive concepts to emulate the distributed semantic \nstructure of machine learning models introduces a significant human bias into \nthe model. As a consequence, I suggest that, if intuitive interpretability is \nneeded, singular representations of internal model states should be avoided. \n</p>"}, "author": "Fabian Offert", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787734", "id": "tag:google.com,2005:reader/item/00000003388e93bb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "A generative adversarial framework for positive-unlabeled classification. (arXiv:1711.08054v1 [cs.LG])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08054"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08054", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we consider the task of classifying the binary \npositive-unlabeled (PU) data. The existing discriminative learning based PU \nmodels attempt to seek an optimal re-weighting strategy for U data, so that a \ndecent decision boundary can be found. In contrast, we provide a totally new \nparadigm to attack the binary PU task, from perspective of generative learning \nby leveraging the powerful generative adversarial networks (GANs). Our \ngenerative positive-unlabeled (GPU) learning model is devised to express P and \nN data distributions. It comprises of three discriminators and two generators \nwith different roles, producing both positive and negative samples that \nresemble those come from the real training dataset. Even with rather limited \nlabeled P data, our GPU framework is capable of capturing the underlying P and \nN data distribution with infinite realistic sample streams. In this way, an \noptimal classifier can be trained on those generated samples using a very deep \nneural networks (DNNs). Moreover, an useful variant of GPU is also introduced \nfor semi-supervised classification. \n</p>"}, "author": "Ming Hou, Qibin Zhao, Chao Li, Brahim Chaib-draa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787733", "id": "tag:google.com,2005:reader/item/00000003388e93c7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Clonal analysis of newborn hippocampal dentate granule cell proliferation and development in temporal lobe epilepsy. (arXiv:1711.08063v1 [stat.ML])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08063"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08063", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c6e6610f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c6e6610f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Hippocampal dentate granule cells are among the few neuronal cell types \ngenerated throughout adult life in mammals. In the normal brain, new granule \ncells are generated from progenitors in the subgranular zone and integrate in a \ntypical fashion. During the development of epilepsy, granule cell integration \nis profoundly altered. The new cells migrate to ectopic locations and develop \nmisoriented basal dendrites. Although it has been established that these \nabnormal cells are newly generated, it is not known whether they arise \nubiquitously throughout the progenitor cell pool or are derived from a smaller \nnumber of bad actor progenitors. To explore this question, we conducted a \nclonal analysis study in mice expressing the Brainbow fluorescent protein \nreporter construct in dentate granule cell progenitors. Mice were examined 2 \nmonths after pilocarpine-induced status epilepticus, a treatment that leads to \nthe development of epilepsy. Brain sections were rendered translucent so that \nentire hippocampi could be reconstructed and all fluorescently labeled cells \nidentified. Our findings reveal that a small number of progenitors produce the \nmajority of ectopic cells following status epilepticus, indicating that either \nthe affected progenitors or their local microenvironments have become \npathological. By contrast, granule cells with basal dendrites were equally \ndistributed among clonal groups. This indicates that these progenitors can \nproduce normal cells and suggests that global factors sporadically disrupt the \ndendritic development of some new cells. Together, these findings strongly \npredict that distinct mechanisms regulate different aspects \n</p>"}, "author": "Shatrunjai P. Singh, Candi L. LaSarge, Amen An, John J. McAuliffe, Steve C. Danzer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787732", "id": "tag:google.com,2005:reader/item/00000003388e93cb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "SNeCT: Scalable network constrained Tucker decomposition for integrative multi-platform data analysis. (arXiv:1711.08095v1 [cs.LG])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08095"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08095", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c6eec2a1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c6eec2a1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Motivation: How do we integratively analyze large-scale multi-platform \ngenomic data that are high dimensional and sparse? Furthermore, how can we \nincorporate prior knowledge, such as the association between genes, in the \nanalysis systematically? Method: To solve this problem, we propose a Scalable \nNetwork Constrained Tucker decomposition method we call SNeCT. SNeCT adopts \nparallel stochastic gradient descent approach on the proposed parallelizable \nnetwork constrained optimization function. SNeCT decomposition is applied to \ntensor constructed from large scale multi-platform multi-cohort cancer data, \nPanCan12, constrained on a network built from PathwayCommons database. Results: \nThe decomposed factor matrices are applied to stratify cancers, to search for \ntop-k similar patients, and to illustrate how the matrices can be used for \npersonalized interpretation. In the stratification test, combined twelve-cohort \ndata is clustered to form thirteen subclasses. The thirteen subclasses have a \nhigh correlation to tissue of origin in addition to other interesting \nobservations, such as clear separation of OV cancers to two groups, and high \nclinical correlation within subclusters formed in cohorts BRCA and UCEC. In the \ntop-k search, a new patient's genomic profile is generated and searched against \nexisting patients based on the factor matrices. The similarity of the top-k \npatient to the query is high for 23 clinical features, including \nestrogen/progesterone receptor statuses of BRCA patients with average precision \nvalue ranges from 0.72 to 0.86 and from 0.68 to 0.86, respectively. We also \nprovide an illustration of how the factor matrices can be used for \ninterpretable personalized analysis of each patient. \n</p>"}, "author": "Dongjin Choi, Lee Sael", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787731", "id": "tag:google.com,2005:reader/item/00000003388e93d3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Locally Smoothed Neural Networks. (arXiv:1711.08132v1 [cs.LG])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08132"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08132", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolutional Neural Networks (CNN) and the locally connected layer are \nlimited in capturing the importance and relations of different local receptive \nfields, which are often crucial for tasks such as face verification, visual \nquestion answering, and word sequence prediction. To tackle the issue, we \npropose a novel locally smoothed neural network (LSNN) in this paper. The main \nidea is to represent the weight matrix of the locally connected layer as the \nproduct of the kernel and the smoother, where the kernel is shared over \ndifferent local receptive fields, and the smoother is for determining the \nimportance and relations of different local receptive fields. Specifically, a \nmulti-variate Gaussian function is utilized to generate the smoother, for \nmodeling the location relations among different local receptive fields. \nFurthermore, the content information can also be leveraged by setting the mean \nand precision of the Gaussian function according to the content. Experiments on \nsome variant of MNIST clearly show our advantages over CNN and locally \nconnected layer. \n</p>"}, "author": "Liang Pang, Yanyan Lan, Jun Xu, Jiafeng Guo, Xueqi Cheng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787730", "id": "tag:google.com,2005:reader/item/00000003388e93d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "An Interpretable and Sparse Neural Network Model for Nonlinear Granger Causality Discovery. (arXiv:1711.08160v1 [stat.ML])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08160"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08160", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>While most classical approaches to Granger causality detection repose upon \nlinear time series assumptions, many interactions in neuroscience and economics \napplications are nonlinear. We develop an approach to nonlinear Granger \ncausality detection using multilayer perceptrons where the input to the network \nis the past time lags of all series and the output is the future value of a \nsingle series. A sufficient condition for Granger non-causality in this setting \nis that all of the outgoing weights of the input data, the past lags of a \nseries, to the first hidden layer are zero. For estimation, we utilize a group \nlasso penalty to shrink groups of input weights to zero. We also propose a \nhierarchical penalty for simultaneous Granger causality and lag estimation. We \nvalidate our approach on simulated data from both a sparse linear \nautoregressive model and the sparse and nonlinear Lorenz-96 model. \n</p>"}, "author": "Alex Tank, Ian Cover, Nicholas J. Foti, Ali Shojaie, Emily B. Fox", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787729", "id": "tag:google.com,2005:reader/item/00000003388e93ea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Hypergraph $p$-Laplacian: A Differential Geometry View. (arXiv:1711.08171v1 [stat.ML])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08171"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08171", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The graph Laplacian plays key roles in information processing of relational \ndata, and has analogies with the Laplacian in differential geometry. In this \npaper, we generalize the analogy between graph Laplacian and differential \ngeometry to the hypergraph setting, and propose a novel hypergraph \n$p$-Laplacian. Unlike the existing two-node graph Laplacians, this \ngeneralization makes it possible to analyze hypergraphs, where the edges are \nallowed to connect any number of nodes. Moreover, we propose a semi-supervised \nlearning method based on the proposed hypergraph $p$-Laplacian, and formalize \nthem as the analogue to the Dirichlet problem, which often appears in physics. \nWe further explore theoretical connections to normalized hypergraph cut on a \nhypergraph, and propose normalized cut corresponding to hypergraph \n$p$-Laplacian. The proposed $p$-Laplacian is shown to outperform standard \nhypergraph Laplacians in the experiment on a hypergraph semi-supervised \nlearning and normalized cut setting. \n</p>"}, "author": "Shota Saito, Danilo P Mandic, Hideyuki Suzuki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787728", "id": "tag:google.com,2005:reader/item/00000003388e93ee", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Post-hoc labeling of arbitrary EEG recordings for data-efficient evaluation of neural decoding methods. (arXiv:1711.08208v1 [cs.LG])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08208"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08208", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many cognitive, sensory and motor processes have correlates in oscillatory \nneural sources, which are embedded as a subspace into the recorded brain \nsignals. Decoding such processes from noisy \nmagnetoencephalogram/electroencephalogram (M/EEG) signals usually requires the \nuse of data-driven analysis methods. The objective evaluation of such decoding \nalgorithms on experimental raw signals, however, is a challenge: the amount of \navailable M/EEG data typically is limited, labels can be unreliable, and raw \nsignals often are contaminated with artifacts. The latter is specifically \nproblematic, if the artifacts stem from behavioral confounds of the oscillatory \nneural processes of interest. \n</p> \n<p>To overcome some of these problems, simulation frameworks have been \nintroduced for benchmarking decoding methods. Generating artificial brain \nsignals, however, most simulation frameworks make strong and partially \nunrealistic assumptions about brain activity, which limits the generalization \nof obtained results to real-world conditions. \n</p> \n<p>In the present contribution, we thrive to remove many shortcomings of current \nsimulation frameworks and propose a versatile alternative, that allows for \nobjective evaluation and benchmarking of novel data-driven decoding methods for \nneural signals. Its central idea is to utilize post-hoc labelings of arbitrary \nM/EEG recordings. This strategy makes it paradigm-agnostic and allows to \ngenerate comparatively large datasets with noiseless labels. Source code and \ndata of the novel simulation approach are made available for facilitating its \nadoption. \n</p>"}, "author": "Sebastian Casta&#xf1;o-Candamil, Andreas Meinel, Michael Tangermann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787727", "id": "tag:google.com,2005:reader/item/00000003388e93fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Adversarial Phenomenon in the Eyes of Bayesian Deep Learning. (arXiv:1711.08244v1 [stat.ML])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08244"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08244", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep Learning models are vulnerable to adversarial examples, i.e.\\ images \nobtained via deliberate imperceptible perturbations, such that the model \nmisclassifies them with high confidence. However, class confidence by itself is \nan incomplete picture of uncertainty. We therefore use principled Bayesian \nmethods to capture model uncertainty in prediction for observing adversarial \nmisclassification. We provide an extensive study with different Bayesian neural \nnetworks attacked in both white-box and black-box setups. The behaviour of the \nnetworks for noise, attacks and clean test data is compared. We observe that \nBayesian neural networks are uncertain in their predictions for adversarial \nperturbations, a behaviour similar to the one observed for random Gaussian \nperturbations. Thus, we conclude that Bayesian neural networks can be \nconsidered for detecting adversarial examples. \n</p>"}, "author": "Ambrish Rawat, Martin Wistuba, Maria-Irina Nicolae", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787726", "id": "tag:google.com,2005:reader/item/00000003388e940a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Decomposition Strategies for Constructive Preference Elicitation. (arXiv:1711.08247v1 [stat.ML])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08247"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08247", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We tackle the problem of constructive preference elicitation, that is the \nproblem of learning user preferences over very large decision problems, \ninvolving a combinatorial space of possible outcomes. In this setting, the \nsuggested configuration is synthesized on-the-fly by solving a constrained \noptimization problem, while the preferences are learned itera tively by \ninteracting with the user. Previous work has shown that Coactive Learning is a \nsuitable method for learning user preferences in constructive scenarios. In \nCoactive Learning the user provides feedback to the algorithm in the form of an \nimprovement to a suggested configuration. When the problem involves many \ndecision variables and constraints, this type of interaction poses a \nsignificant cognitive burden on the user. We propose a decomposition technique \nfor large preference-based decision problems relying exclusively on inference \nand feedback over partial configurations. This has the clear advantage of \ndrastically reducing the user cognitive load. Additionally, part-wise inference \ncan be (up to exponentially) less computationally demanding than inference over \nfull configurations. We discuss the theoretical implications of working with \nparts and present promising empirical results on one synthetic and two \nrealistic constructive problems. \n</p>"}, "author": "Paolo Dragone, Stefano Teso, Mohit Kumar, Andrea Passerini", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787725", "id": "tag:google.com,2005:reader/item/00000003388e9410", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "GraphGAN: Graph Representation Learning with Generative Adversarial Nets. (arXiv:1711.08267v1 [cs.LG])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08267"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08267", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The goal of graph representation learning is to embed each vertex in a graph \ninto a low-dimensional vector space. Existing graph representation learning \nmethods can be classified into two categories: generative models that learn the \nunderlying connectivity distribution in the graph, and discriminative models \nthat predict the probability of edge existence between a pair of vertices. In \nthis paper, we propose GraphGAN, an innovative graph representation learning \nframework unifying above two classes of methods, in which the generative model \nand discriminative model play a game-theoretical minimax game. Specifically, \nfor a given vertex, the generative model tries to fit its underlying true \nconnectivity distribution over all other vertices and produces \"fake\" samples \nto fool the discriminative model, while the discriminative model tries to \ndetect whether the sampled vertex is from ground truth or generated by the \ngenerative model. With the competition between these two models, both of them \ncan alternately and iteratively boost their performance. Moreover, when \nconsidering the implementation of generative model, we propose a novel graph \nsoftmax to overcome the limitations of traditional softmax function, which can \nbe proven satisfying desirable properties of normalization, graph structure \nawareness, and computational efficiency. Through extensive experiments on \nreal-world datasets, we demonstrate that GraphGAN achieves substantial gains in \na variety of applications, including link prediction, node classification, and \nrecommendation, over state-of-the-art baselines. \n</p>"}, "author": "Hongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, Minyi Guo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787724", "id": "tag:google.com,2005:reader/item/00000003388e941d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Unleashing the Potential of CNNs for Interpretable Few-Shot Learning. (arXiv:1711.08277v1 [cs.CV])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08277"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08277", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolutional neural networks (CNNs) have been generally acknowledged as one \nof the driving forces for the advancement of computer vision. Despite their \npromising performances on many tasks, CNNs still face major obstacles on the \nroad to achieving ideal machine intelligence. One is the difficulty of \ninterpreting them and understanding their inner workings, which is important \nfor diagnosing their failures and correcting them. Another is that standard \nCNNs require large amounts of annotated data, which is sometimes very hard to \nobtain. Hence, it is desirable to enable them to learn from few examples. In \nthis work, we address these two limitations of CNNs by developing novel and \ninterpretable models for few-shot learning. Our models are based on the idea of \nencoding objects in terms of visual concepts, which are interpretable visual \ncues represented within CNNs. We first use qualitative visualizations and \nquantitative statistics, to uncover several key properties of feature encoding \nusing visual concepts. Motivated by these properties, we present two intuitive \nmodels for the problem of few-shot learning. Experiments show that our models \nachieve competitive performances, while being much more flexible and \ninterpretable than previous state-of-the-art few-shot learning methods. We \nconclude that visual concepts expose the natural capability of CNNs for \nfew-shot learning. \n</p>"}, "author": "Boyang Deng, Qing Liu, Siyuan Qiao, Alan Yuille", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787723", "id": "tag:google.com,2005:reader/item/00000003388e9428", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Utilizing artificial neural networks to predict demand for weather-sensitive products at retail stores. (arXiv:1711.08325v1 [cs.LG])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08325"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08325", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c6eec5fc\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c6eec5fc&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>One key requirement for effective supply chain management is the quality of \nits inventory management. Various inventory management methods are typically \nemployed for different types of products based on their demand patterns, \nproduct attributes, and supply network. In this paper, our goal is to develop \nrobust demand prediction methods for weather sensitive products at retail \nstores. We employ historical datasets from Walmart, whose customers and markets \nare often exposed to extreme weather events which can have a huge impact on \nsales regarding the affected stores and products. We want to accurately predict \nthe sales of 111 potentially weather-sensitive products around the time of \nmajor weather events at 45 of Walmart retails locations in the U.S. \nIntuitively, we may expect an uptick in the sales of umbrellas before a big \nthunderstorm, but it is difficult for replenishment managers to predict the \nlevel of inventory needed to avoid being out-of-stock or overstock during and \nafter that storm. While they rely on a variety of vendor tools to predict sales \naround extreme weather events, they mostly employ a time-consuming process that \nlacks a systematic measure of effectiveness. We employ all the methods critical \nto any analytics project and start with data exploration. Critical features are \nextracted from the raw historical dataset for demand forecasting accuracy and \nrobustness. In particular, we employ Artificial Neural Network for forecasting \ndemand for each product sold around the time of major weather events. Finally, \nwe evaluate our model to evaluate their accuracy and robustness. \n</p>"}, "author": "Elham Taghizadeh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787722", "id": "tag:google.com,2005:reader/item/00000003388e9430", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Adaptive Cardinality Estimation. (arXiv:1711.08330v1 [cs.DB])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08330"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08330", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we address cardinality estimation problem which is an important \nsubproblem in query optimization. Query optimization is a part of every \nrelational DBMS responsible for finding the best way of the execution for the \ngiven query. These ways are called plans. The execution time of different plans \nmay differ by several orders, so query optimizer has a great influence on the \nwhole DBMS performance. We consider cost-based query optimization approach as \nthe most popular one. It was observed that cost-based optimization quality \ndepends much on cardinality estimation quality. Cardinality of the plan node is \nthe number of tuples returned by it. \n</p> \n<p>In the paper we propose a novel cardinality estimation approach with the use \nof machine learning methods. The main point of the approach is using query \nexecution statistics of the previously executed queries to improve cardinality \nestimations. We called this approach adaptive cardinality estimation to reflect \nthis point. The approach is general, flexible, and easy to implement. The \nexperimental evaluation shows that this approach significantly increases the \nquality of cardinality estimation, and therefore increases the DBMS performance \nfor some queries by several times or even by several dozens of times. \n</p>"}, "author": "Oleg Ivanov, Sergey Bartunov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787721", "id": "tag:google.com,2005:reader/item/00000003388e9438", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Learning User Preferences to Incentivize Exploration in the Sharing Economy. (arXiv:1711.08331v1 [cs.LG])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08331"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08331", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study platforms in the sharing economy and discuss the need for \nincentivizing users to explore options that otherwise would not be chosen. For \ninstance, rental platforms such as Airbnb typically rely on customer reviews to \nprovide users with relevant information about different options. Yet, often a \nlarge fraction of options does not have any reviews available. Such options are \nfrequently neglected as viable choices, and in turn are unlikely to be \nevaluated, creating a vicious cycle. Platforms can engage users to deviate from \ntheir preferred choice by offering monetary incentives for choosing a different \noption instead. To efficiently learn the optimal incentives to offer, we \nconsider structural information in user preferences and introduce a novel \nalgorithm - Coordinated Online Learning (CoOL) - for learning with structural \ninformation modeled as convex constraints. We provide formal guarantees on the \nperformance of our algorithm and test the viability of our approach in a user \nstudy with data of apartments on Airbnb. Our findings suggest that our approach \nis well-suited to learn appropriate incentives and increase exploration on the \ninvestigated platform. \n</p>"}, "author": "Christoph Hirnschall, Adish Singla, Sebastian Tschiatschek, Andreas Krause", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787720", "id": "tag:google.com,2005:reader/item/00000003388e9441", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "DeepSign: Deep Learning for Automatic Malware Signature Generation and Classification. (arXiv:1711.08336v1 [cs.CR])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08336"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08336", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a novel deep learning based method for automatic malware \nsignature generation and classification. The method uses a deep belief network \n(DBN), implemented with a deep stack of denoising autoencoders, generating an \ninvariant compact representation of the malware behavior. While conventional \nsignature and token based methods for malware detection do not detect a \nmajority of new variants for existing malware, the results presented in this \npaper show that signatures generated by the DBN allow for an accurate \nclassification of new malware variants. Using a dataset containing hundreds of \nvariants for several major malware families, our method achieves 98.6% \nclassification accuracy using the signatures generated by the DBN. The \npresented method is completely agnostic to the type of malware behavior that is \nlogged (e.g., API calls and their parameters, registry entries, websites and \nports accessed, etc.), and can use any raw input from a sandbox to successfully \ntrain the deep neural network which is used to generate malware signatures. \n</p>"}, "author": "Eli David, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787719", "id": "tag:google.com,2005:reader/item/00000003388e944a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Genetic Algorithms for Evolving Computer Chess Programs. (arXiv:1711.08337v1 [cs.NE])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08337"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08337", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper demonstrates the use of genetic algorithms for evolving: 1) a \ngrandmaster-level evaluation function, and 2) a search mechanism for a chess \nprogram, the parameter values of which are initialized randomly. The evaluation \nfunction of the program is evolved by learning from databases of (human) \ngrandmaster games. At first, the organisms are evolved to mimic the behavior of \nhuman grandmasters, and then these organisms are further improved upon by means \nof coevolution. The search mechanism is evolved by learning from tactical test \nsuites. Our results show that the evolved program outperforms a two-time world \ncomputer chess champion and is at par with the other leading computer chess \nprograms. \n</p>"}, "author": "Eli David, H. Jaap van den Herik, Moshe Koppel, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787718", "id": "tag:google.com,2005:reader/item/00000003388e944f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Likelihood Almost Free Inference Networks. (arXiv:1711.08352v1 [cs.LG])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08352"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08352", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Variational inference for latent variable models is prevalent in various \nmachine learning problems, typically solved by maximizing the Evidence Lower \nBound (ELBO) of the true data likelihood with respect to a variational \ndistribution. However, freely enriching the family of variational distribution \nis challenging since the ELBO requires variational likelihood evaluations of \nthe latent variables. In this paper, we propose a novel framework to enrich the \nvariational family based on an alternative lower bound, by introducing \nauxiliary random variables to the variational distribution only. While offering \na much richer family of complex variational distributions, the resulting \ninference network is likelihood almost free in the sense that only the latent \nvariables require evaluations from simple likelihoods and samples from all the \nauxiliary variables are sufficient for maximum likelihood inference. We show \nthat the proposed approach is essentially optimizing a probabilistic mixture of \nELBOs, thus enriching modeling capacity and enhancing robustness. It \noutperforms state-of-the-art methods in our experiments on several density \nestimation tasks. \n</p>"}, "author": "Guoqing Zheng, Yiming Yang, Jaime Carbonell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787717", "id": "tag:google.com,2005:reader/item/00000003388e9455", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Riemannian tangent space mapping and elastic net regularization for cost-effective EEG markers of brain atrophy in Alzheimer's disease. (arXiv:1711.08359v1 [stat.ML])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08359"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08359", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The diagnosis of Alzheimer's disease (AD) in routine clinical practice is \nmost commonly based on subjective clinical interpretations. Quantitative \nelectroencephalography (QEEG) measures have been shown to reflect \nneurodegenerative processes in AD and might qualify as affordable and thereby \nwidely available markers to facilitate the objectivization of AD assessment. \nHere, we present a novel framework combining Riemannian tangent space mapping \nand elastic net regression for the development of brain atrophy markers. While \nmost AD QEEG studies are based on small sample sizes and psychological test \nscores as outcome measures, here we train and test our models using data of one \nof the largest prospective EEG AD trials ever conducted, including MRI \nbiomarkers of brain atrophy. \n</p>"}, "author": "Wolfgang Fr&#xfc;hwirt, Matthias Gerstgrasser, Pengfei Zhang, Leonard Weydemann, Markus Waser, Reinhold Schmidt, Thomas Benke, Peter Dal-Bianco, Gerhard Ransmayr, Dieter Grossegger, Heinrich Garn, Gareth W. Peters, Stephen Roberts, Georg Dorffner", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787716", "id": "tag:google.com,2005:reader/item/00000003388e9483", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "ForestHash: Semantic Hashing With Shallow Random Forests and Tiny Convolutional Networks. (arXiv:1711.08364v1 [cs.CV])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08364"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08364", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Hash codes are efficient data representations for coping with the ever \ngrowing amounts of data. In this paper, we introduce a random forest semantic \nhashing scheme that embeds tiny convolutional neural networks (CNN) into \nshallow random forests, with near-optimal information-theoretic code \naggregation among trees. We start with a simple hashing scheme, where random \ntrees in a forest act as hashing functions by setting `1' for the visited tree \nleaf, and `0' for the rest. We show that traditional random forests fail to \ngenerate hashes that preserve the underlying similarity between the trees, \nrendering the random forests approach to hashing challenging. To address this, \nwe propose to first randomly group arriving classes at each tree split node \ninto two groups, obtaining a significantly simplified two-class classification \nproblem, which can be handled using a light-weight CNN weak learner. Such \nrandom class grouping scheme enables code uniqueness by enforcing each class to \nshare its code with different classes in different trees. A non-conventional \nlow-rank loss is further adopted for the CNN weak learners to encourage code \nconsistency by minimizing intra-class variations and maximizing inter-class \ndistance for the two random class groups. Finally, we introduce an \ninformation-theoretic approach for aggregating codes of individual trees into a \nsingle hash code, producing a near-optimal unique hash for each class. The \nproposed approach significantly outperforms state-of-the-art hashing methods \nfor image retrieval tasks on large-scale public datasets, while performing at \nthe level of other state-of-the-art image classification techniques while \nutilizing a more compact and efficient scalable representation. This work \nproposes a principled and robust procedure to train and deploy in parallel an \nensemble of light-weight CNNs, instead of simply going deeper. \n</p>"}, "author": "Qiang Qiu, Jose Lezama, Alex Bronstein, Guillermo Sapiro", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787715", "id": "tag:google.com,2005:reader/item/00000003388e94b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Variational Bayesian Inference For A Scale Mixture Of Normal Distributions Handling Missing Data. (arXiv:1711.08374v1 [stat.ML])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08374"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08374", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, a scale mixture of Normal distributions model is developed for \nclassification and clustering of data having outliers and missing values. The \nclassification method, based on a mixture model, focuses on the introduction of \nlatent variables that gives us the possibility to handle sensitivity of model \nto outliers and to allow a less restrictive modelling of missing data. \nInference is processed through a Variational Bayesian Approximation and a \nBayesian treatment is adopted for model learning, supervised classification and \nclustering. \n</p>"}, "author": "G. Revillon, A. Djafari, C. Enderli", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787714", "id": "tag:google.com,2005:reader/item/00000003388e94ba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "An Efficient ADMM Algorithm for Structural Break Detection in Multivariate Time Series. (arXiv:1711.08392v1 [stat.ML])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08392"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08392", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present an efficient alternating direction method of multipliers (ADMM) \nalgorithm for segmenting a multivariate non-stationary time series with \nstructural breaks into stationary regions. We draw from recent work where the \nseries is assumed to follow a vector autoregressive model within segments and a \nconvex estimation procedure may be formulated using group fused lasso \npenalties. Our ADMM approach first splits the convex problem into a global \nquadratic program and a simple group lasso proximal update. We show that the \nglobal problem may be parallelized over rows of the time dependent transition \nmatrices and furthermore that each subproblem may be rewritten in a form \nidentical to the log-likelihood of a Gaussian state space model. Consequently, \nwe develop a Kalman smoothing algorithm to solve the global update in time \nlinear in the length of the series. \n</p>"}, "author": "Alex Tank, Emily B. Fox, Ali Shojaie", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787713", "id": "tag:google.com,2005:reader/item/00000003388e94c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "SolarisNet: A Deep Regression Network for Solar Radiation Prediction. (arXiv:1711.08413v1 [cs.CV])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08413"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08413", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c6eec99d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c6eec99d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Effective utilization of photovoltaic (PV) plants requires weather \nvariability robust global solar radiation (GSR) forecasting models. Random \nweather turbulence phenomena coupled with assumptions of clear sky model as \nsuggested by Hottel pose significant challenges to parametric &amp; non-parametric \nmodels in GSR conversion rate estimation. Also, a decent GSR estimate requires \ncostly high-tech radiometer and expert dependent instrument handling and \nmeasurements, which are subjective. As such, a computer aided monitoring (CAM) \nsystem to evaluate PV plant operation feasibility by employing smart grid past \ndata analytics and deep learning is developed. Our algorithm, SolarisNet is a \n6-layer deep neural network trained on data collected at two weather stations \nlocated near Kalyani metrological site, West Bengal, India. The daily GSR \nprediction performance using SolarisNet outperforms the existing state of art \nand its efficacy in inferring past GSR data insights to comprehend daily and \nseasonal GSR variability along with its competence for short term forecasting \nis discussed. \n</p>"}, "author": "Subhadip Dey, Sawon Pratiher, Saon Banerjee, Chanchal Kumar Mukherjee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787712", "id": "tag:google.com,2005:reader/item/00000003388e94d6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Relief-Based Feature Selection: Introduction and Review. (arXiv:1711.08421v1 [cs.DS])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08421"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08421", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c6f85520\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c6f85520&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Feature selection plays a critical role in data mining, driven by increasing \nfeature dimensionality in target problems and growing interest in advanced but \ncomputationally expensive methodologies able to model complex associations. \nSpecifically, there is a need for feature selection methods that are \ncomputationally efficient, yet sensitive to complex patterns of association, \ne.g. interactions, so that informative features are not mistakenly eliminated \nprior to downstream modeling. This paper focuses on Relief-based algorithms \n(RBAs), a unique family of filter-style feature selection algorithms that \nstrike an effective balance between these objectives while flexibly adapting to \nvarious data characteristics, e.g. classification vs. regression. First, this \nwork broadly examines types of feature selection and defines RBAs within that \ncontext. Next, we introduce the original Relief algorithm and associated \nconcepts, emphasizing the intuition behind how it works, how feature weights \ngenerated by the algorithm can be interpreted, and why it is sensitive to \nfeature interactions without evaluating combinations of features. Lastly, we \ninclude an expansive review of RBA methodological research beyond Relief and \nits popular descendant, ReliefF. In particular, we characterize branches of RBA \nresearch, and provide comparative summaries of RBA algorithms including \ncontributions, strategies, functionality, time complexity, adaptation to key \ndata characteristics, and software availability. \n</p>"}, "author": "Ryan J. Urbanowicz, Melissa Meeker, William LaCava, Randal S. Olson, Jason H. Moore", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787711", "id": "tag:google.com,2005:reader/item/00000003388e94e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Leverage Score Sampling for Faster Accelerated Regression and ERM. (arXiv:1711.08426v1 [stat.ML])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08426"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08426", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Given a matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ and a vector $b \n\\in\\mathbb{R}^{d}$, we show how to compute an $\\epsilon$-approximate solution \nto the regression problem $ \\min_{x\\in\\mathbb{R}^{d}}\\frac{1}{2} \\|\\mathbf{A} x \n- b\\|_{2}^{2} $ in time $ \\tilde{O} ((n+\\sqrt{d\\cdot\\kappa_{\\text{sum}}})\\cdot \ns\\cdot\\log\\epsilon^{-1}) $ where \n$\\kappa_{\\text{sum}}=\\mathrm{tr}\\left(\\mathbf{A}^{\\top}\\mathbf{A}\\right)/\\lambda_{\\min}(\\mathbf{A}^{T}\\mathbf{A})$ \nand $s$ is the maximum number of non-zero entries in a row of $\\mathbf{A}$. Our \nalgorithm improves upon the previous best running time of $ \\tilde{O} \n((n+\\sqrt{n \\cdot\\kappa_{\\text{sum}}})\\cdot s\\cdot\\log\\epsilon^{-1})$. \n</p> \n<p>We achieve our result through a careful combination of leverage score \nsampling techniques, proximal point methods, and accelerated coordinate \ndescent. Our method not only matches the performance of previous methods, but \nfurther improves whenever leverage scores of rows are small (up to \npolylogarithmic factors). We also provide a non-linear generalization of these \nresults that improves the running time for solving a broader class of ERM \nproblems. \n</p>"}, "author": "Naman Agarwal, Sham Kakade, Rahul Kidambi, Yin Tat Lee, Praneeth Netrapalli, Aaron Sidford", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787710", "id": "tag:google.com,2005:reader/item/00000003388e94f8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "From Monte Carlo to Las Vegas: Improving Restricted Boltzmann Machine Training Through Stopping Sets. (arXiv:1711.08442v1 [cs.LG])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08442"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08442", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a Las Vegas transformation of Markov Chain Monte Carlo (MCMC) \nestimators of Restricted Boltzmann Machines (RBMs). We denote our approach \nMarkov Chain Las Vegas (MCLV). MCLV gives statistical guarantees in exchange \nfor random running times. MCLV uses a stopping set built from the training data \nand has maximum number of Markov chain steps K (referred as MCLV-K). We present \na MCLV-K gradient estimator (LVS-K) for RBMs and explore the correspondence and \ndifferences between LVS-K and Contrastive Divergence (CD-K), with LVS-K \nsignificantly outperforming CD-K training RBMs over the MNIST dataset, \nindicating MCLV to be a promising direction in learning generative models. \n</p>"}, "author": "Pedro H. P. Savarese, Mayank Kakodkar, Bruno Ribeiro", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511428260788", "timestampUsec": "1511428260787709", "id": "tag:google.com,2005:reader/item/00000003388e9509", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Causal nearest neighbor rules for optimal treatment regimes. (arXiv:1711.08451v1 [stat.ML])", "published": 1511428261, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08451"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08451", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The estimation of optimal treatment regimes is of considerable interest to \nprecision medicine. In this work, we propose a causal $k$-nearest neighbor \nmethod to estimate the optimal treatment regime. The method roots in the \nframework of causal inference, and estimates the causal treatment effects \nwithin the nearest neighborhood. Although the method is simple, it possesses \nnice theoretical properties. We show that the causal $k$-nearest neighbor \nregime is universally consistent. That is, the causal $k$-nearest neighbor \nregime will eventually learn the optimal treatment regime as the sample size \nincreases. We also establish its convergence rate. However, the causal \n$k$-nearest neighbor regime may suffer from the curse of dimensionality, i.e. \nperformance deteriorates as dimensionality increases. To alleviate this \nproblem, we develop an adaptive causal $k$-nearest neighbor method to perform \nmetric selection and variable selection simultaneously. The performance of the \nproposed methods is illustrated in simulation studies and in an analysis of a \nchronic depression clinical trial. \n</p>"}, "author": "Xin Zhou, Michael R. Kosorok", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511403796412", "timestampUsec": "1511403796411940", "id": "tag:google.com,2005:reader/item/00000003386138d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "DeepSign: Deep Learning for Automatic Malware Signature Generation and Classification. (arXiv:1711.08336v1 [cs.CR])", "published": 1511403797, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08336"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08336", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a novel deep learning based method for automatic malware \nsignature generation and classification. The method uses a deep belief network \n(DBN), implemented with a deep stack of denoising autoencoders, generating an \ninvariant compact representation of the malware behavior. While conventional \nsignature and token based methods for malware detection do not detect a \nmajority of new variants for existing malware, the results presented in this \npaper show that signatures generated by the DBN allow for an accurate \nclassification of new malware variants. Using a dataset containing hundreds of \nvariants for several major malware families, our method achieves 98.6% \nclassification accuracy using the signatures generated by the DBN. The \npresented method is completely agnostic to the type of malware behavior that is \nlogged (e.g., API calls and their parameters, registry entries, websites and \nports accessed, etc.), and can use any raw input from a sandbox to successfully \ntrain the deep neural network which is used to generate malware signatures. \n</p>"}, "author": "Eli David, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511403796412", "timestampUsec": "1511403796411939", "id": "tag:google.com,2005:reader/item/00000003386138e0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Genetic Algorithms for Evolving Computer Chess Programs. (arXiv:1711.08337v1 [cs.NE])", "published": 1511403797, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.08337"}], "alternate": [{"href": "http://arxiv.org/abs/1711.08337", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper demonstrates the use of genetic algorithms for evolving: 1) a \ngrandmaster-level evaluation function, and 2) a search mechanism for a chess \nprogram, the parameter values of which are initialized randomly. The evaluation \nfunction of the program is evolved by learning from databases of (human) \ngrandmaster games. At first, the organisms are evolved to mimic the behavior of \nhuman grandmasters, and then these organisms are further improved upon by means \nof coevolution. The search mechanism is evolved by learning from tactical test \nsuites. Our results show that the evolved program outperforms a two-time world \ncomputer chess champion and is at par with the other leading computer chess \nprograms. \n</p>"}, "author": "Eli David, H. Jaap van den Herik, Moshe Koppel, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833686", "id": "tag:google.com,2005:reader/item/0000000337f96ec5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "MaMaDroid: Detecting Android Malware by Building Markov Chains of Behavioral Models (Extended Version). (arXiv:1711.07477v1 [cs.CR])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07477"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07477", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>As Android becomes increasingly popular, so does malware targeting it, this \nmotivating the research community to propose many different detection \ntechniques. However, the constant evolution of the Android ecosystem, and of \nmalware itself, makes it hard to design robust tools that can operate for long \nperiods of time without the need for modifications or costly re-training. \nAiming to address this issue, we set to detect malware from a behavioral point \nof view, modeled as the sequence of abstracted API calls. We introduce \nMaMaDroid, a static-analysis based system that abstracts app's API calls to \ntheir class, package, or family, and builds a model from their sequences \nobtained from the call graph of an app as Markov chains. This ensures that the \nmodel is more resilient to API changes and the features set is of manageable \nsize. We evaluate MaMaDroid using a dataset of 8.5K benign and 35.5K malicious \napps collected over a period of six years, showing that it effectively detects \nmalware (with up to 0.99 F-measure) and keeps its detection capabilities for \nlong periods of time (up to 0.87 F-measure two years after training). We also \nshow that MaMaDroid remarkably improves over DroidAPIMiner, a state-of-the-art \ndetection system that relies on the frequency of (raw) API calls. Aiming to \nassess whether MaMaDroid's effectiveness mainly stems from the API abstraction \nor from the sequencing modeling, we also evaluate a variant of it that uses \nfrequency (instead of sequences), of abstracted API calls. We find that it is \nnot as accurate, failing to capture maliciousness when trained on malware \nsamples including API calls that are equally or more frequently used by benign \napps. \n</p>"}, "author": "Lucky Onwuzurike, Enrico Mariconti, Panagiotis Andriotis, Emiliano De Cristofaro, Gordon Ross, Gianluca Stringhini", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833685", "id": "tag:google.com,2005:reader/item/0000000337f96ec9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Implementing the Deep Q-Network. (arXiv:1711.07478v1 [cs.LG])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07478"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07478", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Deep Q-Network proposed by Mnih et al. [2015] has become a benchmark and \nbuilding point for much deep reinforcement learning research. However, \nreplicating results for complex systems is often challenging since original \nscientific publications are not always able to describe in detail every \nimportant parameter setting and software engineering solution. In this paper, \nwe present results from our work reproducing the results of the DQN paper. We \nhighlight key areas in the implementation that were not covered in great detail \nin the original paper to make it easier for researchers to replicate these \nresults, including termination conditions and gradient descent algorithms. \nFinally, we discuss methods for improving the computational performance and \nprovide our own implementation that is designed to work with a range of \ndomains, and not just the original Arcade Learning Environment [Bellemare et \nal., 2013]. \n</p>"}, "author": "Melrose Roderick, James MacGlashan, Stefanie Tellex", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833684", "id": "tag:google.com,2005:reader/item/0000000337f96ecf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Teaching a Machine to Read Maps with Deep Reinforcement Learning. (arXiv:1711.07479v1 [cs.RO])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07479"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07479", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The ability to use a 2D map to navigate a complex 3D environment is quite \nremarkable, and even difficult for many humans. Localization and navigation is \nalso an important problem in domains such as robotics, and has recently become \na focus of the deep reinforcement learning community. In this paper we teach a \nreinforcement learning agent to read a map in order to find the shortest way \nout of a random maze it has never seen before. Our system combines several \nstate-of-the-art methods such as A3C and incorporates novel elements such as a \nrecurrent localization cell. Our agent learns to localize itself based on 3D \nfirst person images and an approximate orientation angle. The agent generalizes \nwell to bigger mazes, showing that it learned useful localization and \nnavigation capabilities. \n</p>"}, "author": "Gino Brunner, Oliver Richter, Yuyi Wang, Roger Wattenhofer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833683", "id": "tag:google.com,2005:reader/item/0000000337f96ed7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "On the Distortion of Voting with Multiple Representative Candidates. (arXiv:1711.07600v1 [cs.GT])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07600"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07600", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c6f858ef\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c6f858ef&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study positional voting rules when candidates and voters are embedded in a \ncommon metric space, and cardinal preferences are naturally given by distances \nin the metric space. In a positional voting rule, each candidate receives a \nscore from each ballot based on the ballot's rank order; the candidate with the \nhighest total score wins the election. The cost of a candidate is his sum of \ndistances to all voters, and the distortion of an election is the ratio between \nthe cost of the elected candidate and the cost of the optimum candidate. We \nconsider the case when candidates are representative of the population, in the \nsense that they are drawn i.i.d. from the population of the voters, and analyze \nthe expected distortion of positional voting rules. \n</p> \n<p>Our main result is a clean and tight characterization of positional voting \nrules that have constant expected distortion (independent of the number of \ncandidates and the metric space). Our characterization result immediately \nimplies constant expected distortion for Borda Count and elections in which \neach voter approves a constant fraction of all candidates. On the other hand, \nwe obtain super-constant expected distortion for Plurality, Veto, and approving \na constant number of candidates. These results contrast with previous results \non voting with metric preferences: When the candidates are chosen \nadversarially, all of the preceding voting rules have distortion linear in the \nnumber of candidates or voters. Thus, the model of representative candidates \nallows us to distinguish voting rules which seem equally bad in the worst case. \n</p>"}, "author": "Yu Cheng, Shaddin Dughmi, David Kempe", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833682", "id": "tag:google.com,2005:reader/item/0000000337f96ede", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning. (arXiv:1711.07613v1 [cs.CV])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07613"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07613", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Visual Dialogue task requires an agent to engage in a conversation about \nan image with a human. It represents an extension of the Visual Question \nAnswering task in that the agent needs to answer a question about an image, but \nit needs to do so in light of the previous dialogue that has taken place. The \nkey challenge in Visual Dialogue is thus maintaining a consistent, and natural \ndialogue while continuing to answer questions correctly. We present a novel \napproach that combines Reinforcement Learning and Generative Adversarial \nNetworks (GANs) to generate more human-like responses to questions. The GAN \nhelps overcome the relative paucity of training data, and the tendency of the \ntypical MLE-based approach to generate overly terse answers. Critically, the \nGAN is tightly integrated into the attention mechanism that generates \nhuman-interpretable reasons for each answer. This means that the discriminative \nmodel of the GAN has the task of assessing whether a candidate answer is \ngenerated by a human or not, given the provided reason. This is significant \nbecause it drives the generative model to produce high quality answers that are \nwell supported by the associated reasoning. The method also generates the \nstate-of-the-art results on the primary benchmark. \n</p>"}, "author": "Qi Wu, Peng Wang, Chunhua Shen, Ian Reid, Anton van den Hengel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833681", "id": "tag:google.com,2005:reader/item/0000000337f96ee5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Asking the Difficult Questions: Goal-Oriented Visual Question Generation via Intermediate Rewards. (arXiv:1711.07614v1 [cs.CV])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07614"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07614", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Despite significant progress in a variety of vision-and-language problems, \ndeveloping a method capable of asking intelligent, goal-oriented questions \nabout images is proven to be an inscrutable challenge. Towards this end, we \npropose a Deep Reinforcement Learning framework based on three new intermediate \nrewards, namely goal-achieved, progressive and informativeness that encourage \nthe generation of succinct questions, which in turn uncover valuable \ninformation towards the overall goal. By directly optimizing for questions that \nwork quickly towards fulfilling the overall goal, we avoid the tendency of \nexisting methods to generate long series of insane queries that add little \nvalue. We evaluate our model on the GuessWhat?! dataset and show that the \nresulting questions can help a standard Guesser identify a specific object in \nan image at a much higher success rate. \n</p>"}, "author": "Junjie Zhang, Qi Wu, Chunhua Shen, Jian Zhang, Jianfeng Lu, Anton van den Hengel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833680", "id": "tag:google.com,2005:reader/item/0000000337f96eec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Groupwise Maximin Fair Allocation of Indivisible Goods. (arXiv:1711.07621v1 [cs.GT])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07621"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07621", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of allocating indivisible goods among n agents in a fair \nmanner. For this problem, maximin share (MMS) is a well-studied solution \nconcept which provides a fairness threshold. Specifically, maximin share is \ndefined as the minimum utility that an agent can guarantee for herself when \nasked to partition the set of goods into n bundles such that the remaining \n(n-1) agents pick their bundles adversarially. An allocation is deemed to be \nfair if every agent gets a bundle whose valuation is at least her maximin \nshare. \n</p> \n<p>Even though maximin shares provide a natural benchmark for fairness, it has \nits own drawbacks and, in particular, it is not sufficient to rule out \nunsatisfactory allocations. Motivated by these considerations, in this work we \ndefine a stronger notion of fairness, called groupwise maximin share guarantee \n(GMMS). In GMMS, we require that the maximin share guarantee is achieved not \njust with respect to the grand bundle, but also among all the subgroups of \nagents. Hence, this solution concept strengthens MMS and provides an ex-post \nfairness guarantee. We show that in specific settings, GMMS allocations always \nexist. We also establish the existence of approximate GMMS allocations under \nadditive valuations, and develop a polynomial-time algorithm to find such \nallocations. Moreover, we establish a scale of fairness wherein we show that \nGMMS implies approximate envy freeness. \n</p> \n<p>Finally, we empirically demonstrate the existence of GMMS allocations in a \nlarge set of randomly generated instances. For the same set of instances, we \nadditionally show that our algorithm achieves an approximation factor better \nthan the established, worst-case bound. \n</p>"}, "author": "Siddharth Barman, Arpita Biswas, Sanath Kumar Krishnamurthy, Y. Narahari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833679", "id": "tag:google.com,2005:reader/item/0000000337f96ef2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Generating Thematic Chinese Poetry with Conditional Variational Autoencoder. (arXiv:1711.07632v1 [cs.CL])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07632"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07632", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Computer poetry generation is our first step towards computer writing. \nWriting must have a theme. The current approaches of using sequence-to-sequence \nmodels with attention often produce non-thematic poems. We present a \nconditional variational autoencoder with augmented word2vec architecture that \nexplicitly represents the topic or theme information. This approach \nsignificantly improves the relevance of the generated poems by representing \neach line of the poem not only in a context-sensitive manner but also in a \nholistic way that is highly related to the given keyword and the learned topic. \nThe proposed augmented word2vec model further improves the rhythm and symmetry. \nWe also present a straightforward evaluation metric RHYTHM score to \nautomatically measure the rule-consistency of generated poems. Tests show that \n45.24% generated poems by our model are judged by humans to be written by real \npeople. \n</p>"}, "author": "Xiaopeng Yang, Xiaowen Lin, Shunda Suo, Ming Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833678", "id": "tag:google.com,2005:reader/item/0000000337f96ef8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Cross Temporal Recurrent Networks for Ranking Question Answer Pairs. (arXiv:1711.07656v1 [cs.CL])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07656"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07656", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Temporal gates play a significant role in modern recurrent-based neural \nencoders, enabling fine-grained control over recursive compositional operations \nover time. In recurrent models such as the long short-term memory (LSTM), \ntemporal gates control the amount of information retained or discarded over \ntime, not only playing an important role in influencing the learned \nrepresentations but also serving as a protection against vanishing gradients. \nThis paper explores the idea of learning temporal gates for sequence pairs \n(question and answer), jointly influencing the learned representations in a \npairwise manner. In our approach, temporal gates are learned via 1D \nconvolutional layers and then subsequently cross applied across question and \nanswer for joint learning. Empirically, we show that this conceptually simple \nsharing of temporal gates can lead to competitive performance across multiple \nbenchmarks. Intuitively, what our network achieves can be interpreted as \nlearning representations of question and answer pairs that are aware of what \neach other is remembering or forgetting, i.e., pairwise temporal gating. Via \nextensive experiments, we show that our proposed model achieves \nstate-of-the-art performance on two community-based QA datasets and competitive \nperformance on one factoid-based QA dataset. \n</p>"}, "author": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833677", "id": "tag:google.com,2005:reader/item/0000000337f96efc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Fullie and Wiselie: A Dual-Stream Recurrent Convolutional Attention Model for Activity Recognition. (arXiv:1711.07661v1 [cs.HC])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07661"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07661", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multimodal features play a key role in wearable sensor based Human Activity \nRecognition (HAR). Selecting the most salient features adaptively is a \npromising way to maximize the effectiveness of multimodal sensor data. In this \nregard, we propose a \"collect fully and select wisely (Fullie and Wiselie)\" \nprinciple as well as a dual-stream recurrent convolutional attention model, \nRecurrent Attention and Activity Frame (RAAF), to improve the recognition \nperformance. We first collect modality features and the relations between each \npair of features to generate activity frames, and then introduce an attention \nmechanism to select the most prominent regions from activity frames precisely. \nThe selected frames not only maximize the utilization of valid features but \nalso reduce the number of features to be computed effectively. We further \nanalyze the hyper-parameters, accuracy, interpretability, and annotation \ndependency of the proposed model based on extensive experiments. The results \nshow that RAAF achieves competitive performance on two benchmarked datasets and \nworks well in real life scenarios. \n</p>"}, "author": "Kaixuan Chen, Lina Yao, Tao Gu, Zhiwen Yu, Xianzhi Wang, Dalin Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833676", "id": "tag:google.com,2005:reader/item/0000000337f96f01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Transferring Agent Behaviors from Videos via Motion GANs. (arXiv:1711.07676v1 [cs.LG])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07676"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07676", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A major bottleneck for developing general reinforcement learning agents is \ndetermining rewards that will yield desirable behaviors under various \ncircumstances. We introduce a general mechanism for automatically specifying \nmeaningful behaviors from raw pixels. In particular, we train a generative \nadversarial network to produce short sub-goals represented through motion \ntemplates. We demonstrate that this approach generates visually meaningful \nbehaviors in unknown environments with novel agents and describe how these \nmotions can be used to train reinforcement learning agents. \n</p>"}, "author": "Ashley D. Edwards, Charles L. Isbell Jr", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833675", "id": "tag:google.com,2005:reader/item/0000000337f96f04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "JamBot: Music Theory Aware Chord Based Generation of Polyphonic Music with LSTMs. (arXiv:1711.07682v1 [cs.SD])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07682"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07682", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a novel approach for the generation of polyphonic music based on \nLSTMs. We generate music in two steps. First, a chord LSTM predicts a chord \nprogression based on a chord embedding. A second LSTM then generates polyphonic \nmusic from the predicted chord progression. The generated music sounds pleasing \nand harmonic, with only few dissonant notes. It has clear long-term structure \nthat is similar to what a musician would play during a jam session. We show \nthat our approach is sensible from a music theory perspective by evaluating the \nlearned chord embeddings. Surprisingly, our simple model managed to extract the \ncircle of fifths, an important tool in music theory, from the dataset. \n</p>"}, "author": "Gino Brunner, Yuyi Wang, Roger Wattenhofer, Jonas Wiesendanger", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833674", "id": "tag:google.com,2005:reader/item/0000000337f96f07", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Hidden Tree Markov Networks: Deep and Wide Learning for Structured Data. (arXiv:1711.07784v1 [cs.LG])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07784"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07784", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The paper introduces the Hidden Tree Markov Network (HTN), a \nneuro-probabilistic hybrid fusing the representation power of generative models \nfor trees with the incremental and discriminative learning capabilities of \nneural networks. We put forward a modular architecture in which multiple \ngenerative models of limited complexity are trained to learn structural feature \ndetectors whose outputs are then combined and integrated by neural layers at a \nlater stage. In this respect, the model is both deep, thanks to the unfolding \nof the generative models on the input structures, as well as wide, given the \npotentially large number of generative modules that can be trained in parallel. \nExperimental results show that the proposed approach can outperform \nstate-of-the-art syntactic kernels as well as generative kernels built on the \nsame probabilistic model as the HTN. \n</p>"}, "author": "Davide Bacciu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833673", "id": "tag:google.com,2005:reader/item/0000000337f96f09", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Evaluation of bioinspired algorithms for the solution of the job scheduling problem. (arXiv:1711.07821v1 [cs.NE])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07821"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07821", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c6f85c97\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c6f85c97&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this research we used bio-inspired metaheuristics, as artificial immune \nsystems and ant colony algorithms that are based on a number of characteristics \nand behaviors of living things that are interesting in the computer science \narea. This paper presents an evaluation of bio-inspired solutions to \ncombinatorial optimization problem, called the Job Shop Scheduling or planning \nwork, in a simple way the objective is to find a configuration or job stream \nthat has the least amount of time to be executed in machine settings. The \nperformance of the algorithms was characterized and evaluated for reference \ninstances of the job shop scheduling problem, comparing the quality of the \nsolutions obtained with respect to the best known solution of the most \neffective methods. The solutions were evaluated in two aspects, first in \nrelation of quality of solutions, taking as reference the makespan and secondly \nin relation of performance, taking the number evaluations performed by the \nalgorithm to obtain the best solution. \n</p>"}, "author": "Edson Florez, Nelson Diaz, Wilfredo Gomez, Lola Bautista, Dario Delgado", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833672", "id": "tag:google.com,2005:reader/item/0000000337f96f0a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Situationally Aware Options. (arXiv:1711.07832v1 [cs.AI])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07832"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07832", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c70179b5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c70179b5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Hierarchical abstractions, also known as options -- a type of temporally \nextended action (Sutton et. al. 1999) that enables a reinforcement learning \nagent to plan at a higher level, abstracting away from the lower-level details. \nIn this work, we learn reusable options whose parameters can vary, encouraging \ndifferent behaviors, based on the current situation. In principle, these \nbehaviors can include vigor, defence or even risk-averseness. These are some \nexamples of what we refer to in the broader context as Situational Awareness \n(SA). We incorporate SA, in the form of vigor, into hierarchical RL by defining \nand learning situationally aware options in a Probabilistic Goal Semi-Markov \nDecision Process (PG-SMDP). This is achieved using our Situationally Aware \noPtions (SAP) policy gradient algorithm which comes with a theoretical \nconvergence guarantee. We learn reusable options in different scenarios in a \nRoboCup soccer domain (i.e., winning/losing). These options learn to execute \nwith different levels of vigor resulting in human-like behaviours such as \n`time-wasting' in the winning scenario. We show the potential of the agent to \nexit bad local optima using reusable options in RoboCup. Finally, using SAP, \nthe agent mitigates feature-based model misspecification in a Bottomless Pit of \nDeath domain. \n</p>"}, "author": "Daniel J. Mankowitz, Aviv Tamar, Shie Mannor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833671", "id": "tag:google.com,2005:reader/item/0000000337f96f0c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Application of generative autoencoder in de novo molecular design. (arXiv:1711.07839v1 [cs.LG])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07839"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07839", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A major challenge in computational chemistry is the generation of novel \nmolecular structures with desirable pharmacological and physiochemical \nproperties. In this work, we investigate the potential use of autoencoder, a \ndeep learning methodology, for de novo molecular design. Various generative \nautoencoders were used to map molecule structures into a continuous latent \nspace and vice versa and their performance as structure generator was assessed. \nOur results show that the latent space preserves chemical similarity principle \nand thus can be used for the generation of analogue structures. Furthermore, \nthe latent space created by autoencoders were searched systematically to \ngenerate novel compounds with predicted activity against dopamine receptor type \n2 and compounds similar to known active compounds not included in the training \nset were identified. \n</p>"}, "author": "Thomas Blaschke, Marcus Olivecrona, Ola Engkvist, J&#xfc;rgen Bajorath, Hongming Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833670", "id": "tag:google.com,2005:reader/item/0000000337f96f0f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Constructive Preference Elicitation over Hybrid Combinatorial Spaces. (arXiv:1711.07875v1 [cs.AI])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07875"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07875", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Peference elicitation is the task of suggesting a highly preferred \nconfiguration to a decision maker. The preferences are typically learned by \nquerying the user for choice feedback over pairs or sets of objects. In its \nconstructive variant, new objects are synthesized \"from scratch\" by maximizing \nan estimate of the user utility over a combinatorial (possibly infinite) space \nof candidates. In the constructive setting, most existing elicitation \ntechniques fail because they rely on exhaustive enumeration of the candidates. \nA previous solution explicitly designed for constructive tasks comes with no \nformal performance guarantees, and can be very expensive in (or unapplicable \nto) problems with non-Boolean attributes. We propose the Choice Perceptron, a \nPerceptron-like algorithm for learning user preferences from set-wise choice \nfeedback over constructive domains and hybrid Boolean-numeric feature spaces. \nWe provide a theoretical analysis on the attained regret that holds for a large \nclass of query selection strategies, and devise a heuristic strategy that aims \nat optimizing the regret in practice. Finally, we demonstrate its effectiveness \nby empirical evaluation against existing competitors on constructive scenarios \nof increasing complexity. \n</p>"}, "author": "Paolo Dragone, Stefano Teso, Andrea Passerini", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833669", "id": "tag:google.com,2005:reader/item/0000000337f96f14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Quantifying Performance of Bipedal Standing with Multi-channel EMG. (arXiv:1711.07894v1 [stat.ML])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07894"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07894", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Spinal cord stimulation has enabled humans with motor complete spinal cord \ninjury (SCI) to independently stand and recover some lost autonomic function. \nQuantifying the quality of bipedal standing under spinal stimulation is \nimportant for spinal rehabilitation therapies and for new strategies that seek \nto combine spinal stimulation and rehabilitative robots (such as exoskeletons) \nin real time feedback. To study the potential for automated electromyography \n(EMG) analysis in SCI, we evaluated the standing quality of paralyzed patients \nundergoing electrical spinal cord stimulation using both video and \nmulti-channel surface EMG recordings during spinal stimulation therapy \nsessions. The quality of standing under different stimulation settings was \nquantified manually by experienced clinicians. By correlating features of the \nrecorded EMG activity with the expert evaluations, we show that multi-channel \nEMG recording can provide accurate, fast, and robust estimation for the quality \nof bipedal standing in spinally stimulated SCI patients. Moreover, our analysis \nshows that the total number of EMG channels needed to effectively predict \nstanding quality can be reduced while maintaining high estimation accuracy, \nwhich provides more flexibility for rehabilitation robotic systems to \nincorporate EMG recordings. \n</p>"}, "author": "Yanan Sui, Kun ho Kim, Joel W. Burdick", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833668", "id": "tag:google.com,2005:reader/item/0000000337f96f24", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge. (arXiv:1711.07970v1 [cs.AI])", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07970"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07970", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the use of Deep Learning methods for modeling complex phenomena \nlike those occurring in natural physical processes. With the large amount of \ndata gathered on these phenomena the data intensive paradigm could begin to \nchallenge more traditional approaches elaborated over the years in fields like \nmaths or physics. However, despite considerable successes in a variety of \napplication domains, the machine learning field is not yet ready to handle the \nlevel of complexity required by such problems. Using an example application, \nnamely Sea Surface Temperature Prediction, we show how general background \nknowledge gained from physics could be used as a guideline for designing \nefficient Deep Learning models. In order to motivate the approach and to assess \nits generality we demonstrate a formal link between the solution of a class of \ndifferential equations underlying a large family of physical phenomena and the \nproposed model. Experiments and comparison with series of baselines including a \nstate of the art numerical approach is then provided. \n</p>"}, "author": "Emmanuel de Bezenac, Arthur Pajot, Patrick Gallinari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511355985834", "timestampUsec": "1511355985833646", "id": "tag:google.com,2005:reader/item/0000000337f96f30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Modular Continual Learning in a Unified Visual Environment. (arXiv:1711.07425v1 [cs.LG] CROSS LISTED)", "published": 1511355986, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07425"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07425", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A core aspect of human intelligence is the ability to learn new tasks quickly \nand switch between them flexibly. Here, we describe a modular continual \nreinforcement learning paradigm inspired by these abilities. We first introduce \na visual interaction environment that allows many types of tasks to be unified \nin a single framework. We then describe a reward map prediction scheme that \nlearns new tasks robustly in the very large state and action spaces required by \nsuch an environment. We investigate how properties of module architecture \ninfluence efficiency of task learning, showing that a module motif \nincorporating specific design principles (e.g. early bottlenecks, low-order \npolynomial nonlinearities, and symmetry) significantly outperforms more \nstandard neural network motifs, needing fewer training examples and fewer \nneurons to achieve high levels of performance. Finally, we present a \nmeta-controller architecture for task switching based on a dynamic neural \nvoting scheme, which allows new modules to use information learned from \npreviously-seen tasks to substantially improve their own learning efficiency. \n</p>"}, "author": "Kevin T. Feigelis, Blue Sheffer, Daniel L. K. Yamins", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511317566474", "timestampUsec": "1511317566473796", "id": "tag:google.com,2005:reader/item/0000000337a64de9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "E-PUR: An Energy-Efficient Processing Unit for Recurrent Neural Networks. (arXiv:1711.07480v1 [cs.NE])", "published": 1511317567, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07480"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07480", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent Neural Networks (RNNs) are a key technology for emerging \napplications such as automatic speech recognition, machine translation or image \ndescription. Long Short Term Memory (LSTM) networks are the most successful RNN \nimplementation, as they can learn long term dependencies to achieve high \naccuracy. Unfortunately, the recurrent nature of LSTM networks significantly \nconstrains the amount of parallelism and, hence, multicore CPUs and many-core \nGPUs exhibit poor efficiency for RNN inference. In this paper, we present \nE-PUR, an energy-efficient processing unit tailored to the requirements of LSTM \ncomputation. The main goal of E-PUR is to support large recurrent neural \nnetworks for low-power mobile devices. E-PUR provides an efficient hardware \nimplementation of LSTM networks that is flexible to support diverse \napplications. One of its main novelties is a technique that we call Maximizing \nWeight Locality (MWL), which improves the temporal locality of the memory \naccesses for fetching the synaptic weights, reducing the memory requirements by \na large extent. Our experimental results show that E-PUR achieves real-time \nperformance for different LSTM networks, while reducing energy consumption by \norders of magnitude with respect to general-purpose processors and GPUs, and it \nrequires a very small chip area. Compared to a modern mobile SoC, an NVIDIA \nTegra X1, E-PUR provides an average energy reduction of 92x. \n</p>"}, "author": "Franyell Silfa, Gem Dot, Jose-Maria Arnau, Antonio Gonzalez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511317566474", "timestampUsec": "1511317566473795", "id": "tag:google.com,2005:reader/item/0000000337a64dfa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Genetic Algorithms for Evolving Deep Neural Networks. (arXiv:1711.07655v1 [cs.NE])", "published": 1511317567, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07655"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07655", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years, deep learning methods applying unsupervised learning to \ntrain deep layers of neural networks have achieved remarkable results in \nnumerous fields. In the past, many genetic algorithms based methods have been \nsuccessfully applied to training neural networks. In this paper, we extend \nprevious work and propose a GA-assisted method for deep learning. Our \nexperimental results indicate that this GA-assisted approach improves the \nperformance of a deep autoencoder, producing a sparser neural network. \n</p>"}, "author": "Eli David, Iddo Greental", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511317566474", "timestampUsec": "1511317566473794", "id": "tag:google.com,2005:reader/item/0000000337a64e02", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Variational Probability Flow for Biologically Plausible Training of Deep Neural Networks. (arXiv:1711.07732v1 [cs.LG])", "published": 1511317567, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07732"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07732", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The quest for biologically plausible deep learning is driven, not just by the \ndesire to explain experimentally-observed properties of biological neural \nnetworks, but also by the hope of discovering more efficient methods for \ntraining artificial networks. In this paper, we propose a new algorithm named \nVariational Probably Flow (VPF), an extension of minimum probability flow for \ntraining binary Deep Boltzmann Machines (DBMs). We show that weight updates in \nVPF are local, depending only on the states and firing rates of the adjacent \nneurons. Unlike contrastive divergence, there is no need for Gibbs \nconfabulations; and unlike backpropagation, alternating feedforward and \nfeedback phases are not required. Moreover, the learning algorithm is effective \nfor training DBMs with intra-layer connections between the hidden nodes. \nExperiments with MNIST and Fashion MNIST demonstrate that VPF learns reasonable \nfeatures quickly, reconstructs corrupted images more accurately, and generates \nsamples with a high estimated log-likelihood. Lastly, we note that, \ninterestingly, if an asymmetric version of VPF exists, the weight updates \ndirectly explain experimental results in Spike-Timing-Dependent Plasticity \n(STDP). \n</p>"}, "author": "Zuozhu Liu, Tony Q.S. Quek, Shaowei Lin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511317566474", "timestampUsec": "1511317566473793", "id": "tag:google.com,2005:reader/item/0000000337a64e07", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Hidden Tree Markov Networks: Deep and Wide Learning for Structured Data. (arXiv:1711.07784v1 [cs.LG])", "published": 1511317567, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07784"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07784", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7017bff\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7017bff&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The paper introduces the Hidden Tree Markov Network (HTN), a \nneuro-probabilistic hybrid fusing the representation power of generative models \nfor trees with the incremental and discriminative learning capabilities of \nneural networks. We put forward a modular architecture in which multiple \ngenerative models of limited complexity are trained to learn structural feature \ndetectors whose outputs are then combined and integrated by neural layers at a \nlater stage. In this respect, the model is both deep, thanks to the unfolding \nof the generative models on the input structures, as well as wide, given the \npotentially large number of generative modules that can be trained in parallel. \nExperimental results show that the proposed approach can outperform \nstate-of-the-art syntactic kernels as well as generative kernels built on the \nsame probabilistic model as the HTN. \n</p>"}, "author": "Davide Bacciu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511317566474", "timestampUsec": "1511317566473792", "id": "tag:google.com,2005:reader/item/0000000337a64e12", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Evaluation of bioinspired algorithms for the solution of the job scheduling problem. (arXiv:1711.07821v1 [cs.NE])", "published": 1511317567, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07821"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07821", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this research we used bio-inspired metaheuristics, as artificial immune \nsystems and ant colony algorithms that are based on a number of characteristics \nand behaviors of living things that are interesting in the computer science \narea. This paper presents an evaluation of bio-inspired solutions to \ncombinatorial optimization problem, called the Job Shop Scheduling or planning \nwork, in a simple way the objective is to find a configuration or job stream \nthat has the least amount of time to be executed in machine settings. The \nperformance of the algorithms was characterized and evaluated for reference \ninstances of the job shop scheduling problem, comparing the quality of the \nsolutions obtained with respect to the best known solution of the most \neffective methods. The solutions were evaluated in two aspects, first in \nrelation of quality of solutions, taking as reference the makespan and secondly \nin relation of performance, taking the number evaluations performed by the \nalgorithm to obtain the best solution. \n</p>"}, "author": "Edson Florez, Nelson Diaz, Wilfredo Gomez, Lola Bautista, Dario Delgado", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511317566474", "timestampUsec": "1511317566473791", "id": "tag:google.com,2005:reader/item/0000000337a64e19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Two-Archive Evolutionary Algorithm for Constrained Multi-Objective Optimization. (arXiv:1711.07907v1 [cs.NE])", "published": 1511317567, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07907"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07907", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>When solving constrained multi-objective optimization problems, an important \nissue is how to balance convergence, diversity and feasibility simultaneously. \nTo address this issue, this paper proposes a parameter-free constraint handling \ntechnique, two-archive evolutionary algorithm, for constrained multi-objective \noptimization. It maintains two co-evolving populations simultaneously: one, \ndenoted as convergence archive, is the driving force to push the population \ntoward the Pareto front; the other one, denoted as diversity archive, mainly \ntends to maintain the population diversity. In particular, to complement the \nbehavior of the convergence archive and provide as much diversified information \nas possible, the diversity archive aims at exploring areas under-exploited by \nthe convergence archive including the infeasible regions. To leverage the \ncomplementary effects of both archives, we develop a restricted mating \nselection mechanism that adaptively chooses appropriate mating parents from \nthem according to their evolution status. Comprehensive experiments on a series \nof benchmark problems and a real-world case study fully demonstrate the \ncompetitiveness of our proposed algorithm, comparing to five state-of-the-art \nconstrained evolutionary multi-objective optimizers. \n</p>"}, "author": "Ke Li, Renzhi Chen, Guangtao Fu, Xin Yao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511317566474", "timestampUsec": "1511317566473790", "id": "tag:google.com,2005:reader/item/0000000337a64e20", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Deep Learning for Real-time Gravitational Wave Detection and Parameter Estimation with Advanced LIGO Data. (arXiv:1711.07966v1 [gr-qc])", "published": 1511317567, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07966"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07966", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The recent Nobel-prize-winning detections of gravitational waves from merging \nblack holes and the subsequent detection of the collision of two neutron stars \nin coincidence with electromagnetic observations have inaugurated a new era of \nmultimessenger astrophysics. To enhance the scope of this emergent science, the \nuse of deep convolutional neural networks were proposed for the detection and \ncharacterization of gravitational wave signals in real-time. This approach, \nDeep Filtering, was initially demonstrated using simulated LIGO noise. In this \narticle, we present the extension of Deep Filtering using real noise from the \nfirst observing run of LIGO, for both detection and parameter estimation of \ngravitational waves from binary black hole mergers with continuous data streams \nfrom multiple LIGO detectors. We show for the first time that machine learning \ncan detect and estimate the true parameters of a real GW event observed by \nLIGO. Our comparisons show that Deep Filtering is far more computationally \nefficient than matched-filtering, while retaining similar performance, allowing \nreal-time processing of weak time-series signals in non-stationary non-Gaussian \nnoise, with minimal resources, and also enables the detection of new classes of \ngravitational wave sources that may go unnoticed with existing detection \nalgorithms. This framework is uniquely suited to enable coincident detection \ncampaigns of gravitational waves and their multimessenger counterparts in \nreal-time. \n</p>"}, "author": "Daniel George, E. A. Huerta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834655", "id": "tag:google.com,2005:reader/item/00000003379742fb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Principal Boundary on Riemannian Manifolds. (arXiv:1711.06705v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06705"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06705", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We revisit the classification problem and focus on nonlinear methods for \nclassification on manifolds. For multivariate datasets lying on an embedded \nnonlinear Riemannian manifold within the higher-dimensional space, our aim is \nto acquire a classification boundary between the classes with labels. Motivated \nby the principal flow [Panaretos, Pham and Yao, 2014], a curve that moves along \na path of the maximum variation of the data, we introduce the principal \nboundary. From the classification perspective, the principal boundary is \ndefined as an optimal curve that moves in between the principal flows traced \nout from two classes of the data, and at any point on the boundary, it \nmaximizes the margin between the two classes. We estimate the boundary in \nquality with its direction supervised by the two principal flows. We show that \nthe principal boundary yields the usual decision boundary found by the support \nvector machine, in the sense that locally, the two boundaries coincide. By \nmeans of examples, we illustrate how to find, use and interpret the principal \nboundary. \n</p>"}, "author": "Zhigang Yao, Zhenyue Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834654", "id": "tag:google.com,2005:reader/item/000000033797432b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Manifold learning with bi-stochastic kernels. (arXiv:1711.06711v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06711"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06711", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we answer the following question: what is the infinitesimal \ngenerator of the diffusion process defined by a kernel that is normalized such \nthat it is bi-stochastic with respect to a specified measure? More precisely, \nunder the assumption that data is sampled from a Riemannian manifold we \ndetermine how the resulting infinitesimal generator depends on the potentially \nnonuniform distribution of the sample points, and the specified measure for the \nbi-stochastic normalization. In a special case, we demonstrate a connection to \nthe heat kernel. We consider both the case where only a single data set is \ngiven, and the case where a data set and a reference set are given. The \nspectral theory of the constructed operators is studied, and Nystr\\\"om \nextension formulas for the gradients of the eigenfunctions are computed. \nApplications to discrete point sets and manifold learning are discussed. \n</p>"}, "author": "Ronald R. Coifman, Nicholas F. Marshall", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834653", "id": "tag:google.com,2005:reader/item/0000000337974355", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Techniques for proving Asynchronous Convergence results for Markov Chain Monte Carlo methods. (arXiv:1711.06719v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06719"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06719", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Markov Chain Monte Carlo (MCMC) methods such as Gibbs sampling are finding \nwidespread use in applied statistics and machine learning. These often lead to \ndifficult computational problems, which are increasingly being solved on \nparallel and distributed systems such as compute clusters. Recent work has \nproposed running iterative algorithms such as gradient descent and MCMC in \nparallel asynchronously for increased performance, with good empirical results \nin certain problems. Unfortunately, for MCMC this parallelization technique \nrequires new convergence theory, as it has been explicitly demonstrated to lead \nto divergence on some examples. Recent theory on Asynchronous Gibbs sampling \ndescribes why these algorithms can fail, and provides a way to alter them to \nmake them converge. In this article, we describe how to apply this theory in a \ngeneric setting, to understand the asynchronous behavior of any MCMC algorithm, \nincluding those implemented using parameter servers, and those not based on \nGibbs sampling. \n</p>"}, "author": "Alexander Terenin, Eric P. Xing", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834652", "id": "tag:google.com,2005:reader/item/0000000337974381", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Deep supervised learning using local errors. (arXiv:1711.06756v1 [cs.NE])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06756"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06756", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Error backpropagation is a highly effective mechanism for learning \nhigh-quality hierarchical features in deep networks. Updating the features or \nweights in one layer, however, requires waiting for the propagation of error \nsignals from higher layers. Learning using delayed and non-local errors makes \nit hard to reconcile backpropagation with the learning mechanisms observed in \nbiological neural networks as it requires the neurons to maintain a memory of \nthe input long enough until the higher-layer errors arrive. In this paper, we \npropose an alternative learning mechanism where errors are generated locally in \neach layer using fixed, random auxiliary classifiers. Lower layers could thus \nbe trained independently of higher layers and training could either proceed \nlayer by layer, or simultaneously in all layers using local error information. \nWe address biological plausibility concerns such as weight symmetry \nrequirements and show that the proposed learning mechanism based on fixed, \nbroad, and random tuning of each neuron to the classification categories \noutperforms the biologically-motivated feedback alignment learning technique on \nthe MNIST, CIFAR10, and SVHN datasets, approaching the performance of standard \nbackpropagation. Our approach highlights a potential biological mechanism for \nthe supervised, or task-dependent, learning of feature hierarchies. In \naddition, we show that it is well suited for learning deep networks in custom \nhardware where it can drastically reduce memory traffic and data communication \noverheads. \n</p>"}, "author": "Hesham Mostafa, Vishwajith Ramesh, Gert Cauwenberghs", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834651", "id": "tag:google.com,2005:reader/item/00000003379743a1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/fresh", "user/1005689817/label/arXiv"], "title": "Approximate Gradient Coding via Sparse Random Graphs. (arXiv:1711.06771v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06771"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06771", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Distributed algorithms are often beset by the straggler effect, where the \nslowest compute nodes in the system dictate the overall running time. \nCoding-theoretic techniques have been recently proposed to mitigate stragglers \nvia algorithmic redundancy. Prior work in coded computation and gradient coding \nhas mainly focused on exact recovery of the desired output. However, slightly \ninexact solutions can be acceptable in applications that are robust to noise, \nsuch as model training via gradient-based algorithms. In this work, we present \ncomputationally simple gradient codes based on sparse graphs that guarantee \nfast and approximately accurate distributed computation. We demonstrate that \nsacrificing a small amount of accuracy can significantly increase algorithmic \nrobustness to stragglers. \n</p>"}, "author": "Zachary Charles, Dimitris Papailiopoulos, Jordan Ellenberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834650", "id": "tag:google.com,2005:reader/item/00000003379743c5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks. (arXiv:1711.06788v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06788"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06788", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce MinimalRNN, a new recurrent neural network architecture that \nachieves comparable performance as the popular gated RNNs with a simplified \nstructure. It employs minimal updates within RNN, which not only leads to \nefficient learning and testing but more importantly better interpretability and \ntrainability. We demonstrate that by endorsing the more restrictive update \nrule, MinimalRNN learns disentangled RNN states. We further examine the \nlearning dynamics of different RNN structures using input-output Jacobians, and \nshow that MinimalRNN is able to capture longer range dependencies than existing \nRNN architectures. \n</p>"}, "author": "Minmin Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834649", "id": "tag:google.com,2005:reader/item/0000000337974401", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tree-Structured Boosting: Connections Between Gradient Boosted Stumps and Full Decision Trees. (arXiv:1711.06793v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06793"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06793", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7017e30\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7017e30&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Additive models, such as produced by gradient boosting, and full interaction \nmodels, such as classification and regression trees (CART), are widely used \nalgorithms that have been investigated largely in isolation. We show that these \nmodels exist along a spectrum, revealing never-before-known connections between \nthese two approaches. This paper introduces a novel technique called \ntree-structured boosting for creating a single decision tree, and shows that \nthis method can produce models equivalent to CART or gradient boosted stumps at \nthe extremes by varying a single parameter. Although tree-structured boosting \nis designed primarily to provide both the model interpretability and predictive \nperformance needed for high-stake applications like medicine, it also can \nproduce decision trees represented by hybrid models between CART and boosted \nstumps that can outperform either of these approaches. \n</p>"}, "author": "Jos&#xe9; Marcio Luna, Eric Eaton, Lyle H. Ungar, Eric Diffenderfer, Shane T. Jensen, Efstathios D. Gennatas, Mateo Wirth, Charles B. Simone II, Timothy D. Solberg, Gilmer Valdes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834648", "id": "tag:google.com,2005:reader/item/0000000337974423", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Prediction Scores as a Window into Classifier Behavior. (arXiv:1711.06795v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06795"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06795", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c708f6a0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c708f6a0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Most multi-class classifiers make their prediction for a test sample by \nscoring the classes and selecting the one with the highest score. Analyzing \nthese prediction scores is useful to understand the classifier behavior and to \nassess its reliability. We present an interactive visualization that \nfacilitates per-class analysis of these scores. Our system, called Classilist, \nenables relating these scores to the classification correctness and to the \nunderlying samples and their features. We illustrate how such analysis reveals \nvarying behavior of different classifiers. Classilist is available for use \nonline, along with source code, video tutorials, and plugins for R, RapidMiner, \nand KNIME at https://katehara.github.io/classilist-site/. \n</p>"}, "author": "Medha Katehara, Emma Beauxis-Aussalet, Bilal Alsallakh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834647", "id": "tag:google.com,2005:reader/item/00000003379744c4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Household poverty classification in data-scarce environments: a machine learning approach. (arXiv:1711.06813v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06813"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06813", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe a method to identify poor households in data-scarce countries by \nleveraging information contained in nationally representative household \nsurveys. It employs standard statistical learning techniques---cross-validation \nand parameter regularization---which together reduce the extent to which the \nmodel is over-fitted to match the idiosyncracies of observed survey data. The \nautomated framework satisfies three important constraints of this development \nsetting: i) The prediction model uses at most ten questions, which limits the \ncosts of data collection; ii) No computation beyond simple arithmetic is needed \nto calculate the probability that a given household is poor, immediately after \ndata on the ten indicators is collected; and iii) One specification of the \nmodel (i.e. one scorecard) is used to predict poverty throughout a country that \nmay be characterized by significant sub-national differences. Using survey data \nfrom Zambia, the model's out-of-sample predictions distinguish poor households \nfrom non-poor households using information contained in ten questions. \n</p>"}, "author": "Varun Kshirsagar, Jerzy Wieczorek, Sharada Ramanathan, Rachel Wells", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834646", "id": "tag:google.com,2005:reader/item/00000003379744df", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Acquiring Common Sense Spatial Knowledge through Implicit Spatial Templates. (arXiv:1711.06821v1 [cs.AI])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06821"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06821", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Spatial understanding is a fundamental problem with wide-reaching real-world \napplications. The representation of spatial knowledge is often modeled with \nspatial templates, i.e., regions of acceptability of two objects under an \nexplicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with \nprior work that restricts spatial templates to explicit spatial prepositions \n(e.g., \"glass on table\"), here we extend this concept to implicit spatial \nlanguage, i.e., those relationships (generally actions) for which the spatial \narrangement of the objects is only implicitly implied (e.g., \"man riding \nhorse\"). In contrast with explicit relationships, predicting spatial \narrangements from implicit spatial language requires significant common sense \nspatial understanding. Here, we introduce the task of predicting spatial \ntemplates for two objects under a relationship, which can be seen as a spatial \nquestion-answering task with a (2D) continuous output (\"where is the man w.r.t. \na horse when the man is walking the horse?\"). We present two simple \nneural-based models that leverage annotated images and structured text to learn \nthis task. The good performance of these models reveals that spatial locations \nare to a large extent predictable from implicit spatial language. Crucially, \nthe models attain similar performance in a challenging generalized setting, \nwhere the object-relation-object combinations (e.g.,\"man walking dog\") have \nnever been seen before. Next, we go one step further by presenting the models \nwith unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging \nword embeddings enables the models to output accurate spatial predictions, \nproving that the models acquire solid common sense spatial knowledge allowing \nfor such generalization. \n</p>"}, "author": "Guillem Collell, Luc Van Gool, Marie-Francine Moens", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834645", "id": "tag:google.com,2005:reader/item/0000000337974505", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Proximal Gradient Method with Extrapolation and Line Search for a Class of Nonconvex and Nonsmooth Problems. (arXiv:1711.06831v1 [math.OC])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06831"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06831", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we consider a class of possibly nonconvex, nonsmooth and \nnon-Lipschitz optimization problems arising in many contemporary applications \nsuch as machine learning, variable selection and image processing. To solve \nthis class of problems, we propose a proximal gradient method with \nextrapolation and line search (PGels). This method is developed based on a \nspecial potential function and successfully incorporates both extrapolation and \nnon-monotone line search, which are two simple and efficient accelerating \ntechniques for the proximal gradient method. Thanks to the line search, this \nmethod allows more flexibilities in choosing the extrapolation parameters and \nupdates them adaptively at each iteration if a certain line search criterion is \nnot satisfied. Moreover, with proper choices of parameters, our PGels reduces \nto many existing algorithms. We also show that, under some mild conditions, our \nline search criterion is well defined and any cluster point of the sequence \ngenerated by PGels is a stationary point of our problem. In addition, by \nassuming the Kurdyka-{\\L}ojasiewicz exponent of the objective in our problem, \nwe further analyze the local convergence rate of two special cases of PGels, \nincluding the widely used non-monotone proximal gradient method as one case. \nFinally, we conduct some numerical experiments for solving the $\\ell_1$ \nregularized logistic regression problem and the $\\ell_{1\\text{-}2}$ regularized \nleast squares problem. Our numerical results illustrate the efficiency of PGels \nand show the potential advantage of combining two accelerating techniques. \n</p>"}, "author": "Lei Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834644", "id": "tag:google.com,2005:reader/item/000000033797454e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Genetic Algorithms for Mentor-Assisted Evaluation Function Optimization. (arXiv:1711.06839v1 [cs.NE])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06839"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06839", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we demonstrate how genetic algorithms can be used to reverse \nengineer an evaluation function's parameters for computer chess. Our results \nshow that using an appropriate mentor, we can evolve a program that is on par \nwith top tournament-playing chess programs, outperforming a two-time World \nComputer Chess Champion. This performance gain is achieved by evolving a \nprogram with a smaller number of parameters in its evaluation function to mimic \nthe behavior of a superior mentor which uses a more extensive evaluation \nfunction. In principle, our mentor-assisted approach could be used in a wide \nrange of problems for which appropriate mentors are available. \n</p>"}, "author": "Eli David, Moshe Koppel, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834643", "id": "tag:google.com,2005:reader/item/0000000337974574", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Simulating Human Grandmasters: Evolution and Coevolution of Evaluation Functions. (arXiv:1711.06840v1 [cs.NE])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06840"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06840", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper demonstrates the use of genetic algorithms for evolving a \ngrandmaster-level evaluation function for a chess program. This is achieved by \ncombining supervised and unsupervised learning. In the supervised learning \nphase the organisms are evolved to mimic the behavior of human grandmasters, \nand in the unsupervised learning phase these evolved organisms are further \nimproved upon by means of coevolution. \n</p> \n<p>While past attempts succeeded in creating a grandmaster-level program by \nmimicking the behavior of existing computer chess programs, this paper presents \nthe first successful attempt at evolving a state-of-the-art evaluation function \nby learning only from databases of games played by humans. Our results \ndemonstrate that the evolved program outperforms a two-time World Computer \nChess Champion. \n</p>"}, "author": "Eli David, H. Jaap van den Herik, Moshe Koppel, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834642", "id": "tag:google.com,2005:reader/item/00000003379745aa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Expert-Driven Genetic Algorithms for Simulating Evaluation Functions. (arXiv:1711.06841v1 [cs.NE])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06841"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06841", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we demonstrate how genetic algorithms can be used to reverse \nengineer an evaluation function's parameters for computer chess. Our results \nshow that using an appropriate expert (or mentor), we can evolve a program that \nis on par with top tournament-playing chess programs, outperforming a two-time \nWorld Computer Chess Champion. This performance gain is achieved by evolving a \nprogram that mimics the behavior of a superior expert. The resulting evaluation \nfunction of the evolved program consists of a much smaller number of parameters \nthan the expert's. The extended experimental results provided in this paper \ninclude a report of our successful participation in the 2008 World Computer \nChess Championship. In principle, our expert-driven approach could be used in a \nwide range of problems for which appropriate experts are available. \n</p>"}, "author": "Eli David, Moshe Koppel, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834641", "id": "tag:google.com,2005:reader/item/00000003379745d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Run, skeleton, run: skeletal model in a physics-based simulation. (arXiv:1711.06922v1 [cs.AI])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06922"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06922", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present our approach to solve a physics-based reinforcement \nlearning challenge \"Learning to Run\" with objective to train \nphysiologically-based human model to navigate a complex obstacle course as \nquickly as possible. The environment is computationally expensive, has a \nhigh-dimensional continuous action space and is stochastic. We benchmark state \nof the art policy-gradient methods and test several improvements, such as layer \nnormalization, parameter noise, action and state reflecting, to stabilize \ntraining and improve its sample-efficiency. We found that the Deep \nDeterministic Policy Gradient method is the most efficient method for this \nenvironment and the improvements we have introduced help to stabilize training. \nLearned models are able to generalize to new physical scenarios, e.g. different \nobstacle courses. \n</p>"}, "author": "Mikhail Pavlov, Sergey Kolesnikov, Sergey M. Plis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834640", "id": "tag:google.com,2005:reader/item/0000000337974601", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Gaussian Mixture Models. (arXiv:1711.06929v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06929"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06929", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning is a hierarchical inference method formed by subsequent \nmultiple layers of learning able to more efficiently describe complex \nrelationships. In this work, Deep Gaussian Mixture Models are introduced and \ndiscussed. A Deep Gaussian Mixture model (DGMM) is a network of multiple layers \nof latent variables, where, at each layer, the variables follow a mixture of \nGaussian distributions. Thus, the deep mixture model consists of a set of \nnested mixtures of linear models, which globally provide a nonlinear model able \nto describe the data in a very flexible way. In order to avoid \noverparameterized solutions, dimension reduction by factor models can be \napplied at each layer of the architecture thus resulting in deep mixtures of \nfactor analysers. \n</p>"}, "author": "Cinzia Viroli, Geoffrey J. McLachlan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834639", "id": "tag:google.com,2005:reader/item/0000000337974613", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Robust Synthetic Control. (arXiv:1711.06940v1 [econ.EM])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06940"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06940", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c708f9e7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c708f9e7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present a robust generalization of the synthetic control method for \ncomparative case studies. Like the classical method, we present an algorithm to \nestimate the unobservable counterfactual of a treatment unit. A distinguishing \nfeature of our algorithm is that of de-noising the data matrix via singular \nvalue thresholding, which renders our approach robust in multiple facets: it \nautomatically identifies a good subset of donors, overcomes the challenges of \nmissing data, and continues to work well in settings where covariate \ninformation may not be provided. To begin, we establish the condition under \nwhich the fundamental assumption in synthetic control-like approaches holds, \ni.e. when the linear relationship between the treatment unit and the donor pool \nprevails in both the pre- and post-intervention periods. We provide the first \nfinite sample analysis for a broader class of models, the Latent Variable \nModel, in contrast to Factor Models previously considered in the literature. \nFurther, we show that our de-noising procedure accurately imputes missing \nentries, producing a consistent estimator of the underlying signal matrix \nprovided $p = \\Omega( T^{-1 + \\zeta})$ for some $\\zeta &gt; 0$; here, $p$ is the \nfraction of observed data and $T$ is the time interval of interest. Under the \nsame setting, we prove that the mean-squared-error (MSE) in our prediction \nestimation scales as $O(\\sigma^2/p + 1/\\sqrt{T})$, where $\\sigma^2$ is the \nnoise variance. Using a data aggregation method, we show that the MSE can be \nmade as small as $O(T^{-1/2+\\gamma})$ for any $\\gamma \\in (0, 1/2)$, leading to \na consistent estimator. We also introduce a Bayesian framework to quantify the \nmodel uncertainty through posterior probabilities. Our experiments, using both \nreal-world and synthetic datasets, demonstrate that our robust generalization \nyields an improvement over the classical synthetic control method. \n</p>"}, "author": "Muhammad Jehangir Amjad, Devavrat Shah, Dennis Shen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834638", "id": "tag:google.com,2005:reader/item/0000000337974650", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning. (arXiv:1711.06959v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06959"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06959", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Understanding the global optimality in deep learning (DL) has been attracting \nmore and more attention recently. Conventional DL solvers, however, have not \nbeen developed intentionally to seek for such global optimality. In this paper \nwe propose a novel approximation algorithm, BPGrad, towards optimizing deep \nmodels globally via branch and pruning. Our BPGrad algorithm is based on the \nassumption of Lipschitz continuity in DL, and as a result it can adaptively \ndetermine the step size for current gradient given the history of previous \nupdates, wherein theoretically no smaller steps can achieve the global \noptimality. We prove that, by repeating such branch-and-pruning procedure, we \ncan locate the global optimality within finite iterations. Empirically an \nefficient solver based on BPGrad for DL is proposed as well, and it outperforms \nconventional DL solvers such as Adagrad, Adadelta, RMSProp, and Adam in the \ntasks of object recognition, detection, and segmentation. \n</p>"}, "author": "Ziming Zhang, Yuanwei Wu, Guanghui Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834637", "id": "tag:google.com,2005:reader/item/0000000337974690", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Unsupervised Domain Adaptation for Semantic Segmentation with GANs. (arXiv:1711.06969v1 [cs.CV])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06969"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06969", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Visual Domain Adaptation is a problem of immense importance in computer \nvision. Previous approaches showcase the inability of even deep neural networks \nto learn informative representations across domain shift. This problem is more \nsevere for tasks where acquiring hand labeled data is extremely hard and \ntedious. In this work, we focus on adapting the representations learned by \nsegmentation networks across synthetic and real domains. Contrary to previous \napproaches that use a simple adversarial objective or superpixel information to \naid the process, we propose an approach based on Generative Adversarial \nNetworks (GANs) that brings the embeddings closer in the learned feature space. \nTo showcase the generality and scalability of our approach, we show that we can \nachieve state of the art results on two challenging scenarios of synthetic to \nreal domain adaptation. Additional exploratory experiments show that our \napproach: (1) generalizes to unseen domains and (2) results in improved \nalignment of source and target distributions. \n</p>"}, "author": "Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser Nam Lim, Rama Chellappa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834636", "id": "tag:google.com,2005:reader/item/000000033797469f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sequential Randomized Matrix Factorization for Gaussian Processes: Efficient Predictions and Hyper-parameter Optimization. (arXiv:1711.06989v1 [cs.LG])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06989"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06989", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a sequential randomized lowrank matrix factorization \napproach for incrementally predicting values of an unknown function at test \npoints using the Gaussian Processes framework. It is well-known that in the \nGaussian processes framework, the computational bottlenecks are the inversion \nof the (regularized) kernel matrix and the computation of the hyper-parameters \ndefining the kernel. The main contributions of this paper are two-fold. First, \nwe formalize an approach to compute the inverse of the kernel matrix using \nrandomized matrix factorization algorithms in a streaming scenario, i.e., data \nis generated incrementally over time. The metrics of accuracy and computational \nefficiency of the proposed method are compared against a batch approach based \non use of randomized matrix factorization and an existing streaming approach \nbased on approximating the Gaussian process by a finite set of basis vectors. \nSecond, we extend the sequential factorization approach to a class of kernel \nfunctions for which the hyperparameters can be efficiently optimized. All \nresults are demonstrated on two publicly available datasets. \n</p>"}, "author": "Shaunak D. Bopardikar, George S. Eskander Ekladious", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834635", "id": "tag:google.com,2005:reader/item/00000003379746ac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convergence Analysis of the Dynamics of a Special Kind of Two-Layered Neural Networks with $\\ell_1$ and $\\ell_2$ Regularization. (arXiv:1711.07005v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07005"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07005", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we made an extension to the convergence analysis of the \ndynamics of two-layered bias-free networks with one $ReLU$ output. We took into \nconsideration two popular regularization terms: the $\\ell_1$ and $\\ell_2$ norm \nof the parameter vector $w$, and added it to the square loss function with \ncoefficient $\\lambda/2$. We proved that when $\\lambda$ is small, the weight \nvector $w$ converges to the optimal solution $\\hat{w}$ (with respect to the new \nloss function) with probability $\\geq (1-\\varepsilon)(1-A_d)/2$ under random \ninitiations in a sphere centered at the origin, where $\\varepsilon$ is a small \nvalue and $A_d$ is a constant. Numerical experiments including phase diagrams \nand repeated simulations verified our theory. \n</p>"}, "author": "Zhifeng Kong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834634", "id": "tag:google.com,2005:reader/item/00000003379746b6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Decentralized High-Dimensional Bayesian Optimization with Factor Graphs. (arXiv:1711.07033v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07033"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07033", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a novel decentralized high-dimensional Bayesian \noptimization (DEC-HBO) algorithm that, in contrast to existing HBO algorithms, \ncan exploit the interdependent effects of various input components on the \noutput of the unknown objective function f for boosting the BO performance and \nstill preserve scalability in the number of input dimensions without requiring \nprior knowledge or the existence of a low (effective) dimension of the input \nspace. To realize this, we propose a sparse yet rich factor graph \nrepresentation of f to be exploited for designing an acquisition function that \ncan be similarly represented by a sparse factor graph and hence be efficiently \noptimized in a decentralized manner using distributed message passing. Despite \nrichly characterizing the interdependent effects of the input components on the \noutput of f with a factor graph, DEC-HBO can still guarantee no-regret \nperformance asymptotically. Empirical evaluation on synthetic and real-world \nexperiments (e.g., sparse Gaussian process model with 1811 hyperparameters) \nshows that DEC-HBO outperforms the state-of-the-art HBO algorithms. \n</p>"}, "author": "Trong Nghia Hoang, Quang Minh Hoang, Ruofei Ouyang, Kian Hsiang Low", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834633", "id": "tag:google.com,2005:reader/item/00000003379746bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Improved Oscillating-Error Classifier with Branching. (arXiv:1711.07042v1 [cs.LG])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07042"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07042", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper extends the earlier work on an oscillating error correction \ntechnique. Specifically, it extends the design to include further corrections, \nby adding new layers to the classifier through a branching method. This \ntechnique is still consistent with earlier work and also neural networks in \ngeneral. With this extended design, the classifier can now achieve the high \nlevels of accuracy reported previously. \n</p>"}, "author": "Kieran Greer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834632", "id": "tag:google.com,2005:reader/item/00000003379746de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Classifying Variational Autoencoder with Application to Polyphonic Music Generation. (arXiv:1711.07050v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07050"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07050", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The variational autoencoder (VAE) is a popular probabilistic generative \nmodel. However, one shortcoming of VAEs is that the latent variables cannot be \ndiscrete, which makes it difficult to generate data from different modes of a \ndistribution. Here, we propose an extension of the VAE framework that \nincorporates a classifier to infer the discrete class of the modeled data. To \nmodel sequential data, we can combine our Classifying VAE with a recurrent \nneural network such as an LSTM. We apply this model to algorithmic music \ngeneration, where our model learns to generate musical sequences in different \nkeys. Most previous work in this area avoids modeling key by transposing data \ninto only one or two keys, as opposed to the 10+ different keys in the original \nmusic. We show that our Classifying VAE and Classifying VAE+LSTM models \noutperform the corresponding non-classifying models in generating musical \nsamples that stay in key. This benefit is especially apparent when trained on \nuntransposed music data in the original keys. \n</p>"}, "author": "Jay A. Hennig, Akash Umakantha, Ryan C. Williamson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834631", "id": "tag:google.com,2005:reader/item/00000003379746f9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Does mitigating ML's disparate impact require disparate treatment?. (arXiv:1711.07076v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07076"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07076", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Following related work in law and policy, two notions of prejudice have come \nto shape the study of fairness in algorithmic decision-making. Algorithms \nexhibit disparate treatment if they formally treat people differently according \nto a protected characteristic, like race, or if they intentionally discriminate \n(even if via proxy variables). Algorithms exhibit disparate impact if they \naffect subgroups differently. Disparate impact can arise unintentionally and \nabsent disparate treatment. The natural way to reduce disparate impact would be \nto apply disparate treatment in favor of the disadvantaged group, i.e. to apply \naffirmative action. However, owing to the practice's contested legal status, \nseveral papers have proposed trying to eliminate both forms of unfairness \nsimultaneously, introducing a family of algorithms that we denote disparate \nlearning processes (DLPs). These processes incorporate the protected \ncharacteristic as an input to the learning algorithm (e.g.~via a regularizer) \nbut produce a model that cannot directly access the protected characteristic as \nan input. In this paper, we make the following arguments: (i) DLPs can be \nfunctionally equivalent to disparate treatment, and thus should carry the same \nlegal status; (ii) when the protected characteristic is redundantly encoded in \nthe nonsensitive features, DLPs can exactly apply any disparate treatment \nprotocol; (iii) when the characteristic is only partially encoded, DLPs may \ninduce within-class discrimination. Finally, we argue the normative point that \nrather than masking efforts towards proportional representation, it is \npreferable to undertake them transparently. \n</p>"}, "author": "Zachary C. Lipton, Alexandra Chouldechova, Julian McAuley", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834630", "id": "tag:google.com,2005:reader/item/0000000337974703", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Estimation Considerations in Contextual Bandits. (arXiv:1711.07077v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07077"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07077", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Contextual bandit algorithms seek to learn a personalized treatment \nassignment policy, balancing exploration against exploitation. Although a \nnumber of algorithms have been proposed, there is little guidance available for \napplied researchers to select among various approaches. Motivated by the \neconometrics and statistics literatures on causal effects estimation, we study \na new consideration to the exploration vs. exploitation framework, which is \nthat the way exploration is conducted in the present may contribute to the bias \nand variance in the potential outcome model estimation in subsequent stages of \nlearning. We leverage parametric and non-parametric statistical estimation \nmethods and causal effect estimation methods in order to propose new contextual \nbandit designs. Through a variety of simulations, we show how alternative \ndesign choices impact the learning performance and provide insights on why we \nobserve these effects. \n</p>"}, "author": "Maria Dimakopoulou, Susan Athey, Guido Imbens", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834629", "id": "tag:google.com,2005:reader/item/0000000337974714", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Compression-Based Regularization with an Application to Multi-Task Learning. (arXiv:1711.07099v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07099"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07099", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c708fd1f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c708fd1f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper investigates, from information theoretic grounds, a learning \nproblem based on the principle that any regularity in a given dataset can be \nexploited to extract compact features from data, i.e., using fewer bits than \nneeded to fully describe the data itself, in order to build meaningful \nrepresentations of a relevant content (multiple labels). We begin by \nintroducing the noisy lossy source coding paradigm with the log-loss fidelity \ncriterion which provides the fundamental tradeoffs between the \n\\emph{cross-entropy loss} (average risk) and the information rate of the \nfeatures (model complexity). Our approach allows an information theoretic \nformulation of the \\emph{multi-task learning} (MTL) problem which is a \nsupervised learning framework in which the prediction models for several \nrelated tasks are learned jointly from common representations to achieve better \ngeneralization performance. Then, we present an iterative algorithm for \ncomputing the optimal tradeoffs and its global convergence is proven provided \nthat some conditions hold. An important property of this algorithm is that it \nprovides a natural safeguard against overfitting, because it minimizes the \naverage risk taking into account a penalization induced by the model \ncomplexity. Remarkably, empirical results illustrate that there exists an \noptimal information rate minimizing the \\emph{excess risk} which depends on the \nnature and the amount of available training data. An application to \nhierarchical text categorization is also investigated, extending previous \nworks. \n</p>"}, "author": "Mat&#xed;as Vera, Leonardo Rey Vega, Pablo Piantanida", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834628", "id": "tag:google.com,2005:reader/item/000000033797474f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Double Parametric Bootstrap Test for Topic Models. (arXiv:1711.07104v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07104"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07104", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7114116\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7114116&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Non-negative matrix factorization (NMF) is a technique for finding latent \nrepresentations of data. The method has been applied to corpora to construct \ntopic models. However, NMF has likelihood assumptions which are often violated \nby real document corpora. We present a double parametric bootstrap test for \nevaluating the fit of an NMF-based topic model based on the duality of the KL \ndivergence and Poisson maximum likelihood estimation. The test correctly \nidentifies whether a topic model based on an NMF approach yields reliable \nresults in simulated and real data. \n</p>"}, "author": "Skyler Seto, Hui Fen Tan, Giles Hooker, Martin T. Wells", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834627", "id": "tag:google.com,2005:reader/item/0000000337974775", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Structured Stein Variational Inference for Continuous Graphical Models. (arXiv:1711.07168v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07168"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07168", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a novel distributed inference algorithm for continuous graphical \nmodels by extending Stein variational gradient descent (SVGD) to leverage the \nMarkov dependency structure of the distribution of interest. The idea is to use \na set of local kernel functions over the Markov blanket of each node, which \nalleviates the problem of the curse of high dimensionality and simultaneously \nyields a distributed algorithm for decentralized inference tasks. We justify \nour method with theoretical analysis and show that the use of local kernels can \nbe viewed as a new type of localized approximation that matches the target \ndistribution on the conditional distributions of each node over its Markov \nblanket. Our empirical results demonstrate that our method outperforms a \nvariety of baselines including standard MCMC and particle message passing \nmethods. \n</p>"}, "author": "Dilin Wang, Zhe Zeng, Qiang Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834626", "id": "tag:google.com,2005:reader/item/0000000337974785", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Finite Time Analysis of Optimal Adaptive Policies for Linear-Quadratic Systems. (arXiv:1711.07230v1 [cs.SY])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07230"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07230", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the classical problem of control of linear systems with quadratic \ncost. When the true system dynamics are unknown, an adaptive policy is required \nfor learning the model parameters and planning a control policy simultaneously. \nAddressing this trade-off between accurate estimation and good control \nrepresents the main challenge in the area of adaptive control. Another \nimportant issue is to prevent the system becoming destabilized due to lack of \nknowledge of its dynamics. Asymptotically optimal approaches have been \nextensively studied in the literature, but there are very few non-asymptotic \nresults which also do not provide a comprehensive treatment of the problem. \n</p> \n<p>In this work, we establish finite time high probability regret bounds that \nare optimal up to logarithmic factors. We also provide high probability \nguarantees for a stabilization algorithm based on random linear feedbacks. The \nresults are obtained under very mild assumptions, requiring: (i) \nstabilizability of the matrices encoding the system's dynamics, and (ii) degree \nof heaviness of the noise distribution. To derive our results, we also \nintroduce a number of new concepts and technical tools. \n</p>"}, "author": "Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, George Michailidis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834625", "id": "tag:google.com,2005:reader/item/00000003379747ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Positive semi-definite embedding for dimensionality reduction and out-of-sample extensions. (arXiv:1711.07271v1 [cs.LG])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07271"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07271", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In machine learning or statistics, it is often desirable to reduce the \ndimensionality of high dimensional data. We propose to obtain the low \ndimensional embedding coordinates as the eigenvectors of a positive \nsemi-definite kernel matrix. This kernel matrix is the solution of a \nsemi-definite program promoting a low rank solution and defined with the help \nof a diffusion kernel. Besides, we also discuss an infinite dimensional \nanalogue of the same semi-definite program. From a practical perspective, a \nmain feature of our approach is the existence of a non-linear out-of-sample \nextension formula of the embedding coordinates that we call a projected \nNystr\\\"om approximation. This extension formula yields an extension of the \nkernel matrix to a data-dependent Mercer kernel function. Although the \nsemi-definite program may be solved directly, we propose another strategy based \non a rank constrained formulation solved thanks to a projected power method \nalgorithm followed by a singular value decomposition. This strategy allows for \na reduced computational time. \n</p>"}, "author": "Micha&#xeb;l Fanuel, Antoine Aspeel, Jean-Charles Delvennes, Johan A.K. Suykens", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834624", "id": "tag:google.com,2005:reader/item/00000003379747be", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Speech recognition for medical conversations. (arXiv:1711.07274v1 [cs.CL])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07274"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07274", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we document our experiences with developing speech recognition \nfor medical transcription - a system that automatically transcribes \ndoctor-patient conversations. Towards this goal, we built a system along two \ndifferent methodological lines - a Connectionist Temporal Classification (CTC) \nphoneme based model and a Listen Attend and Spell (LAS) grapheme based model. \nTo train these models we used a corpus of anonymized conversations representing \napproximately 14,000 hours of speech. Because of noisy transcripts and \nalignments in the corpus, a significant amount of effort was invested in data \ncleaning issues. We describe a two-stage strategy we followed for segmenting \nthe data. The data cleanup and development of a matched language model was \nessential to the success of the CTC based models. The LAS based models, however \nwere found to be resilient to alignment and transcript noise and did not \nrequire the use of language models. CTC models were able to achieve a word \nerror rate of 20.1%, and the LAS models were able to achieve 18.3%. Our \nanalysis shows that both models perform well on important medical utterances \nand therefore can be practical for transcribing medical conversations. \n</p>"}, "author": "Chung-Cheng Chiu, Anshuman Tripathi, Katherine Chou, Chris Co, Navdeep Jaitly, Diana Jaunzeikare, Anjuli Kannan, Patrick Nguyen, Hasim Sak, Ananth Sankar, Justin Tansuwan, Nathan Wan, Yonghui Wu, Xuedong Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834623", "id": "tag:google.com,2005:reader/item/00000003379747de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Non-exchangeable random partition models for microclustering. (arXiv:1711.07287v1 [stat.ME])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07287"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07287", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many popular random partition models, such as the Chinese restaurant process \nand its two-parameter extension, fall in the class of exchangeable random \npartitions, and have found wide applicability in model-based clustering, \npopulation genetics, ecology or network analysis. While the exchangeability \nassumption is sensible in many cases, it has some strong implications. In \nparticular, Kingman's representation theorem implies that the size of the \nclusters necessarily grows linearly with the sample size; this feature may be \nundesirable for some applications, as recently pointed out by Miller et al. \n(2015). We present here a flexible class of non-exchangeable random partition \nmodels which are able to generate partitions whose cluster sizes grow \nsublinearly with the sample size, and where the growth rate is controlled by \none parameter. Along with this result, we provide the asymptotic behaviour of \nthe number of clusters of a given size, and show that the model can exhibit a \npower-law behavior, controlled by another parameter. The construction is based \non completely random measures and a Poisson embedding of the random partition, \nand inference is performed using a Sequential Monte Carlo algorithm. \nAdditionally, we show how the model can also be directly used to generate \nsparse multigraphs with power-law degree distributions and degree sequences \nwith sublinear growth. Finally, experiments on real datasets emphasize the \nusefulness of the approach compared to a two-parameter Chinese restaurant \nprocess. \n</p>"}, "author": "Giuseppe Di Benedetto, Fran&#xe7;ois Caron, Yee Whye Teh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834622", "id": "tag:google.com,2005:reader/item/00000003379747fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks. (arXiv:1711.07354v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07354"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07354", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>By lifting the ReLU function into a higher dimensional space, we develop a \nsmooth multi-convex formulation for training feed-forward deep neural networks \n(DNNs). This allows us to develop a block coordinate descent (BCD) training \nalgorithm consisting of a sequence of numerically well-behaved convex \noptimizations. Using ideas from proximal point methods in convex analysis, we \nprove that this BCD algorithm will converge globally to a stationary point with \nR-linear convergence rate of order one. In experiments with the MNIST database, \nDNNs trained with this BCD algorithm consistently yielded better test-set error \nrates than identical DNN architectures trained via all the stochastic gradient \ndescent (SGD) variants in the Caffe toolbox. \n</p>"}, "author": "Ziming Zhang, Matthew Brand", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834621", "id": "tag:google.com,2005:reader/item/000000033797482e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Promise and Peril of Human Evaluation for Model Interpretability. (arXiv:1711.07414v1 [cs.AI])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07414"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07414", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Transparency, user trust, and human comprehension are popular ethical \nmotivations for interpretable machine learning. In support of these goals, \nresearchers evaluate model explanation performance using humans and real world \napplications. This alone presents a challenge in many areas of artificial \nintelligence. In this position paper, we propose a distinction between \ndescriptive and persuasive explanations. We discuss reasoning suggesting that \nfunctional interpretability may be correlated with cognitive function and user \npreferences. If this is indeed the case, evaluation and optimization using \nfunctional metrics could perpetuate implicit cognitive bias in explanations \nthat threaten transparency. Finally, we propose two potential research \ndirections to disambiguate cognitive function and explanation models, retaining \ncontrol over the tradeoff between accuracy and interpretability. \n</p>"}, "author": "Bernease Herman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834620", "id": "tag:google.com,2005:reader/item/000000033797483e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Modular Continual Learning in a Unified Visual Environment. (arXiv:1711.07425v1 [cs.LG])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07425"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07425", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A core aspect of human intelligence is the ability to learn new tasks quickly \nand switch between them flexibly. Here, we describe a modular continual \nreinforcement learning paradigm inspired by these abilities. We first introduce \na visual interaction environment that allows many types of tasks to be unified \nin a single framework. We then describe a reward map prediction scheme that \nlearns new tasks robustly in the very large state and action spaces required by \nsuch an environment. We investigate how properties of module architecture \ninfluence efficiency of task learning, showing that a module motif \nincorporating specific design principles (e.g. early bottlenecks, low-order \npolynomial nonlinearities, and symmetry) significantly outperforms more \nstandard neural network motifs, needing fewer training examples and fewer \nneurons to achieve high levels of performance. Finally, we present a \nmeta-controller architecture for task switching based on a dynamic neural \nvoting scheme, which allows new modules to use information learned from \npreviously-seen tasks to substantially improve their own learning efficiency. \n</p>"}, "author": "Kevin T. Feigelis, Blue Sheffer, Daniel L. K. Yamins", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834619", "id": "tag:google.com,2005:reader/item/000000033797484d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Relaxed Oracles for Semi-Supervised Clustering. (arXiv:1711.07433v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07433"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07433", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7114355\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7114355&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Pairwise \"same-cluster\" queries are one of the most widely used forms of \nsupervision in semi-supervised clustering. However, it is impractical to ask \nhuman oracles to answer every query correctly. In this paper, we study the \ninfluence of allowing \"not-sure\" answers from a weak oracle and propose an \neffective algorithm to handle such uncertainties in query responses. Two \nrealistic weak oracle models are considered where ambiguity in answering \ndepends on the distance between two points. We show that a small query \ncomplexity is adequate for effective clustering with high probability by \nproviding better pairs to the weak oracle. Experimental results on synthetic \nand real data show the effectiveness of our approach in overcoming supervision \nuncertainties and yielding high quality clusters. \n</p>"}, "author": "Taewan Kim, Joydeep Ghosh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834618", "id": "tag:google.com,2005:reader/item/0000000337974876", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On Convergence of Epanechnikov Mean Shift. (arXiv:1711.07441v1 [stat.ML])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07441"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07441", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Epanechnikov Mean Shift is a simple yet empirically very effective algorithm \nfor clustering. It localizes the centroids of data clusters via estimating \nmodes of the probability distribution that generates the data points, using the \n`optimal' Epanechnikov kernel density estimator. However, since the procedure \ninvolves non-smooth kernel density functions, the convergence behavior of \nEpanechnikov mean shift lacks theoretical support as of this writing---most of \nthe existing analyses are based on smooth functions and thus cannot be applied \nto Epanechnikov Mean Shift. In this work, we first show that the original \nEpanechnikov Mean Shift may indeed terminate at a non-critical point, due to \nthe non-smoothness nature. Based on our analysis, we propose a simple remedy to \nfix it. The modified Epanechnikov Mean Shift is guaranteed to terminate at a \nlocal maximum of the estimated density, which corresponds to a cluster \ncentroid, within a finite number of iterations. We also propose a way to avoid \nrunning the Mean Shift iterates from every data point, while maintaining good \nclustering accuracies under non-overlapping spherical Gaussian mixture models. \nThis further pushes Epanechnikov Mean Shift to handle very large and \nhigh-dimensional data sets. Experiments show surprisingly good performance \ncompared to the Lloyd's K-means algorithm and the EM algorithm. \n</p>"}, "author": "Kejun Huang, Xiao Fu, Nicholas D. Sidiropoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834617", "id": "tag:google.com,2005:reader/item/0000000337974897", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A generalised framework for detailed classification of swimming paths inside the Morris Water Maze. (arXiv:1711.07446v1 [q-bio.QM])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07446"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07446", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Morris Water Maze is commonly used in behavioural neuroscience for the \nstudy of spatial learning with rodents. Over the years, various methods of \nanalysing rodent data collected in this task have been proposed. These methods \nspan from classical performance measurements (e.g. escape latency, rodent \nspeed, quadrant preference) to more sophisticated methods of categorisation \nwhich classify the animal swimming path into behavioural classes known as \nstrategies. Classification techniques provide additional insight in relation to \nthe actual animal behaviours but still only a limited amount of studies utilise \nthem mainly because they highly depend on machine learning knowledge. We have \npreviously demonstrated that the animals implement various strategies and by \nclassifying whole trajectories can lead to the loss of important information. \nIn this work, we developed a generalised and robust classification methodology \nwhich implements majority voting to boost the classification performance and \nsuccessfully nullify the need of manual tuning. Based on this framework, we \nbuilt a complete software, capable of performing the full analysis described in \nthis paper. The software provides an easy to use graphical user interface (GUI) \nthrough which users can enter their trajectory data, segment and label them and \nfinally generate reports and figures of the results. \n</p>"}, "author": "Avgoustinos Vouros, Tiago V. Gehring, Kinga Szydlowska, Artur Janusz, Mike Croucher, Katarzyna Lukasiuk, Witold Konopka, Carmen Sandi, Eleni Vasilaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834616", "id": "tag:google.com,2005:reader/item/00000003379748c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bidirectional Conditional Generative Adversarial Networks. (arXiv:1711.07461v1 [cs.LG])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07461"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07461", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Conditional variants of Generative Adversarial Networks (GANs), known as \ncGANs, are generative models that can produce data samples ($x$) conditioned on \nboth latent variables ($z$) and known auxiliary information ($c$). Another GAN \nvariant, Bidirectional GAN (BiGAN) is a recently developed framework for \nlearning the inverse mapping from $x$ to $z$ through an encoder trained \nsimultaneously with the generator and the discriminator of an unconditional \nGAN. We propose the Bidirectional Conditional GAN (BCGAN), which combines cGANs \nand BiGANs into a single framework with an encoder that learns inverse mappings \nfrom $x$ to both $z$ and $c$, trained simultaneously with the conditional \ngenerator and discriminator in an end-to-end setting. We present crucial \ntechniques for training BCGANs, which incorporate an extrinsic factor loss \nalong with an associated dynamically-tuned importance weight. As compared to \nother encoder-based GANs, BCGANs not only encode $c$ more accurately but also \nutilize $z$ and $c$ more effectively and in a more disentangled way to generate \ndata samples. \n</p>"}, "author": "Ayush Jaiswal, Wael AbdAlmageed, Yue Wu, Premkumar Natarajan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834615", "id": "tag:google.com,2005:reader/item/0000000337974914", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Glitch Classification and Clustering for LIGO with Deep Transfer Learning. (arXiv:1711.07468v1 [astro-ph.IM])", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07468"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07468", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The detection of gravitational waves with LIGO and Virgo requires a detailed \nunderstanding of the response of these instruments in the presence of \nenvironmental and instrumental noise. Of particular interest is the study of \nnon-Gaussian noise transients known as glitches, since their high occurrence \nrate in LIGO/Virgo data can obscure or even mimic true gravitational wave \nsignals. Therefore, successfully identifying and excising glitches is of utmost \nimportance to detect and characterize gravitational waves. In this article, we \npresent the first application of Deep Learning combined with Transfer Learning \nfor glitch classification, using real data from LIGO's first discovery campaign \nlabeled by Gravity Spy, showing that knowledge from pre-trained models for \nreal-world object recognition can be transferred for classifying spectrograms \nof glitches. We demonstrate that this method enables the optimal use of very \ndeep convolutional neural networks for glitch classification given small \nunbalanced training datasets, significantly reduces the training time, and \nachieves state-of-the-art accuracy above 98.8%. Once trained via transfer \nlearning, we show that the networks can be truncated and used as feature \nextractors for unsupervised clustering to automatically group new classes of \nglitches. This feature is of critical importance to identify and remove new \ntypes of glitches which will occur as the LIGO/Virgo detectors gradually attain \ndesign sensitivity. \n</p>"}, "author": "Daniel George, Hongyu Shen, E. A. Huerta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834594", "id": "tag:google.com,2005:reader/item/0000000337974a1d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Triangle Generative Adversarial Networks. (arXiv:1709.06548v2 [cs.LG] UPDATED)", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.06548"}], "alternate": [{"href": "http://arxiv.org/abs/1709.06548", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A Triangle Generative Adversarial Network ($\\Delta$-GAN) is developed for \nsemi-supervised cross-domain joint distribution matching, where the training \ndata consists of samples from each domain, and supervision of domain \ncorrespondence is provided by only a few paired samples. $\\Delta$-GAN consists \nof four neural networks, two generators and two discriminators. The generators \nare designed to learn the two-way conditional distributions between the two \ndomains, while the discriminators implicitly define a ternary discriminative \nfunction, which is trained to distinguish real data pairs and two kinds of fake \ndata pairs. The generators and discriminators are trained together using \nadversarial learning. Under mild assumptions, in theory the joint distributions \ncharacterized by the two generators concentrate to the data distribution. In \nexperiments, three different kinds of domain pairs are considered, image-label, \nimage-image and image-attribute pairs. Experiments on semi-supervised image \nclassification, image-to-image translation and attribute-based image generation \ndemonstrate the superiority of the proposed approach. \n</p>"}, "author": "Zhe Gan, Liqun Chen, Weiyao Wang, Yunchen Pu, Yizhe Zhang, Hao Liu, Chunyuan Li, Lawrence Carin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834582", "id": "tag:google.com,2005:reader/item/0000000337974ae6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Three Factors Influencing Minima in SGD. (arXiv:1711.04623v1 [cs.LG] CROSS LISTED)", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04623"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04623", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the properties of the endpoint of stochastic gradient descent (SGD). \nBy approximating SGD as a stochastic differential equation (SDE) we consider \nthe Boltzmann-Gibbs equilibrium distribution of that SDE under the assumption \nof isotropic variance in loss gradients. Through this analysis, we find that \nthree factors - learning rate, batch size and the variance of the loss \ngradients - control the trade-off between the depth and width of the minima \nfound by SGD, with wider minima favoured by a higher ratio of learning rate to \nbatch size. We have direct control over the learning rate and batch size, while \nthe variance is determined by the choice of model architecture, model \nparameterization and dataset. In the equilibrium distribution only the ratio of \nlearning rate to batch size appears, implying that the equilibrium distribution \nis invariant under a simultaneous rescaling of learning rate and batch size by \nthe same amount. We then explore experimentally how learning rate and batch \nsize affect SGD from two perspectives: the endpoint of SGD and the dynamics \nthat lead up to it. For the endpoint, the experiments suggest the endpoint of \nSGD is invariant under simultaneous rescaling of batch size and learning rate, \nand also that a higher ratio leads to flatter minima, both findings are \nconsistent with our theoretical analysis. We note experimentally that the \ndynamics also seem to be invariant under the same rescaling of learning rate \nand batch size, which we explore showing that one can exchange batch size and \nlearning rate for cyclical learning rate schedule. Next, we illustrate how \nnoise affects memorization, showing that high noise levels lead to better \ngeneralization. Finally, we find experimentally that the invariance under \nsimultaneous rescaling of learning rate and batch size breaks down if the \nlearning rate gets too large or the batch size gets too small. \n</p>"}, "author": "Stanis&#x142;aw Jastrz&#x119;bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, Amos Storkey", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511308877835", "timestampUsec": "1511308877834581", "id": "tag:google.com,2005:reader/item/0000000337974b5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Using KL-divergence to focus Deep Visual Explanation. (arXiv:1711.06431v1 [cs.AI] CROSS LISTED)", "published": 1511308877, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06431"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a method for explaining the image classification predictions of \ndeep convolution neural networks, by highlighting the pixels in the image which \ninfluence the final class prediction. Our method requires the identification of \na heuristic method to select parameters hypothesized to be most relevant in \nthis prediction, and here we use Kullback-Leibler divergence to provide this \nfocus. Overall, our approach helps in understanding and interpreting deep \nnetwork predictions and we hope contributes to a foundation for such \nunderstanding of deep learning networks. In this brief paper, our experiments \nevaluate the performance of two popular networks in this context of \ninterpretability. \n</p>"}, "author": "Housam Khalifa Bashier Babiker, Randy Goebel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949161", "id": "tag:google.com,2005:reader/item/0000000337308943", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Is prioritized sweeping the better episodic control?. (arXiv:1711.06677v1 [cs.AI])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06677"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06677", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Episodic control has been proposed as a third approach to reinforcement \nlearning, besides model-free and model-based control, by analogy with the three \ntypes of human memory. i.e. episodic, procedural and semantic memory. But the \ntheoretical properties of episodic control are not well investigated. Here I \nshow that in deterministic tree Markov decision processes, episodic control is \nequivalent to a form of prioritized sweeping in terms of sample efficiency as \nwell as memory and computation demands. For general deterministic and \nstochastic environments, prioritized sweeping performs better even when memory \nand computation demands are restricted to be equal to those of episodic \ncontrol. These results suggest generalizations of prioritized sweeping to \npartially observable environments, its combined use with function approximation \nand the search for possible implementations of prioritized sweeping in brains. \n</p>"}, "author": "Johanni Brea", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949160", "id": "tag:google.com,2005:reader/item/0000000337308954", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning to Organize Knowledge with N-Gram Machines. (arXiv:1711.06744v1 [cs.CL])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06744"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06744", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks (DNNs) had great success on NLP tasks such as language \nmodeling, machine translation and certain question answering (QA) tasks. \nHowever, the success is limited at more knowledge intensive tasks such as QA \nfrom a big corpus. Existing end-to-end deep QA models (Miller et al., 2016; \nWeston et al., 2014) need to read the entire text after observing the question, \nand therefore their complexity in responding a question is linear in the text \nsize. This is prohibitive for practical tasks such as QA from Wikipedia, a \nnovel, or the Web. We propose to solve this scalability issue by using symbolic \nmeaning representations, which can be indexed and retrieved efficiently with \ncomplexity that is independent of the text size. More specifically, we use \nsequence-to-sequence models to encode knowledge symbolically and generate \nprograms to answer questions from the encoded knowledge. We apply our approach, \ncalled the N-Gram Machine (NGM), to the bAbI tasks (Weston et al., 2015) and a \nspecial version of them (\"life-long bAbI\") which has stories of up to 10 \nmillion sentences. Our experiments show that NGM can successfully solve both of \nthese tasks accurately and efficiently. Unlike fully differentiable memory \nmodels, NGM's time complexity and answering quality are not affected by the \nstory length. The whole system of NGM is trained end-to-end with REINFORCE \n(Williams, 1992). To avoid high variance in gradient estimation, which is \ntypical in discrete latent variable models, we use beam search instead of \nsampling. To tackle the exponentially large search space, we use a stabilized \nauto-encoding objective and a structure tweak procedure to iteratively reduce \nand refine the search space. \n</p>"}, "author": "Fan Yang, Jiazhong Nie, William W. Cohen, Ni Lao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949159", "id": "tag:google.com,2005:reader/item/0000000337308969", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Generation and Consolidation of Recollections for Efficient Deep Lifelong Learning. (arXiv:1711.06761v1 [cs.LG])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06761"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06761", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7114621\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7114621&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep lifelong learning systems need to efficiently manage resources to scale \nto large numbers of experiences and non-stationary goals. In this paper, we \nexplore the relationship between lossy compression and the resource constrained \nlifelong learning problem of function transferability. We demonstrate that \nlossy episodic experience storage can enable efficient function transferability \nbetween different architectures and algorithms at a fraction of the storage \ncost of lossless storage. This is achieved by introducing a generative \nknowledge distillation strategy that does not store any full training examples. \nAs an important extension of this idea, we show that lossy recollections \nstabilize deep networks much better than lossless sampling in resource \nconstrained settings of lifelong learning while avoiding catastrophic \nforgetting. For this setting, we propose a novel dual purpose recollection \nbuffer used to both stabilize the recollection generator itself and an \naccompanying reasoning model. \n</p>"}, "author": "Matthew Riemer, Michele Franceschini, Tim Klinger", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949158", "id": "tag:google.com,2005:reader/item/0000000337308979", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Scalable Relaxations of Sparse Packing Constraints: Optimal Biocontrol in Predator-Prey Network. (arXiv:1711.06800v1 [cs.AI])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06800"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06800", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c71a9381\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c71a9381&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Cascades represent rapid changes in networks. A cascading phenomenon of \necological and economic impact is the spread of invasive species in geographic \nlandscapes. The most promising management strategy is often biocontrol, which \nentails introducing a natural predator able to control the invading population, \na setting that can be treated as two interacting cascades of predator and prey \npopulations. We formulate and study a nonlinear problem of optimal biocontrol: \noptimally seeding the predator cascade over time to minimize the harmful prey \npopulation. Recurring budgets, which typically face conservation organizations, \nnaturally leads to sparse constraints which make the problem amenable to \napproximation algorithms. Available methods based on continuous relaxations \nscale poorly, to remedy this we develop a novel and scalable randomized \nalgorithm based on a width relaxation, applicable to a broad class of \ncombinatorial optimization problems. We evaluate our contributions in the \ncontext of biocontrol for the insect pest Hemlock Wolly Adelgid (HWA) in \neastern North America. Our algorithm outperforms competing methods in terms of \nscalability and solution quality, and finds near optimal strategies for the \ncontrol of the HWA for fine-grained networks -- an important problem in \ncomputational sustainability. \n</p>"}, "author": "Johan Bjorck, Yiwei Bai, Xiaojian Wu, Yexiang Xue, Mark C. Whitmore, Carla Gomes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949157", "id": "tag:google.com,2005:reader/item/0000000337308987", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Acquiring Common Sense Spatial Knowledge through Implicit Spatial Templates. (arXiv:1711.06821v1 [cs.AI])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06821"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06821", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Spatial understanding is a fundamental problem with wide-reaching real-world \napplications. The representation of spatial knowledge is often modeled with \nspatial templates, i.e., regions of acceptability of two objects under an \nexplicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with \nprior work that restricts spatial templates to explicit spatial prepositions \n(e.g., \"glass on table\"), here we extend this concept to implicit spatial \nlanguage, i.e., those relationships (generally actions) for which the spatial \narrangement of the objects is only implicitly implied (e.g., \"man riding \nhorse\"). In contrast with explicit relationships, predicting spatial \narrangements from implicit spatial language requires significant common sense \nspatial understanding. Here, we introduce the task of predicting spatial \ntemplates for two objects under a relationship, which can be seen as a spatial \nquestion-answering task with a (2D) continuous output (\"where is the man w.r.t. \na horse when the man is walking the horse?\"). We present two simple \nneural-based models that leverage annotated images and structured text to learn \nthis task. The good performance of these models reveals that spatial locations \nare to a large extent predictable from implicit spatial language. Crucially, \nthe models attain similar performance in a challenging generalized setting, \nwhere the object-relation-object combinations (e.g.,\"man walking dog\") have \nnever been seen before. Next, we go one step further by presenting the models \nwith unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging \nword embeddings enables the models to output accurate spatial predictions, \nproving that the models acquire solid common sense spatial knowledge allowing \nfor such generalization. \n</p>"}, "author": "Guillem Collell, Luc Van Gool, Marie-Francine Moens", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949156", "id": "tag:google.com,2005:reader/item/0000000337308999", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Anonymous Hedonic Game for Task Allocation in a Large-Scale Multiple Agent System. (arXiv:1711.06871v1 [cs.MA])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06871"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06871", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a novel game-theoretical autonomous decision-making \nframework to address a task allocation problem for a swarm of multiple agents. \nWe consider cooperation of self-interested agents and show that agents who have \nsocial inhibition can converge to a Nash stable partition (i.e., social \nagreement) using our proposed decentralised algorithm within polynomial time. \nThe algorithm is simple and executable based on local interactions with \nneighbour agents under a strongly-connected communication network and even in \nasynchronous environments. We analytically present a mathematical formulation \nfor computing the lower bound of a converged solution's suboptimality and \nadditionally show that 50 % of suboptimality can be minimally guaranteed if \nsocial utilities are non-decreasing functions with respect to the number of \nco-working agents. Through numerical experiments, it is confirmed that the \nproposed framework is scalable, fast adaptable against dynamical environments, \nand robust even in a realistic situation where some of the agents temporarily \nsomehow do not operate during a mission. \n</p>"}, "author": "Inmo Jang, Hyo-Sang Shin, Antonios Tsourdos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949155", "id": "tag:google.com,2005:reader/item/00000003373089a5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning to select computations. (arXiv:1711.06892v1 [cs.AI])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06892"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06892", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Efficient use of limited computational resources is essential to \nintelligence. Selecting computations optimally according to rational \nmetareasoning would achieve this, but rational metareasoning is computationally \nintractable. Inspired by psychology and neuroscience, we propose the first \nlearning algorithm for approximating the optimal selection of computations. We \nderive a general, sample-efficient reinforcement learning algorithm for \nlearning to select computations from the insight that the value of computation \nlies between the myopic value of computation and the value of perfect \ninformation. We evaluate the performance of our method against two \nstate-of-the-art methods for approximate metareasoning--the meta-greedy \nheuristic and the blinkered policy--on three increasingly difficult \nmetareasoning problems: metareasoning about when to terminate computation, \nmetareasoning about how to choose between multiple actions, and metareasoning \nabout planning. Across all three domains, our method achieved near-optimal \nperformance and significantly outperformed the meta-greedy heuristic. The \nblinkered policy performed on par with our method in metareasoning about \ndecision-making, but it is not directly applicable to metareasoning about \nplanning where our method outperformed both the meta-greedy heuristic and a \ngeneralization of the blinkered policy. Our results are a step towards building \nself-improving AI systems that can learn to make optimal use of their limited \ncomputational resources to efficiently solve complex problems in real-time. \n</p>"}, "author": "Falk Lieder, Frederick Callaway, Sayan Gul, Paul M. Krueger, Thomas L. Griffiths", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949154", "id": "tag:google.com,2005:reader/item/00000003373089b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Run, skeleton, run: skeletal model in a physics-based simulation. (arXiv:1711.06922v1 [cs.AI])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06922"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06922", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present our approach to solve a physics-based reinforcement \nlearning challenge \"Learning to Run\" with objective to train \nphysiologically-based human model to navigate a complex obstacle course as \nquickly as possible. The environment is computationally expensive, has a \nhigh-dimensional continuous action space and is stochastic. We benchmark state \nof the art policy-gradient methods and test several improvements, such as layer \nnormalization, parameter noise, action and state reflecting, to stabilize \ntraining and improve its sample-efficiency. We found that the Deep \nDeterministic Policy Gradient method is the most efficient method for this \nenvironment and the improvements we have introduced help to stabilize training. \nLearned models are able to generalize to new physical scenarios, e.g. different \nobstacle courses. \n</p>"}, "author": "Mikhail Pavlov, Sergey Kolesnikov, Sergey M. Plis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949153", "id": "tag:google.com,2005:reader/item/00000003373089c3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Computational Results for Extensive-Form Adversarial Team Games. (arXiv:1711.06930v1 [cs.AI])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06930"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06930", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We provide, to the best of our knowledge, the first computational study of \nextensive-form adversarial team games. These games are sequential, zero-sum \ngames in which a team of players, sharing the same utility function, faces an \nadversary. We define three different scenarios according to the communication \ncapabilities of the team. In the first, the teammates can communicate and \ncorrelate their actions both before and during the play. In the second, they \ncan only communicate before the play. In the third, no communication is \npossible at all. We define the most suitable solution concepts, and we study \nthe inefficiency caused by partial or null communication, showing that the \ninefficiency can be arbitrarily large in the size of the game tree. \nFurthermore, we study the computational complexity of the equilibrium-finding \nproblem in the three scenarios mentioned above, and we provide, for each of the \nthree scenarios, an exact algorithm. Finally, we empirically evaluate the \nscalability of the algorithms in random games and the inefficiency caused by \npartial or null communication. \n</p>"}, "author": "Andrea Celli, Nicola Gatti", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949152", "id": "tag:google.com,2005:reader/item/00000003373089ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The destiny of constant structure discrete time closed semantic systems. (arXiv:1711.07071v1 [cs.AI])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07071"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07071", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Constant structure closed semantic systems are the systems each element of \nwhich receives its definition through the correspondent unchangeable set of \nother elements of the system. Discrete time means here that the definitions of \nthe elements change iteratively and simultaneously based on the \"neighbor \nportraits\" from the previous iteration. I prove that the iterative redefinition \nprocess in such class of systems will quickly degenerate into a series of \npairwise isomorphic states and discuss some directions of further research. \n</p>"}, "author": "Evgeny Ivanko", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949151", "id": "tag:google.com,2005:reader/item/00000003373089da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Modeling Epistemological Principles for Bias Mitigation in AI Systems: An Illustration in Hiring Decisions. (arXiv:1711.07111v1 [cs.AI])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07111"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07111", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Artificial Intelligence (AI) has been used extensively in automatic decision \nmaking in a broad variety of scenarios, ranging from credit ratings for loans \nto recommendations of movies. Traditional design guidelines for AI models focus \nessentially on accuracy maximization, but recent work has shown that \neconomically irrational and socially unacceptable scenarios of discrimination \nand unfairness are likely to arise unless these issues are explicitly \naddressed. This undesirable behavior has several possible sources, such as \nbiased datasets used for training that may not be detected in black-box models. \nAfter pointing out connections between such bias of AI and the problem of \ninduction, we focus on Popper's contributions after Hume's, which offer a \nlogical theory of preferences. An AI model can be preferred over others on \npurely rational grounds after one or more attempts at refutation based on \naccuracy and fairness. Inspired by such epistemological principles, this paper \nproposes a structured approach to mitigate discrimination and unfairness caused \nby bias in AI systems. In the proposed computational framework, models are \nselected and enhanced after attempts at refutation. To illustrate our \ndiscussion, we focus on hiring decision scenarios where an AI system filters in \nwhich job applicants should go to the interview phase. \n</p>"}, "author": "Marisa Vasconcelos, Carlos Cardonha, Bernardo Gon&#xe7;alves", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949150", "id": "tag:google.com,2005:reader/item/00000003373089ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise. (arXiv:1711.07131v1 [cs.CV])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07131"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07131", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study the problem of learning image classification models \nwith label noise. Existing approaches depending on human supervision are \ngenerally not scalable as manually identifying correct or incorrect labels is \ntimeconsuming, whereas approaches not relying on human supervision are scalable \nbut less effective. To reduce the amount of human supervision for label noise \ncleaning, we introduce CleanNet, a joint neural embedding network, which only \nrequires a fraction of the classes being manually verified to provide the \nknowledge of label noise that can be transferred to other classes. We further \nintegrate CleanNet and conventional convolutional neural network classifier \ninto one framework for image classification learning. We demonstrate the \neffectiveness of the proposed algorithm on both of the label noise detection \ntask and the image classification on noisy data task on several large-scale \ndatasets. Experimental results show that CleanNet can reduce label noise \ndetection error rate on held-out classes where no human supervision available \nby 41.5% compared to current weakly supervised methods. It also achieves 47% of \nthe performance gain of verifying all images with only 3.2% images verified on \nan image classification task. \n</p>"}, "author": "Kuang-Huei Lee, Xiaodong He, Lei Zhang, Linjun Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949149", "id": "tag:google.com,2005:reader/item/0000000337308a09", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interactive, Intelligent Tutoring for Auxiliary Constructions in Geometry Proofs. (arXiv:1711.07154v1 [cs.HC])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07154"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07154", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c71a9777\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c71a9777&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Geometry theorem proving forms a major and challenging component in the K-12 \nmathematics curriculum. A particular difficult task is to add auxiliary \nconstructions (i.e, additional lines or points) to aid proof discovery. \nAlthough there exist many intelligent tutoring systems proposed for geometry \nproofs, few teach students how to find auxiliary constructions. And the few \nexceptions are all limited by their underlying reasoning processes for \nsupporting auxiliary constructions. This paper tackles these weaknesses of \nprior systems by introducing an interactive geometry tutor, the Advanced \nGeometry Proof Tutor (AGPT). It leverages a recent automated geometry prover to \nprovide combined benefits that any geometry theorem prover or intelligent \ntutoring system alone cannot accomplish. In particular, AGPT not only can \nautomatically process images of geometry problems directly, but also can \ninteractively train and guide students toward discovering auxiliary \nconstructions on their own. We have evaluated AGPT via a pilot study with 78 \nhigh school students. The study results show that, on training students how to \nfind auxiliary constructions, there is no significant perceived difference \nbetween AGPT and human tutors, and AGPT is significantly more effective than \nthe state-of-the-art geometry solver that produces human-readable proofs. \n</p>"}, "author": "Ke Wang, Zhendong Su", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949148", "id": "tag:google.com,2005:reader/item/0000000337308a1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dynamic Neural Program Embedding for Program Repair. (arXiv:1711.07163v1 [cs.AI])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07163"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07163", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural program embeddings have shown much promise recently for a variety of \nprogram analysis tasks, including program synthesis, program repair, fault \nlocalization, etc. However, most existing program embeddings are based on \nsyntactic features of programs, such as raw token sequences or abstract syntax \ntrees. Unlike images and text, a program has an unambiguous semantic meaning \nthat can be difficult to capture by only considering its syntax (i.e. \nsyntactically similar pro- grams can exhibit vastly different run-time \nbehavior), which makes syntax-based program embeddings fundamentally limited. \nThis paper proposes a novel semantic program embedding that is learned from \nprogram execution traces. Our key insight is that program states expressed as \nsequential tuples of live variable values not only captures program semantics \nmore precisely, but also offer a more natural fit for Recurrent Neural Networks \nto model. We evaluate different syntactic and semantic program embeddings on \npredicting the types of errors that students make in their submissions to an \nintroductory programming class and two exercises on the CodeHunt education \nplatform. Evaluation results show that our new semantic program embedding \nsignificantly outperforms the syntactic program embeddings based on token \nsequences and abstract syntax trees. In addition, we augment a search-based \nprogram repair system with the predictions obtained from our se- mantic \nembedding, and show that search efficiency is also significantly improved. \n</p>"}, "author": "Ke Wang, Rishabh Singh, Zhendong Su", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949147", "id": "tag:google.com,2005:reader/item/0000000337308a33", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Maximizing Non-monotone/Non-submodular Functions by Multi-objective Evolutionary Algorithms. (arXiv:1711.07214v1 [cs.NE])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07214"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07214", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Evolutionary algorithms (EAs) are a kind of nature-inspired general-purpose \noptimization algorithm, and have shown empirically good performance in solving \nvarious real-word optimization problems. However, due to the highly randomized \nand complex behavior, the theoretical analysis of EAs is difficult and is an \nongoing challenge, which has attracted a lot of research attentions. During the \nlast two decades, promising results on the running time analysis (one essential \ntheoretical aspect) of EAs have been obtained, while most of them focused on \nisolated combinatorial optimization problems, which do not reflect the \ngeneral-purpose nature of EAs. To provide a general theoretical explanation of \nthe behavior of EAs, it is desirable to study the performance of EAs on a \ngeneral class of combinatorial optimization problems. To the best of our \nknowledge, this direction has been rarely touched and the only known result is \nthe provably good approximation guarantees of EAs for the problem class of \nmaximizing monotone submodular set functions with matroid constraints, which \nincludes many NP-hard combinatorial optimization problems. The aim of this work \nis to contribute to this line of research. As many combinatorial optimization \nproblems also involve non-monotone or non-submodular objective functions, we \nconsider these two general problem classes, maximizing non-monotone submodular \nfunctions without constraints and maximizing monotone non-submodular functions \nwith a size constraint. We prove that a simple multi-objective EA called GSEMO \ncan generally achieve good approximation guarantees in polynomial expected \nrunning time. \n</p>"}, "author": "Chao Qian, Yang Yu, Ke Tang, Xin Yao, Zhi-Hua Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949146", "id": "tag:google.com,2005:reader/item/0000000337308a5a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Facets, Tiers and Gems: Ontology Patterns for Hypernormalisation. (arXiv:1711.07273v1 [cs.AI])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07273"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07273", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>There are many methodologies and techniques for easing the task of ontology \nbuilding. Here we describe the intersection of two of these: ontology \nnormalisation and fully programmatic ontology development. The first of these \ndescribes a standardized organisation for an ontology, with singly inherited \nself-standing entities, and a number of small taxonomies of refining entities. \nThe former are described and defined in terms of the latter and used to manage \nthe polyhierarchy of the self-standing entities. Fully programmatic development \nis a technique where an ontology is developed using a domain-specific language \nwithin a programming language, meaning that as well defining ontological \nentities, it is possible to add arbitrary patterns or new syntax within the \nsame environment. We describe how new patterns can be used to enable a new \nstyle of ontology development that we call hypernormalisation. \n</p>"}, "author": "Phillip Lord, Robert Stevens", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949145", "id": "tag:google.com,2005:reader/item/0000000337308a70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments. (arXiv:1711.07280v1 [cs.CV])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07280"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07280", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A robot that can carry out a natural-language instruction has been a dream \nsince before the Jetsons cartoon series imagined a life of leisure mediated by \na fleet of attentive robot helpers. It is a dream that remains stubbornly \ndistant. However, recent advances in vision and language methods have made \nincredible progress in closely related areas. This is significant because a \nrobot interpreting a natural-language navigation instruction on the basis of \nwhat it sees is carrying out a vision and language process that is similar to \nVisual Question Answering. Both tasks can be interpreted as visually grounded \nsequence-to-sequence translation problems, and many of the same methods are \napplicable. To enable and encourage the application of vision and language \nmethods to the problem of interpreting visually-grounded navigation \ninstructions, we present the Matterport3D Simulator -- a large-scale \nreinforcement learning environment based on real imagery. Using this simulator, \nwhich can in future support a range of embodied vision and language tasks, we \nprovide the first benchmark dataset for visually-grounded natural language \nnavigation in real buildings -- the Room-to-Room (R2R) dataset. \n</p>"}, "author": "Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S&#xfc;nderhauf, Ian Reid, Stephen Gould, Anton van den Hengel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949144", "id": "tag:google.com,2005:reader/item/0000000337308a7c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Related family-based attribute reduction of covering information systems when varying attribute sets. (arXiv:1711.07321v1 [cs.AI])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07321"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07321", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In practical situations, there are many dynamic covering information systems \nwith variations of attributes, but there are few studies on related \nfamily-based attribute reduction of dynamic covering information systems. In \nthis paper, we first investigate updated mechanisms of constructing attribute \nreducts for consistent and inconsistent covering information systems when \nvarying attribute sets by using related families. Then we employ examples to \nillustrate how to compute attribute reducts of dynamic covering information \nsystems with variations of attribute sets. Finally, the experimental results \nillustrates that the related family-based methods are effective to perform \nattribute reduction of dynamic covering information systems when attribute sets \nare varying with time. \n</p>"}, "author": "Guangming Lang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949143", "id": "tag:google.com,2005:reader/item/0000000337308a89", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Active Edge Evaluation on Expensive Graphs. (arXiv:1711.07329v1 [cs.RO])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07329"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07329", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Robots operate in environments with varying implicit structure. For instance, \na helicopter flying over terrain encounters a very different arrangement of \nobstacles than a robotic arm manipulating objects on a cluttered table top. \nState-of-the-art motion planning systems do not exploit this structure, thereby \nexpending valuable planning effort searching for implausible solutions. We are \ninterested in planning algorithms that actively infer the underlying structure \nof the valid configuration space during planning in order to find solutions \nwith minimal effort. Consider the problem of evaluating edges on a graph to \nquickly discover collision-free paths. Evaluating edges is expensive, both for \nrobots with complex geometries like robot arms, and for robots with limited \nonboard computation like UAVs. Until now, this challenge has been addressed via \nlaziness i.e. deferring edge evaluation until absolutely necessary, with the \nhope that edges turn out to be valid. However, all edges are not alike in value \n- some have a lot of potentially good paths flowing through them, and some \nothers encode the likelihood of neighbouring edges being valid. This leads to \nour key insight - instead of passive laziness, we can actively choose edges \nthat reduce the uncertainty about the validity of paths. We show that this is \nequivalent to the Bayesian active learning paradigm of decision region \ndetermination (DRD). However, the DRD problem is not only combinatorially hard, \nbut also requires explicit enumeration of all possible worlds. We propose a \nnovel framework that combines two DRD algorithms, DIRECT and BISECT, to \novercome both issues. We show that our approach outperforms several \nstate-of-the-art algorithms on a spectrum of planning problems for mobile \nrobots, manipulators and autonomous helicopters. \n</p>"}, "author": "Sanjiban Choudhury, Siddhartha Srinivasa, Sebastian Scherer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949142", "id": "tag:google.com,2005:reader/item/0000000337308a8f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension. (arXiv:1711.07341v1 [cs.CL])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07341"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07341", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper introduces a new neural structure called FusionNet, which extends \nexisting attention approaches from three perspectives. First, it puts forward a \nnovel concept of \"history of word\" to characterize attention information from \nthe lowest word-level embedding up to the highest semantic-level \nrepresentation. Second, it introduces an improved attention scoring function \nthat better utilizes the \"history of word\" concept. Third, it proposes a \nfully-aware multi-level attention mechanism to capture the complete information \nin one text (such as a question) and exploit it in its counterpart (such as \ncontext or passage) layer by layer. We apply FusionNet to the Stanford Question \nAnswering Dataset (SQuAD) and it achieves the first position for both single \nand ensemble model on the official SQuAD leaderboard at the time of writing \n(Oct. 4th, 2017). Meanwhile, we verify the generalization of FusionNet with two \nadversarial SQuAD datasets and it sets up the new state-of-the-art on both \ndatasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to \n51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%. \n</p>"}, "author": "Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, Weizhu Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949141", "id": "tag:google.com,2005:reader/item/0000000337308a9a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Classification with Costly Features using Deep Reinforcement Learning. (arXiv:1711.07364v1 [cs.AI])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07364"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07364", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study a classification problem where each feature can be acquired for a \ncost and the goal is to optimize the trade-off between classification precision \nand the total feature cost. We frame the problem as a sequential \ndecision-making problem, where we classify one sample in each episode. At each \nstep, an agent can use values of acquired features to decide whether to \npurchase another one or whether to classify the sample. We use vanilla Double \nDeep Q-learning, a standard reinforcement learning technique, to find a \nclassification policy. We show that this generic approach outperforms \nAdapt-Gbrt, currently the best-performing algorithm developed specifically for \nclassification with costly features. \n</p>"}, "author": "Jarom&#xed;r Janisch, Tom&#xe1;&#x161; Pevn&#xfd;, Viliam Lis&#xfd;", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949140", "id": "tag:google.com,2005:reader/item/0000000337308a9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "How morphological development can guide evolution. (arXiv:1711.07387v1 [q-bio.PE])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07387"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07387", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Organisms result from multiple adaptive processes occurring and interacting \nat different time scales. One such interaction is that between development and \nevolution. In modeling studies, it has been shown that development sweeps over \na series of traits in a single agent, and sometimes exposes promising static \ntraits. Subsequent evolution can then canalize these rare traits. Thus, \ndevelopment can, under the right conditions, increase evolvability. Here, we \nreport on a previously unknown phenomenon when embodied agents are allowed to \ndevelop and evolve: Evolution discovers body plans which are robust to control \nchanges, these body plans become genetically assimilated, yet controllers for \nthese agents are not assimilated. This allows evolution to continue climbing \nfitness gradients by tinkering with the developmental programs for controllers \nwithin these permissive body plans. This exposes a previously unknown detail \nabout the Baldwin effect: instead of all useful traits becoming genetically \nassimilated, only phenotypic traits that render the agent robust to changes in \nother traits become assimilated. We refer to this phenomenon as differential \ncanalization. This finding also has important implications for the evolutionary \ndesign of artificial and embodied agents such as robots: robots that are robust \nto internal changes in their controllers may also be robust to external changes \nin their environment, such as transferal from simulation to reality, or \ndeployment in novel environments. \n</p>"}, "author": "Sam Kriegman, Nick Cheney, Josh Bongard", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949139", "id": "tag:google.com,2005:reader/item/0000000337308aa5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Promise and Peril of Human Evaluation for Model Interpretability. (arXiv:1711.07414v1 [cs.AI])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07414"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07414", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c71a9b70\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c71a9b70&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Transparency, user trust, and human comprehension are popular ethical \nmotivations for interpretable machine learning. In support of these goals, \nresearchers evaluate model explanation performance using humans and real world \napplications. This alone presents a challenge in many areas of artificial \nintelligence. In this position paper, we propose a distinction between \ndescriptive and persuasive explanations. We discuss reasoning suggesting that \nfunctional interpretability may be correlated with cognitive function and user \npreferences. If this is indeed the case, evaluation and optimization using \nfunctional metrics could perpetuate implicit cognitive bias in explanations \nthat threaten transparency. Finally, we propose two potential research \ndirections to disambiguate cognitive function and explanation models, retaining \ncontrol over the tradeoff between accuracy and interpretability. \n</p>"}, "author": "Bernease Herman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949138", "id": "tag:google.com,2005:reader/item/0000000337308aad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A generalised framework for detailed classification of swimming paths inside the Morris Water Maze. (arXiv:1711.07446v1 [q-bio.QM])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07446"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07446", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7253afa\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7253afa&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The Morris Water Maze is commonly used in behavioural neuroscience for the \nstudy of spatial learning with rodents. Over the years, various methods of \nanalysing rodent data collected in this task have been proposed. These methods \nspan from classical performance measurements (e.g. escape latency, rodent \nspeed, quadrant preference) to more sophisticated methods of categorisation \nwhich classify the animal swimming path into behavioural classes known as \nstrategies. Classification techniques provide additional insight in relation to \nthe actual animal behaviours but still only a limited amount of studies utilise \nthem mainly because they highly depend on machine learning knowledge. We have \npreviously demonstrated that the animals implement various strategies and by \nclassifying whole trajectories can lead to the loss of important information. \nIn this work, we developed a generalised and robust classification methodology \nwhich implements majority voting to boost the classification performance and \nsuccessfully nullify the need of manual tuning. Based on this framework, we \nbuilt a complete software, capable of performing the full analysis described in \nthis paper. The software provides an easy to use graphical user interface (GUI) \nthrough which users can enter their trajectory data, segment and label them and \nfinally generate reports and figures of the results. \n</p>"}, "author": "Avgoustinos Vouros, Tiago V. Gehring, Kinga Szydlowska, Artur Janusz, Mike Croucher, Katarzyna Lukasiuk, Witold Konopka, Carmen Sandi, Eleni Vasilaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511265635949", "timestampUsec": "1511265635949137", "id": "tag:google.com,2005:reader/item/0000000337308ab1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SquishedNets: Squishing SqueezeNet further for edge device scenarios via deep evolutionary synthesis. (arXiv:1711.07459v1 [cs.NE])", "published": 1511265636, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07459"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07459", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>While deep neural networks have been shown in recent years to outperform \nother machine learning methods in a wide range of applications, one of the \nbiggest challenges with enabling deep neural networks for widespread deployment \non edge devices such as mobile and other consumer devices is high computational \nand memory requirements. Recently, there has been greater exploration into \nsmall deep neural network architectures that are more suitable for edge \ndevices, with one of the most popular architectures being SqueezeNet, with an \nincredibly small model size of 4.8MB. Taking further advantage of the notion \nthat many applications of machine learning on edge devices are often \ncharacterized by a low number of target classes, this study explores the \nutility of combining architectural modifications and an evolutionary synthesis \nstrategy for synthesizing even smaller deep neural architectures based on the \nmore recent SqueezeNet v1.1 macroarchitecture for applications with fewer \ntarget classes. In particular, architectural modifications are first made to \nSqueezeNet v1.1 to accommodate for a 10-class ImageNet-10 dataset, and then an \nevolutionary synthesis strategy is leveraged to synthesize more efficient deep \nneural networks based on this modified macroarchitecture. The resulting \nSquishedNets possess model sizes ranging from 2.4MB to 0.95MB (~5.17X smaller \nthan SqueezeNet v1.1, or 253X smaller than AlexNet). Furthermore, the \nSquishedNets are still able to achieve accuracies ranging from 81.2% to 77%, \nand able to process at speeds of 156 images/sec to as much as 256 images/sec on \na Nvidia Jetson TX1 embedded chip. These preliminary results show that a \ncombination of architectural modifications and an evolutionary synthesis \nstrategy can be a useful tool for producing very small deep neural network \narchitectures that are well-suited for edge device scenarios. \n</p>"}, "author": "Mohammad Javad Shafiee, Francis Li, Brendan Chwyl, Alexander Wong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302204", "id": "tag:google.com,2005:reader/item/000000033723049c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Discriminative Affine Regions via Discriminability. (arXiv:1711.06704v1 [cs.CV])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06704"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06704", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present an accurate method for estimation of the affine shape of local \nfeatures. The method is trained in a novel way, exploiting the recently \nproposed HardNet triplet loss. The loss function is driven by patch descriptor \ndifferences, avoiding problems with symmetries. Moreover, such training process \ndoes not require precisely geometrically aligned patches. The affine shape is \nrepresented in a way amenable to learning by stochastic gradient descent. \n</p> \n<p>When plugged into a state-of-the-art wide baseline matching algorithm, the \nperformance on standard datasets improves in both the number of challenging \npairs matched and the number of inliers. Finally, AffNet with combination of \nHessian detector and HardNet descriptor improves bag-of-visual-words based \nstate of the art on Oxford5k and Paris6k by large margin, 4.5 and 4.2 mAP \npoints respectively. \n</p> \n<p>The source code and trained networks are available at \nhttps://github.com/ducha-aiki/affnet \n</p>"}, "author": "Dmytro Mishkin, Filip Radenovic, Jiri Matas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302203", "id": "tag:google.com,2005:reader/item/00000003372304aa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Deep supervised learning using local errors. (arXiv:1711.06756v1 [cs.NE])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06756"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06756", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Error backpropagation is a highly effective mechanism for learning \nhigh-quality hierarchical features in deep networks. Updating the features or \nweights in one layer, however, requires waiting for the propagation of error \nsignals from higher layers. Learning using delayed and non-local errors makes \nit hard to reconcile backpropagation with the learning mechanisms observed in \nbiological neural networks as it requires the neurons to maintain a memory of \nthe input long enough until the higher-layer errors arrive. In this paper, we \npropose an alternative learning mechanism where errors are generated locally in \neach layer using fixed, random auxiliary classifiers. Lower layers could thus \nbe trained independently of higher layers and training could either proceed \nlayer by layer, or simultaneously in all layers using local error information. \nWe address biological plausibility concerns such as weight symmetry \nrequirements and show that the proposed learning mechanism based on fixed, \nbroad, and random tuning of each neuron to the classification categories \noutperforms the biologically-motivated feedback alignment learning technique on \nthe MNIST, CIFAR10, and SVHN datasets, approaching the performance of standard \nbackpropagation. Our approach highlights a potential biological mechanism for \nthe supervised, or task-dependent, learning of feature hierarchies. In \naddition, we show that it is well suited for learning deep networks in custom \nhardware where it can drastically reduce memory traffic and data communication \noverheads. \n</p>"}, "author": "Hesham Mostafa, Vishwajith Ramesh, Gert Cauwenberghs", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302202", "id": "tag:google.com,2005:reader/item/00000003372304ba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Generation and Consolidation of Recollections for Efficient Deep Lifelong Learning. (arXiv:1711.06761v1 [cs.LG])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06761"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06761", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep lifelong learning systems need to efficiently manage resources to scale \nto large numbers of experiences and non-stationary goals. In this paper, we \nexplore the relationship between lossy compression and the resource constrained \nlifelong learning problem of function transferability. We demonstrate that \nlossy episodic experience storage can enable efficient function transferability \nbetween different architectures and algorithms at a fraction of the storage \ncost of lossless storage. This is achieved by introducing a generative \nknowledge distillation strategy that does not store any full training examples. \nAs an important extension of this idea, we show that lossy recollections \nstabilize deep networks much better than lossless sampling in resource \nconstrained settings of lifelong learning while avoiding catastrophic \nforgetting. For this setting, we propose a novel dual purpose recollection \nbuffer used to both stabilize the recollection generator itself and an \naccompanying reasoning model. \n</p>"}, "author": "Matthew Riemer, Michele Franceschini, Tim Klinger", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302201", "id": "tag:google.com,2005:reader/item/00000003372304c3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Addressing Expensive Multi-objective Games with Postponed Preference Articulation via Memetic Co-evolution. (arXiv:1711.06763v1 [cs.NE])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06763"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06763", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents algorithmic and empirical contributions demonstrating \nthat the convergence characteristics of a co-evolutionary approach to tackle \nMulti-Objective Games (MOGs) with postponed preference articulation can often \nbe hampered due to the possible emergence of the so-called Red Queen effect. \nAccordingly, it is hypothesized that the convergence characteristics can be \nsignificantly improved through the incorporation of memetics (local solution \nrefinements as a form of lifelong learning), as a promising means of mitigating \n(or at least suppressing) the Red Queen phenomenon by providing a guiding hand \nto the purely genetic mechanisms of co-evolution. Our practical motivation is \nto address MOGs of a time-sensitive nature that are characterized by \ncomputationally expensive evaluations, wherein there is a natural need to \nreduce the total number of true function evaluations consumed in achieving good \nquality solutions. To this end, we propose novel enhancements to \nco-evolutionary approaches for tackling MOGs, such that memetic local \nrefinements can be efficiently applied on evolved candidate strategies by \nsearching on computationally cheap surrogate payoff landscapes (that preserve \npostponed preference conditions). The efficacy of the proposal is demonstrated \non a suite of test MOGs that have been designed. \n</p>"}, "author": "Adam &#x17b;ychowski, Abhishek Gupta, Jacek Ma&#x144;dziuk, Yew Soon Ong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302200", "id": "tag:google.com,2005:reader/item/00000003372304ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Image Registration of Very Large Images via Genetic Programming. (arXiv:1711.06764v1 [cs.CV])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06764"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06764", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Image registration (IR) is a fundamental task in image processing for \nmatching two or more images of the same scene taken at different times, from \ndifferent viewpoints and/or by different sensors. Due to the enormous diversity \nof IR applications, automatic IR remains a challenging problem to this day. A \nwide range of techniques has been developed for various data types and \nproblems. However, they might not handle effectively very large images, which \ngive rise usually to more complex transformations, e.g., deformations and \nvarious other distortions. \n</p> \n<p>In this paper we present a genetic programming (GP)-based approach for IR, \nwhich could offer a significant advantage in dealing with very large images, as \nit does not make any prior assumptions about the transformation model. Thus, by \nincorporating certain generic building blocks into the proposed GP framework, \nwe hope to realize a large set of specialized transformations that should yield \naccurate registration of very large images. \n</p>"}, "author": "Sarit Chicotay, Eli (Omid) David, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302199", "id": "tag:google.com,2005:reader/item/00000003372304de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Two-Phase Genetic Algorithm for Image Registration. (arXiv:1711.06765v1 [cs.CV])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06765"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06765", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Image Registration (IR) is the process of aligning two (or more) images of \nthe same scene taken at different times, different viewpoints and/or by \ndifferent sensors. It is an important, crucial step in various image analysis \ntasks where multiple data sources are integrated/fused, in order to extract \nhigh-level information. \n</p> \n<p>Registration methods usually assume a relevant transformation model for a \ngiven problem domain. The goal is to search for the \"optimal\" instance of the \ntransformation model assumed with respect to a similarity measure in question. \n</p> \n<p>In this paper we present a novel genetic algorithm (GA)-based approach for \nIR. Since GA performs effective search in various optimization problems, it \ncould prove useful also for IR. Indeed, various GAs have been proposed for IR. \nHowever, most of them assume certain constraints, which simplify the \ntransformation model, restrict the search space or make additional \npreprocessing requirements. In contrast, we present a generalized GA-based \nsolution for an almost fully affine transformation model, which achieves \ncompetitive results without such limitations using a two-phase method and a \nmulti-objective optimization (MOO) approach. \n</p> \n<p>We present good results for multiple dataset and demonstrate the robustness \nof our method in the presence of noisy data. \n</p>"}, "author": "Sarit Chicotay, Eli David, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302198", "id": "tag:google.com,2005:reader/item/00000003372304e8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Genetic Algorithm-Based Solver for Very Large Multiple Jigsaw Puzzles of Unknown Dimensions and Piece Orientation. (arXiv:1711.06766v1 [cs.CV])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06766"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06766", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we propose the first genetic algorithm (GA)-based solver for \njigsaw puzzles of unknown puzzle dimensions and unknown piece location and \norientation. Our solver uses a novel crossover technique, and sets a new \nstate-of-the-art in terms of the puzzle sizes solved and the accuracy obtained. \nThe results are significantly improved, even when compared to previous solvers \nassuming known puzzle dimensions. Moreover, the solver successfully contends \nwith a mixed bag of multiple puzzle pieces, assembling simultaneously all \npuzzles. \n</p>"}, "author": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302197", "id": "tag:google.com,2005:reader/item/00000003372304ef", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "An Automatic Solver for Very Large Jigsaw Puzzles Using Genetic Algorithms. (arXiv:1711.06767v1 [cs.CV])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06767"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06767", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7253d9c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7253d9c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper we propose the first effective genetic algorithm (GA)-based \njigsaw puzzle solver. We introduce a novel crossover procedure that merges two \n\"parent\" solutions to an improved \"child\" configuration by detecting, \nextracting, and combining correctly assembled puzzle segments. The solver \nproposed exhibits state-of-the-art performance, as far as handling previously \nattempted puzzles more accurately and efficiently, as well puzzle sizes that \nhave not been attempted before. The extended experimental results provided in \nthis paper include, among others, a thorough inspection of up to 30,745-piece \npuzzles (compared to previous attempts on 22,755-piece puzzles), using a \nconsiderably faster concurrent implementation of the algorithm. Furthermore, we \nexplore the impact of different phases of the novel crossover operator by \nexperimenting with several variants of the GA. Finally, we compare different \nfitness functions and their effect on the overall results of the GA-based \nsolver. \n</p>"}, "author": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302196", "id": "tag:google.com,2005:reader/item/00000003372304f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "A Generalized Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles of Complex Types. (arXiv:1711.06768v1 [cs.CV])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06768"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06768", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we introduce new types of square-piece jigsaw puzzles, where in \naddition to the unknown location and orientation of each piece, a piece might \nalso need to be flipped. These puzzles, which are associated with a number of \nreal world problems, are considerably harder, from a computational standpoint. \nSpecifically, we present a novel generalized genetic algorithm (GA)-based \nsolver that can handle puzzle pieces of unknown location and orientation (Type \n2 puzzles) and (two-sided) puzzle pieces of unknown location, orientation, and \nface (Type 4 puzzles). To the best of our knowledge, our solver provides a new \nstate-of-the-art, solving previously attempted puzzles faster and far more \naccurately, handling puzzle sizes that have never been attempted before, and \nassembling the newly introduced two-sided puzzles automatically and \neffectively. This paper also presents, among other results, the most extensive \nset of experimental results, compiled as of yet, on Type 2 puzzles. \n</p>"}, "author": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302195", "id": "tag:google.com,2005:reader/item/00000003372304fb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles. (arXiv:1711.06769v1 [cs.CV])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06769"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06769", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we propose the first effective automated, genetic algorithm \n(GA)-based jigsaw puzzle solver. We introduce a novel procedure of merging two \n\"parent\" solutions to an improved \"child\" solution by detecting, extracting, \nand combining correctly assembled puzzle segments. The solver proposed exhibits \nstate-of-the-art performance solving previously attempted puzzles faster and \nfar more accurately, and also puzzles of size never before attempted. Other \ncontributions include the creation of a benchmark of large images, previously \nunavailable. We share the data sets and all of our results for future testing \nand comparative evaluation of jigsaw puzzle solvers. \n</p>"}, "author": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302194", "id": "tag:google.com,2005:reader/item/000000033723050c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neural Network Reinforcement Learning for Audio-Visual Gaze Control in Human-Robot Interaction. (arXiv:1711.06834v1 [cs.RO])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06834"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06834", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper introduces a novel neural network-based reinforcement learning \napproach for robot gaze control. Our approach enables a robot to learn and \nadapt its gaze control strategy for human-robot interaction without the use of \nexternal sensors or human supervision. The robot learns to focus its attention \non groups of people from its own audio-visual experiences, and independently of \nthe number of people in the environment, their position and physical \nappearance. In particular, we use recurrent neural networks and Q-learning to \nfind an optimal action-selection policy, and we pretrain on a synthetic \nenvironment that simulates sound sources and moving participants to avoid the \nneed of interacting with people for hours. Our experimental evaluation suggests \nthat the proposed method is robust in terms of parameters configuration (i.e. \nthe selection of the parameter values has not a decisive impact on the \nperformance). The best results are obtained when audio and video information \nare jointly used, and when a late fusion strategy is employed (i.e. when both \nsources of information are separately processed and then fused). Successful \nexperiments on a real environment with the Nao robot indicate that our \nframework is a step forward towards the autonomous learning of a perceivable \nand socially acceptable gaze behavior. \n</p>"}, "author": "St&#xe9;phane Lathuili&#xe8;re, Benoit Mass&#xe9;, Pablo Mesejo, Radu Horaud", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302193", "id": "tag:google.com,2005:reader/item/0000000337230514", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Genetic Algorithms for Mentor-Assisted Evaluation Function Optimization. (arXiv:1711.06839v1 [cs.NE])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06839"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06839", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we demonstrate how genetic algorithms can be used to reverse \nengineer an evaluation function's parameters for computer chess. Our results \nshow that using an appropriate mentor, we can evolve a program that is on par \nwith top tournament-playing chess programs, outperforming a two-time World \nComputer Chess Champion. This performance gain is achieved by evolving a \nprogram with a smaller number of parameters in its evaluation function to mimic \nthe behavior of a superior mentor which uses a more extensive evaluation \nfunction. In principle, our mentor-assisted approach could be used in a wide \nrange of problems for which appropriate mentors are available. \n</p>"}, "author": "Eli David, Moshe Koppel, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302192", "id": "tag:google.com,2005:reader/item/0000000337230524", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Simulating Human Grandmasters: Evolution and Coevolution of Evaluation Functions. (arXiv:1711.06840v1 [cs.NE])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06840"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06840", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper demonstrates the use of genetic algorithms for evolving a \ngrandmaster-level evaluation function for a chess program. This is achieved by \ncombining supervised and unsupervised learning. In the supervised learning \nphase the organisms are evolved to mimic the behavior of human grandmasters, \nand in the unsupervised learning phase these evolved organisms are further \nimproved upon by means of coevolution. \n</p> \n<p>While past attempts succeeded in creating a grandmaster-level program by \nmimicking the behavior of existing computer chess programs, this paper presents \nthe first successful attempt at evolving a state-of-the-art evaluation function \nby learning only from databases of games played by humans. Our results \ndemonstrate that the evolved program outperforms a two-time World Computer \nChess Champion. \n</p>"}, "author": "Eli David, H. Jaap van den Herik, Moshe Koppel, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302191", "id": "tag:google.com,2005:reader/item/000000033723052f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Expert-Driven Genetic Algorithms for Simulating Evaluation Functions. (arXiv:1711.06841v1 [cs.NE])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06841"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06841", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we demonstrate how genetic algorithms can be used to reverse \nengineer an evaluation function's parameters for computer chess. Our results \nshow that using an appropriate expert (or mentor), we can evolve a program that \nis on par with top tournament-playing chess programs, outperforming a two-time \nWorld Computer Chess Champion. This performance gain is achieved by evolving a \nprogram that mimics the behavior of a superior expert. The resulting evaluation \nfunction of the evolved program consists of a much smaller number of parameters \nthan the expert's. The extended experimental results provided in this paper \ninclude a report of our successful participation in the 2008 World Computer \nChess Championship. In principle, our expert-driven approach could be used in a \nwide range of problems for which appropriate experts are available. \n</p>"}, "author": "Eli David, Moshe Koppel, Nathan S. Netanyahu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302190", "id": "tag:google.com,2005:reader/item/0000000337230539", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hello Edge: Keyword Spotting on Microcontrollers. (arXiv:1711.07128v1 [cs.SD])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07128"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07128", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Keyword spotting (KWS) is a critical component for enabling speech based user \ninteractions on smart devices. It requires real-time response and high accuracy \nfor good user experience. Recently, neural networks have become an attractive \nchoice for KWS architecture because of their superior accuracy compared to \ntraditional speech processing algorithms. Due to its always-on nature, KWS \napplication has highly constrained power budget and typically runs on tiny \nmicrocontrollers with limited memory and compute capability. The design of \nneural network architecture for KWS must consider these constraints. In this \nwork, we perform neural network architecture evaluation and exploration for \nrunning KWS on resource-constrained microcontrollers. We train various neural \nnetwork architectures for keyword spotting published in literature to compare \ntheir accuracy and memory/compute requirements. We show that it is possible to \noptimize these neural network architectures to fit within the memory and \ncompute constraints of microcontrollers without sacrificing accuracy. We \nfurther explore the depthwise separable convolutional neural network (DS-CNN) \nand compare it against other neural network architectures. DS-CNN achieves an \naccuracy of 95.4%, which is ~10% higher than the DNN model with similar number \nof parameters. \n</p>"}, "author": "Yundong Zhang, Naveen Suda, Liangzhen Lai, Vikas Chandra", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302189", "id": "tag:google.com,2005:reader/item/0000000337230552", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Maximizing Non-monotone/Non-submodular Functions by Multi-objective Evolutionary Algorithms. (arXiv:1711.07214v1 [cs.NE])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07214"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07214", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Evolutionary algorithms (EAs) are a kind of nature-inspired general-purpose \noptimization algorithm, and have shown empirically good performance in solving \nvarious real-word optimization problems. However, due to the highly randomized \nand complex behavior, the theoretical analysis of EAs is difficult and is an \nongoing challenge, which has attracted a lot of research attentions. During the \nlast two decades, promising results on the running time analysis (one essential \ntheoretical aspect) of EAs have been obtained, while most of them focused on \nisolated combinatorial optimization problems, which do not reflect the \ngeneral-purpose nature of EAs. To provide a general theoretical explanation of \nthe behavior of EAs, it is desirable to study the performance of EAs on a \ngeneral class of combinatorial optimization problems. To the best of our \nknowledge, this direction has been rarely touched and the only known result is \nthe provably good approximation guarantees of EAs for the problem class of \nmaximizing monotone submodular set functions with matroid constraints, which \nincludes many NP-hard combinatorial optimization problems. The aim of this work \nis to contribute to this line of research. As many combinatorial optimization \nproblems also involve non-monotone or non-submodular objective functions, we \nconsider these two general problem classes, maximizing non-monotone submodular \nfunctions without constraints and maximizing monotone non-submodular functions \nwith a size constraint. We prove that a simple multi-objective EA called GSEMO \ncan generally achieve good approximation guarantees in polynomial expected \nrunning time. \n</p>"}, "author": "Chao Qian, Yang Yu, Ke Tang, Xin Yao, Zhi-Hua Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302188", "id": "tag:google.com,2005:reader/item/0000000337230567", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Community detection with spiking neural networks for neuromorphic hardware. (arXiv:1711.07361v1 [cs.NE])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07361"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07361", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present results related to the performance of an algorithm for community \ndetection which incorporates event-driven computation. We define a mapping \nwhich takes a graph G to a system of spiking neurons. Using a fully connected \nspiking neuron system, with both inhibitory and excitatory synaptic \nconnections, the firing patterns of neurons within the same community can be \ndistinguished from firing patterns of neurons in different communities. On a \nrandom graph with 128 vertices and known community structure we show that by \nusing binary decoding and a Hamming-distance based metric, individual \ncommunities can be identified from spike train similarities. Using bipolar \ndecoding and finite rate thresholding, we verify that inhibitory connections \nprevent the spread of spiking patterns. \n</p>"}, "author": "Kathleen E. Hamilton, Neena Imam, Travis S. Humble", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511260348302", "timestampUsec": "1511260348302187", "id": "tag:google.com,2005:reader/item/0000000337230576", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SquishedNets: Squishing SqueezeNet further for edge device scenarios via deep evolutionary synthesis. (arXiv:1711.07459v1 [cs.NE])", "published": 1511260348, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.07459"}], "alternate": [{"href": "http://arxiv.org/abs/1711.07459", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7253ff2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7253ff2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>While deep neural networks have been shown in recent years to outperform \nother machine learning methods in a wide range of applications, one of the \nbiggest challenges with enabling deep neural networks for widespread deployment \non edge devices such as mobile and other consumer devices is high computational \nand memory requirements. Recently, there has been greater exploration into \nsmall deep neural network architectures that are more suitable for edge \ndevices, with one of the most popular architectures being SqueezeNet, with an \nincredibly small model size of 4.8MB. Taking further advantage of the notion \nthat many applications of machine learning on edge devices are often \ncharacterized by a low number of target classes, this study explores the \nutility of combining architectural modifications and an evolutionary synthesis \nstrategy for synthesizing even smaller deep neural architectures based on the \nmore recent SqueezeNet v1.1 macroarchitecture for applications with fewer \ntarget classes. In particular, architectural modifications are first made to \nSqueezeNet v1.1 to accommodate for a 10-class ImageNet-10 dataset, and then an \nevolutionary synthesis strategy is leveraged to synthesize more efficient deep \nneural networks based on this modified macroarchitecture. The resulting \nSquishedNets possess model sizes ranging from 2.4MB to 0.95MB (~5.17X smaller \nthan SqueezeNet v1.1, or 253X smaller than AlexNet). Furthermore, the \nSquishedNets are still able to achieve accuracies ranging from 81.2% to 77%, \nand able to process at speeds of 156 images/sec to as much as 256 images/sec on \na Nvidia Jetson TX1 embedded chip. These preliminary results show that a \ncombination of architectural modifications and an evolutionary synthesis \nstrategy can be a useful tool for producing very small deep neural network \narchitectures that are well-suited for edge device scenarios. \n</p>"}, "author": "Mohammad Javad Shafiee, Francis Li, Brendan Chwyl, Alexander Wong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010057", "id": "tag:google.com,2005:reader/item/0000000336d072f8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Poverty Mapping Using Convolutional Neural Networks Trained on High and Medium Resolution Satellite Images, With an Application in Mexico. (arXiv:1711.06323v1 [stat.ML])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06323"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06323", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c73168f7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c73168f7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Mapping the spatial distribution of poverty in developing countries remains \nan important and costly challenge. These \"poverty maps\" are key inputs for \npoverty targeting, public goods provision, political accountability, and impact \nevaluation, that are all the more important given the geographic dispersion of \nthe remaining bottom billion severely poor individuals. In this paper we train \nConvolutional Neural Networks (CNNs) to estimate poverty directly from high and \nmedium resolution satellite images. We use both Planet and Digital Globe \nimagery with spatial resolutions of 3-5 sq. m. and 50 sq. cm. respectively, \ncovering all 2 million sq. km. of Mexico. Benchmark poverty estimates come from \nthe 2014 MCS-ENIGH combined with the 2015 Intercensus and are used to estimate \npoverty rates for 2,456 Mexican municipalities. CNNs are trained using the 896 \nmunicipalities in the 2014 MCS-ENIGH. We experiment with several architectures \n(GoogleNet, VGG) and use GoogleNet as a final architecture where weights are \nfine-tuned from ImageNet. We find that 1) the best models, which incorporate \nsatellite-estimated land use as a predictor, explain approximately 57% of the \nvariation in poverty in a validation sample of 10 percent of MCS-ENIGH \nmunicipalities; 2) Across all MCS-ENIGH municipalities explanatory power \nreduces to 44% in a CNN prediction and landcover model; 3) Predicted poverty \nfrom the CNN predictions alone explains 47% of the variation in poverty in the \nvalidation sample, and 37% over all MCS-ENIGH municipalities; 4) In urban areas \nwe see slight improvements from using Digital Globe versus Planet imagery, \nwhich explain 61% and 54% of poverty variation respectively. We conclude that \nCNNs can be trained end-to-end on satellite imagery to estimate poverty, \nalthough there is much work to be done to understand how the training process \ninfluences out of sample validation. \n</p>"}, "author": "Boris Babenko (1), Jonathan Hersh (2), David Newhouse (3), Anusha Ramakrishnan (3), Tom Swartz (1) ((1) Orbital Insight, (2) Chapman University, (3) World Bank)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010056", "id": "tag:google.com,2005:reader/item/0000000336d0730d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Mosquito detection with low-cost smartphones: data acquisition for malaria research. (arXiv:1711.06346v1 [stat.ML])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06346"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06346", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Mosquitoes are a major vector for malaria, causing hundreds of thousands of \ndeaths in the developing world each year. Not only is the prevention of \nmosquito bites of paramount importance to the reduction of malaria transmission \ncases, but understanding in more forensic detail the interplay between malaria, \nmosquito vectors, vegetation, standing water and human populations is crucial \nto the deployment of more effective interventions. Typically the presence and \ndetection of malaria-vectoring mosquitoes is only quantified by hand-operated \ninsect traps or signified by the diagnosis of malaria. If we are to gather \ntimely, large-scale data to improve this situation, we need to automate the \nprocess of mosquito detection and classification as much as possible. In this \npaper, we present a candidate mobile sensing system that acts as both a \nportable early warning device and an automatic acoustic data acquisition \npipeline to help fuel scientific inquiry and policy. The machine learning \nalgorithm that powers the mobile system achieves excellent off-line \nmulti-species detection performance while remaining computationally efficient. \nFurther, we have conducted preliminary live mosquito detection tests using \nlow-cost mobile phones and achieved promising results. The deployment of this \nsystem for field usage in Southeast Asia and Africa is planned in the near \nfuture. In order to accelerate processing of field recordings and labelling of \ncollected data, we employ a citizen science platform in conjunction with \nautomated methods, the former implemented using the Zooniverse platform, \nallowing crowdsourcing on a grand scale. \n</p>"}, "author": "Yunpeng Li, Davide Zilli, Henry Chan, Ivan Kiskin, Marianne Sinka, Stephen Roberts", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010055", "id": "tag:google.com,2005:reader/item/0000000336d0731e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Deep Learning Models for Psychological State Prediction using Smartphone Data: Challenges and Opportunities. (arXiv:1711.06350v1 [cs.LG])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06350"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06350", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>There is an increasing interest in exploiting mobile sensing technologies and \nmachine learning techniques for mental health monitoring and intervention. \nResearchers have effectively used contextual information, such as mobility, \ncommunication and mobile phone usage patterns for quantifying individuals' mood \nand wellbeing. In this paper, we investigate the effectiveness of neural \nnetwork models for predicting users' level of stress by using the location \ninformation collected by smartphones. We characterize the mobility patterns of \nindividuals using the GPS metrics presented in the literature and employ these \nmetrics as input to the network. We evaluate our approach on the open-source \nStudentLife dataset. Moreover, we discuss the challenges and trade-offs \ninvolved in building machine learning models for digital mental health and \nhighlight potential future work in this direction. \n</p>"}, "author": "Gatis Mikelsons, Matthew Smith, Abhinav Mehrotra, Mirco Musolesi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010054", "id": "tag:google.com,2005:reader/item/0000000336d07335", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Thoracic Disease Identification and Localization with Limited Supervision. (arXiv:1711.06373v1 [cs.CV])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06373"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06373", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Accurate identification and localization of abnormalities from radiology \nimages play an integral part in clinical diagnosis and treatment planning. \nBuilding a highly accurate prediction model for these tasks usually requires a \nlarge number of images manually annotated with labels and finding sites of \nabnormalities. In reality, however, such annotated data are expensive to \nacquire, especially the ones with location annotations. We need methods that \ncan work well with only a small amount of location annotations. To address this \nchallenge, we present a unified approach that simultaneously performs disease \nidentification and localization through the same underlying model for all \nimages. We demonstrate that our approach can effectively leverage both class \ninformation as well as limited location annotation, and significantly \noutperforms the comparative reference baseline in both classification and \nlocalization tasks. \n</p>"}, "author": "Zhe Li, Chong Wang, Mei Han, Yuan Xue, Wei Wei, Li-Jia Li, Fei-Fei Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010053", "id": "tag:google.com,2005:reader/item/0000000336d0735f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improving Palliative Care with Deep Learning. (arXiv:1711.06402v1 [cs.CY])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06402"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06402", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Improving the quality of end-of-life care for hospitalized patients is a \npriority for healthcare organizations. Studies have shown that physicians tend \nto over-estimate prognoses, which in combination with treatment inertia results \nin a mismatch between patients wishes and actual care at the end of life. We \ndescribe a method to address this problem using Deep Learning and Electronic \nHealth Record (EHR) data, which is currently being piloted, with Institutional \nReview Board approval, at an academic medical center. The EHR data of admitted \npatients are automatically evaluated by an algorithm, which brings patients who \nare likely to benefit from palliative care services to the attention of the \nPalliative Care team. The algorithm is a Deep Neural Network trained on the EHR \ndata from previous years, to predict all-cause 3-12 month mortality of patients \nas a proxy for patients that could benefit from palliative care. Our \npredictions enable the Palliative Care team to take a proactive approach in \nreaching out to such patients, rather than relying on referrals from treating \nphysicians, or conduct time consuming chart reviews of all patients. We also \npresent a novel interpretation technique which we use to provide explanations \nof the model's predictions. \n</p>"}, "author": "Anand Avati, Kenneth Jung, Stephanie Harman, Lance Downing, Andrew Ng, Nigam H. Shah", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010052", "id": "tag:google.com,2005:reader/item/0000000336d07381", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Ubenwa: Cry-based Diagnosis of Birth Asphyxia. (arXiv:1711.06405v1 [stat.ML])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06405"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06405", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Every year, 3 million newborns die within the first month of life. Birth \nasphyxia and other breathing-related conditions are a leading cause of \nmortality during the neonatal phase. Current diagnostic methods are too \nsophisticated in terms of equipment, required expertise, and general logistics. \nConsequently, early detection of asphyxia in newborns is very difficult in many \nparts of the world, especially in resource-poor settings. We are developing a \nmachine learning system, dubbed Ubenwa, which enables diagnosis of asphyxia \nthrough automated analysis of the infant cry. Deployed via smartphone and \nwearable technology, Ubenwa will drastically reduce the time, cost and skill \nrequired to make accurate and potentially life-saving diagnoses. \n</p>"}, "author": "Charles C Onu, Innocent Udeogu, Eyenimi Ndiomu, Urbain Kengni, Doina Precup, Guilherme M Sant&#x27;anna, Edward Alikor, Peace Opara", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010051", "id": "tag:google.com,2005:reader/item/0000000336d073a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Resizable Mini-batch Gradient Descent based on a Randomized Weighted Majority. (arXiv:1711.06424v1 [stat.ML])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06424"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06424", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Determining the appropriate batch size for mini-batch gradient descent is \nalways time consuming as it often relies on grid search. This paper considers a \nresizable mini-batch gradient descent (RMGD) algorithm-inspired by the \nrandomized weighted majority algorithm-for achieving best performance in grid \nsearch by selecting an appropriate batch size at each epoch with a probability \ndefined as a function of its previous success/failure and the validation error. \nThis probability encourages exploration of different batch size and then later \nexploitation batch size with history of success. At each epoch, the RMGD \nsamples a batch size from its probability distribution, then uses the selected \nbatch size for mini-batch gradient descent. After obtaining the validation \nerror at each epoch, the probability distribution is updated to incorporate the \neffectiveness of the sampled batch size. The RMGD essentially assists the \nlearning process to explore the possible domain of the batch size and exploit \nsuccessful batch size. Experimental results show that the RMGD achieves \nperformance better than the best performing single batch size. Furthermore, it \nattains this performance in a shorter amount of time than that of the best \nperforming. It is surprising that the RMGD achieves better performance than \ngrid search. \n</p>"}, "author": "Seong Jin Cho, Sunghun Kang, Chang D. Yoo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010050", "id": "tag:google.com,2005:reader/item/0000000336d073d1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-Objective Maximization of Monotone Submodular Functions with Cardinality Constraint. (arXiv:1711.06428v1 [cs.DS])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06428"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06428", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of multi-objective maximization of monotone \nsubmodular functions subject to cardinality constraint, one formulation of \nwhich is $\\max_{|A|=k}\\min_{i\\in\\{1,\\dots,m\\}}f_i(A)$. Krause et al. (2008) \nshowed that when the number of functions $m$ grows as the cardinality $k$ i.e., \n$m=\\Omega(k)$, the problem is inapproximable (unless $P=NP$). For the more \ngeneral case of matroid constraint, Chekuri et al. (2010) gave a randomized \n$(1-1/e)-\\epsilon$ approximation for constant $m$. The runtime (number of \nqueries to function oracle) scales exponentially as $n^{m/\\epsilon^3}$. We give \nthe first polynomial time asymptotically constant factor approximations for \n$m=o(\\frac{k}{\\log^3 k})$: $(i)$ A randomized $(1-1/e)$ algorithm based on \nChekuri et al. (2010). $(ii)$ A faster and more practical \n$\\tilde{O}(n/\\delta^3)$ time, randomized $(1-1/e)^2-\\delta$ approximation based \non Multiplicative-Weight-Updates. Finally, we characterize the variation in \noptimal solution value as a function of the cardinality $k$, leading to a \nderandomized approximation for constant $m$. \n</p>"}, "author": "Rajan Udwani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010049", "id": "tag:google.com,2005:reader/item/0000000336d07414", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Non-convex Ordinal Embedding with Stabilized Barzilai-Borwein Step Size. (arXiv:1711.06446v1 [stat.ML])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06446"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06446", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning representation from relative similarity comparisons, often called \nordinal embedding, gains rising attention in recent years. Most of the existing \nmethods are batch methods designed mainly based on the convex optimization, \nsay, the projected gradient descent method. However, they are generally \ntime-consuming due to that the singular value decomposition (SVD) is commonly \nadopted during the update, especially when the data size is very large. To \novercome this challenge, we propose a stochastic algorithm called SVRG-SBB, \nwhich has the following features: (a) SVD-free via dropping convexity, with \ngood scalability by the use of stochastic algorithm, i.e., stochastic variance \nreduced gradient (SVRG), and (b) adaptive step size choice via introducing a \nnew stabilized Barzilai-Borwein (SBB) method as the original version for convex \nproblems might fail for the considered stochastic \\textit{non-convex} \noptimization problem. Moreover, we show that the proposed algorithm converges \nto a stationary point at a rate $\\mathcal{O}(\\frac{1}{T})$ in our setting, \nwhere $T$ is the number of total iterations. Numerous simulations and \nreal-world data experiments are conducted to show the effectiveness of the \nproposed algorithm via comparing with the state-of-the-art methods, \nparticularly, much lower computational cost with good prediction performance. \n</p>"}, "author": "Ke Ma, Jinshan Zeng, Jiechao Xiong, Qianqian Xu, Xiaochun Cao, Wei Liu, Yuan Yao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010048", "id": "tag:google.com,2005:reader/item/0000000336d07427", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A unified deep artificial neural network approach to partial differential equations in complex geometries. (arXiv:1711.06464v1 [stat.ML])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06464"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06464", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7316b36\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7316b36&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We use deep feedforward artificial neural networks to approximate solutions \nof partial differential equations of advection and diffusion type in complex \ngeometries. We derive analytical expressions of the gradients of the cost \nfunction with respect to the network parameters, as well as the gradient of the \nnetwork itself with respect to the input, for arbitrarily deep networks. The \nmethod is based on an ansatz for the solution, which requires nothing but \nfeedforward neural networks, and an unconstrained gradient based optimization \nmethod such as gradient descent or quasi-Newton methods. \n</p> \n<p>We provide detailed examples on how to use deep feedforward neural networks \nas a basis for further work on deep neural network approximations to partial \ndifferential equations. We highlight the benefits of deep compared to shallow \nneural networks and other convergence enhancing techniques. \n</p>"}, "author": "Jens Berg, Kaj Nystr&#xf6;m", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010047", "id": "tag:google.com,2005:reader/item/0000000336d07448", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improved Bayesian Compression. (arXiv:1711.06494v1 [stat.ML])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06494"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06494", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Compression of Neural Networks (NN) has become a highly studied topic in \nrecent years. The main reason for this is the demand for industrial scale usage \nof NNs such as deploying them on mobile devices, storing them efficiently, \ntransmitting them via band-limited channels and most importantly doing \ninference at scale. In this work, we propose to join the Soft-Weight Sharing \nand Variational Dropout approaches that show strong results to define a new \nstate-of-the-art in terms of model compression. \n</p>"}, "author": "Marco Federici, Karen Ullrich, Max Welling", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010046", "id": "tag:google.com,2005:reader/item/0000000336d07483", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Detecting hip fractures with radiologist-level performance using deep neural networks. (arXiv:1711.06504v1 [cs.CV])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06504"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06504", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We developed an automated deep learning system to detect hip fractures from \nfrontal pelvic x-rays, an important and common radiological task. Our system \nwas trained on a decade of clinical x-rays (~53,000 studies) and can be applied \nto clinical data, automatically excluding inappropriate and technically \nunsatisfactory studies. We demonstrate diagnostic performance equivalent to a \nhuman radiologist and an area under the ROC curve of 0.994. Translated to \nclinical practice, such a system has the potential to increase the efficiency \nof diagnosis, reduce the need for expensive additional testing, expand access \nto expert level medical image interpretation, and improve overall patient \noutcomes. \n</p>"}, "author": "William Gale, Luke Oakden-Rayner, Gustavo Carneiro, Andrew P. Bradley, Lyle J. Palmer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010045", "id": "tag:google.com,2005:reader/item/0000000336d0749b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Introduction to intelligent computing unit 1. (arXiv:1711.06552v1 [cs.LG])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06552"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06552", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This brief note highlights some basic concepts required toward understanding \nthe evolution of machine learning and deep learning models. The note starts \nwith an overview of artificial intelligence and its relationship to biological \nneuron that ultimately led to the evolution of todays intelligent models. \n</p>"}, "author": "Isa Inuwa-Dutse", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010044", "id": "tag:google.com,2005:reader/item/0000000336d074e1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Calibration of Distributionally Robust Empirical Optimization Models. (arXiv:1711.06565v1 [stat.ML])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06565"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06565", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study the out-of-sample properties of robust empirical \noptimization and develop a theory for data-driven calibration of the robustness \nparameter for worst-case maximization problems with concave reward functions. \nBuilding on the intuition that robust optimization reduces the sensitivity of \nthe expected reward to errors in the model by controlling the spread of the \nreward distribution, we show that the first-order benefit of little bit of \nrobustness is a significant reduction in the variance of the out-of-sample \nreward while the corresponding impact on the mean is almost an order of \nmagnitude smaller. One implication is that a substantial reduction in the \nvariance of the out-of-sample reward (i.e. sensitivity of the expected reward \nto model misspecification) is possible at little cost if the robustness \nparameter is properly calibrated. To this end, we introduce the notion of a \nrobust mean-variance frontier to select the robustness parameter and show that \nit can be approximated using resampling methods like the bootstrap. Our \nexamples also show that open loop calibration methods (e.g. selecting a 90% \nconfidence level regardless of the data and objective function) can lead to \nsolutions that are very conservative out-of-sample. \n</p>"}, "author": "Jun-Ya Gotoh, Michael Jong Kim, Andrew E.B. Lim", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010043", "id": "tag:google.com,2005:reader/item/0000000336d074f4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to Play Othello with Deep Neural Networks. (arXiv:1711.06583v1 [cs.AI])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06583"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06583", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Achieving superhuman playing level by AlphaGo corroborated the capabilities \nof convolutional neural architectures (CNNs) for capturing complex spatial \npatterns. This result was to a great extent due to several analogies between Go \nboard states and 2D images CNNs have been designed for, in particular \ntranslational invariance and a relatively large board. In this paper, we verify \nwhether CNN-based move predictors prove effective for Othello, a game with \nsignificantly different characteristics, including a much smaller board size \nand complete lack of translational invariance. We compare several CNN \narchitectures and board encodings, augment them with state-of-the-art \nextensions, train on an extensive database of experts' moves, and examine them \nwith respect to move prediction accuracy and playing strength. The empirical \nevaluation confirms high capabilities of neural move predictors and suggests a \nstrong correlation between prediction accuracy and playing strength. The best \nCNNs not only surpass all other 1-ply Othello players proposed to date but \ndefeat (2-ply) Edax, the best open-source Othello player. \n</p>"}, "author": "Pawe&#x142; Liskowski, Wojciech Ja&#x15b;kowski, Krzysztof Krawiec", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010042", "id": "tag:google.com,2005:reader/item/0000000336d07502", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "How Wrong Am I? - Studying Adversarial Examples and their Impact on Uncertainty in Gaussian Process Machine Learning Models. (arXiv:1711.06598v1 [cs.CR])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06598"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06598", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning models are vulnerable to adversarial examples: minor, in \nmany cases imperceptible, perturbations to classification inputs. Among other \nsuspected causes, adversarial examples exploit ML models that offer no \nwell-defined indication as to how well a particular prediction is supported by \ntraining data, yet are forced to confidently extrapolate predictions in areas \nof high entropy. In contrast, Bayesian ML models, such as Gaussian Processes \n(GP), inherently model the uncertainty accompanying a prediction in the \nwell-studied framework of Bayesian Inference. This paper is first to explore \nadversarial examples and their impact on uncertainty estimates for Gaussian \nProcesses. To this end, we first present three novel attacks on Gaussian \nProcesses: GPJM and GPFGS exploit forward derivatives in GP latent functions, \nand Latent Space Approximation Networks mimic the latent space representation \nin unsupervised GP models to facilitate attacks. Further, we show that these \nnew attacks compute adversarial examples that transfer to non-GP classification \nmodels, and vice versa. Finally, we show that GP uncertainty estimates not only \ndiffer between adversarial examples and benign data, but also between \nadversarial examples computed by different algorithms. \n</p>"}, "author": "Kathrin Grosse, David Pfaff, Michael Thomas Smith, Michael Backes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010041", "id": "tag:google.com,2005:reader/item/0000000336d0751c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Nonparametric independence testing via mutual information. (arXiv:1711.06642v1 [stat.ME])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06642"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06642", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a test of independence of two multivariate random vectors, given a \nsample from the underlying population. Our approach, which we call MINT, is \nbased on the estimation of mutual information, whose decomposition into joint \nand marginal entropies facilitates the use of recently-developed efficient \nentropy estimators derived from nearest neighbour distances. The proposed \ncritical values, which may be obtained from simulation (in the case where one \nmarginal is known) or resampling, guarantee that the test has nominal size, and \nwe provide local power analyses, uniformly over classes of densities whose \nmutual information satisfies a lower bound. Our ideas may be extended to \nprovide a new goodness-of-fit tests of normal linear models based on assessing \nthe independence of our vector of covariates and an appropriately-defined \nnotion of an error vector. The theory is supported by numerical studies on both \nsimulated and real data. \n</p>"}, "author": "Thomas B. Berrett, Richard J. Samworth", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010040", "id": "tag:google.com,2005:reader/item/0000000336d07537", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Parallelizable Acceleration Framework for Packing Linear Programs. (arXiv:1711.06656v1 [math.OC])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06656"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06656", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents an acceleration framework for packing linear programming \nproblems where the amount of data available is limited, i.e., where the number \nof constraints m is small compared to the variable dimension n. The framework \ncan be used as a black box to speed up linear programming solvers dramatically, \nby two orders of magnitude in our experiments. We present worst-case guarantees \non the quality of the solution and the speedup provided by the algorithm, \nshowing that the framework provides an approximately optimal solution while \nrunning the original solver on a much smaller problem. The framework can be \nused to accelerate exact solvers, approximate solvers, and parallel/distributed \nsolvers. Further, it can be used for both linear programs and integer linear \nprograms. \n</p>"}, "author": "Palma London, Shai Vardi, Adam Wierman, Hanling Yi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010039", "id": "tag:google.com,2005:reader/item/0000000336d0754c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Predict Responsibly: Increasing Fairness by Learning To Defer. (arXiv:1711.06664v1 [stat.ML])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06664"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06664", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning systems, which are often used for high-stakes decisions, \nsuffer from two mutually reinforcing problems: unfairness and opaqueness. Many \npopular models, although generally accurate, cannot express uncertainty about \ntheir predictions. Even in regimes where a model is inaccurate, users may trust \nthe model's predictions too fully, and allow its biases to reinforce the user's \nown. \n</p> \n<p>In this work, we explore models that learn to defer. In our scheme, a model \nlearns to classify accurately and fairly, but also to defer if necessary, \npassing judgment to a downstream decision-maker such as a human user. We \nfurther propose a learning algorithm which accounts for potential biases held \nby decision-makers later in a pipeline. Experiments on real-world datasets \ndemonstrate that learning to defer can make a model not only more accurate but \nalso less biased. Even when operated by highly biased users, we show that \ndeferring models can still greatly improve the fairness of the entire pipeline. \n</p>"}, "author": "David Madras, Toniann Pitassi, Richard Zemel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010038", "id": "tag:google.com,2005:reader/item/0000000336d07562", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neon2: Finding Local Minima via First-Order Oracles. (arXiv:1711.06673v1 [cs.LG])", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06673"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06673", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7316d5c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7316d5c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a reduction for non-convex optimization that can (1) turn a \nstationary-point finding algorithm into a local-minimum finding one, and (2) \nreplace the Hessian-vector product computations with only gradient \ncomputations. It works both in the stochastic and the deterministic settings, \nwithout hurting the algorithm's performance. \n</p> \n<p>As applications, our reduction turns Natasha2 into a first-order method \nwithout hurting its performance. It also converts SGD, GD, SCSG, and SVRG into \nlocal-minimum finding algorithms outperforming some best known results. \n</p>"}, "author": "Zeyuan Allen-Zhu, Yuanzhi Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010036", "id": "tag:google.com,2005:reader/item/0000000336d07570", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Collaborative Information Bottleneck. (arXiv:1604.01433v2 [cs.IT] UPDATED)", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1604.01433"}], "alternate": [{"href": "http://arxiv.org/abs/1604.01433", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c73a7355\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c73a7355&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper investigates a multi-terminal source coding problem under a \nlogarithmic loss fidelity which does not necessarily lead to an additive \ndistortion measure. The problem is motivated by an extension of the Information \nBottleneck method to a multi-source scenario where several encoders have to \nbuild cooperatively rate-limited descriptions of their sources in order to \nmaximize information with respect to other unobserved (hidden) sources. More \nprecisely, we study fundamental information-theoretic limits of the so-called: \n(i) Two-way Collaborative Information Bottleneck (TW-CIB) and (ii) the \nCollaborative Distributed Information Bottleneck (CDIB) problems. The TW-CIB \nproblem consists of two distant encoders that separately observe marginal \n(dependent) components $X_1$ and $X_2$ and can cooperate through multiple \nexchanges of limited information with the aim of extracting information about \nhidden variables $(Y_1,Y_2)$, which can be arbitrarily dependent on \n$(X_1,X_2)$. On the other hand, in CDIB there are two cooperating encoders \nwhich separately observe $X_1$ and $X_2$ and a third node which can listen to \nthe exchanges between the two encoders in order to obtain information about a \nhidden variable $Y$. The relevance (figure-of-merit) is measured in terms of a \nnormalized (per-sample) multi-letter mutual information metric (log-loss \nfidelity) and an interesting tradeoff arises by constraining the complexity of \ndescriptions, measured in terms of the rates needed for the exchanges between \nthe encoders and decoders involved. Inner and outer bounds to the \ncomplexity-relevance region of these problems are derived from which optimality \nis characterized for several cases of interest. Our resulting theoretical \ncomplexity-relevance regions are finally evaluated for binary symmetric and \nGaussian statistical models. \n</p>"}, "author": "Mat&#xed;as Vera, Leonardo Rey Vega, Pablo Piantanida", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010024", "id": "tag:google.com,2005:reader/item/0000000336d07646", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Medical Diagnosis From Laboratory Tests by Combining Generative and Discriminative Learning. (arXiv:1711.04329v2 [cs.AI] UPDATED)", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04329"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04329", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A primary goal of computational phenotype research is to conduct medical \ndiagnosis. In hospital, physicians rely on massive clinical data to make \ndiagnosis decisions, among which laboratory tests are one of the most important \nresources. However, the longitudinal and incomplete nature of laboratory test \ndata casts a significant challenge on its interpretation and usage, which may \nresult in harmful decisions by both human physicians and automatic diagnosis \nsystems. In this work, we take advantage of deep generative models to deal with \nthe complex laboratory tests. Specifically, we propose an end-to-end \narchitecture that involves a deep generative variational recurrent neural \nnetworks (VRNN) to learn robust and generalizable features, and a \ndiscriminative neural network (NN) model to learn diagnosis decision making, \nand the two models are trained jointly. Our experiments are conducted on a \ndataset involving 46,252 patients, and the 50 most frequent tests are used to \npredict the 50 most common diagnoses. The results show that our model, VRNN+NN, \nsignificantly (p&lt;0.001) outperforms other baseline models. Moreover, we \ndemonstrate that the representations learned by the joint training are more \ninformative than those learned by pure generative models. Finally, we find that \nour model offers a surprisingly good imputation for missing values. \n</p>"}, "author": "Shiyue Zhang, Pengtao Xie, Dong Wang, Eric P. Xing", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511218849010", "timestampUsec": "1511218849010022", "id": "tag:google.com,2005:reader/item/0000000336d07665", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Disease Prediction from Electronic Health Records Using Generative Adversarial Networks. (arXiv:1711.04126v1 [cs.LG] CROSS LISTED)", "published": 1511218849, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04126"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04126", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Electronic health records (EHRs) have contributed to the computerization of \npatient records so that it can be used not only for efficient and systematic \nmedical services, but also for research on data science. In this paper, we \ncompared disease prediction performance of generative adversarial networks \n(GANs) and conventional learning algorithms in combination with missing value \nprediction methods. As a result, the highest accuracy of 98.05% was obtained \nusing stacked autoencoder as the missing value prediction method and auxiliary \nclassifier GANs (AC-GANs) as the disease predicting method. Results show that \nthe combination of stacked autoencoder and AC-GANs performs significantly \ngreater than existing algorithms at the problem of disease prediction in which \nmissing values and class imbalance exist. \n</p>"}, "author": "Uiwon Hwang, Sungwoon Choi, Sungroh Yoon", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334974", "id": "tag:google.com,2005:reader/item/000000033668610c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Best-Arm Identification for Selecting Influenza Mitigation Strategies. (arXiv:1711.06299v1 [cs.LG])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06299"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06299", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Pandemic influenza has the epidemic potential to kill millions of people. \nWhile various preventive measures exist (i.a., vaccination and school \nclosures), deciding on strategies that lead to their most effective and \nefficient use, remains challenging. To this end, individual-based \nepidemiological models are essential to assist decision makers in determining \nthe best strategy to curve epidemic spread. However, individual-based models \nare computationally intensive and therefore it is pivotal to identify the \noptimal strategy using a minimal amount of model evaluations. Additionally, as \nepidemiological modeling experiments need to be planned, a computational budget \nneeds to be specified a priori. Consequently, we present a new sampling method \nto optimize the evaluation of preventive strategies using fixed budget best-arm \nidentification algorithms. We use epidemiological modeling theory to derive \nknowledge about the reward distribution which we exploit using Bayesian \nbest-arm identification algorithms (i.e., Top-two Thompson sampling and \nBayesGap). We evaluate these algorithms in a realistic experimental setting and \ndemonstrate that it is possible to identify the optimal strategy using only a \nlimited number of model evaluations, i.e., 2-to-3 times faster compared to the \nuniform sampling method, the predominant technique used for epidemiological \ndecision making in the literature. Finally, we contribute and evaluate a \nstatistic for Top-two Thompson sampling to inform the decision makers about the \nconfidence of an arm recommendation. \n</p>"}, "author": "Pieter Libin, Timothy Verstraeten, Diederik M. Roijers, Jelena Grujic, Kristof Theys, Philippe Lemey, Ann Now&#xe9;", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334973", "id": "tag:google.com,2005:reader/item/000000033668611c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "One Model for the Learning of Language. (arXiv:1711.06301v1 [cs.AI])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06301"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06301", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A major target of linguistics and cognitive science has been to understand \nwhat class of learning systems can acquire the key structures of natural \nlanguage. Until recently, the computational requirements of language have been \nused to argue that learning is impossible without a highly constrained \nhypothesis space. Here, we describe a learning system that is maximally \nunconstrained, operating over the space of all computations, and is able to \nacquire several of the key structures present natural language from positive \nevidence alone. The model successfully acquires regular (e.g. $(ab)^n$), \ncontext-free (e.g. $a^n b^n$, $x x^R$), and context-sensitive (e.g. \n$a^nb^nc^n$, $a^nb^mc^nd^m$, $xx$) formal languages. Our approach develops the \nconcept of factorized programs in Bayesian program induction in order to help \nmanage the complexity of representation. We show in learning, the model \npredicts several phenomena empirically observed in human grammar acquisition \nexperiments. \n</p>"}, "author": "Yuan Yang, Steven T. Piantadosi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334972", "id": "tag:google.com,2005:reader/item/0000000336686126", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "GA-PSO-Optimized Neural-Based Control Scheme for Adaptive Congestion Control to Improve Performance in Multimedia Applications. (arXiv:1711.06317v1 [cs.NE])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06317"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06317", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Active queue control aims to improve the overall communication network \nthroughput while providing lower delay and small packet loss rate. The basic \nidea is to actively trigger packet dropping (or marking provided by explicit \ncongestion notification (ECN)) before buffer overflow. In this paper, two \nartificial neural networks (ANN)-based control schemes are proposed for \nadaptive queue control in TCP communication networks. The structure of these \ncontrollers is optimized using genetic algorithm (GA) and the output weights of \nANNs are optimized using particle swarm optimization (PSO) algorithm. The \ncontrollers are radial bias function (RBF)-based, but to improve the robustness \nof RBF controller, an error-integral term is added to RBF equation in the \nsecond scheme. Experimental results show that GA- PSO-optimized improved RBF \n(I-RBF) model controls network congestion effectively in terms of link \nutilization with a low packet loss rate and outperform Drop Tail, \nproportional-integral (PI), random exponential marking (REM), and adaptive \nrandom early detection (ARED) controllers. \n</p>"}, "author": "Mansour Sheikhan, Ehsan Hemmati, Reza Shahnazi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334971", "id": "tag:google.com,2005:reader/item/000000033668612d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Conditional Markov Chain Search for the Simple Plant Location Problem improves upper bounds on twelve K\\\"orkel-Ghosh instances. (arXiv:1711.06347v1 [cs.DS])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06347"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06347", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We address a family of hard benchmark instances for the Simple Plant Location \nProblem (also known as the Uncapacitated Facility Location Problem). The recent \nattempt by Fischetti et al. to tackle the K\\\"orkel-Ghosh instances resulted in \nseven new optimal solutions and 22 improved upper bounds. We use automated \ngeneration of heuristics to obtain a new algorithm for the Simple Plant \nLocation Problem. In our experiments, our new algorithm matched all the \nprevious best known and optimal solutions, and further improved 12 upper \nbounds, all within shorter time budgets compared to the previous efforts. \n</p> \n<p>Our algorithm design process is split into two phases: (i) development of \nalgorithmic components such as local search procedures and mutation operators, \nand (ii) composition of a metaheuristic from the available components. Phase \n(i) requires human expertise and often can be completed by implementing several \nsimple domain-specific routines known from the literature. Phase (ii) is \nentirely automated by employing the Conditional Markov Chain Search (CMCS) \nframework. In CMCS, a metaheuristic is flexibly defined by a set of parameters, \ncalled configuration. Then the process of composition of a metaheuristic from \nthe algorithmic components is reduced to an optimisation problem seeking the \nbest performing CMCS configuration. \n</p> \n<p>We discuss the problem of comparing configurations, and propose a new \nefficient technique to select the best performing configuration from a large \nset. To employ this method, we restrict the original CMCS to a simple \ndeterministic case that leaves us with a finite and manageable number of \nmeaningful configurations. \n</p>"}, "author": "Daniel Karapetyan, Boris Goldengorin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334970", "id": "tag:google.com,2005:reader/item/0000000336686133", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Deep Learning Models for Psychological State Prediction using Smartphone Data: Challenges and Opportunities. (arXiv:1711.06350v1 [cs.LG])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06350"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06350", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>There is an increasing interest in exploiting mobile sensing technologies and \nmachine learning techniques for mental health monitoring and intervention. \nResearchers have effectively used contextual information, such as mobility, \ncommunication and mobile phone usage patterns for quantifying individuals' mood \nand wellbeing. In this paper, we investigate the effectiveness of neural \nnetwork models for predicting users' level of stress by using the location \ninformation collected by smartphones. We characterize the mobility patterns of \nindividuals using the GPS metrics presented in the literature and employ these \nmetrics as input to the network. We evaluate our approach on the open-source \nStudentLife dataset. Moreover, we discuss the challenges and trade-offs \ninvolved in building machine learning models for digital mental health and \nhighlight potential future work in this direction. \n</p>"}, "author": "Gatis Mikelsons, Matthew Smith, Abhinav Mehrotra, Mirco Musolesi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334969", "id": "tag:google.com,2005:reader/item/0000000336686139", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Question Asking as Program Generation. (arXiv:1711.06351v1 [cs.CL])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06351"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06351", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A hallmark of human intelligence is the ability to ask rich, creative, and \nrevealing questions. Here we introduce a cognitive model capable of \nconstructing human-like questions. Our approach treats questions as formal \nprograms that, when executed on the state of the world, output an answer. The \nmodel specifies a probability distribution over a complex, compositional space \nof programs, favoring concise programs that help the agent learn in the current \ncontext. We evaluate our approach by modeling the types of open-ended questions \ngenerated by humans who were attempting to learn about an ambiguous situation \nin a game. We find that our model predicts what questions people will ask, and \ncan creatively produce novel questions that were not present in the training \nset. In addition, we compare a number of model variants, finding that both \nquestion informativeness and complexity are important for producing human-like \nquestions. \n</p>"}, "author": "Anselm Rothe, Brenden M. Lake, Todd M. Gureckis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334968", "id": "tag:google.com,2005:reader/item/000000033668613e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exploring the Use of Shatter for AllSAT Through Ramsey-Type Problems. (arXiv:1711.06362v1 [cs.AI])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06362"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06362", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c73a77a6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c73a77a6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In the context of SAT solvers, Shatter is a popular tool for symmetry \nbreaking on CNF formulas. Nevertheless, little has been said about its use in \nthe context of AllSAT problems: problems where we are interested in listing all \nthe models of a Boolean formula. AllSAT has gained much popularity in recent \nyears due to its many applications in domains like model checking, data mining, \netc. One example of a particularly transparent application of AllSAT to other \nfields of computer science is computational Ramsey theory. In this paper we \nstudy the effect of incorporating Shatter to the workflow of using Boolean \nformulas to generate all possible edge colorings of a graph avoiding prescribed \nmonochromatic subgraphs. Generating complete sets of colorings is an important \nbuilding block in computational Ramsey theory. We identify two drawbacks in the \nna\\\"ive use of Shatter to break the symmetries of Boolean formulas encoding \nRamsey-type problems for graphs: a \"blow-up\" in the number of models and the \ngeneration of incomplete sets of colorings. The issues presented in this work \nare not intended to discourage the use of Shatter as a preprocessing tool for \nAllSAT problems in combinatorial computing but to help researchers properly use \nthis tool by avoiding these potential pitfalls. To this end, we provide \nstrategies and additional tools to cope with the negative effects of using \nShatter for AllSAT. While the specific application addressed in this paper is \nthat of Ramsey-type problems, the analysis we carry out applies to many other \nareas in which highly-symmetrical Boolean formulas arise and we wish to find \nall of their models. \n</p>"}, "author": "David E. Narv&#xe1;ez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334967", "id": "tag:google.com,2005:reader/item/0000000336686145", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "3D Reconstruction of Incomplete Archaeological Objects Using a Generative Adversary Network. (arXiv:1711.06363v1 [cs.CV])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06363"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06363", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a data-driven approach to aid the repairing and conservation of \narchaeological objects: ORGAN, an object reconstruction generative adversarial \nnetwork (GAN). By using an encoder-decoder 3D deep neural network on a GAN \narchitecture, and combining two loss objectives: a completion loss and an \nImproved Wasserstein GAN loss, we can train a network to effectively predict \nthe missing geometry of damaged objects. As archaeological objects can greatly \ndiffer between them, the network is conditioned on a variable, which can be a \nculture, a region or any metadata of the object. In our results, we show that \nour method can recover most of the information from damaged objects, even in \ncases where more than half of the voxels are missing, without producing many \nerrors. \n</p>"}, "author": "Renato Hermoza, Ivan Sipiran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334966", "id": "tag:google.com,2005:reader/item/0000000336686150", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Using KL-divergence to focus Deep Visual Explanation. (arXiv:1711.06431v1 [cs.AI])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06431"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a method for explaining the image classification predictions of \ndeep convolution neural networks, by highlighting the pixels in the image which \ninfluence the final class prediction. Our method requires the identification of \na heuristic method to select parameters hypothesized to be most relevant in \nthis prediction, and here we use Kullback-Leibler divergence to provide this \nfocus. Overall, our approach helps in understanding and interpreting deep \nnetwork predictions and we hope contributes to a foundation for such \nunderstanding of deep learning networks. In this brief paper, our experiments \nevaluate the performance of two popular networks in this context of \ninterpretability. \n</p>"}, "author": "Housam Khalifa Bashier Babiker, Randy Goebel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334965", "id": "tag:google.com,2005:reader/item/000000033668615a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Win Prediction in Esports: Mixed-Rank Match Prediction in Multi-player Online Battle Arena Games. (arXiv:1711.06498v1 [cs.AI])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06498"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06498", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Esports has emerged as a popular genre for players as well as spectators, \nsupporting a global entertainment industry. Esports analytics has evolved to \naddress the requirement for data-driven feedback, and is focused on \ncyber-athlete evaluation, strategy and prediction. Towards the latter, previous \nwork has used match data from a variety of player ranks from hobbyist to \nprofessional players. However, professional players have been shown to behave \ndifferently than lower ranked players. Given the comparatively limited supply \nof professional data, a key question is thus whether mixed-rank match datasets \ncan be used to create data-driven models which predict winners in professional \nmatches and provide a simple in-game statistic for viewers and broadcasters. \nHere we show that, although there is a slightly reduced accuracy, mixed-rank \ndatasets can be used to predict the outcome of professional matches, with \nsuitably optimized configurations. \n</p>"}, "author": "Victoria Hodge, Sam Devlin, Nick Sephton, Florian Block, Anders Drachen, Peter Cowling", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334964", "id": "tag:google.com,2005:reader/item/0000000336686166", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Double Deep Machine Learning. (arXiv:1711.06517v1 [cs.AI])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06517"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06517", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Very important breakthroughs in data-centric machine learning algorithms led \nto impressive performance in transactional point applications such as detecting \nanger in speech, alerts from a Face Recognition system, or EKG interpretation. \nNon-transactional applications, e.g. medical diagnosis beyond the EKG results, \nrequire AI algorithms that integrate deeper and broader knowledge in their \nproblem-solving capabilities, e.g. integrating knowledge about anatomy and \nphysiology of the heart with EKG results and additional patient findings. \nSimilarly, for military aerial interpretation, where knowledge about enemy \ndoctrines on force composition and spread helps immensely in situation \nassessment beyond image recognition of individual objects. The Double Deep \nLearning approach advocates integrating data-centric machine self-learning \ntechniques with machine-teaching techniques to leverage the power of both and \novercome their corresponding limitations. To take AI to the next level, it is \nessential that we rebalance the roles of data and knowledge. Data is important \nbut knowledge- deep and commonsense- are equally important. An initiative is \nproposed to build Wikipedia for Smart Machines, meaning target readers are not \nhuman, but rather smart machines. Named ReKopedia, the goal is to develop \nmethodologies, tools, and automatic algorithms to convert humanity knowledge \nthat we all learn in schools, universities and during our professional life \ninto Reusable Knowledge structures that smart machines can use in their \ninference algorithms. Ideally, ReKopedia would be an open source shared \nknowledge repository similar to the well-known shared open source software code \nrepositories. Examples in the article are based on- or inspired by- real-life \nnon-transactional AI systems I deployed over decades of AI career that benefit \nhundreds of millions of people around the globe. \n</p>"}, "author": "Moshe BenBassat", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334963", "id": "tag:google.com,2005:reader/item/0000000336686172", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Training Simplification and Model Simplification for Deep Learning: A Minimal Effort Back Propagation Method. (arXiv:1711.06528v1 [cs.LG])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06528"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06528", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a simple yet effective technique to simplify the training and the \nresulting model of neural networks. In back propagation, only a small subset of \nthe full gradient is computed to update the model parameters. The gradient \nvectors are sparsified in such a way that only the top-$k$ elements (in terms \nof magnitude) are kept. As a result, only $k$ rows or columns (depending on the \nlayout) of the weight matrix are modified, leading to a linear reduction in the \ncomputational cost. Based on the sparsified gradients, we further simplify the \nmodel by eliminating the rows or columns that are seldom updated, which will \nreduce the computational cost both in the training and decoding, and \npotentially accelerate decoding in real-world applications. Surprisingly, \nexperimental results demonstrate that most of time we only need to update fewer \nthan 5% of the weights at each back propagation pass. More interestingly, the \naccuracy of the resulting models is actually improved rather than degraded, and \na detailed analysis is given. The model simplification results show that we \ncould adaptively simplify the model which could often be reduced by around 9x, \nwithout any loss on accuracy or even with improved accuracy. \n</p>"}, "author": "Xu Sun, Xuancheng Ren, Shuming Ma, Bingzhen Wei, Wei Li, Houfeng Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334962", "id": "tag:google.com,2005:reader/item/000000033668618c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reconstruction of the External Stimuli from Brain Signals. (arXiv:1711.06550v1 [eess.SP])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06550"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06550", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Despite the rapid advances in Brain-computer Interfacing (BCI) and continuous \neffort to improve the accuracy of brain decoding systems, the urge for the \nsystems to reconstruct the experiences of the users has been widely \nacknowledged. This urge has been investigated by some researchers during the \npast years in terms of reconstruction of the naturalistic images, abstract \nimages, video and audio. In this study, we try to tackle this issue by \nregressing the stimuli spectrogram using the spectrogram analysis of the brain \nsignals. The results of our regression-based method suggest the feasibility of \nsuch reconstructions using the neuroimaging techniques that are appropriate for \nout-of-lab scenarios. \n</p>"}, "author": "Pouya Ghaemmaghami", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334961", "id": "tag:google.com,2005:reader/item/0000000336686194", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Iterative Closest Points Approach to Neural Generative Models. (arXiv:1711.06562v1 [cs.LG])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06562"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06562", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a simple way to learn a transformation that maps samples of one \ndistribution to the samples of another distribution. Our algorithm comprises an \niteration of 1) drawing samples from some simple distribution and transforming \nthem using a neural network, 2) determining pairwise correspondences between \nthe transformed samples and training data (or a minibatch), and 3) optimizing \nthe weights of the neural network being trained to minimize the distances \nbetween the corresponding vectors. This can be considered as a variant of the \nIterative Closest Points (ICP) algorithm, common in geometric computer vision, \nalthough ICP typically operates on sensor point clouds and linear transforms \ninstead of random sample sets and neural nonlinear transforms. We demonstrate \nthe algorithm on simple synthetic data and MNIST data. We furthermore \ndemonstrate that the algorithm is capable of handling distributions with both \ncontinuous and discrete variables. \n</p>"}, "author": "Joose Rajam&#xe4;ki, Perttu H&#xe4;m&#xe4;l&#xe4;inen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334960", "id": "tag:google.com,2005:reader/item/00000003366861b0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning to Play Othello with Deep Neural Networks. (arXiv:1711.06583v1 [cs.AI])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06583"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06583", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Achieving superhuman playing level by AlphaGo corroborated the capabilities \nof convolutional neural architectures (CNNs) for capturing complex spatial \npatterns. This result was to a great extent due to several analogies between Go \nboard states and 2D images CNNs have been designed for, in particular \ntranslational invariance and a relatively large board. In this paper, we verify \nwhether CNN-based move predictors prove effective for Othello, a game with \nsignificantly different characteristics, including a much smaller board size \nand complete lack of translational invariance. We compare several CNN \narchitectures and board encodings, augment them with state-of-the-art \nextensions, train on an extensive database of experts' moves, and examine them \nwith respect to move prediction accuracy and playing strength. The empirical \nevaluation confirms high capabilities of neural move predictors and suggests a \nstrong correlation between prediction accuracy and playing strength. The best \nCNNs not only surpass all other 1-ply Othello players proposed to date but \ndefeat (2-ply) Edax, the best open-source Othello player. \n</p>"}, "author": "Pawe&#x142; Liskowski, Wojciech Ja&#x15b;kowski, Krzysztof Krawiec", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334959", "id": "tag:google.com,2005:reader/item/00000003366861bc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dependent landmark drift: robust point set registration based on the Gaussian mixture model with a statistical shape model. (arXiv:1711.06588v1 [cs.AI])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06588"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06588", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Point set registration is to find point-by-point correspondences between \npoint sets, each of which characterizes the shape of an object. Due to the \nassumption of the local preservation of object geometry, prevalent algorithms \nin the area can often elegantly solve the problems without using geometric \ninformation specific to the objects. This means that registration performance \ncan be further improved by using the prior knowledge of object geometry. In \nthis paper, we propose a novel point set registration method using the Gaussian \nmixture model with prior shape information encoded as a statistical shape \nmodel. Our transformation model is defined as the combination of the rigid \ntransformation, the motion coherence, and the statistical shape model. \nTherefore, the proposed method works effectively if the target point set \nincludes outliers and missing regions, or if it is rotated. The computational \ncost can be reduced to linear and thereby the method is scalable to large point \nsets. The effectiveness of the method will be verified through comparisons with \nexisting algorithms using datasets concerning human hands and faces. \n</p>"}, "author": "Osamu Hirose", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334958", "id": "tag:google.com,2005:reader/item/00000003366861c7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Evolving soft locomotion in aquatic and terrestrial environments: effects of material properties and environmental transitions. (arXiv:1711.06605v1 [cs.AI])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06605"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06605", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c73a7ac7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c73a7ac7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Designing soft robots poses considerable challenges: automated design \napproaches may be particularly appealing in this field, as they promise to \noptimize complex multi-material machines with very little or no human \nintervention. Evolutionary soft robotics is concerned with the application of \noptimization algorithms inspired by natural evolution in order to let soft \nrobots (both morphologies and controllers) spontaneously evolve within \nphysically-realistic simulated environments, figuring out how to satisfy a set \nof objectives defined by human designers. In this paper a powerful evolutionary \nsystem is put in place in order to perform a broad investigation on the \nfree-form evolution of walking and swimming soft robots in different \nenvironments. Three sets of experiments are reported, tackling different \naspects of the evolution of soft locomotion. The first two sets explore the \neffects of different material properties on the evolution of terrestrial and \naquatic soft locomotion: particularly, we show how different materials lead to \nthe evolution of different morphologies, behaviors, and energy-performance \ntradeoffs. It is found that within our simplified physics world stiffer robots \nevolve more sophisticated and effective gaits and morphologies on land, while \nsofter ones tend to perform better in water. The third set of experiments \nstarts investigating the effect and potential benefits of major environmental \ntransitions (land - water) during evolution. Results provide interesting \nmorphological exaptation phenomena, and point out a potential asymmetry between \nland-water and water-land transitions: while the first type of transition \nappears to be detrimental, the second one seems to have some beneficial \neffects. \n</p>"}, "author": "Francesco Corucci, Nick Cheney, Francesco Giorgio-Serchi, Josh Bongard, Cecilia Laschi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334957", "id": "tag:google.com,2005:reader/item/00000003366861da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Driven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments. (arXiv:1711.06623v1 [cs.RO])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06623"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06623", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c743c7c1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c743c7c1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present a self-supervised approach to ignoring \"distractors\" in camera \nimages for the purposes of robustly estimating vehicle motion in cluttered \nurban environments. We leverage offline multi-session mapping approaches to \nautomatically generate a per-pixel ephemerality mask and depth map for each \ninput image, which we use to train a deep convolutional network. At run-time we \nuse the predicted ephemerality and depth as an input to a monocular visual \nodometry (VO) pipeline, using either sparse features or dense photometric \nmatching. Our approach yields metric-scale VO using only a single camera and \ncan recover the correct egomotion even when 90% of the image is obscured by \ndynamic, independently moving objects. We evaluate our robust VO methods on \nmore than 400km of driving from the Oxford RobotCar Dataset and demonstrate \nreduced odometry drift and significantly improved egomotion estimation in the \npresence of large moving vehicles in urban traffic. \n</p>"}, "author": "Dan Barnes, Will Maddern, Geoffrey Pascoe, Ingmar Posner", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511175747335", "timestampUsec": "1511175747334956", "id": "tag:google.com,2005:reader/item/00000003366861ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "ATRank: An Attention-Based User Behavior Modeling Framework for Recommendation. (arXiv:1711.06632v1 [cs.AI])", "published": 1511175748, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06632"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06632", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A user can be represented as what he/she does along the history. A common way \nto deal with the user modeling problem is to manually extract all kinds of \naggregated features over the heterogeneous behaviors, which may fail to fully \nrepresent the data itself due to limited human instinct. Recent works usually \nuse RNN-based methods to give an overall embedding of a behavior sequence, \nwhich then could be exploited by the downstream applications. However, this can \nonly preserve very limited information, or aggregated memories of a person. \nWhen a downstream application requires to facilitate the modeled user features, \nit may lose the integrity of the specific highly correlated behavior of the \nuser, and introduce noises derived from unrelated behaviors. This paper \nproposes an attention based user behavior modeling framework called ATRank, \nwhich we mainly use for recommendation tasks. Heterogeneous user behaviors are \nconsidered in our model that we project all types of behaviors into multiple \nlatent semantic spaces, where influence can be made among the behaviors via \nself-attention. Downstream applications then can use the user behavior vectors \nvia vanilla attention. Experiments show that ATRank can achieve better \nperformance and faster training process. We further explore ATRank to use one \nunified model to predict different types of user behaviors at the same time, \nshowing a comparable performance with the highly optimized individual models. \n</p>"}, "author": "Chang Zhou, Jinze Bai, Junshuai Song, Xiaofei Liu, Zhengchao Zhao, Xiusi Chen, Jun Gao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511144501564", "timestampUsec": "1511144501564375", "id": "tag:google.com,2005:reader/item/000000033629d250", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "GA-PSO-Optimized Neural-Based Control Scheme for Adaptive Congestion Control to Improve Performance in Multimedia Applications. (arXiv:1711.06317v1 [cs.NE])", "published": 1511144502, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06317"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06317", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Active queue control aims to improve the overall communication network \nthroughput while providing lower delay and small packet loss rate. The basic \nidea is to actively trigger packet dropping (or marking provided by explicit \ncongestion notification (ECN)) before buffer overflow. In this paper, two \nartificial neural networks (ANN)-based control schemes are proposed for \nadaptive queue control in TCP communication networks. The structure of these \ncontrollers is optimized using genetic algorithm (GA) and the output weights of \nANNs are optimized using particle swarm optimization (PSO) algorithm. The \ncontrollers are radial bias function (RBF)-based, but to improve the robustness \nof RBF controller, an error-integral term is added to RBF equation in the \nsecond scheme. Experimental results show that GA- PSO-optimized improved RBF \n(I-RBF) model controls network congestion effectively in terms of link \nutilization with a low packet loss rate and outperform Drop Tail, \nproportional-integral (PI), random exponential marking (REM), and adaptive \nrandom early detection (ARED) controllers. \n</p>"}, "author": "Mansour Sheikhan, Ehsan Hemmati, Reza Shahnazi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511144501564", "timestampUsec": "1511144501564374", "id": "tag:google.com,2005:reader/item/000000033629d26b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adaptive active queue management controller for TCP communication networks using PSO-RBF models. (arXiv:1711.06356v1 [cs.NI])", "published": 1511144502, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06356"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06356", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Addressing performance degradations in end-to-end congestion control has been \none of the most active research areas in the last decade. Active queue \nmanagement (AQM) aims to improve the overall network throughput, while \nproviding lower delay and reduce packet loss and improving network. The basic \nidea is to actively trigger packet dropping (or marking provided by explicit \ncongestion notification (ECN)) before buffer overflow. Radial bias function \n(RBF)-based AQM controller is proposed in this paper. RBF controller is \nsuitable as an AQM scheme to control congestion in TCP communication networks \nsince it is nonlinear. Particle swarm optimization (PSO) algorithm is also \nemployed to derive RBF parameters such that the integrated-absolute error (IAE) \nis minimized. Furthermore, in order to improve the robustness of RBF \ncontroller, an error-integral term is added to RBF equation. The results of the \ncomparison with Drop Tail, adaptive random early detection (ARED), random \nexponential marking (REM), and proportional-integral (PI) controllers are \npresented. Integral-RBF has better performance not only in comparison with RBF \nbut also with ARED, REM and PI controllers in the case of link utilization \nwhile packet loss rate is small. \n</p>"}, "author": "Mansour Sheikhan, Reza Shahnazi, Ehasn Hemmati", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511144501564", "timestampUsec": "1511144501564373", "id": "tag:google.com,2005:reader/item/000000033629d276", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improvements to context based self-supervised learning. (arXiv:1711.06379v1 [cs.CV])", "published": 1511144502, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06379"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06379", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop a set of methods to improve on the results of self-supervised \nlearning using context. We start with a baseline of patch based arrangement \ncontext learning and go from there. Our methods address some overt problems \nsuch as chromatic aberration as well as other potential problems such as \nspatial skew and mid-level feature neglect. We prevent problems with testing \ngeneralization on common self-supervised benchmark tests by using different \ndatasets during our development. The results of our methods combined yield top \nscores on all standard self-supervised benchmarks, including classification and \ndetection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and \"linear \ntests\" on the ImageNet and CSAIL Places datasets. We obtain an improvement over \nour baseline method of between 4.0 to 7.1 percentage points on transfer \nlearning classification tests. We also show results on different standard \nnetwork architectures to demonstrate generalization as well as portability. \n</p>"}, "author": "T. Nathan Mundhenk, Daniel Ho, Barry Y. Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511144501564", "timestampUsec": "1511144501564372", "id": "tag:google.com,2005:reader/item/000000033629d27f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bidirectional deep echo state networks. (arXiv:1711.06509v1 [cs.NE])", "published": 1511144502, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06509"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06509", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work we propose a deep architecture for the classification of \nmultivariate time series. By means of a recurrent and untrained reservoir we \ngenerate a vectorial representation that embeds the temporal relationships in \nthe data. To overcome the limitations of the reservoir vanishing memory, we \nintroduce a bidirectional reservoir, whose last state captures also the past \ndependencies in the input. We apply dimensionality reduction to the final \nreservoir states to obtain compressed fixed size representations of the time \nseries. These are subsequently fed into a deep feedforward network, which is \ntrained to perform the final classification. We test our architecture on \nbenchmark datasets and on a real-world use-case of blood samples \nclassification. Results show that our method performs better than a standard \necho state network, and it can be trained much faster than a fully-trained \nrecurrent network. \n</p>"}, "author": "Filippo Maria Bianchi, Simone Scardapane, Sigurd L&#xf8;kse, Robert Jenssen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511144501564", "timestampUsec": "1511144501564371", "id": "tag:google.com,2005:reader/item/000000033629d284", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Classification of postoperative surgical site infections from blood measurements with missing data using recurrent neural networks. (arXiv:1711.06516v1 [cs.NE])", "published": 1511144502, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06516"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06516", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Clinical measurements that can be represented as time series constitute an \nimportant fraction of the electronic health records and are often both \nuncertain and incomplete. Recurrent neural networks are a special class of \nneural networks that are particularly suitable to process time series data but, \nin their original formulation, cannot explicitly deal with missing data. In \nthis paper, we explore imputation strategies for handling missing values in \nclassifiers based on recurrent neural network (RNN) and apply a recently \nproposed recurrent architecture, the Gated Recurrent Unit with Decay, \nspecifically designed to handle missing data. We focus on the problem of \ndetecting surgical site infection in patients by analyzing time series of their \nblood sample measurements and we compare the results obtained with different \nRNN-based classifiers. \n</p>"}, "author": "Andreas Storvik Strauman, Filippo Maria Bianchi, Karl &#xd8;yvind Mikalsen, Michael Kampffmeyer, Cristina Soguero-Ruiz, Robert Jenssen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511144501564", "timestampUsec": "1511144501564370", "id": "tag:google.com,2005:reader/item/000000033629d289", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Evolving soft locomotion in aquatic and terrestrial environments: effects of material properties and environmental transitions. (arXiv:1711.06605v1 [cs.AI])", "published": 1511144502, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06605"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06605", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Designing soft robots poses considerable challenges: automated design \napproaches may be particularly appealing in this field, as they promise to \noptimize complex multi-material machines with very little or no human \nintervention. Evolutionary soft robotics is concerned with the application of \noptimization algorithms inspired by natural evolution in order to let soft \nrobots (both morphologies and controllers) spontaneously evolve within \nphysically-realistic simulated environments, figuring out how to satisfy a set \nof objectives defined by human designers. In this paper a powerful evolutionary \nsystem is put in place in order to perform a broad investigation on the \nfree-form evolution of walking and swimming soft robots in different \nenvironments. Three sets of experiments are reported, tackling different \naspects of the evolution of soft locomotion. The first two sets explore the \neffects of different material properties on the evolution of terrestrial and \naquatic soft locomotion: particularly, we show how different materials lead to \nthe evolution of different morphologies, behaviors, and energy-performance \ntradeoffs. It is found that within our simplified physics world stiffer robots \nevolve more sophisticated and effective gaits and morphologies on land, while \nsofter ones tend to perform better in water. The third set of experiments \nstarts investigating the effect and potential benefits of major environmental \ntransitions (land - water) during evolution. Results provide interesting \nmorphological exaptation phenomena, and point out a potential asymmetry between \nland-water and water-land transitions: while the first type of transition \nappears to be detrimental, the second one seems to have some beneficial \neffects. \n</p>"}, "author": "Francesco Corucci, Nick Cheney, Francesco Giorgio-Serchi, Josh Bongard, Cecilia Laschi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1511144501564", "timestampUsec": "1511144501564369", "id": "tag:google.com,2005:reader/item/000000033629d293", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neon2: Finding Local Minima via First-Order Oracles. (arXiv:1711.06673v1 [cs.LG])", "published": 1511144502, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06673"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06673", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a reduction for non-convex optimization that can (1) turn a \nstationary-point finding algorithm into a local-minimum finding one, and (2) \nreplace the Hessian-vector product computations with only gradient \ncomputations. It works both in the stochastic and the deterministic settings, \nwithout hurting the algorithm's performance. \n</p> \n<p>As applications, our reduction turns Natasha2 into a first-order method \nwithout hurting its performance. It also converts SGD, GD, SCSG, and SVRG into \nlocal-minimum finding algorithms outperforming some best known results. \n</p>"}, "author": "Zeyuan Allen-Zhu, Yuanzhi Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200649", "id": "tag:google.com,2005:reader/item/00000003353a2c20", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Neural Network Pushdown Automaton: Model, Stack and Learning Simulations. (arXiv:1711.05738v1 [cs.AI])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05738"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05738", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c743cb50\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c743cb50&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In order for neural networks to learn complex languages or grammars, they \nmust have sufficient computational power or resources to recognize or generate \nsuch languages. Though many approaches have been discussed, one ob- vious \napproach to enhancing the processing power of a recurrent neural network is to \ncouple it with an external stack memory - in effect creating a neural network \npushdown automata (NNPDA). This paper discusses in detail this NNPDA - its \nconstruction, how it can be trained and how useful symbolic information can be \nextracted from the trained network. \n</p> \n<p>In order to couple the external stack to the neural network, an optimization \nmethod is developed which uses an error function that connects the learning of \nthe state automaton of the neural network to the learning of the operation of \nthe external stack. To minimize the error function using gradient descent \nlearning, an analog stack is designed such that the action and storage of \ninformation in the stack are continuous. One interpretation of a continuous \nstack is the probabilistic storage of and action on data. After training on \nsample strings of an unknown source grammar, a quantization procedure extracts \nfrom the analog stack and neural network a discrete pushdown automata (PDA). \nSimulations show that in learning deterministic context-free grammars - the \nbalanced parenthesis language, 1*n0*n, and the deterministic Palindrome - the \nextracted PDA is correct in the sense that it can correctly recognize unseen \nstrings of arbitrary length. In addition, the extracted PDAs can be shown to be \nidentical or equivalent to the PDAs of the source grammars which were used to \ngenerate the training strings. \n</p>"}, "author": "G.Z. Sun, C.L. Giles, H.H. Chen, Y.C. Lee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200648", "id": "tag:google.com,2005:reader/item/00000003353a2c24", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast Predictive Simple Geodesic Regression. (arXiv:1711.05766v1 [cs.CV])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05766"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05766", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deformable image registration and regression are important tasks in medical \nimage analysis. However, they are computationally expensive, especially when \nanalyzing large-scale datasets that contain thousands of images. Hence, cluster \ncomputing is typically used, making the approaches dependent on such \ncomputational infrastructure. Even larger computational resources are required \nas study sizes increase. This limits the use of deformable image registration \nand regression for clinical applications and as component algorithms for other \nimage analysis approaches. We therefore propose using a fast predictive \napproach to perform image registrations. In particular, we employ these fast \nregistration predictions to approximate a simplified geodesic regression model \nto capture longitudinal brain changes. The resulting method is orders of \nmagnitude faster than the standard optimization-based regression model and \nhence facilitates large-scale analysis on a single graphics processing unit \n(GPU). We evaluate our results on 3D brain magnetic resonance images (MRI) from \nthe ADNI datasets. \n</p>"}, "author": "Zhipeng Ding, Greg Fleishman, Xiao Yang, Paul Thompson, Roland Kwitt, Marc Niethammer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200647", "id": "tag:google.com,2005:reader/item/00000003353a2c2c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Predicting vehicular travel times by modeling heterogeneous influences between arterial roads. (arXiv:1711.05767v1 [cs.AI])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05767"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05767", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Predicting travel times of vehicles in urban settings is a useful and \ntangible quantity of interest in the context of intelligent transportation \nsystems. We address the problem of travel time prediction in arterial roads \nusing data sampled from probe vehicles. There is only a limited literature on \nmethods using data input from probe vehicles. The spatio-temporal dependencies \ncaptured by existing data driven approaches are either too detailed or very \nsimplistic. We strike a balance of the existing data driven approaches to \naccount for varying degrees of influence a given road may experience from its \nneighbors, while controlling the number of parameters to be learnt. \nSpecifically, we use a NoisyOR conditional probability distribution (CPD) in \nconjunction with a dynamic bayesian network (DBN) to model state transitions of \nvarious roads. We propose an efficient algorithm to learn model parameters. We \npropose an algorithm for predicting travel times on trips of arbitrary \ndurations. Using synthetic and real world data traces we demonstrate the \nsuperior performance of the proposed method under different traffic conditions. \n</p>"}, "author": "Avinash Achar, Venkatesh Sarangan, R Rohith, Anand Sivasubramaniam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200646", "id": "tag:google.com,2005:reader/item/00000003353a2c33", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Quantile Markov Decision Process. (arXiv:1711.05788v1 [cs.AI])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05788"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05788", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we consider the problem of optimizing the quantiles of the \ncumulative rewards of Markov Decision Processes (MDP), to which we refers as \nQuantile Markov Decision Processes (QMDP). Traditionally, the goal of a Markov \nDecision Process (MDP) is to maximize expected cumulative reward over a defined \nhorizon (possibly to be infinite). In many applications, however, a decision \nmaker may be interested in optimizing a specific quantile of the cumulative \nreward instead of its expectation. (If we have some reference here, it would be \ngood.) Our framework of QMDP provides analytical results characterizing the \noptimal QMDP solution and presents the algorithm for solving the QMDP. We \nprovide analytical results characterizing the optimal QMDP solution and present \nthe algorithms for solving the QMDP. We illustrate the model with two \nexperiments: a grid game and a HIV optimal treatment experiment. \n</p>"}, "author": "Xiaocheng Li, Huaiyang Zhong, Margaret L. Brandeau", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200645", "id": "tag:google.com,2005:reader/item/00000003353a2c3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "K3, L3, LP, RM3, A3, FDE: How to Make Many-Valued Logics Work for You. (arXiv:1711.05816v1 [cs.AI])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05816"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05816", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We investigate some well-known (and a few not-so-well-known) many-valued \nlogics that have a small number (3 or 4) of truth values. For some of them we \ncomplain that they do not have any \\emph{logical} use (despite their perhaps \nhaving some intuitive semantic interest) and we look at ways to add features so \nas to make them useful, while retaining their intuitive appeal. At the end, we \nshow some surprising results in the system FDE, and its relationships with \nfeatures of other logics. We close with some new examples of \"synonymous \nlogics.\" An Appendix contains a natural deduction system for our augmented FDE, \nand proofs of soundness and completeness. \n</p>"}, "author": "Allen P. Hazen, Francis Jeffry Pelletier", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200644", "id": "tag:google.com,2005:reader/item/00000003353a2c47", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bootstrapped synthetic likelihood. (arXiv:1711.05825v1 [stat.CO])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05825"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05825", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The development of approximate Bayesian computation (ABC) and synthetic \nlikelihood (SL) techniques has enabled the use of Bayesian inference for models \nthat may be simulated, but for which the likelihood is not available to \nevaluate pointwise at values of an unknown parameter $\\theta$. The main idea in \nABC and SL is to, for different values of $\\theta$ (usually chosen using a \nMonte Carlo algorithm), build estimates of the likelihood based on simulations \nfrom the model conditional on $\\theta$. The quality of these estimates \ndetermines the efficiency of an ABC/SL algorithm. In standard ABC/SL, the only \nmeans to improve an estimated likelihood at $\\theta$ is to simulate more times \nfrom the model conditional on $\\theta$, which is infeasible in cases where the \nsimulator is computationally expensive. In this paper we describe how to use \nbootstrapping as a means for improving synthetic likelihood estimates whilst \nusing fewer simulations from the model, and also investigate its use in ABC. \nFurther, we investigate the use of the bag of little bootstraps as a means for \napplying this approach to large datasets, yielding to Monte Carlo algorithms \nthat accurately approximate posterior distributions whilst only simulating \nsubsamples of the full data. Examples of the approach applied to i.i.d., \ntemporal and spatial data are given. \n</p>"}, "author": "Richard G. Everitt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200643", "id": "tag:google.com,2005:reader/item/00000003353a2c4e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning. (arXiv:1711.05851v1 [cs.CL])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05851"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05851", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Knowledge bases (KB), both automatically and manually constructed, are often \nincomplete --- many valid facts can be inferred from the KB by synthesizing \nexisting information. A popular approach to KB completion is to infer new \nrelations by combinatory reasoning over the information found along other paths \nconnecting a pair of entities. Given the enormous size of KBs and the \nexponential number of paths, previous path-based models have considered only \nthe problem of predicting a missing relation given two entities or evaluating \nthe truth of a proposed triple. Additionally, these methods have traditionally \nused random paths between fixed entity pairs or more recently learned to pick \npaths between them. We propose a new algorithm MINERVA, which addresses the \nmuch more difficult and practical task of answering questions where the \nrelation is known, but only one entity. Since random walks are impractical in a \nsetting with combinatorially many destinations from a start node, we present a \nneural reinforcement learning approach which learns how to navigate the graph \nconditioned on the input query to find predictive paths. Empirically, this \napproach obtains state-of-the-art results on several datasets, significantly \noutperforming prior methods. \n</p>"}, "author": "Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, Andrew McCallum", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200642", "id": "tag:google.com,2005:reader/item/00000003353a2c53", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "End-to-end 3D shape inverse rendering of different classes of objects from a single input image. (arXiv:1711.05858v1 [cs.CV])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05858"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05858", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper a semi-supervised deep framework is proposed for the problem of \n3D shape inverse rendering from a single 2D input image. The main structure of \nproposed framework consists of unsupervised pre-trained components which \nsignificantly reduce the need to labeled data for training the whole framework. \nusing labeled data has the advantage of achieving to accurate results without \nthe need to predefined assumptions about image formation process. Three main \ncomponents are used in the proposed network: an encoder which maps 2D input \nimage to a representation space, a 3D decoder which decodes a representation to \na 3D structure and a mapping component in order to map 2D to 3D representation. \nThe only part that needs label for training is the mapping part with not too \nmany parameters. The other components in the network can be pre-trained \nunsupervised using only 2D images or 3D data in each case. The way of \nreconstructing 3D shapes in the decoder component, inspired by the model based \nmethods for 3D reconstruction, maps a low dimensional representation to 3D \nshape space with the advantage of extracting the basis vectors of shape space \nfrom training data itself and is not restricted to a small set of examples as \nused in predefined models. Therefore, the proposed framework deals directly \nwith coordinate values of the point cloud representation which leads to achieve \ndense 3D shapes in the output. The experimental results on several benchmark \ndatasets of objects and human faces and comparing with recent similar methods \nshows the power of proposed network in recovering more details from single 2D \nimages. \n</p>"}, "author": "Shima Kamyab, S. Zohreh Azimifar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200641", "id": "tag:google.com,2005:reader/item/00000003353a2c61", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Using Noisy Extractions to Discover Causal Knowledge. (arXiv:1711.05900v1 [cs.AI])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05900"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05900", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Knowledge bases (KB) constructed through information extraction from text \nplay an important role in query answering and reasoning. In this work, we study \na particular reasoning task, the problem of discovering causal relationships \nbetween entities, known as causal discovery. There are two contrasting types of \napproaches to discovering causal knowledge. One approach attempts to identify \ncausal relationships from text using automatic extraction techniques, while the \nother approach infers causation from observational data. However, extractions \nalone are often insufficient to capture complex patterns and full observational \ndata is expensive to obtain. We introduce a probabilistic method for fusing \nnoisy extractions with observational data to discover causal knowledge. We \npropose a principled approach that uses the probabilistic soft logic (PSL) \nframework to encode well-studied constraints to recover long-range patterns and \nconsistent predictions, while cheaply acquired extractions provide a proxy for \nunseen observations. We apply our method gene regulatory networks and show the \npromise of exploiting KB signals in causal discovery, suggesting a critical, \nnew area of research. \n</p>"}, "author": "Dhanya Sridhar, Jay Pujara, Lise Getoor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200640", "id": "tag:google.com,2005:reader/item/00000003353a2c6a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Using experimental game theory to transit human values to ethical AI. (arXiv:1711.05905v1 [cs.AI])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05905"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05905", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Knowing the reflection of game theory and ethics, we develop a mathematical \nrepresentation to bridge the gap between the concepts in moral philosophy \n(e.g., Kantian and Utilitarian) and AI ethics industry technology standard \n(e.g., IEEE P7000 standard series for Ethical AI). As an application, we \ndemonstrate how human value can be obtained from the experimental game theory \n(e.g., trust game experiment) so as to build an ethical AI. Moreover, an \napproach to test the ethics (rightness or wrongness) of a given AI algorithm by \nusing an iterated Prisoner's Dilemma Game experiment is discussed as an \nexample. Compared with existing mathematical frameworks and testing method on \nAI ethics technology, the advantages of the proposed approach are analyzed. \n</p>"}, "author": "Yijia Wang, Yan Wan, Zhijian Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200639", "id": "tag:google.com,2005:reader/item/00000003353a2c74", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Budget-Constrained Multi-Armed Bandits with Multiple Plays. (arXiv:1711.05928v1 [cs.LG])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05928"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05928", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c743cf09\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c743cf09&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study the multi-armed bandit problem with multiple plays and a budget \nconstraint for both the stochastic and the adversarial setting. At each round, \nexactly $K$ out of $N$ possible arms have to be played (with $1\\leq K \\leq N$). \nIn addition to observing the individual rewards for each arm played, the player \nalso learns a vector of costs which has to be covered with an a-priori defined \nbudget $B$. The game ends when the sum of current costs associated with the \nplayed arms exceeds the remaining budget. \n</p> \n<p>Firstly, we analyze this setting for the stochastic case, for which we assume \neach arm to have an underlying cost and reward distribution with support \n$[c_{\\min}, 1]$ and $[0, 1]$, respectively. We derive an Upper Confidence Bound \n(UCB) algorithm which achieves $O(NK^4 \\log B)$ regret. \n</p> \n<p>Secondly, for the adversarial case in which the entire sequence of rewards \nand costs is fixed in advance, we derive an upper bound on the regret of order \n$O(\\sqrt{NB\\log(N/K)})$ utilizing an extension of the well-known \n$\\texttt{Exp3}$ algorithm. We also provide upper bounds that hold with high \nprobability and a lower bound of order $\\Omega((1 - K/N)^2 \\sqrt{NB/K})$. \n</p>"}, "author": "Datong P. Zhou, Claire J. Tomlin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200638", "id": "tag:google.com,2005:reader/item/00000003353a2c7e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Remedies against the Vocabulary Gap in Information Retrieval. (arXiv:1711.06004v1 [cs.IR])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06004"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06004", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c74c8280\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c74c8280&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Search engines rely heavily on term-based approaches that represent queries \nand documents as bags of words. Text---a document or a query---is represented \nby a bag of its words that ignores grammar and word order, but retains word \nfrequency counts. When presented with a search query, the engine then ranks \ndocuments according to their relevance scores by computing, among other things, \nthe matching degrees between query and document terms. While term-based \napproaches are intuitive and effective in practice, they are based on the \nhypothesis that documents that exactly contain the query terms are highly \nrelevant regardless of query semantics. Inversely, term-based approaches assume \ndocuments that do not contain query terms as irrelevant. However, it is known \nthat a high matching degree at the term level does not necessarily mean high \nrelevance and, vice versa, documents that match null query terms may still be \nrelevant. Consequently, there exists a vocabulary gap between queries and \ndocuments that occurs when both use different words to describe the same \nconcepts. It is the alleviation of the effect brought forward by this \nvocabulary gap that is the topic of this dissertation. More specifically, we \npropose (1) methods to formulate an effective query from complex textual \nstructures and (2) latent vector space models that circumvent the vocabulary \ngap in information retrieval. \n</p>"}, "author": "Christophe Van Gysel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200637", "id": "tag:google.com,2005:reader/item/00000003353a2c83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hindsight policy gradients. (arXiv:1711.06006v1 [cs.LG])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06006"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06006", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Goal-conditional policies allow reinforcement learning agents to pursue \nspecific goals during different episodes. In addition to their potential to \ngeneralize desired behavior to unseen goals, such policies may also help in \ndefining options for arbitrary subgoals, enabling higher-level planning. While \ntrying to achieve a specific goal, an agent may also be able to exploit \ninformation about the degree to which it has achieved alternative goals. \nReinforcement learning agents have only recently been endowed with such \ncapacity for hindsight, which is highly valuable in environments with sparse \nrewards. In this paper, we show how hindsight can be introduced to \nlikelihood-ratio policy gradient methods, generalizing this capacity to an \nentire class of highly successful algorithms. Our preliminary experiments \nsuggest that hindsight may increase the sample efficiency of policy gradient \nmethods. \n</p>"}, "author": "Paulo Rauber, Filipe Mutz, Juergen Schmidhuber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200636", "id": "tag:google.com,2005:reader/item/00000003353a2c88", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sub-committee Approval Voting and Generalised Justified Representation Axioms. (arXiv:1711.06030v1 [cs.GT])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06030"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06030", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Social choice is replete with various settings including single-winner \nvoting, multi-winner voting, probabilistic voting, multiple referenda, and \npublic decision making. We study a general model of social choice called \nSub-Committee Voting (SCV) that simultaneously generalizes these settings. We \nthen focus on sub-committee voting with approvals and propose extensions of the \njustified representation axioms that have been considered for proportional \nrepresentation in approval-based committee voting. We study the properties and \nrelations of these axioms. For each of the axioms, we analyse whether a \nrepresentative committee exists and also examine the complexity of computing \nand verifying such a committee. \n</p>"}, "author": "Haris Aziz, Barton E. Lee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200635", "id": "tag:google.com,2005:reader/item/00000003353a2c90", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "From Algorithmic Black Boxes to Adaptive White Boxes: Declarative Decision-Theoretic Ethical Programs as Codes of Ethics. (arXiv:1711.06035v1 [cs.AI])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06035"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06035", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Ethics of algorithms is an emerging topic in various disciplines such as \nsocial science, law, and philosophy, but also artificial intelligence (AI). The \nvalue alignment problem expresses the challenge of (machine) learning values \nthat are, in some way, aligned with human requirements or values. In this paper \nI argue for looking at how humans have formalized and communicated values, in \nprofessional codes of ethics, and for exploring declarative decision-theoretic \nethical programs (DDTEP) to formalize codes of ethics. This renders machine \nethical reasoning and decision-making, as well as learning, more transparent \nand hopefully more accountable. The paper includes proof-of-concept examples of \nknown toy dilemmas and gatekeeping domains such as archives and libraries. \n</p>"}, "author": "Martijn van Otterlo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200634", "id": "tag:google.com,2005:reader/item/00000003353a2c96", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Enabling Reasoning with LegalRuleML. (arXiv:1711.06128v1 [cs.AI])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06128"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06128", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In order to automate verification process, regulatory rules written in \nnatural language needs to be translated into a format that machines can \nunderstand. However, none of the existing formalisms can fully represent the \nelements that appear in legal norms. For instance, most of these formalisms do \nnot provide features to capture the behavior of deontic effects, which is an \nimportant aspect in automated compliance checking. This paper presents an \napproach for transforming legal norms represented using LegalRuleML to a \nvariant of Modal Defeasible Logic (and vice versa) such that legal statement \nrepresented using LegalRuleML can be transformed into a machine readable format \nthat can be understand and reasoned about depending upon the client's \npreferences. \n</p>"}, "author": "Ho-Pun Lam, Mustafa Hashmi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200633", "id": "tag:google.com,2005:reader/item/00000003353a2c9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Robust Genetic Algorithm for Learning Temporal Specifications from Data. (arXiv:1711.06202v1 [cs.AI])", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06202"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06202", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of mining signal temporal logical requirements from a \ndataset of regular (good) and anomalous (bad) trajectories of a dynamical \nsystem. We assume the training set to be labeled by human experts and that we \nhave access only to a limited amount of data, typically noisy. We provide a \nsystematic approach to synthesize both the syntactical structure and the \nparameters of the temporal logic formula using a two-steps procedure: first, we \nleverage a novel evolutionary algorithm for learning the structure of the \nformula, second, we perform the parameter synthesis operating on the \nstatistical emulation of the average robustness for a candidate formula w.r.t. \nits parameters. We test our algorithm on a anomalous trajectory detection \nproblem of a naval surveillance system and we compare our results with our \nprevious work~\\cite{BufoBSBLB14} and with a recently proposed \ndecision-tree~\\cite{bombara_decision_2016} based method. Our experiments \nindicate that the proposed approach outperforms our previous work w.r.t. \naccuracy and show that it produces in general smaller and more compact temporal \nlogic specifications w.r.t. the decision-tree based approach with a comparable \nspeed and accuracy. \n</p>"}, "author": "Simone Silvetti, Laura Nenzi, Luca Bortolussi, Ezio Bartocci", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510995733201", "timestampUsec": "1510995733200623", "id": "tag:google.com,2005:reader/item/00000003353a2cb9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Three Factors Influencing Minima in SGD. (arXiv:1711.04623v1 [cs.LG] CROSS LISTED)", "published": 1510995733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04623"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04623", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the properties of the endpoint of stochastic gradient descent (SGD). \nBy approximating SGD as a stochastic differential equation (SDE) we consider \nthe Boltzmann-Gibbs equilibrium distribution of that SDE under the assumption \nof isotropic variance in loss gradients. Through this analysis, we find that \nthree factors - learning rate, batch size and the variance of the loss \ngradients - control the trade-off between the depth and width of the minima \nfound by SGD, with wider minima favoured by a higher ratio of learning rate to \nbatch size. We have direct control over the learning rate and batch size, while \nthe variance is determined by the choice of model architecture, model \nparameterization and dataset. In the equilibrium distribution only the ratio of \nlearning rate to batch size appears, implying that the equilibrium distribution \nis invariant under a simultaneous rescaling of learning rate and batch size by \nthe same amount. We then explore experimentally how learning rate and batch \nsize affect SGD from two perspectives: the endpoint of SGD and the dynamics \nthat lead up to it. For the endpoint, the experiments suggest the endpoint of \nSGD is invariant under simultaneous rescaling of batch size and learning rate, \nand also that a higher ratio leads to flatter minima, both findings are \nconsistent with our theoretical analysis. We note experimentally that the \ndynamics also seem to be invariant under the same rescaling of learning rate \nand batch size, which we explore showing that one can exchange batch size and \nlearning rate for cyclical learning rate schedule. Next, we illustrate how \nnoise affects memorization, showing that high noise levels lead to better \ngeneralization. Finally, we find experimentally that the invariance under \nsimultaneous rescaling of learning rate and batch size breaks down if the \nlearning rate gets too large or the batch size gets too small. \n</p>"}, "author": "Stanis&#x142;aw Jastrz&#x119;bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, Amos Storkey", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510964080747", "timestampUsec": "1510964080746612", "id": "tag:google.com,2005:reader/item/00000003350a134f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Chipmunk: A Systolically Scalable 0.9 mm${}^2$, 3.08 Gop/s/mW @ 1.2 mW Accelerator for Near-Sensor Recurrent Neural Network Inference. (arXiv:1711.05734v1 [cs.DC])", "published": 1510964081, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05734"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05734", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural networks (RNNs) are state-of-the-art in voice \nawareness/understanding and speech recognition. On-device computation of RNNs \non low-power mobile and wearable devices would be key to applications such as \nzero-latency voice-based human-machine interfaces. Here we present Chipmunk, a \nsmall (&lt;1 mm${}^2$) hardware accelerator for Long-Short Term Memory RNNs in UMC \n65 nm technology capable to operate at a measured peak efficiency up to 3.08 \nGop/s/mW at 1.24 mW peak power. To implement big RNN models without incurring \nin huge memory transfer overhead, multiple Chipmunk engines can cooperate to \nform a single systolic array. In this way, the Chipmunk architecture in a 75 \ntiles configuration can achieve real-time phoneme extraction on a demanding RNN \ntopology proposed by Graves et al., consuming less than 13 mW of average power. \n</p>"}, "author": "Francesco Conti, Lukas Cavigelli, Gianna Paulin, Igor Susmelj, Luca Benini", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510964080747", "timestampUsec": "1510964080746611", "id": "tag:google.com,2005:reader/item/00000003350a1365", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition. (arXiv:1711.05747v1 [cs.SD])", "published": 1510964081, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05747"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05747", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We investigate the effectiveness of generative adversarial networks (GANs) \nfor speech enhancement, in the context of improving noise robustness of \nautomatic speech recognition (ASR) systems. Prior work demonstrates that GANs \ncan effectively suppress additive noise in raw waveform speech signals, \nimproving perceptual quality metrics; however this technique was not justified \nin the context of ASR. In this work, we conduct a detailed study to measure the \neffectiveness of GANs in enhancing speech contaminated by both additive and \nreverberant noise. Motivated by recent advances in image processing, we propose \noperating GANs on log-Mel filterbank spectra instead of waveforms, which \nrequires less computation and is more robust to reverberant noise. While GAN \nenhancement improves the performance of a clean-trained ASR system on noisy \nspeech, it falls short of the performance achieved by conventional multi-style \ntraining (MTR). By appending the GAN-enhanced features to the noisy inputs and \nretraining, we achieve a 7% WER improvement relative to the MTR system. \n</p>"}, "author": "Chris Donahue, Bo Li, Rohit Prabhavalkar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510964080747", "timestampUsec": "1510964080746610", "id": "tag:google.com,2005:reader/item/00000003350a137a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models. (arXiv:1711.05772v1 [cs.LG])", "published": 1510964081, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05772"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05772", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c74c86f5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c74c86f5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep generative neural networks have proven effective at both conditional and \nunconditional modeling of complex data distributions. Conditional generation \nenables interactive control, but creating new controls often requires expensive \nretraining. In this paper, we develop a method to condition generation without \nretraining the model. By post-hoc learning latent constraints, value functions \nthat identify regions in latent space that generate outputs with desired \nattributes, we can conditionally sample from these regions with gradient-based \noptimization or amortized actor functions. Combining attribute constraints with \na universal \"realism\" constraint, which enforces similarity to the data \ndistribution, we generate realistic conditional images from an unconditional \nvariational autoencoder. Further, using gradient-based optimization, we \ndemonstrate identity-preserving transformations that make the minimal \nadjustment in latent space to modify the attributes of an image. Finally, with \ndiscrete sequences of musical notes, we demonstrate zero-shot conditional \ngeneration, learning latent constraints in the absence of labeled data or a \ndifferentiable reward function. Code with dedicated cloud instance has been \nmade publicly available (https://goo.gl/STGMGx). \n</p>"}, "author": "Jesse Engel, Matthew Hoffman, Adam Roberts", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510964080747", "timestampUsec": "1510964080746609", "id": "tag:google.com,2005:reader/item/00000003350a1388", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Finer Grained Entity Typing with TypeNet. (arXiv:1711.05795v1 [cs.CL])", "published": 1510964081, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05795"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05795", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the challenging problem of entity typing over an extremely fine \ngrained set of types, wherein a single mention or entity can have many \nsimultaneous and often hierarchically-structured types. Despite the importance \nof the problem, there is a relative lack of resources in the form of \nfine-grained, deep type hierarchies aligned to existing knowledge bases. In \nresponse, we introduce TypeNet, a dataset of entity types consisting of over \n1941 types organized in a hierarchy, obtained by manually annotating a mapping \nfrom 1081 Freebase types to WordNet. We also experiment with several models \ncomparable to state-of-the-art systems and explore techniques to incorporate a \nstructure loss on the hierarchy with the standard mention typing loss, as a \nfirst step towards future research on this dataset. \n</p>"}, "author": "Shikhar Murty, Patrick Verga, Luke Vilnis, Andrew McCallum", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510964080747", "timestampUsec": "1510964080746608", "id": "tag:google.com,2005:reader/item/00000003350a139d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy. (arXiv:1711.05852v1 [cs.LG])", "published": 1510964081, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05852"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05852", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning networks have achieved state-of-the-art accuracies on computer \nvision workloads like image classification and object detection. The performant \nsystems, however, typically involve big models with numerous parameters. Once \ntrained, a challenging aspect for such top performing models is deployment on \nresource constrained inference systems - the models (often deep networks or \nwide networks or both) are compute and memory intensive. Low-precision numerics \nand model compression using knowledge distillation are popular techniques to \nlower both the compute requirements and memory footprint of these deployed \nmodels. In this paper, we study the combination of these two techniques and \nshow that the performance of low-precision networks can be significantly \nimproved by using knowledge distillation techniques. Our approach, Apprentice, \nachieves state-of-the-art accuracies using ternary precision and 4-bit \nprecision for variants of ResNet architecture on ImageNet dataset. We present \nthree schemes using which one can apply knowledge distillation techniques to \nvarious stages of the train-and-deploy pipeline. \n</p>"}, "author": "Asit Mishra, Debbie Marr", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510964080747", "timestampUsec": "1510964080746607", "id": "tag:google.com,2005:reader/item/00000003350a13a8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A General Neural Network Hardware Architecture on FPGA. (arXiv:1711.05860v1 [cs.CV])", "published": 1510964081, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05860"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05860", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Field Programmable Gate Arrays (FPGAs) plays an increasingly important role \nin data sampling and processing industries due to its highly parallel \narchitecture, low power consumption, and flexibility in custom algorithms. \nEspecially, in the artificial intelligence field, for training and implement \nthe neural networks and machine learning algorithms, high energy efficiency \nhardware implement and massively parallel computing capacity are heavily \ndemanded. Therefore, many global companies have applied FPGAs into AI and \nMachine learning fields such as autonomous driving and Automatic Spoken \nLanguage Recognition (Baidu) [1] [2] and Bing search (Microsoft) [3]. \nConsidering the FPGAs great potential in these fields, we tend to implement a \ngeneral neural network hardware architecture on XILINX ZU9CG System On Chip \n(SOC) platform [4], which contains abundant hardware resource and powerful \nprocessing capacity. The general neural network architecture on the FPGA SOC \nplatform can perform forward and backward algorithms in deep neural networks \n(DNN) with high performance and easily be adjusted according to the type and \nscale of the neural networks. \n</p>"}, "author": "Yufeng Hao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510964080747", "timestampUsec": "1510964080746606", "id": "tag:google.com,2005:reader/item/00000003350a13b2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "On evolutionary selection of blackjack strategies. (arXiv:1711.05993v1 [cs.NE])", "published": 1510964081, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05993"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05993", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We apply the approach of evolutionary programming to the problem of \noptimization of the blackjack basic strategy. We demonstrate that the \npopulation of initially random blackjack strategies evolves and saturates to a \nprofitable performance in about one hundred generations. The resulting strategy \nresembles the known blackjack basic strategies in the specifics of its \nprescriptions, and has a similar performance. We also study evolution of the \npopulation of strategies initialized to the Thorp's basic strategy. \n</p>"}, "author": "Mikhail Goykhman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510964080747", "timestampUsec": "1510964080746605", "id": "tag:google.com,2005:reader/item/00000003350a13bb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hindsight policy gradients. (arXiv:1711.06006v1 [cs.LG])", "published": 1510964081, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06006"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06006", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Goal-conditional policies allow reinforcement learning agents to pursue \nspecific goals during different episodes. In addition to their potential to \ngeneralize desired behavior to unseen goals, such policies may also help in \ndefining options for arbitrary subgoals, enabling higher-level planning. While \ntrying to achieve a specific goal, an agent may also be able to exploit \ninformation about the degree to which it has achieved alternative goals. \nReinforcement learning agents have only recently been endowed with such \ncapacity for hindsight, which is highly valuable in environments with sparse \nrewards. In this paper, we show how hindsight can be introduced to \nlikelihood-ratio policy gradient methods, generalizing this capacity to an \nentire class of highly successful algorithms. Our preliminary experiments \nsuggest that hindsight may increase the sample efficiency of policy gradient \nmethods. \n</p>"}, "author": "Paulo Rauber, Filipe Mutz, Juergen Schmidhuber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510949253897", "timestampUsec": "1510949253896879", "id": "tag:google.com,2005:reader/item/0000000334ef390f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neurology-as-a-Service for the Developing World. (arXiv:1711.06195v1 [stat.ML])", "published": 1510949254, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06195"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06195", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Electroencephalography (EEG) is an extensively-used and well-studied \ntechnique in the field of medical diagnostics and treatment for brain \ndisorders, including epilepsy, migraines, and tumors. The analysis and \ninterpretation of EEGs require physicians to have specialized training, which \nis not common even among most doctors in the developed world, let alone the \ndeveloping world where physician shortages plague society. This problem can be \naddressed by teleEEG that uses remote EEG analysis by experts or by local \ncomputer processing of EEGs. However, both of these options are prohibitively \nexpensive and the second option requires abundant computing resources and \ninfrastructure, which is another concern in developing countries where there \nare resource constraints on capital and computing infrastructure. In this work, \nwe present a cloud-based deep neural network approach to provide decision \nsupport for non-specialist physicians in EEG analysis and interpretation. Named \n`neurology-as-a-service,' the approach requires almost no manual intervention \nin feature engineering and in the selection of an optimal architecture and \nhyperparameters of the neural network. In this study, we deploy a pipeline that \nincludes moving EEG data to the cloud and getting optimal models for various \nclassification tasks. Our initial prototype has been tested only in developed \nworld environments to-date, but our intention is to test it in developing world \nenvironments in future work. We demonstrate the performance of our proposed \napproach using the BCI2000 EEG MMI dataset, on which our service attains 63.4\\% \naccuracy for the task of classifying real vs.\\ imaginary activity performed by \nthe subject, which is significantly higher than what is obtained with a shallow \napproach such as support vector machines. \n</p>"}, "author": "Tejas Dharamsi, Payel Das, Tejaswini Pedapati, Gregory Bramble, Vinod Muthusamy, Horst Samulowitz, Kush R. Varshney. Yuvaraj Rajamanickam, John Thomas, Justin Dauwels", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510949253897", "timestampUsec": "1510949253896878", "id": "tag:google.com,2005:reader/item/0000000334ef39f3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Forward-Backward Approach for Visualizing Information Flow in Deep Networks. (arXiv:1711.06221v1 [stat.ML])", "published": 1510949254, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06221"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06221", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a new, systematic framework for visualizing information flow in \ndeep networks. Specifically, given any trained deep convolutional network model \nand a given test image, our method produces a compact support in the image \ndomain that corresponds to a (high-resolution) feature that contributes to the \ngiven explanation. Our method is both computationally efficient as well as \nnumerically robust. We present several preliminary numerical results that \nsupport the benefits of our framework over existing methods. \n</p>"}, "author": "Aditya Balu, Thanh V. Nguyen, Apurva Kokate, Chinmay Hegde, Soumik Sarkar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702086", "id": "tag:google.com,2005:reader/item/00000003345d6cac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Random gradient extrapolation for distributed and stochastic optimization. (arXiv:1711.05762v1 [math.OC])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05762"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05762", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we consider a class of finite-sum convex optimization problems \ndefined over a distributed multiagent network with $m$ agents connected to a \ncentral server. In particular, the objective function consists of the average \nof $m$ ($\\ge 1$) smooth components associated with each network agent together \nwith a strongly convex term. Our major contribution is to develop a new \nrandomized incremental gradient algorithm, namely random gradient extrapolation \nmethod (RGEM), which does not require any exact gradient evaluation even for \nthe initial point, but can achieve the optimal ${\\cal O}(\\log(1/\\epsilon))$ \ncomplexity bound in terms of the total number of gradient evaluations of \ncomponent functions to solve the finite-sum problems. Furthermore, we \ndemonstrate that for stochastic finite-sum optimization problems, RGEM \nmaintains the optimal ${\\cal O}(1/\\epsilon)$ complexity (up to a certain \nlogarithmic factor) in terms of the number of stochastic gradient computations, \nbut attains an ${\\cal O}(\\log(1/\\epsilon))$ complexity in terms of \ncommunication rounds (each round involves only one agent). It is worth noting \nthat the former bound is independent of the number of agents $m$, while the \nlatter one only linearly depends on $m$ or even $\\sqrt m$ for ill-conditioned \nproblems. To the best of our knowledge, this is the first time that these \ncomplexity bounds have been obtained for distributed and stochastic \noptimization problems. Moreover, our algorithms were developed based on a novel \ndual perspective of Nesterov's accelerated gradient method. \n</p>"}, "author": "Guanghui Lan, Yi Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702085", "id": "tag:google.com,2005:reader/item/00000003345d6cb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models. (arXiv:1711.05772v1 [cs.LG])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05772"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05772", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep generative neural networks have proven effective at both conditional and \nunconditional modeling of complex data distributions. Conditional generation \nenables interactive control, but creating new controls often requires expensive \nretraining. In this paper, we develop a method to condition generation without \nretraining the model. By post-hoc learning latent constraints, value functions \nthat identify regions in latent space that generate outputs with desired \nattributes, we can conditionally sample from these regions with gradient-based \noptimization or amortized actor functions. Combining attribute constraints with \na universal \"realism\" constraint, which enforces similarity to the data \ndistribution, we generate realistic conditional images from an unconditional \nvariational autoencoder. Further, using gradient-based optimization, we \ndemonstrate identity-preserving transformations that make the minimal \nadjustment in latent space to modify the attributes of an image. Finally, with \ndiscrete sequences of musical notes, we demonstrate zero-shot conditional \ngeneration, learning latent constraints in the absence of labeled data or a \ndifferentiable reward function. Code with dedicated cloud instance has been \nmade publicly available (https://goo.gl/STGMGx). \n</p>"}, "author": "Jesse Engel, Matthew Hoffman, Adam Roberts", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702084", "id": "tag:google.com,2005:reader/item/00000003345d6cb7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Distance for HMMs based on Aggregated Wasserstein Metric and State Registration. (arXiv:1711.05792v1 [cs.LG])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05792"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05792", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c74c8b7e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c74c8b7e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a framework, named Aggregated Wasserstein, for computing a \ndissimilarity measure or distance between two Hidden Markov Models with state \nconditional distributions being Gaussian. For such HMMs, the marginal \ndistribution at any time position follows a Gaussian mixture distribution, a \nfact exploited to softly match, aka register, the states in two HMMs. We refer \nto such HMMs as Gaussian mixture model-HMM (GMM-HMM). The registration of \nstates is inspired by the intrinsic relationship of optimal transport and the \nWasserstein metric between distributions. Specifically, the components of the \nmarginal GMMs are matched by solving an optimal transport problem where the \ncost between components is the Wasserstein metric for Gaussian distributions. \nThe solution of the optimization problem is a fast approximation to the \nWasserstein metric between two GMMs. The new Aggregated Wasserstein distance is \na semi-metric and can be computed without generating Monte Carlo samples. It is \ninvariant to relabeling or permutation of states. The distance is defined \nmeaningfully even for two HMMs that are estimated from data of different \ndimensionality, a situation that can arise due to missing variables. This \ndistance quantifies the dissimilarity of GMM-HMMs by measuring both the \ndifference between the two marginal GMMs and that between the two transition \nmatrices. Our new distance is tested on tasks of retrieval, classification, and \nt-SNE visualization of time series. Experiments on both synthetic and real data \nhave demonstrated its advantages in terms of accuracy as well as efficiency in \ncomparison with existing distances based on the Kullback-Leibler divergence. \n</p>"}, "author": "Yukun Chen, Jianbo Ye, Jia Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702083", "id": "tag:google.com,2005:reader/item/00000003345d6cc7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hierarchical Modeling of Seed Variety Yields and Decision Making for Future Planting Plans. (arXiv:1711.05809v1 [cs.LG])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05809"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05809", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7577ea7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7577ea7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Eradicating hunger and malnutrition is a key development goal of the 21st \ncentury. We address the problem of optimally identifying seed varieties to \nreliably increase crop yield within a risk-sensitive decision-making framework. \nSpecifically, we introduce a novel hierarchical machine learning mechanism for \npredicting crop yield (the yield of different seed varieties of the same crop). \nWe integrate this prediction mechanism with a weather forecasting model, and \npropose three different approaches for decision making under uncertainty to \nselect seed varieties for planting so as to balance yield maximization and \nrisk.We apply our model to the problem of soybean variety selection given in \nthe 2016 Syngenta Crop Challenge. Our prediction model achieves a median \nabsolute error of 3.74 bushels per acre and thus provides good estimates for \ninput into the decision models.Our decision models identify the selection of \nsoybean varieties that appropriately balance yield and risk as a function of \nthe farmer's risk aversion level. More generally, our models support farmers in \ndecision making about which seed varieties to plant. \n</p>"}, "author": "Huaiyang Zhong, Xiaocheng Li, David Lobell, Stefano Ermon, Margaret L. Brandeau", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702082", "id": "tag:google.com,2005:reader/item/00000003345d6cda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bootstrapped synthetic likelihood. (arXiv:1711.05825v1 [stat.CO])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05825"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05825", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The development of approximate Bayesian computation (ABC) and synthetic \nlikelihood (SL) techniques has enabled the use of Bayesian inference for models \nthat may be simulated, but for which the likelihood is not available to \nevaluate pointwise at values of an unknown parameter $\\theta$. The main idea in \nABC and SL is to, for different values of $\\theta$ (usually chosen using a \nMonte Carlo algorithm), build estimates of the likelihood based on simulations \nfrom the model conditional on $\\theta$. The quality of these estimates \ndetermines the efficiency of an ABC/SL algorithm. In standard ABC/SL, the only \nmeans to improve an estimated likelihood at $\\theta$ is to simulate more times \nfrom the model conditional on $\\theta$, which is infeasible in cases where the \nsimulator is computationally expensive. In this paper we describe how to use \nbootstrapping as a means for improving synthetic likelihood estimates whilst \nusing fewer simulations from the model, and also investigate its use in ABC. \nFurther, we investigate the use of the bag of little bootstraps as a means for \napplying this approach to large datasets, yielding to Monte Carlo algorithms \nthat accurately approximate posterior distributions whilst only simulating \nsubsamples of the full data. Examples of the approach applied to i.i.d., \ntemporal and spatial data are given. \n</p>"}, "author": "Richard G. Everitt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702081", "id": "tag:google.com,2005:reader/item/00000003345d6ce9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "BoostJet: Towards Combining Statistical Aggregates with Neural Embeddings for Recommendations. (arXiv:1711.05828v1 [cs.IR])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05828"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05828", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recommenders have become widely popular in recent years because of their \nbroader applicability in many e-commerce applications. These applications rely \non recommenders for generating advertisements for various offers or providing \ncontent recommendations. However, the quality of the generated recommendations \ndepends on user features (like demography, temporality), offer features (like \npopularity, price), and user-offer features (like implicit or explicit \nfeedback). Current state-of-the-art recommenders do not explore such diverse \nfeatures concurrently while generating the recommendations. \n</p> \n<p>In this paper, we first introduce the notion of Trackers which enables us to \ncapture the above-mentioned features and thus incorporate users' online \nbehaviour through statistical aggregates of different features (demography, \ntemporality, popularity, price). We also show how to capture offer-to-offer \nrelations, based on their consumption sequence, leveraging neural embeddings \nfor offers in our Offer2Vec algorithm. We then introduce BoostJet, a novel \nrecommender which integrates the Trackers along with the neural embeddings \nusing MatrixNet, an efficient distributed implementation of gradient boosted \ndecision tree, to improve the recommendation quality significantly. We provide \nan in-depth evaluation of BoostJet on Yandex's dataset, collecting online \nbehaviour from tens of millions of online users, to demonstrate the \npracticality of BoostJet in terms of recommendation quality as well as \nscalability. \n</p>"}, "author": "Rhicheek Patra, Egor Samosvat, Michael Roizner, Andrei Mishchenko", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702080", "id": "tag:google.com,2005:reader/item/00000003345d6cf4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Predictive Independence Testing, Predictive Conditional Independence Testing, and Predictive Graphical Modelling. (arXiv:1711.05869v1 [stat.ML])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05869"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05869", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Testing (conditional) independence of multivariate random variables is a task \ncentral to statistical inference and modelling in general - though \nunfortunately one for which to date there does not exist a practicable \nworkflow. State-of-art workflows suffer from the need for heuristic or \nsubjective manual choices, high computational complexity, or strong parametric \nassumptions. \n</p> \n<p>We address these problems by establishing a theoretical link between \nmultivariate/conditional independence testing, and model comparison in the \nmultivariate predictive modelling aka supervised learning task. This link \nallows advances in the extensively studied supervised learning workflow to be \ndirectly transferred to independence testing workflows - including automated \ntuning of machine learning type which addresses the need for a heuristic \nchoice, the ability to quantitatively trade-off computational demand with \naccuracy, and the modern black-box philosophy for checking and interfacing. \n</p> \n<p>As a practical implementation of this link between the two workflows, we \npresent a python package 'pcit', which implements our novel multivariate and \nconditional independence tests, interfacing the supervised learning API of the \nscikit-learn package. Theory and package also allow for straightforward \nindependence test based learning of graphical model structure. \n</p> \n<p>We empirically show that our proposed predictive independence test outperform \nor are on par to current practice, and the derived graphical model structure \nlearning algorithms asymptotically recover the 'true' graph. This paper, and \nthe 'pcit' package accompanying it, thus provide powerful, scalable, \ngeneralizable, and easy-to-use methods for multivariate and conditional \nindependence testing, as well as for graphical model structure learning. \n</p>"}, "author": "Samuel Burkart, Franz J Kir&#xe1;ly", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702079", "id": "tag:google.com,2005:reader/item/00000003345d6d06", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Budget-Constrained Multi-Armed Bandits with Multiple Plays. (arXiv:1711.05928v1 [cs.LG])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05928"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05928", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the multi-armed bandit problem with multiple plays and a budget \nconstraint for both the stochastic and the adversarial setting. At each round, \nexactly $K$ out of $N$ possible arms have to be played (with $1\\leq K \\leq N$). \nIn addition to observing the individual rewards for each arm played, the player \nalso learns a vector of costs which has to be covered with an a-priori defined \nbudget $B$. The game ends when the sum of current costs associated with the \nplayed arms exceeds the remaining budget. \n</p> \n<p>Firstly, we analyze this setting for the stochastic case, for which we assume \neach arm to have an underlying cost and reward distribution with support \n$[c_{\\min}, 1]$ and $[0, 1]$, respectively. We derive an Upper Confidence Bound \n(UCB) algorithm which achieves $O(NK^4 \\log B)$ regret. \n</p> \n<p>Secondly, for the adversarial case in which the entire sequence of rewards \nand costs is fixed in advance, we derive an upper bound on the regret of order \n$O(\\sqrt{NB\\log(N/K)})$ utilizing an extension of the well-known \n$\\texttt{Exp3}$ algorithm. We also provide upper bounds that hold with high \nprobability and a lower bound of order $\\Omega((1 - K/N)^2 \\sqrt{NB/K})$. \n</p>"}, "author": "Datong P. Zhou, Claire J. Tomlin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702078", "id": "tag:google.com,2005:reader/item/00000003345d6d0e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "HodgeRank with Information Maximization for Crowdsourced Pairwise Ranking Aggregation. (arXiv:1711.05957v1 [stat.ML])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05957"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05957", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recently, crowdsourcing has emerged as an effective paradigm for \nhuman-powered large scale problem solving in various domains. However, task \nrequester usually has a limited amount of budget, thus it is desirable to have \na policy to wisely allocate the budget to achieve better quality. In this \npaper, we study the principle of information maximization for active sampling \nstrategies in the framework of HodgeRank, an approach based on Hodge \nDecomposition of pairwise ranking data with multiple workers. The principle \nexhibits two scenarios of active sampling: Fisher information maximization that \nleads to unsupervised sampling based on a sequential maximization of graph \nalgebraic connectivity without considering labels; and Bayesian information \nmaximization that selects samples with the largest information gain from prior \nto posterior, which gives a supervised sampling involving the labels collected. \nExperiments show that the proposed methods boost the sampling efficiency as \ncompared to traditional sampling schemes and are thus valuable to practical \ncrowdsourcing experiments. \n</p>"}, "author": "Qianqian Xu, Jiechao Xiong, Xi Chen, Qingming Huang, Yuan Yao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702077", "id": "tag:google.com,2005:reader/item/00000003345d6d15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Deep Matching Autoencoders. (arXiv:1711.06047v1 [cs.CV])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06047"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06047", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Increasingly many real world tasks involve data in multiple modalities or \nviews. This has motivated the development of many effective algorithms for \nlearning a common latent space to relate multiple domains. However, most \nexisting cross-view learning algorithms assume access to paired data for \ntraining. Their applicability is thus limited as the paired data assumption is \noften violated in practice: many tasks have only a small subset of data \navailable with pairing annotation, or even no paired data at all. In this paper \nwe introduce Deep Matching Autoencoders (DMAE), which learn a common latent \nspace and pairing from unpaired multi-modal data. Specifically we formulate \nthis as a cross-domain representation learning and object matching problem. We \nsimultaneously optimise parameters of representation learning auto-encoders and \nthe pairing of unpaired multi-modal data. This framework elegantly spans the \nfull regime from fully supervised, semi-supervised, and unsupervised (no paired \ndata) multi-modal learning. We show promising results in image captioning, and \non a new task that is uniquely enabled by our methodology: unsupervised \nclassifier learning. \n</p>"}, "author": "Tanmoy Mukherjee, Makoto Yamada, Timothy M. Hospedales", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702076", "id": "tag:google.com,2005:reader/item/00000003345d6d19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gaussian Process Decentralized Data Fusion Meets Transfer Learning in Large-Scale Distributed Cooperative Perception. (arXiv:1711.06064v1 [stat.ML])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06064"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06064", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents novel Gaussian process decentralized data fusion \nalgorithms exploiting the notion of agent-centric support sets for distributed \ncooperative perception of large-scale environmental phenomena. To overcome the \nlimitations of scale in existing works, our proposed algorithms allow every \nmobile sensing agent to choose a different support set and dynamically switch \nto another during execution for encapsulating its own data into a local summary \nthat, perhaps surprisingly, can still be assimilated with the other agents' \nlocal summaries (i.e., based on their current choices of support sets) into a \nglobally consistent summary to be used for predicting the phenomenon. To \nachieve this, we propose a novel transfer learning mechanism for a team of \nagents capable of sharing and transferring information encapsulated in a \nsummary based on a support set to that utilizing a different support set with \nsome loss that can be theoretically bounded and analyzed. To alleviate the \nissue of information loss accumulating over multiple instances of transfer \nlearning, we propose a new information sharing mechanism to be incorporated \ninto our algorithms in order to achieve memory-efficient lazy transfer \nlearning. Empirical evaluation on real-world datasets show that our algorithms \noutperform the state-of-the-art methods. \n</p>"}, "author": "Ruofei Ouyang, Kian Hsiang Low", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702075", "id": "tag:google.com,2005:reader/item/00000003345d6d26", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Sequences, Items And Latent Links: Recommendation With Consumed Item Packs. (arXiv:1711.06100v1 [cs.IR])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06100"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06100", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recommenders personalize the web content by typically using collaborative \nfiltering to relate users (or items) based on explicit feedback, e.g., ratings. \nThe difficulty of collecting this feedback has recently motivated to consider \nimplicit feedback (e.g., item consumption along with the corresponding time). \n</p> \n<p>In this paper, we introduce the notion of consumed item pack (CIP) which \nenables to link users (or items) based on their implicit analogous consumption \nbehavior. Our proposal is generic, and we show that it captures three novel \nimplicit recommenders: a user-based (CIP-U), an item-based (CIP-I), and a word \nembedding-based (DEEPCIP), as well as a state-of-the-art technique using \nimplicit feedback (FISM). We show that our recommenders handle incremental \nupdates incorporating freshly consumed items. We demonstrate that all three \nrecommenders provide a recommendation quality that is competitive with \nstate-of-the-art ones, including one incorporating both explicit and implicit \nfeedback. \n</p>"}, "author": "Rachid Guerraoui, Erwan Le Merrer, Rhicheek Patra, Jean-Ronan Vigouroux", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702074", "id": "tag:google.com,2005:reader/item/00000003345d6d2e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A unified view of gradient-based attribution methods for Deep Neural Networks. (arXiv:1711.06104v1 [cs.LG])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06104"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06104", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7578111\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7578111&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Understanding the flow of information in Deep Neural Networks is a \nchallenging problem that has gain increasing attention over the last few years. \nWhile several methods have been proposed to explain network predictions, only \nfew attempts to analyze them from a theoretical perspective have been made in \nthe past. In this work we analyze various state-of-the-art attribution methods \nand prove unexplored connections between them. We also show how some methods \ncan be reformulated and more conveniently implemented. Finally, we perform an \nempirical evaluation with six attribution methods on a variety of tasks and \narchitectures and discuss their strengths and limitations. \n</p>"}, "author": "Marco Ancona, Enea Ceolini, Cengiz &#xd6;ztireli, Markus Gross", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702073", "id": "tag:google.com,2005:reader/item/00000003345d6d38", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Robust Unsupervised Domain Adaptation for Neural Networks via Moment Alignment. (arXiv:1711.06114v1 [stat.ML])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06114"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06114", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A novel approach for unsupervised domain adaptation for neural networks is \nproposed that relies on a metric-based regularization of the learning process. \nThe metric-based regularization aims at domain-invariant latent feature \nrepresentations by means of maximizing the similarity between domain-specific \nactivation distributions. The proposed metric results from modifying an \nintegral probability metric in a way such that it becomes translation-invariant \non a polynomial reproducing kernel Hilbert space. The metric has an intuitive \ninterpretation in the dual space as sum of differences of central moments of \nthe corresponding activation distributions. As demonstrated by an analysis on \nstandard benchmark datasets for sentiment analysis and object recognition the \noutlined approach shows more robustness \\wrt parameter changes than \nstate-of-the-art approaches while achieving even higher classification \naccuracies. \n</p>"}, "author": "Werner Zellinger, Bernhard A. Moser, Thomas Grubinger, Edwin Lughofer, Thomas Natschl&#xe4;ger, Susanne Saminger-Platz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702072", "id": "tag:google.com,2005:reader/item/00000003345d6d42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Beyond Sparsity: Tree Regularization of Deep Models for Interpretability. (arXiv:1711.06178v1 [stat.ML])", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06178"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06178", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The lack of interpretability remains a key barrier to the adoption of deep \nmodels in many applications. In this work, we explicitly regularize deep models \nso human users might step through the process behind their predictions in \nlittle time. Specifically, we train deep time-series models so their \nclass-probability predictions have high accuracy while being closely modeled by \ndecision trees with few nodes. Using intuitive toy examples as well as medical \ntasks for treating sepsis and HIV, we demonstrate that this new tree \nregularization yields models that are easier for humans to simulate than \nsimpler L1 or L2 penalties without sacrificing predictive power. \n</p>"}, "author": "Mike Wu, Michael C. Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, Finale Doshi-Velez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510882705702", "timestampUsec": "1510882705702061", "id": "tag:google.com,2005:reader/item/00000003345d6d6a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "One pixel attack for fooling deep neural networks. (arXiv:1710.08864v2 [cs.LG] UPDATED)", "published": 1510882706, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08864"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08864", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent research has revealed that the output of Deep Neural Networks (DNN) \ncan be easily altered by adding relatively small perturbations to the input \nvector. In this paper, we analyze an attack in an extremely limited scenario \nwhere only one pixel can be modified. For that we propose a novel method for \ngenerating one-pixel adversarial perturbations based on differential evolution. \nIt requires less adversarial information and can fool more types of networks. \nThe results show that 70.97% of the natural images can be perturbed to at least \none target class by modifying just one pixel with 97.47% confidence on average. \nThus, the proposed attack explores a different take on adversarial machine \nlearning in an extreme limited scenario, showing that current DNNs are also \nvulnerable to such low dimension attacks. \n</p>"}, "author": "Jiawei Su, Danilo Vasconcellos Vargas, Sakurai Kouichi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510876816046", "timestampUsec": "1510876816045967", "id": "tag:google.com,2005:reader/item/000000033453305d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep-ESN: A Multiple Projection-encoding Hierarchical Reservoir Computing Framework. (arXiv:1711.05255v1 [cs.LG])", "published": 1510876816, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05255"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05255", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>As an efficient recurrent neural network (RNN) model, reservoir computing \n(RC) models, such as Echo State Networks, have attracted widespread attention \nin the last decade. However, while they have had great success with time series \ndata [1], [2], many time series have a multiscale structure, which a \nsingle-hidden-layer RC model may have difficulty capturing. In this paper, we \npropose a novel hierarchical reservoir computing framework we call Deep Echo \nState Networks (Deep-ESNs). The most distinctive feature of a Deep-ESN is its \nability to deal with time series through hierarchical projections. \nSpecifically, when an input time series is projected into the high-dimensional \necho-state space of a reservoir, a subsequent encoding layer (e.g., a PCA, \nautoencoder, or a random projection) can project the echo-state representations \ninto a lower-dimensional space. These low-dimensional representations can then \nbe processed by another ESN. By using projection layers and encoding layers \nalternately in the hierarchical framework, a Deep-ESN can not only attenuate \nthe effects of the collinearity problem in ESNs, but also fully take advantage \nof the temporal kernel property of ESNs to explore multiscale dynamics of time \nseries. To fuse the multiscale representations obtained by each reservoir, we \nadd connections from each encoding layer to the last output layer. Theoretical \nanalyses prove that stability of a Deep-ESN is guaranteed by the echo state \nproperty (ESP), and the time complexity is equivalent to a conventional ESN. \nExperimental results on some artificial and real world time series demonstrate \nthat Deep-ESNs can capture multiscale dynamics, and outperform both standard \nESNs and previous hierarchical ESN-based models. \n</p>"}, "author": "Qianli Ma, Lifeng Shen, Garrison W. Cottrell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510876816046", "timestampUsec": "1510876816045966", "id": "tag:google.com,2005:reader/item/0000000334533076", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Revisiting Simple Neural Networks for Learning Representations of Knowledge Graphs. (arXiv:1711.05401v2 [cs.AI] UPDATED)", "published": 1511175748, "updated": 1511175750, "canonical": [{"href": "http://arxiv.org/abs/1711.05401"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05401", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We address the problem of learning vector representations for entities and \nrelations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This \nproblem has received significant attention in the past few years and multiple \nmethods have been proposed. Most of the existing methods in the literature use \na predefined characteristic scoring function for evaluating the correctness of \nKG triples. These scoring functions distinguish correct triples (high score) \nfrom incorrect ones (low score). However, their performance vary across \ndifferent datasets. In this work, we demonstrate that a simple neural network \nbased score function can consistently achieve near start-of-the-art performance \non multiple datasets. We also quantitatively demonstrate biases in standard \nbenchmark datasets, and highlight the need to perform evaluation spanning \nvarious datasets. \n</p>"}, "author": "Srinivas Ravishankar, Chandrahas, Partha Pratim Talukdar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510876816046", "timestampUsec": "1510876816045965", "id": "tag:google.com,2005:reader/item/0000000334533092", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "IKBT: solving closed-form Inverse Kinematics with Behavior Tree. (arXiv:1711.05412v1 [cs.RO])", "published": 1510876816, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05412"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05412", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Serial robot arms have complicated kinematic equations which must be solved \nto write effective arm planning and control software (the Inverse Kinematics \nProblem). Existing software packages for inverse kinematics often rely on \nnumerical methods which have significant shortcomings. Here we report a new \nsymbolic inverse kinematics solver which overcomes the limitations of numerical \nmethods, and the shortcomings of previous symbolic software packages. We \nintegrate Behavior Trees, an execution planning framework previously used for \ncontrolling intelligent robot behavior, to organize the equation solving \nprocess, and a modular architecture for each solution technique. The system \nsuccessfully solved, generated a LaTex report, and generated a Python code \ntemplate for 18 out of 19 example robots of 4-6 DOF. The system is readily \nextensible, maintainable, and multi-platform with few dependencies. The \ncomplete package is available with a Modified BSD license on Github. \n</p>"}, "author": "Dianmu Zhang, Blake Hannaford", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510876816046", "timestampUsec": "1510876816045964", "id": "tag:google.com,2005:reader/item/00000003345330ac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "TorusE: Knowledge Graph Embedding on a Lie Group. (arXiv:1711.05435v1 [cs.AI])", "published": 1510876816, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05435"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05435", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Knowledge graphs are useful for many artificial intelligence (AI) tasks. \nHowever, knowledge graphs often have missing facts. To populate the graphs, \nknowledge graph embedding models have been developed. Knowledge graph embedding \nmodels map entities and relations in a knowledge graph to a vector space and \npredict unknown triples by scoring candidate triples. TransE is the first \ntranslation-based method and it is well known because of its simplicity and \nefficiency for knowledge graph completion. It employs the principle that the \ndifferences between entity embeddings represent their relations. The principle \nseems very simple, but it can effectively capture the rules of a knowledge \ngraph. However, TransE has a problem with its regularization. TransE forces \nentity embeddings to be on a sphere in the embedding vector space. This \nregularization warps the embeddings and makes it difficult for them to fulfill \nthe abovementioned principle. The regularization also affects adversely the \naccuracies of the link predictions. On the other hand, regularization is \nimportant because entity embeddings diverge by negative sampling without it. \nThis paper proposes a novel embedding model, TorusE, to solve the \nregularization problem. The principle of TransE can be defined on any Lie \ngroup. A torus, which is one of the compact Lie groups, can be chosen for the \nembedding space to avoid regularization. To the best of our knowledge, TorusE \nis the first model that embeds objects on other than a real or complex vector \nspace, and this paper is the first to formally discuss the problem of \nregularization of TransE. Our approach outperforms other state-of-the-art \napproaches such as TransE, DistMult and ComplEx on a standard link prediction \ntask. We show that TorusE is scalable to large-size knowledge graphs and is \nfaster than the original TransE. \n</p>"}, "author": "Takuma Ebisu, Ryutaro Ichise", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510876816046", "timestampUsec": "1510876816045963", "id": "tag:google.com,2005:reader/item/00000003345330c4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hibikino-Musashi@Home 2017 Team Description Paper. (arXiv:1711.05457v1 [cs.RO])", "published": 1510876816, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05457"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05457", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Our team Hibikino-Musashi@Home was founded in 2010. It is based in Kitakyushu \nScience and Research Park, Japan. Since 2010, we have participated in the \nRoboCup@Home Japan open competition open-platform league every year. Currently, \nthe Hibikino-Musashi@Home team has 24 members from seven different laboratories \nbased in the Kyushu Institute of Technology. Our home-service robots are used \nas platforms for both education and implementation of our research outcomes. In \nthis paper, we introduce our team and the technologies that we have implemented \nin our robots. \n</p>"}, "author": "Sansei Hori, Yutaro Ishida, Yuta Kiyama, Yuichiro Tanaka, Yuki Kuroda, Masataka Hisano, Yuto Imamura, Tomotaka Himaki, Yuma Yoshimoto, Yoshiya Aratani, Kouhei Hashimoto, Gouki Iwamoto, Hiroto Fujita, Takashi Morie, Hakaru Tamukoh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510876816046", "timestampUsec": "1510876816045962", "id": "tag:google.com,2005:reader/item/00000003345330df", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Generally Applicable, Highly Scalable Measurement Computation and Optimization Approach to Sequential Model-Based Diagnosis. (arXiv:1711.05508v1 [cs.AI])", "published": 1510876816, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05508"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05508", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Model-Based Diagnosis deals with the identification of the real cause of a \nsystem's malfunction based on a formal system model and observations of the \nsystem behavior. When a malfunction is detected, there is usually not enough \ninformation available to pinpoint the real cause and one needs to discriminate \nbetween multiple fault hypotheses (called diagnoses). To this end, Sequential \nDiagnosis approaches ask an oracle for additional system measurements. \n</p> \n<p>This work presents strategies for (optimal) measurement selection in \nmodel-based sequential diagnosis. In particular, assuming a set of leading \ndiagnoses being given, we show how queries (sets of measurements) can be \ncomputed and optimized along two dimensions: expected number of queries and \ncost per query. By means of a suitable decoupling of two optimizations and a \nclever search space reduction the computations are done without any inference \nengine calls. For the full search space, we give a method requiring only a \npolynomial number of inferences and show how query properties can be guaranteed \nwhich existing methods do not provide. Evaluation results using real-world \nproblems indicate that the new method computes (virtually) optimal queries \ninstantly independently of the size and complexity of the considered diagnosis \nproblems and outperforms equally general methods not exploiting the proposed \ntheory by orders of magnitude. \n</p>"}, "author": "Patrick Rodler, Wolfgang Schmid, Konstantin Schekotihin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510876816046", "timestampUsec": "1510876816045961", "id": "tag:google.com,2005:reader/item/00000003345330f4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Note on Representing attribute reduction and concepts in concepts lattice using graphs. (arXiv:1711.05509v1 [cs.AI])", "published": 1510876816, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05509"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05509", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c757835f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c757835f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Mao H. (2017, Representing attribute reduction and concepts in concept \nlattice using graphs. Soft Computing 21(24):7293--7311) claims to make \ncontributions to the study of reduction of attributes in concept lattices by \nusing graph theory. We show that her results are either trivial or already \nwell-known and all three algorithms proposed in the paper are incorrect. \n</p>"}, "author": "Jan Konecny", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510876816046", "timestampUsec": "1510876816045960", "id": "tag:google.com,2005:reader/item/0000000334533108", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Good and safe uses of AI Oracles. (arXiv:1711.05541v1 [cs.AI])", "published": 1510876816, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05541"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05541", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c761c516\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c761c516&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>An Oracle is a design for potentially high power artificial intelligences \n(AIs), where the AI is made safe by restricting it to only answer questions. \nUnfortunately most designs cause the Oracle to be motivated to manipulate \nhumans with the contents of their answers, and Oracles of potentially high \nintelligence might be very successful at this. Solving the problem, without \ncompromising the accuracy of the answer, is tricky. This paper reduces the \nissue to a cryptographic-style problem of Alice ensuring that her Oracle \nanswers her questions while not providing key information to an eavesdropping \nEve. Two Oracle designs solve this problem, one counterfactual (the Oracle \nanswers as if it expected its answer to never be read) and one on-policy \n(limited by the quantity of information it can transmit). \n</p>"}, "author": "Stuart Armstrong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510876816046", "timestampUsec": "1510876816045959", "id": "tag:google.com,2005:reader/item/0000000334533115", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Phrase-based Image Captioning with Hierarchical LSTM Model. (arXiv:1711.05557v1 [cs.CV])", "published": 1510876816, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05557"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05557", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Automatic generation of caption to describe the content of an image has been \ngaining a lot of research interests recently, where most of the existing works \ntreat the image caption as pure sequential data. Natural language, however \npossess a temporal hierarchy structure, with complex dependencies between each \nsubsequence. In this paper, we propose a phrase-based hierarchical Long \nShort-Term Memory (phi-LSTM) model to generate image description. In contrast \nto the conventional solutions that generate caption in a pure sequential \nmanner, our proposed model decodes image caption from phrase to sentence. It \nconsists of a phrase decoder at the bottom hierarchy to decode noun phrases of \nvariable length, and an abbreviated sentence decoder at the upper hierarchy to \ndecode an abbreviated form of the image description. A complete image caption \nis formed by combining the generated phrases with sentence during the inference \nstage. Empirically, our proposed model shows a better or competitive result on \nthe Flickr8k, Flickr30k and MS-COCO datasets in comparison to the state-of-the \nart models. We also show that our proposed model is able to generate more novel \ncaptions (not seen in the training data) which are richer in word contents in \nall these three datasets. \n</p>"}, "author": "Ying Hua Tan, Chee Seng Chan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510876816046", "timestampUsec": "1510876816045958", "id": "tag:google.com,2005:reader/item/0000000334533123", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exploiting Layerwise Convexity of Rectifier Networks with Sign Constrained Weights. (arXiv:1711.05627v1 [cs.LG])", "published": 1510876816, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05627"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05627", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>By introducing sign constraints on the weights, this paper proposes sign \nconstrained rectifier networks (SCRNs), whose training can be solved \nefficiently by the well known majorization-minimization (MM) algorithms. We \nprove that the proposed two-hidden-layer SCRNs, which exhibit negative weights \nin the second hidden layer and negative weights in the output layer, are \ncapable of separating any two (or more) disjoint pattern sets. Furthermore, the \nproposed two-hidden-layer SCRNs can decompose the patterns of each class into \nseveral clusters so that each cluster is convexly separable from all the \npatterns from the other classes. This provides a means to learn the pattern \nstructures and analyse the discriminant factors between different classes of \npatterns. \n</p>"}, "author": "Senjian An, Farid Boussaid, Mohammed Bennamoun, Ferdous Sohel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510876816046", "timestampUsec": "1510876816045957", "id": "tag:google.com,2005:reader/item/000000033453314d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems. (arXiv:1711.05715v1 [cs.AI])", "published": 1510876816, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05715"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05715", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a new algorithm that significantly improves the efficiency of \nexploration for deep Q-learning agents in dialogue systems. Our agents explore \nvia Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop \nneural network. Our algorithm learns much faster than common exploration \nstrategies such as \\epsilon-greedy, Boltzmann, bootstrapping, and \nintrinsic-reward-based ones. Additionally, we show that spiking the replay \nbuffer with experiences from just a few successful episodes can make Q-learning \nfeasible when it might otherwise fail. \n</p>"}, "author": "Zachary Lipton, Xiujun Li, Jianfeng Gao, Lihong Li, Faisal Ahmed, Li Deng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510876816046", "timestampUsec": "1510876816045956", "id": "tag:google.com,2005:reader/item/0000000334533175", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Markov Decision Processes with Continuous Side Information. (arXiv:1711.05726v1 [stat.ML])", "published": 1510876816, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05726"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05726", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider a reinforcement learning (RL) setting in which the agent \ninteracts with a sequence of episodic MDPs. At the start of each episode the \nagent has access to some side-information or context that determines the \ndynamics of the MDP for that episode. Our setting is motivated by applications \nin healthcare where baseline measurements of a patient at the start of a \ntreatment episode form the context that may provide information about how the \npatient might respond to treatment decisions. We propose algorithms for \nlearning in such Contextual Markov Decision Processes (CMDPs) under an \nassumption that the unobserved MDP parameters vary smoothly with the observed \ncontext. We also give lower and upper PAC bounds under the smoothness \nassumption. Because our lower bound has an exponential dependence on the \ndimension, we consider a tractable linear setting where the context is used to \ncreate linear combinations of a finite set of MDPs. For the linear setting, we \ngive a PAC learning algorithm based on KWIK learning techniques. \n</p>"}, "author": "Aditya Modi, Nan Jiang, Satinder Singh, Ambuj Tewari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510876816046", "timestampUsec": "1510876816045948", "id": "tag:google.com,2005:reader/item/0000000334533216", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Artificial Intelligence as Structural Estimation: Economic Interpretations of Deep Blue, Bonanza, and AlphaGo. (arXiv:1710.10967v2 [econ.EM] UPDATED)", "published": 1510876816, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10967"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10967", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Artificial intelligence (AI) has achieved superhuman performance in a growing \nnumber of tasks, including the classical games of chess, shogi, and Go, but \nunderstanding and explaining AI remain challenging. This paper studies the \nmachine-learning algorithms for developing the game AIs, and provides their \nstructural interpretations. Specifically, chess-playing Deep Blue is a \ncalibrated value function, whereas shogi-playing Bonanza represents an \nestimated value function via Rust's (1987) nested fixed-point method. AlphaGo's \n\"supervised-learning policy network\" is a deep neural network (DNN) version of \nHotz and Miller's (1993) conditional choice probability estimates; its \n\"reinforcement-learning value network\" is equivalent to Hotz, Miller, Sanders, \nand Smith's (1994) simulation method for estimating the value function. Their \nperformances suggest DNNs are a useful functional form when the state space is \nlarge and data are sparse. Explicitly incorporating strategic interactions and \nunobserved heterogeneity in the data-generating process would further improve \nAIs' explicability. \n</p>"}, "author": "Mitsuru Igami", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411189", "id": "tag:google.com,2005:reader/item/00000003343038c5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On Optimal Generalizability in Parametric Learning. (arXiv:1711.05323v1 [stat.ML])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05323"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05323", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the parametric learning problem, where the objective of the \nlearner is determined by a parametric loss function. Employing empirical risk \nminimization with possibly regularization, the inferred parameter vector will \nbe biased toward the training samples. Such bias is measured by the cross \nvalidation procedure in practice where the data set is partitioned into a \ntraining set used for training and a validation set, which is not used in \ntraining and is left to measure the out-of-sample performance. A classical \ncross validation strategy is the leave-one-out cross validation (LOOCV) where \none sample is left out for validation and training is done on the rest of the \nsamples that are presented to the learner, and this process is repeated on all \nof the samples. LOOCV is rarely used in practice due to the high computational \ncomplexity. In this paper, we first develop a computationally efficient \napproximate LOOCV (ALOOCV) and provide theoretical guarantees for its \nperformance. Then we use ALOOCV to provide an optimization algorithm for \nfinding the regularizer in the empirical risk minimization framework. In our \nnumerical experiments, we illustrate the accuracy and efficiency of ALOOCV as \nwell as our proposed framework for the optimization of the regularizer. \n</p>"}, "author": "Ahmad Beirami, Meisam Razaviyayn, Shahin Shahrampour, Vahid Tarokh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411188", "id": "tag:google.com,2005:reader/item/00000003343038da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Automatic Conflict Detection in Police Body-Worn Video. (arXiv:1711.05355v1 [eess.AS])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05355"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05355", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Automatic conflict detection has grown in relevance with the advent of \nbody-worn technology, but existing metrics such as turn-taking and overlap are \npoor indicators of conflict in police-public interactions. Moreover, standard \ntechniques to compute them fall short when applied to such diversified and \nnoisy contexts. We develop a pipeline catered to this task combining adaptive \nnoise removal, non-speech filtering and new measures of conflict based on the \nrepetition and intensity of phrases in speech. We demonstrate the effectiveness \nof our approach on body-worn audio data collected by the Los Angeles Police \nDepartment. \n</p>"}, "author": "Alistair Letcher, Jelena Tri&#x161;ovi&#x107;, Collin Cademartori, Xi Chen, Jason Xu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411187", "id": "tag:google.com,2005:reader/item/00000003343038e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Kernel Conditional Exponential Family. (arXiv:1711.05363v1 [stat.ML])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05363"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05363", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A nonparametric family of conditional distributions is introduced, which \ngeneralizes conditional exponential families using functional parameters in a \nsuitable RKHS. An algorithm is provided for learning the generalized natural \nparameter, and consistency of the estimator is established in the well \nspecified case. In experiments, the new method generally outperforms a \ncompeting approach with consistency guarantees, and is competitive with a deep \nconditional density model on datasets that exhibit abrupt transitions and \nheteroscedasticity. \n</p>"}, "author": "Michael Arbel, Arthur Gretton", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411186", "id": "tag:google.com,2005:reader/item/00000003343038fd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "LIUBoost : Locality Informed Underboosting for Imbalanced Data Classification. (arXiv:1711.05365v1 [cs.LG])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05365"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05365", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c761c885\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c761c885&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The problem of class imbalance along with class-overlapping has become a \nmajor issue in the domain of supervised learning. Most supervised learning \nalgorithms assume equal cardinality of the classes under consideration while \noptimizing the cost function and this assumption does not hold true for \nimbalanced datasets which results in sub-optimal classification. Therefore, \nvarious approaches, such as undersampling, oversampling, cost-sensitive \nlearning and ensemble based methods have been proposed for dealing with \nimbalanced datasets. However, undersampling suffers from information loss, \noversampling suffers from increased runtime and potential overfitting while \ncost-sensitive methods suffer due to inadequately defined cost assignment \nschemes. In this paper, we propose a novel boosting based method called \nLIUBoost. LIUBoost uses under sampling for balancing the datasets in every \nboosting iteration like RUSBoost while incorporating a cost term for every \ninstance based on their hardness into the weight update formula minimizing the \ninformation loss introduced by undersampling. LIUBoost has been extensively \nevaluated on 18 imbalanced datasets and the results indicate significant \nimprovement over existing best performing method RUSBoost. \n</p>"}, "author": "Sajid Ahmed, Farshid Rayhan, Asif Mahbub, Md. Rafsan Jani, Swakkhar Shatabda, Dewan Md. Farid, Chowdhury Mofizur Rahman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411185", "id": "tag:google.com,2005:reader/item/0000000334303911", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimizing Kernel Machines using Deep Learning. (arXiv:1711.05374v1 [stat.ML])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05374"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05374", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Building highly non-linear and non-parametric models is central to several \nstate-of-the-art machine learning systems. Kernel methods form an important \nclass of techniques that induce a reproducing kernel Hilbert space (RKHS) for \ninferring non-linear models through the construction of similarity functions \nfrom data. These methods are particularly preferred in cases where the training \ndata sizes are limited and when prior knowledge of the data similarities is \navailable. Despite their usefulness, they are limited by the computational \ncomplexity and their inability to support end-to-end learning with a \ntask-specific objective. On the other hand, deep neural networks have become \nthe de facto solution for end-to-end inference in several learning paradigms. \nIn this article, we explore the idea of using deep architectures to perform \nkernel machine optimization, for both computational efficiency and end-to-end \ninferencing. To this end, we develop the DKMO (Deep Kernel Machine \nOptimization) framework, that creates an ensemble of dense embeddings using \nNystrom kernel approximations and utilizes deep learning to generate \ntask-specific representations through the fusion of the embeddings. \nIntuitively, the filters of the network are trained to fuse information from an \nensemble of linear subspaces in the RKHS. Furthermore, we introduce the kernel \ndropout regularization to enable improved training convergence. Finally, we \nextend this framework to the multiple kernel case, by coupling a global fusion \nlayer with pre-trained deep kernel machines for each of the constituent \nkernels. Using case studies with limited training data, and lack of explicit \nfeature sources, we demonstrate the effectiveness of our framework over \nconventional model inferencing techniques. \n</p>"}, "author": "Huan Song, Jayaraman J. Thiagarajan, Prasanna Sattigeri, Andreas Spanias", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411184", "id": "tag:google.com,2005:reader/item/000000033430391f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sliced Wasserstein Distance for Learning Gaussian Mixture Models. (arXiv:1711.05376v1 [cs.CV])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05376"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05376", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Gaussian mixture models (GMM) are powerful parametric tools with many \napplications in machine learning and computer vision. Expectation maximization \n(EM) is the most popular algorithm for estimating the GMM parameters. However, \nEM guarantees only convergence to a stationary point of the log-likelihood \nfunction, which could be arbitrarily worse than the optimal solution. Inspired \nby the relationship between the negative log-likelihood function and the \nKullback-Leibler (KL) divergence, we propose an alternative formulation for \nestimating the GMM parameters using the sliced Wasserstein distance, which \ngives rise to a new algorithm. Specifically, we propose minimizing the \nsliced-Wasserstein distance between the mixture model and the data distribution \nwith respect to the GMM parameters. In contrast to the KL-divergence, the \nenergy landscape for the sliced-Wasserstein distance is more well-behaved and \ntherefore more suitable for a stochastic gradient descent scheme to obtain the \noptimal GMM parameters. We show that our formulation results in parameter \nestimates that are more robust to random initializations and demonstrate that \nit can estimate high-dimensional data distributions more faithfully than the EM \nalgorithm. \n</p>"}, "author": "Soheil Kolouri, Gustavo K. Rohde, Heiko Hoffman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411183", "id": "tag:google.com,2005:reader/item/0000000334303925", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Semiblind subgraph reconstruction in Gaussian graphical models. (arXiv:1711.05391v1 [cs.LG])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05391"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05391", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Consider a social network where only a few nodes (agents) have meaningful \ninteractions in the sense that the conditional dependency graph over node \nattribute variables (behaviors) is sparse. A company that can only observe the \ninteractions between its own customers will generally not be able to accurately \nestimate its customers' dependency subgraph: it is blinded to any external \ninteractions of its customers and this blindness creates false edges in its \nsubgraph. In this paper we address the semiblind scenario where the company has \naccess to a noisy summary of the complementary subgraph connecting external \nagents, e.g., provided by a consolidator. The proposed framework applies to \nother applications as well, including field estimation from a network of awake \nand sleeping sensors and privacy-constrained information sharing over social \nsubnetworks. We propose a penalized likelihood approach in the context of a \ngraph signal obeying a Gaussian graphical models (GGM). We use a convex-concave \niterative optimization algorithm to maximize the penalized likelihood. \n</p>"}, "author": "Tianpei Xie, Sijia Liu, Alfred O. Hero III", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411182", "id": "tag:google.com,2005:reader/item/0000000334303934", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Revisiting Simple Neural Networks for Learning Representations of Knowledge Graphs. (arXiv:1711.05401v2 [cs.AI] UPDATED)", "published": 1511218849, "updated": 1511218855, "canonical": [{"href": "http://arxiv.org/abs/1711.05401"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05401", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We address the problem of learning vector representations for entities and \nrelations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This \nproblem has received significant attention in the past few years and multiple \nmethods have been proposed. Most of the existing methods in the literature use \na predefined characteristic scoring function for evaluating the correctness of \nKG triples. These scoring functions distinguish correct triples (high score) \nfrom incorrect ones (low score). However, their performance vary across \ndifferent datasets. In this work, we demonstrate that a simple neural network \nbased score function can consistently achieve near start-of-the-art performance \non multiple datasets. We also quantitatively demonstrate biases in standard \nbenchmark datasets, and highlight the need to perform evaluation spanning \nvarious datasets. \n</p>"}, "author": "Srinivas Ravishankar, Chandrahas, Partha Pratim Talukdar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411181", "id": "tag:google.com,2005:reader/item/0000000334303941", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Influential Sample Selection: A Graph Signal Processing Approach. (arXiv:1711.05407v1 [stat.ML])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05407"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05407", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the growing complexity of machine learning techniques, understanding the \nfunctioning of black-box models is more important than ever. A recently popular \nstrategy towards interpretability is to generate explanations based on examples \n-- called influential samples -- that have the largest influence on the model's \nobserved behavior. However, for such an analysis, we are confronted with a \nplethora of influence metrics. While each of these metrics provide varying \nlevels of representativeness and diversity, existing approaches implicitly \ncouple the definition of influence to their sample selection algorithm, thereby \nmaking it challenging to generalize to specific analysis needs. In this paper, \nwe propose a generic approach to influential sample selection, which analyzes \nthe influence metric as a function on a graph constructed using the samples. We \nshow that samples which are critical to recovering the high-frequency content \nof the function correspond to the most influential samples. Our approach \ndecouples the influence metric from the actual sample selection technique, and \nhence can be used with any type of task-specific influence. Using experiments \nin prototype selection, and semi-supervised classification, we show that, even \nwith popularly used influence metrics, our approach can produce superior \nresults in comparison to state-of-the-art approaches. Furthermore, we \ndemonstrate how a novel influence metric can be used to recover the influence \nstructure in characterizing the decision surface, and recovering corrupted \nlabels efficiently. \n</p>"}, "author": "Rushil Anirudh, Jayaraman J. Thiagarajan, Rahul Sridhar, Timo Bremer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411180", "id": "tag:google.com,2005:reader/item/0000000334303945", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Z-Forcing: Training Stochastic Recurrent Networks. (arXiv:1711.05411v1 [stat.ML])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05411"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05411", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many efforts have been devoted to training generative latent variable models \nwith autoregressive decoders, such as recurrent neural networks (RNN). \nStochastic recurrent models have been successful in capturing the variability \nobserved in natural sequential data such as speech. We unify successful ideas \nfrom recently proposed architectures into a stochastic recurrent model: each \nstep in the sequence is associated with a latent variable that is used to \ncondition the recurrent dynamics for future steps. Training is performed with \namortized variational inference where the approximate posterior is augmented \nwith a RNN that runs backward through the sequence. In addition to maximizing \nthe variational lower bound, we ease training of the latent variables by adding \nan auxiliary cost which forces them to reconstruct the state of the backward \nrecurrent network. This provides the latent variables with a task-independent \nobjective that enhances the performance of the overall model. We found this \nstrategy to perform better than alternative approaches such as KL annealing. \nAlthough being conceptually simple, our model achieves state-of-the-art results \non standard speech benchmarks such as TIMIT and Blizzard and competitive \nperformance on sequential MNIST. Finally, we apply our model to language \nmodeling on the IMDB dataset where the auxiliary cost helps in learning \ninterpretable latent variables. Source Code: \n\\url{https://github.com/anirudh9119/zforcing_nips17} \n</p>"}, "author": "Anirudh Goyal, Alessandro Sordoni, Marc-Alexandre C&#xf4;t&#xe9;, Nan Rosemary Ke, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411179", "id": "tag:google.com,2005:reader/item/000000033430399a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Accelerating Cross-Validation in Multinomial Logistic Regression with $\\ell_1$-Regularization. (arXiv:1711.05420v1 [stat.ML])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05420"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05420", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop an approximate formula for evaluating a cross-validation estimator \nof predictive likelihood for multinomial logistic regression regularized by an \n$\\ell_1$-norm. This allows us to avoid repeated optimizations required for \nliterally conducting cross-validation; hence, the computational time can be \nsignificantly reduced. The formula is derived through a perturbative approach \nemploying the largeness of the data size and the model dimensionality. Its \nusefulness is demonstrated on simulated data and the ISOLET dataset from the \nUCI machine learning repository. \n</p>"}, "author": "Tomoyuki Obuchi, Yoshiyuki Kabashima", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411178", "id": "tag:google.com,2005:reader/item/00000003343039bd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The landscape of the spiked tensor model. (arXiv:1711.05424v1 [math.ST])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05424"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05424", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of estimating a large rank-one tensor ${\\boldsymbol \nu}^{\\otimes k}\\in({\\mathbb R}^{n})^{\\otimes k}$, $k\\ge 3$ in Gaussian noise. \nEarlier work characterized a critical signal-to-noise ratio $\\lambda_{Bayes}= \nO(1)$ above which an ideal estimator achieves strictly positive correlation \nwith the unknown vector of interest. Remarkably no polynomial-time algorithm is \nknown that achieved this goal unless $\\lambda\\ge C n^{(k-2)/4}$ and even \npowerful semidefinite programming relaxations appear to fail for $1\\ll \n\\lambda\\ll n^{(k-2)/4}$. \n</p> \n<p>In order to elucidate this behavior, we consider the maximum likelihood \nestimator, which requires maximizing a degree-$k$ homogeneous polynomial over \nthe unit sphere in $n$ dimensions. We compute the expected number of critical \npoints and local maxima of this objective function and show that it is \nexponential in the dimensions $n$, and give exact formulas for the exponential \ngrowth rate. We show that (for $\\lambda$ larger than a constant) critical \npoints are either very close to the unknown vector ${\\boldsymbol u}$, or are \nconfined in a band of width $\\Theta(\\lambda^{-1/(k-1)})$ around the maximum \ncircle that is orthogonal to ${\\boldsymbol u}$. For local maxima, this band \nshrinks to be of size $\\Theta(\\lambda^{-1/(k-2)})$. These `uninformative' local \nmaxima are likely to cause the failure of optimization algorithms. \n</p>"}, "author": "Gerard Ben Arous, Song Mei, Andrea Montanari, Mihai Nica", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411177", "id": "tag:google.com,2005:reader/item/0000000334303a1c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Lattice Rescoring Strategies for Long Short Term Memory Language Models in Speech Recognition. (arXiv:1711.05448v1 [stat.ML])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05448"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05448", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural network (RNN) language models (LMs) and Long Short Term \nMemory (LSTM) LMs, a variant of RNN LMs, have been shown to outperform \ntraditional N-gram LMs on speech recognition tasks. However, these models are \ncomputationally more expensive than N-gram LMs for decoding, and thus, \nchallenging to integrate into speech recognizers. Recent research has proposed \nthe use of lattice-rescoring algorithms using RNNLMs and LSTMLMs as an \nefficient strategy to integrate these models into a speech recognition system. \nIn this paper, we evaluate existing lattice rescoring algorithms along with new \nvariants on a YouTube speech recognition task. Lattice rescoring using LSTMLMs \nreduces the word error rate (WER) for this task by 8\\% relative to the WER \nobtained using an N-gram LM. \n</p>"}, "author": "Shankar Kumar, Michael Nirschl, Daniel Holtmann-Rice, Hank Liao, Ananda Theertha Suresh, Felix Yu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411176", "id": "tag:google.com,2005:reader/item/0000000334303a61", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Convex Parametrization of a New Class of Universal Kernel Functions for use in Kernel Learning. (arXiv:1711.05477v1 [stat.ML])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05477"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05477", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c761ccdb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c761ccdb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a new class of universal kernel functions which admit a linear \nparametrization using positive semidefinite matrices. These kernels are \ngeneralizations of the Sobolev kernel and are defined by piecewise-polynomial \nfunctions. The class of kernels is termed \"tessellated\" as the resulting \ndiscriminant is defined piecewise with hyper-rectangular domains whose corners \nare determined by the training data. The kernels have scalable complexity, but \neach instance is universal in the sense that its hypothesis space is dense in \n$L_2$. Using numerical testing, we show that for the soft margin SVM, this \nclass can eliminate the need for Gaussian kernels. Furthermore, we demonstrate \nthat when the ratio of the number of training data to features is high, this \nmethod will significantly outperform other kernel learning algorithms. Finally, \nto reduce the complexity associated with SDP-based kernel learning methods, we \nuse a randomized basis for the positive matrices to integrate with existing \nmultiple kernel learning algorithms such as SimpleMKL. \n</p>"}, "author": "Brendon K. Colbert, Matthew M. Peet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411175", "id": "tag:google.com,2005:reader/item/0000000334303a7c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficient Estimation of Generalization Error and Bias-Variance Components of Ensembles. (arXiv:1711.05482v1 [cs.LG])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05482"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05482", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c76a7dd0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c76a7dd0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>For many applications, an ensemble of base classifiers is an effective \nsolution. The tuning of its parameters(number of classes, amount of data on \nwhich each classifier is to be trained on, etc.) requires G, the generalization \nerror of a given ensemble. The efficient estimation of G is the focus of this \npaper. The key idea is to approximate the variance of the class \nscores/probabilities of the base classifiers over the randomness imposed by the \ntraining subset by normal/beta distribution at each point x in the input \nfeature space. We estimate the parameters of the distribution using a small set \nof randomly chosen base classifiers and use those parameters to give efficient \nestimation schemes for G. We give empirical evidence for the quality of the \nvarious estimators. We also demonstrate their usefulness in making design \nchoices such as the number of classifiers in the ensemble and the size of a \nsubset of data used for training that is needed to achieve a certain value of \ngeneralization error. Our approach also has great potential for designing \ndistributed ensemble classifiers. \n</p>"}, "author": "Dhruv Mahajan, Vivek Gupta, S Sathiya Keerthi, Sellamanickam Sundararajan, Shravan Narayanamurthy, Rahul Kidambi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411174", "id": "tag:google.com,2005:reader/item/0000000334303a9d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sound Event Detection in Synthetic Audio: Analysis of the DCASE 2016 Task Results. (arXiv:1711.05551v1 [eess.AS])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05551"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05551", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>As part of the 2016 public evaluation challenge on Detection and \nClassification of Acoustic Scenes and Events (DCASE 2016), the second task \nfocused on evaluating sound event detection systems using synthetic mixtures of \noffice sounds. This task, which follows the `Event Detection - Office \nSynthetic' task of DCASE 2013, studies the behaviour of tested algorithms when \nfacing controlled levels of audio complexity with respect to background noise \nand polyphony/density, with the added benefit of a very accurate ground truth. \nThis paper presents the task formulation, evaluation metrics, submitted \nsystems, and provides a statistical analysis of the results achieved, with \nrespect to various aspects of the evaluation dataset. \n</p>"}, "author": "Gr&#xe9;goire Lafay (1), Emmanouil Benetos (2), Mathieu Lagrange (3) ((1) IRCCyN, (2) QMUL, (3) LS2N)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411173", "id": "tag:google.com,2005:reader/item/0000000334303ada", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variational Adaptive-Newton Method for Explorative Learning. (arXiv:1711.05560v1 [stat.ML])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05560"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05560", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present the Variational Adaptive Newton (VAN) method which is a black-box \noptimization method especially suitable for explorative-learning tasks such as \nactive learning and reinforcement learning. Similar to Bayesian methods, VAN \nestimates a distribution that can be used for exploration, but requires \ncomputations that are similar to continuous optimization methods. Our \ntheoretical contribution reveals that VAN is a second-order method that unifies \nexisting methods in distinct fields of continuous optimization, variational \ninference, and evolution strategies. Our experimental results show that VAN \nperforms well on a wide-variety of learning tasks. This work presents a \ngeneral-purpose explorative-learning method that has the potential to improve \nlearning in areas such as active learning and reinforcement learning. \n</p>"}, "author": "Mohammad Emtiyaz Khan, Wu Lin, Voot Tangkaratt, Zuozhu Liu, Didrik Nielsen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411172", "id": "tag:google.com,2005:reader/item/0000000334303af4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Advances in Variational Inference. (arXiv:1711.05597v1 [cs.LG])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05597"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05597", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many modern unsupervised or semi-supervised machine learning algorithms rely \non Bayesian probabilistic models. These models are usually intractable and thus \nrequire approximate inference. Variational inference (VI) lets us approximate a \nhigh-dimensional Bayesian posterior with a simpler variational distribution by \nsolving an optimization problem. This approach has been successfully used in \nvarious models and large-scale applications. In this review, we give an \noverview of recent trends in variational inference. We first introduce standard \nmean field variational inference, then review recent advances focusing on the \nfollowing aspects: (a) scalable VI, which includes stochastic approximations, \n(b) generic VI, which extends the applicability of VI to a large class of \notherwise intractable models, such as non-conjugate models, (c) accurate VI, \nwhich includes variational models beyond the mean field approximation or with \natypical divergences, and (d) amortized VI, which implements the inference over \nlocal latent variables with inference networks. Finally, we provide a summary \nof promising future research directions. \n</p>"}, "author": "Cheng Zhang, Judith Butepage, Hedvig Kjellstrom, Stephan Mandt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411171", "id": "tag:google.com,2005:reader/item/0000000334303b01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On consistent vertex nomination schemes. (arXiv:1711.05610v1 [stat.ML])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05610"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05610", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Given a vertex of interest in a network $G_1$, the vertex nomination problem \nseeks to find the corresponding vertex of interest (if it exists) in a second \nnetwork $G_2$. Although the vertex nomination problem and related tasks have \nattracted much attention in the machine learning literature, with applications \nto social and biological networks, the framework has so far been confined to a \ncomparatively small class of network models, and the concept of statistically \nconsistent vertex nomination schemes has been only shallowly explored. In this \npaper, we extend the vertex nomination problem to a very general statistical \nmodel of graphs. Further, drawing inspiration from the long-established \nclassification framework in the pattern recognition literature, we provide \ndefinitions for the key notions of Bayes optimality and consistency in our \nextended vertex nomination framework, including a derivation of the Bayes \noptimal vertex nomination scheme. In addition, we prove that no universally \nconsistent vertex nomination schemes exist. Illustrative examples are provided \nthroughout. \n</p>"}, "author": "Vince Lyzinski, Keith Levin, Carey E. Priebe", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411170", "id": "tag:google.com,2005:reader/item/0000000334303b07", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Spatial Mapping with Gaussian Processes and Nonstationary Fourier Features. (arXiv:1711.05615v1 [stat.ML])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05615"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05615", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The use of covariance kernels is ubiquitous in the field of spatial \nstatistics. Kernels allow data to be mapped into high-dimensional feature \nspaces and can thus extend simple linear additive methods to nonlinear methods \nwith higher order interactions. However, until recently, there has been a \nstrong reliance on a limited class of stationary kernels such as the Matern or \nsquared exponential, limiting the expressiveness of these modelling approaches. \nRecent machine learning research has focused on spectral representations to \nmodel arbitrary stationary kernels and introduced more general representations \nthat include classes of nonstationary kernels. In this paper, we exploit the \nconnections between Fourier feature representations, Gaussian processes and \nneural networks to generalise previous approaches and develop a simple and \nefficient framework to learn arbitrarily complex nonstationary kernel functions \ndirectly from the data, while taking care to avoid overfitting using \nstate-of-the-art methods from deep learning. We highlight the very broad array \nof kernel classes that could be created within this framework. We apply this to \na time series dataset and a remote sensing problem involving land surface \ntemperature in Eastern Africa. We show that without increasing the \ncomputational or storage complexity, nonstationary kernels can be used to \nimprove generalisation performance and provide more interpretable results. \n</p>"}, "author": "Jean-Francois Ton, Seth Flaxman, Dino Sejdinovic, Samir Bhatt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411169", "id": "tag:google.com,2005:reader/item/0000000334303b1d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning to Predict with Big Data. (arXiv:1711.05656v1 [stat.AP])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05656"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05656", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Big spatio-temporal datasets, available through both open and administrative \ndata sources, offer significant potential for social science research. The \nmagnitude of the data allows for increased resolution and analysis at \nindividual level. One of the issues researchers face with such data is the \nstationarity assumption. This poses several challenges in how to quantify \nuncertainty and bias. While there are recent advances in forecasting techniques \nfor highly granular temporal data, little attention is given to segmenting the \ntime series and finding homogeneous patterns. In this paper, it is proposed to \nestimate behavioral profiles of individuals' activities over time using \nGaussian Process based models. In particular, the aim is to investigate how \nindividuals or groups may be clustered according to the model parameters. Such \na Bayesian non-parametric method is then tested by looking at the \npredictability of the segments using a combination of models to fit different \nparts of the temporal profiles. Models validity is then tested on a set of hold \nout data. The dataset consists of half hourly energy consumption records from \nsmart meters from more than 100,000 households in the UK and covers the period \nfrom 2015 to 2016. The methodological approach that is developed in the paper \nmay be easily applied to datasets of similar structure and granularity, for \nexample social media data, and may lead to improved accuracy in the prediction \nof social dynamics and behavior. \n</p>"}, "author": "Anastasia Ushakova, Slava J. Mikhaylov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411168", "id": "tag:google.com,2005:reader/item/0000000334303b2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Variational Bi-LSTMs. (arXiv:1711.05717v1 [stat.ML])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05717"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05717", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural networks like long short-term memory (LSTM) are important \narchitectures for sequential prediction tasks. LSTMs (and RNNs in general) \nmodel sequences along the forward time direction. Bidirectional LSTMs \n(Bi-LSTMs) on the other hand model sequences along both forward and backward \ndirections and are generally known to perform better at such tasks because they \ncapture a richer representation of the data. In the training of Bi-LSTMs, the \nforward and backward paths are learned independently. We propose a variant of \nthe Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a \nchannel between the two paths (during training, but which may be omitted during \ninference); thus optimizing the two paths jointly. We arrive at this joint \nobjective for our model by minimizing a variational lower bound of the joint \nlikelihood of the data sequence. Our model acts as a regularizer and encourages \nthe two networks to inform each other in making their respective predictions \nusing distinct information. We perform ablation studies to better understand \nthe different components of our model and evaluate the method on various \nbenchmarks, showing state-of-the-art performance. \n</p>"}, "author": "Samira Shabanian, Devansh Arpit, Adam Trischler, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510859309411", "timestampUsec": "1510859309411167", "id": "tag:google.com,2005:reader/item/0000000334303b3e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Markov Decision Processes with Continuous Side Information. (arXiv:1711.05726v1 [stat.ML])", "published": 1510859309, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05726"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05726", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider a reinforcement learning (RL) setting in which the agent \ninteracts with a sequence of episodic MDPs. At the start of each episode the \nagent has access to some side-information or context that determines the \ndynamics of the MDP for that episode. Our setting is motivated by applications \nin healthcare where baseline measurements of a patient at the start of a \ntreatment episode form the context that may provide information about how the \npatient might respond to treatment decisions. We propose algorithms for \nlearning in such Contextual Markov Decision Processes (CMDPs) under an \nassumption that the unobserved MDP parameters vary smoothly with the observed \ncontext. We also give lower and upper PAC bounds under the smoothness \nassumption. Because our lower bound has an exponential dependence on the \ndimension, we consider a tractable linear setting where the context is used to \ncreate linear combinations of a finite set of MDPs. For the linear setting, we \ngive a PAC learning algorithm based on KWIK learning techniques. \n</p>"}, "author": "Aditya Modi, Nan Jiang, Satinder Singh, Ambuj Tewari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510799335301", "timestampUsec": "1510799335301178", "id": "tag:google.com,2005:reader/item/0000000333a7a112", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Human and Machine Speaker Recognition Based on Short Trivial Events. (arXiv:1711.05443v1 [cs.SD])", "published": 1510799336, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05443"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05443", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c76a818d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c76a818d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Trivial events are ubiquitous in human to human conversations, e.g., cough, \nlaugh and sniff. Compared to regular speech, these trivial events are usually \nshort and unclear, thus generally regarded as not speaker discriminative and so \nare largely ignored by present speaker recognition research. However, these \ntrivial events are highly valuable in some particular circumstances such as \nforensic examination, as they are less subjected to intentional change, so can \nbe used to discover the genuine speaker from disguised speech. In this paper, \nwe collect a trivial event speech database that involves 75 speakers and 6 \ntypes of events, and report preliminary speaker recognition results on this \ndatabase, by both human listeners and machines. Particularly, the deep feature \nlearning technique recently proposed by our group is utilized to analyze and \nrecognize the trivial events, which leads to acceptable equal error rates \n(EERs) despite the extremely short durations (0.2-0.5 seconds) of these events. \nComparing different types of events, 'hmm' seems more speaker discriminative. \n</p>"}, "author": "Miao Zhang, Xiaofei Kang, Yanqing Wang, Lantian Li, Zhiyuan Tang, Haisheng Dai, Dong Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704205", "id": "tag:google.com,2005:reader/item/0000000333923e81", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimised Maintenance of Datalog Materialisations. (arXiv:1711.03987v1 [cs.DB])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03987"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03987", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>To efficiently answer queries, datalog systems often materialise all \nconsequences of a datalog program, so the materialisation must be updated \nwhenever the input facts change. Several solutions to the materialisation \nupdate problem have been proposed. The Delete/Rederive (DRed) and the \nBackward/Forward (B/F) algorithms solve this problem for general datalog, but \nboth contain steps that evaluate rules 'backwards' by matching their heads to a \nfact and evaluating the partially instantiated rule bodies as queries. We show \nthat this can be a considerable source of overhead even on very small updates. \nIn contrast, the Counting algorithm does not evaluate the rules 'backwards', \nbut it can handle only nonrecursive rules. We present two hybrid approaches \nthat combine DRed and B/F with Counting so as to reduce or even eliminate \n'backward' rule evaluation while still handling arbitrary datalog programs. We \nshow empirically that our hybrid algorithms are usually significantly faster \nthan existing approaches, sometimes by orders of magnitude. \n</p>"}, "author": "Pan Hu, Boris Motik, Ian Horrocks", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704204", "id": "tag:google.com,2005:reader/item/0000000333923e88", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stream Reasoning in Temporal Datalog. (arXiv:1711.04013v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04013"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04013", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years, there has been an increasing interest in extending \ntraditional stream processing engines with logical, rule-based, reasoning \ncapabilities. This poses significant theoretical and practical challenges since \nrules can derive new information and propagate it both towards past and future \ntime points; as a result, streamed query answers can depend on data that has \nnot yet been received, as well as on data that arrived far in the past. Stream \nreasoning algorithms, however, must be able to stream out query answers as soon \nas possible, and can only keep a limited number of previous input facts in \nmemory. In this paper, we propose novel reasoning problems to deal with these \nchallenges, and study their computational properties on Datalog extended with a \ntemporal sort and the successor function (a core rule-based language for stream \nreasoning applications). \n</p>"}, "author": "Alessandro Ronca, Mark Kaminski, Bernardo Cuenca Grau, Boris Motik, Ian Horrocks", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704203", "id": "tag:google.com,2005:reader/item/0000000333923e8c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Within-Class Covariance Analysis for Acoustic Scene Classification. (arXiv:1711.04022v1 [cs.LG])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04022"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04022", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Within-Class Covariance Normalization (WCCN) is a powerful post-processing \nmethod for normalizing the within-class covariance of a set of data points. \nWCCN projects the observations into a linear sub-space where the within-class \nvariability is reduced. This property has proven to be beneficial in subsequent \nrecognition tasks. The central idea of this paper is to reformulate the classic \nWCCN as a Deep Neural Network (DNN) compatible version. We propose the Deep \nWithinClass Covariance Analysis (DWCCA) which can be incorporated in a DNN \narchitecture. This formulation enables us to exploit the beneficial properties \nof WCCN, and still allows for training with Stochastic Gradient Descent (SGD) \nin an end-to-end fashion. We investigate the advantages of DWCCA on deep neural \nnetworks with convolutional layers for supervised learning. Our results on \nAcoustic Scene Classification show that via DWCCA we can achieves equal or \nsuperior performance in a VGG-style deep neural network. \n</p>"}, "author": "Hamid Eghbal-zadeh, Matthias Dorfer, Gerhard Widmer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704202", "id": "tag:google.com,2005:reader/item/0000000333923e91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Physiological and behavioral profiling for nociceptive pain estimation using personalized multitask learning. (arXiv:1711.04036v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04036"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04036", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Pain is a subjective experience commonly measured through patient's self \nreport. While there exist numerous situations in which automatic pain \nestimation methods may be preferred, inter-subject variability in physiological \nand behavioral pain responses has hindered the development of such methods. In \nthis work, we address this problem by introducing a novel personalized \nmultitask machine learning method for pain estimation based on individual \nphysiological and behavioral pain response profiles, and show its advantages in \na dataset containing multimodal responses to nociceptive heat pain. \n</p>"}, "author": "Daniel Lopez-Martinez, Ognjen Rudovic, Rosalind Picard", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704201", "id": "tag:google.com,2005:reader/item/0000000333923e95", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "KBGAN: Adversarial Learning for Knowledge Graph Embeddings. (arXiv:1711.04071v1 [cs.CL])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04071"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04071", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce an adversarial learning framework, which we named KBGAN, to \nimprove the performances of a wide range of existing knowledge graph embedding \nmodels. Because knowledge graph datasets typically only contain positive facts, \nsampling useful negative training examples is a non-trivial task. Replacing the \nhead or tail entity of a fact with a uniformly randomly selected entity is a \nconventional method for generating negative facts used by many previous works, \nbut the majority of negative facts generated in this way can be easily \ndiscriminated from positive facts, and will contribute little towards the \ntraining. Inspired by generative adversarial networks (GANs), we use one \nknowledge graph embedding model as a negative sample generator to assist the \ntraining of our desired model, which acts as the discriminator in GANs. The \nobjective of the generator is to generate difficult negative samples that can \nmaximize their likeliness determined by the discriminator, while the \ndiscriminator minimizes its training loss. This framework is independent of the \nconcrete form of generator and discriminator, and therefore can utilize a wide \nvariety of knowledge graph embedding models as its building blocks. In \nexperiments, we adversarially train two translation-based models, TransE and \nTransD, each with assistance from one of the two probability-based models, \nDistMult and ComplEx. We evaluate the performances of KBGAN on the link \nprediction task, using three knowledge base completion datasets: FB15k-237, \nWN18 and WN18RR. Experimental results show that adversarial training \nsubstantially improves the performances of target embedding models under \nvarious settings. \n</p>"}, "author": "Liwei Cai, William Yang Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704200", "id": "tag:google.com,2005:reader/item/0000000333923e99", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Differential Performance Debugging with Discriminant Regression Trees. (arXiv:1711.04076v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04076"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04076", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Differential performance debugging is a technique to find performance \nproblems. It applies in situations where the performance of a program is \n(unexpectedly) different for different classes of inputs. The task is to \nexplain the differences in asymptotic performance among various input classes \nin terms of program internals. We propose a data-driven technique based on \ndiscriminant regression tree (DRT) learning problem where the goal is to \ndiscriminate among different classes of inputs. We propose a new algorithm for \nDRT learning that first clusters the data into functional clusters, capturing \ndifferent asymptotic performance classes, and then invokes off-the-shelf \ndecision tree learning algorithms to explain these clusters. We focus on linear \nfunctional clusters and adapt classical clustering algorithms (K-means and \nspectral) to produce them. For the K-means algorithm, we generalize the notion \nof the cluster centroid from a point to a linear function. We adapt spectral \nclustering by defining a novel kernel function to capture the notion of linear \nsimilarity between two data points. We evaluate our approach on benchmarks \nconsisting of Java programs where we are interested in debugging performance. \nWe show that our algorithm significantly outperforms other well-known \nregression tree learning algorithms in terms of running time and accuracy of \nclassification. \n</p>"}, "author": "Saeid Tizpaz-Niari, Pavol Cerny, Bor-Yuh Evan Chang, Ashutosh Trivedi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704199", "id": "tag:google.com,2005:reader/item/0000000333923e9f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Parkinson's Disease Digital Biomarker Discovery with Optimized Transitions and Inferred Markov Emissions. (arXiv:1711.04078v1 [q-bio.QM])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04078"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04078", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We search for digital biomarkers from Parkinson's Disease by observing \napproximate repetitive patterns matching hypothesized step and stride periodic \ncycles. These observations were modeled as a cycle of hidden states with \nrandomness allowing deviation from a canonical pattern of transitions and \nemissions, under the hypothesis that the averaged features of hidden states \nwould serve to informatively characterize classes of patients/controls. We \npropose a Hidden Semi-Markov Model (HSMM), a latent-state model, emitting \n3D-acceleration vectors. Transitions and emissions are inferred from data. We \nfit separate models per unique device and training label. Hidden Markov Models \n(HMM) force geometric distributions of the duration spent at each state before \ntransition to a new state. Instead, our HSMM allows us to specify the \ndistribution of state duration. This modified version is more effective because \nwe are interested more in each state's duration than the sequence of distinct \nstates, allowing inclusion of these durations the feature vector. \n</p>"}, "author": "Avinash Bukkittu, Baihan Lin, Trung Vu, Itsik Pe&#x27;er", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704198", "id": "tag:google.com,2005:reader/item/0000000333923ea1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Fine Grained Knowledge Transfer for Personalized Task-oriented Dialogue Systems. (arXiv:1711.04079v1 [cs.CL])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04079"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04079", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Training a personalized dialogue system requires a lot of data, and the data \ncollected for a single user is usually insufficient. One common practice for \nthis problem is to share training dialogues between different users and train \nmultiple sequence-to-sequence dialogue models together with transfer learning. \nHowever, current sequence-to-sequence transfer learning models operate on the \nentire sentence, which might cause negative transfer if different personal \ninformation from different users is mixed up. We propose a personalized decoder \nmodel to transfer finer granularity phrase-level knowledge between different \nusers while keeping personal preferences of each user intact. A novel personal \ncontrol gate is introduced, enabling the personalized decoder to switch between \ngenerating personalized phrases and shared phrases. The proposed personalized \ndecoder model can be easily combined with various deep models and can be \ntrained with reinforcement learning. Real-world experimental results \ndemonstrate that the phrase-level personalized decoder improves the BLEU over \nmultiple sentence-level transfer baseline models by as much as 7.5%. \n</p>"}, "author": "Kaixiang Mo, Yu Zhang, Qiang Yang, Pascale Fung", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704197", "id": "tag:google.com,2005:reader/item/0000000333923ea4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "MojiTalk: Generating Emotional Responses at Scale. (arXiv:1711.04090v1 [cs.CL])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04090"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04090", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generating emotional language is a key step towards building empathetic \nnatural language processing agents. However, a major challenge for this line of \nresearch is the lack of large-scale labeled training data, and previous studies \nare limited to only small sets of human annotated sentiment labels. \nAdditionally, explicitly controlling the emotion and sentiment of generated \ntext is also difficult. In this paper, we take a more radical approach: we \nexploit the idea of leveraging Twitter data that are naturally labeled with \nemojis. More specifically, we collect a large corpus of Twitter conversations \nthat include emojis in the response, and assume the emojis convey the \nunderlying emotions of the sentence. We then introduce a reinforced conditional \nvariational encoder approach to train a deep generative model on these \nconversations, which allows us to use emojis to control the emotion of the \ngenerated text. Experimentally, we show in our quantitative and qualitative \nanalyses that the proposed models can successfully generate high-quality \nabstractive conversation responses in accordance with designated emotions. \n</p>"}, "author": "Xianda Zhou, William Yang Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704196", "id": "tag:google.com,2005:reader/item/0000000333923eaa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Recommender Systems with Random Walks: A Survey. (arXiv:1711.04101v1 [cs.IR])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04101"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04101", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c76a8534\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c76a8534&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Recommender engines have become an integral component in today's e-commerce \nsystems. From recommending books in Amazon to finding friends in social \nnetworks such as Facebook, they have become omnipresent. \n</p> \n<p>Generally, recommender systems can be classified into two main categories: \ncontent based and collaborative filtering based models. Both these models build \nrelationships between users and items to provide recommendations. Content based \nsystems achieve this task by utilizing features extracted from the context \navailable, whereas collaborative systems use shared interests between user-item \nsubsets. \n</p> \n<p>There is another relatively unexplored approach for providing recommendations \nthat utilizes a stochastic process named random walks. This study is a survey \nexploring use cases of random walks in recommender systems and an attempt at \nclassifying them. \n</p>"}, "author": "Laknath Semage", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704195", "id": "tag:google.com,2005:reader/item/0000000333923eb1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Building machines that adapt and compute like brains. (arXiv:1711.04203v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04203"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04203", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c776169f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c776169f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Building machines that learn and think like humans is essential not only for \ncognitive science, but also for computational neuroscience, whose ultimate goal \nis to understand how cognition is implemented in biological brains. A new \ncognitive computational neuroscience should build cognitive-level and neural- \nlevel models, understand their relationships, and test both types of models \nwith both brain and behavioral data. \n</p>"}, "author": "Nikolaus Kriegeskorte, Robert M. Mok", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704194", "id": "tag:google.com,2005:reader/item/0000000333923eb6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Commonsense LocatedNear Relation Extraction. (arXiv:1711.04204v1 [cs.CL])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04204"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04204", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>LocatedNear relation describes two typically co-located objects, which is a \ntype of useful commonsense knowledge for computer vision, natural language \nunderstanding, machine comprehension, etc. We propose to automatically extract \nsuch relationship through a sentence-level classifier and aggregating the \nscores of entity pairs detected from a large number of sentences. To enable the \nresearch of these tasks, we release two benchmark datasets, one containing \n5,000 sentences annotated with whether a mentioned entity pair has LocatedNear \nrelation in the given sentence or not; the other containing 500 pairs of \nphysical objects and whether they are commonly located nearby. We also propose \nsome baseline methods for the tasks and compare the results with a \nstate-of-the-art general-purpose relation classifier. \n</p>"}, "author": "Frank F. Xu, Bill Y. Lin, Kenny Q. Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704193", "id": "tag:google.com,2005:reader/item/0000000333923eb9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Unified Spectral Clustering with Optimal Graph. (arXiv:1711.04258v1 [cs.LG])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04258"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04258", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Spectral clustering has found extensive use in many areas. Most traditional \nspectral clustering algorithms work in three separate steps: similarity graph \nconstruction; continuous labels learning; discretizing the learned labels by \nk-means clustering. Such common practice has two potential flaws, which may \nlead to severe information loss and performance degradation. First, predefined \nsimilarity graph might not be optimal for subsequent clustering. It is \nwell-accepted that similarity graph highly affects the clustering results. To \nthis end, we propose to automatically learn similarity information from data \nand simultaneously consider the constraint that the similarity matrix has exact \nc connected components if there are c clusters. Second, the discrete solution \nmay deviate from the spectral solution since k-means method is well-known as \nsensitive to the initialization of cluster centers. In this work, we transform \nthe candidate solution into a new one that better approximates the discrete \none. Finally, those three subtasks are integrated into a unified framework, \nwith each subtask iteratively boosted by using the results of the others \ntowards an overall optimal solution. It is known that the performance of a \nkernel method is largely determined by the choice of kernels. To tackle this \npractical problem of how to select the most suitable kernel for a particular \ndata set, we further extend our model to incorporate multiple kernel learning \nability. Extensive experiments demonstrate the superiority of our proposed \nmethod as compared to existing clustering approaches. \n</p>"}, "author": "Zhao Kang, Chong Peng, Qiang Cheng, Zenglin Xu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704192", "id": "tag:google.com,2005:reader/item/0000000333923ec0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Synthesis of Guaranteed-Quality Plans for Robot Fleets in Logistics Scenarios via Optimization Modulo Theories. (arXiv:1711.04259v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04259"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04259", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In manufacturing, the increasing involvement of autonomous robots in \nproduction processes poses new challenges on the production management. In this \npaper we report on the usage of Optimization Modulo Theories (OMT) to solve \ncertain multi-robot scheduling problems in this area. Whereas currently \nexisting methods are heuristic, our approach guarantees optimality for the \ncomputed solution. We do not only present our final method but also its \nchronological development, and draw some general observations for the \ndevelopment of OMT-based approaches. \n</p>"}, "author": "Francesco Leofante, Erika &#xc1;brah&#xe1;m, Tim Niemueller, Gerhard Lakemeyer, Armando Tacchella", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704191", "id": "tag:google.com,2005:reader/item/0000000333923ec4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Self-Regulating Artificial General Intelligence. (arXiv:1711.04309v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04309"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04309", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Here we examine the paperclip apocalypse concern for artificial general \nintelligence (or AGI) whereby a superintelligent AI with a simple goal (ie., \nproducing paperclips) accumulates power so that all resources are devoted \ntowards that simple goal and are unavailable for any other use. We provide \nconditions under which a paper apocalypse can arise but also show that, under \ncertain architectures for recursive self-improvement of AIs, that a paperclip \nAI may refrain from allowing power capabilities to be developed. The reason is \nthat such developments pose the same control problem for the AI as they do for \nhumans (over AIs) and hence, threaten to deprive it of resources for its \nprimary goal. \n</p>"}, "author": "Joshua S. Gans", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704190", "id": "tag:google.com,2005:reader/item/0000000333923eca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "High-Order Attention Models for Visual Question Answering. (arXiv:1711.04323v1 [cs.CV])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04323"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04323", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The quest for algorithms that enable cognitive abilities is an important part \nof machine learning. A common trait in many recently investigated \ncognitive-like tasks is that they take into account different data modalities, \nsuch as visual and textual input. In this paper we propose a novel and \ngenerally applicable form of attention mechanism that learns high-order \ncorrelations between various data modalities. We show that high-order \ncorrelations effectively direct the appropriate attention to the relevant \nelements in the different data modalities that are required to solve the joint \ntask. We demonstrate the effectiveness of our high-order attention mechanism on \nthe task of visual question answering (VQA), where we achieve state-of-the-art \nperformance on the standard VQA dataset. \n</p>"}, "author": "Idan Schwartz, Alexander G. Schwing, Tamir Hazan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704189", "id": "tag:google.com,2005:reader/item/0000000333923ecd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Medical Diagnosis From Laboratory Tests by Combining Generative and Discriminative Learning. (arXiv:1711.04329v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04329"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04329", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A primary goal of computational phenotype research is to conduct medical \ndiagnosis. In hospital, physicians rely on massive clinical data to make \ndiagnosis decisions, among which laboratory tests are one of the most important \nresources. However, the longitudinal and incomplete nature of laboratory test \ndata casts a significant challenge on its interpretation and usage, which may \nresult in harmful decisions by both human physicians and automatic diagnosis \nsystems. In this work, we take advantage of deep generative models to deal with \nthe complex laboratory tests. Specifically, we propose an end-to-end \narchitecture that involves a deep generative variational recurrent neural \nnetworks (VRNN) to learn robust and generalizable features, and a \ndiscriminative neural network (NN) model to learn diagnosis decision making, \nand the two models are trained jointly. Our experiments are conducted on a \ndataset involving 46,252 patients, and the 50 most frequent tests are used to \npredict the 50 most common diagnoses. The results show that our model, VRNN+NN, \nsignificantly (p&lt;0.001) outperforms other baseline models. Moreover, we \ndemonstrate that the representations learned by the joint training are more \ninformative than those learned by pure generative models. Finally, we find that \nour model offers a surprisingly good imputation for missing values. \n</p>"}, "author": "Shiyue Zhang, Pengtao Xie, Dong Wang, Eric P. Xing", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704188", "id": "tag:google.com,2005:reader/item/0000000333923ecf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Deep Networks tag the location of bird vocalisations on audio spectrograms. (arXiv:1711.04347v1 [eess.AS])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04347"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04347", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This work focuses on reliable detection and segmentation of bird \nvocalizations as recorded in the open field. Acoustic detection of avian sounds \ncan be used for the automatized monitoring of multiple bird taxa and querying \nin long-term recordings for species of interest. These tasks are tackled in \nthis work, by suggesting two approaches: A) First, DenseNets are applied to \nweekly labeled data to infer the attention map of the dataset (i.e. Salience \nand CAM). We push further this idea by directing attention maps to the YOLO v2 \nDeepnet-based, detection framework to localize bird vocalizations. B) A deep \nautoencoder, namely the U-net, maps the audio spectrogram of bird vocalizations \nto its corresponding binary mask that encircles the spectral blobs of \nvocalizations while suppressing other audio sources. We focus solely on \nprocedures requiring minimum human attendance, suitable to scan massive volumes \nof data, in order to analyze them, evaluate insights and hypotheses and \nidentify patterns of bird activity. Hopefully, this approach will be valuable \nto researchers, conservation practitioners, and decision makers that need to \ndesign policies on biodiversity issues. \n</p>"}, "author": "Lefteris Fanioudakis, Ilyas Potamitis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704187", "id": "tag:google.com,2005:reader/item/0000000333923ed4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning. (arXiv:1711.04436v1 [cs.CL])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04436"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04436", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Synthesizing SQL queries from natural language is a long-standing open \nproblem and has been attracting considerable interest recently. Toward solving \nthe problem, the de facto approach is to employ a sequence-to-sequence-style \nmodel. Such an approach will necessarily require the SQL queries to be \nserialized. Since the same SQL query may have multiple equivalent \nserializations, training a sequence-to-sequence-style model is sensitive to the \nchoice from one of them. This phenomenon is documented as the \"order-matters\" \nproblem. Existing state-of-the-art approaches rely on reinforcement learning to \nreward the decoder when it generates any of the equivalent serializations. \nHowever, we observe that the improvement from reinforcement learning is \nlimited. \n</p> \n<p>In this paper, we propose a novel approach, i.e., SQLNet, to fundamentally \nsolve this problem by avoiding the sequence-to-sequence structure when the \norder does not matter. In particular, we employ a sketch-based approach where \nthe sketch contains a dependency graph so that one prediction can be done by \ntaking into consideration only the previous predictions that it depends on. In \naddition, we propose a sequence-to-set model as well as the column attention \nmechanism to synthesize the query based on the sketch. By combining all these \nnovel techniques, we show that SQLNet can outperform the prior art by 9% to 13% \non the WikiSQL task. \n</p>"}, "author": "Xiaojun Xu, Chang Liu, Dawn Song", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704186", "id": "tag:google.com,2005:reader/item/0000000333923ed8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Abduction under Partial Observability. (arXiv:1711.04438v2 [cs.AI] UPDATED)", "published": 1511175748, "updated": 1511175750, "canonical": [{"href": "http://arxiv.org/abs/1711.04438"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04438", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7761b5f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7761b5f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Juba recently proposed a formulation of learning abductive reasoning from \nexamples, in which both the relative plausibility of various explanations, as \nwell as which explanations are valid, are learned directly from data. The main \nshortcoming of this formulation of the task is that it assumes access to \nfull-information (i.e., fully specified) examples; relatedly, it offers no role \nfor declarative background knowledge, as such knowledge is rendered redundant \nin the abduction task by complete information. In this work, we extend the \nformulation to utilize such partially specified examples, along with \ndeclarative background knowledge about the missing data. We show that it is \npossible to use implicitly learned rules together with the explicitly given \ndeclarative knowledge to support hypotheses in the course of abduction. We \nobserve that when a small explanation exists, it is possible to obtain a \nmuch-improved guarantee in the challenging exception-tolerant setting. Such \nsmall, human-understandable explanations are of particular interest for \npotential applications of the task. \n</p>"}, "author": "Brendan Juba, Zongyi Li, Evan Miller", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704185", "id": "tag:google.com,2005:reader/item/0000000333923ede", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Targeted Advertising Based on Browsing History. (arXiv:1711.04498v1 [cs.IR])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04498"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04498", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Audience interest, demography, purchase behavior and other possible \nclassifications are ex- tremely important factors to be carefully studied in a \ntargeting campaign. This information can help advertisers and publishers \ndeliver advertisements to the right audience group. How- ever, it is not easy \nto collect such information, especially for the online audience with whom we \nhave limited interaction and minimum deterministic knowledge. In this paper, we \npro- pose a predictive framework that can estimate online audience demographic \nattributes based on their browsing histories. Under the proposed framework, \nfirst, we retrieve the content of the websites visited by audience, and \nrepresent the content as website feature vectors; second, we aggregate the \nvectors of websites that audience have visited and arrive at feature vectors \nrepresenting the users; finally, the support vector machine is exploited to \npredict the audience demographic attributes. The key to achieving good \nprediction performance is preparing representative features of the audience. \nWord Embedding, a widely used tech- nique in natural language processing tasks, \ntogether with term frequency-inverse document frequency weighting scheme is \nused in the proposed method. This new representation ap- proach is unsupervised \nand very easy to implement. The experimental results demonstrate that the new \naudience feature representation method is more powerful than existing baseline \nmethods, leading to a great improvement in prediction accuracy. \n</p>"}, "author": "Yong Zhang, Hongming Zhou, Nganmeng Tan, Saeed Bagheri, Meng Joo Er", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704184", "id": "tag:google.com,2005:reader/item/0000000333923ee6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Supervised Learning Concept for Reducing User Interaction in Passenger Cars. (arXiv:1711.04518v1 [cs.SY])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04518"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04518", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this article an automation system for human-machine-interfaces (HMI) for \nsetpoint adjustment using supervised learning is presented. We use HMIs of \nmulti-modal thermal conditioning systems in passenger cars as example for a \ncomplex setpoint selection system. The goal is the reduction of interaction \ncomplexity up to full automation. The approach is not limited to climate \ncontrol applications but can be extended to other setpoint-based HMIs. \n</p>"}, "author": "Marius St&#xe4;rk, Damian Backes, Christian Kehl", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704183", "id": "tag:google.com,2005:reader/item/0000000333923ee9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Simple And Efficient Architecture Search for Convolutional Neural Networks. (arXiv:1711.04528v1 [stat.ML])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04528"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04528", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks have recently had a lot of success for many tasks. However, \nneural network architectures that perform well are still typically designed \nmanually by experts in a cumbersome trial-and-error process. We propose a new \nmethod to automatically search for well-performing CNN architectures based on a \nsimple hill climbing procedure whose operators apply network morphisms, \nfollowed by short optimization runs by cosine annealing. Surprisingly, this \nsimple method yields competitive results, despite only requiring resources in \nthe same order of magnitude as training a single network. E.g., on CIFAR-10, \nour method designs and trains networks with an error rate below 6% in only 12 \nhours on a single GPU; training for one day reduces this error further, to \nalmost 5%. \n</p>"}, "author": "Thomas Elsken, Jan-Hendrik Metzen, Frank Hutter", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704182", "id": "tag:google.com,2005:reader/item/0000000333923ef0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Solving the Resource Constrained Project Scheduling Problem Using the Parallel Tabu Search Designed for the CUDA Platform. (arXiv:1711.04556v1 [cs.DC])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04556"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04556", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In the paper, a parallel Tabu Search algorithm for the Resource Constrained \nProject Scheduling Problem is proposed. To deal with this NP-hard combinatorial \nproblem many optimizations have been performed. For example, a resource \nevaluation algorithm is selected by a heuristic and an effective Tabu List was \ndesigned. In addition to that, a capacity-indexed resource evaluation algorithm \nwas proposed and the GPU (Graphics Processing Unit) version uses a homogeneous \nmodel to reduce the required communication bandwidth. According to the \nexperiments, the GPU version outperforms the optimized parallel CPU version \nwith respect to the computational time and the quality of solutions. In \ncomparison with other existing heuristics, the proposed solution often gives \nbetter quality solutions. \n</p>"}, "author": "Libor Bukata, Premysl Sucha, Zdenek Hanzalek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704181", "id": "tag:google.com,2005:reader/item/0000000333923ef8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Phonemic and Graphemic Multilingual CTC Based Speech Recognition. (arXiv:1711.04564v1 [eess.AS])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04564"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04564", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Training automatic speech recognition (ASR) systems requires large amounts of \ndata in the target language in order to achieve good performance. Whereas large \ntraining corpora are readily available for languages like English, there exists \na long tail of languages which do suffer from a lack of resources. One method \nto handle data sparsity is to use data from additional source languages and \nbuild a multilingual system. Recently, ASR systems based on recurrent neural \nnetworks (RNNs) trained with connectionist temporal classification (CTC) have \ngained substantial research interest. In this work, we extended our previous \napproach towards training CTC-based systems multilingually. Our systems feature \na global phone set, based on the joint phone sets of each source language. We \nevaluated the use of different language combinations as well as the addition of \nLanguage Feature Vectors (LFVs). As contrastive experiment, we built systems \nbased on graphemes as well. Systems having a multilingual phone set are known \nto suffer in performance compared to their monolingual counterparts. With our \nproposed approach, we could reduce the gap between these mono- and multilingual \nsetups, using either graphemes or phonemes. \n</p>"}, "author": "Markus M&#xfc;ller, Sebastian St&#xfc;ker, Alex Waibel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704180", "id": "tag:google.com,2005:reader/item/0000000333923efe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multilingual Adaptation of RNN Based ASR Systems. (arXiv:1711.04569v1 [eess.AS])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04569"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04569", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A large amount of data is required for automatic speech recognition (ASR) \nsystems achieving good performance. While such data is readily available for \nlanguages like English, there exists a long tail of languages with only limited \nlanguage resources. By using data from additional source languages, this \nproblem can be mitigated. In this work, we focus on multilingual systems based \non recurrent neural networks (RNNs), trained using the Connectionist Temporal \nClassification (CTC) loss function. Using a multilingual set of acoustic units \nto train systems jointly on multiple languages poses difficulties: While the \nsame phones share the same symbols across languages, they are pronounced \nslightly different because of, e.g., small shifts in tongue positions. To \naddress this issue, we proposed Language Feature Vectors (LFVs) to train \nlanguage adaptive multilingual systems. In this work, we extended this approach \nby introducing a novel technique which we call \"modulation\" to add LFVs . We \nevaluated our approach in multiple conditions, showing improvements in both \nfull and low resource conditions as well as for grapheme and phone based \nsystems. \n</p>"}, "author": "Markus M&#xfc;ller, Sebastian St&#xfc;ker, Alex Waibel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704179", "id": "tag:google.com,2005:reader/item/0000000333923f04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Machine Learning for the Geosciences: Challenges and Opportunities. (arXiv:1711.04708v1 [cs.LG])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04708"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04708", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Geosciences is a field of great societal relevance that requires solutions to \nseveral urgent problems facing our humanity and the planet. As geosciences \nenters the era of big data, machine learning (ML) -- that has been widely \nsuccessful in commercial domains -- offers immense potential to contribute to \nproblems in geosciences. However, problems in geosciences have several unique \nchallenges that are seldom found in traditional applications, requiring novel \nproblem formulations and methodologies in machine learning. This article \nintroduces researchers in the machine learning (ML) community to these \nchallenges offered by geoscience problems and the opportunities that exist for \nadvancing both machine learning and geosciences. We first highlight typical \nsources of geoscience data and describe their properties that make it \nchallenging to use traditional machine learning techniques. We then describe \nsome of the common categories of geoscience problems where machine learning can \nplay a role, and discuss some of the existing efforts and promising directions \nfor methodological development in machine learning. We conclude by discussing \nsome of the emerging research themes in machine learning that are applicable \nacross all problems in the geosciences, and the importance of a deep \ncollaboration between machine learning and geosciences for synergistic \nadvancements in both disciplines. \n</p>"}, "author": "Anuj Karpatne, Imme Ebert-Uphoff, Sai Ravela, Hassan Ali Babaie, Vipin Kumar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704178", "id": "tag:google.com,2005:reader/item/0000000333923f0a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Spatio-Temporal Data Mining: A Survey of Problems and Methods. (arXiv:1711.04710v1 [cs.LG])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04710"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04710", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Large volumes of spatio-temporal data are increasingly collected and studied \nin diverse domains including, climate science, social sciences, neuroscience, \nepidemiology, transportation, mobile health, and Earth sciences. \nSpatio-temporal data differs from relational data for which computational \napproaches are developed in the data mining community for multiple decades, in \nthat both spatial and temporal attributes are available in addition to the \nactual measurements/attributes. The presence of these attributes introduces \nadditional challenges that needs to be dealt with. Approaches for mining \nspatio-temporal data have been studied for over a decade in the data mining \ncommunity. In this article we present a broad survey of this relatively young \nfield of spatio-temporal data mining. We discuss different types of \nspatio-temporal data and the relevant data mining questions that arise in the \ncontext of analyzing each of these datasets. Based on the nature of the data \nmining problem studied, we classify literature on spatio-temporal data mining \ninto six major categories: clustering, predictive learning, change detection, \nfrequent pattern mining, anomaly detection, and relationship mining. We discuss \nthe various forms of spatio-temporal data mining problems in each of these \ncategories. \n</p>"}, "author": "Gowtham Atluri, Anuj Karpatne, Vipin Kumar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704177", "id": "tag:google.com,2005:reader/item/0000000333923f10", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Accelerating HPC codes on Intel(R) Omni-Path Architecture networks: From particle physics to Machine Learning. (arXiv:1711.04883v1 [cs.DC])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04883"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04883", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We discuss practical methods to ensure near wirespeed performance from \nclusters with either one or two Intel(R) Omni-Path host fabric interfaces (HFI) \nper node, and Intel(R) Xeon Phi(TM) 72xx (Knight's Landing) processors, and \nusing the Linux operating system. \n</p> \n<p>The study evaluates the performance improvements achievable and the required \nprogramming approaches in two distinct example problems: firstly in Cartesian \ncommunicator halo exchange problems, appropriate for structured grid PDE \nsolvers that arise in quantum chromodynamics simulations of particle physics, \nand secondly in gradient reduction appropriate to synchronous stochastic \ngradient descent for machine learning. As an example, we accelerate a published \nBaidu Research reduction code and obtain a factor of ten speedup over the \noriginal code using the techniques discussed in this paper. This displays how a \nfactor of ten speedup in strongly scaled distributed machine learning could be \nachieved when synchronous stochastic gradient descent is massively parallelised \nwith a fixed mini-batch size. \n</p> \n<p>We find a significant improvement in performance robustness when memory is \nobtained using carefully allocated 2MB \"huge\" virtual memory pages, implying \nthat either non-standard allocation routines should be used for communication \nbuffers. These can be accessed via a LD\\_PRELOAD override in the manner \nsuggested by libhugetlbfs. We make use of a the Intel(R) MPI 2019 library \n\"Technology Preview\" and underlying software to enable thread concurrency \nthroughout the communication software stake via multiple PSM2 endpoints per \nprocess and use of multiple independent MPI communicators. When using a single \nMPI process per node, we find that this greatly accelerates delivered bandwidth \nin many core Intel(R) Xeon Phi processors. \n</p>"}, "author": "Peter Boyle, Michael Chuvelev, Guido Cossu, Christopher Kelly, Christoph Lehner, Lawrence Meadows", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704176", "id": "tag:google.com,2005:reader/item/0000000333923f17", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "DataVizard: Recommending Visual Presentations for Structured Data. (arXiv:1711.04971v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04971"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04971", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7761f3f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7761f3f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Selecting the appropriate visual presentation of the data such that it \npreserves the semantics of the underlying data and at the same time provides an \nintuitive summary of the data is an important, often the final step of data \nanalytics. Unfortunately, this is also a step involving significant human \neffort starting from selection of groups of columns in the structured results \nfrom analytics stages, to the selection of right visualization by experimenting \nwith various alternatives. In this paper, we describe our \\emph{DataVizard} \nsystem aimed at reducing this overhead by automatically recommending the most \nappropriate visual presentation for the structured result. Specifically, we \nconsider the following two scenarios: first, when one needs to visualize the \nresults of a structured query such as SQL; and the second, when one has \nacquired a data table with an associated short description (e.g., tables from \nthe Web). Using a corpus of real-world database queries (and their results) and \na number of statistical tables crawled from the Web, we show that DataVizard is \ncapable of recommending visual presentations with high accuracy. We also \npresent the results of a user survey that we conducted in order to assess user \nviews of the suitability of the presented charts vis-a-vis the plain text \ncaptions of the data. \n</p>"}, "author": "Rema Ananthanarayanan, Pranay Kr. Lohia, Srikanta Bedathur", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704175", "id": "tag:google.com,2005:reader/item/0000000333923f1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SkipFlow: Incorporating Neural Coherence Features for End-to-End Automatic Text Scoring. (arXiv:1711.04981v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04981"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04981", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c77f0c7f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c77f0c7f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep learning has demonstrated tremendous potential for Automatic Text \nScoring (ATS) tasks. In this paper, we describe a new neural architecture that \nenhances vanilla neural network models with auxiliary neural coherence \nfeatures. Our new method proposes a new \\textsc{SkipFlow} mechanism that models \nrelationships between snapshots of the hidden representations of a long \nshort-term memory (LSTM) network as it reads. Subsequently, the semantic \nrelationships between multiple snapshots are used as auxiliary features for \nprediction. This has two main benefits. Firstly, essays are typically long \nsequences and therefore the memorization capability of the LSTM network may be \ninsufficient. Implicit access to multiple snapshots can alleviate this problem \nby acting as a protection against vanishing gradients. The parameters of the \n\\textsc{SkipFlow} mechanism also acts as an auxiliary memory. Secondly, \nmodeling relationships between multiple positions allows our model to learn \nfeatures that represent and approximate textual coherence. In our model, we \ncall this \\textit{neural coherence} features. Overall, we present a unified \ndeep learning architecture that generates neural coherence features as it reads \nin an end-to-end fashion. Our approach demonstrates state-of-the-art \nperformance on the benchmark ASAP dataset, outperforming not only feature \nengineering baselines but also other deep learning models. \n</p>"}, "author": "Yi Tay, Minh C. Phan, Luu Anh Tuan, Siu Cheung Hui", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704174", "id": "tag:google.com,2005:reader/item/0000000333923f26", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Prediction Under Uncertainty with Error-Encoding Networks. (arXiv:1711.04994v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04994"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04994", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work we introduce a new framework for performing temporal predictions \nin the presence of uncertainty. It is based on a simple idea of disentangling \ncomponents of the future state which are predictable from those which are \ninherently unpredictable, and encoding the unpredictable components into a \nlow-dimensional latent variable which is fed into a forward model. Our method \nuses a supervised training objective which is fast and easy to train. We \nevaluate it in the context of video prediction on multiple datasets and show \nthat it is able to consistently generate diverse predictions without the need \nfor alternating minimization over a latent space or adversarial training. \n</p>"}, "author": "Mikael Henaff, Junbo Zhao, Yann LeCun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704173", "id": "tag:google.com,2005:reader/item/0000000333923f35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A unified decision making framework for supply and demand management in microgrid networks. (arXiv:1711.05078v1 [cs.SY])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05078"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05078", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper considers two important problems - on the supply-side and \ndemand-side respectively and studies both in a unified framework. On the supply \nside, we study the problem of energy sharing among microgrids with the goal of \nmaximizing profit obtained from selling power while meeting customer demand. On \nthe other hand, under shortage of power, this problem becomes one of deciding \nthe amount of power to be bought with dynamically varying prices. On the demand \nside, we consider the problem of optimally scheduling the time-adjustable \ndemand - i.e., of loads with flexible time windows in which they can be \nscheduled. While previous works have treated these two problems in isolation, \nwe combine these problems together and provide for the first time in the \nliterature, a unified Markov decision process (MDP) framework for these \nproblems. We then apply the Q-learning algorithm, a popular model-free \nreinforcement learning technique, to obtain the optimal policy. Through \nsimulations, we show that our model outperforms the traditional power sharing \nmodels. \n</p>"}, "author": "Raghuram Bharadwaj Diddigi, Sai Koti Reddy Danda, Krishnasuri Narayanam, Shalabh Bhatnagar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704172", "id": "tag:google.com,2005:reader/item/0000000333923f4e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficiency Analysis of ASP Encodings for Sequential Pattern Mining Tasks. (arXiv:1711.05090v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05090"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05090", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This article presents the use of Answer Set Programming (ASP) to mine \nsequential patterns. ASP is a high-level declarative logic programming paradigm \nfor high level encoding combinatorial and optimization problem solving as well \nas knowledge representation and reasoning. Thus, ASP is a good candidate for \nimplementing pattern mining with background knowledge, which has been a data \nmining issue for a long time. We propose encodings of the classical sequential \npattern mining tasks within two representations of embeddings (fill-gaps vs \nskip-gaps) and for various kinds of patterns: frequent, constrained and \ncondensed. We compare the computational performance of these encodings with \neach other to get a good insight into the efficiency of ASP encodings. The \nresults show that the fill-gaps strategy is better on real problems due to \nlower memory consumption. Finally, compared to a constraint programming \napproach (CPSM), another declarative programming paradigm, our proposal showed \ncomparable performance. \n</p>"}, "author": "Thomas Guyet (LACODAM), Yves Moinard (LACODAM), Ren&#xe9; Quiniou (LACODAM), Torsten Schaub (LACODAM)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704171", "id": "tag:google.com,2005:reader/item/0000000333923f6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Web Robot Detection in Academic Publishing. (arXiv:1711.05098v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05098"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05098", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent industry reports assure the rise of web robots which comprise more \nthan half of the total web traffic. They not only threaten the security, \nprivacy and efficiency of the web but they also distort analytics and metrics, \ndoubting the veracity of the information being promoted. In the academic \npublishing domain, this can cause articles to be faulty presented as prominent \nand influential. In this paper, we present our approach on detecting web robots \nin academic publishing websites. We use different supervised learning \nalgorithms with a variety of characteristics deriving from both the log files \nof the server and the content served by the website. Our approach relies on the \nassumption that human users will be interested in specific domains or articles, \nwhile web robots crawl a web library incoherently. We experiment with features \nadopted in previous studies with the addition of novel semantic characteristics \nwhich derive after performing a semantic analysis using the Latent Dirichlet \nAllocation (LDA) algorithm. Our real-world case study shows promising results, \npinpointing the significance of semantic features in the web robot detection \nproblem. \n</p>"}, "author": "Athanasios Lagopoulos, Grigorios Tsoumakas, Georgios Papadopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704170", "id": "tag:google.com,2005:reader/item/0000000333923f7a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Empirical Study of the Effects of Spurious Transitions on Abstraction-based Heuristics. (arXiv:1711.05105v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05105"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05105", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The efficient solution of state space search problems is often attempted by \nguiding search algorithms with heuristics (estimates of the distance from any \nstate to the goal). A popular way for creating heuristic functions is by using \nan abstract version of the state space. However, the quality of \nabstraction-based heuristic functions, and thus the speed of search, can suffer \nfrom spurious transitions, i.e., state transitions in the abstract state space \nfor which no corresponding transitions in the reachable component of the \noriginal state space exist. Our first contribution is a quantitative study \ndemonstrating that the harmful effects of spurious transitions on heuristic \nfunctions can be substantial, in terms of both the increase in the number of \nabstract states and the decrease in the heuristic values, which may slow down \nsearch. Our second contribution is an empirical study on the benefits of \nremoving a certain kind of spurious transition, namely those that involve \nstates with a pair of mutually exclusive (mutex) variablevalue assignments. In \nthe context of state space planning, a mutex pair is a pair of variable-value \nassignments that does not occur in any reachable state. Detecting mutex pairs \nis a problem that has been addressed frequently in the planning literature. Our \nstudy shows that there are cases in which mutex detection helps to eliminate \nharmful spurious transitions to a large extent and thus to speed up search \nsubstantially. \n</p>"}, "author": "Mehdi Sadeqi, Robert C. Holte, Sandra Zilles", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704169", "id": "tag:google.com,2005:reader/item/0000000333923f81", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering. (arXiv:1711.05116v1 [cs.CL])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05116"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05116", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A popular recent approach to answering open-domain questions is to first \nsearch for question-related passages and then apply reading comprehension \nmodels to extract answers. Existing methods usually extract answers from single \npassages independently. But some questions require a combination of evidence \nfrom across different sources to answer correctly. In this paper, we propose \ntwo models which make use of multiple passages to generate their answers. Both \nuse an answer-reranking approach which reorders the answer candidates generated \nby an existing state-of-the-art QA model. We propose two methods, namely, \nstrength-based re-ranking and coverage-based re-ranking, to make use of the \naggregated evidence from different passages to better determine the answer. Our \nmodels have achieved state-of-the-art results on three public open-domain QA \ndatasets: Quasar-T, SearchQA and the open-domain version of TriviaQA, with \nabout 8 percentage points of improvement over the former two datasets. \n</p>"}, "author": "Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, Murray Campbell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704168", "id": "tag:google.com,2005:reader/item/0000000333923f8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Deep Rewiring: Training very sparse deep networks. (arXiv:1711.05136v1 [cs.NE])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05136"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05136", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neuromorphic hardware tends to pose limits on the connectivity of deep \nnetworks that one can run on them. But also generic hardware and software \nimplementations of deep learning run more efficiently on sparse networks. \nSeveral methods exist for pruning connections of a neural network after it was \ntrained without connectivity constraints. We present an algorithm, DEEP R, that \nenables us to train directly a sparsely connected neural network. DEEP R \nautomatically rewires the network during supervised training so that \nconnections are there where they are most needed for the task, while its total \nnumber is all the time strictly bounded. We demonstrate that DEEP R can be used \nto train very sparse feedforward and recurrent neural networks on standard \nbenchmark tasks with just a minor loss in performance. DEEP R is based on a \nrigorous theoretical foundation that views rewiring as stochastic sampling of \nnetwork configurations from a posterior. \n</p>"}, "author": "Guillaume Bellec, David Kappel, Wolfgang Maass, Robert Legenstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704167", "id": "tag:google.com,2005:reader/item/0000000333923f99", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Saliency-based Sequential Image Attention with Multiset Prediction. (arXiv:1711.05165v1 [cs.CV])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05165"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05165", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Humans process visual scenes selectively and sequentially using attention. \nCentral to models of human visual attention is the saliency map. We propose a \nhierarchical visual architecture that operates on a saliency map and uses a \nnovel attention mechanism to sequentially focus on salient regions and take \nadditional glimpses within those regions. The architecture is motivated by \nhuman visual attention, and is used for multi-label image classification on a \nnovel multiset task, demonstrating that it achieves high precision and recall \nwhile localizing objects with its attention. Unlike conventional multi-label \nimage classification models, the model supports multiset prediction due to a \nreinforcement-learning based training process that allows for arbitrary label \npermutation and multiple instances per label. \n</p>"}, "author": "Sean Welleck, Jialin Mao, Kyunghyun Cho, Zheng Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704166", "id": "tag:google.com,2005:reader/item/0000000333923fbf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tree Projections and Constraint Optimization Problems: Fixed-Parameter Tractability and Parallel Algorithms. (arXiv:1711.05216v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05216"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05216", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c77f10c6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c77f10c6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Tree projections provide a unifying framework to deal with most structural \ndecomposition methods of constraint satisfaction problems (CSPs). Within this \nframework, a CSP instance is decomposed into a number of sub-problems, called \nviews, whose solutions are either already available or can be computed \nefficiently. The goal is to arrange portions of these views in a tree-like \nstructure, called tree projection, which determines an efficiently solvable CSP \ninstance equivalent to the original one. Deciding whether a tree projection \nexists is NP-hard. Solution methods have therefore been proposed in the \nliterature that do not require a tree projection to be given, and that either \ncorrectly decide whether the given CSP instance is satisfiable, or return that \na tree projection actually does not exist. These approaches had not been \ngeneralized so far on CSP extensions for optimization problems, where the goal \nis to compute a solution of maximum value/minimum cost. The paper fills the \ngap, by exhibiting a fixed-parameter polynomial-time algorithm that either \ndisproves the existence of tree projections or computes an optimal solution, \nwith the parameter being the size of the expression of the objective function \nto be optimized over all possible solutions (and not the size of the whole \nconstraint formula, used in related works). Tractability results are also \nestablished for the problem of returning the best K solutions. Finally, \nparallel algorithms for such optimization problems are proposed and analyzed. \nGiven that the classes of acyclic hypergraphs, hypergraphs of bounded \ntreewidth, and hypergraphs of bounded generalized hypertree width are all \ncovered as special cases of the tree projection framework, the results in this \npaper directly apply to these classes. These classes are extensively considered \nin the CSP setting, as well as in conjunctive database query evaluation and \noptimization. \n</p>"}, "author": "Georg Gottlob, Gianlugi Greco, Francesco Scarcello", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704165", "id": "tag:google.com,2005:reader/item/0000000333923fcf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Goal-Driven Query Answering for Existential Rules with Equality. (arXiv:1711.05227v1 [cs.AI])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05227"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05227", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Inspired by the magic sets for Datalog, we present a novel goal-driven \napproach for answering queries over terminating existential rules with equality \n(aka TGDs and EGDs). Our technique improves the performance of query answering \nby pruning the consequences that are not relevant for the query. This is \nchallenging in our setting because equalities can potentially affect all \npredicates in a dataset. We address this problem by combining the existing \nsingularization technique with two new ingredients: an algorithm for \nidentifying the rules relevant to a query and a new magic sets algorithm. We \nshow empirically that our technique can significantly improve the performance \nof query answering, and that it can mean the difference between answering a \nquery in a few seconds or not being able to process the query at all. \n</p>"}, "author": "Michael Benedikt, Boris Motik, Efthymia Tsamoura", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704164", "id": "tag:google.com,2005:reader/item/0000000333923fe6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Weakly-supervised Semantic Parsing with Abstract Examples. (arXiv:1711.05240v1 [cs.CL])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05240"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05240", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Semantic parsers translate language utterances to programs, but are often \ntrained from utterance-denotation pairs only. Consequently, parsers must \novercome the problem of spuriousness at training time, where an incorrect \nprogram found at search time accidentally leads to a correct denotation. We \npropose that in small well-typed domains, we can semi-automatically generate an \nabstract representation for examples that facilitates information sharing \nacross examples. This alleviates spuriousness, as the probability of randomly \nobtaining a correct answer from a program decreases across multiple examples. \nWe test our approach on CNLVR, a challenging visual reasoning dataset, where \nspuriousness is central because denotations are either TRUE or FALSE, and thus \nrandom programs have high probability of leading to a correct denotation. We \ndevelop the first semantic parser for this task and reach 83.5% accuracy, a \n15.7% absolute accuracy improvement compared to the best reported accuracy so \nfar. \n</p>"}, "author": "Omer Goldman, Veronica Latcinnik, Udi Naveh, Amir Globerson, Jonathan Berant", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704163", "id": "tag:google.com,2005:reader/item/0000000333923ff8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Loss Functions for Multiset Prediction. (arXiv:1711.05246v1 [cs.LG])", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05246"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05246", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of multiset prediction. The goal of multiset prediction \nis to train a predictor that maps an input to a multiset consisting of multiple \nitems. Unlike existing problems in supervised learning, such as classification, \nranking and sequence generation, there is no known order among items in a \ntarget multiset, and each item in the multiset may appear more than once, \nmaking this problem extremely challenging. In this paper, we propose a novel \nmultiset loss function by viewing this problem from the perspective of \nsequential decision making. The proposed multiset loss function is empirically \nevaluated on two families of datasets, one synthetic and the other real, with \nvarying levels of difficulty, against various baseline loss functions including \nreinforcement learning, sequence, and aggregated distribution matching loss \nfunctions. The experiments reveal the effectiveness of the proposed loss \nfunction over the others. \n</p>"}, "author": "Sean Welleck, Zixin Yao, Yu Gai, Jialin Mao, Zheng Zhang, Kyunghyun Cho", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704142", "id": "tag:google.com,2005:reader/item/00000003339240c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast Learning of Clusters and Topics via Sparse Posteriors. (arXiv:1609.07521v1 [stat.ML] CROSS LISTED)", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1609.07521"}], "alternate": [{"href": "http://arxiv.org/abs/1609.07521", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Mixture models and topic models generate each observation from a single \ncluster, but standard variational posteriors for each observation assign \npositive probability to all possible clusters. This requires dense storage and \nruntime costs that scale with the total number of clusters, even though \ntypically only a few clusters have significant posterior mass for any data \npoint. We propose a constrained family of sparse variational distributions that \nallow at most $L$ non-zero entries, where the tunable threshold $L$ trades off \nspeed for accuracy. Previous sparse approximations have used hard assignments \n($L=1$), but we find that moderate values of $L&gt;1$ provide superior \nperformance. Our approach easily integrates with stochastic or incremental \noptimization algorithms to scale to millions of examples. Experiments training \nmixture models of image patches and topic models for news articles show that \nour approach produces better-quality models in far less time than baseline \nmethods. \n</p>"}, "author": "Michael C. Hughes, Erik B. Sudderth", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704141", "id": "tag:google.com,2005:reader/item/00000003339240dd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations. (arXiv:1703.03717v2 [cs.LG] CROSS LISTED)", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.03717"}], "alternate": [{"href": "http://arxiv.org/abs/1703.03717", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks are among the most accurate supervised learning methods in \nuse today, but their opacity makes them difficult to trust in critical \napplications, especially when conditions in training differ from those in test. \nRecent work on explanations for black-box models has produced tools (e.g. LIME) \nto show the implicit rules behind predictions, which can help us identify when \nmodels are right for the wrong reasons. However, these methods do not scale to \nexplaining entire datasets and cannot correct the problems they reveal. We \nintroduce a method for efficiently explaining and regularizing differentiable \nmodels by examining and selectively penalizing their input gradients, which \nprovide a normal to the decision boundary. We apply these penalties both based \non expert annotation and in an unsupervised fashion that encourages diverse \nmodels with qualitatively different decision boundaries for the same \nclassification problem. On multiple datasets, we show our approach generates \nfaithful explanations and models that generalize much better when conditions \ndiffer between training and test. \n</p>"}, "author": "Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704140", "id": "tag:google.com,2005:reader/item/00000003339240eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Hybrid Approach with Multi-channel I-Vectors and Convolutional Neural Networks for Acoustic Scene Classification. (arXiv:1706.06525v1 [cs.SD] CROSS LISTED)", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.06525"}], "alternate": [{"href": "http://arxiv.org/abs/1706.06525", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In Acoustic Scene Classification (ASC) two major approaches have been \nfollowed . While one utilizes engineered features such as \nmel-frequency-cepstral-coefficients (MFCCs), the other uses learned features \nthat are the outcome of an optimization algorithm. I-vectors are the result of \na modeling technique that usually takes engineered features as input. It has \nbeen shown that standard MFCCs extracted from monaural audio signals lead to \ni-vectors that exhibit poor performance, especially on indoor acoustic scenes. \nAt the same time, Convolutional Neural Networks (CNNs) are well known for their \nability to learn features by optimizing their filters. They have been applied \non ASC and have shown promising results. In this paper, we first propose a \nnovel multi-channel i-vector extraction and scoring scheme for ASC, improving \ntheir performance on indoor and outdoor scenes. Second, we propose a CNN \narchitecture that achieves promising ASC results. Further, we show that \ni-vectors and CNNs capture complementary information from acoustic scenes. \nFinally, we propose a hybrid system for ASC using multi-channel i-vectors and \nCNNs by utilizing a score fusion technique. Using our method, we participated \nin the ASC task of the DCASE-2016 challenge. Our hybrid approach achieved 1 st \nrank among 49 submissions, substantially improving the previous state of the \nart. \n</p>"}, "author": "Hamid Eghbal-zadeh, Bernhard Lehner, Matthias Dorfer, Gerhard Widmer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704139", "id": "tag:google.com,2005:reader/item/00000003339240f5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Prediction-Constrained Training for Semi-Supervised Mixture and Topic Models. (arXiv:1707.07341v1 [stat.ML] CROSS LISTED)", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.07341"}], "alternate": [{"href": "http://arxiv.org/abs/1707.07341", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Supervisory signals have the potential to make low-dimensional data \nrepresentations, like those learned by mixture and topic models, more \ninterpretable and useful. We propose a framework for training latent variable \nmodels that explicitly balances two goals: recovery of faithful generative \nexplanations of high-dimensional data, and accurate prediction of associated \nsemantic labels. Existing approaches fail to achieve these goals due to an \nincomplete treatment of a fundamental asymmetry: the intended application is \nalways predicting labels from data, not data from labels. Our \nprediction-constrained objective for training generative models coherently \nintegrates loss-based supervisory signals while enabling effective \nsemi-supervised learning from partially labeled data. We derive learning \nalgorithms for semi-supervised mixture and topic models using stochastic \ngradient descent with automatic differentiation. We demonstrate improved \nprediction quality compared to several previous supervised topic models, \nachieving predictions competitive with high-dimensional logistic regression on \ntext sentiment analysis and electronic health records tasks while \nsimultaneously learning interpretable topics. \n</p>"}, "author": "Michael C. Hughes, Leah Weiner, Gabriel Hope, Thomas H. McCoy Jr., Roy H. Perlis, Erik B. Sudderth, Finale Doshi-Velez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704138", "id": "tag:google.com,2005:reader/item/0000000333924101", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Likelihood Estimation for Generative Adversarial Networks. (arXiv:1707.07530v1 [cs.LG] CROSS LISTED)", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.07530"}], "alternate": [{"href": "http://arxiv.org/abs/1707.07530", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a simple method for assessing the quality of generated images in \nGenerative Adversarial Networks (GANs). The method can be applied in any kind \nof GAN without interfering with the learning procedure or affecting the \nlearning objective. The central idea is to define a likelihood function that \ncorrelates with the quality of the generated images. In particular, we derive a \nGaussian likelihood function from the distribution of the embeddings (hidden \nactivations) of the real images in the discriminator, and based on this, define \ntwo simple measures of how likely it is that the embeddings of generated images \nare from the distribution of the embeddings of the real images. This yields a \nsimple measure of fitness for generated images, for all varieties of GANs. \nEmpirical results on CIFAR-10 demonstrate a strong correlation between the \nproposed measures and the perceived quality of the generated images. \n</p>"}, "author": "Hamid Eghbal-zadeh, Gerhard Widmer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704137", "id": "tag:google.com,2005:reader/item/000000033392410b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Recurrent Ladder Networks. (arXiv:1707.09219v3 [cs.NE] CROSS LISTED)", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.09219"}], "alternate": [{"href": "http://arxiv.org/abs/1707.09219", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a recurrent extension of the Ladder networks whose structure is \nmotivated by the inference required in hierarchical latent variable models. We \ndemonstrate that the recurrent Ladder is able to handle a wide variety of \ncomplex learning tasks that benefit from iterative inference and temporal \nmodeling. The architecture shows close-to-optimal results on temporal modeling \nof video data, competitive results on music modeling, and improved perceptual \ngrouping based on higher order abstractions, such as stochastic textures and \nmotion cues. We present results for fully supervised, semi-supervised, and \nunsupervised tasks. The results suggest that the proposed architecture and \nprinciples are powerful tools for learning a hierarchy of abstractions, \nlearning iterative inference and handling temporal information. \n</p>"}, "author": "Isabeau Pr&#xe9;mont-Schwarz, Alexander Ilin, Tele Hotloo Hao, Antti Rasmus, Rinu Boney, Harri Valpola", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510787214704", "timestampUsec": "1510787214704135", "id": "tag:google.com,2005:reader/item/000000033392411b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Physics-guided Neural Networks (PGNN): An Application in Lake Temperature Modeling. (arXiv:1710.11431v1 [cs.LG] CROSS LISTED)", "published": 1510787215, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11431"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c77f1527\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c77f1527&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper introduces a novel framework for learning data science models by \nusing the scientific knowledge encoded in physics-based models. This framework, \ntermed as physics-guided neural network (PGNN), leverages the output of \nphysics-based model simulations along with observational features to generate \npredictions using a neural network architecture. Further, we present a novel \nclass of learning objective for training neural networks, which ensures that \nthe model predictions not only show lower errors on the training data but are \nalso \\emph{consistent} with the known physics. We illustrate the effectiveness \nof PGNN for the problem of lake temperature modeling, where physical \nrelationships between the temperature, density, and depth of water are used in \nthe learning of neural network model parameters. By using scientific knowledge \nto guide the construction and learning of neural networks, we are able to show \nthat the proposed framework ensures better generalizability as well as physical \nconsistency of results. \n</p>"}, "author": "Anuj Karpatne, William Watkins, Jordan Read, Vipin Kumar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510784958537", "timestampUsec": "1510784958537299", "id": "tag:google.com,2005:reader/item/00000003338dd13f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "BP-STDP: Approximating Backpropagation using Spike Timing Dependent Plasticity. (arXiv:1711.04214v1 [cs.NE])", "published": 1510784959, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04214"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04214", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7909547\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7909547&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The problem of training spiking neural networks (SNNs) is a necessary \nprecondition to understanding computations within the brain, a field still in \nits infancy. Previous work has shown that supervised learning in multi-layer \nSNNs enables bio-inspired networks to recognize patterns of stimuli through \nhierarchical feature acquisition. Although gradient descent has shown \nimpressive performance in multi-layer (and deep) SNNs, it is generally not \nconsidered biologically plausible and is also computationally expensive. This \npaper proposes a novel supervised learning approach based on an event-based \nspike-timing-dependent plasticity (STDP) rule embedded in a network of \nintegrate-and-fire (IF) neurons. The proposed temporally local learning rule \nfollows the backpropagation weight change updates applied at each time step. \nThis approach enjoys benefits of both accurate gradient descent and temporally \nlocal, efficient STDP. Thus, this method is able to address some open questions \nregarding accurate and efficient computations that occur in the brain. The \nexperimental results on the XOR problem, the Iris data, and the MNIST dataset \ndemonstrate that the proposed SNN performs as successfully as the traditional \nNNs. Our approach also compares favorably with the state-of-the-art multi-layer \nSNNs. \n</p>"}, "author": "Amirhossein Tavanaei, Anthony S. Maida", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510784958537", "timestampUsec": "1510784958537298", "id": "tag:google.com,2005:reader/item/00000003338dd150", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Data Augmentation Generative Adversarial Networks. (arXiv:1711.04340v1 [stat.ML])", "published": 1510784959, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04340"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04340", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Effective training of neural networks requires much data. In the low-data \nregime, parameters are underdetermined, and learnt networks generalise poorly. \nData Augmentation \\cite{krizhevsky2012imagenet} alleviates this by using \nexisting data more effectively. However standard data augmentation produces \nonly limited plausible alternative data. Given there is potential to generate a \nmuch broader set of augmentations, we design and train a generative model to do \ndata augmentation. The model, based on image conditional Generative Adversarial \nNetworks, takes data from a source domain and learns to take any data item and \ngeneralise it to generate other within-class data items. As this generative \nprocess does not depend on the classes themselves, it can be applied to novel \nunseen classes of data. We show that a Data Augmentation Generative Adversarial \nNetwork (DAGAN) augments standard vanilla classifiers well. We also show a \nDAGAN can enhance few-shot learning systems such as Matching Networks. We \ndemonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on \nOmniglot, and VGG-Face data. In our experiments we can see over 13\\% increase \nin accuracy in the low-data regime experiments in Omniglot (from 69\\% to 82\\%), \nEMNIST (73.9\\% to 76\\%) and VGG-Face (4.5\\% to 12\\%); in Matching Networks for \nOmniglot we observe an increase of 0.5\\% (from 96.9\\% to 97.4\\%) and an \nincrease of 1.8\\% in EMNIST (from 59.5\\% to 61.3\\%). \n</p>"}, "author": "Antreas Antoniou, Amos Storkey, Harrison Edwards", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510784958537", "timestampUsec": "1510784958537297", "id": "tag:google.com,2005:reader/item/00000003338dd168", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Supervised Learning Concept for Reducing User Interaction in Passenger Cars. (arXiv:1711.04518v1 [cs.SY])", "published": 1510784959, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04518"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04518", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this article an automation system for human-machine-interfaces (HMI) for \nsetpoint adjustment using supervised learning is presented. We use HMIs of \nmulti-modal thermal conditioning systems in passenger cars as example for a \ncomplex setpoint selection system. The goal is the reduction of interaction \ncomplexity up to full automation. The approach is not limited to climate \ncontrol applications but can be extended to other setpoint-based HMIs. \n</p>"}, "author": "Marius St&#xe4;rk, Damian Backes, Christian Kehl", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510784958537", "timestampUsec": "1510784958537296", "id": "tag:google.com,2005:reader/item/00000003338dd181", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning Explanatory Rules from Noisy Data. (arXiv:1711.04574v1 [cs.NE])", "published": 1510784959, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04574"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04574", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Artificial Neural Networks are powerful function approximators capable of \nmodelling solutions to a wide variety of problems, both supervised and \nunsupervised. As their size and expressivity increases, so too does the \nvariance of the model, yielding a nearly ubiquitous overfitting problem. \nAlthough mitigated by a variety of model regularisation methods, the common \ncure is to seek large amounts of training data---which is not necessarily \neasily obtained---that sufficiently approximates the data distribution of the \ndomain we wish to test on. In contrast, logic programming methods such as \nInductive Logic Programming offer an extremely data-efficient process by which \nmodels can be trained to reason on symbolic domains. However, these methods are \nunable to deal with the variety of domains neural networks can be applied to: \nthey are not robust to noise in or mislabelling of inputs, and perhaps more \nimportantly, cannot be applied to non-symbolic domains where the data is \nambiguous, such as operating on raw pixels. In this paper, we propose a \nDifferentiable Inductive Logic framework ($\\partial$ILP), which can not only \nsolve tasks which traditional ILP systems are suited for, but shows a \nrobustness to noise and error in the training data which ILP cannot cope with. \nFurthermore, as it is trained by backpropagation against a likelihood \nobjective, it can be hybridised by connecting it with neural networks over \nambiguous data in order to be applied to domains which ILP cannot address, \nwhile providing data efficiency and generalisation beyond what neural networks \non their own can achieve. \n</p>"}, "author": "Richard Evans, Edward Grefenstette", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510784958537", "timestampUsec": "1510784958537295", "id": "tag:google.com,2005:reader/item/00000003338dd197", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Neural Networks Architecture Evaluation in a Quantum Computer. (arXiv:1711.04759v1 [cs.NE])", "published": 1510784959, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04759"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04759", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we propose a quantum algorithm to evaluate neural networks \narchitectures named Quantum Neural Network Architecture Evaluation (QNNAE). The \nproposed algorithm is based on a quantum associative memory and the learning \nalgorithm for artificial neural networks. Unlike conventional algorithms for \nevaluating neural network architectures, QNNAE does not depend on \ninitialization of weights. The proposed algorithm has a binary output and \nresults in 0 with probability proportional to the performance of the network. \nAnd its computational cost is equal to the computational cost to train a neural \nnetwork. \n</p>"}, "author": "Adenilton Jos&#xe9; da Silva, Rodolfo Luan F. de Oliveira", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510784958537", "timestampUsec": "1510784958537294", "id": "tag:google.com,2005:reader/item/00000003338dd1ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improving Factor-Based Quantitative Investing by Forecasting Company Fundamentals. (arXiv:1711.04837v1 [stat.ML])", "published": 1510784959, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04837"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04837", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>On a periodic basis, publicly traded companies are required to report \nfundamentals: financial data such as revenue, operating income, debt, among \nothers. These data points provide some insight into the financial health of a \ncompany. Academic research has identified some factors, i.e. computed features \nof the reported data, that are known through retrospective analysis to \noutperform the market average. Two popular factors are the book value \nnormalized by market capitalization (book-to-market) and the operating income \nnormalized by the enterprise value (EBIT/EV). In this paper: we first show \nthrough simulation that if we could (clairvoyantly) select stocks using factors \ncalculated on future fundamentals (via oracle), then our portfolios would far \noutperform a standard factor approach. Motivated by this analysis, we train \ndeep neural networks to forecast future fundamentals based on a trailing \n5-years window. Quantitative analysis demonstrates a significant improvement in \nMSE over a naive strategy. Moreover, in retrospective analysis using an \nindustry-grade stock portfolio simulator (backtester), we show an improvement \nin compounded annual return to 17.1% (MLP) vs 14.4% for a standard factor \nmodel. \n</p>"}, "author": "John Alberg, Zachary C. Lipton", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510784958537", "timestampUsec": "1510784958537293", "id": "tag:google.com,2005:reader/item/00000003338dd1f4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reliability and Sharpness in Border Crossing Traffic Interval Prediction. (arXiv:1711.04848v1 [cs.NE])", "published": 1510784959, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04848"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04848", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Short-term traffic volume prediction models have been extensively studied in \nthe past few decades. However, most of the previous studies only focus on \nsingle-value prediction. Considering the uncertain and chaotic nature of the \ntransportation system, an accurate and reliable prediction interval with upper \nand lower bounds may be better than a single point value for transportation \nmanagement. In this paper, we introduce a neural network model called Extreme \nLearning Machine (ELM) for interval prediction of short-term traffic volume and \nimprove it with the heuristic particle swarm optimization algorithm (PSO). The \nhybrid PSO-ELM model can generate the prediction intervals under different \nconfidence levels and guarantee the quality by minimizing a multi-objective \nfunction which considers two criteria reliability and interval sharpness. The \nPSO-ELM models are built based on an hourly traffic dataset and compared with \nARMA and Kalman Filter models. The results show that ARMA models are the worst \nfor all confidence levels, and the PSO-ELM models are comparable with Kalman \nFilter from the aspects of reliability and narrowness of the intervals, \nalthough the parameters of PSO-ELM are fixed once the training is done while \nKalman Filter is updated in an online approach. Additionally, only the PSO-ELMs \nare able to produce intervals with coverage probabilities higher than or equal \nto the confidence levels. For the points outside of the prediction levels given \nby PSO-ELMs, they lie very close to the bounds. \n</p>"}, "author": "Lei Lin, John Handley, Adel Sadek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510784958537", "timestampUsec": "1510784958537292", "id": "tag:google.com,2005:reader/item/00000003338dd21b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Concurrent Pump Scheduling and Storage Level Optimization Using Meta-Models and Evolutionary Algorithms. (arXiv:1711.04988v1 [cs.NE])", "published": 1510784959, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04988"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04988", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In spite of the growing computational power offered by the commodity \nhardware, fast pump scheduling of complex water distribution systems is still a \nchallenge. In this paper, the Artificial Neural Network (ANN) meta-modeling \ntechnique has been employed with a Genetic Algorithm (GA) for simultaneously \noptimizing the pump operation and the tank levels at the ends of the cycle. The \ngeneralized GA+ANN algorithm has been tested on a real system in the UK. \nComparing to the existing operation, the daily cost is reduced by about 10-15%, \nwhile the number of pump switches are kept below 4 switches-per-day. In \naddition, tank levels are optimized ensure a periodic behavior, which results \nin a predictable and stable performance over repeated cycles. \n</p>"}, "author": "Morad Behandish, Zheng Yi Wu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510784958537", "timestampUsec": "1510784958537291", "id": "tag:google.com,2005:reader/item/00000003338dd22d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fixing Weight Decay Regularization in Adam. (arXiv:1711.05101v1 [cs.LG])", "published": 1510784959, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05101"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05101", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We note that common implementations of adaptive gradient algorithms, such as \nAdam, limit the potential benefit of weight decay regularization, because the \nweights do not decay multiplicatively (as would be expected for standard weight \ndecay) but by an additive constant factor. We propose a simple way to resolve \nthis issue by decoupling weight decay and the optimization steps taken w.r.t. \nthe loss function. We provide empirical evidence that our proposed modification \n(i) decouples the optimal choice of weight decay factor from the setting of the \nlearning rate for both standard SGD and Adam, and (ii) substantially improves \nAdam's generalization performance, allowing it to compete with SGD with \nmomentum on image classification datasets (on which it was previously typically \noutperformed by the latter). We also demonstrate that longer optimization runs \nrequire smaller weight decay values for optimal results and introduce a \nnormalized variant of weight decay to reduce this dependence. Finally, we \npropose a version of Adam with warm restarts (AdamWR) that has strong anytime \nperformance while achieving state-of-the-art results on CIFAR-10 and \nImageNet32x32. Our source code is available at \nhttps://github.com/loshchil/AdamW-and-SGDW \n</p>"}, "author": "Ilya Loshchilov, Frank Hutter", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510784958537", "timestampUsec": "1510784958537290", "id": "tag:google.com,2005:reader/item/00000003338dd24c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Deep Rewiring: Training very sparse deep networks. (arXiv:1711.05136v1 [cs.NE])", "published": 1510784959, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05136"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05136", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7909930\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7909930&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Neuromorphic hardware tends to pose limits on the connectivity of deep \nnetworks that one can run on them. But also generic hardware and software \nimplementations of deep learning run more efficiently on sparse networks. \nSeveral methods exist for pruning connections of a neural network after it was \ntrained without connectivity constraints. We present an algorithm, DEEP R, that \nenables us to train directly a sparsely connected neural network. DEEP R \nautomatically rewires the network during supervised training so that \nconnections are there where they are most needed for the task, while its total \nnumber is all the time strictly bounded. We demonstrate that DEEP R can be used \nto train very sparse feedforward and recurrent neural networks on standard \nbenchmark tasks with just a minor loss in performance. DEEP R is based on a \nrigorous theoretical foundation that views rewiring as stochastic sampling of \nnetwork configurations from a posterior. \n</p>"}, "author": "Guillaume Bellec, David Kappel, Wolfgang Maass, Robert Legenstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126119", "id": "tag:google.com,2005:reader/item/00000003336d40f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Applications of Deep Learning and Reinforcement Learning to Biological Data. (arXiv:1711.03985v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03985"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03985", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Rapid advancement in the hardware based technologies over past decades opened \nup new possibilities for Biological and Life scientists to gather multimodal \ndata from various application domains (e.g., Omics, Bioimaging, Medical \nImaging, and [Brain/Body]-Machine Interfaces). Novel data intensive machine \nlearning techniques are required to decipher these data. Recent research in \nDeep learning (DL), Reinforcement learning (RL), and their combination (Deep \nRL) promise to revolutionize Artificial Intelligence. Increasing computational \npower, faster data storage devices, and declining computing costs allowed \nscientists to apply these techniques on such enormous and complex datasets \nwhich otherwise would not have been possible. This review article provides a \ncomprehensive survey of the applications of DL, RL, and Deep RL techniques in \nmining Biological data coming from various application domains. In addition, \nthe performances of DL techniques when applied to different datasets pertaining \nto the various application domains have been compared. Finally, it outlines \nsome open issues on this challenging research area and postulates possible \nfuture perspectives. \n</p>"}, "author": "Mufti Mahmud, M. Shamim Kaiser, Amir Hussain, Stefano Vassanelli", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126118", "id": "tag:google.com,2005:reader/item/00000003336d4106", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "WMRB: Learning to Rank in a Scalable Batch Training Approach. (arXiv:1711.04015v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04015"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04015", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new learning to rank algorithm, named Weighted Margin-Rank Batch \nloss (WMRB), to extend the popular Weighted Approximate-Rank Pairwise loss \n(WARP). WMRB uses a new rank estimator and an efficient batch training \nalgorithm. The approach allows more accurate item rank approximation and \nexplicit utilization of parallel computation to accelerate training. In three \nitem recommendation tasks, WMRB consistently outperforms WARP and other \nbaselines. Moreover, WMRB shows clear time efficiency advantages as data scale \nincreases. \n</p>"}, "author": "Kuan Liu, Prem Natarajan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126117", "id": "tag:google.com,2005:reader/item/00000003336d4119", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Batch Learning Framework for Scalable Personalized Ranking. (arXiv:1711.04019v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04019"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04019", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In designing personalized ranking algorithms, it is desirable to encourage a \nhigh precision at the top of the ranked list. Existing methods either seek a \nsmooth convex surrogate for a non-smooth ranking metric or directly modify \nupdating procedures to encourage top accuracy. In this work we point out that \nthese methods do not scale well to a large-scale setting, and this is partly \ndue to the inaccurate pointwise or pairwise rank estimation. We propose a new \nframework for personalized ranking. It uses batch-based rank estimators and \nsmooth rank-sensitive loss functions. This new batch learning framework leads \nto more stable and accurate rank approximations compared to previous work. \nMoreover, it enables explicit use of parallel computation to speed up training. \nWe conduct empirical evaluation on three item recommendation tasks. Our method \nshows consistent accuracy improvements over state-of-the-art methods. \nAdditionally, we observe time efficiency advantages when data scale increases. \n</p>"}, "author": "Kuan Liu, Prem Natarajan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126116", "id": "tag:google.com,2005:reader/item/00000003336d4137", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Few-Shot Learning with Graph Neural Networks. (arXiv:1711.04043v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04043"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04043", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose to study the problem of few-shot learning with the prism of \ninference on a partially observed graphical model, constructed from a \ncollection of input images whose label can be either observed or not. By \nassimilating generic message-passing inference algorithms with their \nneural-network counterparts, we define a graph neural network architecture that \ngeneralizes several of the recently proposed few-shot learning models. Besides \nproviding improved numerical performance, our framework is easily extended to \nvariants of few-shot learning, such as semi-supervised or active learning, \ndemonstrating the ability of graph-based models to operate well on 'relational' \ntasks. \n</p>"}, "author": "Victor Garcia, Joan Bruna", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126115", "id": "tag:google.com,2005:reader/item/00000003336d4148", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficient Representation for Natural Language Processing via Kernelized Hashcodes. (arXiv:1711.04044v1 [cs.CL])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04044"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04044", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Kernel similarity functions have been successfully applied in classification \nmodels such as Support Vector Machines, Gaussian Processes and k-Nearest \nNeighbors (kNN), but found to be computationally expensive for Natural Language \nProcessing (NLP) tasks due to the cost of computing kernel similarities between \ndiscrete natural language structures. A well-known technique, Kernelized \nLocality Sensitive Hashing (KLSH), allows for an approximate computation of kNN \ngraphs and significantly reduces the number of kernel computations; however, \napplying KLSH to other classifiers have not been explored. In this paper, we \npropose to use random subspaces of KLSH codes for constructing an efficient \nrepresentation that preserves fine-grained structure of the data and is \nsuitable for general classification methods. Further, we proposed an approach \nfor optimizing KLSH model for supervised classification problems, by maximizing \na variational lower bound on the mutual information between the KLSH codes \n(feature vectors) and the class labels.We apply the proposed approach to the \ntask of extracting information about bio-molecular interactions from the \nsemantic parsing of scientific papers. Our empirical results on a variety of \ndatasets demonstrate significant improvements over the state of the art. \n</p>"}, "author": "Sahil Garg, Aram Galstyan, Irina Rish, Guillermo Cecchi, Shuyang Gao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126114", "id": "tag:google.com,2005:reader/item/00000003336d416d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Parkinson's Disease Digital Biomarker Discovery with Optimized Transitions and Inferred Markov Emissions. (arXiv:1711.04078v1 [q-bio.QM])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04078"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04078", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We search for digital biomarkers from Parkinson's Disease by observing \napproximate repetitive patterns matching hypothesized step and stride periodic \ncycles. These observations were modeled as a cycle of hidden states with \nrandomness allowing deviation from a canonical pattern of transitions and \nemissions, under the hypothesis that the averaged features of hidden states \nwould serve to informatively characterize classes of patients/controls. We \npropose a Hidden Semi-Markov Model (HSMM), a latent-state model, emitting \n3D-acceleration vectors. Transitions and emissions are inferred from data. We \nfit separate models per unique device and training label. Hidden Markov Models \n(HMM) force geometric distributions of the duration spent at each state before \ntransition to a new state. Instead, our HSMM allows us to specify the \ndistribution of state duration. This modified version is more effective because \nwe are interested more in each state's duration than the sequence of distinct \nstates, allowing inclusion of these durations the feature vector. \n</p>"}, "author": "Avinash Bukkittu, Baihan Lin, Trung Vu, Itsik Pe&#x27;er", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126113", "id": "tag:google.com,2005:reader/item/00000003336d41b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalized Neural Graph Embedding with Matrix Factorization. (arXiv:1711.04094v1 [cs.SI])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04094"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04094", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent advances in language modeling such as word2vec motivate a number of \ngraph embedding approaches by treating random walk sequences as sentences to \nencode structural proximity in a graph. However, most of the existing \nprinciples of neural graph embedding do not incorporate auxiliary information \nsuch as node content flexibly. In this paper we take a matrix factorization \nperspective of graph embedding which generalizes to structural embedding as \nwell as content embedding in a natural way. For structure embedding, we \nvalidate that the matrix we construct and factorize preserves the high-order \nproximities of the graph. Label information can be further integrated into the \nmatrix via the process of random walk sampling to enhance the quality of \nembedding. In addition, we generalize the Skip-Gram Negative Sampling model to \nintegrate the content of the graph in a matrix factorization framework. As a \nconsequence, graph embedding can be learned in a unified framework integrating \ngraph structure and node content as well as label information simultaneously. \nWe demonstrate the efficacy of the proposed model with the tasks of \nsemi-supervised node classification and link prediction on a variety of \nreal-world benchmark network datasets. \n</p>"}, "author": "Junliang Guo, Linli Xu, Xunpeng Huang, Enhong Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126112", "id": "tag:google.com,2005:reader/item/00000003336d4218", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Sparse Graph-Structured Lasso Mixed Model for Genetic Association with Confounding Correction. (arXiv:1711.04162v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04162"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04162", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>While linear mixed model (LMM) has shown a competitive performance in \ncorrecting spurious associations raised by population stratification, family \nstructures, and cryptic relatedness, more challenges are still to be addressed \nregarding the complex structure of genotypic and phenotypic data. For example, \ngeneticists have discovered that some clusters of phenotypes are more \nco-expressed than others. Hence, a joint analysis that can utilize such \nrelatedness information in a heterogeneous data set is crucial for genetic \nmodeling. \n</p> \n<p>We proposed the sparse graph-structured linear mixed model (sGLMM) that can \nincorporate the relatedness information from traits in a dataset with \nconfounding correction. Our method is capable of uncovering the genetic \nassociations of a large number of phenotypes together while considering the \nrelatedness of these phenotypes. Through extensive simulation experiments, we \nshow that the proposed model outperforms other existing approaches and can \nmodel correlation from both population structure and shared signals. Further, \nwe validate the effectiveness of sGLMM in the real-world genomic dataset on two \ndifferent species from plants and humans. In Arabidopsis thaliana data, sGLMM \nbehaves better than all other baseline models for 63.4% traits. We also discuss \nthe potential causal genetic variation of Human Alzheimer's disease discovered \nby our model and justify some of the most important genetic loci. \n</p>"}, "author": "Wenting Ye, Xiang Liu, Haohan Wang, Eric P. Xing", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126111", "id": "tag:google.com,2005:reader/item/00000003336d4243", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning Document Embeddings With CNNs. (arXiv:1711.04168v1 [cs.CL])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04168"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04168", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new model for unsupervised document embedding. Existing \napproaches either require complex inference or use recurrent neural networks \nthat are difficult to parallelize. We take a different route and use recent \nadvances in language modelling to develop a convolutional neural network \nembedding model. This allows us to train deeper architectures that are fully \nparallelizable. Stacking layers together increases the receptive field allowing \neach successive layer to model increasingly longer range semantic dependencies \nwithin the document. Empirically, we demonstrate superior results on two \npublicly available benchmarks. \n</p>"}, "author": "Chundi Liu, Shunan Zhao, Maksims Volkovs", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126110", "id": "tag:google.com,2005:reader/item/00000003336d4261", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Linking Sequences of Events with Sparse or No Common Occurrence across Data Sets. (arXiv:1711.04248v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04248"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04248", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7909d0a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7909d0a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Data of practical interest - such as personal records, transaction logs, and \nmedical histories - are sequential collections of events relevant to a \nparticular source entity. Recent studies have attempted to link sequences that \nrepresent a common entity across data sets to allow more comprehensive \nstatistical analyses and to identify potential privacy failures. Yet, current \napproaches remain tailored to their specific domains of application, and they \nfail when co-referent sequences in different data sets contain sparse or no \ncommon events, which occurs frequently in many cases. \n</p> \n<p>To address this, we formalize the general problem of \"sequence linkage\" and \ndescribe \"LDA-Link,\" a generic solution that is applicable even when \nco-referent event sequences contain no common items at all. LDA-Link is built \nupon \"Split-Document\" model, a new mixed-membership probabilistic model for the \ngeneration of event sequence collections. It detects the latent similarity of \nsequences and thus achieves robustness particularly when co-referent sequences \nshare sparse or no event overlap. We apply LDA-Link in the context of social \nmedia profile reconciliation where users make no common posts across platforms, \ncomparing to the state-of-the-art generic solution to sequence linkage. \n</p>"}, "author": "Yunsung Kim", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126109", "id": "tag:google.com,2005:reader/item/00000003336d428e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Unified Spectral Clustering with Optimal Graph. (arXiv:1711.04258v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04258"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04258", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c79c647e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c79c647e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Spectral clustering has found extensive use in many areas. Most traditional \nspectral clustering algorithms work in three separate steps: similarity graph \nconstruction; continuous labels learning; discretizing the learned labels by \nk-means clustering. Such common practice has two potential flaws, which may \nlead to severe information loss and performance degradation. First, predefined \nsimilarity graph might not be optimal for subsequent clustering. It is \nwell-accepted that similarity graph highly affects the clustering results. To \nthis end, we propose to automatically learn similarity information from data \nand simultaneously consider the constraint that the similarity matrix has exact \nc connected components if there are c clusters. Second, the discrete solution \nmay deviate from the spectral solution since k-means method is well-known as \nsensitive to the initialization of cluster centers. In this work, we transform \nthe candidate solution into a new one that better approximates the discrete \none. Finally, those three subtasks are integrated into a unified framework, \nwith each subtask iteratively boosted by using the results of the others \ntowards an overall optimal solution. It is known that the performance of a \nkernel method is largely determined by the choice of kernels. To tackle this \npractical problem of how to select the most suitable kernel for a particular \ndata set, we further extend our model to incorporate multiple kernel learning \nability. Extensive experiments demonstrate the superiority of our proposed \nmethod as compared to existing clustering approaches. \n</p>"}, "author": "Zhao Kang, Chong Peng, Qiang Cheng, Zenglin Xu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126108", "id": "tag:google.com,2005:reader/item/00000003336d42a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Scale out for large minibatch SGD: Residual network training on ImageNet-1K with improved accuracy and reduced time to train. (arXiv:1711.04291v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04291"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04291", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>For the past 5 years, the ILSVRC competition and the ImageNet dataset have \nattracted a lot of interest from the Computer Vision community, allowing for \nstate-of-the-art accuracy to grow tremendously. This should be credited to the \nuse of deep artificial neural network designs. As these became more complex, \nthe storage, bandwidth, and compute requirements increased. This means that \nwith a non-distributed approach, even when using the most high-density server \navailable, the training process may take weeks, making it prohibitive. \nFurthermore, as datasets grow, the representation learning potential of deep \nnetworks grows as well by using more complex models. This synchronicity \ntriggers a sharp increase in the computational requirements and motivates us to \nexplore the scaling behaviour on petaflop scale supercomputers. In this paper \nwe will describe the challenges and novel solutions needed in order to train \nResNet-50 in this large scale environment. We demonstrate above 90\\% scaling \nefficiency and a training time of 28 minutes using up to 104K x86 cores. This \nis supported by software tools from Intel's ecosystem. Moreover, we show that \nwith regular 90 - 120 epoch train runs we can achieve a top-1 accuracy as high \nas 77\\% for the unmodified ResNet-50 topology. We also introduce the novel \nCollapsed Ensemble (CE) technique that allows us to obtain a 77.5\\% top-1 \naccuracy, similar to that of a ResNet-152, while training a unmodified \nResNet-50 topology for the same fixed training budget. All ResNet-50 models as \nwell as the scripts needed to replicate them will be posted shortly. \n</p>"}, "author": "Valeriu Codreanu, Damian Podareanu, Vikram Saletore", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126107", "id": "tag:google.com,2005:reader/item/00000003336d432c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Sequence-Based Mesh Classifier for the Prediction of Protein-Protein Interactions. (arXiv:1711.04294v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04294"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04294", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The worldwide surge of multiresistant microbial strains has propelled the \nsearch for alternative treatment options. The study of Protein-Protein \nInteractions (PPIs) has been a cornerstone in the clarification of complex \nphysiological and pathogenic processes, thus being a priority for the \nidentification of vital components and mechanisms in pathogens. Despite the \nadvances of laboratorial techniques, computational models allow the screening \nof protein interactions between entire proteomes in a fast and inexpensive \nmanner. Here, we present a supervised machine learning model for the prediction \nof PPIs based on the protein sequence. We cluster amino acids regarding their \nphysicochemical properties, and use the discrete cosine transform to represent \nprotein sequences. A mesh of classifiers was constructed to create \nhyper-specialised classifiers dedicated to the most relevant pairs of molecular \nfunction annotations from Gene Ontology. Based on an exhaustive evaluation that \nincludes datasets with different configurations, cross-validation and \nout-of-sampling validation, the obtained results outscore the state-of-the-art \nfor sequence-based methods. For the final mesh model using SVM with RBF, a \nconsistent average AUC of 0.84 was attained. \n</p>"}, "author": "Edgar D. Coelho, Igor N. Cruz, Andr&#xe9; Santiago, Jos&#xe9; Luis Oliveira, Ant&#xf3;nio Dourado, Joel P. Arrais", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126106", "id": "tag:google.com,2005:reader/item/00000003336d43aa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the ERM Principle with Networked Data. (arXiv:1711.04297v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04297"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04297", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Networked data, in which every training example involves two objects and may \nshare some common objects with others, is used in many machine learning tasks \nsuch as learning to rank and link prediction. A challenge of learning from \nnetworked examples is that target values are not known for some pairs of \nobjects. In this case, neither the classical i.i.d.\\ assumption nor techniques \nbased on complete U-statistics can be used. Most existing theoretical results \nof this problem only deal with the classical empirical risk minimization (ERM) \nprinciple that always weights every example equally, but this strategy leads to \nunsatisfactory bounds. We consider general weighted ERM and show new universal \nrisk bounds for this problem. These new bounds naturally define an optimization \nproblem which leads to appropriate weights for networked examples. Though this \noptimization problem is not convex in general, we devise a new fully \npolynomial-time approximation scheme (FPTAS) to solve it. \n</p>"}, "author": "Yuanhong Wang, Yuyi Wang, Xingwu Liu, Juhua Pu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126105", "id": "tag:google.com,2005:reader/item/00000003336d43e4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sensor Selection and Random Field Reconstruction for Robust and Cost-effective Heterogeneous Weather Sensor Networks for the Developing World. (arXiv:1711.04308v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04308"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04308", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We address the two fundamental problems of spatial field reconstruction and \nsensor selection in heterogeneous sensor networks: (i) how to efficiently \nperform spatial field reconstruction based on measurements obtained \nsimultaneously from networks with both high and low quality sensors; and (ii) \nhow to perform query based sensor set selection with predictive MSE performance \nguarantee. For the first problem, we developed a low complexity algorithm based \non the spatial best linear unbiased estimator (S-BLUE). Next, building on the \nS-BLUE, we address the second problem, and develop an efficient algorithm for \nquery based sensor set selection with performance guarantee. Our algorithm is \nbased on the Cross Entropy method which solves the combinatorial optimization \nproblem in an efficient manner. \n</p>"}, "author": "Pengfei Zhang, Ido Nevat, Gareth W. Peters, Wolfgang Fruhwirt, Yongchao Huang, Ivonne Anders, Michael Osborne", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126104", "id": "tag:google.com,2005:reader/item/00000003336d4425", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Semi-Supervised Learning via New Deep Network Inversion. (arXiv:1711.04313v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04313"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04313", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We exploit a recently derived inversion scheme for arbitrary deep neural \nnetworks to develop a new semi-supervised learning framework that applies to a \nwide range of systems and problems. The approach outperforms current \nstate-of-the-art methods on MNIST reaching $99.14\\%$ of test set accuracy while \nusing $5$ labeled examples per class. Experiments with one-dimensional signals \nhighlight the generality of the method. Importantly, our approach is simple, \nefficient, and requires no change in the deep network architecture. \n</p>"}, "author": "Randall Balestriero, Vincent Roger, Herve G. Glotin, Richard G. Baraniuk", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126103", "id": "tag:google.com,2005:reader/item/00000003336d443d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A machine learning approach for efficient uncertainty quantification using multiscale methods. (arXiv:1711.04315v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04315"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04315", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Several multiscale methods account for sub-grid scale features using coarse \nscale basis functions. For example, in the Multiscale Finite Volume method the \ncoarse scale basis functions are obtained by solving a set of local problems \nover dual-grid cells. We introduce a data-driven approach for the estimation of \nthese coarse scale basis functions. Specifically, we employ a neural network \npredictor fitted using a set of solution samples from which it learns to \ngenerate subsequent basis functions at a lower computational cost than solving \nthe local problems. The computational advantage of this approach is realized \nfor uncertainty quantification tasks where a large number of realizations has \nto be evaluated. We attribute the ability to learn these basis functions to the \nmodularity of the local problems and the redundancy of the permeability patches \nbetween samples. The proposed method is evaluated on elliptic problems yielding \nvery promising results. \n</p>"}, "author": "Shing Chan, Ahmed H. Elsheikh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126102", "id": "tag:google.com,2005:reader/item/00000003336d444c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Data Augmentation Generative Adversarial Networks. (arXiv:1711.04340v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04340"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04340", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Effective training of neural networks requires much data. In the low-data \nregime, parameters are underdetermined, and learnt networks generalise poorly. \nData Augmentation \\cite{krizhevsky2012imagenet} alleviates this by using \nexisting data more effectively. However standard data augmentation produces \nonly limited plausible alternative data. Given there is potential to generate a \nmuch broader set of augmentations, we design and train a generative model to do \ndata augmentation. The model, based on image conditional Generative Adversarial \nNetworks, takes data from a source domain and learns to take any data item and \ngeneralise it to generate other within-class data items. As this generative \nprocess does not depend on the classes themselves, it can be applied to novel \nunseen classes of data. We show that a Data Augmentation Generative Adversarial \nNetwork (DAGAN) augments standard vanilla classifiers well. We also show a \nDAGAN can enhance few-shot learning systems such as Matching Networks. We \ndemonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on \nOmniglot, and VGG-Face data. In our experiments we can see over 13\\% increase \nin accuracy in the low-data regime experiments in Omniglot (from 69\\% to 82\\%), \nEMNIST (73.9\\% to 76\\%) and VGG-Face (4.5\\% to 12\\%); in Matching Networks for \nOmniglot we observe an increase of 0.5\\% (from 96.9\\% to 97.4\\%) and an \nincrease of 1.8\\% in EMNIST (from 59.5\\% to 61.3\\%). \n</p>"}, "author": "Antreas Antoniou, Amos Storkey, Harrison Edwards", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126101", "id": "tag:google.com,2005:reader/item/00000003336d44b6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Alpha-Divergences in Variational Dropout. (arXiv:1711.04345v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04345"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04345", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We investigate the use of alternative divergences to Kullback-Leibler (KL) in \nvariational inference(VI), based on the Variational Dropout \\cite{kingma2015}. \nStochastic gradient variational Bayes (SGVB) \\cite{aevb} is a general framework \nfor estimating the evidence lower bound (ELBO) in Variational Bayes. In this \nwork, we extend the SGVB estimator with using Alpha-Divergences, which are \nalternative to divergences to VI' KL objective. The Gaussian dropout can be \nseen as a local reparametrization trick of the SGVB objective. We extend the \nVariational Dropout to use alpha divergences for variational inference. Our \nresults compare $\\alpha$-divergence variational dropout with standard \nvariational dropout with correlated and uncorrelated weight noise. We show that \nthe $\\alpha$-divergence with $\\alpha \\rightarrow 1$ (or KL divergence) is still \na good measure for use in variational inference, in spite of the efficient use \nof Alpha-divergences for Dropout VI \\cite{Li17}. $\\alpha \\rightarrow 1$ can \nyield the lowest training error, and optimizes a good lower bound for the \nevidence lower bound (ELBO) among all values of the parameter $\\alpha \\in \n[0,\\infty)$. \n</p>"}, "author": "Bogdan Mazoure, Riashat Islam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126100", "id": "tag:google.com,2005:reader/item/00000003336d44c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Machine vs Machine: Defending Classifiers Against Learning-based Adversarial Attacks. (arXiv:1711.04368v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04368"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04368", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c79c68b6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c79c68b6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Recently, researchers have discovered that the state-of-the-art object \nclassifiers can be fooled easily by small perturbations in the input \nunnoticeable to human eyes. Several methods were proposed to craft adversarial \nexamples, as well as methods of robustifying the classifier against such \nexamples. An attacker with the knowledge of the classifier parameters can \ngenerate strong adversarial patterns. Conversely, a classifier with the \nknowledge of such patterns can be trained to be robust to them. The \ncat-and-mouse game nature of the attacks and the defenses raises the question \nof the presence of an equilibrium in the dynamic. In this paper, we propose a \ngame framework to formulate the interaction of attacks and defenses and present \nthe natural notion of the best worst-case defense and attack. We propose simple \nalgorithms to numerically find those solutions motivated by sensitivity \npenalization. In addition, we show the potentials of learning-based attacks, \nand present the close relationship between the adversarial attack and the \nprivacy attack problems. The results are demonstrated with MNIST and CIFAR-10 \ndatasets. \n</p>"}, "author": "Jihun Hamm", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126099", "id": "tag:google.com,2005:reader/item/00000003336d44ff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Should You Derive, Or Let the Data Drive? An Optimization Framework for Hybrid First-Principles Data-Driven Modeling. (arXiv:1711.04374v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04374"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04374", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Mathematical models are used extensively for diverse tasks including \nanalysis, optimization, and decision making. Frequently, those models are \nprincipled but imperfect representations of reality. This is either due to \nincomplete physical description of the underlying phenomenon (simplified \ngoverning equations, defective boundary conditions, etc.), or due to numerical \napproximations (discretization, linearization, round-off error, etc.). Model \nmisspecification can lead to erroneous model predictions, and respectively \nsuboptimal decisions associated with the intended end-goal task. To mitigate \nthis effect, one can amend the available model using limited data produced by \nexperiments or higher fidelity models. A large body of research has focused on \nestimating explicit model parameters. This work takes a different perspective \nand targets the construction of a correction model operator with implicit \nattributes. We investigate the case where the end-goal is inversion and \nillustrate how appropriate choices of properties imposed upon the correction \nand corrected operator lead to improved end-goal insights. \n</p>"}, "author": "Remi R. Lam, Lior Horesh, Haim Avron, Karen E. Willcox", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126098", "id": "tag:google.com,2005:reader/item/00000003336d453d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variance Reduced methods for Non-convex Composition Optimization. (arXiv:1711.04416v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04416"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04416", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper explores the non-convex composition optimization in the form \nincluding inner and outer finite-sum functions with a large number of component \nfunctions. This problem arises in some important applications such as nonlinear \nembedding and reinforcement learning. Although existing approaches such as \nstochastic gradient descent (SGD) and stochastic variance reduced gradient \n(SVRG) descent can be applied to solve this problem, their query complexity \ntends to be high, especially when the number of inner component functions is \nlarge. In this paper, we apply the variance-reduced technique to derive two \nvariance reduced algorithms that significantly improve the query complexity if \nthe number of inner component functions is large. To the best of our knowledge, \nthis is the first work that establishes the query complexity analysis for \nnon-convex stochastic composition. Experiments validate the proposed algorithms \nand theoretical analysis. \n</p>"}, "author": "Liu Liu, Ji Liu, Dacheng Tao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126097", "id": "tag:google.com,2005:reader/item/00000003336d4551", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Analyzing and Improving Stein Variational Gradient Descent for High-dimensional Marginal Inference. (arXiv:1711.04425v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04425"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04425", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stein variational gradient descent (SVGD) is a nonparametric inference \nmethod, which iteratively transports a set of randomly initialized particles to \napproximate a differentiable target distribution, along the direction that \nmaximally decreases the KL divergence within a vector-valued reproducing kernel \nHilbert space (RKHS). Compared to Monte Carlo methods, SVGD is \nparticle-efficient because of the repulsive force induced by kernels. In this \npaper, we develop the first analysis about the high dimensional performance of \nSVGD and emonstrate that the repulsive force drops at least polynomially with \nincreasing dimensions, which results in poor marginal approximation. To improve \nthe marginal inference of SVGD, we propose Marginal SVGD (M-SVGD), which \nincorporates structural information described by a Markov random field (MRF) \ninto kernels. M-SVGD inherits the particle efficiency of SVGD and can be used \nas a general purpose marginal inference tool for MRFs. Experimental results on \ngrid based Markov random fields show the effectiveness of our methods. \n</p>"}, "author": "Jingwei Zhuo, Chang Liu, Ning Chen, Bo Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126096", "id": "tag:google.com,2005:reader/item/00000003336d4579", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Thresholding Bandit for Dose-ranging: The Impact of Monotonicity. (arXiv:1711.04454v1 [math.ST])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04454"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04454", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We analyze the sample complexity of the thresholding bandit problem, with and \nwithout the assumption that the mean values of the arms are increasing. In each \ncase, we provide a lower bound valid for any risk $\\delta$ and any \n$\\delta$-correct algorithm; in addition, we propose an algorithm whose sample \ncomplexity is of the same order of magnitude for small risks. This work is \nmotivated by phase 1 clinical trials, a practically important setting where the \narm means are increasing by nature, and where no satisfactory solution is \navailable so far. \n</p>"}, "author": "Aur&#xe9;lien Garivier (IMT), Pierre M&#xe9;nard (IMT), Laurent Rossi (IMT)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126095", "id": "tag:google.com,2005:reader/item/00000003336d4590", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Blind Source Separation Using Mixtures of Alpha-Stable Distributions. (arXiv:1711.04460v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04460"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04460", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new blind source separation algorithm based on mixtures of \nalpha-stable distributions. Complex symmetric alpha-stable distributions have \nbeen recently showed to better model audio signals in the time-frequency domain \nthan classical Gaussian distributions thanks to their larger dynamic range. \nHowever, inference of these models is notoriously hard to perform because their \nprobability density functions do not have a closed-form expression in general. \nHere, we introduce a novel method for estimating mixture of alpha-stable \ndistributions based on random moment matching. We apply this to the blind \nestimation of binary masks in individual frequency bands from multichannel \nconvolutive audio mixes. We show that the proposed method yields better \nseparation performance than Gaussian-based binary-masking methods. \n</p>"}, "author": "Nicolas Keriven, Antoine Deleforge (PANAMA), Antoine Liutkus (ZENITH)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126094", "id": "tag:google.com,2005:reader/item/00000003336d45a0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Parallel Best-Response Algorithm with Exact Line Search for Nonconvex Sparsity-Regularized Rank Minimization. (arXiv:1711.04489v1 [cs.DC])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04489"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04489", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a convergent parallel best-response algorithm with \nthe exact line search for the nondifferentiable nonconvex sparsity-regularized \nrank minimization problem. On the one hand, it exhibits a faster convergence \nthan subgradient algorithms and block coordinate descent algorithms. On the \nother hand, its convergence to a stationary point is guaranteed, while ADMM \nalgorithms only converge for convex problems. Furthermore, the exact line \nsearch procedure in the proposed algorithm is performed efficiently in \nclosed-form to avoid the meticulous choice of stepsizes, which is however a \ncommon bottleneck in subgradient algorithms and successive convex approximation \nalgorithms. Finally, the proposed algorithm is numerically tested. \n</p>"}, "author": "Yang Yang, Marius Pesavento", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126093", "id": "tag:google.com,2005:reader/item/00000003336d45b7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Simple And Efficient Architecture Search for Convolutional Neural Networks. (arXiv:1711.04528v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04528"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04528", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks have recently had a lot of success for many tasks. However, \nneural network architectures that perform well are still typically designed \nmanually by experts in a cumbersome trial-and-error process. We propose a new \nmethod to automatically search for well-performing CNN architectures based on a \nsimple hill climbing procedure whose operators apply network morphisms, \nfollowed by short optimization runs by cosine annealing. Surprisingly, this \nsimple method yields competitive results, despite only requiring resources in \nthe same order of magnitude as training a single network. E.g., on CIFAR-10, \nour method designs and trains networks with an error rate below 6% in only 12 \nhours on a single GPU; training for one day reduces this error further, to \nalmost 5%. \n</p>"}, "author": "Thomas Elsken, Jan-Hendrik Metzen, Frank Hutter", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126092", "id": "tag:google.com,2005:reader/item/00000003336d45bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Model Criticism in Latent Space. (arXiv:1711.04674v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04674"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04674", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Model criticism is usually carried out by assessing if replicated data \ngenerated under the fitted model looks similar to the observed data, see e.g. \nGelman, Carlin, Stern, and Rubin (2004, p. 165). This paper presents a method \nfor latent variable models by pulling back the data into the space of latent \nvariables, and carrying out model criticism in that space. Making use of a \nmodel's structure enables a more direct assessment of the assumptions made in \nthe prior and likelihood. We demonstrate the method with examples of model \ncriticism in latent space applied to ANOVA, factor analysis, linear dynamical \nsystems and Gaussian processes. \n</p>"}, "author": "Sohan Seth, Iain Murray, Christopher K. I. Williams", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126091", "id": "tag:google.com,2005:reader/item/00000003336d45ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Attention-based Information Fusion using Multi-Encoder-Decoder Recurrent Neural Networks. (arXiv:1711.04679v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04679"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04679", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the rising number of interconnected devices and sensors, modeling \ndistributed sensor networks is of increasing interest. Recurrent neural \nnetworks (RNN) are considered particularly well suited for modeling sensory and \nstreaming data. When predicting future behavior, incorporating information from \nneighboring sensor stations is often beneficial. We propose a new RNN based \narchitecture for context specific information fusion across multiple spatially \ndistributed sensor stations. Hereby, latent representations of multiple local \nmodels, each modeling one sensor station, are jointed and weighted, according \nto their importance for the prediction. The particular importance is assessed \ndepending on the current context using a separate attention function. We \ndemonstrate the effectiveness of our model on three different real-world sensor \nnetwork datasets. \n</p>"}, "author": "Stephan Baier, Sigurd Spieckermann, Volker Tresp", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126090", "id": "tag:google.com,2005:reader/item/00000003336d45d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Randomized Near Neighbor Graphs, Giant Components, and Applications in Data Science. (arXiv:1711.04712v1 [math.CO])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04712"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04712", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c79c6cb3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c79c6cb3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>If we pick $n$ random points uniformly in $[0,1]^d$ and connect each point to \nits $k-$nearest neighbors, then it is well known that there exists a giant \nconnected component with high probability. We prove that in $[0,1]^d$ it \nsuffices to connect every point to $ c_{d,1} \\log{\\log{n}}$ points chosen \nrandomly among its $ c_{d,2} \\log{n}-$nearest neighbors to ensure a giant \ncomponent of size $n - o(n)$ with high probability. This construction yields a \nmuch sparser random graph with $\\sim n \\log\\log{n}$ instead of $\\sim n \\log{n}$ \nedges that has comparable connectivity properties. This result has nontrivial \nimplications for problems in data science where an affinity matrix is \nconstructed: instead of picking the $k-$nearest neighbors, one can often pick \n$k' \\ll k$ random points out of the $k-$nearest neighbors without sacrificing \nefficiency. This can massively simplify and accelerate computation, we \nillustrate this with several numerical examples. \n</p>"}, "author": "George C. Linderman, Gal Mishne, Yuval Kluger, Stefan Steinerberger", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126089", "id": "tag:google.com,2005:reader/item/00000003336d45e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. (arXiv:1711.04735v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04735"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04735", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7ab55d5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7ab55d5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>It is well known that the initialization of weights in deep neural networks \ncan have a dramatic impact on learning speed. For example, ensuring the mean \nsquared singular value of a network's input-output Jacobian is $O(1)$ is \nessential for avoiding the exponential vanishing or explosion of gradients. The \nstronger condition that all singular values of the Jacobian concentrate near \n$1$ is a property known as dynamical isometry. For deep linear networks, \ndynamical isometry can be achieved through orthogonal weight initialization and \nhas been shown to dramatically speed up learning; however, it has remained \nunclear how to extend these results to the nonlinear setting. We address this \nquestion by employing powerful tools from free probability theory to compute \nanalytically the entire singular value distribution of a deep network's \ninput-output Jacobian. We explore the dependence of the singular value \ndistribution on the depth of the network, the weight initialization, and the \nchoice of nonlinearity. Intriguingly, we find that ReLU networks are incapable \nof dynamical isometry. On the other hand, sigmoidal networks can achieve \nisometry, but only with orthogonal weight initialization. Moreover, we \ndemonstrate empirically that deep nonlinear networks achieving dynamical \nisometry learn orders of magnitude faster than networks that do not. Indeed, we \nshow that properly-initialized deep sigmoidal networks consistently outperform \ndeep ReLU networks. Overall, our analysis reveals that controlling the entire \ndistribution of Jacobian singular values is an important design consideration \nin deep learning. \n</p>"}, "author": "Jeffrey Pennington, Samuel S. Schoenholz, Surya Ganguli", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126088", "id": "tag:google.com,2005:reader/item/00000003336d45fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "ACtuAL: Actor-Critic Under Adversarial Learning. (arXiv:1711.04755v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04755"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04755", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative Adversarial Networks (GANs) are a powerful framework for deep \ngenerative modeling. Posed as a two-player minimax problem, GANs are typically \ntrained end-to-end on real-valued data and can be used to train a generator of \nhigh-dimensional and realistic images. However, a major limitation of GANs is \nthat training relies on passing gradients from the discriminator through the \ngenerator via back-propagation. This makes it fundamentally difficult to train \nGANs with discrete data, as generation in this case typically involves a \nnon-differentiable function. These difficulties extend to the reinforcement \nlearning setting when the action space is composed of discrete decisions. We \naddress these issues by reframing the GAN framework so that the generator is no \nlonger trained using gradients through the discriminator, but is instead \ntrained using a learned critic in the actor-critic framework with a Temporal \nDifference (TD) objective. This is a natural fit for sequence modeling and we \nuse it to achieve improvements on language modeling tasks over the standard \nTeacher-Forcing methods. \n</p>"}, "author": "Anirudh Goyal, Nan Rosemary Ke, Alex Lamb, R Devon Hjelm, Chris Pal, Joelle Pineau, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126087", "id": "tag:google.com,2005:reader/item/00000003336d4617", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "\"Found in Translation\": Predicting Outcome of Complex Organic Chemistry Reactions using Neural Sequence-to-Sequence Models. (arXiv:1711.04810v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04810"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04810", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>There is an intuitive analogy of an organic chemist's understanding of a \ncompound and a language speaker's understanding of a word. Consequently, it is \npossible to introduce the basic concepts and analyze potential impacts of \nlinguistic analysis to the world of organic chemistry. In this work, we cast \nthe reaction prediction task as a translation problem by introducing a \ntemplate-free sequence-to-sequence model, trained end-to-end and fully \ndata-driven. We propose a novel way of tokenization, which is arbitrarily \nextensible with reaction information. With this approach, we demonstrate \nresults superior to the state-of-the-art solution by a significant margin on \nthe top-1 accuracy. Specifically, our approach achieves an accuracy of 80.1% \nwithout relying on auxiliary knowledge such as reaction templates. Also, 66.4% \naccuracy is reached on a larger and noisier dataset. \n</p>"}, "author": "Philippe Schwaller, Theophile Gaudin, David Lanyi, Costas Bekas, Teodoro Laino", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126086", "id": "tag:google.com,2005:reader/item/00000003336d462b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sparse quadratic classification rules via linear dimension reduction. (arXiv:1711.04817v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04817"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04817", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of high-dimensional classification between the two \ngroups with unequal covariance matrices. Rather than estimating the full \nquadratic discriminant rule, we perform simultaneous variable selection and \nlinear dimension reduction on original data, with the subsequent application of \nquadratic discriminant analysis on the reduced space. The projection vectors \ncan be efficiently estimated by solving the convex optimization problem with \nsparsity-inducing penalty. The new rule performs comparably to linear \ndiscriminant analysis when the assumption of equal covariance matrices is \nsatisfied, and improves the misclassification error rates when this assumption \nis violated. In contrast to quadratic discriminant analysis, the proposed \nframework doesn't require estimation of precision matrices and scales linearly \nwith the number of measurements, making it especially attractive for the use on \nhigh-dimensional datasets. We support the methodology with theoretical \nguarantees on variable selection consistency, and empirical comparison with \ncompeting approaches. \n</p>"}, "author": "Irina Gaynanova, Tianying Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126085", "id": "tag:google.com,2005:reader/item/00000003336d463c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improving Factor-Based Quantitative Investing by Forecasting Company Fundamentals. (arXiv:1711.04837v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04837"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04837", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>On a periodic basis, publicly traded companies are required to report \nfundamentals: financial data such as revenue, operating income, debt, among \nothers. These data points provide some insight into the financial health of a \ncompany. Academic research has identified some factors, i.e. computed features \nof the reported data, that are known through retrospective analysis to \noutperform the market average. Two popular factors are the book value \nnormalized by market capitalization (book-to-market) and the operating income \nnormalized by the enterprise value (EBIT/EV). In this paper: we first show \nthrough simulation that if we could (clairvoyantly) select stocks using factors \ncalculated on future fundamentals (via oracle), then our portfolios would far \noutperform a standard factor approach. Motivated by this analysis, we train \ndeep neural networks to forecast future fundamentals based on a trailing \n5-years window. Quantitative analysis demonstrates a significant improvement in \nMSE over a naive strategy. Moreover, in retrospective analysis using an \nindustry-grade stock portfolio simulator (backtester), we show an improvement \nin compounded annual return to 17.1% (MLP) vs 14.4% for a standard factor \nmodel. \n</p>"}, "author": "John Alberg, Zachary C. Lipton", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126084", "id": "tag:google.com,2005:reader/item/00000003336d4674", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Invariances and Data Augmentation for Supervised Music Transcription. (arXiv:1711.04845v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04845"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04845", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper explores a variety of models for frame-based music transcription, \nwith an emphasis on the methods needed to reach state-of-the-art on human \nrecordings. The translation-invariant network discussed in this paper, which \ncombines a traditional filterbank with a convolutional neural network, was the \ntop-performing model in the 2017 MIREX Multiple Fundamental Frequency \nEstimation evaluation. This class of models shares parameters in the \nlog-frequency domain, which exploits the frequency invariance of music to \nreduce the number of model parameters and avoid overfitting to the training \ndata. All models in this paper were trained with supervision by labeled data \nfrom the MusicNet dataset, augmented by random label-preserving pitch-shift \ntransformations. \n</p>"}, "author": "John Thickstun, Zaid Harchaoui, Dean Foster, Sham M. Kakade", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126083", "id": "tag:google.com,2005:reader/item/00000003336d4686", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning and Visualizing Localized Geometric Features Using 3D-CNN: An Application to Manufacturability Analysis of Drilled Holes. (arXiv:1711.04851v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04851"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04851", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>3D Convolutional Neural Networks (3D-CNN) have been used for object \nrecognition based on the voxelized shape of an object. However, interpreting \nthe decision making process of these 3D-CNNs is still an infeasible task. In \nthis paper, we present a unique 3D-CNN based Gradient-weighted Class Activation \nMapping method (3D-GradCAM) for visual explanations of the distinct local \ngeometric features of interest within an object. To enable efficient learning \nof 3D geometries, we augment the voxel data with surface normals of the object \nboundary. We then train a 3D-CNN with this augmented data and identify the \nlocal features critical for decision-making using 3D GradCAM. An application of \nthis feature identification framework is to recognize difficult-to-manufacture \ndrilled hole features in a complex CAD geometry. The framework can be extended \nto identify difficult-to-manufacture features at multiple spatial scales \nleading to a real-time design for manufacturability decision support system. \n</p>"}, "author": "Sambit Ghadai, Aditya Balu, Adarsh Krishnamurthy, Soumik Sarkar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126082", "id": "tag:google.com,2005:reader/item/00000003336d46a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Estimating prediction error for complex samples. (arXiv:1711.04877v1 [stat.ME])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04877"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04877", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Non-uniform random samples are commonly generated in multiple scientific \nfields ranging from economics to medicine. Complex sampling designs afford \nresearch with increased precision for estimating parameters of interest in less \nprevalent sub-populations. With a growing interest in using complex samples to \ngenerate prediction models for numerous outcomes it is necessary to account for \nthe sampling design that gave rise to the data in order to assess the \ngeneralized predictive utility of a proposed prediction rule. Specifically, \nafter learning a prediction rule based on a complex sample, it is of interest \nto estimate the rule's error rate when applied to unobserved members of the \npopulation. Efron proposed a general class of covariance-inflated prediction \nerror estimators that assumed the available training data is representative of \nthe target population for which the prediction rule is to be applied. We extend \nEfron's estimator to the complex sample context by incorporating \nHorvitz-Thompson sampling weights and show that it is consistent for the true \ngeneralization error rate when applied to the underlying superpopulation giving \nrise to the training sample. The resulting Horvitz-Thompson-Efron (HTE) \nestimator is equivalent to dAIC---a recent extension of AIC to survey sampling \ndata---and is more widely applicable. The proposed methodology is assessed via \nempirical simulations and is applied to data predicting renal function that was \nobtained from the National Health and Nutrition Examination Survey (NHANES). \n</p>"}, "author": "Andrew Holbrook, Daniel Gillen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126081", "id": "tag:google.com,2005:reader/item/00000003336d46cc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "STARK: Structured Dictionary Learning Through Rank-one Tensor Recovery. (arXiv:1711.04887v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04887"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04887", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years, a class of dictionaries have been proposed for \nmultidimensional (tensor) data representation that exploit the structure of \ntensor data by imposing a Kronecker structure on the dictionary underlying the \ndata. In this work, a novel algorithm called \"STARK\" is provided to learn \nKronecker structured dictionaries that can represent tensors of any order. By \nestablishing that the Kronecker product of any number of matrices can be \nrearranged to form a rank-1 tensor, we show that Kronecker structure can be \nenforced on the dictionary by solving a rank-1 tensor recovery problem. Because \nrank-1 tensor recovery is a challenging nonconvex problem, we resort to solving \na convex relaxation of this problem. Empirical experiments on synthetic and \nreal data show promising results for our proposed algorithm. \n</p>"}, "author": "Mohsen Ghassemi, Zahra Shakeri, Anand D. Sarwate, Waheed U. Bajwa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126080", "id": "tag:google.com,2005:reader/item/00000003336d4700", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Sobolev GAN. (arXiv:1711.04894v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04894"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04894", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7ab5929\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7ab5929&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a new Integral Probability Metric (IPM) between distributions: the \nSobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions \nfor functions (critic) restricted to a Sobolev ball defined with respect to a \ndominant measure $\\mu$. We show that the Sobolev IPM compares two distributions \nin high dimensions based on weighted conditional Cumulative Distribution \nFunctions (CDF) of each coordinate on a leave one out basis. The Dominant \nmeasure $\\mu$ plays a crucial role as it defines the support on which \nconditional CDFs are compared. Sobolev IPM can be seen as an extension of the \none dimensional Von-Mises Cram\\'er statistics to high dimensional \ndistributions. We show how Sobolev IPM can be used to train Generative \nAdversarial Networks (GANs). We then exploit the intrinsic conditioning implied \nby Sobolev IPM in text generation. Finally we show that a variant of Sobolev \nGAN achieves competitive results in semi-supervised learning on CIFAR-10, \nthanks to the smoothness enforced on the critic by Sobolev GAN which relates to \nLaplacian regularization. \n</p>"}, "author": "Youssef Mroueh, Chun-Liang Li, Tom Sercu, Anant Raj, Yu Cheng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126079", "id": "tag:google.com,2005:reader/item/00000003336d4741", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Statistically Optimal and Computationally Efficient Low Rank Tensor Completion from Noisy Entries. (arXiv:1711.04934v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04934"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04934", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this article, we develop methods for estimating a low rank tensor from \nnoisy observations on a subset of its entries to achieve both statistical and \ncomputational efficiencies. There have been a lot of recent interests in this \nproblem of noisy tensor completion. Much of the attention has been focused on \nthe fundamental computational challenges often associated with problems \ninvolving higher order tensors, yet very little is known about their \nstatistical performance. To fill in this void, in this article, we characterize \nthe fundamental statistical limits of noisy tensor completion by establishing \nminimax optimal rates of convergence for estimating a $k$th order low rank \ntensor under the general $\\ell_p$ ($1\\le p\\le 2$) norm which suggest \nsignificant room for improvement over the existing approaches. Furthermore, we \npropose a polynomial-time computable estimating procedure based upon power \niteration and a second-order spectral initialization that achieves the optimal \nrates of convergence. Our method is fairly easy to implement and numerical \nexperiments are presented to further demonstrate the practical merits of our \nestimator. \n</p>"}, "author": "Dong Xia, Ming Yuan, Cun-Hui Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126078", "id": "tag:google.com,2005:reader/item/00000003336d478c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sparse High-Dimensional Linear Regression. Algorithmic Barriers and a Local Search Algorithm. (arXiv:1711.04952v1 [math.ST])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04952"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04952", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider a sparse high dimensional regression model where the goal is to \nrecover a k-sparse unknown vector \\beta^* from n noisy linear observations of \nthe form Y=X\\beta^*+W \\in R^n where X \\in R^{n \\times p} has iid N(0,1) entries \nand W \\in R^n has iid N(0,\\sigma^2) entries. Under certain assumptions on the \nparameters, an intriguing assymptotic gap appears between the minimum value of \nn, call it n^*, for which the recovery is information theoretically possible, \nand the minimum value of n, call it n_{alg}, for which an efficient algorithm \nis known to provably recover \\beta^*. In a recent paper it was conjectured that \nthe gap is not artificial, in the sense that for sample sizes n \\in \n[n^*,n_{alg}] the problem is algorithmically hard. \n</p> \n<p>We support this conjecture in two ways. Firstly, we show that a well known \nrecovery mechanism called Basis Pursuit Denoising Scheme provably fails to \n\\ell_2-stably recover the vector when n \\in [n^*,c n_{alg}], for some \nsufficiently small constant c&gt;0. Secondly, we establish that n_{alg}, up to a \nmultiplicative constant factor, is a phase transition point for the appearance \nof a certain Overlap Gap Property (OGP) over the space of k-sparse vectors. The \npresence of such an Overlap Gap Property phase transition, which originates in \nstatistical physics, is known to provide evidence of an algorithmic hardness. \nFinally we show that if n&gt;C n_{alg} for some large enough constant C&gt;0, a very \nsimple algorithm based on a local search improvement is able to infer correctly \nthe support of the unknown vector \\beta^*, adding it to the list of provably \nsuccessful algorithms for the high dimensional linear regression problem. \n</p>"}, "author": "David Gamarnik, Ilias Zadik", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126077", "id": "tag:google.com,2005:reader/item/00000003336d47a4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Strictly Contractive Peaceman-Rachford Splitting Method. (arXiv:1711.04955v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04955"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04955", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a couple of new Stochastic Strictly Contractive \nPeaceman-Rachford Splitting Method (SCPRSM), called Stochastic SCPRSM (SS-PRSM) \nand Stochastic Conjugate Gradient SCPRSM (SCG-PRSM) for large-scale \noptimization problems. The two types of Stochastic PRSM algorithms respectively \nincorporate stochastic variance reduced gradient (SVRG) and conjugate gradient \nmethod. Stochastic PRSM methods and most stochastic ADMM algorithms can only \nachieve a $O(1/\\sqrt{t})$ convergence rate on general convex problems, while \nour SS-PRSM has a $O(1/t)$ convergence rate in general convexity case which \nmatches the convergence rate of the batch ADMM and SCPRSM algorithms. Besides \nour methods has faster convergence rate and lower memory cost. SCG-PRSM is the \nfirst to improve the performance by incorporating conjugate gradient and using \nthe Armijo line search method. Experiments shows that the proposed algorithms \nare faster than stochastic and batch ADMM algorithms. The numerical experiments \nshow SCG-PRSM achieve the state-of-the-art performance on our benchmark \ndatasets. \n</p>"}, "author": "Sen Na, Mingyuan Ma, Shuming Ma, Guangju Peng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126076", "id": "tag:google.com,2005:reader/item/00000003336d47d3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Straggler Mitigation in Distributed Optimization Through Data Encoding. (arXiv:1711.04969v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04969"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04969", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Slow running or straggler tasks can significantly reduce computation speed in \ndistributed computation. Recently, coding-theory-inspired approaches have been \napplied to mitigate the effect of straggling, through embedding redundancy in \ncertain linear computational steps of the optimization algorithm, thus \ncompleting the computation without waiting for the stragglers. In this paper, \nwe propose an alternate approach where we embed the redundancy directly in the \ndata itself, and allow the computation to proceed completely oblivious to \nencoding. We propose several encoding schemes, and demonstrate that popular \nbatch algorithms, such as gradient descent and L-BFGS, applied in a \ncoding-oblivious manner, deterministically achieve sample path linear \nconvergence to an approximate solution of the original problem, using an \narbitrarily varying subset of the nodes at each iteration. Moreover, this \napproximation can be controlled by the amount of redundancy and the number of \nnodes used in each iteration. We provide experimental results demonstrating the \nadvantage of the approach over uncoded and data replication strategies. \n</p>"}, "author": "Can Karakus, Yifan Sun, Suhas Diggavi, Wotao Yin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126075", "id": "tag:google.com,2005:reader/item/00000003336d47e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Quantum transport senses community structure in networks. (arXiv:1711.04979v1 [quant-ph])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04979"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04979", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Quantum time evolution exhibits rich physics, attributable to the interplay \nbetween the density and phase of a wave function. However, unlike classical \nheat diffusion, the wave nature of quantum mechanics has not yet been \nextensively explored in modern data analysis. We propose that the Laplace \ntransform of quantum transport (QT) can be used to construct a powerful \nensemble of maps from a given complex network to a circle $S^1$, such that \nclosely-related nodes on the network are grouped into sharply concentrated \nclusters on $S^1$. The resulting QT clustering (QTC) algorithm is shown to \noutperform the state-of-the-art spectral clustering method on synthetic and \nreal data sets containing complex geometric patterns. The observed phenomenon \nof QTC can be interpreted as a collective behavior of the microscopic nodes \nthat evolve as macroscopic cluster orbitals in an effective tight-binding model \nrecapitulating the network. \n</p>"}, "author": "Chenchao Zhao, Jun S. Song", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126074", "id": "tag:google.com,2005:reader/item/00000003336d47f0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Feature importance scores and lossless feature pruning using Banzhaf power indices. (arXiv:1711.04992v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.04992"}], "alternate": [{"href": "http://arxiv.org/abs/1711.04992", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Understanding the influence of features in machine learning is crucial to \ninterpreting models and selecting the best features for classification. In this \nwork we propose the use of principles from coalitional game theory to reason \nabout importance of features. In particular, we propose the use of the Banzhaf \npower index as a measure of influence of features on the outcome of a \nclassifier. We show that features having Banzhaf power index of zero can be \nlosslessly pruned without damage to classifier accuracy. Computing the power \nindices does not require having access to data samples. However, if samples are \navailable, the indices can be empirically estimated. We compute Banzhaf power \nindices for a neural network classifier on real-life data, and compare the \nresults with gradient-based feature saliency, and coefficients of a logistic \nregression model with $L_1$ regularization. \n</p>"}, "author": "Bogdan Kulynych, Carmela Troncoso", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126073", "id": "tag:google.com,2005:reader/item/00000003336d4809", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Robust Matrix Elastic Net based Canonical Correlation Analysis: An Effective Algorithm for Multi-View Unsupervised Learning. (arXiv:1711.05068v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05068"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05068", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a robust matrix elastic net based canonical correlation \nanalysis (RMEN-CCA) for multiple view unsupervised learning problems, which \nemphasizes the combination of CCA and the robust matrix elastic net (RMEN) used \nas coupled feature selection. The RMEN-CCA leverages the strength of the RMEN \nto distill naturally meaningful features without any prior assumption and to \nmeasure effectively correlations between different 'views'. We can further \nemploy directly the kernel trick to extend the RMEN-CCA to the kernel scenario \nwith theoretical guarantees, which takes advantage of the kernel trick for \nhighly complicated nonlinear feature learning. Rather than simply incorporating \nexisting regularization minimization terms into CCA, this paper provides a new \nlearning paradigm for CCA and is the first to derive a coupled feature \nselection based CCA algorithm that guarantees convergence. More significantly, \nfor CCA, the newly-derived RMEN-CCA bridges the gap between measurement of \nrelevance and coupled feature selection. Moreover, it is nontrivial to tackle \ndirectly the RMEN-CCA by previous optimization approaches derived from its \nsophisticated model architecture. Therefore, this paper further offers a bridge \nbetween a new optimization problem and an existing efficient iterative \napproach. As a consequence, the RMEN-CCA can overcome the limitation of CCA and \naddress large-scale and streaming data problems. Experimental results on four \npopular competing datasets illustrate that the RMEN-CCA performs more \neffectively and efficiently than do state-of-the-art approaches. \n</p>"}, "author": "Peng-Bo Zhang, Zhi-Xin Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126072", "id": "tag:google.com,2005:reader/item/00000003336d4817", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "TripletGAN: Training Generative Model with Triplet Loss. (arXiv:1711.05084v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05084"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05084", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>As an effective way of metric learning, triplet loss has been widely used in \nmany deep learning tasks, including face recognition and person-ReID, leading \nto many states of the arts. The main innovation of triplet loss is using \nfeature map to replace softmax in the classification task. Inspired by this \nconcept, we propose here a new adversarial modeling method by substituting the \nclassification loss of discriminator with triplet loss. Theoretical proof based \non IPM (Integral probability metric) demonstrates that such setting will help \nthe generator converge to the given distribution theoretically under some \nconditions. Moreover, since triplet loss requires the generator to maximize \ndistance within a class, we justify tripletGAN is also helpful to prevent mode \ncollapse through both theory and experiment. \n</p>"}, "author": "Gongze Cao, Yezhou Yang, Jie Lei, Cheng Jin, Yang Liu, Mingli Song", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126071", "id": "tag:google.com,2005:reader/item/00000003336d4825", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficiency Analysis of ASP Encodings for Sequential Pattern Mining Tasks. (arXiv:1711.05090v1 [cs.AI])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05090"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05090", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This article presents the use of Answer Set Programming (ASP) to mine \nsequential patterns. ASP is a high-level declarative logic programming paradigm \nfor high level encoding combinatorial and optimization problem solving as well \nas knowledge representation and reasoning. Thus, ASP is a good candidate for \nimplementing pattern mining with background knowledge, which has been a data \nmining issue for a long time. We propose encodings of the classical sequential \npattern mining tasks within two representations of embeddings (fill-gaps vs \nskip-gaps) and for various kinds of patterns: frequent, constrained and \ncondensed. We compare the computational performance of these encodings with \neach other to get a good insight into the efficiency of ASP encodings. The \nresults show that the fill-gaps strategy is better on real problems due to \nlower memory consumption. Finally, compared to a constraint programming \napproach (CPSM), another declarative programming paradigm, our proposal showed \ncomparable performance. \n</p>"}, "author": "Thomas Guyet (LACODAM), Yves Moinard (LACODAM), Ren&#xe9; Quiniou (LACODAM), Torsten Schaub (LACODAM)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126070", "id": "tag:google.com,2005:reader/item/00000003336d4860", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Overcoming data scarcity with transfer learning. (arXiv:1711.05099v1 [cs.LG])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05099"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05099", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7ab5c44\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7ab5c44&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Despite increasing focus on data publication and discovery in materials \nscience and related fields, the global view of materials data is highly sparse. \nThis sparsity encourages training models on the union of multiple datasets, but \nsimple unions can prove problematic as (ostensibly) equivalent properties may \nbe measured or computed differently depending on the data source. These hidden \ncontextual differences introduce irreducible errors into analyses, \nfundamentally limiting their accuracy. Transfer learning, where information \nfrom one dataset is used to inform a model on another, can be an effective tool \nfor bridging sparse data while preserving the contextual differences in the \nunderlying measurements. Here, we describe and compare three techniques for \ntransfer learning: multi-task, difference, and explicit latent variable \narchitectures. We show that difference architectures are most accurate in the \nmulti-fidelity case of mixed DFT and experimental band gaps, while multi-task \nmost improves classification performance of color with band gaps. For \nactivation energies of steps in NO reduction, the explicit latent variable \nmethod is not only the most accurate, but also enjoys cancellation of errors in \nfunctions that depend on multiple tasks. These results motivate the publication \nof high quality materials datasets that encode transferable information, \nindependent of industrial or academic interest in the particular labels, and \nencourage further development and application of transfer learning methods to \nmaterials informatics problems. \n</p>"}, "author": "Maxwell L. Hutchinson, Erin Antono, Brenna M. Gibbons, Sean Paradiso, Julia Ling, Bryce Meredig", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126069", "id": "tag:google.com,2005:reader/item/00000003336d4882", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "The Multi-layer Information Bottleneck Problem. (arXiv:1711.05102v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05102"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05102", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7b95482\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7b95482&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The muti-layer information bottleneck (IB) problem, where information is \npropagated (or successively refined) from layer to layer, is considered. Based \non information forwarded by the preceding layer, each stage of the network is \nrequired to preserve a certain level of relevance with regards to a specific \nhidden variable, quantified by the mutual information. The hidden variables and \nthe source can be arbitrarily correlated. The optimal trade-off between rates \nof relevance and compression (or complexity) is obtained through a \nsingle-letter characterization, referred to as the rate-relevance region. \nConditions of successive refinabilty are given. Binary source with BSC hidden \nvariables and binary source with BSC/BEC mixed hidden variables are both proved \nto be successively refinable. We further extend our result to Guassian models. \nA counterexample of successive refinability is also provided. \n</p>"}, "author": "Qianqian Yang, Pablo Piantanida, Deniz G&#xfc;nd&#xfc;z", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126068", "id": "tag:google.com,2005:reader/item/00000003336d48a3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Rewiring: Training very sparse deep networks. (arXiv:1711.05136v1 [cs.NE])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05136"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05136", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neuromorphic hardware tends to pose limits on the connectivity of deep \nnetworks that one can run on them. But also generic hardware and software \nimplementations of deep learning run more efficiently on sparse networks. \nSeveral methods exist for pruning connections of a neural network after it was \ntrained without connectivity constraints. We present an algorithm, DEEP R, that \nenables us to train directly a sparsely connected neural network. DEEP R \nautomatically rewires the network during supervised training so that \nconnections are there where they are most needed for the task, while its total \nnumber is all the time strictly bounded. We demonstrate that DEEP R can be used \nto train very sparse feedforward and recurrent neural networks on standard \nbenchmark tasks with just a minor loss in performance. DEEP R is based on a \nrigorous theoretical foundation that views rewiring as stochastic sampling of \nnetwork configurations from a posterior. \n</p>"}, "author": "Guillaume Bellec, David Kappel, Wolfgang Maass, Robert Legenstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126067", "id": "tag:google.com,2005:reader/item/00000003336d48b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast and reliable inference algorithm for hierarchical stochastic block models. (arXiv:1711.05150v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05150"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05150", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Network clustering reveals the organization of a network or corresponding \ncomplex system with elements represented as vertices and interactions as edges \nin a (directed, weighted) graph. Although the notion of clustering can be \nsomewhat loose, network clusters or groups are generally considered as nodes \nwith enriched interactions and edges sharing common patterns. Statistical \ninference often treats groups as latent variables, with observed networks \ngenerated from latent group structure, termed a stochastic block model. \nRegardless of the definitions, statistical inference can be either translated \nto modularity maximization, which is provably an NP-complete problem. \n</p> \n<p>Here we present scalable and reliable algorithms that recover hierarchical \nstochastic block models fast and accurately. Our algorithm scales almost \nlinearly in number of edges, and inferred models were more accurate that other \nscalable methods. \n</p>"}, "author": "Yongjin Park, Joel S. Bader", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126066", "id": "tag:google.com,2005:reader/item/00000003336d48db", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Near-Optimal Discrete Optimization for Experimental Design: A Regret Minimization Approach. (arXiv:1711.05174v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05174"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05174", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The experimental design problem concerns the selection of k points from a \npotentially large design pool of p-dimensional vectors, so as to maximize the \nstatistical efficiency regressed on the selected k design points. Statistical \nefficiency is measured by optimality criteria, including A(verage), \nD(eterminant), T(race), E(igen), V(ariance) and G-optimality. Except for the \nT-optimality, exact optimization is NP-hard. \n</p> \n<p>We propose a polynomial-time regret minimization framework to achieve a \n$(1+\\varepsilon)$ approximation with only $O(p/\\varepsilon^2)$ design points, \nfor all the optimality criteria above. \n</p> \n<p>In contrast, to the best of our knowledge, before our work, no \npolynomial-time algorithm achieves $(1+\\varepsilon)$ approximations for \nD/E/G-optimality, and the best poly-time algorithm achieving \n$(1+\\varepsilon)$-approximation for A/V-optimality requires $k = \n\\Omega(p^2/\\varepsilon)$ design points. \n</p>"}, "author": "Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, Yining Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126065", "id": "tag:google.com,2005:reader/item/00000003336d48fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Joint Gaussian Processes for Biophysical Parameter Retrieval. (arXiv:1711.05197v1 [stat.ML])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05197"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05197", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Solving inverse problems is central to geosciences and remote sensing. \nRadiative transfer models (RTMs) represent mathematically the physical laws \nwhich govern the phenomena in remote sensing applications (forward models). The \nnumerical inversion of the RTM equations is a challenging and computationally \ndemanding problem, and for this reason, often the application of a nonlinear \nstatistical regression is preferred. In general, regression models predict the \nbiophysical parameter of interest from the corresponding received radiance. \nHowever, this approach does not employ the physical information encoded in the \nRTMs. An alternative strategy, which attempts to include the physical \nknowledge, consists in learning a regression model trained using data simulated \nby an RTM code. In this work, we introduce a nonlinear nonparametric regression \nmodel which combines the benefits of the two aforementioned approaches. The \ninversion is performed taking into account jointly both real observations and \nRTM-simulated data. The proposed Joint Gaussian Process (JGP) provides a solid \nframework for exploiting the regularities between the two types of data. The \nJGP automatically detects the relative quality of the simulated and real data, \nand combines them accordingly. This occurs by learning an additional \nhyper-parameter w.r.t. a standard GP model, and fitting parameters through \nmaximizing the pseudo-likelihood of the real observations. The resulting scheme \nis both simple and robust, i.e., capable of adapting to different scenarios. \nThe advantages of the JGP method compared to benchmark strategies are shown \nconsidering RTM-simulated and real observations in different experiments. \nSpecifically, we consider leaf area index (LAI) retrieval from Landsat data \ncombined with simulated data generated by the PROSAIL model. \n</p>"}, "author": "Daniel Heestermans Svendsen, Luca Martino, Manuel Campos-Taberner, Francisco Javier Garc&#xed;a-Haro, Gustau Camps-Valls", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126064", "id": "tag:google.com,2005:reader/item/00000003336d4924", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning. (arXiv:1711.05225v1 [cs.CV])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05225"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05225", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop an algorithm that can detect pneumonia from chest X-rays at a \nlevel exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer \nconvolutional neural network trained on ChestX-ray14, currently the largest \npublicly available chest X-ray dataset, containing over 100,000 frontal-view \nX-ray images with 14 diseases. Four practicing academic radiologists annotate a \ntest set, on which we compare the performance of CheXNet to that of \nradiologists. We find that CheXNet exceeds average radiologist performance on \npneumonia detection on both sensitivity and specificity. We extend CheXNet to \ndetect all 14 diseases in ChestX-ray14 and achieve state of the art results on \nall 14 diseases. \n</p>"}, "author": "Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, Matthew P. Lungren, Andrew Y. Ng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510769311126", "timestampUsec": "1510769311126063", "id": "tag:google.com,2005:reader/item/00000003336d4932", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A visual search engine for Bangladeshi laws. (arXiv:1711.05233v1 [cs.HC])", "published": 1510769311, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.05233"}], "alternate": [{"href": "http://arxiv.org/abs/1711.05233", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Browsing and finding relevant information for Bangladeshi laws is a challenge \nfaced by all law students and researchers in Bangladesh, and by citizens who \nwant to learn about any legal procedure. Some law archives in Bangladesh are \ndigitized, but lack proper tools to organize the data meaningfully. We present \na text visualization tool that utilizes machine learning techniques to make the \nsearching of laws quicker and easier. Using Doc2Vec to layout law article \nnodes, link mining techniques to visualize relevant citation networks, and \nnamed entity recognition to quickly find relevant sections in long law \narticles, our tool provides a faster and better search experience to the users. \nQualitative feedback from law researchers, students, and government officials \nshow promise for visually intuitive search tools in the context of \ngovernmental, legal, and constitutional data in developing countries, where \ndigitized data does not necessarily pave the way towards an easy access to \ninformation. \n</p>"}, "author": "Manash Kumar Mandal, Pinku Deb Nath, Arpeeta Shams Mizan, Nazmus Saquib", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693757", "id": "tag:google.com,2005:reader/item/00000003320b77cc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Picasso, Matisse, or a Fake? Automated Analysis of Drawings at the Stroke Level for Attribution and Authentication. (arXiv:1711.03536v1 [eess.IV])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03536"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03536", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a computational approach for analysis of strokes in line \ndrawings by artists. We aim at developing an AI methodology that facilitates \nattribution of drawings of unknown authors in a way that is not easy to be \ndeceived by forged art. The methodology used is based on quantifying the \ncharacteristics of individual strokes in drawings. We propose a novel algorithm \nfor segmenting individual strokes. We designed and compared different \nhand-crafted and learned features for the task of quantifying stroke \ncharacteristics. We also propose and compare different classification methods \nat the drawing level. We experimented with a dataset of 300 digitized drawings \nwith over 80 thousands strokes. The collection mainly consisted of drawings of \nPablo Picasso, Henry Matisse, and Egon Schiele, besides a small number of \nrepresentative works of other artists. The experiments shows that the proposed \nmethodology can classify individual strokes with accuracy 70%-90%, and \naggregate over drawings with accuracy above 80%, while being robust to be \ndeceived by fakes (with accuracy 100% for detecting fakes in most settings). \n</p>"}, "author": "Ahmed Elgammal, Yan Kang, Milko Den Leeuw", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693756", "id": "tag:google.com,2005:reader/item/00000003320b77f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Discovery of potential collaboration networks from open knowledge sources. (arXiv:1711.03537v1 [cs.DL])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03537"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03537", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Scientific publishing conveys the outputs of an academic or research \nactivity, in this sense; it also reflects the efforts and issues in which \npeople engage. To identify potential collaborative networks one of the simplest \napproaches is to leverage the co-authorship relations. In this approach, \nsemantic and hierarchic relationships defined by a Knowledge Organization \nSystem are used in order to improve the system's ability to recommend potential \nnetworks beyond the lexical or syntactic analysis of the topics or concepts \nthat are of interest to academics. \n</p>"}, "author": "Nelson Piedra, Janneth Chicaiza, Jorge Lopez-Vargas, Edmundo Tovar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693755", "id": "tag:google.com,2005:reader/item/00000003320b7802", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Change-Detection based Framework for Piecewise-stationary Multi-Armed Bandit Problem. (arXiv:1711.03539v1 [cs.LG])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03539"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03539", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7b9571a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7b9571a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The multi-armed bandit problem has been extensively studied under the \nstationary assumption. However in reality, this assumption often does not hold \nbecause the distributions of rewards themselves may change over time. In this \npaper, we propose a change-detection (CD) based framework for multi-armed \nbandit problems under the piecewise-stationary setting, and study a class of \nchange-detection based UCB (Upper Confidence Bound) policies, CD-UCB, that \nactively detects change points and restarts the UCB indices. We then develop \nCUSUM-UCB and PHT-UCB, that belong to the CD-UCB class and use cumulative sum \n(CUSUM) and Page-Hinkley Test (PHT) to detect changes. We show that CUSUM-UCB \nobtains the best known regret upper bound under mild assumptions. We also \ndemonstrate the regret reduction of the CD-UCB policies over arbitrary \nBernoulli rewards and Yahoo! datasets of webpage click-through rates. \n</p>"}, "author": "Fang Liu, Joohyun Lee, Ness Shroff", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693754", "id": "tag:google.com,2005:reader/item/00000003320b7828", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "DLPaper2Code: Auto-generation of Code from Deep Learning Research Papers. (arXiv:1711.03543v1 [cs.LG])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03543"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03543", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With an abundance of research papers in deep learning, reproducibility or \nadoption of the existing works becomes a challenge. This is due to the lack of \nopen source implementations provided by the authors. Further, re-implementing \nresearch papers in a different library is a daunting task. To address these \nchallenges, we propose a novel extensible approach, DLPaper2Code, to extract \nand understand deep learning design flow diagrams and tables available in a \nresearch paper and convert them to an abstract computational graph. The \nextracted computational graph is then converted into execution ready source \ncode in both Keras and Caffe, in real-time. An arXiv-like website is created \nwhere the automatically generated designs is made publicly available for 5,000 \nresearch papers. The generated designs could be rated and edited using an \nintuitive drag-and-drop UI framework in a crowdsourced manner. To evaluate our \napproach, we create a simulated dataset with over 216,000 valid design \nvisualizations using a manually defined grammar. Experiments on the simulated \ndataset show that the proposed framework provide more than $93\\%$ accuracy in \nflow diagram content extraction. \n</p>"}, "author": "Akshay Sethi, Anush Sankaran, Naveen Panwar, Shreya Khare, Senthil Mani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693753", "id": "tag:google.com,2005:reader/item/00000003320b7842", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "First Results from Using Game Refinement Measure and Learning Coefficient in Scrabble. (arXiv:1711.03580v1 [cs.AI])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03580"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03580", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper explores the entertainment experience and learning experience in \nScrabble. It proposes a new measure from the educational point of view, which \nwe call learning coefficient, based on the balance between the learner's skill \nand the challenge in Scrabble. Scrabble variants, generated using different \nsize of board and dictionary, are analyzed with two measures of game refinement \nand learning coefficient. The results show that 13x13 Scrabble yields the best \nentertainment experience and 15x15 (standard) Scrabble with 4% of original \ndictionary size yields the most effective environment for language learners. \nMoreover, 15x15 Scrabble with 10% of original dictionary size has a good \nbalance between entertainment and learning experience. \n</p>"}, "author": "Kananat Suwanviwatana, Hiroyuki Iida", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693752", "id": "tag:google.com,2005:reader/item/00000003320b7861", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning and Real-time Classification of Hand-written Digits With Spiking Neural Networks. (arXiv:1711.03637v1 [stat.ML])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03637"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03637", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe a novel spiking neural network (SNN) for automated, real-time \nhandwritten digit classification and its implementation on a GP-GPU platform. \nInformation processing within the network, from feature extraction to \nclassification is implemented by mimicking the basic aspects of neuronal spike \ninitiation and propagation in the brain. The feature extraction layer of the \nSNN uses fixed synaptic weight maps to extract the key features of the image \nand the classifier layer uses the recently developed NormAD approximate \ngradient descent based supervised learning algorithm for spiking neural \nnetworks to adjust the synaptic weights. On the standard MNIST database images \nof handwritten digits, our network achieves an accuracy of 99.80% on the \ntraining set and 98.06% on the test set, with nearly 7x fewer parameters \ncompared to the state-of-the-art spiking networks. We further use this network \nin a GPU based user-interface system demonstrating real-time SNN simulation to \ninfer digits written by different users. On a test set of 500 such images, this \nreal-time platform achieves an accuracy exceeding 97% while making a prediction \nwithin an SNN emulation time of less than 100ms. \n</p>"}, "author": "Shruti R. Kulkarni, John M. Alexiades, Bipin Rajendran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693751", "id": "tag:google.com,2005:reader/item/00000003320b787a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Deep Learning in Memristive Networks. (arXiv:1711.03640v1 [stat.ML])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03640"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03640", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the performance of stochastically trained deep neural networks \n(DNNs) whose synaptic weights are implemented using emerging memristive devices \nthat exhibit limited dynamic range, resolution, and variability in their \nprogramming characteristics. We show that a key device parameter to optimize \nthe learning efficiency of DNNs is the variability in its programming \ncharacteristics. DNNs with such memristive synapses, even with dynamic range as \nlow as $15$ and only $32$ discrete levels, when trained based on stochastic \nupdates suffer less than $3\\%$ loss in accuracy compared to floating point \nsoftware baseline. We also study the performance of stochastic memristive DNNs \nwhen used as inference engines with noise corrupted data and find that if the \ndevice variability can be minimized, the relative degradation in performance \nfor the Stochastic DNN is better than that of the software baseline. Hence, our \nstudy presents a new optimization corner for memristive devices for building \nlarge noise-immune deep learning systems. \n</p>"}, "author": "Anakha V Babu, Bipin Rajendran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693750", "id": "tag:google.com,2005:reader/item/00000003320b78a6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Communicative Capital for Prosthetic Agents. (arXiv:1711.03676v1 [cs.AI])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03676"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03676", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This work presents an overarching perspective on the role that machine \nintelligence can play in enhancing human abilities, especially those that have \nbeen diminished due to injury or illness. As a primary contribution, we develop \nthe hypothesis that assistive devices, and specifically artificial arms and \nhands, can and should be viewed as agents in order for us to most effectively \nimprove their collaboration with their human users. We believe that increased \nagency will enable more powerful interactions between human users and next \ngeneration prosthetic devices, especially when the sensorimotor space of the \nprosthetic technology greatly exceeds the conventional control and \ncommunication channels available to a prosthetic user. To more concretely \nexamine an agency-based view on prosthetic devices, we propose a new schema for \ninterpreting the capacity of a human-machine collaboration as a function of \nboth the human's and machine's degrees of agency. We then introduce the idea of \ncommunicative capital as a way of thinking about the communication resources \ndeveloped by a human and a machine during their ongoing interaction. Using this \nschema of agency and capacity, we examine the benefits and disadvantages of \nincreasing the agency of a prosthetic limb. To do so, we present an analysis of \nexamples from the literature where building communicative capital has enabled a \nprogression of fruitful, task-directed interactions between prostheses and \ntheir human users. We then describe further work that is needed to concretely \nevaluate the hypothesis that prostheses are best thought of as agents. The \nagent-based viewpoint developed in this article significantly extends current \nthinking on how best to support the natural, functional use of increasingly \ncomplex prosthetic enhancements, and opens the door for more powerful \ninteractions between humans and their assistive technologies. \n</p>"}, "author": "Patrick M. Pilarski, Richard S. Sutton, Kory W. Mathewson, Craig Sherstan, Adam S. R. Parker, Ann L. Edwards", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693749", "id": "tag:google.com,2005:reader/item/00000003320b78bc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Self-Supervised Intrinsic Image Decomposition. (arXiv:1711.03678v1 [cs.CV])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03678"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03678", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Intrinsic decomposition from a single image is a highly challenging task, due \nto its inherent ambiguity and the scarcity of training data. In contrast to \ntraditional fully supervised learning approaches, in this paper we propose \nlearning intrinsic image decomposition by explaining the input image. Our \nmodel, the Rendered Intrinsics Network (RIN), joins together an image \ndecomposition pipeline, which predicts reflectance, shape, and lighting \nconditions given a single image, with a recombination function, a learned \nshading model used to recompose the original input based off of intrinsic image \npredictions. Our network can then use unsupervised reconstruction error as an \nadditional signal to improve its intermediate representations. This allows \nlarge-scale unlabeled data to be useful during training, and also enables \ntransferring learned knowledge to images of unseen object categories, lighting \nconditions, and shapes. Extensive experiments demonstrate that our method \nperforms well on both intrinsic image decomposition and knowledge transfer. \n</p>"}, "author": "Michael Janner, Jiajun Wu, Tejas D. Kulkarni, Ilker Yildirim, Joshua B. Tenenbaum", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693748", "id": "tag:google.com,2005:reader/item/00000003320b78d4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Saliency Prediction for Mobile User Interfaces. (arXiv:1711.03726v2 [cs.CV] UPDATED)", "published": 1511175748, "updated": 1511175750, "canonical": [{"href": "http://arxiv.org/abs/1711.03726"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03726", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce models for saliency prediction for mobile user interfaces. A \nmobile interface may include elements like buttons, text, etc. in addition to \nnatural images which enable performing a variety of tasks. Saliency in natural \nimages is a well studied area. However, given the difference in what \nconstitutes a mobile interface, and the usage context of these devices, we \npostulate that saliency prediction for mobile interface images requires a fresh \napproach. Mobile interface design involves operating on elements, the building \nblocks of the interface. We first collected eye-gaze data from mobile devices \nfor free viewing task. Using this data, we develop a novel autoencoder based \nmulti-scale deep learning model that provides saliency prediction at the mobile \ninterface element level. Compared to saliency prediction approaches developed \nfor natural images, we show that our approach performs significantly better on \na range of established metrics. \n</p>"}, "author": "Prakhar Gupta, Shubh Gupta, Ajaykrishnan Jayagopal, Sourav Pal, Ritwik Sinha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693747", "id": "tag:google.com,2005:reader/item/00000003320b78e7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Lattice embeddings between types of fuzzy sets. Closed-valued fuzzy sets. (arXiv:1711.03752v1 [cs.AI])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03752"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03752", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we deal with the problem of extending Zadeh's operators on \nfuzzy sets (FSs) to interval-valued (IVFSs), set-valued (SVFSs) and type-2 \n(T2FSs) fuzzy sets. Namely, it is known that seeing FSs as SVFSs, or T2FSs, \nwhose membership degrees are singletons is not order-preserving. We then \ndescribe a family of lattice embeddings from FSs to SVFSs. Alternatively, if \nthe former singleton viewpoint is required, we reformulate the intersection on \nhesitant fuzzy sets and introduce what we have called closed-valued fuzzy sets. \nThis new type of fuzzy sets extends standard union and intersection on FSs. In \naddition, it allows handling together membership degrees of different nature \nas, for instance, closed intervals and finite sets. Finally, all these \nconstructions are viewed as T2FSs forming a chain of lattices. \n</p>"}, "author": "F. J. Lobillo, Luis Merino, Gabriel Navarro, Evangelina Santos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693746", "id": "tag:google.com,2005:reader/item/00000003320b7905", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning with Options that Terminate Off-Policy. (arXiv:1711.03817v1 [cs.AI])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03817"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03817", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A temporally abstract action, or an option, is specified by a policy and a \ntermination condition: the policy guides option behavior, and the termination \ncondition roughly determines its length. Generally, learning with longer \noptions (like learning with multi-step returns) is known to be more efficient. \nHowever, if the option set for the task is not ideal, and cannot express the \nprimitive optimal policy exactly, shorter options offer more flexibility and \ncan yield a better solution. Thus, the termination condition puts learning \nefficiency at odds with solution quality. We propose to resolve this dilemma by \ndecoupling the behavior and target terminations, just like it is done with \npolicies in off-policy learning. To this end, we give a new algorithm, \nQ(\\beta), that learns the solution with respect to any termination condition, \nregardless of how the options actually terminate. We derive Q(\\beta) by casting \nlearning with options into a common framework with well-studied multi-step \noff-policy learning. We validate our algorithm empirically, and show that it \nholds up to its motivating claims. \n</p>"}, "author": "Anna Harutyunyan, Peter Vrancx, Pierre-Luc Bacon, Doina Precup, Ann Nowe", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693745", "id": "tag:google.com,2005:reader/item/00000003320b7927", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "\"Dave...I can assure you...that it's going to be all right...\" -- A definition, case for, and survey of algorithmic assurances in human-autonomy trust relationships. (arXiv:1711.03846v1 [cs.CY])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03846"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03846", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7b95ac0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7b95ac0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>As technology becomes more advanced, those who design, use and are otherwise \naffected by it want to know that it will perform correctly, and understand why \nit does what it does, and how to use it appropriately. In essence they want to \nbe able to trust the systems that are being designed. In this survey we present \nassurances that are the method by which users can understand how to trust \nautonomous systems. Trust between humans and autonomy is reviewed, and the \nimplications for the design of assurances are highlighted. A survey of existing \nresearch related to assurances is presented. Much of the surveyed research \noriginates from fields such as interpretable, comprehensible, transparent, and \nexplainable machine learning, as well as human-computer interaction, \nhuman-robot interaction, and e-commerce. Several key ideas are extracted from \nthis work in order to refine the definition of assurances. The design of \nassurances is found to be highly dependent not only on the capabilities of the \nautonomous system, but on the characteristics of the human user, and the \nappropriate trust-related behaviors. Several directions for future research are \nidentified and discussed. \n</p>"}, "author": "Brett W Israelsen, Nisar R Ahmed", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693744", "id": "tag:google.com,2005:reader/item/00000003320b7934", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Arrhythmia Classification from the Abductive Interpretation of Short Single-Lead ECG Records. (arXiv:1711.03892v1 [cs.AI])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03892"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03892", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7c40e67\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7c40e67&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this work we propose a new method for the rhythm classification of short \nsingle-lead ECG records, using a set of high-level and clinically meaningful \nfeatures provided by the abductive interpretation of the records. These \nfeatures include morphological and rhythm-related features that are used to \nbuild two classifiers: one that evaluates the record globally, using aggregated \nvalues for each feature; and another one that evaluates the record as a \nsequence, using a Recurrent Neural Network fed with the individual features for \neach detected heartbeat. The two classifiers are finally combined using the \nstacking technique, providing an answer by means of four target classes: Normal \nsinus rhythm, Atrial fibrillation, Other anomaly, and Noisy. The approach has \nbeen validated against the 2017 Physionet/CinC Challenge dataset, obtaining a \nfinal score of 0.83 and ranking first in the competition. \n</p>"}, "author": "Tom&#xe1;s Teijeiro, Constantino A. Garc&#xed;a, Daniel Castro, Paulo F&#xe9;lix", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693743", "id": "tag:google.com,2005:reader/item/00000003320b7960", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neural-Symbolic Learning and Reasoning: A Survey and Interpretation. (arXiv:1711.03902v1 [cs.AI])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03902"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03902", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The study and understanding of human behaviour is relevant to computer \nscience, artificial intelligence, neural computation, cognitive science, \nphilosophy, psychology, and several other areas. Presupposing cognition as \nbasis of behaviour, among the most prominent tools in the modelling of \nbehaviour are computational-logic systems, connectionist models of cognition, \nand models of uncertainty. Recent studies in cognitive science, artificial \nintelligence, and psychology have produced a number of cognitive models of \nreasoning, learning, and language that are underpinned by computation. In \naddition, efforts in computer science research have led to the development of \ncognitive computational systems integrating machine learning and automated \nreasoning. Such systems have shown promise in a range of applications, \nincluding computational biology, fault diagnosis, training and assessment in \nsimulators, and software verification. This joint survey reviews the personal \nideas and views of several researchers on neural-symbolic learning and \nreasoning. The article is organised in three parts: Firstly, we frame the scope \nand goals of neural-symbolic computation and have a look at the theoretical \nfoundations. We then proceed to describe the realisations of neural-symbolic \ncomputation, systems, and applications. Finally we present the challenges \nfacing the area and avenues for further research. \n</p>"}, "author": "Tarek R. Besold, Artur d&#x27;Avila Garcez, Sebastian Bader, Howard Bowman, Pedro Domingos, Pascal Hitzler, Kai-Uwe Kuehnberger, Luis C. Lamb, Daniel Lowd, Priscila Machado Vieira Lima, Leo de Penning, Gadi Pinkas, Hoifung Poon, Gerson Zaverucha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693742", "id": "tag:google.com,2005:reader/item/00000003320b797c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "CARLA: An Open Urban Driving Simulator. (arXiv:1711.03938v1 [cs.LG])", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03938"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03938", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce CARLA, an open-source simulator for autonomous driving research. \nCARLA has been developed from the ground up to support development, training, \nand validation of autonomous urban driving systems. In addition to open-source \ncode and protocols, CARLA provides open digital assets (urban layouts, \nbuildings, vehicles) that were created for this purpose and can be used freely. \nThe simulation platform supports flexible specification of sensor suites and \nenvironmental conditions. We use CARLA to study the performance of three \napproaches to autonomous driving: a classic modular pipeline, an end-to-end \nmodel trained via imitation learning, and an end-to-end model trained via \nreinforcement learning. The approaches are evaluated in controlled scenarios of \nincreasing difficulty, and their performance is examined via metrics provided \nby CARLA, illustrating the platform's utility for autonomous driving research. \nThe supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E \n</p>"}, "author": "Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510606883694", "timestampUsec": "1510606883693731", "id": "tag:google.com,2005:reader/item/00000003320b79d3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Fault Analysis and Subset Selection in Solar Power Grids. (arXiv:1711.02810v1 [cs.LG] CROSS LISTED)", "published": 1510606884, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02810"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02810", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Non-availability of reliable and sustainable electric power is a major \nproblem in the developing world. Renewable energy sources like solar are not \nvery lucrative in the current stage due to various uncertainties like weather, \nstorage, land use among others. There also exists various other issues like \nmis-commitment of power, absence of intelligent fault analysis, congestion, \netc. In this paper, we propose a novel deep learning-based system for \npredicting faults and selecting power generators optimally so as to reduce \ncosts and ensure higher reliability in solar power systems. The results are \nhighly encouraging and they suggest that the approaches proposed in this paper \nhave the potential to be applied successfully in the developing world. \n</p>"}, "author": "Biswarup Bhattacharya, Abhishek Sinha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510605917418", "timestampUsec": "1510605917417911", "id": "tag:google.com,2005:reader/item/00000003320980eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "What Really is Deep Learning Doing?. (arXiv:1711.03577v1 [cs.LG])", "published": 1510605918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03577"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03577", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning has achieved a great success in many areas, from computer \nvision to natural language processing, to game playing, and much more. Yet, \nwhat deep learning is really doing is still an open question. There are a lot \nof works in this direction. For example, [5] tried to explain deep learning by \ngroup renormalization, and [6] tried to explain deep learning from the view of \nfunctional approximation. In order to address this very crucial question, here \nwe see deep learning from perspective of mechanical learning and learning \nmachine (see [1], [2]). From this particular angle, we can see deep learning \nmuch better and answer with confidence: What deep learning is really doing? why \nit works well, how it works, and how much data is necessary for learning. We \nalso will discuss advantages and disadvantages of deep learning at the end of \nthis work. \n</p>"}, "author": "Chuyu Xiong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510605917418", "timestampUsec": "1510605917417910", "id": "tag:google.com,2005:reader/item/00000003320980f4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning and Real-time Classification of Hand-written Digits With Spiking Neural Networks. (arXiv:1711.03637v1 [stat.ML])", "published": 1510605918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03637"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03637", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe a novel spiking neural network (SNN) for automated, real-time \nhandwritten digit classification and its implementation on a GP-GPU platform. \nInformation processing within the network, from feature extraction to \nclassification is implemented by mimicking the basic aspects of neuronal spike \ninitiation and propagation in the brain. The feature extraction layer of the \nSNN uses fixed synaptic weight maps to extract the key features of the image \nand the classifier layer uses the recently developed NormAD approximate \ngradient descent based supervised learning algorithm for spiking neural \nnetworks to adjust the synaptic weights. On the standard MNIST database images \nof handwritten digits, our network achieves an accuracy of 99.80% on the \ntraining set and 98.06% on the test set, with nearly 7x fewer parameters \ncompared to the state-of-the-art spiking networks. We further use this network \nin a GPU based user-interface system demonstrating real-time SNN simulation to \ninfer digits written by different users. On a test set of 500 such images, this \nreal-time platform achieves an accuracy exceeding 97% while making a prediction \nwithin an SNN emulation time of less than 100ms. \n</p>"}, "author": "Shruti R. Kulkarni, John M. Alexiades, Bipin Rajendran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510605917418", "timestampUsec": "1510605917417909", "id": "tag:google.com,2005:reader/item/00000003320980fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Deep Learning in Memristive Networks. (arXiv:1711.03640v1 [stat.ML])", "published": 1510605918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03640"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03640", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the performance of stochastically trained deep neural networks \n(DNNs) whose synaptic weights are implemented using emerging memristive devices \nthat exhibit limited dynamic range, resolution, and variability in their \nprogramming characteristics. We show that a key device parameter to optimize \nthe learning efficiency of DNNs is the variability in its programming \ncharacteristics. DNNs with such memristive synapses, even with dynamic range as \nlow as $15$ and only $32$ discrete levels, when trained based on stochastic \nupdates suffer less than $3\\%$ loss in accuracy compared to floating point \nsoftware baseline. We also study the performance of stochastic memristive DNNs \nwhen used as inference engines with noise corrupted data and find that if the \ndevice variability can be minimized, the relative degradation in performance \nfor the Stochastic DNN is better than that of the software baseline. Hence, our \nstudy presents a new optimization corner for memristive devices for building \nlarge noise-immune deep learning systems. \n</p>"}, "author": "Anakha V Babu, Bipin Rajendran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443760", "id": "tag:google.com,2005:reader/item/0000000331e17cac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Change-Detection based Framework for Piecewise-stationary Multi-Armed Bandit Problem. (arXiv:1711.03539v1 [cs.LG])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03539"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03539", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The multi-armed bandit problem has been extensively studied under the \nstationary assumption. However in reality, this assumption often does not hold \nbecause the distributions of rewards themselves may change over time. In this \npaper, we propose a change-detection (CD) based framework for multi-armed \nbandit problems under the piecewise-stationary setting, and study a class of \nchange-detection based UCB (Upper Confidence Bound) policies, CD-UCB, that \nactively detects change points and restarts the UCB indices. We then develop \nCUSUM-UCB and PHT-UCB, that belong to the CD-UCB class and use cumulative sum \n(CUSUM) and Page-Hinkley Test (PHT) to detect changes. We show that CUSUM-UCB \nobtains the best known regret upper bound under mild assumptions. We also \ndemonstrate the regret reduction of the CD-UCB policies over arbitrary \nBernoulli rewards and Yahoo! datasets of webpage click-through rates. \n</p>"}, "author": "Fang Liu, Joohyun Lee, Ness Shroff", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443759", "id": "tag:google.com,2005:reader/item/0000000331e17d07", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "DLPaper2Code: Auto-generation of Code from Deep Learning Research Papers. (arXiv:1711.03543v1 [cs.LG])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03543"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03543", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With an abundance of research papers in deep learning, reproducibility or \nadoption of the existing works becomes a challenge. This is due to the lack of \nopen source implementations provided by the authors. Further, re-implementing \nresearch papers in a different library is a daunting task. To address these \nchallenges, we propose a novel extensible approach, DLPaper2Code, to extract \nand understand deep learning design flow diagrams and tables available in a \nresearch paper and convert them to an abstract computational graph. The \nextracted computational graph is then converted into execution ready source \ncode in both Keras and Caffe, in real-time. An arXiv-like website is created \nwhere the automatically generated designs is made publicly available for 5,000 \nresearch papers. The generated designs could be rated and edited using an \nintuitive drag-and-drop UI framework in a crowdsourced manner. To evaluate our \napproach, we create a simulated dataset with over 216,000 valid design \nvisualizations using a manually defined grammar. Experiments on the simulated \ndataset show that the proposed framework provide more than $93\\%$ accuracy in \nflow diagram content extraction. \n</p>"}, "author": "Akshay Sethi, Anush Sankaran, Naveen Panwar, Shreya Khare, Senthil Mani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443758", "id": "tag:google.com,2005:reader/item/0000000331e17d69", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SHOPPER: A Probabilistic Model of Consumer Choice with Substitutes and Complements. (arXiv:1711.03560v1 [stat.ML])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03560"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03560", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7c412f1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7c412f1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We develop SHOPPER, a sequential probabilistic model of market baskets. \nSHOPPER uses interpretable components to model the forces that drive how a \ncustomer chooses products; in particular, we designed SHOPPER to capture how \nitems interact with other items. We develop an efficient posterior inference \nalgorithm to estimate these forces from large-scale data, and we analyze a \nlarge dataset from a major chain grocery store. We are interested in answering \ncounterfactual queries about changes in prices. We found that SHOPPER provides \naccurate predictions even under price interventions, and that it helps identify \ncomplementary and substitutable pairs of products. \n</p>"}, "author": "Francisco J. R. Ruiz, Susan Athey, David M. Blei", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443757", "id": "tag:google.com,2005:reader/item/0000000331e17db1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interpretable Vector AutoRegressions with Exogenous Time Series. (arXiv:1711.03623v1 [stat.ML])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03623"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03623", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Vector AutoRegressive (VAR) model is fundamental to the study of \nmultivariate time series. Although VAR models are intensively investigated by \nmany researchers, practitioners often show more interest in analyzing VARX \nmodels that incorporate the impact of unmodeled exogenous variables (X) into \nthe VAR. However, since the parameter space grows quadratically with the number \nof time series, estimation quickly becomes challenging. While several proposals \nhave been made to sparsely estimate large VAR models, the estimation of large \nVARX models is under-explored. Moreover, typically these sparse proposals \ninvolve a lasso-type penalty and do not incorporate lag selection into the \nestimation procedure. As a consequence, the resulting models may be difficult \nto interpret. In this paper, we propose a lag-based hierarchically sparse \nestimator, called \"HVARX\", for large VARX models. We illustrate the usefulness \nof HVARX on a cross-category management marketing application. Our results show \nhow it provides a highly interpretable model, and improves out-of-sample \nforecast accuracy compared to a lasso-type approach. \n</p>"}, "author": "Ines Wilms, Sumanta Basu, Jacob Bien, David S. Matteson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443756", "id": "tag:google.com,2005:reader/item/0000000331e17e40", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Alternating minimization for dictionary learning with random initialization. (arXiv:1711.03634v1 [stat.ML])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03634"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03634", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present theoretical guarantees for an alternating minimization algorithm \nfor the dictionary learning/sparse coding problem. The dictionary learning \nproblem is to factorize vector samples $y^{1},y^{2},\\ldots, y^{n}$ into an \nappropriate basis (dictionary) $A^*$ and sparse vectors $x^{1*},\\ldots,x^{n*}$. \nOur algorithm is a simple alternating minimization procedure that switches \nbetween $\\ell_1$ minimization and gradient descent in alternate steps. \nDictionary learning and specifically alternating minimization algorithms for \ndictionary learning are well studied both theoretically and empirically. \nHowever, in contrast to previous theoretical analyses for this problem, we \nreplace the condition on the operator norm (that is, the largest magnitude \nsingular value) of the true underlying dictionary $A^*$ with a condition on the \nmatrix infinity norm (that is, the largest magnitude term). This not only \nallows us to get convergence rates for the error of the estimated dictionary \nmeasured in the matrix infinity norm, but also ensures that a random \ninitialization will provably converge to the global optimum. Our guarantees are \nunder a reasonable generative model that allows for dictionaries with growing \noperator norms, and can handle an arbitrary level of overcompleteness, while \nhaving sparsity that is information theoretically optimal. We also establish \nupper bounds on the sample complexity of our algorithm. \n</p>"}, "author": "Niladri S. Chatterji, Peter L. Bartlett", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443755", "id": "tag:google.com,2005:reader/item/0000000331e17e85", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning and Real-time Classification of Hand-written Digits With Spiking Neural Networks. (arXiv:1711.03637v1 [stat.ML])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03637"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03637", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe a novel spiking neural network (SNN) for automated, real-time \nhandwritten digit classification and its implementation on a GP-GPU platform. \nInformation processing within the network, from feature extraction to \nclassification is implemented by mimicking the basic aspects of neuronal spike \ninitiation and propagation in the brain. The feature extraction layer of the \nSNN uses fixed synaptic weight maps to extract the key features of the image \nand the classifier layer uses the recently developed NormAD approximate \ngradient descent based supervised learning algorithm for spiking neural \nnetworks to adjust the synaptic weights. On the standard MNIST database images \nof handwritten digits, our network achieves an accuracy of 99.80% on the \ntraining set and 98.06% on the test set, with nearly 7x fewer parameters \ncompared to the state-of-the-art spiking networks. We further use this network \nin a GPU based user-interface system demonstrating real-time SNN simulation to \ninfer digits written by different users. On a test set of 500 such images, this \nreal-time platform achieves an accuracy exceeding 97% while making a prediction \nwithin an SNN emulation time of less than 100ms. \n</p>"}, "author": "Shruti R. Kulkarni, John M. Alexiades, Bipin Rajendran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443754", "id": "tag:google.com,2005:reader/item/0000000331e17ec8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Provable Approach for Double-Sparse Coding. (arXiv:1711.03638v1 [stat.ML])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03638"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03638", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sparse coding is a crucial subroutine in algorithms for various signal \nprocessing, deep learning, and other machine learning applications. The central \ngoal is to learn an overcomplete dictionary that can sparsely represent a given \ndataset. However, storage, transmission, and processing of the learned \ndictionary can be untenably high if the data dimension is high. In this paper, \nwe consider the double-sparsity model introduced by Rubinstein, Zibulevsky, and \nElad (2010) where the dictionary itself is the product of a fixed, known basis \nand a data-adaptive sparse component. First, we introduce a simple algorithm \nfor double-sparse coding that can be amenable to efficient implementation via \nneural architectures. Second, we theoretically analyze its performance and \ndemonstrate asymptotic sample complexity and running time benefits over \nexisting (provable) approaches for sparse coding. To our knowledge, our work \nintroduces the first computationally efficient algorithm for double-sparse \ncoding that enjoys rigorous statistical guarantees. Finally, we support our \nanalysis via several numerical experiments on simulated data, confirming that \nour method can indeed be useful in problem sizes encountered in practical \napplications. \n</p>"}, "author": "Thanh Nguyen, Raymond K. W. Wong, Chinmay Hegde", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443753", "id": "tag:google.com,2005:reader/item/0000000331e17f02", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Deep Learning in Memristive Networks. (arXiv:1711.03640v1 [stat.ML])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03640"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03640", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the performance of stochastically trained deep neural networks \n(DNNs) whose synaptic weights are implemented using emerging memristive devices \nthat exhibit limited dynamic range, resolution, and variability in their \nprogramming characteristics. We show that a key device parameter to optimize \nthe learning efficiency of DNNs is the variability in its programming \ncharacteristics. DNNs with such memristive synapses, even with dynamic range as \nlow as $15$ and only $32$ discrete levels, when trained based on stochastic \nupdates suffer less than $3\\%$ loss in accuracy compared to floating point \nsoftware baseline. We also study the performance of stochastic memristive DNNs \nwhen used as inference engines with noise corrupted data and find that if the \ndevice variability can be minimized, the relative degradation in performance \nfor the Stochastic DNN is better than that of the software baseline. Hence, our \nstudy presents a new optimization corner for memristive devices for building \nlarge noise-immune deep learning systems. \n</p>"}, "author": "Anakha V Babu, Bipin Rajendran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443752", "id": "tag:google.com,2005:reader/item/0000000331e17f74", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Poverty Prediction with Public Landsat 7 Satellite Imagery and Machine Learning. (arXiv:1711.03654v1 [stat.ML])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03654"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03654", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Obtaining detailed and reliable data about local economic livelihoods in \ndeveloping countries is expensive, and data are consequently scarce. Previous \nwork has shown that it is possible to measure local-level economic livelihoods \nusing high-resolution satellite imagery. However, such imagery is relatively \nexpensive to acquire, often not updated frequently, and is mainly available for \nrecent years. We train CNN models on free and publicly available multispectral \ndaytime satellite images of the African continent from the Landsat 7 satellite, \nwhich has collected imagery with global coverage for almost two decades. We \nshow that despite these images' lower resolution, we can achieve accuracies \nthat exceed previous benchmarks. \n</p>"}, "author": "Anthony Perez, Christopher Yeh, George Azzari, Marshall Burke, David Lobell, Stefano Ermon", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443751", "id": "tag:google.com,2005:reader/item/0000000331e17fa8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Traffic Analysis with Deep Learning. (arXiv:1711.03656v1 [cs.CR])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03656"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03656", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep Neural Networks (DNN) has obtained enormous attention with its \nadvantageous feature learning and its powerful prediction ability. In this \npaper, we broadly study the applicability of deep learning to traffic analysis \nand present its effectiveness on the feature extraction for state-of-the-art \nmachine learning algorithms, website and keyword fingerprinting attacks, and \nthe prediction on the fingerprintability of websites. To the best of our \nknowledge, this is the first extensive work to introduce various applications \nusing DNN in traffic analysis. With great help of DNN, the quality of cutting \nedge website fingerprinting attacks is upgraded while the feature dimension \nbecomes much lower. As the classifiers, DNN successfully detects which website \nthe user visited among 100 websites with 91% TPR and 1% FPR against 100,000 \nbackground websites, and as the fingerprintability predictors, it almost \nperfectly determines the fingerprintability of 4,500 website traffic instances \nwith 99% of accuracy. \n</p>"}, "author": "Se Eun Oh, Saikrishna Sunkam, Nicholas Hopper", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443750", "id": "tag:google.com,2005:reader/item/0000000331e17fc8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Breast density classification with deep convolutional neural networks. (arXiv:1711.03674v1 [cs.CV])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03674"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03674", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Breast density classification is an essential part of breast cancer \nscreening. Although a lot of prior work considered this problem as a task for \nlearning algorithms, to our knowledge, all of them used small and not \nclinically realistic data both for training and evaluation of their models. In \nthis work, we explore the limits of this task with a data set coming from over \n200,000 breast cancer screening exams. We use this data to train and evaluate a \nstrong convolutional neural network classifier. In a reader study, we find that \nour model can perform this task comparably to a human expert. \n</p>"}, "author": "Nan Wu, Krzysztof J. Geras, Yiqiu Shen, Jingyi Su, S. Gene Kim, Eric Kim, Stacey Wolfson, Linda Moy, Kyunghyun Cho", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443749", "id": "tag:google.com,2005:reader/item/0000000331e17fed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reinforcement Learning of Speech Recognition System Based on Policy Gradient and Hypothesis Selection. (arXiv:1711.03689v1 [cs.CL])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03689"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03689", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Speech recognition systems have achieved high recognition performance for \nseveral tasks. However, the performance of such systems is dependent on the \ntremendously costly development work of preparing vast amounts of task-matched \ntranscribed speech data for supervised training. The key problem here is the \ncost of transcribing speech data. The cost is repeatedly required to support \nnew languages and new tasks. Assuming broad network services for transcribing \nspeech data for many users, a system would become more self-sufficient and more \nuseful if it possessed the ability to learn from very light feedback from the \nusers without annoying them. In this paper, we propose a general reinforcement \nlearning framework for speech recognition systems based on the policy gradient \nmethod. As a particular instance of the framework, we also propose a hypothesis \nselection-based reinforcement learning method. The proposed framework provides \na new view for several existing training and adaptation methods. The \nexperimental results show that the proposed method improves the recognition \nperformance compared to unsupervised adaptation. \n</p>"}, "author": "Taku Kato, Takahiro Shinozaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443748", "id": "tag:google.com,2005:reader/item/0000000331e18035", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Quantized Memory-Augmented Neural Networks. (arXiv:1711.03712v1 [cs.LG])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03712"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03712", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7c4172a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7c4172a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Memory-augmented neural networks (MANNs) refer to a class of neural network \nmodels equipped with external memory (such as neural Turing machines and memory \nnetworks). These neural networks outperform conventional recurrent neural \nnetworks (RNNs) in terms of learning long-term dependency, allowing them to \nsolve intriguing AI tasks that would otherwise be hard to address. This paper \nconcerns the problem of quantizing MANNs. Quantization is known to be effective \nwhen we deploy deep models on embedded systems with limited resources. \nFurthermore, quantization can substantially reduce the energy consumption of \nthe inference procedure. These benefits justify recent developments of \nquantized multi layer perceptrons, convolutional networks, and RNNs. However, \nno prior work has reported the successful quantization of MANNs. The in-depth \nanalysis presented here reveals various challenges that do not appear in the \nquantization of the other networks. Without addressing them properly, quantized \nMANNs would normally suffer from excessive quantization error which leads to \ndegraded performance. In this paper, we identify memory addressing \n(specifically, content-based addressing) as the main reason for the performance \ndegradation and propose a robust quantization method for MANNs to address the \nchallenge. In our experiments, we achieved a computation-energy gain of 22x \nwith 8-bit fixed-point and binary quantization compared to the floating-point \nimplementation. Measured on the bAbI dataset, the resulting model, named the \nquantized MANN (Q-MANN), improved the error rate by 46% and 30% with 8-bit \nfixed-point and binary quantization, respectively, compared to the MANN \nquantized using conventional techniques. \n</p>"}, "author": "Seongsik Park, Seijoon Kim, Seil Lee, Ho Bae, Sungroh Yoon", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443747", "id": "tag:google.com,2005:reader/item/0000000331e18072", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "LSTM Networks for Data-Aware Remaining Time Prediction of Business Process Instances. (arXiv:1711.03822v1 [cs.LG])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03822"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03822", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7d025d0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7d025d0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Predicting the completion time of business process instances would be a very \nhelpful aid when managing processes under service level agreement constraints. \nThe ability to know in advance the trend of running process instances would \nallow business managers to react in time, in order to prevent delays or \nundesirable situations. However, making such accurate forecasts is not easy: \nmany factors may influence the required time to complete a process instance. In \nthis paper, we propose an approach based on deep Recurrent Neural Networks \n(specifically LSTMs) that is able to exploit arbitrary information associated \nto single events, in order to produce an as-accurate-as-possible prediction of \nthe completion time of running instances. Experiments on real-world datasets \nconfirm the quality of our proposal. \n</p>"}, "author": "Nicol&#xf2; Navarin, Beatrice Vincenzi, Mirko Polato, Alessandro Sperduti", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443746", "id": "tag:google.com,2005:reader/item/0000000331e180a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "GPflowOpt: A Bayesian Optimization Library using TensorFlow. (arXiv:1711.03845v1 [stat.ML])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03845"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03845", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A novel Python framework for Bayesian optimization known as GPflowOpt is \nintroduced. The package is based on the popular GPflow library for Gaussian \nprocesses, leveraging the benefits of TensorFlow including automatic \ndifferentiation, parallelization and GPU computations for Bayesian \noptimization. Design goals focus on a framework that is easy to extend with \ncustom acquisition functions and models. The framework is thoroughly tested and \nwell documented, and provides scalability. The current released version of \nGPflowOpt includes some standard single-objective acquisition functions, the \nstate-of-the-art max-value entropy search, as well as a Bayesian \nmulti-objective approach. Finally, it permits easy use of custom modeling \nstrategies implemented in GPflow. \n</p>"}, "author": "Nicolas Knudde, Joachim van der Herten, Tom Dhaene, Ivo Couckuyt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443745", "id": "tag:google.com,2005:reader/item/0000000331e180e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "\"Dave...I can assure you...that it's going to be all right...\" -- A definition, case for, and survey of algorithmic assurances in human-autonomy trust relationships. (arXiv:1711.03846v1 [cs.CY])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03846"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03846", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>As technology becomes more advanced, those who design, use and are otherwise \naffected by it want to know that it will perform correctly, and understand why \nit does what it does, and how to use it appropriately. In essence they want to \nbe able to trust the systems that are being designed. In this survey we present \nassurances that are the method by which users can understand how to trust \nautonomous systems. Trust between humans and autonomy is reviewed, and the \nimplications for the design of assurances are highlighted. A survey of existing \nresearch related to assurances is presented. Much of the surveyed research \noriginates from fields such as interpretable, comprehensible, transparent, and \nexplainable machine learning, as well as human-computer interaction, \nhuman-robot interaction, and e-commerce. Several key ideas are extracted from \nthis work in order to refine the definition of assurances. The design of \nassurances is found to be highly dependent not only on the capabilities of the \nautonomous system, but on the characteristics of the human user, and the \nappropriate trust-related behaviors. Several directions for future research are \nidentified and discussed. \n</p>"}, "author": "Brett W Israelsen, Nisar R Ahmed", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443744", "id": "tag:google.com,2005:reader/item/0000000331e180f5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Attend and Diagnose: Clinical Time Series Analysis using Attention Models. (arXiv:1711.03905v2 [stat.ML] UPDATED)", "published": 1511308877, "updated": 1511308893, "canonical": [{"href": "http://arxiv.org/abs/1711.03905"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03905", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With widespread adoption of electronic health records, there is an increased \nemphasis for predictive models that can effectively deal with clinical \ntime-series data. Powered by Recurrent Neural Network (RNN) architectures with \nLong Short-Term Memory (LSTM) units, deep neural networks have achieved \nstate-of-the-art results in several clinical prediction tasks. Despite the \nsuccess of RNNs, its sequential nature prohibits parallelized computing, thus \nmaking it inefficient particularly when processing long sequences. Recently, \narchitectures which are based solely on attention mechanisms have shown \nremarkable success in transduction tasks in NLP, while being computationally \nsuperior. In this paper, for the first time, we utilize attention models for \nclinical time-series modeling, thereby dispensing recurrence entirely. We \ndevelop the \\textit{SAnD} (Simply Attend and Diagnose) architecture, which \nemploys a masked, self-attention mechanism, and uses positional encoding and \ndense interpolation strategies for incorporating temporal order. Furthermore, \nwe develop a multi-task variant of \\textit{SAnD} to jointly infer models with \nmultiple diagnosis tasks. Using the recent MIMIC-III benchmark datasets, we \ndemonstrate that the proposed approach achieves state-of-the-art performance in \nall tasks, outperforming LSTM models and classical baselines with \nhand-engineered features. \n</p>"}, "author": "Huan Song, Deepta Rajan, Jayaraman J. Thiagarajan, Andreas Spanias", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443743", "id": "tag:google.com,2005:reader/item/0000000331e18101", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Paragraph Vectors. (arXiv:1711.03946v1 [cs.CL])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03946"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03946", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Word2vec (Mikolov et al., 2013) has proven to be successful in natural \nlanguage processing by capturing the semantic relationships between different \nwords. Built on top of single-word embeddings, paragraph vectors (Le and \nMikolov, 2014) find fixed-length representations for pieces of text with \narbitrary lengths, such as documents, paragraphs, and sentences. In this work, \nwe propose a novel interpretation for neural-network-based paragraph vectors by \ndeveloping an unsupervised generative model whose maximum likelihood solution \ncorresponds to traditional paragraph vectors. This probabilistic formulation \nallows us to go beyond point estimates of parameters and to perform Bayesian \nposterior inference. We find that the entropy of paragraph vectors decreases \nwith the length of documents, and that information about posterior uncertainty \nimproves performance in supervised learning tasks such as sentiment analysis \nand paraphrase detection. \n</p>"}, "author": "Geng Ji, Robert Bamler, Erik B. Sudderth, Stephan Mandt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510588781444", "timestampUsec": "1510588781443742", "id": "tag:google.com,2005:reader/item/0000000331e18113", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dynamic Analysis of Executables to Detect and Characterize Malware. (arXiv:1711.03947v1 [cs.CR])", "published": 1510588781, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03947"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03947", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is needed to ensure the integrity of systems that process sensitive \ninformation and control many aspects of everyday life. We examine the use of \nmachine learning algorithms to detect malware using the system calls generated \nby executables-alleviating attempts at obfuscation as the behavior is monitored \nrather than the bytes of an executable. We examine several machine learning \ntechniques for detecting malware including random forests, deep learning \ntechniques, and liquid state machines. The experiments examine the effects of \nconcept drift on each algorithm to understand how well the algorithms \ngeneralize to novel malware samples by testing them on data that was collected \nafter the training data. The results suggest that each of the examined machine \nlearning algorithms is a viable solution to detect malware-achieving between \n90% and 95% class-averaged accuracy (CAA). In real-world scenarios, the \nperformance evaluation on an operational network may not match the performance \nachieved in training. Namely, the CAA may be about the same, but the values for \nprecision and recall over the malware can change significantly. We structure \nexperiments to highlight these caveats and offer insights into expected \nperformance in operational environments. In addition, we use the induced models \nto gain a better understanding about what differentiates the malware samples \nfrom the goodware, which can further be used as a forensics tool to understand \nwhat the malware (or goodware) was doing to provide directions for \ninvestigation and remediation. \n</p>"}, "author": "Michael R. Smith, Joe B. Ingram, Christopher C. Lamb, Timothy J. Draelos, Justin E. Doak, James B. Aimone, Conrad D. James", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148069", "id": "tag:google.com,2005:reader/item/000000033032b1e3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the incorporation of interval-valued fuzzy sets into the Bousi-Prolog system: declarative semantics, implementation and applications. (arXiv:1711.03147v1 [cs.AI])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03147"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03147", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we analyse the benefits of incorporating interval-valued fuzzy \nsets into the Bousi-Prolog system. A syntax, declarative semantics and im- \nplementation for this extension is presented and formalised. We show, by using \npotential applications, that fuzzy logic programming frameworks enhanced with \nthem can correctly work together with lexical resources and ontologies in order \nto improve their capabilities for knowledge representation and reasoning. \n</p>"}, "author": "Clemente Rubio-Manzano, Martin Pereira-Fari&#xf1;a", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148068", "id": "tag:google.com,2005:reader/item/000000033032b1ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Information Directed Sampling for Stochastic Bandits with Graph Feedback. (arXiv:1711.03198v1 [cs.LG])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03198"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03198", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider stochastic multi-armed bandit problems with graph feedback, where \nthe decision maker is allowed to observe the neighboring actions of the chosen \naction. We allow the graph structure to vary with time and consider both \ndeterministic and Erd\\H{o}s-R\\'enyi random graph models. For such a graph \nfeedback model, we first present a novel analysis of Thompson sampling that \nleads to tighter performance bound than existing work. Next, we propose new \nInformation Directed Sampling based policies that are graph-aware in their \ndecision making. Under the deterministic graph case, we establish a Bayesian \nregret bound for the proposed policies that scales with the clique cover number \nof the graph instead of the number of actions. Under the random graph case, we \nprovide a Bayesian regret bound for the proposed policies that scales with the \nratio of the number of actions over the expected number of observations per \niteration. To the best of our knowledge, this is the first analytical result \nfor stochastic bandits with random graph feedback. Finally, using numerical \nevaluations, we demonstrate that our proposed IDS policies outperform existing \napproaches, including adaptions of upper confidence bound, $\\epsilon$-greedy \nand Exp3 algorithms. \n</p>"}, "author": "Fang Liu, Swapna Buccapatnam, Ness Shroff", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148067", "id": "tag:google.com,2005:reader/item/000000033032b1fb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Large-scale Cloze Test Dataset Designed by Teachers. (arXiv:1711.03225v1 [cs.CL])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03225"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03225", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Cloze test is widely adopted in language exams to evaluate students' language \nproficiency. In this paper, we propose the first large-scale human-designed \ncloze test dataset CLOTH, in which the questions were used in middle-school and \nhigh-school language exams. With the missing blanks carefully created by \nteachers and candidate choices purposely designed to be confusing, CLOTH \nrequires a deeper language understanding and a wider attention span than \nprevious automatically generated cloze datasets. We show humans outperform \ndedicated designed baseline models by a significant margin, even when the model \nis trained on sufficiently large external data. We investigate the source of \nthe performance gap, trace model deficiencies to some distinct properties of \nCLOTH, and identify the limited ability of comprehending a long-term context to \nbe the key bottleneck. \n</p>"}, "author": "Qizhe Xie, Guokun Lai, Zihang Dai, Eduard Hovy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148066", "id": "tag:google.com,2005:reader/item/000000033032b221", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "CogSciK: Clustering for Cognitive Science Motivated Decision Making. (arXiv:1711.03237v1 [cs.AI])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03237"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03237", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7d02bb8\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7d02bb8&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Computational models of decisionmaking must contend with the variance of \ncontext and any number of possible decisions that a defined strategic actor can \nmake at a given time. Relying on cognitive science theory, the authors have \ncreated an algorithm that captures the orientation of the actor towards an \nobject and arrays the possible decisions available to that actor based on their \ngiven intersubjective orientation. This algorithm, like a traditional K-means \nclustering algorithm, relies on a core-periphery structure that gives the \nlikelihood of moves as those closest to the cluster's centroid. The result is \nan algorithm that enables unsupervised classification of an array of decision \npoints belonging to an actor's present state and deeply rooted in cognitive \nscience theory. \n</p>"}, "author": "Dr. W. A. Rivera, James C. Wu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148065", "id": "tag:google.com,2005:reader/item/000000033032b229", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to select examples for program synthesis. (arXiv:1711.03243v1 [cs.AI])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03243"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03243", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Program synthesis is a class of regression problems where one seeks a \nsolution, in the form of a source-code program, mapping the inputs to their \ncorresponding outputs exactly. Due to its precise and combinatorial nature, it \nis commonly formulated as a constraint satisfaction problem, where input-output \nexamples are encoded as constraints and solved with a constraint solver. A key \nchallenge of this formulation is scalability: while constraint solvers work \nwell with few well-chosen examples, a large set of examples can incur \nsignificant overhead in both time and memory. We address this challenge by \nconstructing a representative subset of examples that is both small and able to \nconstrain the solver sufficiently. We build the subset one example at a time, \nusing a neural network to predict the probability of unchosen input-output \nexamples conditioned on the chosen input-output examples, and adding the least \nprobable example to the subset. Experiment on a diagram drawing domain shows \nour approach produces subsets of examples that are small and representative for \nthe constraint solver. \n</p>"}, "author": "Yewen Pu, Zachery Miranda, Armando Solar-Lezama, Leslie Pack Kaelbling", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148064", "id": "tag:google.com,2005:reader/item/000000033032b238", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Automated Distribution System Planning for Large-Scale Network Integration Studies. (arXiv:1711.03331v1 [cs.CE])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03331"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03331", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Network integration studies try to assess the impact of future developments, \nsuch as the increase of Renewable Energy Sources or the introduction of Smart \nGrid Technologies, on large-scale network areas. Goals can be to support \nstrategic alignment in the regulatory framework or to adapt the network \nplanning principles of Distribution System Operators. This study outlines an \napproach for the automated distribution system planning that can calculate \nnetwork reconfiguration, reinforcement and extension plans in a fully automated \nfashion. This allows the estimation of the expected cost in massive \nprobabilistic simulations of large numbers of real networks and constitutes a \ncore component of a framework for large-scale network integration studies. \nExemplary case study results are presented that were performed in cooperation \nwith different major distribution system operators. The case studies cover the \nestimation of expected network reinforcement costs, technical and economical \nassessment of smart grid technologies and structural network optimisation. \n</p>"}, "author": "Alexander Scheidler, Leon Thurner, Martin Braun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148063", "id": "tag:google.com,2005:reader/item/000000033032b242", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Repairing Ontologies via Axiom Weakening. (arXiv:1711.03430v1 [cs.AI])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03430"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03430", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Ontology engineering is a hard and error-prone task, in which small changes \nmay lead to errors, or even produce an inconsistent ontology. As ontologies \ngrow in size, the need for automated methods for repairing inconsistencies \nwhile preserving as much of the original knowledge as possible increases. Most \nprevious approaches to this task are based on removing a few axioms from the \nontology to regain consistency. We propose a new method based on weakening \nthese axioms to make them less restrictive, employing the use of refinement \noperators. We introduce the theoretical framework for weakening DL ontologies, \npropose algorithms to repair ontologies based on the framework, and provide an \nanalysis of the computational complexity. Through an empirical analysis made \nover real-life ontologies, we show that our approach preserves significantly \nmore of the original knowledge of the ontology than removing axioms. \n</p>"}, "author": "Nicolas Troquard, Roberto Confalonieri, Pietro Galliani, Rafael Penaloza, Daniele Porello, Oliver Kutz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148062", "id": "tag:google.com,2005:reader/item/000000033032b248", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Open-World Knowledge Graph Completion. (arXiv:1711.03438v1 [cs.AI])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03438"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03438", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Knowledge Graphs (KGs) have been applied to many tasks including Web search, \nlink prediction, recommendation, natural language processing, and entity \nlinking. However, most KGs are far from complete and are growing at a rapid \npace. To address these problems, Knowledge Graph Completion (KGC) has been \nproposed to improve KGs by filling in its missing connections. Unlike existing \nmethods which hold a closed-world assumption, i.e., where KGs are fixed and new \nentities cannot be easily added, in the present work we relax this assumption \nand propose a new open-world KGC task. As a first attempt to solve this task we \nintroduce an open-world KGC model called ConMask. This model learns embeddings \nof the entity's name and parts of its text-description to connect unseen \nentities to the KG. To mitigate the presence of noisy text descriptions, \nConMask uses a relationship-dependent content masking to extract relevant \nsnippets and then trains a fully convolutional neural network to fuse the \nextracted snippets with entities in the KG. Experiments on large data sets, \nboth old and new, show that ConMask performs well in the open-world KGC task \nand even outperforms existing KGC models on the standard closed-world KGC task. \n</p>"}, "author": "Baoxu Shi, Tim Weninger", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148061", "id": "tag:google.com,2005:reader/item/000000033032b255", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Worm-level Control through Search-based Reinforcement Learning. (arXiv:1711.03467v1 [cs.NE])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03467"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03467", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Through natural evolution, nervous systems of organisms formed near-optimal \nstructures to express behavior. Here, we propose an effective way to create \ncontrol agents, by \\textit{re-purposing} the function of biological neural \ncircuit models, to govern similar real world applications. We model the \ntap-withdrawal (TW) neural circuit of the nematode, \\textit{C. elegans}, a \ncircuit responsible for the worm's reflexive response to external mechanical \ntouch stimulations, and learn its synaptic and neural parameters as a policy \nfor controlling the inverted pendulum problem. For reconfiguration of the \npurpose of the TW neural circuit, we manipulate a search-based reinforcement \nlearning. We show that our neural policy performs as good as existing \ntraditional control theory and machine learning approaches. A video \ndemonstration of the performance of our method can be accessed at \n\\url{https://youtu.be/o-Ia5IVyff8}. \n</p>"}, "author": "Mathias Lechner, Radu Grosu, Ramin M. Hasani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148060", "id": "tag:google.com,2005:reader/item/000000033032b26d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Scalable Log Determinants for Gaussian Process Kernel Learning. (arXiv:1711.03481v1 [stat.ML])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03481"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03481", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>For applications as varied as Bayesian neural networks, determinantal point \nprocesses, elliptical graphical models, and kernel learning for Gaussian \nprocesses (GPs), one must compute a log determinant of an $n \\times n$ positive \ndefinite matrix, and its derivatives - leading to prohibitive \n$\\mathcal{O}(n^3)$ computations. We propose novel $\\mathcal{O}(n)$ approaches \nto estimating these quantities from only fast matrix vector multiplications \n(MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and \nsurrogate models, and converge quickly even for kernel matrices that have \nchallenging spectra. We leverage these approximations to develop a scalable \nGaussian process approach to kernel learning. We find that Lanczos is generally \nsuperior to Chebyshev for kernel learning, and that a surrogate approach can be \nhighly efficient and accurate with popular kernels. \n</p>"}, "author": "Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, Andrew Gordon Wilson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148059", "id": "tag:google.com,2005:reader/item/000000033032b27d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Multi-Modal Word Representation Grounded in Visual Context. (arXiv:1711.03483v1 [cs.CL])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03483"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03483", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Representing the semantics of words is a long-standing problem for the \nnatural language processing community. Most methods compute word semantics \ngiven their textual context in large corpora. More recently, researchers \nattempted to integrate perceptual and visual features. Most of these works \nconsider the visual appearance of objects to enhance word representations but \nthey ignore the visual environment and context in which objects appear. We \npropose to unify text-based techniques with vision-based techniques by \nsimultaneously leveraging textual and visual context to learn multimodal word \nembeddings. We explore various choices for what can serve as a visual context \nand present an end-to-end method to integrate visual context elements in a \nmultimodal skip-gram model. We provide experiments and extensive analysis of \nthe obtained results. \n</p>"}, "author": "&#xc9;loi Zablocki, Benjamin Piwowarski, Laure Soulier, Patrick Gallinari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510337405148", "timestampUsec": "1510337405148058", "id": "tag:google.com,2005:reader/item/000000033032b285", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Fast Meta-Learning for Adaptive Hierarchical Classifier Design. (arXiv:1711.03512v1 [cs.LG])", "published": 1510337406, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03512"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03512", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new splitting criterion for a meta-learning approach to \nmulticlass classifier design that adaptively merges the classes into a \ntree-structured hierarchy of increasingly difficult binary classification \nproblems. The classification tree is constructed from empirical estimates of \nthe Henze-Penrose bounds on the pairwise Bayes misclassification rates that \nrank the binary subproblems in terms of difficulty of classification. The \nproposed empirical estimates of the Bayes error rate are computed from the \nminimal spanning tree (MST) of the samples from each pair of classes. Moreover, \na meta-learning technique is presented for quantifying the one-vs-rest Bayes \nerror rate for each individual class from a single MST on the entire dataset. \nExtensive simulations on benchmark datasets show that the proposed hierarchical \nmethod can often be learned much faster than competing methods, while achieving \ncompetitive accuracy. \n</p>"}, "author": "Gerrit J. J. van den Burg, Alfred O. Hero", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023778", "id": "tag:google.com,2005:reader/item/000000033006469d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An asymptotic analysis of distributed nonparametric methods. (arXiv:1711.03149v1 [math.ST])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03149"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03149", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We investigate and compare the fundamental performance of several distributed \nlearning methods that have been proposed recently. We do this in the context of \na distributed version of the classical signal-in-Gaussian-white-noise model, \nwhich serves as a benchmark model for studying performance in this setting. The \nresults show how the design and tuning of a distributed method can have great \nimpact on convergence rates and validity of uncertainty quantification. \nMoreover, we highlight the difficulty of designing nonparametric distributed \nprocedures that automatically adapt to smoothness. \n</p>"}, "author": "Botond Szabo, Harry van Zanten", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023777", "id": "tag:google.com,2005:reader/item/00000003300646a4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Hyperspherical Learning. (arXiv:1711.03189v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03189"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03189", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7d02f7a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7d02f7a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Convolution as inner product has been the founding basis of convolutional \nneural networks (CNNs) and the key to end-to-end visual representation \nlearning. Benefiting from deeper architectures, recent CNNs have demonstrated \nincreasingly strong representation abilities. Despite such improvement, the \nincreased depth and larger parameter space have also led to challenges in \nproperly training a network. In light of such challenges, we propose \nhyperspherical convolution (SphereConv), a novel learning framework that gives \nangular representations on hyperspheres. We introduce SphereNet, deep \nhyperspherical convolution networks that are distinct from conventional inner \nproduct based convolutional networks. In particular, SphereNet adopts \nSphereConv as its basic convolution operator and is supervised by generalized \nangular softmax loss - a natural loss formulation under SphereConv. We show \nthat SphereNet can effectively encode discriminative representation and \nalleviate training difficulty, leading to easier optimization, faster \nconvergence and comparable (even better) classification accuracy over \nconvolutional counterparts. We also provide some theoretical insights for the \nadvantages of learning on hyperspheres. In addition, we introduce the learnable \nSphereConv, i.e., a natural improvement over prefixed SphereConv, and \nSphereNorm, i.e., hyperspherical learning as a normalization method. \nExperiments have verified our conclusions. \n</p>"}, "author": "Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, Le Song", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023776", "id": "tag:google.com,2005:reader/item/00000003300646ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Creating Credible Models. (arXiv:1711.03190v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03190"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03190", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7d7e7bb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7d7e7bb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In many settings, it is important that a model be capable of providing \nreasons for its predictions (i.e., the model must be interpretable). However, \nthe model's reasoning may not conform with well-established knowledge. In such \ncases, while interpretable, the model lacks \\textit{credibility}. In this work, \nwe formally define credibility in the linear setting and focus on techniques \nfor learning models that are both accurate and credible. In particular, we \npropose a regularization penalty, expert yielded estimates (EYE), that \nincorporates expert knowledge about well-known relationships among covariates \nand the outcome of interest. We give both theoretical and empirical results \ncomparing our proposed method to several other regularization techniques. \nAcross a range of settings, experiments on both synthetic and real data show \nthat models learned using the EYE penalty are significantly more credible than \nthose learned using other penalties. Applied to a large-scale patient risk \nstratification task, our proposed technique results in a model whose top \nfeatures overlap significantly with known clinical risk factors, while still \nachieving good predictive performance. \n</p>"}, "author": "Jiaxuan Wang, Jeeheh Oh, Jenna Wiens", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023775", "id": "tag:google.com,2005:reader/item/00000003300646b6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Long-Term Sequential Prediction Using Expert Advice. (arXiv:1711.03194v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03194"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03194", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>For the prediction with expert advice setting, we consider methods to \nconstruct forecasting algorithms that suffer loss not much more than of any \nexpert in the pool. In contrast to the standard approach, we investigate the \ncase of long-term interval forecasting of time series, that is, each expert \nissues a sequence of forecasts for a time interval ahead and the master \nalgorithm combines these forecasts into one aggregated sequence of forecasts. \nTwo new approaches for aggregating experts long-term interval predictions are \npresented. One is based on Vovk's aggregation algorithm and considers sliding \nexperts, the other applies the approach of Mixing Past Posteriors method to the \nlong-term prediction. The upper bounds for regret of these algorithms for \nadversarial case are obtained. We also present results of numerical experiments \nof time series long-term prediction. \n</p>"}, "author": "Eugeny Burnaev, Alexander Korotin, Vladimir V&#x27;yugin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023774", "id": "tag:google.com,2005:reader/item/00000003300646c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Information Directed Sampling for Stochastic Bandits with Graph Feedback. (arXiv:1711.03198v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03198"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03198", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider stochastic multi-armed bandit problems with graph feedback, where \nthe decision maker is allowed to observe the neighboring actions of the chosen \naction. We allow the graph structure to vary with time and consider both \ndeterministic and Erd\\H{o}s-R\\'enyi random graph models. For such a graph \nfeedback model, we first present a novel analysis of Thompson sampling that \nleads to tighter performance bound than existing work. Next, we propose new \nInformation Directed Sampling based policies that are graph-aware in their \ndecision making. Under the deterministic graph case, we establish a Bayesian \nregret bound for the proposed policies that scales with the clique cover number \nof the graph instead of the number of actions. Under the random graph case, we \nprovide a Bayesian regret bound for the proposed policies that scales with the \nratio of the number of actions over the expected number of observations per \niteration. To the best of our knowledge, this is the first analytical result \nfor stochastic bandits with random graph feedback. Finally, using numerical \nevaluations, we demonstrate that our proposed IDS policies outperform existing \napproaches, including adaptions of upper confidence bound, $\\epsilon$-greedy \nand Exp3 algorithms. \n</p>"}, "author": "Fang Liu, Swapna Buccapatnam, Ness Shroff", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023773", "id": "tag:google.com,2005:reader/item/00000003300646cf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Crafting Adversarial Examples For Speech Paralinguistics Applications. (arXiv:1711.03280v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03280"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03280", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Computational paralinguistic analysis is increasingly being used in a wide \nrange of applications, including security-sensitive applications such as \nspeaker verification, deceptive speech detection, and medical diagnosis. While \nstate-of-the-art machine learning techniques, such as deep neural networks, can \nprovide robust and accurate speech analysis, they are susceptible to \nadversarial attacks. In this work, we propose a novel end-to-end scheme to \ngenerate adversarial examples by perturbing directly the raw waveform of an \naudio recording rather than specific acoustic features. Our experiments show \nthat the proposed adversarial perturbation can lead to a significant \nperformance drop of state-of-the-art deep neural networks, while only minimally \nimpairing the audio quality. \n</p>"}, "author": "Yuan Gong, Christian Poellabauer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023772", "id": "tag:google.com,2005:reader/item/00000003300646da", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Separation Principle for Control in the Age of Deep Learning. (arXiv:1711.03321v1 [stat.ML])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03321"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03321", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We review the problem of defining and inferring a \"state\" for a control \nsystem based on complex, high-dimensional, highly uncertain measurement streams \nsuch as videos. Such a state, or representation, should contain all and only \nthe information needed for control, and discount nuisance variability in the \ndata. It should also have finite complexity, ideally modulated depending on \navailable resources. This representation is what we want to store in memory in \nlieu of the data, as it \"separates\" the control task from the measurement \nprocess. For the trivial case with no dynamics, a representation can be \ninferred by minimizing the Information Bottleneck Lagrangian in a function \nclass realized by deep neural networks. The resulting representation has much \nhigher dimension than the data, already in the millions, but it is smaller in \nthe sense of information content, retaining only what is needed for the task. \nThis process also yields representations that are invariant to nuisance factors \nand having maximally independent components. We extend these ideas to the \ndynamic case, where the representation is the posterior density of the task \nvariable given the measurements up to the current time, which is in general \nmuch simpler than the prediction density maintained by the classical Bayesian \nfilter. Again this can be finitely-parametrized using a deep neural network, \nand already some applications are beginning to emerge. No explicit assumption \nof Markovianity is needed; instead, complexity trades off approximation of an \noptimal representation, including the degree of Markovianity. \n</p>"}, "author": "Alessandro Achille, Stefano Soatto", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023771", "id": "tag:google.com,2005:reader/item/00000003300646e0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Analysis of Dropout in Online Learning. (arXiv:1711.03343v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03343"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03343", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning is the state-of-the-art in fields such as visual object \nrecognition and speech recognition. This learning uses a large number of layers \nand a huge number of units and connections. Therefore, overfitting is a serious \nproblem with it, and the dropout which is a kind of regularization tool is \nused. However, in online learning, the effect of dropout is not well known. \nThis paper presents our investigation on the effect of dropout in online \nlearning. We analyzed the effect of dropout on convergence speed near the \nsingular point. Our results indicated that dropout is effective in online \nlearning. Dropout tends to avoid the singular point for convergence speed near \nthat point. \n</p>"}, "author": "Kazuyuki Hara", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023770", "id": "tag:google.com,2005:reader/item/00000003300646e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dimension Reduction of High-Dimensional Datasets Based on Stepwise SVM. (arXiv:1711.03346v1 [stat.AP])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03346"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03346", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The current study proposes a dimension reduction method, stepwise support \nvector machine (SVM), to reduce the dimensions of large p small n datasets. The \nproposed method is compared with other dimension reduction methods, namely, the \nPearson product difference correlation coefficient (PCCs), recursive feature \nelimination based on random forest (RF-RFE), and principal component analysis \n(PCA), by using five gene expression datasets. Additionally, the prediction \nperformance of the variables selected by our method is evaluated. The study \nfound that stepwise SVM can effectively select the important variables and \nachieve good prediction performance. Moreover, the predictions of stepwise SVM \nfor reduced datasets was better than those for the unreduced datasets. The \nperformance of stepwise SVM was more stable than that of PCA and RF-RFE, but \nthe performance difference with respect to PCCs was minimal. It is necessary to \nreduce the dimensions of large p small n datasets. We believe that stepwise SVM \ncan effectively eliminate noise in data and improve the prediction accuracy in \nany large p small n dataset. \n</p>"}, "author": "Elizabeth P. Chou, Tzu-Wei Ko", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023769", "id": "tag:google.com,2005:reader/item/00000003300646f3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Multi-Relevance Transfer Learning. (arXiv:1711.03361v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03361"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03361", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Transfer learning aims to faciliate learning tasks in a label-scarce target \ndomain by leveraging knowledge from a related source domain with plenty of \nlabeled data. Often times we may have multiple domains with little or no \nlabeled data as targets waiting to be solved. Most existing efforts tackle \ntarget domains separately by modeling the `source-target' pairs without \nexploring the relatedness between them, which would cause loss of crucial \ninformation, thus failing to achieve optimal capability of knowledge transfer. \nIn this paper, we propose a novel and effective approach called Multi-Relevance \nTransfer Learning (MRTL) for this purpose, which can simultaneously transfer \ndifferent knowledge from the source and exploits the shared common latent \nfactors between target domains. Specifically, we formulate the problem as an \noptimization task based on a collective nonnegative matrix tri-factorization \nframework. The proposed approach achieves both source-target transfer and \ntarget-target leveraging by sharing multiple decomposed latent subspaces. \nFurther, an alternative minimization learning algorithm is developed with \nconvergence guarantee. Empirical study validates the performance and \neffectiveness of MRTL compared to the state-of-the-art methods. \n</p>"}, "author": "Tianchun Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023768", "id": "tag:google.com,2005:reader/item/00000003300646fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A random matrix analysis and improvement of semi-supervised learning for large dimensional data. (arXiv:1711.03404v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03404"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03404", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This article provides an original understanding of the behavior of a class of \ngraph-oriented semi-supervised learning algorithms in the limit of large and \nnumerous data. It is demonstrated that the intuition at the root of these \nmethods collapses in this limit and that, as a result, most of them become \ninconsistent. Corrective measures and a new data-driven parametrization scheme \nare proposed along with a theoretical analysis of the asymptotic performances \nof the resulting approach. A surprisingly close behavior between theoretical \nperformances on Gaussian mixture models and on real datasets is also \nillustrated throughout the article, thereby suggesting the importance of the \nproposed analysis for dealing with practical data. As a result, significant \nperformance gains are observed on practical data classification using the \nproposed parametrization. \n</p>"}, "author": "Xiaoyi Mai, Romain Couillet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023767", "id": "tag:google.com,2005:reader/item/0000000330064707", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Using Phone Sensors and an Artificial Neural Network to Detect Gait Changes During Drinking Episodes in the Natural Environment. (arXiv:1711.03410v1 [cs.CY])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03410"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03410", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7d7eb17\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7d7eb17&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Phone sensors could be useful in assessing changes in gait that occur with \nalcohol consumption. This study determined (1) feasibility of collecting \ngait-related data during drinking occasions in the natural environment, and (2) \nhow gait-related features measured by phone sensors relate to estimated blood \nalcohol concentration (eBAC). Ten young adult heavy drinkers were prompted to \ncomplete a 5-step gait task every hour from 8pm to 12am over four consecutive \nweekends. We collected 3-xis accelerometer, gyroscope, and magnetometer data \nfrom phone sensors, and computed 24 gait-related features using a sliding \nwindow technique. eBAC levels were calculated at each time point based on \nEcological Momentary Assessment (EMA) of alcohol use. We used an artificial \nneural network model to analyze associations between sensor features and eBACs \nin training (70% of the data) and validation and test (30% of the data) \ndatasets. We analyzed 128 data points where both eBAC and gait-related sensor \ndata was captured, either when not drinking (n=60), while eBAC was ascending \n(n=55) or eBAC was descending (n=13). 21 data points were captured at times \nwhen the eBAC was greater than the legal limit (0.08 mg/dl). Using a Bayesian \nregularized neural network, gait-related phone sensor features showed a high \ncorrelation with eBAC (Pearson's r &gt; 0.9), and &gt;95% of estimated eBAC would \nfall between -0.012 and +0.012 of actual eBAC. It is feasible to collect \ngait-related data from smartphone sensors during drinking occasions in the \nnatural environment. Sensor-based features can be used to infer gait changes \nassociated with elevated blood alcohol content. \n</p>"}, "author": "Brian Suffoletto, Pedram Gharani, Tammy Chung, Hassan Karimi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023766", "id": "tag:google.com,2005:reader/item/0000000330064711", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Can clustering scale sublinearly with its clusters? A variational EM acceleration of GMMs and $k$-means. (arXiv:1711.03431v1 [stat.ML])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03431"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>One iteration of $k$-means or EM for Gaussian mixture models (GMMs) scales \nlinearly with the number of data points $N$, the number of clusters $C$, and \nthe data dimensionality $D$. In this study, we explore whether one iteration of \n$k$-means or EM for GMMs can scale sublinearly with $C$ at run-time, while the \nincrease of the clustering objective remains effective. The tool we apply for \ncomplexity reduction is variational EM, which is typically applied to make \ntraining of generative models with exponentially many hidden states tractable. \nHere, we apply novel theoretical results on truncated variational EM to make \ntractable clustering algorithms more efficient. The basic idea is the use of a \npartial variational E-step which reduces the linear complexity of \n$\\mathcal{O}(NCD)$ required for a full E-step to a sublinear complexity. Our \nmain observation is that the linear dependency on $C$ can be reduced to a \ndependency on a much smaller parameter $G$, related to the cluster neighborhood \nrelationship. We focus on two versions of partial variational EM for \nclustering: variational GMM, scaling with $\\mathcal{O}(NG^2D)$, and variational \n$k$-means, scaling with $\\mathcal{O}(NGD)$ per iteration. Empirical results \nthen show that these algorithms still require comparable numbers of iterations \nto increase the clustering objective to the same values as $k$-means. For data \nwith many clusters, we consequently observe reductions of the net computational \ndemands between two and three orders of magnitude. More generally, our results \nprovide substantial empirical evidence in favor of clustering to scale \nsublinearly with $C$. \n</p>"}, "author": "Dennis Forster, J&#xf6;rg L&#xfc;cke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023765", "id": "tag:google.com,2005:reader/item/0000000330064724", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization. (arXiv:1711.03439v1 [math.OC])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03439"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03439", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new randomized coordinate descent method for a convex \noptimization template with broad applications. Our analysis relies on a novel \ncombination of four ideas applied to the primal-dual gap function: smoothing, \nacceleration, homotopy, and coordinate descent with non-uniform sampling. As a \nresult, our method features the first convergence rate guarantees among the \ncoordinate descent methods, that are the best-known under a variety of common \nstructure assumptions on the template. We provide numerical evidence to support \nthe theoretical results with a comparison to state-of-the-art algorithms. \n</p>"}, "author": "Ahmet Alacaoglu, Quoc Tran-Dinh, Olivier Fercoq, Volkan Cevher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023764", "id": "tag:google.com,2005:reader/item/0000000330064733", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels. (arXiv:1711.03440v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03440"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03440", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we consider parameter recovery for non-overlapping \nconvolutional neural networks (CNNs) with multiple kernels. We show that when \nthe inputs follow Gaussian distribution and the sample size is sufficiently \nlarge, the squared loss of such CNNs is $\\mathit{~locally~strongly~convex}$ in \na basin of attraction near the global optima for most popular activation \nfunctions, like ReLU, Leaky ReLU, Squared ReLU, Sigmoid and Tanh. The required \nsample complexity is proportional to the dimension of the input and polynomial \nin the number of kernels and a condition number of the parameters. We also show \nthat tensor methods are able to initialize the parameters to the local strong \nconvex region. Hence, for most smooth activations, gradient descent following \ntensor initialization is guaranteed to converge to the global optimal with time \nthat is linear in input dimension, logarithmic in precision and polynomial in \nother factors. To the best of our knowledge, this is the first work that \nprovides recovery guarantees for CNNs with multiple kernels under polynomial \nsample and computational complexities. \n</p>"}, "author": "Kai Zhong, Zhao Song, Inderjit S. Dhillon", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023763", "id": "tag:google.com,2005:reader/item/000000033006473f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Scalable Log Determinants for Gaussian Process Kernel Learning. (arXiv:1711.03481v1 [stat.ML])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03481"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03481", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>For applications as varied as Bayesian neural networks, determinantal point \nprocesses, elliptical graphical models, and kernel learning for Gaussian \nprocesses (GPs), one must compute a log determinant of an $n \\times n$ positive \ndefinite matrix, and its derivatives - leading to prohibitive \n$\\mathcal{O}(n^3)$ computations. We propose novel $\\mathcal{O}(n)$ approaches \nto estimating these quantities from only fast matrix vector multiplications \n(MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and \nsurrogate models, and converge quickly even for kernel matrices that have \nchallenging spectra. We leverage these approximations to develop a scalable \nGaussian process approach to kernel learning. We find that Lanczos is generally \nsuperior to Chebyshev for kernel learning, and that a surrogate approach can be \nhighly efficient and accurate with popular kernels. \n</p>"}, "author": "Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, Andrew Gordon Wilson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023762", "id": "tag:google.com,2005:reader/item/000000033006474e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Fast Meta-Learning for Adaptive Hierarchical Classifier Design. (arXiv:1711.03512v1 [cs.LG])", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03512"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03512", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new splitting criterion for a meta-learning approach to \nmulticlass classifier design that adaptively merges the classes into a \ntree-structured hierarchy of increasingly difficult binary classification \nproblems. The classification tree is constructed from empirical estimates of \nthe Henze-Penrose bounds on the pairwise Bayes misclassification rates that \nrank the binary subproblems in terms of difficulty of classification. The \nproposed empirical estimates of the Bayes error rate are computed from the \nminimal spanning tree (MST) of the samples from each pair of classes. Moreover, \na meta-learning technique is presented for quantifying the one-vs-rest Bayes \nerror rate for each individual class from a single MST on the entire dataset. \nExtensive simulations on benchmark datasets show that the proposed hierarchical \nmethod can often be learned much faster than competing methods, while achieving \ncompetitive accuracy. \n</p>"}, "author": "Gerrit J. J. van den Burg, Alfred O. Hero", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510319034024", "timestampUsec": "1510319034023745", "id": "tag:google.com,2005:reader/item/00000003300647e4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Finding Heavily-Weighted Features in Data Streams. (arXiv:1711.02305v1 [cs.LG] CROSS LISTED)", "published": 1510319034, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02305"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02305", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a new sub-linear space data structure---the Weight-Median \nSketch---that captures the most heavily weighted features in linear classifiers \ntrained over data streams. This enables memory-limited execution of several \nstatistical analyses over streams, including online feature selection, \nstreaming data explanation, relative deltoid detection, and streaming \nestimation of pointwise mutual information. In contrast with related sketches \nthat capture the most commonly occurring features (or items) in a data stream, \nthe Weight-Median Sketch captures the features that are most discriminative of \none stream (or class) compared to another. The Weight-Median sketch adopts the \ncore data structure used in the Count-Sketch, but, instead of sketching counts, \nit captures sketched gradient updates to the model parameters. We provide a \ntheoretical analysis of this approach that establishes recovery guarantees in \nthe online learning setting, and demonstrate substantial empirical improvements \nin accuracy-memory trade-offs over alternatives, including count-based sketches \nand feature hashing. \n</p>"}, "author": "Kai Sheng Tai, Vatsal Sharan, Peter Bailis, Gregory Valiant", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510280106271", "timestampUsec": "1510280106270932", "id": "tag:google.com,2005:reader/item/000000032fb7308b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Deep Learning for Real-time Gravitational Wave Detection and Parameter Estimation: Results with Advanced LIGO Data. (arXiv:1711.03121v1 [gr-qc])", "published": 1510280107, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03121"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03121", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The recent Nobel-prize-winning detections of gravitational waves from merging \nblack holes and the subsequent detection of the collision of two neutron stars \nin coincidence with electromagnetic observations have inaugurated a new era of \nmultimessenger astrophysics. To enhance the scope of this emergent field of \nscience, we pioneered the use of deep learning with convolutional neural \nnetworks, that take time-series inputs, for rapid detection and \ncharacterization of gravitational wave signals. This approach, Deep Filtering, \nwas initially demonstrated using simulated LIGO noise. In this article, we \npresent the extension of Deep Filtering using real data from LIGO, for both \ndetection and parameter estimation of gravitational waves from binary black \nhole mergers using continuous data streams from multiple LIGO detectors. We \ndemonstrate for the first time that machine learning can detect and estimate \nthe true parameters of real events observed by LIGO. Our results show that Deep \nFiltering achieves similar sensitivities and lower errors compared to \nmatched-filtering while being far more computationally efficient and more \nresilient to glitches, allowing real-time processing of weak time-series \nsignals in non-stationary non-Gaussian noise with minimal resources, and also \nenables the detection of new classes of gravitational wave sources that may go \nunnoticed with existing detection algorithms. This unified framework for data \nanalysis is ideally suited to enable coincident detection campaigns of \ngravitational waves and their multimessenger counterparts in real-time. \n</p>"}, "author": "Daniel George, E. A. Huerta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510280106271", "timestampUsec": "1510280106270931", "id": "tag:google.com,2005:reader/item/000000032fb7308f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "MarrNet: 3D Shape Reconstruction via 2.5D Sketches. (arXiv:1711.03129v1 [cs.CV])", "published": 1510280107, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03129"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03129", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>3D object reconstruction from a single image is a highly under-determined \nproblem, requiring strong prior knowledge of plausible 3D shapes. This \nintroduces challenges for learning-based approaches, as 3D object annotations \nare scarce in real images. Previous work chose to train on synthetic data with \nground truth 3D information, but suffered from domain adaptation when tested on \nreal data. In this work, we propose MarrNet, an end-to-end trainable model that \nsequentially estimates 2.5D sketches and 3D object shape. Our disentangled, \ntwo-step formulation has three advantages. First, compared to full 3D shape, \n2.5D sketches are much easier to be recovered from a 2D image; models that \nrecover 2.5D sketches are also more likely to transfer from synthetic to real \ndata. Second, for 3D reconstruction from 2.5D sketches, systems can learn \npurely from synthetic data. This is because we can easily render realistic 2.5D \nsketches without modeling object appearance variations in real images, \nincluding lighting, texture, etc. This further relieves the domain adaptation \nproblem. Third, we derive differentiable projective functions from 3D shape to \n2.5D sketches; the framework is therefore end-to-end trainable on real images, \nrequiring no human annotations. Our model achieves state-of-the-art performance \non 3D shape reconstruction. \n</p>"}, "author": "Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, William T Freeman, Joshua B Tenenbaum", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510280106271", "timestampUsec": "1510280106270930", "id": "tag:google.com,2005:reader/item/000000032fb7309a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep D-bar: Real time Electrical Impedance Tomography Imaging with Deep Neural Networks. (arXiv:1711.03180v1 [math.NA])", "published": 1510280107, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03180"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03180", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The mathematical problem for Electrical Impedance Tomography (EIT) is a \nhighly nonlinear ill-posed inverse problem requiring carefully designed \nreconstruction procedures to ensure reliable image generation. D-bar methods \nare based on a rigorous mathematical analysis and provide robust direct \nreconstructions by using a low-pass filtering of the associated nonlinear \nFourier data. Similarly to low-pass filtering of linear Fourier data, only \nusing low frequencies in the image recovery process results in blurred images \nlacking sharp features such as clear organ boundaries. Convolutional Neural \nNetworks provide a powerful framework for post-processing such convolved direct \nreconstructions. In this study, we demonstrate that these CNN techniques lead \nto sharp and reliable reconstructions even for the highly nonlinear inverse \nproblem of EIT. The network is trained on data sets of simulated examples and \nthen applied to experimental data without the need to perform an additional \ntransfer training. Results are presented on experimental EIT data from the ACT4 \nand KIT4 EIT systems. \n</p>"}, "author": "Sarah Jane Hamilton, Andreas Hauptmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510280106271", "timestampUsec": "1510280106270929", "id": "tag:google.com,2005:reader/item/000000032fb7309e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz. (arXiv:1711.03357v1 [cs.NE])", "published": 1510280107, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03357"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03357", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7d7ee74\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7d7ee74&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The goal of this paper is to demonstrate a method for tensorizing neural \nnetworks based upon an efficient way of approximating scale invariant quantum \nstates, the Multi-scale Entanglement Renormalization Ansatz (MERA). We employ \nMERA as a replacement for linear layers in a neural network and test this \nimplementation on the CIFAR-10 dataset. The proposed method outperforms \nfactorization using tensor trains, providing greater compression for the same \nlevel of accuracy and greater accuracy for the same level of compression. We \ndemonstrate MERA-layers with 3900 times fewer parameters and a reduction in \naccuracy of less than 1% compared to the equivalent fully connected layers. \n</p>"}, "author": "Andrew Hallam, Edward Grant, Vid Stojevic, Simone Severini, Andrew G. Green", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510280106271", "timestampUsec": "1510280106270928", "id": "tag:google.com,2005:reader/item/000000032fb730a6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Further Analysis of The Role of Heterogeneity in Coevolutionary Spatial Games. (arXiv:1711.03417v1 [physics.soc-ph])", "published": 1510280107, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03417"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03417", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7e4a85c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7e4a85c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Heterogeneity has been studied as one of the most common explanations of the \npuzzle of cooperation in social dilemmas. A large number of papers have been \npublished discussing the effects of increasing heterogeneity in structured \npopulations of agents, where it has been established that heterogeneity may \nfavour cooperative behaviour if it supports agents to locally coordinate their \nstrategies. In this paper, assuming an existing model of a heterogeneous \nweighted network, we aim to further this analysis by exploring the relationship \n(if any) between heterogeneity and cooperation. We adopt a weighted network \nwhich is fully populated by agents playing both the Prisoner's Dilemma or the \nOptional Prisoner's Dilemma games with coevolutionary rules, i.e., not only the \nstrategies but also the link weights evolve over time. Surprisingly, results \nshow that the heterogeneity of link weights (states) on their own does not \nalways promote cooperation; rather cooperation is actually favoured by the \nincrease in the number of overlapping states and not by the heterogeneity \nitself. We believe that these results can guide further research towards a more \naccurate analysis of the role of heterogeneity in social dilemmas. \n</p>"}, "author": "Marcos Cardinot, Josephine Griffith, Colm O&#x27;Riordan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510280106271", "timestampUsec": "1510280106270927", "id": "tag:google.com,2005:reader/item/000000032fb730ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Worm-level Control through Search-based Reinforcement Learning. (arXiv:1711.03467v1 [cs.NE])", "published": 1510280107, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03467"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03467", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Through natural evolution, nervous systems of organisms formed near-optimal \nstructures to express behavior. Here, we propose an effective way to create \ncontrol agents, by \\textit{re-purposing} the function of biological neural \ncircuit models, to govern similar real world applications. We model the \ntap-withdrawal (TW) neural circuit of the nematode, \\textit{C. elegans}, a \ncircuit responsible for the worm's reflexive response to external mechanical \ntouch stimulations, and learn its synaptic and neural parameters as a policy \nfor controlling the inverted pendulum problem. For reconfiguration of the \npurpose of the TW neural circuit, we manipulate a search-based reinforcement \nlearning. We show that our neural policy performs as good as existing \ntraditional control theory and machine learning approaches. A video \ndemonstration of the performance of our method can be accessed at \n\\url{https://youtu.be/o-Ia5IVyff8}. \n</p>"}, "author": "Mathias Lechner, Radu Grosu, Ramin M. Hasani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605507", "id": "tag:google.com,2005:reader/item/000000032f758eca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Recurrent Autoregressive Networks for Online Multi-Object Tracking. (arXiv:1711.02741v1 [cs.CV])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02741"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02741", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The main challenge of online multi-object tracking is to reliably associate \nobject trajectories with detections in each video frame based on their tracking \nhistory. In this work, we propose the Recurrent Autoregressive Network (RAN), a \ntemporal generative modeling framework to characterize the appearance and \nmotion dynamics of multiple objects over time. The RAN couples an external \nmemory and an internal memory. The external memory explicitly stores previous \ninputs of each trajectory in a time window, while the internal memory learns to \nsummarize long-term tracking history and associate detections by processing the \nexternal memory. We conduct experiments on the MOT 2015 and 2016 datasets to \ndemonstrate the robustness of our tracking method in highly crowded and \noccluded scenes. Our method achieves top-ranked results on the two benchmarks. \n</p>"}, "author": "Kuan Fang, Yu Xiang, Silvio Savarese", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605506", "id": "tag:google.com,2005:reader/item/000000032f758ee9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Block-Sparse Recurrent Neural Networks. (arXiv:1711.02782v1 [cs.LG])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02782"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02782", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent Neural Networks (RNNs) are used in state-of-the-art models in \ndomains such as speech recognition, machine translation, and language \nmodelling. Sparsity is a technique to reduce compute and memory requirements of \ndeep learning models. Sparse RNNs are easier to deploy on devices and high-end \nserver processors. Even though sparse operations need less compute and memory \nrelative to their dense counterparts, the speed-up observed by using sparse \noperations is less than expected on different hardware platforms. In order to \naddress this issue, we investigate two different approaches to induce block \nsparsity in RNNs: pruning blocks of weights in a layer and using group lasso \nregularization to create blocks of weights with zeros. Using these techniques, \nwe demonstrate that we can create block-sparse RNNs with sparsity ranging from \n80% to 90% with small loss in accuracy. This allows us to reduce the model size \nby roughly 10x. Additionally, we can prune a larger dense network to recover \nthis loss in accuracy while maintaining high block sparsity and reducing the \noverall parameter count. Our technique works with a variety of block sizes up \nto 32x32. Block-sparse RNNs eliminate overheads related to data storage and \nirregular memory accesses while increasing hardware efficiency compared to \nunstructured sparsity. \n</p>"}, "author": "Sharan Narang, Eric Undersander, Gregory Diamos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605505", "id": "tag:google.com,2005:reader/item/000000032f758ef9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Faster Fuzzing: Reinitialization with Deep Neural Models. (arXiv:1711.02807v1 [cs.AI])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02807"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02807", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We improve the performance of the American Fuzzy Lop (AFL) fuzz testing \nframework by using Generative Adversarial Network (GAN) models to reinitialize \nthe system with novel seed files. We assess performance based on the temporal \nrate at which we produce novel and unseen code paths. We compare this approach \nto seed file generation from a random draw of bytes observed in the training \nseed files. The code path lengths and variations were not sufficiently diverse \nto fully replace AFL input generation. However, augmenting native AFL with \nthese additional code paths demonstrated improvements over AFL alone. \nSpecifically, experiments showed the GAN was faster and more effective than the \nLSTM and out-performed a random augmentation strategy, as measured by the \nnumber of unique code paths discovered. GAN helps AFL discover 14.23% more code \npaths than the random strategy in the same amount of CPU time, finds 6.16% more \nunique code paths, and finds paths that are on average 13.84% longer. Using GAN \nshows promise as a reinitialization strategy for AFL to help the fuzzer \nexercise deep paths in software. \n</p>"}, "author": "Nicole Nichols, Mark Raugas, Robert Jasper, Nathan Hilliard", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605504", "id": "tag:google.com,2005:reader/item/000000032f758f12", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Inverse Reward Design. (arXiv:1711.02827v1 [cs.AI])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02827"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02827", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Autonomous agents optimize the reward function we give them. What they don't \nknow is how hard it is for us to design a reward function that actually \ncaptures what we want. When designing the reward, we might think of some \nspecific training scenarios, and make sure that the reward will lead to the \nright behavior in those scenarios. Inevitably, agents encounter new scenarios \n(e.g., new types of terrain) where optimizing that same reward may lead to \nundesired behavior. Our insight is that reward functions are merely \nobservations about what the designer actually wants, and that they should be \ninterpreted in the context in which they were designed. We introduce inverse \nreward design (IRD) as the problem of inferring the true objective based on the \ndesigned reward and the training MDP. We introduce approximate methods for \nsolving IRD problems, and use their solution to plan risk-averse behavior in \ntest MDPs. Empirical results suggest that this approach can help alleviate \nnegative side effects of misspecified reward functions and mitigate reward \nhacking. \n</p>"}, "author": "Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, Anca Dragan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605503", "id": "tag:google.com,2005:reader/item/000000032f758f2b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SIMILARnet: Simultaneous Intelligent Localization and Recognition Network. (arXiv:1711.02831v1 [cs.CV])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02831"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02831", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Global Average Pooling (GAP) [4] has been used previously to generate class \nactivation for image classification tasks. The motivation behind SIMILARnet \ncomes from the fact that the convolutional filters possess position information \nof the essential features and hence, combination of the feature maps could help \nus locate the class instances in an image. We propose a biologically inspired \nmodel that is free of differential connections and doesn't require separate \ntraining thereby reducing computation overhead. Our novel architecture \ngenerates promising results and unlike existing methods, the model is not \nsensitive to the input image size, thus promising wider application. Codes for \nthe experiment and illustrations can be found at: \nhttps://github.com/brcsomnath/Advanced-GAP . \n</p>"}, "author": "Arna Ghosh, Biswarup Bhattacharya, Somnath Basu Roy Chowdhury", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605502", "id": "tag:google.com,2005:reader/item/000000032f758f3f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Sparse Visual Representations with Leaky Capped Norm Regularizers. (arXiv:1711.02857v1 [cs.LG])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02857"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02857", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sparsity inducing regularization is an important part for learning \nover-complete visual representations. Despite the popularity of $\\ell_1$ \nregularization, in this paper, we investigate the usage of non-convex \nregularizations in this problem. Our contribution consists of three parts. \nFirst, we propose the leaky capped norm regularization (LCNR), which allows \nmodel weights below a certain threshold to be regularized more strongly as \nopposed to those above, therefore imposes strong sparsity and only introduces \ncontrollable estimation bias. We propose a majorization-minimization algorithm \nto optimize the joint objective function. Second, our study over monocular 3D \nshape recovery and neural networks with LCNR outperforms $\\ell_1$ and other \nnon-convex regularizations, achieving state-of-the-art performance and faster \nconvergence. Third, we prove a theoretical global convergence speed on the 3D \nrecovery problem. To the best of our knowledge, this is the first convergence \nanalysis of the 3D recovery problem. \n</p>"}, "author": "Jianqiao Wangni, Dahua Lin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605501", "id": "tag:google.com,2005:reader/item/000000032f758f4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Un r\\'esultat intrigant en commande sans mod\\`ele. (arXiv:1711.02877v1 [cs.SY])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02877"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02877", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>An elementary mathematical example proves, thanks to the Routh-Hurwitz \ncriterion, a result that is intriguing with respect to today's practical \nunderstanding of model-free control, i.e., an \"intelligent\" proportional \ncontroller (iP) may turn to be more difficult to tune than an intelligent \nproportional-derivative one (iPD). The vast superiority of iPDs when compared \nto classic PIDs is shown via computer simulations. The introduction as well as \nthe conclusion analyse model-free control in the light of recent advances. \n</p>"}, "author": "C&#xe9;dric Join, Emmanuel Delaleau, Michel Fliess, Claude H. Moog", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605500", "id": "tag:google.com,2005:reader/item/000000032f758f5f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Clustering with feature selection using alternating minimization, Application to computational biology. (arXiv:1711.02974v1 [cs.LG])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02974"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02974", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7e4aad9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7e4aad9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper deals with unsupervised clustering with feature selection. The \nproblem is to estimate both labels and a sparse projection matrix of weights. \nTo address this combinatorial non-convex problem maintaining a strict control \non the sparsity of the matrix of weights, we propose an alternating \nminimization of the Frobenius norm criterion. We provide a new efficient \nalgorithm named K-sparse which alternates k-means with projection-gradient \nminimization. The projection-gradient step is a method of splitting type, with \nexact projection on the $\\ell^1$ ball to promote sparsity. The convergence of \nthe gradient-projection step is addressed, and a preliminary analysis of the \nalternating minimization is made. The Frobenius norm criterion converges as the \nnumber of iterates in Algorithm K-sparse goes to infinity. Experiments on \nSingle Cell RNA sequencing datasets show that our method significantly improves \nthe results of PCA k-means, spectral clustering, SIMLR, and Sparcl methods, and \nachieves a relevant selection of genes. The complexity of K-sparse is linear in \nthe number of samples (cells), so that the method scales up to large datasets. \n</p>"}, "author": "Cyprien Gilet, Marie Deprez, Jean-Baptiste Caillau, Michel Barlaud", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605499", "id": "tag:google.com,2005:reader/item/000000032f758f67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Intelligent Fault Analysis in Electrical Power Grids. (arXiv:1711.03026v1 [cs.CE])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03026"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03026", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Power grids are one of the most important components of infrastructure in \ntoday's world. Every nation is dependent on the security and stability of its \nown power grid to provide electricity to the households and industries. A \nmalfunction of even a small part of a power grid can cause loss of \nproductivity, revenue and in some cases even life. Thus, it is imperative to \ndesign a system which can detect the health of the power grid and take \nprotective measures accordingly even before a serious anomaly takes place. To \nachieve this objective, we have set out to create an artificially intelligent \nsystem which can analyze the grid information at any given time and determine \nthe health of the grid through the usage of sophisticated formal models and \nnovel machine learning techniques like recurrent neural networks. Our system \nsimulates grid conditions including stimuli like faults, generator output \nfluctuations, load fluctuations using Siemens PSS/E software and this data is \ntrained using various classifiers like SVM, LSTM and subsequently tested. The \nresults are excellent with our methods giving very high accuracy for the data. \nThis model can easily be scaled to handle larger and more complex grid \narchitectures. \n</p>"}, "author": "Biswarup Bhattacharya, Abhishek Sinha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605498", "id": "tag:google.com,2005:reader/item/000000032f758f70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning K-way D-dimensional Discrete Code For Compact Embedding Representations. (arXiv:1711.03067v1 [cs.LG])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03067"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03067", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Embedding methods such as word embedding have become pillars for many \napplications containing discrete structures. Conventional embedding methods \ndirectly associate each symbol with a continuous embedding vector, which is \nequivalent to applying linear transformation based on \"one-hot\" encoding of the \ndiscrete symbols. Despite its simplicity, such approach yields number of \nparameters that grows linearly with the vocabulary size and can lead to \noverfitting. In this work we propose a much more compact K-way D-dimensional \ndiscrete encoding scheme to replace the \"one-hot\" encoding. In \"KD encoding\", \neach symbol is represented by a $D$-dimensional code, and each of its dimension \nhas a cardinality of $K$. The final symbol embedding vector can be generated by \ncomposing the code embedding vectors. To learn the semantically meaningful \ncode, we derive a relaxed discrete optimization technique based on stochastic \ngradient descent. By adopting the new coding system, the efficiency of \nparameterization can be significantly improved (from linear to logarithmic), \nand this can also mitigate the over-fitting problem. In our experiments with \nlanguage modeling, the number of embedding parameters can be reduced by 97\\% \nwhile achieving similar or better performance. \n</p>"}, "author": "Ting Chen, Martin Renqiang Min, Yizhou Sun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510247283606", "timestampUsec": "1510247283605497", "id": "tag:google.com,2005:reader/item/000000032f758f85", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exploration in NetHack with Secret Discovery. (arXiv:1711.03087v1 [cs.AI])", "published": 1510247284, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03087"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03087", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Roguelike games generally feature exploration problems as a critical, yet \noften repetitive element of gameplay. Automated approaches, however, face \nchallenges in terms of optimality, as well as due to incomplete information, \nsuch as from the presence of secret doors. This paper presents an algorithmic \napproach to exploration of roguelike dungeon environments. Our design aims to \nminimize exploration time, balancing coverage and discovery of secret areas \nwith resource cost. Our algorithm is based on the concept of occupancy maps \npopular in robotics, adapted to encourage efficient discovery of secret access \npoints. Through extensive experimentation on NetHack maps we show that this \ntechnique is significantly more efficient than simpler greedy approaches. We \nfurther investigate optimized parameterization for the algorithm through a \ncomprehensive data analysis. These results point towards better automation for \nplayers as well as heuristics applicable to fully automated gameplay. \n</p>"}, "author": "Jonathan C. Campbell (1), Clark Verbrugge (1) ((1) McGill University)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885321", "id": "tag:google.com,2005:reader/item/000000032f482b5a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neural Variational Inference and Learning in Undirected Graphical Models. (arXiv:1711.02679v2 [cs.LG] UPDATED)", "published": 1511218849, "updated": 1511218854, "canonical": [{"href": "http://arxiv.org/abs/1711.02679"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02679", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many problems in machine learning are naturally expressed in the language of \nundirected graphical models. Here, we propose black-box learning and inference \nalgorithms for undirected models that optimize a variational approximation to \nthe log-likelihood of the model. Central to our approach is an upper bound on \nthe log-partition function parametrized by a function q that we express as a \nflexible neural network. Our bound makes it possible to track the partition \nfunction during learning, to speed-up sampling, and to train a broad class of \nhybrid directed/undirected models via a unified variational inference \nframework. We empirically demonstrate the effectiveness of our method on \nseveral popular generative modeling datasets. \n</p>"}, "author": "Volodymyr Kuleshov, Stefano Ermon", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885320", "id": "tag:google.com,2005:reader/item/000000032f482b72", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tangent: Automatic Differentiation Using Source Code Transformation in Python. (arXiv:1711.02712v1 [cs.MS])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02712"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02712", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Automatic differentiation (AD) is an essential primitive for machine learning \nprogramming systems. Tangent is a new library that performs AD using source \ncode transformation (SCT) in Python. It takes numeric functions written in a \nsyntactic subset of Python and NumPy as input, and generates new Python \nfunctions which calculate a derivative. This approach to automatic \ndifferentiation is different from existing packages popular in machine \nlearning, such as TensorFlow and Autograd. Advantages are that Tangent \ngenerates gradient code in Python which is readable by the user, easy to \nunderstand and debug, and has no runtime overhead. Tangent also introduces \nabstractions for easily injecting logic into the generated gradient code, \nfurther improving usability. \n</p>"}, "author": "Bart van Merri&#xeb;nboer, Alexander B. Wiltschko, Dan Moldovan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885319", "id": "tag:google.com,2005:reader/item/000000032f482bbc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Discrimination-Generalization Tradeoff in GANs. (arXiv:1711.02771v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02771"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02771", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative adversarial training can be generally understood as minimizing \ncertain moment matching loss defined by a set of discriminator functions, \ntypically neural networks. The discriminator set should be large enough to be \nable to uniquely identify the true distribution (discriminative), and also be \nsmall enough to go beyond memorizing samples (generalizable). In this paper, we \nshow that a discriminator set is guaranteed to be discriminative whenever its \nlinear span is dense in the set of bounded continuous functions. This is a very \nmild condition satisfied even by neural networks with a single neuron. Further, \nwe develop generalization bounds between the learned distribution and true \ndistribution under different evaluation metrics. When evaluated with neural or \nWasserstein distances, our bounds show that generalization is guaranteed as \nlong as the discriminator set is small enough, regardless of the size of the \ngenerator or hypothesis set. When evaluated with KL divergence, our bound \nprovides an explanation on the counter-intuitive behaviors of testing \nlikelihood in GAN training. Our analysis sheds lights on understanding the \npractical performance of GANs. \n</p>"}, "author": "Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, Xiaodong He", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885318", "id": "tag:google.com,2005:reader/item/000000032f482c21", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Block-Sparse Recurrent Neural Networks. (arXiv:1711.02782v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02782"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02782", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent Neural Networks (RNNs) are used in state-of-the-art models in \ndomains such as speech recognition, machine translation, and language \nmodelling. Sparsity is a technique to reduce compute and memory requirements of \ndeep learning models. Sparse RNNs are easier to deploy on devices and high-end \nserver processors. Even though sparse operations need less compute and memory \nrelative to their dense counterparts, the speed-up observed by using sparse \noperations is less than expected on different hardware platforms. In order to \naddress this issue, we investigate two different approaches to induce block \nsparsity in RNNs: pruning blocks of weights in a layer and using group lasso \nregularization to create blocks of weights with zeros. Using these techniques, \nwe demonstrate that we can create block-sparse RNNs with sparsity ranging from \n80% to 90% with small loss in accuracy. This allows us to reduce the model size \nby roughly 10x. Additionally, we can prune a larger dense network to recover \nthis loss in accuracy while maintaining high block sparsity and reducing the \noverall parameter count. Our technique works with a variety of block sizes up \nto 32x32. Block-sparse RNNs eliminate overheads related to data storage and \nirregular memory accesses while increasing hardware efficiency compared to \nunstructured sparsity. \n</p>"}, "author": "Sharan Narang, Eric Undersander, Gregory Diamos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885317", "id": "tag:google.com,2005:reader/item/000000032f482c6d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Approximate message passing for nonconvex sparse regularization with stability and asymptotic analysis. (arXiv:1711.02795v2 [stat.ML] UPDATED)", "published": 1511308877, "updated": 1511308893, "canonical": [{"href": "http://arxiv.org/abs/1711.02795"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02795", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We analyze linear regression problem with a nonconvex regularization called \nsmoothly clipped absolute deviation (SCAD) under overcomplete Gaussian basis \nfor Gaussian random data. We develop a message passing algorithm SCAD-AMP and \nanalytically show that the stability condition is corresponding to the AT \ncondition in spin glass literature. As asymptotic analysis, we show the \ncorrespondence between density evolution of SCAD-AMP and replica symmetric \nsolution. Numerical experiments confirm that for sufficiently large system \nsize, SCAD-AMP achieves the optimal performance predicted by replica method. \nFrom replica analysis, phase transition between replica symmetric (RS) and \nreplica symmetry breaking (RSB) region is found in the parameter space of SCAD. \nThe appearance of RS region for nonconvex penalty is a great advantage which \nindicate the region of smooth landscape of the optimization problem. \nFurthermore, we analytically show that the statistical representation \nperformance of SCAD penalty is improved compared with $\\ell_1$-based methods, \nand the minimum representation error under RS assumption is obtained at the \nedge of RS/RSB phase. The correspondence between the convergence of the \nexisting coordinate descent algorithm and RS/RSB transition is also indicated. \n</p>"}, "author": "Ayaka Sakata, Yingying Xu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885316", "id": "tag:google.com,2005:reader/item/000000032f482c85", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Fault Analysis and Subset Selection in Solar Power Grids. (arXiv:1711.02810v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02810"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02810", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Non-availability of reliable and sustainable electric power is a major \nproblem in the developing world. Renewable energy sources like solar are not \nvery lucrative in the current stage due to various uncertainties like weather, \nstorage, land use among others. There also exists various other issues like \nmis-commitment of power, absence of intelligent fault analysis, congestion, \netc. In this paper, we propose a novel deep learning-based system for \npredicting faults and selecting power generators optimally so as to reduce \ncosts and ensure higher reliability in solar power systems. The results are \nhighly encouraging and they suggest that the approaches proposed in this paper \nhave the potential to be applied successfully in the developing world. \n</p>"}, "author": "Biswarup Bhattacharya, Abhishek Sinha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885315", "id": "tag:google.com,2005:reader/item/000000032f482cab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Cubic Regularization for Fast Nonconvex Optimization. (arXiv:1711.02838v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02838"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02838", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7e4acf3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7e4acf3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper proposes a stochastic variant of a classic algorithm---the \ncubic-regularized Newton method [Nesterov and Polyak 2006]. The proposed \nalgorithm efficiently escapes saddle points and finds approximate local minima \nfor general smooth, nonconvex functions in only \n$\\mathcal{\\tilde{O}}(\\epsilon^{-3.5})$ stochastic gradient and stochastic \nHessian-vector product evaluations. The latter can be computed as efficiently \nas stochastic gradients. This improves upon the \n$\\mathcal{\\tilde{O}}(\\epsilon^{-4})$ rate of stochastic gradient descent. Our \nrate matches the best-known result for finding local minima without requiring \nany delicate acceleration or variance-reduction techniques. \n</p>"}, "author": "Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, Michael I. Jordan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885314", "id": "tag:google.com,2005:reader/item/000000032f482d5a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Intriguing Properties of Adversarial Examples. (arXiv:1711.02846v1 [stat.ML])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02846"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02846", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7ee2cb1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7ee2cb1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>It is becoming increasingly clear that many machine learning classifiers are \nvulnerable to adversarial examples. In attempting to explain the origin of \nadversarial examples, previous studies have typically focused on the fact that \nneural networks operate on high dimensional data, they overfit, or they are too \nlinear. Here we argue that the origin of adversarial examples is primarily due \nto an inherent uncertainty that neural networks have about their predictions. \nWe show that the functional form of this uncertainty is independent of \narchitecture, dataset, and training protocol; and depends only on the \nstatistics of the logit differences of the network, which do not change \nsignificantly during training. This leads to adversarial error having a \nuniversal scaling, as a power-law, with respect to the size of the adversarial \nperturbation. We show that this universality holds for a broad range of \ndatasets (MNIST, CIFAR10, ImageNet, and random data), models (including \nstate-of-the-art deep networks, linear models, adversarially trained networks, \nand networks trained on randomly shuffled labels), and attacks (FGSM, step \nl.l., PGD). Motivated by these results, we study the effects of reducing \nprediction entropy on adversarial robustness. Finally, we study the effect of \nnetwork architectures on adversarial sensitivity. To do this, we use neural \narchitecture search with reinforcement learning to find adversarially robust \narchitectures on CIFAR10. Our resulting architecture is more robust to white \n\\emph{and} black box attacks compared to previous attempts. \n</p>"}, "author": "Ekin D. Cubuk, Barret Zoph, Samuel S. Schoenholz, Quoc V. Le", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885313", "id": "tag:google.com,2005:reader/item/000000032f482d71", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Sparse Visual Representations with Leaky Capped Norm Regularizers. (arXiv:1711.02857v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02857"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02857", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sparsity inducing regularization is an important part for learning \nover-complete visual representations. Despite the popularity of $\\ell_1$ \nregularization, in this paper, we investigate the usage of non-convex \nregularizations in this problem. Our contribution consists of three parts. \nFirst, we propose the leaky capped norm regularization (LCNR), which allows \nmodel weights below a certain threshold to be regularized more strongly as \nopposed to those above, therefore imposes strong sparsity and only introduces \ncontrollable estimation bias. We propose a majorization-minimization algorithm \nto optimize the joint objective function. Second, our study over monocular 3D \nshape recovery and neural networks with LCNR outperforms $\\ell_1$ and other \nnon-convex regularizations, achieving state-of-the-art performance and faster \nconvergence. Third, we prove a theoretical global convergence speed on the 3D \nrecovery problem. To the best of our knowledge, this is the first convergence \nanalysis of the 3D recovery problem. \n</p>"}, "author": "Jianqiao Wangni, Dahua Lin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885312", "id": "tag:google.com,2005:reader/item/000000032f482e02", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dimension Estimation Using Random Connection Models. (arXiv:1711.02876v1 [stat.ML])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02876"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02876", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Information about intrinsic dimension is crucial to perform dimensionality \nreduction, compress information, design efficient algorithms, and do \nstatistical adaptation. In this paper we propose an estimator for the intrinsic \ndimension of a data set. The estimator is based on binary neighbourhood \ninformation about the observations in the form of two adjacency matrices, and \ndoes not require any explicit distance information. The underlying graph is \nmodelled according to a subset of a specific random connection model, sometimes \nreferred to as the Poisson blob model. Computationally the estimator scales \nlike n log n, and we specify its asymptotic distribution and rate of \nconvergence. A simulation study on both real and simulated data shows that our \napproach compares favourably with some competing methods from the literature, \nincluding approaches that rely on distance information. \n</p>"}, "author": "Paulo Serra, Michel Mandjes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885311", "id": "tag:google.com,2005:reader/item/000000032f482e3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Universal consistency and minimax rates for online Mondrian Forests. (arXiv:1711.02887v1 [stat.ML])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02887"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02887", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We establish the consistency of an algorithm of Mondrian Forests, a \nrandomized classification algorithm that can be implemented online. First, we \namend the original Mondrian Forest algorithm, that considers a fixed lifetime \nparameter. Indeed, the fact that this parameter is fixed hinders the \nstatistical consistency of the original procedure. Our modified Mondrian Forest \nalgorithm grows trees with increasing lifetime parameters $\\lambda_n$, and uses \nan alternative updating rule, allowing to work also in an online fashion. \nSecond, we provide a theoretical analysis establishing simple conditions for \nconsistency. Our theoretical analysis also exhibits a surprising fact: our \nalgorithm achieves the minimax rate (optimal rate) for the estimation of a \nLipschitz regression function, which is a strong extension of previous results \nto an arbitrary dimension. \n</p>"}, "author": "Jaouad Mourtada, St&#xe9;phane Ga&#xef;ffas, Erwan Scornet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885310", "id": "tag:google.com,2005:reader/item/000000032f482e67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Clustering with feature selection using alternating minimization, Application to computational biology. (arXiv:1711.02974v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02974"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02974", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper deals with unsupervised clustering with feature selection. The \nproblem is to estimate both labels and a sparse projection matrix of weights. \nTo address this combinatorial non-convex problem maintaining a strict control \non the sparsity of the matrix of weights, we propose an alternating \nminimization of the Frobenius norm criterion. We provide a new efficient \nalgorithm named K-sparse which alternates k-means with projection-gradient \nminimization. The projection-gradient step is a method of splitting type, with \nexact projection on the $\\ell^1$ ball to promote sparsity. The convergence of \nthe gradient-projection step is addressed, and a preliminary analysis of the \nalternating minimization is made. The Frobenius norm criterion converges as the \nnumber of iterates in Algorithm K-sparse goes to infinity. Experiments on \nSingle Cell RNA sequencing datasets show that our method significantly improves \nthe results of PCA k-means, spectral clustering, SIMLR, and Sparcl methods, and \nachieves a relevant selection of genes. The complexity of K-sparse is linear in \nthe number of samples (cells), so that the method scales up to large datasets. \n</p>"}, "author": "Cyprien Gilet, Marie Deprez, Jean-Baptiste Caillau, Michel Barlaud", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885309", "id": "tag:google.com,2005:reader/item/000000032f482e91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variational Gaussian Dropout is not Bayesian. (arXiv:1711.02989v1 [stat.ML])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02989"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02989", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Gaussian multiplicative noise is commonly used as a stochastic regularisation \ntechnique in training of deterministic neural networks. A recent paper \nreinterpreted the technique as a specific algorithm for approximate inference \nin Bayesian neural networks; several extensions ensued. We show that the \nlog-uniform prior used in all the above publications does not generally induce \na proper posterior, and thus Bayesian inference in such models is ill-posed. \nIndependent of the log-uniform prior, the correlated weight noise approximation \nhas further issues leading to either infinite objective or high risk of \noverfitting. The above implies that the reported sparsity of obtained solutions \ncannot be explained by Bayesian or the related minimum description length \narguments. We thus study the objective from a non-Bayesian perspective, provide \nits previously unknown analytical form which allows exact gradient evaluation, \nand show that the later proposed additive reparametrisation introduces minima \nnot present in the original multiplicative parametrisation. Implications and \nfuture research directions are discussed. \n</p>"}, "author": "Jiri Hron, Alexander G. de G. Matthews, Zoubin Ghahramani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885308", "id": "tag:google.com,2005:reader/item/000000032f482eab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Intelligent Fault Analysis in Electrical Power Grids. (arXiv:1711.03026v1 [cs.CE])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03026"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03026", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Power grids are one of the most important components of infrastructure in \ntoday's world. Every nation is dependent on the security and stability of its \nown power grid to provide electricity to the households and industries. A \nmalfunction of even a small part of a power grid can cause loss of \nproductivity, revenue and in some cases even life. Thus, it is imperative to \ndesign a system which can detect the health of the power grid and take \nprotective measures accordingly even before a serious anomaly takes place. To \nachieve this objective, we have set out to create an artificially intelligent \nsystem which can analyze the grid information at any given time and determine \nthe health of the grid through the usage of sophisticated formal models and \nnovel machine learning techniques like recurrent neural networks. Our system \nsimulates grid conditions including stimuli like faults, generator output \nfluctuations, load fluctuations using Siemens PSS/E software and this data is \ntrained using various classifiers like SVM, LSTM and subsequently tested. The \nresults are excellent with our methods giving very high accuracy for the data. \nThis model can easily be scaled to handle larger and more complex grid \narchitectures. \n</p>"}, "author": "Biswarup Bhattacharya, Abhishek Sinha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885307", "id": "tag:google.com,2005:reader/item/000000032f482f0c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Recency-weighted Markovian inference. (arXiv:1711.03038v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03038"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03038", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe a Markov latent state space (MLSS) model, where the latent state \ndistribution is a decaying mixture over multiple past states. We present a \nsimple sampling algorithm that allows to approximate such high-order MLSS with \nfixed time and memory costs. \n</p>"}, "author": "Kristjan Kalm", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885306", "id": "tag:google.com,2005:reader/item/000000032f482f23", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Matrix-normal models for fMRI analysis. (arXiv:1711.03058v2 [stat.ML] UPDATED)", "published": 1510588781, "updated": 1510588789, "canonical": [{"href": "http://arxiv.org/abs/1711.03058"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03058", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multivariate analysis of fMRI data has benefited substantially from advances \nin machine learning. Most recently, a range of probabilistic latent variable \nmodels applied to fMRI data have been successful in a variety of tasks, \nincluding identifying similarity patterns in neural data (Representational \nSimilarity Analysis and its empirical Bayes variant, RSA and BRSA; Intersubject \nFunctional Connectivity, ISFC), combining multi-subject datasets (Shared \nResponse Mapping; SRM), and mapping between brain and behavior (Joint \nModeling). Although these methods share some underpinnings, they have been \ndeveloped as distinct methods, with distinct algorithms and software tools. We \nshow how the matrix-variate normal (MN) formalism can unify some of these \nmethods into a single framework. In doing so, we gain the ability to reuse \nnoise modeling assumptions, algorithms, and code across models. Our primary \ntheoretical contribution shows how some of these methods can be written as \ninstantiations of the same model, allowing us to generalize them to flexibly \nmodeling structured noise covariances. Our formalism permits novel model \nvariants and improved estimation strategies: in contrast to SRM, the number of \nparameters for MN-SRM does not scale with the number of voxels or subjects; in \ncontrast to BRSA, the number of parameters for MN-RSA scales additively rather \nthan multiplicatively in the number of voxels. We empirically demonstrate \nadvantages of two new methods derived in the formalism: for MN-RSA, we show up \nto 10x improvement in runtime, up to 6x improvement in RMSE, and more \nconservative behavior under the null. For MN-SRM, our method grants a modest \nimprovement to out-of-sample reconstruction while relaxing an orthonormality \nconstraint of SRM. We also provide a software prototyping tool for MN models \nthat can flexibly reuse noise covariance assumptions and algorithms across \nmodels. \n</p>"}, "author": "Michael Shvartsman, Narayanan Sundaram, Mikio C. Aoi, Adam Charles, Theodore C. Wilke, Jonathan D. Cohen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885305", "id": "tag:google.com,2005:reader/item/000000032f482f66", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning K-way D-dimensional Discrete Code For Compact Embedding Representations. (arXiv:1711.03067v1 [cs.LG])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03067"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03067", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7ee2f36\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7ee2f36&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Embedding methods such as word embedding have become pillars for many \napplications containing discrete structures. Conventional embedding methods \ndirectly associate each symbol with a continuous embedding vector, which is \nequivalent to applying linear transformation based on \"one-hot\" encoding of the \ndiscrete symbols. Despite its simplicity, such approach yields number of \nparameters that grows linearly with the vocabulary size and can lead to \noverfitting. In this work we propose a much more compact K-way D-dimensional \ndiscrete encoding scheme to replace the \"one-hot\" encoding. In \"KD encoding\", \neach symbol is represented by a $D$-dimensional code, and each of its dimension \nhas a cardinality of $K$. The final symbol embedding vector can be generated by \ncomposing the code embedding vectors. To learn the semantically meaningful \ncode, we derive a relaxed discrete optimization technique based on stochastic \ngradient descent. By adopting the new coding system, the efficiency of \nparameterization can be significantly improved (from linear to logarithmic), \nand this can also mitigate the over-fitting problem. In our experiments with \nlanguage modeling, the number of embedding parameters can be reduced by 97\\% \nwhile achieving similar or better performance. \n</p>"}, "author": "Ting Chen, Martin Renqiang Min, Yizhou Sun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885304", "id": "tag:google.com,2005:reader/item/000000032f482f9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Lower bounds over Boolean inputs for deep neural networks with ReLU gates. (arXiv:1711.03073v1 [cs.CC])", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03073"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03073", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Motivated by the resurgence of neural networks in being able to solve complex \nlearning tasks we undertake a study of high depth networks using ReLU gates \nwhich implement the function $x \\mapsto \\max\\{0,x\\}$. We try to understand the \nrole of depth in such neural networks by showing size lowerbounds against such \nnetwork architectures in parameter regimes hitherto unexplored. In particular \nwe show the following two main results about neural nets computing Boolean \nfunctions of input dimension $n$, \n</p> \n<p>1. We use the method of random restrictions to show almost linear, \n$\\Omega(\\epsilon^{2(1-\\delta)}n^{1-\\delta})$, lower bound for completely weight \nunrestricted LTF-of-ReLU circuits to match the Andreev function on at least \n$\\frac{1}{2} +\\epsilon$ fraction of the inputs for $\\epsilon &gt; \n\\sqrt{2\\frac{\\log^{\\frac {2}{2-\\delta}}(n)}{n}}$ for any $\\delta \\in (0,\\frac 1 \n2)$ \n</p> \n<p>2. We use the method of sign-rank to show exponential in dimension lower \nbounds for ReLU circuits ending in a LTF gate and of depths upto $O(n^{\\xi})$ \nwith $\\xi &lt; \\frac{1}{8}$ with some restrictions on the weights in the bottom \nmost layer. All other weights in these circuits are kept unrestricted. This in \nturns also implies the same lowerbounds for LTF circuits with the same \narchitecture and the same weight restrictions on their bottom most layer. \n</p> \n<p>Along the way we also show that there exists a $\\mathbb{R}^ n\\rightarrow \n\\mathbb{R}$ Sum-of-ReLU-of-ReLU function which Sum-of-ReLU neural nets can \nnever represent no matter how large they are allowed to be. \n</p>"}, "author": "Anirbit Mukherjee, Amitabh Basu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885282", "id": "tag:google.com,2005:reader/item/000000032f4831ab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Machine Learning Approach to RF Transmitter Identification. (arXiv:1711.01559v2 [eess.SP] CROSS LISTED)", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01559"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01559", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the development and widespread use of wireless devices in recent years \n(mobile phones, Internet of Things, Wi-Fi), the electromagnetic spectrum has \nbecome extremely crowded. In order to counter security threats posed by rogue \nor unknown transmitters, it is important to identify RF transmitters not by the \ndata content of the transmissions but based on the intrinsic physical \ncharacteristics of the transmitters. RF waveforms represent a particular \nchallenge because of the extremely high data rates involved and the potentially \nlarge number of transmitters present in a given location. These factors outline \nthe need for rapid fingerprinting and identification methods that go beyond the \ntraditional hand-engineered approaches. In this study, we investigate the use \nof machine learning (ML) strategies to the classification and identification \nproblems, and the use of wavelets to reduce the amount of data required. Four \ndifferent ML strategies are evaluated: deep neural nets (DNN), convolutional \nneural nets (CNN), support vector machines (SVM), and multi-stage training \n(MST) using accelerated Levenberg-Marquardt (A-LM) updates. The A-LM MST method \npreconditioned by wavelets was by far the most accurate, achieving 100% \nclassification accuracy of transmitters, as tested using data originating from \n12 different transmitters. We discuss strategies for extension of MST to a much \nlarger number of transmitters. \n</p>"}, "author": "K. Youssef, Louis-S. Bouchard, K.Z. Haigh, H. Krovi, J. Silovsky, C.P. Vander Valk", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510229028885", "timestampUsec": "1510229028885281", "id": "tag:google.com,2005:reader/item/000000032f4831ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "From Multimodal to Unimodal Webpages for Developing Countries. (arXiv:1711.02068v1 [cs.HC] CROSS LISTED)", "published": 1510229029, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02068"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02068", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The multimodal web elements such as text and images are associated with \ninherent memory costs to store and transfer over the Internet. With the limited \nnetwork connectivity in developing countries, webpage rendering gets delayed in \nthe presence of high-memory demanding elements such as images (relative to \ntext). To overcome this limitation, we propose a Canonical Correlation Analysis \n(CCA) based computational approach to replace high-cost modality with an \nequivalent low-cost modality. Our model learns a common subspace for low-cost \nand high-cost modalities that maximizes the correlation between their visual \nfeatures. The obtained common subspace is used for determining the low-cost \n(text) element of a given high-cost (image) element for the replacement. We \nanalyze the cost-saving performance of the proposed approach through an \neye-tracking experiment conducted on real-world webpages. Our approach reduces \nthe memory-cost by at least 83.35% by replacing images with text. \n</p>"}, "author": "Vidyapu Sandeep, V Vijaya Saradhi, Samit Bhattacharya", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510193354097", "timestampUsec": "1510193354096502", "id": "tag:google.com,2005:reader/item/000000032f0058c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Fidelity-Weighted Learning. (arXiv:1711.02799v1 [cs.LG])", "published": 1510193354, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02799"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02799", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Training deep neural networks requires many training samples, but in practice \ntraining labels are expensive to obtain and may be of varying quality, as some \nmay be from trusted expert labelers while others might be from heuristics or \nother sources of weak supervision such as crowd-sourcing. This creates a \nfundamental quality versus-quantity trade-off in the learning process. Do we \nlearn from the small amount of high-quality data or the potentially large \namount of weakly-labeled data? We argue that if the learner could somehow know \nand take the label-quality into account when learning the data representation, \nwe could get the best of both worlds. To this end, we propose \n\"fidelity-weighted learning\" (FWL), a semi-supervised student-teacher approach \nfor training deep neural networks using weakly-labeled data. FWL modulates the \nparameter updates to a student network (trained on the task we care about) on a \nper-sample basis according to the posterior confidence of its label-quality \nestimated by a teacher (who has access to the high-quality labels). Both \nstudent and teacher are learned from the data. We evaluate FWL on two tasks in \ninformation retrieval and natural language processing where we outperform \nstate-of-the-art alternative semi-supervised methods, indicating that our \napproach makes better use of strong and weak labels, and leads to better \ntask-dependent data representations. \n</p>"}, "author": "Mostafa Dehghani, Arash Mehrjou, Stephan Gouws, Jaap Kamps, Bernhard Sch&#xf6;lkopf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510193354097", "timestampUsec": "1510193354096501", "id": "tag:google.com,2005:reader/item/000000032f0058d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Traffic Prediction Based on Random Connectivity in Deep Learning with Long Short-Term Memory. (arXiv:1711.02833v1 [cs.NI])", "published": 1510193354, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02833"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02833", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Traffic prediction plays an important role in evaluating the performance of \ntelecommunication networks and attracts intense research interests. A \nsignificant number of algorithms and models have been proposed to learn \nknowledge from traffic data and improve the prediction accuracy. In the recent \nbig data era, the relevant research enthusiasm remains and deep learning has \nbeen exploited to extract the useful information in depth. In particular, Long \nShort-Term Memory (LSTM), one kind of Recurrent Neural Network (RNN) schemes, \nhas attracted significant attentions due to the long-range dependency embedded \nin the sequential traffic data. However, LSTM has considerable computational \ncost, which can not be tolerated in tasks with stringent latency requirement. \nIn this paper, we propose a deep learning model based on LSTM, called Random \nConnectivity LSTM (RCLSTM). Compared to the conventional LSTM, RCLSTM achieves \na significant breakthrough in the architecture formation of neural network, \nwhose connectivity is determined in a stochastic manner rather than full \nconnected. So, the neural network in RCLSTM can exhibit certain sparsity, which \nmeans many neural connections are absent (distinguished from the full \nconnectivity) and thus the number of parameters to be trained is reduced and \nmuch fewer computations are required. We apply the RCLSTM solution to predict \ntraffic and validate that the RCLSTM with even 35% neural connectivity still \nshows a strong capability in traffic prediction. Also, along with increasing \nthe number of training samples, the performance of RCLSTM becomes closer to the \nconventional LSTM. Moreover, the RCLSTM exhibits even superior prediction \naccuracy than the conventional LSTM when the length of input traffic sequences \nincreases. \n</p>"}, "author": "Yuxiu Hua, Zhifeng Zhao, Rongpeng Li, Xianfu Chen, Zhiming Liu, Honggang Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510193354097", "timestampUsec": "1510193354096500", "id": "tag:google.com,2005:reader/item/000000032f0058e4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Lower bounds over Boolean inputs for deep neural networks with ReLU gates. (arXiv:1711.03073v1 [cs.CC])", "published": 1510193354, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.03073"}], "alternate": [{"href": "http://arxiv.org/abs/1711.03073", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Motivated by the resurgence of neural networks in being able to solve complex \nlearning tasks we undertake a study of high depth networks using ReLU gates \nwhich implement the function $x \\mapsto \\max\\{0,x\\}$. We try to understand the \nrole of depth in such neural networks by showing size lowerbounds against such \nnetwork architectures in parameter regimes hitherto unexplored. In particular \nwe show the following two main results about neural nets computing Boolean \nfunctions of input dimension $n$, \n</p> \n<p>1. We use the method of random restrictions to show almost linear, \n$\\Omega(\\epsilon^{2(1-\\delta)}n^{1-\\delta})$, lower bound for completely weight \nunrestricted LTF-of-ReLU circuits to match the Andreev function on at least \n$\\frac{1}{2} +\\epsilon$ fraction of the inputs for $\\epsilon &gt; \n\\sqrt{2\\frac{\\log^{\\frac {2}{2-\\delta}}(n)}{n}}$ for any $\\delta \\in (0,\\frac 1 \n2)$ \n</p> \n<p>2. We use the method of sign-rank to show exponential in dimension lower \nbounds for ReLU circuits ending in a LTF gate and of depths upto $O(n^{\\xi})$ \nwith $\\xi &lt; \\frac{1}{8}$ with some restrictions on the weights in the bottom \nmost layer. All other weights in these circuits are kept unrestricted. This in \nturns also implies the same lowerbounds for LTF circuits with the same \narchitecture and the same weight restrictions on their bottom most layer. \n</p> \n<p>Along the way we also show that there exists a $\\mathbb{R}^ n\\rightarrow \n\\mathbb{R}$ Sum-of-ReLU-of-ReLU function which Sum-of-ReLU neural nets can \nnever represent no matter how large they are allowed to be. \n</p>"}, "author": "Anirbit Mukherjee, Amitabh Basu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510193354097", "timestampUsec": "1510193354096496", "id": "tag:google.com,2005:reader/item/000000032f0058f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Machine Learning Approach to RF Transmitter Identification. (arXiv:1711.01559v2 [eess.SP] CROSS LISTED)", "published": 1510193354, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01559"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01559", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the development and widespread use of wireless devices in recent years \n(mobile phones, Internet of Things, Wi-Fi), the electromagnetic spectrum has \nbecome extremely crowded. In order to counter security threats posed by rogue \nor unknown transmitters, it is important to identify RF transmitters not by the \ndata content of the transmissions but based on the intrinsic physical \ncharacteristics of the transmitters. RF waveforms represent a particular \nchallenge because of the extremely high data rates involved and the potentially \nlarge number of transmitters present in a given location. These factors outline \nthe need for rapid fingerprinting and identification methods that go beyond the \ntraditional hand-engineered approaches. In this study, we investigate the use \nof machine learning (ML) strategies to the classification and identification \nproblems, and the use of wavelets to reduce the amount of data required. Four \ndifferent ML strategies are evaluated: deep neural nets (DNN), convolutional \nneural nets (CNN), support vector machines (SVM), and multi-stage training \n(MST) using accelerated Levenberg-Marquardt (A-LM) updates. The A-LM MST method \npreconditioned by wavelets was by far the most accurate, achieving 100% \nclassification accuracy of transmitters, as tested using data originating from \n12 different transmitters. We discuss strategies for extension of MST to a much \nlarger number of transmitters. \n</p>"}, "author": "K. Youssef, Louis-S. Bouchard, K.Z. Haigh, H. Krovi, J. Silovsky, C.P. Vander Valk", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164296", "id": "tag:google.com,2005:reader/item/000000032eb5f0d3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bounding and Counting Linear Regions of Deep Neural Networks. (arXiv:1711.02114v1 [cs.LG])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02114"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02114", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study the representational power of deep neural networks \n(DNN) that belong to the family of piecewise-linear (PWL) functions, based on \nPWL activation units such as rectifier or maxout. We investigate the complexity \nof such networks by studying the number of linear regions of the PWL function. \nTypically, a PWL function from a DNN can be seen as a large family of linear \nfunctions acting on millions of such regions. We directly build upon the work \nof Montufar et al. (2014) and Raghu et al. (2017) by refining the upper and \nlower bounds on the number of linear regions for rectified and maxout networks. \nIn addition to achieving tighter bounds, we also develop a novel method to \nperform exact enumeration or counting of the number of linear regions with a \nmixed-integer linear formulation that maps the input space to output. We use \nthis new capability to visualize how the number of linear regions change while \ntraining DNNs. \n</p>"}, "author": "Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164295", "id": "tag:google.com,2005:reader/item/000000032eb5f0e1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Weighted Transformer Network for Machine Translation. (arXiv:1711.02132v1 [cs.AI])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02132"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02132", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>State-of-the-art results on neural machine translation often use attentional \nsequence-to-sequence models with some form of convolution or recursion. Vaswani \net al. (2017) propose a new architecture that avoids recurrence and convolution \ncompletely. Instead, it uses only self-attention and feed-forward layers. While \nthe proposed architecture achieves state-of-the-art results on several machine \ntranslation tasks, it requires a large number of parameters and training \niterations to converge. We propose Weighted Transformer, a Transformer with \nmodified attention layers, that not only outperforms the baseline network in \nBLEU score but also converges 15-40% faster. Specifically, we replace the \nmulti-head attention by multiple self-attention branches that the model learns \nto combine during the training process. Our model improves the state-of-the-art \nperformance by 0.5 BLEU points on the WMT 2014 English-to-German translation \ntask and by 0.4 on the English-to-French translation task. \n</p>"}, "author": "Karim Ahmed, Nitish Shirish Keskar, Richard Socher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164294", "id": "tag:google.com,2005:reader/item/000000032eb5f0ea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adaptive Bayesian Sampling with Monte Carlo EM. (arXiv:1711.02159v1 [cs.LG])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02159"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02159", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7ee317f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7ee317f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present a novel technique for learning the mass matrices in samplers \nobtained from discretized dynamics that preserve some energy function. Existing \nadaptive samplers use Riemannian preconditioning techniques, where the mass \nmatrices are functions of the parameters being sampled. This leads to \nsignificant complexities in the energy reformulations and resultant dynamics, \noften leading to implicit systems of equations and requiring inversion of \nhigh-dimensional matrices in the leapfrog steps. Our approach provides a \nsimpler alternative, by using existing dynamics in the sampling step of a Monte \nCarlo EM framework, and learning the mass matrices in the M step with a novel \nonline technique. We also propose a way to adaptively set the number of samples \ngathered in the E step, using sampling error estimates from the leapfrog \ndynamics. Along with a novel stochastic sampler based on Nos\\'{e}-Poincar\\'{e} \ndynamics, we use this framework with standard Hamiltonian Monte Carlo (HMC) as \nwell as newer stochastic algorithms such as SGHMC and SGNHT, and show strong \nperformance on synthetic and real high-dimensional sampling scenarios; we \nachieve sampling accuracies comparable to Riemannian samplers while being \nsignificantly faster. \n</p>"}, "author": "Anirban Roychowdhury, Srinivasan Parthasarathy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164293", "id": "tag:google.com,2005:reader/item/000000032eb5f0f2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Alpha-expansion is Exact on Stable Instances. (arXiv:1711.02195v1 [stat.ML])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02195"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02195", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7fb8e48\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7fb8e48&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Approximate algorithms for structured prediction problems---such as the \npopular alpha-expansion algorithm (Boykov et al. 2001) in computer \nvision---typically far exceed their theoretical performance guarantees on \nreal-world instances. These algorithms often find solutions that are very close \nto optimal. The goal of this paper is to partially explain the performance of \nalpha-expansion on MAP inference in Ferromagnetic Potts models (FPMs). Our main \nresults use the connection between energy minimization in FPMs and the Uniform \nMetric Labeling problem to give a stability condition under which the \nalpha-expansion algorithm provably recovers the optimal MAP solution. This \ntheoretical result complements the numerous empirical observations of \nalpha-expansion's performance. Additionally, we give a different stability \ncondition under which an LP-based algorithm recovers the optimal solution. \n</p>"}, "author": "Hunter Lang, David Sontag, Aravindan Vijayaraghavan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164292", "id": "tag:google.com,2005:reader/item/000000032eb5f100", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Visually-Aware Fashion Recommendation and Design with Generative Image Models. (arXiv:1711.02231v1 [cs.CV])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02231"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02231", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Building effective recommender systems for domains like fashion is \nchallenging due to the high level of subjectivity and the semantic complexity \nof the features involved (i.e., fashion styles). Recent work has shown that \napproaches to `visual' recommendation (e.g.~clothing, art, etc.) can be made \nmore accurate by incorporating visual signals directly into the recommendation \nobjective, using `off-the-shelf' feature representations derived from deep \nnetworks. Here, we seek to extend this contribution by showing that \nrecommendation performance can be significantly improved by learning `fashion \naware' image representations directly, i.e., by training the image \nrepresentation (from the pixel level) and the recommender system jointly; this \ncontribution is related to recent work using Siamese CNNs, though we are able \nto show improvements over state-of-the-art recommendation techniques such as \nBPR and variants that make use of pre-trained visual features. Furthermore, we \nshow that our model can be used \\emph{generatively}, i.e., given a user and a \nproduct category, we can generate new images (i.e., clothing items) that are \nmost consistent with their personal taste. This represents a first step towards \nbuilding systems that go beyond recommending existing items from a product \ncorpus, but which can be used to suggest styles and aid the design of new \nproducts. \n</p>"}, "author": "Wang-Cheng Kang, Chen Fang, Zhaowen Wang, Julian McAuley", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164291", "id": "tag:google.com,2005:reader/item/000000032eb5f106", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?. (arXiv:1711.02301v1 [cs.AI])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02301"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02301", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep reinforcement learning has achieved many recent successes, but our \nunderstanding of its strengths and limitations is hampered by the lack of rich \nenvironments in which we can fully characterize optimal behavior, and \ncorrespondingly diagnose individual actions against such a characterization. \nHere we consider a family of combinatorial games, arising from work of Erdos, \nSelfridge, and Spencer, and we propose their use as environments for evaluating \nand comparing different approaches to reinforcement learning. These games have \na number of appealing features: they are challenging for current learning \napproaches, but they form (i) a low-dimensional, simply parametrized \nenvironment where (ii) there is a linear closed form solution for optimal \nbehavior from any state, and (iii) the difficulty of the game can be tuned by \nchanging environment parameters in an interpretable way. We use these \nErdos-Selfridge-Spencer games not only to compare different algorithms, but \nalso to compare approaches based on supervised and reinforcement learning, to \nanalyze the power of multi-agent approaches in improving performance, and to \nevaluate generalization to environments outside the training set. \n</p>"}, "author": "Maithra Raghu, Alex Irpan, Jacob Andreas, Robert Kleinberg, Quoc V. Le, Jon Kleinberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164290", "id": "tag:google.com,2005:reader/item/000000032eb5f109", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Overcomplete HMMs. (arXiv:1711.02309v1 [cs.LG])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02309"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02309", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of learning overcomplete HMMs---those that have many \nhidden states but a small output alphabet. Despite having significant practical \nimportance, such HMMs are poorly understood with no known positive or negative \nresults for efficient learning. In this paper, we present several new \nresults---both positive and negative---which help define the boundaries between \nthe tractable and intractable settings. Specifically, we show positive results \nfor a large subclass of HMMs whose transition matrices are sparse, \nwell-conditioned, and have small probability mass on short cycles. On the other \nhand, we show that learning is impossible given only a polynomial number of \nsamples for HMMs with a small output alphabet and whose transition matrices are \nrandom regular graphs with large degree. We also discuss these results in the \ncontext of learning HMMs which can capture long-term dependencies. \n</p>"}, "author": "Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164289", "id": "tag:google.com,2005:reader/item/000000032eb5f10e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks. (arXiv:1711.02326v1 [cs.AI])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02326"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02326", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A major drawback of backpropagation through time (BPTT) is the difficulty of \nlearning long-term dependencies, coming from having to propagate credit \ninformation backwards through every single step of the forward computation. \nThis makes BPTT both computationally impractical and biologically implausible. \nFor this reason, full backpropagation through time is rarely used on long \nsequences, and truncated backpropagation through time is used as a heuristic. \nHowever, this usually leads to biased estimates of the gradient in which longer \nterm dependencies are ignored. Addressing this issue, we propose an alternative \nalgorithm, Sparse Attentive Backtracking, which might also be related to \nprinciples used by brains to learn long-term dependencies. Sparse Attentive \nBacktracking learns an attention mechanism over the hidden states of the past \nand selectively backpropagates through paths with high attention weights. This \nallows the model to learn long term dependencies while only backtracking for a \nsmall number of time steps, not just from the recent past but also from \nattended relevant past states. \n</p>"}, "author": "Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Laurent Charlin, Chris Pal, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164288", "id": "tag:google.com,2005:reader/item/000000032eb5f114", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distributed Bayesian Piecewise Sparse Linear Models. (arXiv:1711.02368v1 [cs.AI])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02368"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02368", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The importance of interpretability of machine learning models has been \nincreasing due to emerging enterprise predictive analytics, threat of data \nprivacy, accountability of artificial intelligence in society, and so on. \nPiecewise linear models have been actively studied to achieve both accuracy and \ninterpretability. They often produce competitive accuracy against \nstate-of-the-art non-linear methods. In addition, their representations (i.e., \nrule-based segmentation plus sparse linear formula) are often preferred by \ndomain experts. A disadvantage of such models, however, is high computational \ncost for simultaneous determinations of the number of \"pieces\" and cardinality \nof each linear predictor, which has restricted their applicability to \nmiddle-scale data sets. This paper proposes a distributed factorized asymptotic \nBayesian (FAB) inference of learning piece-wise sparse linear models on \ndistributed memory architectures. The distributed FAB inference solves the \nsimultaneous model selection issue without communicating $O(N)$ data where N is \nthe number of training samples and achieves linear scale-out against the number \nof CPU cores. Experimental results demonstrate that the distributed FAB \ninference achieves high prediction accuracy and performance scalability with \nboth synthetic and benchmark data. \n</p>"}, "author": "Masato Asahara, Ryohei Fujimaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164287", "id": "tag:google.com,2005:reader/item/000000032eb5f117", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Non-monotone Continuous DR-submodular Maximization: Structure and Algorithms. (arXiv:1711.02515v1 [cs.LG])", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02515"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02515", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>DR-submodular continuous functions are important objectives with wide \nreal-world applications spanning MAP inference in determinantal point processes \n(DPPs), and mean-field inference for probabilistic submodular models, amongst \nothers. DR-submodularity captures a subclass of non-convex functions that \nenables both exact minimization and approximate maximization in polynomial \ntime. \n</p> \n<p>In this work we study the problem of maximizing non-monotone DR-submodular \ncontinuous functions under general down-closed convex constraints. We start by \ninvestigating geometric properties that underlie such objectives, e.g., a \nstrong relation between (approximately) stationary points and global optimum is \nproved. These properties are then used to devise two optimization algorithms \nwith provable guarantees. Concretely, we first devise a \"two-phase\" algorithm \nwith $1/4$ approximation guarantee. This algorithm allows the use of existing \nmethods for finding (approximately) stationary points as a subroutine, thus, \nharnessing recent progress in non-convex optimization. Then we present a \nnon-monotone Frank-Wolfe variant with $1/e$ approximation guarantee and \nsublinear convergence rate. Finally, we extend our approach to a broader class \nof generalized DR-submodular continuous functions, which captures a wider \nspectrum of applications. Our theoretical findings are validated on synthetic \nand real-world problem instances. \n</p>"}, "author": "An Bian, Kfir Y. Levy, Andreas Krause, Joachim M. Buhmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164285", "id": "tag:google.com,2005:reader/item/000000032eb5f120", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Off-policy evaluation for slate recommendation. (arXiv:1605.04812v3 [cs.LG] UPDATED)", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1605.04812"}], "alternate": [{"href": "http://arxiv.org/abs/1605.04812", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper studies the evaluation of policies that recommend an ordered set \nof items (e.g., a ranking) based on some context---a common scenario in web \nsearch, ads, and recommendation. We build on techniques from combinatorial \nbandits to introduce a new practical estimator that uses logged data to \nestimate a policy's performance. A thorough empirical evaluation on real-world \ndata reveals that our estimator is accurate in a variety of settings, including \nas a subroutine in a learning-to-rank task, where it achieves competitive \nperformance. We derive conditions under which our estimator is unbiased---these \nconditions are weaker than prior heuristics for slate evaluation---and \nexperimentally demonstrate a smaller bias than parametric approaches, even when \nthese conditions are violated. Finally, our theory and experiments also show \nexponential savings in the amount of required data compared with general \nunbiased estimators. \n</p>"}, "author": "Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dud&#xed;k, John Langford, Damien Jose, Imed Zitouni", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510156945164", "timestampUsec": "1510156945164276", "id": "tag:google.com,2005:reader/item/000000032eb5f15e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network. (arXiv:1705.08584v2 [cs.LG] CROSS LISTED)", "published": 1510156945, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.08584"}], "alternate": [{"href": "http://arxiv.org/abs/1705.08584", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative moment matching network (GMMN) is a deep generative model that \ndiffers from Generative Adversarial Network (GAN) by replacing the \ndiscriminator in GAN with a two-sample test based on kernel maximum mean \ndiscrepancy (MMD). Although some theoretical guarantees of MMD have been \nstudied, the empirical performance of GMMN is still not as competitive as that \nof GAN on challenging and large benchmark datasets. The computational \nefficiency of GMMN is also less desirable in comparison with GAN, partially due \nto its requirement for a rather large batch size during the training. In this \npaper, we propose to improve both the model expressiveness of GMMN and its \ncomputational efficiency by introducing adversarial kernel learning techniques, \nas the replacement of a fixed Gaussian kernel in the original GMMN. The new \napproach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. \nThe new distance measure in MMD GAN is a meaningful loss that enjoys the \nadvantage of weak topology and can be optimized via gradient descent with \nrelatively small batch sizes. In our evaluation on multiple benchmark datasets, \nincluding MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN \nsignificantly outperforms GMMN, and is competitive with other representative \nGAN works. \n</p>"}, "author": "Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, Barnab&#xe1;s P&#xf3;czos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228481", "id": "tag:google.com,2005:reader/item/000000032e88cfdb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bounding and Counting Linear Regions of Deep Neural Networks. (arXiv:1711.02114v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02114"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02114", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7fb9844\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7fb9844&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we study the representational power of deep neural networks \n(DNN) that belong to the family of piecewise-linear (PWL) functions, based on \nPWL activation units such as rectifier or maxout. We investigate the complexity \nof such networks by studying the number of linear regions of the PWL function. \nTypically, a PWL function from a DNN can be seen as a large family of linear \nfunctions acting on millions of such regions. We directly build upon the work \nof Montufar et al. (2014) and Raghu et al. (2017) by refining the upper and \nlower bounds on the number of linear regions for rectified and maxout networks. \nIn addition to achieving tighter bounds, we also develop a novel method to \nperform exact enumeration or counting of the number of linear regions with a \nmixed-integer linear formulation that maps the input space to output. We use \nthis new capability to visualize how the number of linear regions change while \ntraining DNNs. \n</p>"}, "author": "Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228480", "id": "tag:google.com,2005:reader/item/000000032e88cfeb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adaptive Bayesian Sampling with Monte Carlo EM. (arXiv:1711.02159v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02159"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02159", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a novel technique for learning the mass matrices in samplers \nobtained from discretized dynamics that preserve some energy function. Existing \nadaptive samplers use Riemannian preconditioning techniques, where the mass \nmatrices are functions of the parameters being sampled. This leads to \nsignificant complexities in the energy reformulations and resultant dynamics, \noften leading to implicit systems of equations and requiring inversion of \nhigh-dimensional matrices in the leapfrog steps. Our approach provides a \nsimpler alternative, by using existing dynamics in the sampling step of a Monte \nCarlo EM framework, and learning the mass matrices in the M step with a novel \nonline technique. We also propose a way to adaptively set the number of samples \ngathered in the E step, using sampling error estimates from the leapfrog \ndynamics. Along with a novel stochastic sampler based on Nos\\'{e}-Poincar\\'{e} \ndynamics, we use this framework with standard Hamiltonian Monte Carlo (HMC) as \nwell as newer stochastic algorithms such as SGHMC and SGNHT, and show strong \nperformance on synthetic and real high-dimensional sampling scenarios; we \nachieve sampling accuracies comparable to Riemannian samplers while being \nsignificantly faster. \n</p>"}, "author": "Anirban Roychowdhury, Srinivasan Parthasarathy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228479", "id": "tag:google.com,2005:reader/item/000000032e88cff8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Alpha-expansion is Exact on Stable Instances. (arXiv:1711.02195v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02195"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02195", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Approximate algorithms for structured prediction problems---such as the \npopular alpha-expansion algorithm (Boykov et al. 2001) in computer \nvision---typically far exceed their theoretical performance guarantees on \nreal-world instances. These algorithms often find solutions that are very close \nto optimal. The goal of this paper is to partially explain the performance of \nalpha-expansion on MAP inference in Ferromagnetic Potts models (FPMs). Our main \nresults use the connection between energy minimization in FPMs and the Uniform \nMetric Labeling problem to give a stability condition under which the \nalpha-expansion algorithm provably recovers the optimal MAP solution. This \ntheoretical result complements the numerous empirical observations of \nalpha-expansion's performance. Additionally, we give a different stability \ncondition under which an LP-based algorithm recovers the optimal solution. \n</p>"}, "author": "Hunter Lang, David Sontag, Aravindan Vijayaraghavan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228478", "id": "tag:google.com,2005:reader/item/000000032e88d007", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Regret Bounds and Regimes of Optimality for User-User and Item-Item Collaborative Filtering. (arXiv:1711.02198v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02198"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02198", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider an online model for recommendation systems, with each user being \nrecommended an item at each time-step and providing 'like' or 'dislike' \nfeedback. A latent variable model specifies the user preferences: both users \nand items are clustered into types. All users of a given type have identical \npreferences for the items, and similarly, items of a given type are either all \nliked or all disliked by a given user. The model captures structure in both the \nitem and user spaces, and in this paper, we assume that the type preference \nmatrix is randomly generated. We describe two algorithms inspired by user-user \nand item-item collaborative filtering (CF), modified to explicitly make \nexploratory recommendations, and prove performance guarantees in terms of their \nexpected regret. For two regimes of model parameters, with structure only in \nitem space or only in user space, we prove information-theoretic lower bounds \non regret that match our upper bounds up to logarithmic factors. Our analysis \nelucidates system operating regimes in which existing CF algorithms are nearly \noptimal. \n</p>"}, "author": "Guy Bresler, Mina Karzand", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228477", "id": "tag:google.com,2005:reader/item/000000032e88d017", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Unsupervised Learning of Semantic Audio Representations. (arXiv:1711.02209v1 [cs.SD])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02209"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02209", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Even in the absence of any explicit semantic annotation, vast collections of \naudio recordings provide valuable information for learning the categorical \nstructure of sounds. We consider several class-agnostic semantic constraints \nthat apply to unlabeled nonspeech audio: (i) noise and translations in time do \nnot change the underlying sound category, (ii) a mixture of two sound events \ninherits the categories of the constituents, and (iii) the categories of events \nin close temporal proximity are likely to be the same or related. Without \nlabels to ground them, these constraints are incompatible with classification \nloss functions. However, they may still be leveraged to identify geometric \ninequalities needed for triplet loss-based training of convolutional neural \nnetworks. The result is low-dimensional embeddings of the input spectrograms \nthat recover 41% and 84% of the performance of their fully-supervised \ncounterparts when applied to downstream query-by-example sound retrieval and \nsound event classification tasks, respectively. Moreover, in \nlimited-supervision settings, our unsupervised embeddings double the \nstate-of-the-art classification performance. \n</p>"}, "author": "Aren Jansen, Manoj Plakal, Ratheet Pandya, Daniel P. W. Ellis, Shawn Hershey, Jiayang Liu, R. Channing Moore, Rif A. Saurous", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228476", "id": "tag:google.com,2005:reader/item/000000032e88d030", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks. (arXiv:1711.02213v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02213"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02213", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks are commonly developed and trained in 32-bit floating \npoint format. Significant gains in performance and energy efficiency could be \nrealized by training and inference in numerical formats optimized for deep \nlearning. Despite advances in limited precision inference in recent years, \ntraining of neural networks in low bit-width remains a challenging problem. \nHere we present the Flexpoint data format, aiming at a complete replacement of \n32-bit floating point format training and inference, designed to support modern \ndeep network topologies without modifications. Flexpoint tensors have a shared \nexponent that is dynamically adjusted to minimize overflows and maximize \navailable dynamic range. We validate Flexpoint by training AlexNet, a deep \nresidual network and a generative adversarial network, using a simulator \nimplemented with the neon deep learning framework. We demonstrate that 16-bit \nFlexpoint closely matches 32-bit floating point in training all three models, \nwithout any need for tuning of model hyperparameters. Our results suggest \nFlexpoint as a promising numerical format for future hardware for training and \ninference. \n</p>"}, "author": "Urs K&#xf6;ster, Tristan Webb, Xin Wang, Marcel Nassar, Arjun Bansal, William Constable, Oguz Elibol, Stewart Hall, Luke Hornof, Amir Khosrowshahi, Carey Kloss, Ruby Pai, Naveen Rao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228475", "id": "tag:google.com,2005:reader/item/000000032e88d037", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Unsupervised Transformation Learning via Convex Relaxations. (arXiv:1711.02226v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02226"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02226", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Our goal is to extract meaningful transformations from raw images, such as \nvarying the thickness of lines in handwriting or the lighting in a portrait. We \npropose an unsupervised approach to learn such transformations by attempting to \nreconstruct an image from a linear combination of transformations of its \nnearest neighbors. On handwritten digits and celebrity portraits, we show that \neven with linear transformations, our method generates visually high-quality \nmodified images. Moreover, since our method is semiparametric and does not \nmodel the data distribution, the learned transformations extrapolate off the \ntraining data and can be applied to new types of images. \n</p>"}, "author": "Tatsunori B. Hashimoto, John C. Duchi, Percy Liang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228474", "id": "tag:google.com,2005:reader/item/000000032e88d03d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net. (arXiv:1711.02282v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02282"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02282", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a novel method to directly learn a stochastic transition operator \nwhose repeated application provides generated samples. Traditional undirected \ngraphical models approach this problem indirectly by learning a Markov chain \nmodel whose stationary distribution obeys detailed balance with respect to a \nparameterized energy function. The energy function is then modified so the \nmodel and data distributions match, with no guarantee on the number of steps \nrequired for the Markov chain to converge. Moreover, the detailed balance \ncondition is highly restrictive: energy based models corresponding to neural \nnetworks must have symmetric weights, unlike biological neural circuits. In \ncontrast, we develop a method for directly learning arbitrarily parameterized \ntransition operators capable of expressing non-equilibrium stationary \ndistributions that violate detailed balance, thereby enabling us to learn more \nbiologically plausible asymmetric neural networks and more general non-energy \nbased dynamical systems. The proposed training objective, which we derive via \nprincipled variational methods, encourages the transition operator to \"walk \nback\" in multi-step trajectories that start at data-points, as quickly as \npossible back to the original data points. We present a series of experimental \nresults illustrating the soundness of the proposed approach, Variational \nWalkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating \nsuperior samples compared to earlier attempts to learn a transition operator. \nWe also show that although each rapid training trajectory is limited to a \nfinite but variable number of steps, our transition operator continues to \ngenerate good samples well past the length of such trajectories, thereby \ndemonstrating the match of its non-equilibrium stationary distribution to the \ndata distribution. Source Code: <a href=\"http://github.com/anirudh9119/walkback_nips17\">this http URL</a> \n</p>"}, "author": "Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228473", "id": "tag:google.com,2005:reader/item/000000032e88d044", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Large-Scale Optimal Transport and Mapping Estimation. (arXiv:1711.02283v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02283"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02283", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a novel two-step approach for the fundamental problem of \nlearning an optimal map from one distribution to another. First, we learn an \noptimal transport (OT) plan, which can be thought as a one-to-many map between \nthe two distributions. To that end, we propose a stochastic dual approach of \nregularized OT, and show empirically that it scales better than a recent \nrelated approach when the amount of samples is very large. Second, we estimate \na Monge map as a deep neural network learned by approximating the barycentric \nprojection of the previously-obtained OT plan. We prove two theoretical \nstability results of regularized OT which show that our estimations converge to \nthe OT plan and Monge map between the underlying continuous measures. We \nshowcase our proposed approach on two applications: domain adaptation and \ngenerative modeling. \n</p>"}, "author": "Vivien Seguy, Bharath Bhushan Damodaran, R&#xe9;mi Flamary, Nicolas Courty, Antoine Rolet, Mathieu Blondel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228472", "id": "tag:google.com,2005:reader/item/000000032e88d048", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?. (arXiv:1711.02301v1 [cs.AI])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02301"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02301", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep reinforcement learning has achieved many recent successes, but our \nunderstanding of its strengths and limitations is hampered by the lack of rich \nenvironments in which we can fully characterize optimal behavior, and \ncorrespondingly diagnose individual actions against such a characterization. \nHere we consider a family of combinatorial games, arising from work of Erdos, \nSelfridge, and Spencer, and we propose their use as environments for evaluating \nand comparing different approaches to reinforcement learning. These games have \na number of appealing features: they are challenging for current learning \napproaches, but they form (i) a low-dimensional, simply parametrized \nenvironment where (ii) there is a linear closed form solution for optimal \nbehavior from any state, and (iii) the difficulty of the game can be tuned by \nchanging environment parameters in an interpretable way. We use these \nErdos-Selfridge-Spencer games not only to compare different algorithms, but \nalso to compare approaches based on supervised and reinforcement learning, to \nanalyze the power of multi-agent approaches in improving performance, and to \nevaluate generalization to environments outside the training set. \n</p>"}, "author": "Maithra Raghu, Alex Irpan, Jacob Andreas, Robert Kleinberg, Quoc V. Le, Jon Kleinberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228471", "id": "tag:google.com,2005:reader/item/000000032e88d04f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Overcomplete HMMs. (arXiv:1711.02309v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02309"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02309", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c7fb9aa7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c7fb9aa7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study the problem of learning overcomplete HMMs---those that have many \nhidden states but a small output alphabet. Despite having significant practical \nimportance, such HMMs are poorly understood with no known positive or negative \nresults for efficient learning. In this paper, we present several new \nresults---both positive and negative---which help define the boundaries between \nthe tractable and intractable settings. Specifically, we show positive results \nfor a large subclass of HMMs whose transition matrices are sparse, \nwell-conditioned, and have small probability mass on short cycles. On the other \nhand, we show that learning is impossible given only a polynomial number of \nsamples for HMMs with a small output alphabet and whose transition matrices are \nrandom regular graphs with large degree. We also discuss these results in the \ncontext of learning HMMs which can capture long-term dependencies. \n</p>"}, "author": "Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228470", "id": "tag:google.com,2005:reader/item/000000032e88d075", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-Player Bandits Models Revisited. (arXiv:1711.02317v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02317"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02317", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c805c904\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c805c904&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Multi-player Multi-Armed Bandits (MAB) have been extensively studied in the \nliterature, motivated by applications to Cognitive Radio systems. Driven by \nsuch applications as well, we motivate the introduction of several levels of \nfeedback for multi-player MAB algorithms. Most existing work assume that \nsensing information is available to the algorithm. Under this assumption, we \nimprove the state-of-the-art lower bound for the regret of any decentralized \nalgorithms and introduce two algorithms, RandTopM and MCTopM, that are shown to \nempirically outperform existing algorithms. Moreover, we provide strong \ntheoretical guarantees for these algorithms, including a notion of asymptotic \noptimality in terms of the number of selections of bad arms. We then introduce \na promising heuristic, called Selfish, that can operate without sensing \ninformation, which is crucial for emerging applications to Internet of Things \nnetworks. We investigate the empirical performance of this algorithm and \nprovide some first theoretical elements for the understanding of its behavior. \n</p>"}, "author": "Lilian Besson (IETR), Emilie Kaufmann (SEQUEL)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228469", "id": "tag:google.com,2005:reader/item/000000032e88d096", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks. (arXiv:1711.02326v1 [cs.AI])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02326"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02326", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A major drawback of backpropagation through time (BPTT) is the difficulty of \nlearning long-term dependencies, coming from having to propagate credit \ninformation backwards through every single step of the forward computation. \nThis makes BPTT both computationally impractical and biologically implausible. \nFor this reason, full backpropagation through time is rarely used on long \nsequences, and truncated backpropagation through time is used as a heuristic. \nHowever, this usually leads to biased estimates of the gradient in which longer \nterm dependencies are ignored. Addressing this issue, we propose an alternative \nalgorithm, Sparse Attentive Backtracking, which might also be related to \nprinciples used by brains to learn long-term dependencies. Sparse Attentive \nBacktracking learns an attention mechanism over the hidden states of the past \nand selectively backpropagates through paths with high attention weights. This \nallows the model to learn long term dependencies while only backtracking for a \nsmall number of time steps, not just from the recent past but also from \nattended relevant past states. \n</p>"}, "author": "Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Laurent Charlin, Chris Pal, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228468", "id": "tag:google.com,2005:reader/item/000000032e88d0b2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interpreting Convolutional Neural Networks Through Compression. (arXiv:1711.02329v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02329"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02329", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolutional neural networks (CNNs) achieve state-of-the-art performance in \na wide variety of tasks in computer vision. However, interpreting CNNs still \nremains a challenge. This is mainly due to the large number of parameters in \nthese networks. Here, we investigate the role of compression and particularly \npruning filters in the interpretation of CNNs. We exploit our recently-proposed \ngreedy structural compression scheme that prunes filters in a trained CNN. In \nour compression, the filter importance index is defined as the classification \naccuracy reduction (CAR) of the network after pruning that filter. The filters \nare then iteratively pruned based on the CAR index. We demonstrate the \ninterpretability of CAR-compressed CNNs by showing that our algorithm prunes \nfilters with visually redundant pattern selectivity. Specifically, we show the \nimportance of shape-selective filters for object recognition, as opposed to \ncolor-selective filters. Out of top 20 CAR-pruned filters in AlexNet, 17 of \nthem in the first layer and 14 of them in the second layer are color-selective \nfilters. Finally, we introduce a variant of our CAR importance index that \nquantifies the importance of each image class to each CNN filter. We show that \nthe most and the least important class labels present a meaningful \ninterpretation of each filter that is consistent with the visualized pattern \nselectivity of that filter. \n</p>"}, "author": "Reza Abbasi-Asl, Bin Yu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228467", "id": "tag:google.com,2005:reader/item/000000032e88d0dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "FADO: A Deterministic Detection/Learning Algorithm. (arXiv:1711.02361v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02361"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02361", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes and studies a detection technique for adversarial \nscenarios (dubbed deterministic detection). This technique provides an \nalternative detection methodology in case the usual stochastic methods are not \napplicable: this can be because the studied phenomenon does not follow a \nstochastic sampling scheme, samples are high-dimensional and subsequent \nmultiple-testing corrections render results overly conservative, sample sizes \nare too low for asymptotic results (as e.g. the central limit theorem) to kick \nin, or one cannot allow for the small probability of failure inherent to \nstochastic approaches. This paper instead designs a method based on insights \nfrom machine learning and online learning theory: this detection algorithm - \nnamed Online FAult Detection (FADO) - comes with theoretical guarantees of its \ndetection capabilities. A version of the margin is found to regulate the \ndetection performance of FADO. A precise expression is derived for bounding the \nperformance, and experimental results are presented assessing the influence of \ninvolved quantities. A case study of scene detection is used to illustrate the \napproach. The technology is closely related to the linear perceptron rule, \ninherits its computational attractiveness and flexibility towards various \nextensions. \n</p>"}, "author": "Kristiaan Pelckmans", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228466", "id": "tag:google.com,2005:reader/item/000000032e88d0ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distributed Bayesian Piecewise Sparse Linear Models. (arXiv:1711.02368v1 [cs.AI])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02368"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02368", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The importance of interpretability of machine learning models has been \nincreasing due to emerging enterprise predictive analytics, threat of data \nprivacy, accountability of artificial intelligence in society, and so on. \nPiecewise linear models have been actively studied to achieve both accuracy and \ninterpretability. They often produce competitive accuracy against \nstate-of-the-art non-linear methods. In addition, their representations (i.e., \nrule-based segmentation plus sparse linear formula) are often preferred by \ndomain experts. A disadvantage of such models, however, is high computational \ncost for simultaneous determinations of the number of \"pieces\" and cardinality \nof each linear predictor, which has restricted their applicability to \nmiddle-scale data sets. This paper proposes a distributed factorized asymptotic \nBayesian (FAB) inference of learning piece-wise sparse linear models on \ndistributed memory architectures. The distributed FAB inference solves the \nsimultaneous model selection issue without communicating $O(N)$ data where N is \nthe number of training samples and achieves linear scale-out against the number \nof CPU cores. Experimental results demonstrate that the distributed FAB \ninference achieves high prediction accuracy and performance scalability with \nboth synthetic and benchmark data. \n</p>"}, "author": "Masato Asahara, Ryohei Fujimaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228465", "id": "tag:google.com,2005:reader/item/000000032e88d0fc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Tutorial on Canonical Correlation Methods. (arXiv:1711.02391v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02391"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02391", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Canonical correlation analysis is a family of multivariate statistical \nmethods for the analysis of paired sets of variables. Since its proposition, \ncanonical correlation analysis has for instance been extended to extract \nrelations between two sets of variables when the sample size is insufficient in \nrelation to the data dimensionality, when the relations have been considered to \nbe non-linear, and when the dimensionality is too large for human \ninterpretation. This tutorial explains the theory of canonical correlation \nanalysis including its regularised, kernel, and sparse variants. Additionally, \nthe deep and Bayesian CCA extensions are briefly reviewed. Together with the \nnumerical examples, this overview provides a coherent compendium on the \napplicability of the variants of canonical correlation analysis. By bringing \ntogether techniques for solving the optimisation problems, evaluating the \nstatistical significance and generalisability of the canonical correlation \nmodel, and interpreting the relations, we hope that this article can serve as a \nhands-on tool for applying canonical correlation methods in data analysis. \n</p>"}, "author": "Viivi Uurtio, Jo&#xe3;o M. Monteiro, Jaz Kandola, John Shawe-Taylor, Delmiro Fernandez-Reyes, Juho Rousu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228464", "id": "tag:google.com,2005:reader/item/000000032e88d104", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gaussian Lower Bound for the Information Bottleneck Limit. (arXiv:1711.02421v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02421"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02421", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Information Bottleneck (IB) is a conceptual method for extracting the \nmost compact, yet informative, representation of a set of variables, with \nrespect to the target. It generalizes the notion of minimal sufficient \nstatistics from classical parametric statistics to a broader \ninformation-theoretic sense. The IB curve defines the optimal trade-off between \nrepresentation complexity and its predictive power. Specifically, it is \nachieved by minimizing the level of mutual information (MI) between the \nrepresentation and the original variables, subject to a minimal level of MI \nbetween the representation and the target. This problem is shown to be in \ngeneral NP hard. One important exception is the multivariate Gaussian case, for \nwhich the Gaussian IB (GIB) is known to obtain an analytical closed form \nsolution, similar to Canonical Correlation Analysis (CCA). In this work we \nintroduce a Gaussian lower bound to the IB curve; we find an embedding of the \ndata which maximizes its \"Gaussian part\", on which we apply the GIB. This \nembedding provides an efficient (and practical) representation of any arbitrary \ndata-set (in the IB sense), which in addition holds the favorable properties of \na Gaussian distribution. Importantly, we show that the optimal Gaussian \nembedding is bounded from above by non-linear CCA. This allows a fundamental \nlimit for our ability to Gaussianize arbitrary data-sets and solve complex \nproblems by linear methods. \n</p>"}, "author": "Amichai Painsky, Naftali Tishby", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228463", "id": "tag:google.com,2005:reader/item/000000032e88d11d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Cortical microcircuits as gated-recurrent neural networks. (arXiv:1711.02448v1 [q-bio.NC])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02448"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02448", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Cortical circuits exhibit intricate recurrent architectures that are \nremarkably similar across different brain areas. Such stereotyped structure \nsuggests the existence of common computational principles. However, such \nprinciples have remained largely elusive. Inspired by gated-memory networks, \nnamely long short-term memory networks (LSTMs), we introduce a recurrent neural \nnetwork in which information is gated through inhibitory cells that are \nsubtractive (subLSTM). We propose a natural mapping of subLSTMs onto known \ncanonical excitatory-inhibitory cortical microcircuits. Our empirical \nevaluation across sequential image classification and language modelling tasks \nshows that subLSTM units can achieve similar performance to LSTM units. These \nresults suggest that cortical circuits can be optimised to solve complex \ncontextual problems and proposes a novel view on their computational function. \nOverall our work provides a step towards unifying recurrent networks as used in \nmachine learning with their biological counterparts. \n</p>"}, "author": "Rui Ponte Costa, Yannis M. Assael, Brendan Shillingford, Nando de Freitas, Tim P. Vogels", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228462", "id": "tag:google.com,2005:reader/item/000000032e88d126", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian model and dimension reduction for uncertainty propagation: applications in random media. (arXiv:1711.02475v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02475"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02475", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Well-established methods for the solution of stochastic partial differential \nequations (SPDEs) typically struggle in problems with high-dimensional \ninputs/outputs. Such difficulties are only amplified in large-scale \napplications where even a few tens of full-order model runs are impractical. \nWhile dimensionality reduction can alleviate some of these issues, it is not \nknown which and how many features of the (high-dimensional) input are actually \npredictive of the (high-dimensional) output. In this paper, we advocate a \nBayesian formulation that is capable of performing simultaneous dimension and \nmodel-order reduction. It consists of a component that encodes the \nhigh-dimensional input into a low-dimensional set of feature functions by \nemploying sparsity-enforcing priors and a decoding component that makes use of \nthe solution of a coarse-grained model in order to reconstruct that of the \nfull-order model. Both components are represented with latent variables in a \nprobabilistic graphical model and are simultaneously trained using Stochastic \nVariational Inference methods. The model is capable of quantifying the \npredictive uncertainty due to the information loss that unavoidably takes place \nin any model-order/dimension reduction as well as the uncertainty arising from \nfinite-sized training datasets. We demonstrate its capabilities in the context \nof random media where fine-scale fluctuations can give rise to random inputs \nwith tens of thousands of variables. With a few tens of full-order model \nsimulations, the proposed model is capable of identifying salient physical \nfeatures and produce sharp predictions under different boundary conditions of \nthe full output which itself consists of thousands of components. \n</p>"}, "author": "Constantin Grigo, Phaedon-Stelios Koutsourelakis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228461", "id": "tag:google.com,2005:reader/item/000000032e88d131", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Grafting for Combinatorial Boolean Model using Frequent Itemset Mining. (arXiv:1711.02478v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02478"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02478", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c805ccc1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c805ccc1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper introduces the combinatorial Boolean model (CBM), which is defined \nas the class of linear combinations of conjunctions of Boolean attributes. This \npaper addresses the issue of learning CBM from labeled data. CBM is of high \nknowledge interpretability but na\\\"{i}ve learning of it requires exponentially \nlarge computation time with respect to data dimension and sample size. To \novercome this computational difficulty, we propose an algorithm GRAB (GRAfting \nfor Boolean datasets), which efficiently learns CBM within the \n$L_1$-regularized loss minimization framework. The key idea of GRAB is to \nreduce the loss minimization problem to the weighted frequent itemset mining, \nin which frequent patterns are efficiently computable. We employ benchmark \ndatasets to empirically demonstrate that GRAB is effective in terms of \ncomputational efficiency, prediction accuracy and knowledge discovery. \n</p>"}, "author": "Taito Lee, Shin Matsushima, Kenji Yamanishi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228460", "id": "tag:google.com,2005:reader/item/000000032e88d136", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Non-monotone Continuous DR-submodular Maximization: Structure and Algorithms. (arXiv:1711.02515v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02515"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02515", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>DR-submodular continuous functions are important objectives with wide \nreal-world applications spanning MAP inference in determinantal point processes \n(DPPs), and mean-field inference for probabilistic submodular models, amongst \nothers. DR-submodularity captures a subclass of non-convex functions that \nenables both exact minimization and approximate maximization in polynomial \ntime. \n</p> \n<p>In this work we study the problem of maximizing non-monotone DR-submodular \ncontinuous functions under general down-closed convex constraints. We start by \ninvestigating geometric properties that underlie such objectives, e.g., a \nstrong relation between (approximately) stationary points and global optimum is \nproved. These properties are then used to devise two optimization algorithms \nwith provable guarantees. Concretely, we first devise a \"two-phase\" algorithm \nwith $1/4$ approximation guarantee. This algorithm allows the use of existing \nmethods for finding (approximately) stationary points as a subroutine, thus, \nharnessing recent progress in non-convex optimization. Then we present a \nnon-monotone Frank-Wolfe variant with $1/e$ approximation guarantee and \nsublinear convergence rate. Finally, we extend our approach to a broader class \nof generalized DR-submodular continuous functions, which captures a wider \nspectrum of applications. Our theoretical findings are validated on synthetic \nand real-world problem instances. \n</p>"}, "author": "An Bian, Kfir Y. Levy, Andreas Krause, Joachim M. Buhmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228459", "id": "tag:google.com,2005:reader/item/000000032e88d140", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Online Learning for Changing Environments using Coin Betting. (arXiv:1711.02545v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02545"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02545", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A key challenge in online learning is that classical algorithms can be slow \nto adapt to changing environments. Recent studies have proposed \"meta\" \nalgorithms that convert any online learning algorithm to one that is adaptive \nto changing environments, where the adaptivity is analyzed in a quantity called \nthe strongly-adaptive regret. This paper describes a new meta algorithm that \nhas a strongly-adaptive regret bound that is a factor of $\\sqrt{\\log(T)}$ \nbetter than other algorithms with the same time complexity, where $T$ is the \ntime horizon. We also extend our algorithm to achieve a first-order (i.e., \ndependent on the observed losses) strongly-adaptive regret bound for the first \ntime, to our knowledge. At its heart is a new parameter-free algorithm for the \nlearning with expert advice (LEA) problem in which experts sometimes do not \noutput advice for consecutive time steps (i.e., \\emph{sleeping} experts). This \nalgorithm is derived by a reduction from optimal algorithms for the so-called \ncoin betting problem. Empirical results show that our algorithm outperforms \nstate-of-the-art methods in both learning with expert advice and metric \nlearning scenarios. \n</p>"}, "author": "Kwang-Sung Jun, Francesco Orabona, Stephen Wright, Rebecca Willett", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228458", "id": "tag:google.com,2005:reader/item/000000032e88d14c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach. (arXiv:1711.02598v1 [cs.DS])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02598"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02598", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the classical problem of maximizing a monotone submodular function \nsubject to a cardinality constraint k, with two additional twists: (i) elements \narrive in a streaming fashion, and (ii) m items from the algorithm's memory are \nremoved after the stream is finished. We develop a robust submodular algorithm \nSTAR-T. It is based on a novel partitioning structure and an exponentially \ndecreasing thresholding rule. STAR-T makes one pass over the data and retains a \nshort but robust summary. We show that after the removal of any m elements from \nthe obtained summary, a simple greedy algorithm STAR-T-GREEDY that runs on the \nremaining elements achieves a constant-factor approximation guarantee. In two \ndifferent data summarization tasks, we demonstrate that it matches or \noutperforms existing greedy and streaming methods, even if they are allowed the \nbenefit of knowing the removed subset in advance. \n</p>"}, "author": "Slobodan Mitrovi&#x107;, Ilija Bogunovic, Ashkan Norouzi-Fard, Jakub Tarnawski, Volkan Cevher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228457", "id": "tag:google.com,2005:reader/item/000000032e88d157", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Moonshine: Distilling with Cheap Convolutions. (arXiv:1711.02613v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02613"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02613", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Model distillation compresses a trained machine learning model, such as a \nneural network, into a smaller alternative such that it could be easily \ndeployed in a resource limited setting. Unfortunately, this requires \nengineering two architectures: a student architecture smaller than the first \nteacher architecture but trained to emulate it. In this paper, we present a \ndistillation strategy that produces a student architecture that is a simple \ntransformation of the teacher architecture. Recent model distillation methods \nallow us to preserve most of the performance of the trained model after \nreplacing convolutional blocks with a cheap alternative. In addition, \ndistillation by attention transfer provides student network performance that is \nbetter than training that student architecture directly on data. \n</p>"}, "author": "Elliot J. Crowley, Gavin Gray, Amos Storkey", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228456", "id": "tag:google.com,2005:reader/item/000000032e88d168", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Theoretical limitations of Encoder-Decoder GAN architectures. (arXiv:1711.02651v1 [cs.LG])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02651"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02651", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Encoder-decoder GANs architectures (e.g., BiGAN and ALI) seek to add an \ninference mechanism to the GANs setup, consisting of a small encoder deep net \nthat maps data-points to their succinct encodings. The intuition is that being \nforced to train an encoder alongside the usual generator forces the system to \nlearn meaningful mappings from the code to the data-point and vice-versa, which \nshould improve the learning of the target distribution and ameliorate \nmode-collapse. It should also yield meaningful codes that are useful as \nfeatures for downstream tasks. The current paper shows rigorously that even on \nreal-life distributions of images, the encode-decoder GAN training objectives \n(a) cannot prevent mode collapse; i.e. the objective can be near-optimal even \nwhen the generated distribution has low and finite support (b) cannot prevent \nlearning meaningless codes for data -- essentially white noise. Thus if \nencoder-decoder GANs do indeed work then it must be due to reasons as yet not \nunderstood, since the training objective can be low even for meaningless \nsolutions. \n</p>"}, "author": "Sanjeev Arora, Andrej Risteski, Yi Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228455", "id": "tag:google.com,2005:reader/item/000000032e88d16e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neural system identification for large populations separating \"what\" and \"where\". (arXiv:1711.02653v1 [stat.ML])", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02653"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02653", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neuroscientists classify neurons into different types that perform similar \ncomputations at different locations in the visual field. Traditional methods \nfor neural system identification do not capitalize on this separation of 'what' \nand 'where'. Learning deep convolutional feature spaces that are shared among \nmany neurons provides an exciting path forward, but the architectural design \nneeds to account for data limitations: While new experimental techniques enable \nrecordings from thousands of neurons, experimental time is limited so that one \ncan sample only a small fraction of each neuron's response space. Here, we show \nthat a major bottleneck for fitting convolutional neural networks (CNNs) to \nneural data is the estimation of the individual receptive field locations, a \nproblem that has been scratched only at the surface thus far. We propose a CNN \narchitecture with a sparse readout layer factorizing the spatial (where) and \nfeature (what) dimensions. Our network scales well to thousands of neurons and \nshort recordings and can be trained end-to-end. We evaluate this architecture \non ground-truth data to explore the challenges and limitations of CNN-based \nsystem identification. Moreover, we show that our network model outperforms \ncurrent state-of-the art system identification models of mouse primary visual \ncortex. \n</p>"}, "author": "David A. Klindt, Alexander S. Ecker, Thomas Euler, Matthias Bethge", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228454", "id": "tag:google.com,2005:reader/item/000000032e88d176", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Off-policy evaluation for slate recommendation. (arXiv:1605.04812v3 [cs.LG] UPDATED)", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1605.04812"}], "alternate": [{"href": "http://arxiv.org/abs/1605.04812", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper studies the evaluation of policies that recommend an ordered set \nof items (e.g., a ranking) based on some context---a common scenario in web \nsearch, ads, and recommendation. We build on techniques from combinatorial \nbandits to introduce a new practical estimator that uses logged data to \nestimate a policy's performance. A thorough empirical evaluation on real-world \ndata reveals that our estimator is accurate in a variety of settings, including \nas a subroutine in a learning-to-rank task, where it achieves competitive \nperformance. We derive conditions under which our estimator is unbiased---these \nconditions are weaker than prior heuristics for slate evaluation---and \nexperimentally demonstrate a smaller bias than parametric approaches, even when \nthese conditions are violated. Finally, our theory and experiments also show \nexponential savings in the amount of required data compared with general \nunbiased estimators. \n</p>"}, "author": "Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dud&#xed;k, John Langford, Damien Jose, Imed Zitouni", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510139306228", "timestampUsec": "1510139306228433", "id": "tag:google.com,2005:reader/item/000000032e88d308", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network. (arXiv:1705.08584v2 [cs.LG] CROSS LISTED)", "published": 1510139306, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.08584"}], "alternate": [{"href": "http://arxiv.org/abs/1705.08584", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative moment matching network (GMMN) is a deep generative model that \ndiffers from Generative Adversarial Network (GAN) by replacing the \ndiscriminator in GAN with a two-sample test based on kernel maximum mean \ndiscrepancy (MMD). Although some theoretical guarantees of MMD have been \nstudied, the empirical performance of GMMN is still not as competitive as that \nof GAN on challenging and large benchmark datasets. The computational \nefficiency of GMMN is also less desirable in comparison with GAN, partially due \nto its requirement for a rather large batch size during the training. In this \npaper, we propose to improve both the model expressiveness of GMMN and its \ncomputational efficiency by introducing adversarial kernel learning techniques, \nas the replacement of a fixed Gaussian kernel in the original GMMN. The new \napproach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. \nThe new distance measure in MMD GAN is a meaningful loss that enjoys the \nadvantage of weak topology and can be optimized via gradient descent with \nrelatively small batch sizes. In our evaluation on multiple benchmark datasets, \nincluding MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN \nsignificantly outperforms GMMN, and is competitive with other representative \nGAN works. \n</p>"}, "author": "Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, Barnab&#xe1;s P&#xf3;czos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510106836167", "timestampUsec": "1510106836166582", "id": "tag:google.com,2005:reader/item/000000032e4743f3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bounding and Counting Linear Regions of Deep Neural Networks. (arXiv:1711.02114v1 [cs.LG])", "published": 1510106836, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02114"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02114", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study the representational power of deep neural networks \n(DNN) that belong to the family of piecewise-linear (PWL) functions, based on \nPWL activation units such as rectifier or maxout. We investigate the complexity \nof such networks by studying the number of linear regions of the PWL function. \nTypically, a PWL function from a DNN can be seen as a large family of linear \nfunctions acting on millions of such regions. We directly build upon the work \nof Montufar et al. (2014) and Raghu et al. (2017) by refining the upper and \nlower bounds on the number of linear regions for rectified and maxout networks. \nIn addition to achieving tighter bounds, we also develop a novel method to \nperform exact enumeration or counting of the number of linear regions with a \nmixed-integer linear formulation that maps the input space to output. We use \nthis new capability to visualize how the number of linear regions change while \ntraining DNNs. \n</p>"}, "author": "Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510106836167", "timestampUsec": "1510106836166581", "id": "tag:google.com,2005:reader/item/000000032e4743f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net. (arXiv:1711.02282v1 [stat.ML])", "published": 1510106836, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02282"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02282", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c805d013\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c805d013&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a novel method to directly learn a stochastic transition operator \nwhose repeated application provides generated samples. Traditional undirected \ngraphical models approach this problem indirectly by learning a Markov chain \nmodel whose stationary distribution obeys detailed balance with respect to a \nparameterized energy function. The energy function is then modified so the \nmodel and data distributions match, with no guarantee on the number of steps \nrequired for the Markov chain to converge. Moreover, the detailed balance \ncondition is highly restrictive: energy based models corresponding to neural \nnetworks must have symmetric weights, unlike biological neural circuits. In \ncontrast, we develop a method for directly learning arbitrarily parameterized \ntransition operators capable of expressing non-equilibrium stationary \ndistributions that violate detailed balance, thereby enabling us to learn more \nbiologically plausible asymmetric neural networks and more general non-energy \nbased dynamical systems. The proposed training objective, which we derive via \nprincipled variational methods, encourages the transition operator to \"walk \nback\" in multi-step trajectories that start at data-points, as quickly as \npossible back to the original data points. We present a series of experimental \nresults illustrating the soundness of the proposed approach, Variational \nWalkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating \nsuperior samples compared to earlier attempts to learn a transition operator. \nWe also show that although each rapid training trajectory is limited to a \nfinite but variable number of steps, our transition operator continues to \ngenerate good samples well past the length of such trajectories, thereby \ndemonstrating the match of its non-equilibrium stationary distribution to the \ndata distribution. Source Code: <a href=\"http://github.com/anirudh9119/walkback_nips17\">this http URL</a> \n</p>"}, "author": "Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510106836167", "timestampUsec": "1510106836166580", "id": "tag:google.com,2005:reader/item/000000032e4743fd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?. (arXiv:1711.02301v1 [cs.AI])", "published": 1510106836, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02301"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02301", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c80d9d65\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c80d9d65&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep reinforcement learning has achieved many recent successes, but our \nunderstanding of its strengths and limitations is hampered by the lack of rich \nenvironments in which we can fully characterize optimal behavior, and \ncorrespondingly diagnose individual actions against such a characterization. \nHere we consider a family of combinatorial games, arising from work of Erdos, \nSelfridge, and Spencer, and we propose their use as environments for evaluating \nand comparing different approaches to reinforcement learning. These games have \na number of appealing features: they are challenging for current learning \napproaches, but they form (i) a low-dimensional, simply parametrized \nenvironment where (ii) there is a linear closed form solution for optimal \nbehavior from any state, and (iii) the difficulty of the game can be tuned by \nchanging environment parameters in an interpretable way. We use these \nErdos-Selfridge-Spencer games not only to compare different algorithms, but \nalso to compare approaches based on supervised and reinforcement learning, to \nanalyze the power of multi-agent approaches in improving performance, and to \nevaluate generalization to environments outside the training set. \n</p>"}, "author": "Maithra Raghu, Alex Irpan, Jacob Andreas, Robert Kleinberg, Quoc V. Le, Jon Kleinberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510106836167", "timestampUsec": "1510106836166579", "id": "tag:google.com,2005:reader/item/000000032e474400", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent Networks. (arXiv:1711.02326v1 [cs.AI])", "published": 1510106836, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02326"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02326", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A major drawback of backpropagation through time (BPTT) is the difficulty of \nlearning long-term dependencies, coming from having to propagate credit \ninformation backwards through every single step of the forward computation. \nThis makes BPTT both computationally impractical and biologically implausible. \nFor this reason, full backpropagation through time is rarely used on long \nsequences, and truncated backpropagation through time is used as a heuristic. \nHowever, this usually leads to biased estimates of the gradient in which longer \nterm dependencies are ignored. Addressing this issue, we propose an alternative \nalgorithm, Sparse Attentive Backtracking, which might also be related to \nprinciples used by brains to learn long-term dependencies. Sparse Attentive \nBacktracking learns an attention mechanism over the hidden states of the past \nand selectively backpropagates through paths with high attention weights. This \nallows the model to learn long term dependencies while only backtracking for a \nsmall number of time steps, not just from the recent past but also from \nattended relevant past states. \n</p>"}, "author": "Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Laurent Charlin, Chris Pal, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510106836167", "timestampUsec": "1510106836166578", "id": "tag:google.com,2005:reader/item/000000032e474405", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Beetle Antennae Search without Parameter Tuning (BAS-WPT) for Multi-objective Optimization. (arXiv:1711.02395v1 [cs.NE])", "published": 1510106836, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02395"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02395", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Beetle antennae search (BAS) is an efficient meta-heuristic algorithm \ninspired by foraging behaviors of beetles. This algorithm includes several \nparameters for tuning and the existing results are limited to solve single \nobjective optimization. This work pushes forward the research on BAS by \nproviding one variant that releases the tuning parameters and is able to handle \nmulti-objective optimization. This new approach applies normalization to \nsimplify the original algorithm and uses a penalty function to exploit \ninfeasible solutions with low constraint violation to solve the constraint \noptimization problem. Extensive experimental studies are carried out and the \nresults reveal efficacy of the proposed approach to constraint handling. \n</p>"}, "author": "Xiangyuan Jiang, Shuai Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510106836167", "timestampUsec": "1510106836166577", "id": "tag:google.com,2005:reader/item/000000032e47440b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Cortical microcircuits as gated-recurrent neural networks. (arXiv:1711.02448v1 [q-bio.NC])", "published": 1510106836, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02448"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02448", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Cortical circuits exhibit intricate recurrent architectures that are \nremarkably similar across different brain areas. Such stereotyped structure \nsuggests the existence of common computational principles. However, such \nprinciples have remained largely elusive. Inspired by gated-memory networks, \nnamely long short-term memory networks (LSTMs), we introduce a recurrent neural \nnetwork in which information is gated through inhibitory cells that are \nsubtractive (subLSTM). We propose a natural mapping of subLSTMs onto known \ncanonical excitatory-inhibitory cortical microcircuits. Our empirical \nevaluation across sequential image classification and language modelling tasks \nshows that subLSTM units can achieve similar performance to LSTM units. These \nresults suggest that cortical circuits can be optimised to solve complex \ncontextual problems and proposes a novel view on their computational function. \nOverall our work provides a step towards unifying recurrent networks as used in \nmachine learning with their biological counterparts. \n</p>"}, "author": "Rui Ponte Costa, Yannis M. Assael, Brendan Shillingford, Nando de Freitas, Tim P. Vogels", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188538", "id": "tag:google.com,2005:reader/item/000000032df4f63c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Mandolin: A Knowledge Discovery Framework for the Web of Data. (arXiv:1711.01283v1 [cs.DB])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01283"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01283", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Markov Logic Networks join probabilistic modeling with first-order logic and \nhave been shown to integrate well with the Semantic Web foundations. While \nseveral approaches have been devised to tackle the subproblems of rule mining, \ngrounding, and inference, no comprehensive workflow has been proposed so far. \nIn this paper, we fill this gap by introducing a framework called Mandolin, \nwhich implements a workflow for knowledge discovery specifically on RDF \ndatasets. Our framework imports knowledge from referenced graphs, creates \nsimilarity relationships among similar literals, and relies on state-of-the-art \ntechniques for rule mining, grounding, and inference computation. We show that \nour best configuration scales well and achieves at least comparable results \nwith respect to other statistical-relational-learning algorithms on link \nprediction. \n</p>"}, "author": "Tommaso Soru, Diego Esteves, Edgard Marx, Axel-Cyrille Ngonga Ngomo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188537", "id": "tag:google.com,2005:reader/item/000000032df4f645", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Discovering More Precise Process Models from Event Logs by Filtering Out Chaotic Activities. (arXiv:1711.01287v1 [cs.DB])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01287"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01287", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Process Discovery is concerned with the automatic generation of a process \nmodel that describes a business process from execution data of that business \nprocess. Real life event logs can contain chaotic activities. These activities \nare independent of the state of the process and can, therefore, happen at \nrather arbitrary points in time. We show that the presence of such chaotic \nactivities in an event log heavily impacts the quality of the process models \nthat can be discovered with process discovery techniques. The current modus \noperandi for filtering activities from event logs is to simply filter out \ninfrequent activities. We show that frequency-based filtering of activities \ndoes not solve the problems that are caused by chaotic activities. Moreover, we \npropose a novel technique to filter out chaotic activities from event logs. We \nevaluate this technique on a collection of seventeen real-life event logs that \noriginate from both the business process management domain and the smart home \nenvironment domain. As demonstrated, the developed activity filtering methods \nenable the discovery of process models that are more behaviorally specific \ncompared to process models that are discovered using standard frequency-based \nfiltering. \n</p>"}, "author": "Niek Tax, Natalia Sidorova, Wil M. P. van der Aalst", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188536", "id": "tag:google.com,2005:reader/item/000000032df4f64d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Decentralised firewall for malware detection. (arXiv:1711.01353v1 [cs.CR])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01353"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01353", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper describes the design and development of a decentralized firewall \nsystem powered by a novel malware detection engine. The firewall is built using \nblockchain technology. The detection engine aims to classify Portable \nExecutable (PE) files as malicious or benign. File classification is carried \nout using a deep belief neural network (DBN) as the detection engine. Our \napproach is to model the files as grayscale images and use the DBN to classify \nthose images into the aforementioned two classes. An extensive data set of \n10,000 files is used to train the DBN. Validation is carried out using 4,000 \nfiles previously unexposed to the network. The final result of whether to allow \nor block a file is obtained by arriving at a proof of work based consensus in \nthe blockchain network. \n</p>"}, "author": "Saurabh Raje, Shyamal Vaderia, Neil Wilson, Rudrakh Panigrahi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188535", "id": "tag:google.com,2005:reader/item/000000032df4f655", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Guiding the search in continuous state-action spaces by learning an action sampling distribution from off-target samples. (arXiv:1711.01391v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01391"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01391", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In robotics, it is essential to be able to plan efficiently in \nhigh-dimensional continuous state-action spaces for long horizons. For such \ncomplex planning problems, unguided uniform sampling of actions until a path to \na goal is found is hopelessly inefficient, and gradient-based approaches often \nfall short when the optimization manifold of a given problem is not smooth. In \nthis paper we present an approach that guides the search of a state-space \nplanner, such as A*, by learning an action-sampling distribution that can \ngeneralize across different instances of a planning problem. The motivation is \nthat, unlike typical learning approaches for planning for continuous action \nspace that estimate a policy, an estimated action sampler is more robust to \nerror since it has a planner to fall back on. We use a Generative Adversarial \nNetwork (GAN), and address an important issue: search experience consists of a \nrelatively large number of actions that are not on a solution path and a \nrelatively small number of actions that actually are on a solution path. We \nintroduce a new technique, based on an importance-ratio estimation method, for \nusing samples from a non-target distribution to make GAN learning more \ndata-efficient. We provide theoretical guarantees and empirical evaluation in \nthree challenging continuous robot planning problems to illustrate the \neffectiveness of our algorithm. \n</p>"}, "author": "Beomjoon Kim, Leslie Pack Kaelbling, Tomas Lozano-Perez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188534", "id": "tag:google.com,2005:reader/item/000000032df4f663", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Case for Meta-Cognitive Machine Learning: On Model Entropy and Concept Formation in Deep Learning. (arXiv:1711.01431v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01431"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning is usually defined in behaviourist terms, where external \nvalidation is the primary mechanism of learning. In this paper, I argue for a \nmore holistic interpretation in which finding more probable, efficient and \nabstract representations is as central to learning as performance. In other \nwords, machine learning should be extended with strategies to reason over its \nown learning process, leading to so-called meta-cognitive machine learning. As \nsuch, the de facto definition of machine learning should be reformulated in \nthese intrinsically multi-objective terms, taking into account not only the \ntask performance but also internal learning objectives. To this end, we suggest \na \"model entropy function\" to be defined that quantifies the efficiency of the \ninternal learning processes. It is conjured that the minimization of this model \nentropy leads to concept formation. Besides philosophical aspects, some initial \nillustrations are included to support the claims. \n</p>"}, "author": "Johan Loeckx", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188533", "id": "tag:google.com,2005:reader/item/000000032df4f672", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Searching for Biophysically Realistic Parameters for Dynamic Neuron Models by Genetic Algorithms from Calcium Imaging Recording. (arXiv:1711.01436v1 [q-bio.QM])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01436"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01436", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c80da0ec\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c80da0ec&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Individual Neurons in the nervous systems exploit various dynamics. To \ncapture these dynamics for single neurons, we tune the parameters of an \nelectrophysiological model of nerve cells, to fit experimental data obtained by \ncalcium imaging. A search for the biophysical parameters of this model is \nperformed by means of a genetic algorithm, where the model neuron is exposed to \na predefined input current representing overall inputs from other parts of the \nnervous system. The algorithm is then constrained for keeping the ion-channel \ncurrents within reasonable ranges, while producing the best fit to a calcium \nimaging time series of the AVA interneuron, from the brain of the soil-worm, C. \nelegans. Our settings enable us to project a set of biophysical parameters to \nthe the neuron kinetics observed in neuronal imaging. \n</p>"}, "author": "Magdalena Fuchs, Manuel Zimmer, Radu Grosu, Ramin M. Hasani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188532", "id": "tag:google.com,2005:reader/item/000000032df4f677", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Ensembles of Multiple Models and Architectures for Robust Brain Tumour Segmentation. (arXiv:1711.01468v1 [cs.CV])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01468"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01468", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning approaches such as convolutional neural nets have consistently \noutperformed previous methods on challenging tasks such as dense, semantic \nsegmentation. However, the various proposed networks perform differently, with \nbehaviour largely influenced by architectural choices and training settings. \nThis paper explores Ensembles of Multiple Models and Architectures (EMMA) for \nrobust performance through aggregation of predictions from a wide range of \nmethods. The approach reduces the influence of the meta-parameters of \nindividual models and the risk of overfitting the configuration to a particular \ndatabase. EMMA can be seen as an unbiased, generic deep learning model which is \nshown to yield excellent performance, winning the first position in the BRATS \n2017 competition among 50+ participating teams. \n</p>"}, "author": "Konstantinos Kamnitsas, Wenjia Bai, Enzo Ferrante, Steven McDonagh, Matthew Sinclair, Nick Pawlowski, Martin Rajchl, Matthew Lee, Bernhard Kainz, Daniel Rueckert, Ben Glocker", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188531", "id": "tag:google.com,2005:reader/item/000000032df4f681", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Composing Meta-Policies for Autonomous Driving Using Hierarchical Deep Reinforcement Learning. (arXiv:1711.01503v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01503"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01503", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Rather than learning new control policies for each new task, it is possible, \nwhen tasks share some structure, to compose a \"meta-policy\" from previously \nlearned policies. This paper reports results from experiments using Deep \nReinforcement Learning on a continuous-state, discrete-action autonomous \ndriving simulator. We explore how Deep Neural Networks can represent \nmeta-policies that switch among a set of previously learned policies, \nspecifically in settings where the dynamics of a new scenario are composed of a \nmixture of previously learned dynamics and where the state observation is \npossibly corrupted by sensing noise. We also report the results of experiments \nvarying dynamics mixes, distractor policies, magnitudes/distributions of \nsensing noise, and obstacles. In a fully observed experiment, the meta-policy \nlearning algorithm achieves 2.6x the reward achieved by the next best policy \ncomposition technique with 80% less exploration. In a partially observed \nexperiment, the meta-policy learning algorithm converges after 50 iterations \nwhile a direct application of RL fails to converge even after 200 iterations. \n</p>"}, "author": "Richard Liaw, Sanjay Krishnan, Animesh Garg, Daniel Crankshaw, Joseph E. Gonzalez, Ken Goldberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188530", "id": "tag:google.com,2005:reader/item/000000032df4f68f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Semantic Web Today: From Oil Rigs to Panama Papers. (arXiv:1711.01518v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01518"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01518", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The next leap on the internet has already started as Semantic Web. At its \ncore, Semantic Web transforms the document oriented web to a data oriented web \nenriched with semantics embedded as metadata. This change in perspective \ntowards the web offers numerous benefits for vast amount of data intensive \nindustries that are bound to the web and its related applications. The \nindustries are diverse as they range from Oil &amp; Gas exploration to the \ninvestigative journalism, and everything in between. This paper discusses eight \ndifferent industries which currently reap the benefits of Semantic Web. The \npaper also offers a future outlook into Semantic Web applications and discusses \nthe areas in which Semantic Web would play a key role in the future. \n</p>"}, "author": "Rivindu Perera, Parma Nand, Boris Bacic, Wen-Hsin Yang, Kazuhiro Seki, Radek Burget", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188529", "id": "tag:google.com,2005:reader/item/000000032df4f6a3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "HPX Smart Executors. (arXiv:1711.01519v1 [cs.DC])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01519"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01519", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The performance of many parallel applications depends on loop-level \nparallelism. However, manually parallelizing all loops may result in degrading \nparallel performance, as some of them cannot scale desirably to a large number \nof threads. In addition, the overheads of manually tuning loop parameters might \nprevent an application from reaching its maximum parallel performance. We \nillustrate how machine learning techniques can be applied to address these \nchallenges. In this research, we develop a framework that is able to \nautomatically capture the static and dynamic information of a loop. Moreover, \nwe advocate a novel method by introducing HPX smart executors for determining \nthe execution policy, chunk size, and prefetching distance of an HPX loop to \nachieve higher possible performance by feeding static information captured \nduring compilation and runtime-based dynamic information to our learning model. \nOur evaluated execution results show that using these smart executors can speed \nup the HPX execution process by around 12%-35% for the Matrix Multiplication, \nStream and $2D$ Stencil benchmarks compared to setting their HPX loop's \nexecution policy/parameters manually or using HPX auto-parallelization \ntechniques. \n</p>"}, "author": "Zahra Khatami, Lukas Troska, Hartmut Kaiser, J. Ramanujam, Adrian Serio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188528", "id": "tag:google.com,2005:reader/item/000000032df4f6ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fisher-Rao Metric, Geometry, and Complexity of Neural Networks. (arXiv:1711.01530v1 [cs.LG])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01530"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01530", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the relationship between geometry and capacity measures for deep \nneural networks from an invariance viewpoint. We introduce a new notion of \ncapacity --- the Fisher-Rao norm --- that possesses desirable invariance \nproperties and is motivated by Information Geometry. We discover an analytical \ncharacterization of the new capacity measure, through which we establish \nnorm-comparison inequalities and further show that the new measure serves as an \numbrella for several existing norm-based complexity measures. We discuss upper \nbounds on the generalization error induced by the proposed measure. Extensive \nnumerical experiments on CIFAR-10 support our theoretical findings. Our \ntheoretical analysis rests on a key structural lemma about partial derivatives \nof multi-layer rectifier networks. \n</p>"}, "author": "Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, James Stokes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188527", "id": "tag:google.com,2005:reader/item/000000032df4f6b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Double Q($\\sigma$) and Q($\\sigma, \\lambda$): Unifying Reinforcement Learning Control Algorithms. (arXiv:1711.01569v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01569"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01569", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Temporal-difference (TD) learning is an important field in reinforcement \nlearning. Sarsa and Q-Learning are among the most used TD algorithms. The \nQ($\\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper \nextends the Q($\\sigma$) algorithm to an online multi-step algorithm Q($\\sigma, \n\\lambda$) using eligibility traces and introduces Double Q($\\sigma$) as the \nextension of Q($\\sigma$) to double learning. Experiments suggest that the new \nQ($\\sigma, \\lambda$) algorithm can outperform the classical TD control methods \nSarsa($\\lambda$), Q($\\lambda$) and Q($\\sigma$). \n</p>"}, "author": "Markus Dumke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188526", "id": "tag:google.com,2005:reader/item/000000032df4f6c1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning. (arXiv:1711.01577v1 [stat.ML])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01577"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01577", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Long Short-Term Memory (LSTM) is a popular approach to boosting the ability \nof Recurrent Neural Networks to store longer term temporal information. The \ncapacity of an LSTM network can be increased by widening and adding layers. \nHowever, usually the former introduces additional parameters, while the latter \nincreases the runtime. As an alternative we propose the Tensorized LSTM in \nwhich the hidden states are represented by tensors and updated via a \ncross-layer convolution. By increasing the tensor size, the network can be \nwidened efficiently without additional parameters since the parameters are \nshared across different locations in the tensor; by delaying the output, the \nnetwork can be deepened implicitly with little additional runtime since deep \ncomputations for each timestep are merged into temporal computations of the \nsequence. Experiments conducted on five challenging sequence learning tasks \nshow the potential of the proposed model. \n</p>"}, "author": "Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188525", "id": "tag:google.com,2005:reader/item/000000032df4f6c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Strategies for Conceptual Change in Convolutional Neural Networks. (arXiv:1711.01634v1 [cs.LG])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01634"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01634", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A remarkable feature of human beings is their capacity for creative \nbehaviour, referring to their ability to react to problems in ways that are \nnovel, surprising, and useful. Transformational creativity is a form of \ncreativity where the creative behaviour is induced by a transformation of the \nactor's conceptual space, that is, the representational system with which the \nactor interprets its environment. In this report, we focus on ways of adapting \nsystems of learned representations as they switch from performing one task to \nperforming another. We describe an experimental comparison of multiple \nstrategies for adaptation of learned features, and evaluate how effectively \neach of these strategies realizes the adaptation, in terms of the amount of \ntraining, and in terms of their ability to cope with restricted availability of \ntraining data. We show, among other things, that across handwritten digits, \nnatural images, and classical music, adaptive strategies are systematically \nmore effective than a baseline method that starts learning from scratch. \n</p>"}, "author": "Maarten Grachten, Carlos Eduardo Cancino Chac&#xf3;n", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188524", "id": "tag:google.com,2005:reader/item/000000032df4f6d4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Multilingual Speech Recognition With A Single End-To-End Model. (arXiv:1711.01694v1 [eess.AS])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01694"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01694", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Training a conventional automatic speech recognition (ASR) system to support \nmultiple languages is challenging because the sub-word unit, lexicon and word \ninventories are typically language specific. In contrast, sequence-to-sequence \nmodels are well suited for multilingual ASR because they encapsulate an \nacoustic, pronunciation and language model jointly in a single network. In this \nwork we present a single sequence-to-sequence ASR model trained on 9 different \nIndian languages, which have very little overlap in their scripts. \nSpecifically, we take a union of language-specific grapheme sets and train a \ngrapheme-based sequence-to-sequence model jointly on data from all languages. \nWe find that this model, which is not explicitly given any information about \nlanguage identity, improves recognition performance by 21% relative compared to \nanalogous sequence-to-sequence models trained on each language individually. By \nmodifying the model to accept a language identifier as an additional input \nfeature, we further improve performance by an additional 7% relative and \neliminate confusion between different languages. \n</p>"}, "author": "Shubham Toshniwal, Tara N. Sainath, Ron J. Weiss, Bo Li, Pedro Moreno, Eugene Weinstein, Kanishka Rao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188523", "id": "tag:google.com,2005:reader/item/000000032df4f6de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "RoboCupSimData: A RoboCup soccer research dataset. (arXiv:1711.01703v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01703"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01703", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c80da46e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c80da46e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>RoboCup is an international scientific robot competition in which teams of \nmultiple robots compete against each other. Its different leagues provide many \nsources of robotics data, that can be used for further analysis and application \nof machine learning. This paper describes a large dataset from games of some of \nthe top teams (from 2016 and 2017) in RoboCup Soccer Simulation League (2D), \nwhere teams of 11 robots (agents) compete against each other. Overall, we used \n10 different teams to play each other, resulting in 45 unique pairings. For \neach pairing, we ran 25 matches (of 10mins), leading to 1125 matches or more \nthan 180 hours of game play. The generated CSV files are 17GB of data (zipped), \nor 229GB (unzipped). The dataset is unique in the sense that it contains both \nthe ground truth data (global, complete, noise-free information of all objects \non the field), as well as the noisy, local and incomplete percepts of each \nrobot. These data are made available as CSV files, as well as in the original \nsoccer simulator formats. \n</p>"}, "author": "Olivia Michael, Oliver Obst, Falk Schmidsberger, Frieder Stolzenburg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188522", "id": "tag:google.com,2005:reader/item/000000032df4f6e7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Coding-theorem Like Behaviour and Emergence of the Universal Distribution from Resource-bounded Algorithmic Probability. (arXiv:1711.01711v7 [cs.IT] UPDATED)", "published": 1511085973, "updated": 1511085973, "canonical": [{"href": "http://arxiv.org/abs/1711.01711"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01711", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8197534\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8197534&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Previously referred to as 'miraculous' because of its surprisingly powerful \nproperties and its application as the optimal theoretical solution to \ninduction/inference, (approximations to) Algorithmic Probability (AP) and the \nUniversal Distribution are of the greatest importance in computer science and \nscience in general. Here we investigate the emergence, the rates of emergence \nand convergence, and the Coding-theorem like behaviour of AP in subuniversal \nmodels of computation. We investigate empirical distributions of computer \nprograms of weaker computational power according to the Chomsky hierarchy. We \nintroduce measures of algorithmic probability and algorithmic complexity based \nupon resource-bounded computation, in contrast to previously thoroughly \ninvestigated distributions produced from the output distribution of Turing \nmachines. This approach allows for numerical approximations to algorithmic \n(Kolmogorov-Chaitin) complexity-based estimations at each of the levels of a \ncomputational hierarchy. We demonstrate that all these estimations are \ncorrelated in rank and that they converge both in rank and values as a function \nof computational power, despite the fundamental differences of each \ncomputational model. In the context of natural processes that may operate below \nthe Turing universal level due to the constraint of resources and physical \ndegradation, the investigation of natural biases coming from algorithmic laws \nis highly relevant. We show that the simplicity/complexity bias in \ndistributions produced even by the weakest of the computational models can be \naccounted up to 60% by Algorithmic Probability in its approximation to the \nUniversal Distribution. \n</p>"}, "author": "Hector Zenil, Liliana Badillo, Santiago Hern&#xe1;ndez-Orozco, Francisco Hern&#xe1;ndez-Quiroz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188521", "id": "tag:google.com,2005:reader/item/000000032df4f6f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "KGAN: How to Break The Minimax Game in GAN. (arXiv:1711.01744v1 [cs.LG])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01744"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01744", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative Adversarial Networks (GANs) were intuitively and attractively \nexplained under the perspective of game theory, wherein two involving parties \nare a discriminator and a generator. In this game, the task of the \ndiscriminator is to discriminate the real and generated (i.e., fake) data, \nwhilst the task of the generator is to generate the fake data that maximally \nconfuses the discriminator. In this paper, we propose a new viewpoint for GANs, \nwhich is termed as the minimizing general loss viewpoint. This viewpoint shows \na connection between the general loss of a classification problem regarding a \nconvex loss function and a f-divergence between the true and fake data \ndistributions. Mathematically, we proposed a setting for the classification \nproblem of the true and fake data, wherein we can prove that the general loss \nof this classification problem is exactly the negative f-divergence for a \ncertain convex function f. This allows us to interpret the problem of learning \nthe generator for dismissing the f-divergence between the true and fake data \ndistributions as that of maximizing the general loss which is equivalent to the \nmin-max problem in GAN if the Logistic loss is used in the classification \nproblem. However, this viewpoint strengthens GANs in two ways. First, it allows \nus to employ any convex loss function for the discriminator. Second, it \nsuggests that rather than limiting ourselves in NN-based discriminators, we can \nalternatively utilize other powerful families. Bearing this viewpoint, we then \npropose using the kernel-based family for discriminators. This family has two \nappealing features: i) a powerful capacity in classifying non-linear nature \ndata and ii) being convex in the feature space. Using the convexity of this \nfamily, we can further develop Fenchel duality to equivalently transform the \nmax-min problem to the max-max dual problem. \n</p>"}, "author": "Trung Le, Tu Dinh Nguyen, Dinh Phung", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188520", "id": "tag:google.com,2005:reader/item/000000032df4f6f5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Solving Procedure for Artificial Neural Network. (arXiv:1711.01754v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01754"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01754", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is expected that progress toward true artificial intelligence will be \nachieved through the emergence of a system that integrates representation \nlearning and complex reasoning (LeCun et al. 2015). In response to this \nprediction, research has been conducted on implementing the symbolic reasoning \nof a von Neumann computer in an artificial neural network (Graves et al. 2016; \nGraves et al. 2014; Reed et al. 2015). However, these studies have many \nlimitations in realizing neural-symbolic integration (Jaeger. 2016). Here, we \npresent a new learning paradigm: a learning solving procedure (LSP) that learns \nthe procedure for solving complex problems. This is not accomplished merely by \nlearning input-output data, but by learning algorithms through a solving \nprocedure that obtains the output as a sequence of tasks for a given input \nproblem. The LSP neural network system not only learns simple problems of \naddition and multiplication, but also the algorithms of complicated problems, \nsuch as complex arithmetic expression, sorting, and Hanoi Tower. To realize \nthis, the LSP neural network structure consists of a deep neural network and \nlong short-term memory, which are recursively combined. Through \nexperimentation, we demonstrate the efficiency and scalability of LSP and its \nvalidity as a mechanism of complex reasoning. \n</p>"}, "author": "Ju-Hong Lee, Moon-Ju Kang, Bumghi Choi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188519", "id": "tag:google.com,2005:reader/item/000000032df4f6fd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Foundry of Human Activities and Infrastructures. (arXiv:1711.01927v1 [cs.AI])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01927"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01927", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Direct representation knowledgebases can enhance and even provide an \nalternative to document-centered digital libraries. Here we consider realist \nsemantic modeling of everyday activities and infrastructures in such \nknowledgebases. Because we want to integrate a wide variety of topics, a \ncollection of ontologies (a foundry) and a range of other knowledge resources \nare needed. We first consider modeling the routine procedures that support \nhuman activities and technologies. Next, we examine the interactions of \ntechnologies with aspects of social organization. Then, we consider approaches \nand issues for developing and validating explanations of the relationships \namong various entities. \n</p>"}, "author": "Robert B. Allen, Eunsang Yang, Tatsawan Timakum", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188518", "id": "tag:google.com,2005:reader/item/000000032df4f701", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Hi, how can I help you?: Automating enterprise IT support help desks. (arXiv:1711.02012v1 [cs.CL])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02012"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02012", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Question answering is one of the primary challenges of natural language \nunderstanding. In realizing such a system, providing complex long answers to \nquestions is a challenging task as opposed to factoid answering as the former \nneeds context disambiguation. The different methods explored in the literature \ncan be broadly classified into three categories namely: 1) classification \nbased, 2) knowledge graph based and 3) retrieval based. Individually, none of \nthem address the need of an enterprise wide assistance system for an IT support \nand maintenance domain. In this domain the variance of answers is large ranging \nfrom factoid to structured operating procedures; the knowledge is present \nacross heterogeneous data sources like application specific documentation, \nticket management systems and any single technique for a general purpose \nassistance is unable to scale for such a landscape. To address this, we have \nbuilt a cognitive platform with capabilities adopted for this domain. Further, \nwe have built a general purpose question answering system leveraging the \nplatform that can be instantiated for multiple products, technologies in the \nsupport domain. The system uses a novel hybrid answering model that \norchestrates across a deep learning classifier, a knowledge graph based context \ndisambiguation module and a sophisticated bag-of-words search system. This \norchestration performs context switching for a provided question and also does \na smooth hand-off of the question to a human expert if none of the automated \ntechniques can provide a confident answer. This system has been deployed across \n675 internal enterprise IT support and maintenance projects. \n</p>"}, "author": "Senthil Mani, Neelamadhav Gantayat, Rahul Aralikatte, Monika Gupta, Sampath Dechu, Anush Sankaran, Shreya Khare, Barry Mitchell, Hemamalini Subramanian, Hema Venkatarangan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188517", "id": "tag:google.com,2005:reader/item/000000032df4f717", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon. (arXiv:1711.02013v1 [cs.CL])", "published": 1510067299, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02013"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02013", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a neural language model capable of unsupervised syntactic \nstructure induction. The model leverages the structure information to form \nbetter semantic representations and better language modeling. Standard \nrecurrent neural networks are limited by their structure and fail to \nefficiently use syntactic information. On the other hand, tree-structured \nrecursive networks usually require additional structural supervision at the \ncost of human expert annotation. In this paper, We propose a novel neural \nlanguage model, called the Parsing-Reading-Predict Networks (PRPN), that can \nsimultaneously induce the syntactic structure from unannotated sentences and \nleverage the inferred structure to learn a better language model. In our model, \nthe gradient can be directly back-propagated from the language model loss into \nthe neural parsing network. Experiments show that the proposed model can \ndiscover the underlying syntactic structure and achieve state-of-the-art \nperformance on word/character-level language model tasks. \n</p>"}, "author": "Yikang Shen, Zhouhan Lin, Chin-Wei Huang, Aaron Courville", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510067299189", "timestampUsec": "1510067299188516", "id": "tag:google.com,2005:reader/item/000000032df4f721", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm. (arXiv:1711.02017v2 [cs.NE] UPDATED)", "published": 1511265636, "updated": 1511265658, "canonical": [{"href": "http://arxiv.org/abs/1711.02017"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02017", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks (NNs) have begun to have a pervasive impact on various \napplications of machine learning. However, the problem of finding an optimal NN \narchitecture for large applications has remained open for several decades. \nConventional approaches search for the optimal NN architecture through \nextensive trial-and-error. Such a procedure is quite inefficient. In addition, \nthe generated NN architectures incur substantial redundancy. To address these \nproblems, we propose an NN synthesis tool (NeST) that automatically generates \nvery compact architectures for a given dataset. NeST starts with a seed NN \narchitecture. It iteratively tunes the architecture with gradient-based growth \nand magnitude-based pruning of neurons and connections. Our experimental \nresults show that NeST yields accurate yet very compact NNs with a wide range \nof seed architecture selection. For example, for the LeNet-300-100 (LeNet-5) NN \narchitecture derived from the MNIST dataset, we reduce network parameters by \n34.1x (74.3x) and floating-point operations (FLOPs) by 35.8x (43.7x). For the \nAlexNet NN architecture derived from the ImageNet dataset, we reduce network \nparameters by 15.7x and FLOPs by 4.6x. All these results are the current \nstate-of-the-art for these architectures. \n</p>"}, "author": "Xiaoliang Dai, Hongxu Yin, Niraj K. Jha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510063178493", "timestampUsec": "1510063178493371", "id": "tag:google.com,2005:reader/item/000000032dea80ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Accelerating Training of Deep Neural Networks via Sparse Edge Processing. (arXiv:1711.01343v1 [cs.NE])", "published": 1510063179, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01343"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01343", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a reconfigurable hardware architecture for deep neural networks \n(DNNs) capable of online training and inference, which uses algorithmically \npre-determined, structured sparsity to significantly lower memory and \ncomputational requirements. This novel architecture introduces the notion of \nedge-processing to provide flexibility and combines junction pipelining and \noperational parallelization to speed up training. The overall effect is to \nreduce network complexity by factors up to 30x and training time by up to 35x \nrelative to GPUs, while maintaining high fidelity of inference results. This \nhas the potential to enable extensive parameter searches and development of the \nlargely unexplored theoretical foundation of DNNs. The architecture \nautomatically adapts itself to different network sizes given available hardware \nresources. As proof of concept, we show results obtained for different bit \nwidths. \n</p>"}, "author": "Sourya Dey, Yinan Shao, Keith M. Chugg, Peter A. Beerel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510063178493", "timestampUsec": "1510063178493370", "id": "tag:google.com,2005:reader/item/000000032dea80d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Language as a matrix product state. (arXiv:1711.01416v1 [cs.CL])", "published": 1510063179, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01416"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01416", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a statistical model for natural language that begins by \nconsidering language as a monoid, then representing it in complex matrices with \na compatible translation invariant probability measure. We interpret the \nprobability measure as arising via the Born rule from a translation invariant \nmatrix product state. \n</p>"}, "author": "Vasily Pestun, John Terilla, Yiannis Vlassopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510063178493", "timestampUsec": "1510063178493369", "id": "tag:google.com,2005:reader/item/000000032dea80dd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Searching for Biophysically Realistic Parameters for Dynamic Neuron Models by Genetic Algorithms from Calcium Imaging Recording. (arXiv:1711.01436v1 [q-bio.QM])", "published": 1510063179, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01436"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01436", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c81978fd\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c81978fd&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Individual Neurons in the nervous systems exploit various dynamics. To \ncapture these dynamics for single neurons, we tune the parameters of an \nelectrophysiological model of nerve cells, to fit experimental data obtained by \ncalcium imaging. A search for the biophysical parameters of this model is \nperformed by means of a genetic algorithm, where the model neuron is exposed to \na predefined input current representing overall inputs from other parts of the \nnervous system. The algorithm is then constrained for keeping the ion-channel \ncurrents within reasonable ranges, while producing the best fit to a calcium \nimaging time series of the AVA interneuron, from the brain of the soil-worm, C. \nelegans. Our settings enable us to project a set of biophysical parameters to \nthe the neuron kinetics observed in neuronal imaging. \n</p>"}, "author": "Magdalena Fuchs, Manuel Zimmer, Radu Grosu, Ramin M. Hasani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510063178493", "timestampUsec": "1510063178493368", "id": "tag:google.com,2005:reader/item/000000032dea80e7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning. (arXiv:1711.01577v1 [stat.ML])", "published": 1510063179, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01577"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01577", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Long Short-Term Memory (LSTM) is a popular approach to boosting the ability \nof Recurrent Neural Networks to store longer term temporal information. The \ncapacity of an LSTM network can be increased by widening and adding layers. \nHowever, usually the former introduces additional parameters, while the latter \nincreases the runtime. As an alternative we propose the Tensorized LSTM in \nwhich the hidden states are represented by tensors and updated via a \ncross-layer convolution. By increasing the tensor size, the network can be \nwidened efficiently without additional parameters since the parameters are \nshared across different locations in the tensor; by delaying the output, the \nnetwork can be deepened implicitly with little additional runtime since deep \ncomputations for each timestep are merged into temporal computations of the \nsequence. Experiments conducted on five challenging sequence learning tasks \nshow the potential of the proposed model. \n</p>"}, "author": "Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510063178493", "timestampUsec": "1510063178493367", "id": "tag:google.com,2005:reader/item/000000032dea80f2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm. (arXiv:1711.02017v1 [cs.NE])", "published": 1510063179, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02017"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02017", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks (NNs) have begun to have a pervasive impact on various \napplications of machine learning. However, the problem of finding an optimal NN \narchitecture for large applications has remained open for several decades. \nConventional approaches search for the optimal NN architecture through \nextensive trial-and-error. Such a procedure is quite inefficient. In addition, \nthe generated NN architectures incur substantial redundancy. To address these \nproblems, we propose an NN synthesis tool (NeST) that automatically generates \nvery compact architectures for a given dataset. NeST starts with a seed NN \narchitecture. It iteratively tunes the architecture with gradient-based growth \nand magnitude-based pruning of neurons and connections. Our experimental \nresults show that NeST yields accurate yet very compact NNs with a wide range \nof seed architecture selection. For example, for the LeNet-300-100 (LeNet-5) NN \narchitecture derived from the MNIST dataset, we reduce network parameters by \n34.1x (74.3x) and floating-point operations (FLOPs) by 35.8x (43.7x). For the \nAlexNet NN architecture derived from the ImageNet dataset, we reduce network \nparameters by 15.7x and FLOPs by 4.6x. All these results are the current \nstate-of-the-art for these architectures. \n</p>"}, "author": "Xiaoliang Dai, Hongxu Yin, Niraj K. Jha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663570", "id": "tag:google.com,2005:reader/item/000000032dc77689", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Implicit Weight Uncertainty in Neural Networks. (arXiv:1711.01297v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01297"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01297", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We interpret HyperNetworks within the framework of variational inference \nwithin implicit distributions. Our method, Bayes by Hypernet, is able to model \na richer variational distribution than previous methods. Experiments show that \nit achieves comparable predictive performance on the MNIST classification task \nwhile providing higher predictive uncertainties compared to MC-Dropout and \nregular maximum likelihood training. \n</p>"}, "author": "Nick Pawlowski, Martin Rajchl, Ben Glocker", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663569", "id": "tag:google.com,2005:reader/item/000000032dc776ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "NeuralFDR: Learning Discovery Thresholds from Hypothesis Features. (arXiv:1711.01312v4 [stat.ME] UPDATED)", "published": 1511308877, "updated": 1511308893, "canonical": [{"href": "http://arxiv.org/abs/1711.01312"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01312", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>As datasets grow richer, an important challenge is to leverage the full \nfeatures in the data to maximize the number of useful discoveries while \ncontrolling for false positives. We address this problem in the context of \nmultiple hypotheses testing, where for each hypothesis, we observe a p-value \nalong with a set of features specific to that hypothesis. For example, in \ngenetic association studies, each hypothesis tests the correlation between a \nvariant and the trait. We have a rich set of features for each variant (e.g. \nits location, conservation, epigenetics etc.) which could inform how likely the \nvariant is to have a true association. However popular testing approaches, such \nas Benjamini-Hochberg's procedure (BH) and independent hypothesis weighting \n(IHW), either ignore these features or assume that the features are categorical \nor uni-variate. We propose a new algorithm, NeuralFDR, which automatically \nlearns a discovery threshold as a function of all the hypothesis features. We \nparametrize the discovery threshold as a neural network, which enables flexible \nhandling of multi-dimensional discrete and continuous features as well as \nefficient end-to-end optimization. We prove that NeuralFDR has strong false \ndiscovery rate (FDR) guarantees, and show that it makes substantially more \ndiscoveries in synthetic and real datasets. Moreover, we demonstrate that the \nlearned discovery threshold is directly interpretable. \n</p>"}, "author": "Fei Xia, Martin J. Zhang, James Zou, David Tse", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663568", "id": "tag:google.com,2005:reader/item/000000032dc7777c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalized Linear Model Regression under Distance-to-set Penalties. (arXiv:1711.01341v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01341"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01341", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Estimation in generalized linear models (GLM) is complicated by the presence \nof constraints. One can handle constraints by maximizing a penalized \nlog-likelihood. Penalties such as the lasso are effective in high dimensions, \nbut often lead to unwanted shrinkage. This paper explores instead penalizing \nthe squared distance to constraint sets. Distance penalties are more flexible \nthan algebraic and regularization penalties, and avoid the drawback of \nshrinkage. To optimize distance penalized objectives, we make use of the \nmajorization-minimization principle. Resulting algorithms constructed within \nthis framework are amenable to acceleration and come with global convergence \nguarantees. Applications to shape constraints, sparse regression, and \nrank-restricted matrix regression on synthetic and real data showcase strong \nempirical performance, even under non-convex constraints. \n</p>"}, "author": "Jason Xu, Eric C. Chi, Kenneth Lange", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663567", "id": "tag:google.com,2005:reader/item/000000032dc77819", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Automatic Differentiation for Tensor Algebras. (arXiv:1711.01348v1 [cs.SC])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01348"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01348", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Kjolstad et. al. proposed a tensor algebra compiler. It takes expressions \nthat define a tensor element-wise, such as $f_{ij}(a,b,c,d) = \n\\exp\\left[-\\sum_{k=0}^4 \\left((a_{ik}+b_{jk})^2\\, c_{ii} + d_{i+k}^3 \\right) \n\\right]$, and generates the corresponding compute kernel code. \n</p> \n<p>For machine learning, especially deep learning, it is often necessary to \ncompute the gradient of a loss function $l(a,b,c,d)=l(f(a,b,c,d))$ with respect \nto parameters $a,b,c,d$. If tensor compilers are to be applied in this field, \nit is necessary to derive expressions for the derivatives of element-wise \ndefined tensors, i.e. expressions for $(da)_{ik}=\\partial l/\\partial a_{ik}$. \n</p> \n<p>When the mapping between function indices and argument indices is not 1:1, \nspecial attention is required. For the function $f_{ij} (x) = x_i^2$, the \nderivative of the loss is $(dx)_i=\\partial l/\\partial x_i=\\sum_j \n(df)_{ij}2x_i$; the sum is necessary because index $j$ does not appear in the \nindices of $f$. Another example is $f_{i}(x)=x_{ii}^2$, where $x$ is a matrix; \nhere we have $(dx)_{ij}=\\delta_{ij}(df)_i2x_{ii}$; the Kronecker delta is \nnecessary because the derivative is zero for off-diagonal elements. Another \nindexing scheme is used by $f_{ij}(x)=\\exp x_{i+j}$; here the correct \nderivative is $(dx)_{k}=\\sum_i (df)_{i,k-i} \\exp x_{k}$, where the range of the \nsum must be chosen appropriately. \n</p> \n<p>In this publication we present an algorithm that can handle any case in which \nthe indices of an argument are an arbitrary linear combination of the indices \nof the function, thus all the above examples can be handled. Sums (and their \nranges) and Kronecker deltas are automatically inserted into the derivatives as \nnecessary. Additionally, the indices are transformed, if required (as in the \nlast example). The algorithm outputs a symbolic expression that can be \nsubsequently fed into a tensor algebra compiler. \n</p> \n<p>Source code is provided. \n</p>"}, "author": "Sebastian Urban, Patrick van der Smagt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663566", "id": "tag:google.com,2005:reader/item/000000032dc778e7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Language as a matrix product state. (arXiv:1711.01416v1 [cs.CL])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01416"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01416", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a statistical model for natural language that begins by \nconsidering language as a monoid, then representing it in complex matrices with \na compatible translation invariant probability measure. We interpret the \nprobability measure as arising via the Born rule from a translation invariant \nmatrix product state. \n</p>"}, "author": "Vasily Pestun, John Terilla, Yiannis Vlassopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663565", "id": "tag:google.com,2005:reader/item/000000032dc77924", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Case for Meta-Cognitive Machine Learning: On Model Entropy and Concept Formation in Deep Learning. (arXiv:1711.01431v1 [cs.AI])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01431"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning is usually defined in behaviourist terms, where external \nvalidation is the primary mechanism of learning. In this paper, I argue for a \nmore holistic interpretation in which finding more probable, efficient and \nabstract representations is as central to learning as performance. In other \nwords, machine learning should be extended with strategies to reason over its \nown learning process, leading to so-called meta-cognitive machine learning. As \nsuch, the de facto definition of machine learning should be reformulated in \nthese intrinsically multi-objective terms, taking into account not only the \ntask performance but also internal learning objectives. To this end, we suggest \na \"model entropy function\" to be defined that quantifies the efficiency of the \ninternal learning processes. It is conjured that the minimization of this model \nentropy leads to concept formation. Besides philosophical aspects, some initial \nillustrations are included to support the claims. \n</p>"}, "author": "Johan Loeckx", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663564", "id": "tag:google.com,2005:reader/item/000000032dc77966", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distribution-Preserving k-Anonymity. (arXiv:1711.01514v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01514"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01514", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Preserving the privacy of individuals by protecting their sensitive \nattributes is an important consideration during microdata release. However, it \nis equally important to preserve the quality or utility of the data for at \nleast some targeted workloads. We propose a novel framework for privacy \npreservation based on the k-anonymity model that is ideally suited for \nworkloads that require preserving the probability distribution of the \nquasi-identifier variables in the data. Our framework combines the principles \nof distribution-preserving quantization and k-member clustering, and we \nspecialize it to two variants that respectively use intra-cluster and Gaussian \ndithering of cluster centers to achieve distribution preservation. We perform \ntheoretical analysis of the proposed schemes in terms of distribution \npreservation, and describe their utility in workloads such as covariate shift \nand transfer learning where such a property is necessary. Using extensive \nexperiments on real-world Medical Expenditure Panel Survey data, we demonstrate \nthe merits of our algorithms over standard k-anonymization for a hallmark \nhealth care application where an insurance company wishes to understand the \nrisk in entering a new market. Furthermore, by empirically quantifying the \nreidentification risk, we also show that the proposed approaches indeed \nmaintain k-anonymity. \n</p>"}, "author": "Dennis Wei, Karthikeyan Natesan Ramamurthy, Kush R. Varshney", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663563", "id": "tag:google.com,2005:reader/item/000000032dc779ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fisher-Rao Metric, Geometry, and Complexity of Neural Networks. (arXiv:1711.01530v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01530"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01530", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8197c5b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8197c5b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study the relationship between geometry and capacity measures for deep \nneural networks from an invariance viewpoint. We introduce a new notion of \ncapacity --- the Fisher-Rao norm --- that possesses desirable invariance \nproperties and is motivated by Information Geometry. We discover an analytical \ncharacterization of the new capacity measure, through which we establish \nnorm-comparison inequalities and further show that the new measure serves as an \numbrella for several existing norm-based complexity measures. We discuss upper \nbounds on the generalization error induced by the proposed measure. Extensive \nnumerical experiments on CIFAR-10 support our theoretical findings. Our \ntheoretical analysis rests on a key structural lemma about partial derivatives \nof multi-layer rectifier networks. \n</p>"}, "author": "Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, James Stokes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663562", "id": "tag:google.com,2005:reader/item/000000032dc77a35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Wasserstein Auto-Encoders. (arXiv:1711.01558v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01558"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01558", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c828dabc\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c828dabc&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building \na generative model of the data distribution. WAE minimizes a penalized form of \nthe Wasserstein distance between the model distribution and the target \ndistribution, which leads to a different regularizer than the one used by the \nVariational Auto-Encoder (VAE). This regularizer encourages the encoded \ntraining distribution to match the prior. We compare our algorithm with several \nother techniques and show that it is a generalization of adversarial \nauto-encoders (AAE). Our experiments show that WAE shares many of the \nproperties of VAEs (stable training, encoder-decoder architecture, nice latent \nmanifold structure) while generating samples of better quality, as measured by \nthe FID score. \n</p>"}, "author": "Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, Bernhard Schoelkopf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663561", "id": "tag:google.com,2005:reader/item/000000032dc77a98", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Submodular Maximization: The Case of Coverage Functions. (arXiv:1711.01566v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01566"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01566", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stochastic optimization of continuous objectives is at the heart of modern \nmachine learning. However, many important problems are of discrete nature and \noften involve submodular objectives. We seek to unleash the power of stochastic \ncontinuous optimization, namely stochastic gradient descent and its variants, \nto such discrete problems. We first introduce the problem of stochastic \nsubmodular optimization, where one needs to optimize a submodular objective \nwhich is given as an expectation. Our model captures situations where the \ndiscrete objective arises as an empirical risk (e.g., in the case of \nexemplar-based clustering), or is given as an explicit stochastic model (e.g., \nin the case of influence maximization in social networks). By exploiting that \ncommon extensions act linearly on the class of submodular functions, we employ \nprojected stochastic gradient ascent and its variants in the continuous domain, \nand perform rounding to obtain discrete solutions. We focus on the rich and \nwidely used family of weighted coverage functions. We show that our approach \nyields solutions that are guaranteed to match the optimal approximation \nguarantees, while reducing the computational cost by several orders of \nmagnitude, as we demonstrate empirically. \n</p>"}, "author": "Mohammad Reza Karimi, Mario Lucic, Hamed Hassani, Andreas Krause", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663560", "id": "tag:google.com,2005:reader/item/000000032dc77af0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Double Q($\\sigma$) and Q($\\sigma, \\lambda$): Unifying Reinforcement Learning Control Algorithms. (arXiv:1711.01569v1 [cs.AI])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01569"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01569", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Temporal-difference (TD) learning is an important field in reinforcement \nlearning. Sarsa and Q-Learning are among the most used TD algorithms. The \nQ($\\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper \nextends the Q($\\sigma$) algorithm to an online multi-step algorithm Q($\\sigma, \n\\lambda$) using eligibility traces and introduces Double Q($\\sigma$) as the \nextension of Q($\\sigma$) to double learning. Experiments suggest that the new \nQ($\\sigma, \\lambda$) algorithm can outperform the classical TD control methods \nSarsa($\\lambda$), Q($\\lambda$) and Q($\\sigma$). \n</p>"}, "author": "Markus Dumke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663559", "id": "tag:google.com,2005:reader/item/000000032dc77bac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/starred", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning. (arXiv:1711.01577v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01577"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01577", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Long Short-Term Memory (LSTM) is a popular approach to boosting the ability \nof Recurrent Neural Networks to store longer term temporal information. The \ncapacity of an LSTM network can be increased by widening and adding layers. \nHowever, usually the former introduces additional parameters, while the latter \nincreases the runtime. As an alternative we propose the Tensorized LSTM in \nwhich the hidden states are represented by tensors and updated via a \ncross-layer convolution. By increasing the tensor size, the network can be \nwidened efficiently without additional parameters since the parameters are \nshared across different locations in the tensor; by delaying the output, the \nnetwork can be deepened implicitly with little additional runtime since deep \ncomputations for each timestep are merged into temporal computations of the \nsequence. Experiments conducted on five challenging sequence learning tasks \nshow the potential of the proposed model. \n</p>"}, "author": "Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663558", "id": "tag:google.com,2005:reader/item/000000032dc77c29", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multilayer tensor factorization with applications to recommender systems. (arXiv:1711.01598v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01598"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01598", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recommender systems have been widely adopted by electronic commerce and \nentertainment industries for individualized prediction and recommendation, \nwhich benefit consumers and improve business intelligence. In this article, we \npropose an innovative method, namely the recommendation engine of multilayers \n(REM), for tensor recommender systems. The proposed method utilizes the \nstructure of a tensor response to integrate information from multiple modes, \nand creates an additional layer of nested latent factors to accommodate \nbetween-subjects dependency. One major advantage is that the proposed method is \nable to address the \"cold-start\" issue in the absence of information from new \ncustomers, new products or new contexts. Specifically, it provides more \neffective recommendations through sub-group information. To achieve scalable \ncomputation, we develop a new algorithm for the proposed method, which \nincorporates a maximum block improvement strategy into the cyclic \nblockwise-coordinate-descent algorithm. In theory, we investigate both \nalgorithmic properties for global and local convergence, along with the \nasymptotic consistency of estimated parameters. Finally, the proposed method is \napplied in simulations and IRI marketing data with 116 million observations of \nproduct sales. Numerical studies demonstrate that the proposed method \noutperforms existing competitors in the literature. \n</p>"}, "author": "Xuan Bi, Annie Qu, Xiaotong Shen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663557", "id": "tag:google.com,2005:reader/item/000000032dc77c8d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Approximating Partition Functions in Constant Time. (arXiv:1711.01655v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01655"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01655", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study approximations of the partition function of dense graphical models. \nPartition functions of graphical models play a fundamental role is statistical \nphysics, in statistics and in machine learning. Two of the main methods for \napproximating the partition function are Markov Chain Monte Carlo and \nVariational Methods. An impressive body of work in mathematics, physics and \ntheoretical computer science provides conditions under which Markov Chain Monte \nCarlo methods converge in polynomial time. These methods often lead to \npolynomial time approximation algorithms for the partition function in cases \nwhere the underlying model exhibits correlation decay. There are very few \ntheoretical guarantees for the performance of variational methods. One \nexception is recent results by Risteski (2016) who considered dense graphical \nmodels and showed that using variational methods, it is possible to find an \n$O(\\epsilon n)$ additive approximation to the log partition function in time \n$n^{O(1/\\epsilon^2)}$ even in a regime where correlation decay does not hold. \n</p> \n<p>We show that under essentially the same conditions, an $O(\\epsilon n)$ \nadditive approximation of the log partition function can be found in constant \ntime, independent of $n$. In particular, our results cover dense Ising and \nPotts models as well as dense graphical models with $k$-wise interaction. They \nalso apply for low threshold rank models. \n</p> \n<p>To the best of our knowledge, our results are the first to give a constant \ntime approximation to log partition functions and the first to use the \nalgorithmic regularity lemma for estimating partition functions. As an \napplication of our results we derive a constant time algorithm for \napproximating the magnetization of Ising and Potts model on dense graphs. \n</p>"}, "author": "Vishesh Jain, Frederic Koehler, Elchanan Mossel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663556", "id": "tag:google.com,2005:reader/item/000000032dc77cd9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Provenance and Pseudo-Provenance for Seeded Learning-Based Automated Test Generation. (arXiv:1711.01661v2 [stat.ML] UPDATED)", "published": 1510859309, "updated": 1510859315, "canonical": [{"href": "http://arxiv.org/abs/1711.01661"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01661", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many methods for automated software test generation, including some that \nexplicitly use machine learning (and some that use ML more broadly conceived) \nderive new tests from existing tests (often referred to as seeds). Often, the \nseed tests from which new tests are derived are manually constructed, or at \nleast simpler than the tests that are produced as the final outputs of such \ntest generators. We propose annotation of generated tests with a provenance \n(trail) showing how individual generated tests of interest (especially failing \ntests) derive from seed tests, and how the population of generated tests \nrelates to the original seed tests. In some cases, post-processing of generated \ntests can invalidate provenance information, in which case we also propose a \nmethod for attempting to construct \"pseudo-provenance\" describing how the tests \ncould have been (partly) generated from seeds. \n</p>"}, "author": "Alex Groce, Josie Holmes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663555", "id": "tag:google.com,2005:reader/item/000000032dc77d67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Estimation of Low-Rank Matrices via Approximate Message Passing. (arXiv:1711.01682v1 [math.ST])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01682"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01682", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Consider the problem of estimating a low-rank symmetric matrix when its \nentries are perturbed by Gaussian noise, a setting that is also known as \n`spiked model' or `deformed Wigner matrix.' If the empirical distribution of \nthe entries of the spikes is known, optimal estimators that exploit this \nknowledge can substantially outperform simple spectral approaches. Recent work \ncharacterizes the asymptotic accuracy of Bayes-optimal estimators in the \nhigh-dimensional limit. In this paper we present a practical algorithm that can \nachieve Bayes-optimal accuracy above the spectral threshold. A bold conjecture \nfrom statistical physics posits that no polynomial-time algorithm achieves \noptimal error below the same threshold (unless the best estimator is trivial). \n</p> \n<p>Our approach uses Approximate Message Passing (AMP) in conjunction with a \nspectral initialization. AMP algorithms have proved successful in a variety of \nstatistical estimation tasks, and are amenable to exact asymptotic analysis via \nstate evolution. Unfortunately, state evolution is uninformative when the \nalgorithm is initialized near an unstable fixed point, as is often happens in \nlow-rank matrix estimation problems. We develop a a new analysis of AMP that \nallows for spectral initializations, and builds on a decoupling between the \noutlier eigenvectors and the bulk in the spiked random matrix model. \n</p> \n<p>Our main theorem is general and applies beyond matrix estimation. However, we \nuse it to derive detailed predictions for the problem of estimating a rank-one \nmatrix in noise. Special cases of these problem are closely related -- via \nuniversality arguments -- to the network community detection problem for two \nasymmetric communities. As a further illustration, we consider the example of a \nblock-constant low-rank matrix with symmetric blocks, which we refer to as \n`Gaussian Block Model'. \n</p>"}, "author": "Andrea Montanari, Ramji Venkataramanan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663554", "id": "tag:google.com,2005:reader/item/000000032dc77dbb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Memory-efficient Kernel PCA via Partial Matrix Sampling and Nonconvex Optimization: a Model-free Analysis of Local Minima. (arXiv:1711.01742v1 [math.OC])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01742"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01742", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Kernel PCA is a widely used nonlinear dimension reduction technique in \nmachine learning, but storing the kernel matrix is notoriously challenging when \nthe sample size is large. Inspired by [YPCC16], where the idea of partial \nmatrix sampling followed by nonconvex optimization is proposed for matrix \ncompletion and robust PCA, we apply a similar approach to memory-efficient \nKernel PCA. In theory, with no assumptions on the kernel matrix in terms of \neigenvalues or eigenvectors, we established a model-free theory for the \nlow-rank approximation based on any local minimum of the proposed objective \nfunction. As interesting byproducts, when the underlying positive semidefinite \nmatrix is assumed to be low-rank and highly structured, corollaries of our main \ntheorem improve the state-of-the-art results [GLM16, GJZ17] for nonconvex \nmatrix completion with no spurious local minima. Numerical experiments also \nshow that our approach is competitive in terms of approximation accuracy \ncompared to the well-known Nystr\\\"{o}m algorithm for Kernel PCA. \n</p>"}, "author": "Ji Chen, Xiaodong Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663553", "id": "tag:google.com,2005:reader/item/000000032dc77e03", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "KGAN: How to Break The Minimax Game in GAN. (arXiv:1711.01744v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01744"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01744", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c828def8\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c828def8&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Generative Adversarial Networks (GANs) were intuitively and attractively \nexplained under the perspective of game theory, wherein two involving parties \nare a discriminator and a generator. In this game, the task of the \ndiscriminator is to discriminate the real and generated (i.e., fake) data, \nwhilst the task of the generator is to generate the fake data that maximally \nconfuses the discriminator. In this paper, we propose a new viewpoint for GANs, \nwhich is termed as the minimizing general loss viewpoint. This viewpoint shows \na connection between the general loss of a classification problem regarding a \nconvex loss function and a f-divergence between the true and fake data \ndistributions. Mathematically, we proposed a setting for the classification \nproblem of the true and fake data, wherein we can prove that the general loss \nof this classification problem is exactly the negative f-divergence for a \ncertain convex function f. This allows us to interpret the problem of learning \nthe generator for dismissing the f-divergence between the true and fake data \ndistributions as that of maximizing the general loss which is equivalent to the \nmin-max problem in GAN if the Logistic loss is used in the classification \nproblem. However, this viewpoint strengthens GANs in two ways. First, it allows \nus to employ any convex loss function for the discriminator. Second, it \nsuggests that rather than limiting ourselves in NN-based discriminators, we can \nalternatively utilize other powerful families. Bearing this viewpoint, we then \npropose using the kernel-based family for discriminators. This family has two \nappealing features: i) a powerful capacity in classifying non-linear nature \ndata and ii) being convex in the feature space. Using the convexity of this \nfamily, we can further develop Fenchel duality to equivalently transform the \nmax-min problem to the max-max dual problem. \n</p>"}, "author": "Trung Le, Tu Dinh Nguyen, Dinh Phung", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663552", "id": "tag:google.com,2005:reader/item/000000032dc77e76", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "AdaBatch: Efficient Gradient Aggregation Rules for Sequential and Parallel Stochastic Gradient Methods. (arXiv:1711.01761v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01761"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01761", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study a new aggregation operator for gradients coming from a mini-batch \nfor stochastic gradient (SG) methods that allows a significant speed-up in the \ncase of sparse optimization problems. We call this method AdaBatch and it only \nrequires a few lines of code change compared to regular mini-batch SGD \nalgorithms. We provide a theoretical insight to understand how this new class \nof algorithms is performing and show that it is equivalent to an implicit \nper-coordinate rescaling of the gradients, similarly to what Adagrad methods \ncan do. In theory and in practice, this new aggregation allows to keep the same \nsample efficiency of SG methods while increasing the batch size. \nExperimentally, we also show that in the case of smooth convex optimization, \nour procedure can even obtain a better loss when increasing the batch size for \na fixed number of samples. We then apply this new algorithm to obtain a \nparallelizable stochastic gradient method that is synchronous but allows \nspeed-up on par with Hogwild! methods as convergence does not deteriorate with \nthe increase of the batch size. The same approach can be used to make \nmini-batch provably efficient for variance-reduced SG methods such as SVRG. \n</p>"}, "author": "Alexandre D&#xe9;fossez (FAIR), Francis Bach (SIERRA)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663551", "id": "tag:google.com,2005:reader/item/000000032dc77f32", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Whitening Black-Box Neural Networks. (arXiv:1711.01768v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01768"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01768", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many deployed learned models are black boxes: given input, returns output. \nInternal information about the model, such as the architecture, optimisation \nprocedure, or training data, is not disclosed explicitly as it might contain \nproprietary information or make the system more vulnerable. This work shows \nthat such attributes of neural networks can be exposed from a sequence of \nqueries. This has multiple implications. On the one hand, our work exposes the \nvulnerability of black-box neural networks to different types of attacks -- we \nshow that the revealed internal information helps generate more effective \nadversarial examples against the black box model. On the other hand, this \ntechnique can be used for better protection of private content from automatic \nrecognition models using adversarial examples. Our paper suggests that it is \nactually hard to draw a line between white box and black box models. \n</p>"}, "author": "Seong Joon Oh, Max Augustin, Bernt Schiele, Mario Fritz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663550", "id": "tag:google.com,2005:reader/item/000000032dc77fd9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Simultaneous Block-Sparse Signal Recovery Using Pattern-Coupled Sparse Bayesian Learning. (arXiv:1711.01790v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01790"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01790", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we consider the block-sparse signals recovery problem in the \ncontext of multiple measurement vectors (MMV) with common row sparsity \npatterns. We develop a new method for recovery of common row sparsity MMV \nsignals, where a pattern-coupled hierarchical Gaussian prior model is \nintroduced to characterize both the block-sparsity of the coefficients and the \nstatistical dependency between neighboring coefficients of the common row \nsparsity MMV signals. Unlike many other methods, the proposed method is able to \nautomatically capture the block sparse structure of the unknown signal. Our \nmethod is developed using an expectation-maximization (EM) framework. \nSimulation results show that our proposed method offers competitive performance \nin recovering block-sparse common row sparsity pattern MMV signals. \n</p>"}, "author": "Hang Xiao, Zhengli Xing, Linxiao Yang, Jun Fang, Yanlun Wu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663549", "id": "tag:google.com,2005:reader/item/000000032dc78055", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Independently Interpretable Lasso: A New Regularizer for Sparse Regression with Uncorrelated Variables. (arXiv:1711.01796v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01796"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01796", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sparse regularization such as $\\ell_1$ regularization is a quite powerful and \nwidely used strategy for high dimensional learning problems. The effectiveness \nof sparse regularization have been supported practically and theoretically by \nseveral studies. However, one of the biggest issues in sparse regularization is \nthat its performance is quite sensitive to correlations between features. \nOrdinary $\\ell_1$ regularization often selects variables correlated with each \nother, which results in deterioration of not only its generalization error but \nalso interpretability. In this paper, we propose a new regularization method, \n\"Independently Interpretable Lasso\" (IILasso for short). Our proposed \nregularizer suppresses selecting correlated variables, and thus each active \nvariables independently affect the objective variable in the model. Hence, we \ncan interpret regression coefficients intuitively and also improve the \nperformance by avoiding overfitting. We analyze theoretical property of IILasso \nand show that the proposed method is much advantageous for its sign recovery \nand achieves almost minimax optimal convergence rate. Synthetic and real data \nanalyses also indicate the effectiveness of IILasso. \n</p>"}, "author": "Masaaki Takada, Taiji Suzuki, Hironori Fujisawa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663548", "id": "tag:google.com,2005:reader/item/000000032dc780e8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast amortized inference of neural activity from calcium imaging data with variational autoencoders. (arXiv:1711.01846v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01846"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01846", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Calcium imaging permits optical measurement of neural activity. Since \nintracellular calcium concentration is an indirect measurement of neural \nactivity, computational tools are necessary to infer the true underlying \nspiking activity from fluorescence measurements. Bayesian model inversion can \nbe used to solve this problem, but typically requires either computationally \nexpensive MCMC sampling, or faster but approximate maximum-a-posteriori \noptimization. Here, we introduce a flexible algorithmic framework for fast, \nefficient and accurate extraction of neural spikes from imaging data. Using the \nframework of variational autoencoders, we propose to amortize inference by \ntraining a deep neural network to perform model inversion efficiently. The \nrecognition network is trained to produce samples from the posterior \ndistribution over spike trains. Once trained, performing inference amounts to a \nfast single forward pass through the network, without the need for iterative \noptimization or sampling. We show that amortization can be applied flexibly to \na wide range of nonlinear generative models and significantly improves upon the \nstate of the art in computation time, while achieving competitive accuracy. Our \nframework is also able to represent posterior distributions over spike-trains. \nWe demonstrate the generality of our method by proposing the first \nprobabilistic approach for separating backpropagating action potentials from \nputative synaptic inputs in calcium imaging of dendritic spines. \n</p>"}, "author": "Artur Speiser, Jinyao Yan, Evan Archer, Lars Buesing, Srinivas C. Turaga, Jakob H. Macke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663547", "id": "tag:google.com,2005:reader/item/000000032dc7817b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations. (arXiv:1711.01847v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01847"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01847", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A powerful approach for understanding neural population dynamics is to \nextract low-dimensional trajectories from population recordings using \ndimensionality reduction methods. Current approaches for dimensionality \nreduction on neural data are limited to single population recordings, and can \nnot identify dynamics embedded across multiple measurements. We propose an \napproach for extracting low-dimensional dynamics from multiple, sequential \nrecordings. Our algorithm scales to data comprising millions of observed \ndimensions, making it possible to access dynamics distributed across large \npopulations or multiple brain areas. Building on subspace-identification \napproaches for dynamical systems, we perform parameter estimation by minimizing \na moment-matching objective using a scalable stochastic gradient descent \nalgorithm: The model is optimized to predict temporal covariations across \nneurons and across time. We show how this approach naturally handles missing \ndata and multiple partial recordings, and can identify dynamics and predict \ncorrelations even in the presence of severe subsampling and small overlap \nbetween recordings. We demonstrate the effectiveness of the approach both on \nsimulated data and a whole-brain larval zebrafish imaging dataset. \n</p>"}, "author": "Marcel Nonnenmacher, Srinivas C. Turaga, Jakob H. Macke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663546", "id": "tag:google.com,2005:reader/item/000000032dc7820c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Flexible statistical inference for mechanistic models of neural dynamics. (arXiv:1711.01861v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01861"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01861", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Mechanistic models of single-neuron dynamics have been extensively studied in \ncomputational neuroscience. However, identifying which models can \nquantitatively reproduce empirically measured data has been challenging. We \npropose to overcome this limitation by using likelihood-free inference \napproaches (also known as Approximate Bayesian Computation, ABC) to perform \nfull Bayesian inference on single-neuron models. Our approach builds on recent \nadvances in ABC by learning a neural network which maps features of the \nobserved data to the posterior distribution over parameters. We learn a \nBayesian mixture-density network approximating the posterior over multiple \nrounds of adaptively chosen simulations. Furthermore, we propose an efficient \napproach for handling missing features and parameter settings for which the \nsimulator fails, as well as a strategy for automatically learning relevant \nfeatures using recurrent neural networks. On synthetic data, our approach \nefficiently estimates posterior distributions and recovers ground-truth \nparameters. On in-vitro recordings of membrane voltages, we recover \nmultivariate posteriors over biophysical parameters, which yield \nmodel-predicted voltage traces that accurately match empirical data. Our \napproach will enable neuroscientists to perform Bayesian inference on complex \nneuron models without having to design model-specific algorithms, closing the \ngap between mechanistic and statistical approaches to single-neuron modelling. \n</p>"}, "author": "Jan-Matthis Lueckmann, Pedro J. Goncalves, Giacomo Bassetto, Kaan &#xd6;cal, Marcel Nonnenmacher, Jakob H. Macke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663545", "id": "tag:google.com,2005:reader/item/000000032dc782d0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interpretable Feature Recommendation for Signal Analytics. (arXiv:1711.01870v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01870"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01870", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents an automated approach for interpretable feature \nrecommendation for solving signal data analytics problems. The method has been \ntested by performing experiments on datasets in the domain of prognostics where \ninterpretation of features is considered very important. The proposed approach \nis based on Wide Learning architecture and provides means for interpretation of \nthe recommended features. It is to be noted that such an interpretation is not \navailable with feature learning approaches like Deep Learning (such as \nConvolutional Neural Network) or feature transformation approaches like \nPrincipal Component Analysis. Results show that the feature recommendation and \ninterpretation techniques are quite effective for the problems at hand in terms \nof performance and drastic reduction in time to develop a solution. It is \nfurther shown by an example, how this human-in-loop interpretation system can \nbe used as a prescriptive system. \n</p>"}, "author": "Snehasis Banerjee, Tanushyam Chattopadhyay, Ayan Mukherjee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663544", "id": "tag:google.com,2005:reader/item/000000032dc78331", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "$A^{4}NT$: Author Attribute Anonymity by Adversarial Training of Neural Machine Translation. (arXiv:1711.01921v2 [cs.CR] UPDATED)", "published": 1510139306, "updated": 1510139311, "canonical": [{"href": "http://arxiv.org/abs/1711.01921"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01921", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Text-based analysis methods allow to reveal privacy relevant author \nattributes such as gender, age and identify of the text's author. Such methods \ncan compromise the privacy of an anonymous author even when the author tries to \nremove privacy sensitive content. In this paper, we propose an automatic \nmethod, called Adversarial Author Attribute Anonymity Neural Translation \n($A^4NT$), to combat such text-based adversaries. We combine \nsequence-to-sequence language models used in machine translation and generative \nadversarial networks to obfuscate author attributes. Unlike machine translation \ntechniques which need paired data, our method can be trained on unpaired \ncorpora of text containing different authors. Importantly, we propose and \nevaluate techniques to impose constraints on our $A^4NT$ to preserve the \nsemantics of the input text. $A^4NT$ learns to make minimal changes to the \ninput text to successfully fool author attribute classifiers, while aiming to \nmaintain the meaning of the input. We show through experiments on two different \ndatasets and three settings that our proposed method is effective in fooling \nthe author attribute classifiers and thereby improving the anonymity of \nauthors. \n</p>"}, "author": "Rakshith Shetty, Bernt Schiele, Mario Fritz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663543", "id": "tag:google.com,2005:reader/item/000000032dc7842b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time. (arXiv:1711.01944v2 [math.OC] UPDATED)", "published": 1510229029, "updated": 1510229038, "canonical": [{"href": "http://arxiv.org/abs/1711.01944"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01944", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c828e2e4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c828e2e4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Two classes of methods have been proposed for escaping from saddle points \nwith one using the second-order information carried by the Hessian and the \nother adding the noise into the first-order information. The existing analysis \nfor algorithms using noise in the first-order information is quite involved and \nhides the essence of added noise, which hinder further improvements of these \nalgorithms. In this paper, we present a novel perspective of noise-adding \ntechnique, i.e., adding the noise into the first-order information can help \nextract the negative curvature from the Hessian matrix, and provide a formal \nreasoning of this perspective by analyzing a simple first-order procedure. More \nimportantly, the proposed procedure enables one to design purely first-order \nstochastic algorithms for escaping from non-degenerate saddle points with a \nmuch better time complexity (almost linear time in terms of the problem's \ndimensionality). In particular, we develop a {\\bf first-order stochastic \nalgorithm} based on our new technique and an existing algorithm that only \nconverges to a first-order stationary point to enjoy a time complexity of \n{$\\widetilde O(d/\\epsilon^{3.5})$ for finding a nearly second-order stationary \npoint $\\bf{x}$ such that $\\|\\nabla F(bf{x})\\|\\leq \\epsilon$ and $\\nabla^2 \nF(bf{x})\\geq -\\sqrt{\\epsilon}I$ (in high probability), where $F(\\cdot)$ denotes \nthe objective function and $d$ is the dimensionality of the problem. To the \nbest of our knowledge, this is the best theoretical result of first-order \nalgorithms for stochastic non-convex optimization, which is even competitive \nwith if not better than existing stochastic algorithms hinging on the \nsecond-order information. \n</p>"}, "author": "Yi Xu, Tianbao Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663542", "id": "tag:google.com,2005:reader/item/000000032dc784e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deformable Deep Convolutional Generative Adversarial Network in Microwave Based Hand Gesture Recognition System. (arXiv:1711.01968v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01968"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01968", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8512b8e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8512b8e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Traditional vision-based hand gesture recognition systems is limited under \ndark circumstances. In this paper, we build a hand gesture recognition system \nbased on microwave transceiver and deep learning algorithm. A Doppler radar \nsensor with dual receiving channels at 5.8GHz is used to acquire a big database \nof hand gestures signals. The received hand gesture signals are then processed \nwith time-frequency analysis. Based on these big databases of hand gesture, we \npropose a new machine learning architecture called deformable deep \nconvolutional generative adversarial network. Experimental results show the new \narchitecture can upgrade the recognition rate by 10% and the deformable kernel \ncan reduce the testing time cost by 30%. \n</p>"}, "author": "Jiajun Zhang, Zhiguo Shi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663541", "id": "tag:google.com,2005:reader/item/000000032dc785fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal transport maps for distribution preserving operations on latent spaces of Generative Models. (arXiv:1711.01970v1 [cs.LG])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01970"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01970", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative models such as Variational Auto Encoders (VAEs) and Generative \nAdversarial Networks (GANs) are typically trained for a fixed prior \ndistribution in the latent space, such as uniform or Gaussian. After a trained \nmodel is obtained, one can sample the Generator in various forms for \nexploration and understanding, such as interpolating between two samples, \nsampling in the vicinity of a sample or exploring differences between a pair of \nsamples applied to a third sample. In this paper, we show that the latent space \noperations used in the literature so far induce a distribution mismatch between \nthe resulting outputs and the prior distribution the model was trained on. To \naddress this, we propose to use distribution matching transport maps to ensure \nthat such latent space operations preserve the prior distribution, while \nminimally modifying the original operation. Our experimental results validate \nthat the proposed operations give higher quality samples compared to the \noriginal operations. \n</p>"}, "author": "Eirikur Agustsson, Alexander Sage, Radu Timofte, Luc Van Gool", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663540", "id": "tag:google.com,2005:reader/item/000000032dc78698", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Estimating Cosmological Parameters from the Dark Matter Distribution. (arXiv:1711.02033v1 [astro-ph.CO])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02033"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02033", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A grand challenge of the 21st century cosmology is to accurately estimate the \ncosmological parameters of our Universe. A major approach to estimating the \ncosmological parameters is to use the large-scale matter distribution of the \nUniverse. Galaxy surveys provide the means to map out cosmic large-scale \nstructure in three dimensions. Information about galaxy locations is typically \nsummarized in a \"single\" function of scale, such as the galaxy correlation \nfunction or power-spectrum. We show that it is possible to estimate these \ncosmological parameters directly from the distribution of matter. This paper \npresents the application of deep 3D convolutional networks to volumetric \nrepresentation of dark-matter simulations as well as the results obtained using \na recently proposed distribution regression framework, showing that machine \nlearning techniques are comparable to, and can sometimes outperform, \nmaximum-likelihood point estimates using \"cosmological models\". This opens the \nway to estimating the parameters of our Universe with higher accuracy. \n</p>"}, "author": "Siamak Ravanbakhsh, Junier Oliva, Sebastien Fromenteau, Layne C. Price, Shirley Ho, Jeff Schneider, Barnabas Poczos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663539", "id": "tag:google.com,2005:reader/item/000000032dc78712", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Computing Maximum Entropy Distributions Everywhere. (arXiv:1711.02036v1 [cs.DS])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02036"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02036", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of computing the maximum entropy distribution with a \nspecified expectation over a large discrete domain. Maximum entropy \ndistributions arise and have found numerous applications in economics, machine \nlearning and various sub-disciplines of mathematics and computer science. The \nkey computational questions related to maximum entropy distributions are \nwhether they have succinct descriptions and whether they can be efficiently \ncomputed. Here we provide positive answers to both of these questions for very \ngeneral domains and, importantly, with no restriction on the expectation. This \ncompletes the picture left open by the prior work on this problem which \nrequires that the expectation vector is polynomially far in the interior of the \nconvex hull of the domain. As a consequence we obtain a general algorithmic \ntool and show how it can be applied to derive several old and new results in a \nunified manner. In particular, our results imply that certain recent continuous \noptimization formulations, for instance, for discrete counting and optimization \nproblems, the matrix scaling problem, and the worst case Brascamp-Lieb \nconstants in the rank-1 regime, are efficiently computable. Attaining these \nimplications requires reformulating the underlying problem as a version of \nmaximum entropy computation where optimization also involves the expectation \nvector and, hence, cannot be assumed to be sufficiently deep in the interior. \nThe key new technical ingredient in our work is a polynomial bound on the bit \ncomplexity of near-optimal dual solutions to the maximum entropy convex \nprogram. This result is obtained by a geometrical reasoning that involves \nconvex analysis and polyhedral geometry, avoiding combinatorial arguments based \non the specific structure of the domain. We also provide a lower bound on the \nbit complexity of near-optimal solutions showing the tightness of our results. \n</p>"}, "author": "Damian Straszak, Nisheeth K. Vishnoi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663538", "id": "tag:google.com,2005:reader/item/000000032dc78785", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Randomized Nonnegative Matrix Factorization. (arXiv:1711.02037v1 [stat.ML])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02037"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02037", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Nonnegative matrix factorization (NMF) is a powerful tool for data mining. \nHowever, the emergence of `big data' has severely challenged our ability to \ncompute this fundamental decomposition using deterministic algorithms. This \npaper presents a randomized hierarchical alternating least squares (HALS) \nalgorithm to compute the NMF. By deriving a smaller matrix from the nonnegative \ninput data, a more efficient nonnegative decomposition can be computed. Our \nalgorithm scales to big data applications while attaining a near-optimal \nfactorization, i.e., the algorithm scales with the target rank of the data \nrather than the ambient dimension of measurement space. The proposed algorithm \nis evaluated using synthetic and real world data and shows substantial speedups \ncompared to deterministic HALS. \n</p>"}, "author": "N. Benjamin Erichson, Ariana Mendible, Sophie Wihlborn, J. Nathan Kutz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663537", "id": "tag:google.com,2005:reader/item/000000032dc787f0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An efficient quantum algorithm for generative machine learning. (arXiv:1711.02038v1 [quant-ph])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02038"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02038", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A central task in the field of quantum computing is to find applications \nwhere quantum computer could provide exponential speedup over any classical \ncomputer. Machine learning represents an important field with broad \napplications where quantum computer may offer significant speedup. Several \nquantum algorithms for discriminative machine learning have been found based on \nefficient solving of linear algebraic problems, with potential exponential \nspeedup in runtime under the assumption of effective input from a quantum \nrandom access memory. In machine learning, generative models represent another \nlarge class which is widely used for both supervised and unsupervised learning. \nHere, we propose an efficient quantum algorithm for machine learning based on a \nquantum generative model. We prove that our proposed model is exponentially \nmore powerful to represent probability distributions compared with classical \ngenerative models and has exponential speedup in training and inference at \nleast for some instances under a reasonable assumption in computational \ncomplexity theory. Our result opens a new direction for quantum machine \nlearning and offers a remarkable example in which a quantum algorithm shows \nexponential improvement over any classical algorithm in an important \napplication field. \n</p>"}, "author": "Xun Gao, Zhengyu Zhang, Luming Duan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663536", "id": "tag:google.com,2005:reader/item/000000032dc7883b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "End-to-End Abnormality Detection in Medical Imaging. (arXiv:1711.02074v1 [cs.CV])", "published": 1510049223, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.02074"}], "alternate": [{"href": "http://arxiv.org/abs/1711.02074", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Nearly all of the deep learning based image analysis methods work on \nreconstructed images, which are obtained from original acquisitions via solving \ninverse problems. The reconstruction algorithms are designed for human \nobservers, but not necessarily optimized for DNNs. It is desirable to train the \nDNNs directly from the original data which lie in a different domain with the \nimages. In this work, we proposed an end-to-end DNN for abnormality detection \nin medical imaging. A DNN was built as the unrolled version of iterative \nreconstruction algorithms to map the acquisitions to images, and followed by a \n3D convolutional neural network (CNN) to detect the abnormality in the \nreconstructed images. The two networks were trained jointly in order to \noptimize the entire DNN for the detection task from the original acquisitions. \nThe DNN was implemented for lung nodule detection in low-dose chest CT. The \nproposed end-to-end DNN demonstrated better sensitivity and accuracy for the \ntask compared to a two-step approach, in which the reconstruction and detection \nDNNs were trained separately. A significant reduction of false positive rate on \nsuspicious lesions were observed, which is crucial for the known over-diagnosis \nin low-dose lung CT imaging. The images reconstructed by the proposed \nend-to-end network also presented enhanced details in the region of interest. \n</p>"}, "author": "Dufan Wu, Kyungsang Kim, Bin Dong, Quanzheng Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224664", "timestampUsec": "1510049224663517", "id": "tag:google.com,2005:reader/item/000000032dc789ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Factoring Exogenous State for Model-Free Monte Carlo. (arXiv:1703.09390v2 [cs.LG] UPDATED)", "published": 1510049224, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.09390"}], "alternate": [{"href": "http://arxiv.org/abs/1703.09390", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Policy analysts wish to visualize a range of policies for large \nsimulator-defined Markov Decision Processes (MDPs). One visualization approach \nis to invoke the simulator to generate on-policy trajectories and then \nvisualize those trajectories. When the simulator is expensive, this is not \npractical, and some method is required for generating trajectories for new \npolicies without invoking the simulator. The method of Model-Free Monte Carlo \n(MFMC) can do this by stitching together state transitions for a new policy \nbased on previously-sampled trajectories from other policies. This \"off-policy \nMonte Carlo simulation\" method works well when the state space has low \ndimension but fails as the dimension grows. This paper describes a method for \nfactoring out some of the state and action variables so that MFMC can work in \nhigh-dimensional MDPs. The new method, MFMCi, is evaluated on a very \nchallenging wildfire management MDP. \n</p>"}, "author": "Sean McGregor, Rachel Houtman, Claire Montgomery, Ronald Metoyer, Thomas G. Dietterich", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1510049224663", "timestampUsec": "1510049224663488", "id": "tag:google.com,2005:reader/item/000000032dc7980e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Supervised STDP-based Training Algorithm for Living Neural Networks. (arXiv:1710.10944v2 [cs.NE] UPDATED)", "published": 1510049224, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10944"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10944", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks have shown great potential in many applications like speech \nrecognition, drug discovery, image classification, and object detection. Neural \nnetwork models are inspired by biological neural networks, but they are \noptimized to perform machine learning tasks on digital computers. The proposed \nwork explores the possibilities of using living neural networks in vitro as \nbasic computational elements for machine learning applications. A new \nsupervised STDP-based learning algorithm is proposed in this work, which \nconsiders neuron engineering constrains. A 74.7% accuracy is achieved on the \nMNIST benchmark for handwritten digit recognition. \n</p>"}, "author": "Yuan Zeng, Kevin Devincentis, Yao Xiao, Zubayer Ibne Ferdous, Xiaochen Guo, Zhiyuan Yan, Yevgeny Berdichevsky", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509977069829", "timestampUsec": "1509977069829003", "id": "tag:google.com,2005:reader/item/000000032d34447f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Weight-Based Variable Ordering in the Context of High-Level Consistencies. (arXiv:1711.00909v1 [cs.AI])", "published": 1509977070, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00909"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00909", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8512fa0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8512fa0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Dom/wdeg is one of the best performing heuristics for dynamic variable \nordering in backtrack search [Boussemart et al., 2004]. As originally defined, \nthis heuristic increments the weight of the constraint that causes a domain \nwipeout (i.e., a dead-end) when enforcing arc consistency during search. \"The \nprocess of weighting constraints with dom/wdeg is not defined when more than \none constraint lead to a domain wipeout [Vion et al., 2011].\" In this paper, we \ninvestigate how weights should be updated in the context of two high-level \nconsistencies, namely, singleton (POAC) and relational consistencies (RNIC). We \npropose, analyze, and empirically evaluate several strategies for updating the \nweights. We statistically compare the proposed strategies and conclude with our \nrecommendations. \n</p>"}, "author": "Robert J. Woodward, Berthe Y. Choueiry", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509977069829", "timestampUsec": "1509977069829002", "id": "tag:google.com,2005:reader/item/000000032d344490", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Running Time Analysis of the (1+1)-EA for OneMax and LeadingOnes under Bit-wise Noise. (arXiv:1711.00956v1 [cs.NE])", "published": 1509977070, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00956"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00956", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In many real-world optimization problems, the objective function evaluation \nis subject to noise, and we cannot obtain the exact objective value. \nEvolutionary algorithms (EAs), a type of general-purpose randomized \noptimization algorithm, have shown able to solve noisy optimization problems \nwell. However, previous theoretical analyses of EAs mainly focused on \nnoise-free optimization, which makes the theoretical understanding largely \ninsufficient. Meanwhile, the few existing theoretical studies under noise often \nconsidered the one-bit noise model, which flips a randomly chosen bit of a \nsolution before evaluation; while in many realistic applications, several bits \nof a solution can be changed simultaneously. In this paper, we study a natural \nextension of one-bit noise, the bit-wise noise model, which independently flips \neach bit of a solution with some probability. We analyze the running time of \nthe (1+1)-EA solving OneMax and LeadingOnes under bit-wise noise for the first \ntime, and derive the ranges of the noise level for polynomial and \nsuper-polynomial running time bounds. The analysis on LeadingOnes under \nbit-wise noise can be easily transferred to one-bit noise, and improves the \npreviously known results. Since our analysis discloses that the (1+1)-EA can be \nefficient only under low noise levels, we also study whether the sampling \nstrategy can bring robustness to noise. We prove that using sampling can \nsignificantly increase the largest noise level allowing a polynomial running \ntime, that is, sampling is robust to noise. \n</p>"}, "author": "Chao Qian, Chao Bian, Wu Jiang, Ke Tang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509977069829", "timestampUsec": "1509977069829001", "id": "tag:google.com,2005:reader/item/000000032d3444a0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SPARK: Static Program Analysis Reasoning and Retrieving Knowledge. (arXiv:1711.01024v1 [cs.PL])", "published": 1509977070, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01024"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01024", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Program analysis is a technique to reason about programs without executing \nthem, and it has various applications in compilers, integrated development \nenvironments, and security. In this work, we present a machine learning \npipeline that induces a security analyzer for programs by example. The security \nanalyzer determines whether a program is either secure or insecure based on \nsymbolic rules that were deduced by our machine learning pipeline. The machine \npipeline is two-staged consisting of a Recurrent Neural Networks (RNN) and an \nExtractor that converts an RNN to symbolic rules. \n</p> \n<p>To evaluate the quality of the learned symbolic rules, we propose a \nsampling-based similarity measurement between two infinite regular languages. \nWe conduct a case study using real-world data. In this work, we discuss the \nlimitations of existing techniques and possible improvements in the future. The \nresults show that with sufficient training data and a fair distribution of \nprogram paths it is feasible to deducing symbolic security rules for the \nOpenJDK library with millions lines of code. \n</p>"}, "author": "Wasuwee Sodsong, Bernhard Scholz, Sanjay Chawla", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509977069829", "timestampUsec": "1509977069829000", "id": "tag:google.com,2005:reader/item/000000032d3444a8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Spintronics based Stochastic Computing for Efficient Bayesian Inference System. (arXiv:1711.01125v1 [cs.ET])", "published": 1509977070, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01125"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01125", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Bayesian inference is an effective approach for solving statistical learning \nproblems especially with uncertainty and incompleteness. However, inference \nefficiencies are physically limited by the bottlenecks of conventional \ncomputing platforms. In this paper, an emerging Bayesian inference system is \nproposed by exploiting spintronics based stochastic computing. A stochastic \nbitstream generator is realized as the kernel components by leveraging the \ninherent randomness of spintronics devices. The proposed system is evaluated by \ntypical applications of data fusion and Bayesian belief networks. Simulation \nresults indicate that the proposed approach could achieve significant \nimprovement on inference efficiencies in terms of power consumption and \ninference speed. \n</p>"}, "author": "Xiaotao Jia, Jianlei Yang, Zhaohao Wang, Yiran Chen, Hai (Helen)Li, Weisheng Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509977069829", "timestampUsec": "1509977069828999", "id": "tag:google.com,2005:reader/item/000000032d3444b2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Accountability of AI Under the Law: The Role of Explanation. (arXiv:1711.01134v1 [cs.AI])", "published": 1509977070, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01134"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01134", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The ubiquity of systems using artificial intelligence or \"AI\" has brought \nincreasing attention to how those systems should be regulated. The choice of \nhow to regulate AI systems will require care. AI systems have the potential to \nsynthesize large amounts of data, allowing for greater levels of \npersonalization and precision than ever before---applications range from \nclinical decision support to autonomous driving and predictive policing. That \nsaid, there exist legitimate concerns about the intentional and unintentional \nnegative consequences of AI systems. There are many ways to hold AI systems \naccountable. In this work, we focus on one: explanation. Questions about a \nlegal right to explanation from AI systems was recently debated in the EU \nGeneral Data Protection Regulation, and thus thinking carefully about when and \nhow explanation from AI systems might improve accountability is timely. In this \nwork, we review contexts in which explanation is currently required under the \nlaw, and then list the technical considerations that must be considered if we \ndesired AI systems that could provide kinds of explanations that are currently \nrequired of humans. \n</p>"}, "author": "Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David O&#x27;Brien, Stuart Schieber, James Waldo, David Weinberger, Alexandra Wood", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509977069829", "timestampUsec": "1509977069828998", "id": "tag:google.com,2005:reader/item/000000032d3444c1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Lifelong Learning by Adjusting Priors. (arXiv:1711.01244v1 [stat.ML])", "published": 1509977070, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01244"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01244", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In representational lifelong learning an agent aims to continually learn to \nsolve novel tasks while updating its representation in light of previous tasks. \nUnder the assumption that future tasks are 'related' to previous tasks, \nrepresentations should be learned in such a way that they capture the common \nstructure across learned tasks, while allowing the learner sufficient \nflexibility to adapt to novel aspects of a new task. We develop a framework for \nlifelong learning in deep neural networks that is based on generalization \nbounds, developed within the PAC-Bayes framework. Learning takes place through \nthe construction of a distribution over networks based on the tasks seen so \nfar, and its utilization for learning a new task. Thus, prior knowledge is \nincorporated through setting a history-dependent prior for novel tasks. We \ndevelop a gradient-based algorithm implementing these ideas, based on \nminimizing an objective function motivated by generalization bounds, and \ndemonstrate its effectiveness through numerical examples. In addition to \nestablishing the improved performance available through lifelong learning, we \ndemonstrate the intuitive way by which prior information is manifested at \ndifferent levels of the network. \n</p>"}, "author": "Ron Amit, Ron Meir", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228294", "id": "tag:google.com,2005:reader/item/000000032d09f359", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The (Un)reliability of saliency methods. (arXiv:1711.00867v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00867"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00867", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Saliency methods aim to explain the predictions of deep neural networks. \nThese methods lack reliability when the explanation is sensitive to factors \nthat do not contribute to the model prediction. We use a simple and common \npre-processing step ---adding a constant shift to the input data--- to show \nthat a transformation with no effect on the model can cause numerous methods to \nincorrectly attribute. In order to guarantee reliability, we posit that methods \nshould fulfill input invariance, the requirement that a saliency method mirror \nthe sensitivity of the model with respect to transformations of the input. We \nshow, through several examples, that saliency methods that do not satisfy input \ninvariance result in misleading attribution. \n</p>"}, "author": "Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Sch&#xfc;tt, Sven D&#xe4;hne, Dumitru Erhan, Been Kim", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228293", "id": "tag:google.com,2005:reader/item/000000032d09f369", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Correcting Nuisance Variation using Wasserstein Distance. (arXiv:1711.00882v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00882"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00882", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Profiling cellular phenotypes from microscopic imaging can provide meaningful \nbiological information resulting from various factors affecting the cells. One \nmotivating application is drug development: morphological cell features can be \ncaptured from images, from which similarities between different drugs applied \nat different dosages can be quantified. The general approach is to find a \nfunction mapping the images to an embedding space of manageable dimensionality \nwhose geometry captures relevant features of the input images. An important \nknown issue for such methods is separating relevant biological signal from \nnuisance variation. For example, the embedding vectors tend to be more \ncorrelated for cells that were cultured and imaged during the same week than \nfor cells from a different week, despite having identical drug compounds \napplied in both cases. In this case, the particular batch a set of experiments \nwere conducted in constitutes the domain of the data; an ideal set of image \nembeddings should contain only the relevant biological information (e.g. drug \neffects). We develop a method for adjusting the image embeddings in order to \n`forget' domain-specific information while preserving relevant biological \ninformation. To do this, we minimize a loss function based on the Wasserstein \ndistance. We find for our transformed embeddings (1) the underlying geometric \nstructure is preserved and (2) less domain-specific information is present. \n</p>"}, "author": "Gil Tabak, Minjie Fan, Samuel J. Yang, Stephan Hoyer, Geoff Davis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228292", "id": "tag:google.com,2005:reader/item/000000032d09f378", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sparse-View X-Ray CT Reconstruction Using $\\ell_1$ Prior with Learned Transform. (arXiv:1711.00905v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00905"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00905", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A major challenge in X-ray computed tomography (CT) is reducing radiation \ndose while maintaining high quality of reconstructed images. To reduce the \nradiation dose, one can reduce the number of projection views (sparse-view CT); \nhowever, it becomes difficult to achieve high quality image reconstruction as \nthe number of projection views decreases. Researchers have applied the concept \nof learning sparse representations from (high-quality) CT image dataset to the \nsparse-view CT reconstruction. We propose a new statistical CT reconstruction \nmodel that combines penalized weighted-least squares (PWLS) and $\\ell_1$ \nregularization with learned sparsifying transform (PWLS-ST-$\\ell_1$), and an \nalgorithm for PWLS-ST-$\\ell_1$. Numerical experiments for sparse-view 2D \nfan-beam CT and 3D axial cone-beam CT show that the $\\ell_1$ regularizer \nsignificantly improves the sharpness of edges of reconstructed images compared \nto the CT reconstruction methods using edge-preserving regularizer and $\\ell_2$ \nregularization with learned ST. \n</p>"}, "author": "Xuehang Zheng, Il Yong Chun, Zhipeng Li, Yong Long, Jeffrey A. Fessler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228291", "id": "tag:google.com,2005:reader/item/000000032d09f384", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Binary Bouncy Particle Sampler. (arXiv:1711.00922v1 [stat.CO])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00922"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00922", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Bouncy Particle Sampler is a novel rejection-free non-reversible sampler \nfor differentiable probability distributions over continuous variables. We \ngeneralize the algorithm to piecewise differentiable distributions and apply it \nto generic binary distributions using a piecewise differentiable augmentation. \nWe illustrate the new algorithm in a binary Markov Random Field example, and \ncompare it to binary Hamiltonian Monte Carlo. Our results suggest that binary \nBPS samplers are better for easy to mix distributions. \n</p>"}, "author": "Ari Pakman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228290", "id": "tag:google.com,2005:reader/item/000000032d09f38c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Linear Dynamical Systems via Spectral Filtering. (arXiv:1711.00946v1 [cs.LG])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00946"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00946", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8513265\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8513265&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present an efficient and practical algorithm for the online prediction of \ndiscrete-time linear dynamical systems with a symmetric transition matrix. We \ncircumvent the non-convex optimization problem using improper learning: \ncarefully overparameterize the class of LDSs by a polylogarithmic factor, in \nexchange for convexity of the loss functions. From this arises a \npolynomial-time algorithm with a near-optimal regret guarantee, with an \nanalogous sample complexity bound for agnostic learning. Our algorithm is based \non a novel filtering technique, which may be of independent interest: we \nconvolve the time series with the eigenvectors of a certain Hankel matrix. \n</p>"}, "author": "Elad Hazan, Karan Singh, Cyril Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228289", "id": "tag:google.com,2005:reader/item/000000032d09f393", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting. (arXiv:1711.00950v1 [cs.LG])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00950"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00950", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c85bdbb0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c85bdbb0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present an algorithm to identify sparse dependence structure in continuous \nand non-Gaussian probability distributions, given a corresponding set of data. \nThe conditional independence structure of an arbitrary distribution can be \nrepresented as an undirected graph (or Markov random field), but most \nalgorithms for learning this structure are restricted to the discrete or \nGaussian cases. Our new approach allows for more realistic and accurate \ndescriptions of the distribution in question, and in turn better estimates of \nits sparse Markov structure. Sparsity in the graph is of interest as it can \naccelerate inference, improve sampling methods, and reveal important \ndependencies between variables. The algorithm relies on exploiting the \nconnection between the sparsity of the graph and the sparsity of transport \nmaps, which deterministically couple one probability measure to another. \n</p>"}, "author": "Rebecca E. Morrison, Ricardo Baptista, Youssef Marzouk", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228288", "id": "tag:google.com,2005:reader/item/000000032d09f39b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Classification-Based Perspective on GAN Distributions. (arXiv:1711.00970v3 [cs.LG] UPDATED)", "published": 1511308877, "updated": 1511308893, "canonical": [{"href": "http://arxiv.org/abs/1711.00970"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00970", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A fundamental, and still largely unanswered, question in the context of \nGenerative Adversarial Networks (GANs) is whether GANs are actually able to \ncapture the key characteristics of the datasets they are trained on. The \ncurrent approaches to examining this issue require significant human \nsupervision, such as visual inspection of sampled images, and often offer only \nfairly limited scalability. In this paper, we propose new techniques that \nemploy a classification-based perspective to evaluate synthetic GAN \ndistributions and their capability to accurately reflect the essential \nproperties of the training data. These techniques require only minimal human \nsupervision and can easily be scaled and adapted to evaluate a variety of \nstate-of-the-art GANs on large, popular datasets. Our analysis indicates that \nGANs have significant problems in reproducing the more distributional \nproperties of the training dataset. In particular, when seen through the lens \nof classification, the diversity of GAN data is orders of magnitude less than \nthat of the original data. \n</p>"}, "author": "Shibani Santurkar, Ludwig Schmidt, Aleksander M&#x105;dry", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228287", "id": "tag:google.com,2005:reader/item/000000032d09f3a4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "From which world is your graph?. (arXiv:1711.00982v1 [cs.LG])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00982"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00982", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Discovering statistical structure from links is a fundamental problem in the \nanalysis of social networks. Choosing a misspecified model, or equivalently, an \nincorrect inference algorithm will result in an invalid analysis or even \nfalsely uncover patterns that are in fact artifacts of the model. This work \nfocuses on unifying two of the most widely used link-formation models: the \nstochastic blockmodel (SBM) and the small world (or latent space) model (SWM). \nIntegrating techniques from kernel learning, spectral graph theory, and \nnonlinear dimensionality reduction, we develop the first statistically sound \npolynomial-time algorithm to discover latent patterns in sparse graphs for both \nmodels. When the network comes from an SBM, the algorithm outputs a block \nstructure. When it is from an SWM, the algorithm outputs estimates of each \nnode's latent position. \n</p>"}, "author": "Cheng Li, Felix Wong, Zhenming Liu, Varun Kanade", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228286", "id": "tag:google.com,2005:reader/item/000000032d09f3a6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Analysis of Approximate Stochastic Gradient Using Quadratic Constraints and Sequential Semidefinite Programs. (arXiv:1711.00987v1 [math.OC])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00987"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00987", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present convergence rate analysis for the approximate stochastic gradient \nmethod, where individual gradient updates are corrupted by computation errors. \nWe develop stochastic quadratic constraints to formulate a small linear matrix \ninequality (LMI) whose feasible set characterizes convergence properties of the \napproximate stochastic gradient. Based on this LMI condition, we develop a \nsequential minimization approach to analyze the intricate trade-offs that \ncouple stepsize selection, convergence rate, optimization accuracy, and \nrobustness to gradient inaccuracy. We also analytically solve this LMI \ncondition and obtain theoretical formulas that quantify the convergence \nproperties of the approximate stochastic gradient under various assumptions on \nthe loss functions. \n</p>"}, "author": "Bin Hu, Peter Seiler, Laurent Lessard", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228285", "id": "tag:google.com,2005:reader/item/000000032d09f3b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Partial correlation graphs and the neighborhood lattice. (arXiv:1711.00991v1 [math.ST])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00991"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00991", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We define and study partial correlation graphs (PCGs) with variables in a \ngeneral Hilbert space and their connections to generalized neighborhood \nregression, without making any distributional assumptions. Using \noperator-theoretic arguments, and especially the properties of projection \noperators on Hilbert spaces, we show that these neighborhood regressions have \nthe algebraic structure of a lattice, which we call a neighborhood lattice. \nThis lattice property significantly reduces the number of conditions one has to \ncheck when testing all partial correlation relations among a collection of \nvariables. In addition, we generalize the notion of perfectness in graphical \nmodels for a general PCG to this Hilbert space setting, and establish that \nalmost all Gram matrices are perfect. Under this perfectness assumption, we \nshow how these neighborhood lattices may be \"graphically\" computed using \nseparation properties of PCGs. We also discuss extensions of these ideas to \ndirected models, which present unique challenges compared to their undirected \ncounterparts. Our results have implications for multivariate statistical \nlearning in general, including structural equation models, subspace clustering, \nand dimension reduction. For example, we discuss how to compute neighborhood \nlattices efficiently and furthermore how they can be used to reduce the sample \ncomplexity of learning directed acyclic graphs. Our work demonstrates that this \nabstract viewpoint via projection operators significantly simplifies existing \nideas and arguments from the graphical modeling literature, and furthermore can \nbe used to extend these ideas to more general nonparametric settings. \n</p>"}, "author": "Arash A. Amini, Bryon Aragam, Qing Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228284", "id": "tag:google.com,2005:reader/item/000000032d09f3c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Genetic Policy Optimization. (arXiv:1711.01012v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01012"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01012", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Genetic algorithms have been widely used in many practical optimization \nproblems. Inspired by natural selection, operators, including mutation, \ncrossover and selection, provide effective heuristics for search and black-box \noptimization. However, they have not been shown useful for deep reinforcement \nlearning, possibly due to the catastrophic consequence of parameter crossovers \nof neural networks. Here, we present Genetic Policy Optimization (GPO), a new \ngenetic algorithm for sample-efficient deep policy optimization. GPO uses \nimitation learning for policy crossover in the state space and applies policy \ngradient methods for mutation. Our experiments on Mujoco tasks show that GPO as \na genetic algorithm is able to provide superior performance over the \nstate-of-the-art policy gradient methods and achieves comparable or higher \nsample efficiency. \n</p>"}, "author": "Tanmay Gangwani, Jian Peng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228283", "id": "tag:google.com,2005:reader/item/000000032d09f3ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Structured Variational Inference for Coupled Gaussian Processes. (arXiv:1711.01131v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01131"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01131", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sparse variational approximations allow for principled and scalable inference \nin Gaussian Process (GP) models. In settings where several GPs are part of the \ngenerative model, theses GPs are a posteriori coupled. For many applications \nsuch as regression where predictive accuracy is the quantity of interest, this \ncoupling is not crucial. Howewer if one is interested in posterior uncertainty, \nit cannot be ignored. A key element of variational inference schemes is the \nchoice of the approximate posterior parameterization. When the number of latent \nvariable is large, mean field (MF) methods provide fast and accurate posterior \nmeans while more structured posterior lead to inference algorithm of greater \ncomputational complexity. Here, we extend previous sparse GP approximations and \npropose a novel parameterization of variational posteriors in the multi-GP \nsetting allowing for fast and scalable inference capturing posterior \ndependencies. \n</p>"}, "author": "Vincent Adam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228282", "id": "tag:google.com,2005:reader/item/000000032d09f3e1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Accountability of AI Under the Law: The Role of Explanation. (arXiv:1711.01134v1 [cs.AI])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01134"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01134", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The ubiquity of systems using artificial intelligence or \"AI\" has brought \nincreasing attention to how those systems should be regulated. The choice of \nhow to regulate AI systems will require care. AI systems have the potential to \nsynthesize large amounts of data, allowing for greater levels of \npersonalization and precision than ever before---applications range from \nclinical decision support to autonomous driving and predictive policing. That \nsaid, there exist legitimate concerns about the intentional and unintentional \nnegative consequences of AI systems. There are many ways to hold AI systems \naccountable. In this work, we focus on one: explanation. Questions about a \nlegal right to explanation from AI systems was recently debated in the EU \nGeneral Data Protection Regulation, and thus thinking carefully about when and \nhow explanation from AI systems might improve accountability is timely. In this \nwork, we review contexts in which explanation is currently required under the \nlaw, and then list the technical considerations that must be considered if we \ndesired AI systems that could provide kinds of explanations that are currently \nrequired of humans. \n</p>"}, "author": "Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David O&#x27;Brien, Stuart Schieber, James Waldo, David Weinberger, Alexandra Wood", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228281", "id": "tag:google.com,2005:reader/item/000000032d09f3fa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A mathematical framework for graph signal processing of time-varying signals. (arXiv:1711.01191v1 [eess.SP])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01191"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01191", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a general framework from which to understand the design of filters \nfor time-series signals supported on graphs. We organize linear, time-invariant \nfilters into three increasingly restrictive classes of operators: linear \ntime-invariant filters, linear time-invariant filters which commute with a \ngraph operator, and linear time-invariant filters which are functions of a \ngraph operator. Using spectral theory, we show that these yield \n$\\mathcal{O}(n^2)$, $\\mathcal{O}(n)$, and $\\mathcal{O}(1)$ design parameters \nrespectively. We consider arbitrary graph operators as to accommodate \nnon-self-adjoint weight operators and all classes of graph Laplacian-based \noperators. We provide an example application of each class of filter. \n</p>"}, "author": "Addison Bohannon, Radu Balan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228280", "id": "tag:google.com,2005:reader/item/000000032d09f403", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Metrics for Deep Generative Models. (arXiv:1711.01204v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01204"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01204", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c85bdf9d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c85bdf9d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Neural samplers such as variational autoencoders (VAEs) or generative \nadversarial networks (GANs) approximate distributions by transforming samples \nfrom a simple random source---the latent space---to samples from a more complex \ndistribution represented by a dataset. While the manifold hypothesis implies \nthat the density induced by a dataset contains large regions of low density, \nthe training criterions of VAEs and GANs will make the latent space densely \ncovered. Consequently points that are separated by low-density regions in \nobservation space will be pushed together in latent space, making stationary \ndistances poor proxies for similarity. We transfer ideas from Riemannian \ngeometry to this setting, letting the distance between two points be the \nshortest path on a Riemannian manifold induced by the transformation. The \nmethod yields a principled distance measure, provides a tool for visual \ninspection of deep generative models, and an alternative to linear \ninterpolation in latent space. In addition, it can be applied for robot \nmovement generalization using previously learned skills. The method is \nevaluated on a synthetic dataset with known ground truth; on a simulated robot \narm dataset; on human motion capture data; and on a generative model of \nhandwritten digits. \n</p>"}, "author": "Nutan Chen, Alexej Klushyn, Richard Kurle, Xueyan Jiang, Justin Bayer, Patrick van der Smagt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509959154228", "timestampUsec": "1509959154228279", "id": "tag:google.com,2005:reader/item/000000032d09f40e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Lifelong Learning by Adjusting Priors. (arXiv:1711.01244v1 [stat.ML])", "published": 1509959154, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01244"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01244", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In representational lifelong learning an agent aims to continually learn to \nsolve novel tasks while updating its representation in light of previous tasks. \nUnder the assumption that future tasks are 'related' to previous tasks, \nrepresentations should be learned in such a way that they capture the common \nstructure across learned tasks, while allowing the learner sufficient \nflexibility to adapt to novel aspects of a new task. We develop a framework for \nlifelong learning in deep neural networks that is based on generalization \nbounds, developed within the PAC-Bayes framework. Learning takes place through \nthe construction of a distribution over networks based on the tasks seen so \nfar, and its utilization for learning a new task. Thus, prior knowledge is \nincorporated through setting a history-dependent prior for novel tasks. We \ndevelop a gradient-based algorithm implementing these ideas, based on \nminimizing an objective function motivated by generalization bounds, and \ndemonstrate its effectiveness through numerical examples. In addition to \nestablishing the improved performance available through lifelong learning, we \ndemonstrate the intuitive way by which prior information is manifested at \ndifferent levels of the network. \n</p>"}, "author": "Ron Amit, Ron Meir", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509932583470", "timestampUsec": "1509932583469668", "id": "tag:google.com,2005:reader/item/000000032cdb10b0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Does Phase Matter For Monaural Source Separation?. (arXiv:1711.00913v1 [cs.SD])", "published": 1509932584, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00913"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00913", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The \"cocktail party\" problem of fully separating multiple sources from a \nsingle channel audio waveform remains unsolved. Current biological \nunderstanding of neural encoding suggests that phase information is preserved \nand utilized at every stage of the auditory pathway. However, current \ncomputational approaches primarily discard phase information in order to mask \namplitude spectrograms of sound. In this paper, we seek to address whether \npreserving phase information in spectral representations of sound provides \nbetter results in monaural separation of vocals from a musical track by using a \nneurally plausible sparse generative model. Our results demonstrate that \npreserving phase information reduces artifacts in the separated tracks, as \nquantified by the signal to artifact ratio (GSAR). Furthermore, our proposed \nmethod achieves state-of-the-art performance for source separation, as \nquantified by a mean signal to interference ratio (GSIR) of 19.46. \n</p>"}, "author": "Mohit Dubey, Garrett Kenyon, Nils Carlson, Austin Thresher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509932583470", "timestampUsec": "1509932583469667", "id": "tag:google.com,2005:reader/item/000000032cdb10b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Running Time Analysis of the (1+1)-EA for OneMax and LeadingOnes under Bit-wise Noise. (arXiv:1711.00956v1 [cs.NE])", "published": 1509932584, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00956"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00956", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In many real-world optimization problems, the objective function evaluation \nis subject to noise, and we cannot obtain the exact objective value. \nEvolutionary algorithms (EAs), a type of general-purpose randomized \noptimization algorithm, have shown able to solve noisy optimization problems \nwell. However, previous theoretical analyses of EAs mainly focused on \nnoise-free optimization, which makes the theoretical understanding largely \ninsufficient. Meanwhile, the few existing theoretical studies under noise often \nconsidered the one-bit noise model, which flips a randomly chosen bit of a \nsolution before evaluation; while in many realistic applications, several bits \nof a solution can be changed simultaneously. In this paper, we study a natural \nextension of one-bit noise, the bit-wise noise model, which independently flips \neach bit of a solution with some probability. We analyze the running time of \nthe (1+1)-EA solving OneMax and LeadingOnes under bit-wise noise for the first \ntime, and derive the ranges of the noise level for polynomial and \nsuper-polynomial running time bounds. The analysis on LeadingOnes under \nbit-wise noise can be easily transferred to one-bit noise, and improves the \npreviously known results. Since our analysis discloses that the (1+1)-EA can be \nefficient only under low noise levels, we also study whether the sampling \nstrategy can bring robustness to noise. We prove that using sampling can \nsignificantly increase the largest noise level allowing a polynomial running \ntime, that is, sampling is robust to noise. \n</p>"}, "author": "Chao Qian, Chao Bian, Wu Jiang, Ke Tang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509932583470", "timestampUsec": "1509932583469666", "id": "tag:google.com,2005:reader/item/000000032cdb10bb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Classification-Based Perspective on GAN Distributions. (arXiv:1711.00970v3 [cs.LG] UPDATED)", "published": 1511260348, "updated": 1511260350, "canonical": [{"href": "http://arxiv.org/abs/1711.00970"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00970", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A fundamental, and still largely unanswered, question in the context of \nGenerative Adversarial Networks (GANs) is whether GANs are actually able to \ncapture the key characteristics of the datasets they are trained on. The \ncurrent approaches to examining this issue require significant human \nsupervision, such as visual inspection of sampled images, and often offer only \nfairly limited scalability. In this paper, we propose new techniques that \nemploy a classification-based perspective to evaluate synthetic GAN \ndistributions and their capability to accurately reflect the essential \nproperties of the training data. These techniques require only minimal human \nsupervision and can easily be scaled and adapted to evaluate a variety of \nstate-of-the-art GANs on large, popular datasets. Our analysis indicates that \nGANs have significant problems in reproducing the more distributional \nproperties of the training dataset. In particular, when seen through the lens \nof classification, the diversity of GAN data is orders of magnitude less than \nthat of the original data. \n</p>"}, "author": "Shibani Santurkar, Ludwig Schmidt, Aleksander M&#x105;dry", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509932583470", "timestampUsec": "1509932583469665", "id": "tag:google.com,2005:reader/item/000000032cdb10bd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convolutional Drift Networks for Video Classification. (arXiv:1711.01201v1 [cs.CV])", "published": 1509932584, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01201"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01201", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Analyzing spatio-temporal data like video is a challenging task that requires \nprocessing visual and temporal information effectively. Convolutional Neural \nNetworks have shown promise as baseline fixed feature extractors through \ntransfer learning, a technique that helps minimize the training cost on visual \ninformation. Temporal information is often handled using hand-crafted features \nor Recurrent Neural Networks, but this can be overly specific or prohibitively \ncomplex. Building a fully trainable system that can efficiently analyze \nspatio-temporal data without hand-crafted features or complex training is an \nopen challenge. We present a new neural network architecture to address this \nchallenge, the Convolutional Drift Network (CDN). Our CDN architecture combines \nthe visual feature extraction power of deep Convolutional Neural Networks with \nthe intrinsically efficient temporal processing provided by Reservoir \nComputing. In this introductory paper on the CDN, we provide a very simple \nbaseline implementation tested on two egocentric (first-person) video activity \ndatasets.We achieve video-level activity classification results on-par with \nstate-of-the art methods. Notably, performance on this complex spatio-temporal \ntask was produced by only training a single feed-forward layer in the CDN. \n</p>"}, "author": "Dillon Graham, Seyed Hamed Fatemi Langroudi, Christopher Kanan, Dhireesha Kudithipudi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509932583470", "timestampUsec": "1509932583469664", "id": "tag:google.com,2005:reader/item/000000032cdb10c3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning. (arXiv:1711.01239v1 [cs.LG])", "published": 1509932584, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01239"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01239", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multi-task learning (MTL) with neural networks leverages commonalities in \ntasks to improve performance, but often suffers from task interference which \nreduces transfer. To address this issue we introduce the routing network \nparadigm, a novel neural network unit and training algorithm. A routing network \nis a kind of self-organizing neural network consisting of two components: a \nrouter and a set of one or more function blocks. A function block may be any \nneural network - for example a fully-connected or a convolutional layer. Given \nan input the router makes a routing decision, choosing a function block to \napply and passing the output back to the router recursively, terminating when \nthe router decides to stop or a fixed recursion depth is reached. In this way \nthe routing network dynamically composes different function blocks for each \ninput. We employ a collaborative multi-agent reinforcement learning (MARL) \napproach to jointly train the router and function blocks. We evaluate our model \non multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our \nexperiments demonstrate significant improvement in accuracy with sharper \nconvergence over challenging joint training baselines for these tasks. \n</p>"}, "author": "Clemens Rosenbaum, Tim Klinger, Matthew Riemer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509932583470", "timestampUsec": "1509932583469663", "id": "tag:google.com,2005:reader/item/000000032cdb10c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "ResBinNet: Residual Binary Neural Network. (arXiv:1711.01243v1 [cs.LG])", "published": 1509932584, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.01243"}], "alternate": [{"href": "http://arxiv.org/abs/1711.01243", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent efforts on training light-weight binary neural networks offer \npromising execution/memory efficiency. This paper introduces ResBinNet, which \nis a composition of two interlinked methodologies aiming to address the slow \nconvergence speed and limited accuracy of binary convolutional neural networks. \nThe first method, called residual binarization, learns a multi-level binary \nrepresentation for the features within a certain neural network layer. The \nsecond method, called temperature adjustment, gradually binarizes the weights \nof a particular layer. The two methods jointly learn a set of soft-binarized \nparameters that improve the convergence rate and accuracy of binary neural \nnetworks. We corroborate the applicability and scalability of ResBinNet by \nimplementing a prototype hardware accelerator. The accelerator is \nreconfigurable in terms of the numerical precision of the binarized features, \noffering a trade-off between runtime and inference accuracy. \n</p>"}, "author": "Mohammad Ghasemzadeh, Mohammad Samragh, Farinaz Koushanfar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594374", "id": "tag:google.com,2005:reader/item/000000032b1d5d95", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "School bus routing by maximizing trip compatibility. (arXiv:1711.00530v2 [math.OC] UPDATED)", "published": 1510876816, "updated": 1510876820, "canonical": [{"href": "http://arxiv.org/abs/1711.00530"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00530", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>School bus planning is usually divided into routing and scheduling due to the \ncomplexity of solving them concurrently. However, the separation between these \ntwo steps may lead to worse solutions with higher overall costs than that from \nsolving them together. When finding the minimal number of trips in the routing \nproblem, neglecting the importance of trip compatibility may increase the \nnumber of buses actually needed in the scheduling problem. This paper proposes \na new formulation for the multi-school homogeneous fleet routing problem that \nmaximizes trip compatibility while minimizing total travel time. This \nincorporates the trip compatibility for the scheduling problem in the routing \nproblem. Since the problem is inherently just a routing problem, finding a good \nsolution is not cumbersome. To compare the performance of the model with \ntraditional routing problems, we generate eight mid-size data sets. Through \nimporting the generated trips of the routing problems into the bus scheduling \n(blocking) problem, it is shown that the proposed model uses up to 13% fewer \nbuses than the common traditional routing models. \n</p>"}, "author": "Ali Shafahi, Zhongxiang Wang, Ali Haghani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594373", "id": "tag:google.com,2005:reader/item/000000032b1d5d9c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "An iterative school decomposition algorithm for solving the multi-school bus routing and scheduling problem. (arXiv:1711.00532v2 [math.OC] UPDATED)", "published": 1510876816, "updated": 1510876820, "canonical": [{"href": "http://arxiv.org/abs/1711.00532"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00532", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Servicing the school transportation demand safely with a minimum number of \nbuses is one of the highest financial goals for school transportation \ndirectors. To achieve that objective, a good and efficient way to solve the \nrouting and scheduling problem is required. Due to the growth of the computing \npower, the spotlight has been shed on solving the combined problem of the \nschool bus routing and scheduling. A recent attempt tried to model the routing \nproblem by maximizing the trip compatibilities with the hope of requiring fewer \nbuses in the scheduling problem. However, an over-counting problem associated \nwith trip compatibility could diminish the performance of this approach. An \nextended model is proposed in this paper to resolve this issue along with an \niterative solution algorithm. This extended model is an integrated model for \nmulti-school bus routing and scheduling problem. The result shows better \nsolutions for 8 test problems can be found with a fewer number of buses (up to \n25%) and shorter travel time (up to 7% per trip). \n</p>"}, "author": "Zhongxiang Wang, Ali Shafahi, Ali Haghani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594372", "id": "tag:google.com,2005:reader/item/000000032b1d5da3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Beautiful and damned. Combined effect of content quality and social ties on user engagement. (arXiv:1711.00536v1 [cs.SI])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00536"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00536", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c85be388\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c85be388&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>User participation in online communities is driven by the intertwinement of \nthe social network structure with the crowd-generated content that flows along \nits links. These aspects are rarely explored jointly and at scale. By looking \nat how users generate and access pictures of varying beauty on Flickr, we \ninvestigate how the production of quality impacts the dynamics of online social \nsystems. We develop a deep learning computer vision model to score images \naccording to their aesthetic value and we validate its output through \ncrowdsourcing. By applying it to over 15B Flickr photos, we study for the first \ntime how image beauty is distributed over a large-scale social system. \nBeautiful images are evenly distributed in the network, although only a small \ncore of people get social recognition for them. To study the impact of exposure \nto quality on user engagement, we set up matching experiments aimed at \ndetecting causality from observational data. Exposure to beauty is \ndouble-edged: following people who produce high-quality content increases one's \nprobability of uploading better photos; however, an excessive imbalance between \nthe quality generated by a user and the user's neighbors leads to a decline in \nengagement. Our analysis has practical implications for improving link \nrecommender systems. \n</p>"}, "author": "Luca M. Aiello, Rossano Schifanella, Miriam Redi, Stacey Svetlichnaya, Frank Liu, Simon Osindero", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594371", "id": "tag:google.com,2005:reader/item/000000032b1d5daa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "TasNet: time-domain audio separation network for real-time, single-channel speech separation. (arXiv:1711.00541v1 [cs.SD])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00541"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00541", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c866d11e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c866d11e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Robust speech processing in multi-talker environments requires effective \nspeech separation. Recent deep learning systems have made significant progress \ntoward solving this problem, yet it remains challenging particularly in \nreal-time, short latency applications. Most methods attempt to construct a mask \nfor each source in time-frequency representation of the mixture signal which is \nnot necessarily an optimal representation for speech separation. In addition, \ntime-frequency decomposition results in inherent problems such as \nphase/magnitude decoupling and long time window which is required to achieve \nsufficient frequency resolution. We propose Time-domain Audio Separation \nNetwork (TasNet) to overcome these limitations. We directly model the signal in \nthe time-domain using encoder-decoder framework and perform the source \nseparation on nonnegative encoder outputs. This method removes the frequency \ndecomposition step and reduces the separation problem to estimation of source \nmasks on encoder outputs which is then synthesized by the decoder. Our system \noutperforms the current state-of-the-art causal speech separation algorithms, \nreduces the computational cost of speech separation, and significantly reduces \nthe minimum required latency of the output. This makes TasNet suitable for \napplications where low-power, real-time implementation is desirable such as in \nhearable and telecommunication devices. \n</p>"}, "author": "Yi Luo, Nima Mesgarani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594370", "id": "tag:google.com,2005:reader/item/000000032b1d5db8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Just ASK: Building an Architecture for Extensible Self-Service Spoken Language Understanding. (arXiv:1711.00549v1 [cs.CL])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00549"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00549", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents the design of the machine learning architecture that \nunderlies the Alexa Skills Kit (ASK), which was the first Spoken Language \nUnderstanding (SLU) Software Development Kit (SDK) for a virtual digital \nassistant, as far as we are aware. At Amazon, the infrastructure powers more \nthan 20,000 skills built through the ASK, as well as AWS's Amazon Lex SLU \nService. The ASK emphasizes flexibility, predictability and a rapid iteration \ncycle for third party developers. It imposes inductive biases that allow it to \nlearn robust SLU models from extremely small and sparse datasets and, in doing \nso, removes significant barriers to entry for software developers and dialog \nsystems researchers. \n</p>"}, "author": "Anjishnu Kumar, Arpit Gupta, Julian Chan, Sam Tucker, Bjorn Hoffmeister, Markus Dreyer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594369", "id": "tag:google.com,2005:reader/item/000000032b1d5dc2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interpretable and Pedagogical Examples. (arXiv:1711.00694v1 [cs.AI])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00694"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00694", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Teachers intentionally pick the most informative examples to show their \nstudents. However, if the teacher and student are neural networks, the examples \nthat the teacher network learns to give, although effective at teaching the \nstudent, are typically uninterpretable. We show that training the student and \nteacher iteratively, rather than jointly, can produce interpretable teaching \nstrategies. We evaluate interpretability by (1) measuring the similarity of the \nteacher's emergent strategies to intuitive strategies in each domain and (2) \nconducting human experiments to evaluate how effective the teacher's strategies \nare at teaching humans. We show that the teacher network learns to select or \ngenerate interpretable, pedagogical examples to teach rule-based, \nprobabilistic, boolean, and hierarchical concepts. \n</p>"}, "author": "Smitha Milli, Pieter Abbeel, Igor Mordatch", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594368", "id": "tag:google.com,2005:reader/item/000000032b1d5dc8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adaptive coordination of working-memory and reinforcement learning in non-human primates performing a trial-and-error problem solving task. (arXiv:1711.00698v1 [cs.AI])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00698"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00698", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Accumulating evidence suggest that human behavior in trial-and-error learning \ntasks based on decisions between discrete actions may involve a combination of \nreinforcement learning (RL) and working-memory (WM). While the understanding of \nbrain activity at stake in this type of tasks often involve the comparison with \nnon-human primate neurophysiological results, it is not clear whether monkeys \nuse similar combined RL and WM processes to solve these tasks. Here we analyzed \nthe behavior of five monkeys with computational models combining RL and WM. Our \nmodel-based analysis approach enables to not only fit trial-by-trial choices \nbut also transient slowdowns in reaction times, indicative of WM use. We found \nthat the behavior of the five monkeys was better explained in terms of a \ncombination of RL and WM despite inter-individual differences. The same \ncoordination dynamics we used in a previous study in humans best explained the \nbehavior of some monkeys while the behavior of others showed the opposite \npattern, revealing a possible different dynamics of WM process. We further \nanalyzed different variants of the tested models to open a discussion on how \nthe long pretraining in these tasks may have favored particular coordination \ndynamics between RL and WM. This points towards either inter-species \ndifferences or protocol differences which could be further tested in humans. \n</p>"}, "author": "Guillaume Viejo (ISIR), Beno&#xee;t Girard (ISIR), Emmanuel Procyk, Mehdi Khamassi (ISIR)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594367", "id": "tag:google.com,2005:reader/item/000000032b1d5dd2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to Represent Programs with Graphs. (arXiv:1711.00740v1 [cs.LG])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00740"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00740", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning tasks on source code (i.e., formal languages) have been considered \nrecently, but most work has tried to transfer natural language methods and does \nnot capitalize on the unique opportunities offered by code's known syntax. For \nexample, long-range dependencies induced by using the same variable or function \nin distant locations are often not considered. We propose to use graphs to \nrepresent both the syntactic and semantic structure of code and use graph-based \ndeep learning methods to learn to reason over program structures. \n</p> \n<p>In this work, we present how to construct graphs from source code and how to \nscale Gated Graph Neural Networks training to such large graphs. We evaluate \nour method on two tasks: VarNaming, in which a network attempts to predict the \nname of a variable given its usage, and VarMisuse, in which the network learns \nto reason about selecting the correct variable that should be used at a given \nprogram location. Our comparison to methods that use less structured program \nrepresentations shows the advantages of modeling known structure, and suggests \nthat our models learn to infer meaningful names and to solve the VarMisuse task \nin many cases. Additionally, our testing showed that VarMisuse identifies a \nnumber of bugs in mature open-source projects. \n</p>"}, "author": "Miltiadis Allamanis, Marc Brockschmidt, Mahmoud Khademi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594366", "id": "tag:google.com,2005:reader/item/000000032b1d5dda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Framework for evaluation of sound event detection in web videos. (arXiv:1711.00804v1 [cs.SD])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00804"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00804", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The largest source of sound events is web videos. Most videos lack sound \nevent labels at segment level, however, a significant number of them do respond \nto text queries, from a match found to their metadata by the search engine. In \nthis paper we explore the extent to which a search query could be used as the \ntrue label for the presence of sound events in the videos. For this, we \ndeveloped a framework for large-scale sound event recognition on web videos. \nThe framework crawls videos using search queries corresponding to 78 sound \nevent labels drawn from three datasets. The datasets are used to train three \nclassifiers, which were then run on 3.7 million video segments. We evaluated \nperformance using the search query as the true label and compare it (on a \nsubset) with human labeling. Both types exhibited close performance, to within \n10%, and similar performance trends as the number of evaluated segments \nincreased. Hence, our experiments show potential for using search query as a \npreliminary true label for sound events in web videos. \n</p>"}, "author": "Rohan Badlani, Ankit Shah, Benjamin Elizalde, Anurag Kumar, Bhiksha Raj", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594365", "id": "tag:google.com,2005:reader/item/000000032b1d5de4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning. (arXiv:1711.00832v1 [cs.AI])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00832"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00832", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>To achieve general intelligence, agents must learn how to interact with \nothers in a shared environment: this is the challenge of multiagent \nreinforcement learning (MARL). The simplest form is independent reinforcement \nlearning (InRL), where each agent treats its experience as part of its \n(non-stationary) environment. In this paper, we first observe that policies \nlearned using InRL can overfit to the other agents' policies during training, \nfailing to sufficiently generalize during execution. We introduce a new metric, \njoint-policy correlation, to quantify this effect. We describe an algorithm for \ngeneral MARL, based on approximate best responses to mixtures of policies \ngenerated using deep reinforcement learning, and empirical game-theoretic \nanalysis to compute meta-strategies for policy selection. The algorithm \ngeneralizes previous ones such as InRL, iterated best response, double oracle, \nand fictitious play. Then, we present a scalable implementation which reduces \nthe memory requirement using decoupled meta-solvers. Finally, we demonstrate \nthe generality of the resulting policies in two partially observable settings: \ngridworld coordination games and poker. \n</p>"}, "author": "Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, Thore Graepel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594364", "id": "tag:google.com,2005:reader/item/000000032b1d5ded", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variational Inference of Disentangled Latent Concepts from Unlabeled Observations. (arXiv:1711.00848v1 [cs.LG])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00848"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00848", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Disentangled representations, where the higher level data generative factors \nare reflected in disjoint latent dimensions, offer several benefits such as \nease of deriving invariant representations, transferability to other tasks, \ninterpretability, etc. We consider the problem of unsupervised learning of \ndisentangled representations from large pool of unlabeled observations, and \npropose a variational inference based approach to infer disentangled latent \nfactors. We introduce a regularizer on the expectation of the approximate \nposterior over observed data that encourages the disentanglement. We evaluate \nthe proposed approach using several quantitative metrics and empirically \nobserve significant gains over existing methods in terms of both \ndisentanglement and data likelihood (reconstruction quality). \n</p>"}, "author": "Abhishek Kumar, Prasanna Sattigeri, Avinash Balakrishnan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594363", "id": "tag:google.com,2005:reader/item/000000032b1d5df8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope. (arXiv:1711.00851v1 [cs.LG])", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00851"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00851", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a method to learn deep ReLU-based classifiers that are provably \nrobust against norm-bounded adversarial perturbations (on the training data; \nfor previously unseen examples, the approach will be guaranteed to detect all \nadversarial examples, though it may flag some non-adversarial examples as \nwell). The basic idea of the approach is to consider a convex outer \napproximation of the set of activations reachable through a norm-bounded \nperturbation, and we develop a robust optimization procedure that minimizes the \nworst case loss over this outer region (via a linear program). Crucially, we \nshow that the dual problem to this linear program can be represented itself as \na deep network similar to the backpropagation network, leading to very \nefficient optimization approaches that produce guaranteed bounds on the robust \nloss. The end result is that by executing a few more forward and backward \npasses through a slightly modified version of the original network (though \npossibly with much larger batch sizes), we can learn a classifier that is \nprovably robust to any norm-bounded adversarial attack. We illustrate the \napproach on a toy 2D robust classification task, and on a simple convolutional \narchitecture applied to MNIST, where we produce a classifier that provably has \nless than 8.4% test error for any adversarial attack with bounded $\\ell_\\infty$ \nnorm less than $\\epsilon = 0.1$. This represents the largest verified network \nthat we are aware of, and we discuss future challenges in scaling the approach \nto much larger domains. \n</p>"}, "author": "J. Zico Kolter, Eric Wong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669620594", "timestampUsec": "1509669620594358", "id": "tag:google.com,2005:reader/item/000000032b1d5e3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Separation of Water and Fat Magnetic Resonance Imaging Signals Using Deep Learning with Convolutional Neural Networks. (arXiv:1711.00107v1 [cs.CV] CROSS LISTED)", "published": 1509669621, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00107"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00107", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c866d3b7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c866d3b7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Purpose: A new method for magnetic resonance (MR) imaging water-fat \nseparation using a convolutional neural network (ConvNet) and deep learning \n(DL) is presented. Feasibility of the method with complex and magnitude images \nis demonstrated with a series of patient studies and accuracy of predicted \nquantitative values is analyzed. \n</p> \n<p>Methods: Water-fat separation of 1200 gradient-echo acquisitions from 90 \nimaging sessions (normal, acute and chronic myocardial infarction) was \nperformed using a conventional model based method with modeling of R2* and \noff-resonance and a multi-peak fat spectrum. A U-Net convolutional neural \nnetwork for calculation of water-only, fat-only, R2* and off-resonance images \nwas trained with 900 gradient-echo Multiple and single-echo complex and \nmagnitude input data algorithms were studied and compared to conventional \nextended echo modeling. \n</p> \n<p>Results: The U-Net ConvNet was easily trained and provided water-fat \nseparation results visually comparable to conventional methods. Myocardial fat \ndeposition in chronic myocardial infarction and intramyocardial hemorrhage in \nacute myocardial infarction were well visualized in the DL results. Predicted \nvalues for R2*, off-resonance, water and fat signal intensities were well \ncorrelated with conventional model based water fat separation (R2&gt;=0.97, \np&lt;0.001). DL images had a 14% higher signal-to-noise ratio (p&lt;0.001) when \ncompared to the conventional method. \n</p> \n<p>Conclusion: Deep learning utilizing ConvNets is a feasible method for MR \nwater-fat separationimaging with complex, magnitude and single echo image data. \nA trained U-Net can be efficiently used for MR water-fat separation, providing \nresults comparable to conventional model based methods. \n</p>"}, "author": "James W Goldfarb", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669484341", "timestampUsec": "1509669484340794", "id": "tag:google.com,2005:reader/item/000000032b1d2085", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning with Latent Language. (arXiv:1711.00482v1 [cs.CL])", "published": 1509669485, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00482"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00482", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The named concepts and compositional operators present in natural language \nprovide a rich source of information about the kinds of abstractions humans use \nto navigate the world. Can this linguistic background knowledge improve the \ngenerality and efficiency of learned classifiers and control policies? This \npaper aims to show that using the space of natural language strings as a \nparameter space is an effective way to capture natural task structure. In a \npretraining phase, we learn a language interpretation model that transforms \ninputs (e.g. images) into outputs (e.g. labels) given natural language \ndescriptions. To learn a new concept (e.g. a classifier), we search directly in \nthe space of descriptions to minimize the interpreter's loss on training \nexamples. Crucially, our models do not require language data to learn these \nconcepts: language is used only in pretraining to impose structure on \nsubsequent learning. Results on image classification, text editing, and \nreinforcement learning show that, in all settings, models with a linguistic \nparameterization outperform those without. \n</p>"}, "author": "Jacob Andreas, Dan Klein, Sergey Levine", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669484341", "timestampUsec": "1509669484340793", "id": "tag:google.com,2005:reader/item/000000032b1d208c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "TasNet: time-domain audio separation network for real-time, single-channel speech separation. (arXiv:1711.00541v1 [cs.SD])", "published": 1509669485, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00541"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00541", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Robust speech processing in multi-talker environments requires effective \nspeech separation. Recent deep learning systems have made significant progress \ntoward solving this problem, yet it remains challenging particularly in \nreal-time, short latency applications. Most methods attempt to construct a mask \nfor each source in time-frequency representation of the mixture signal which is \nnot necessarily an optimal representation for speech separation. In addition, \ntime-frequency decomposition results in inherent problems such as \nphase/magnitude decoupling and long time window which is required to achieve \nsufficient frequency resolution. We propose Time-domain Audio Separation \nNetwork (TasNet) to overcome these limitations. We directly model the signal in \nthe time-domain using encoder-decoder framework and perform the source \nseparation on nonnegative encoder outputs. This method removes the frequency \ndecomposition step and reduces the separation problem to estimation of source \nmasks on encoder outputs which is then synthesized by the decoder. Our system \noutperforms the current state-of-the-art causal speech separation algorithms, \nreduces the computational cost of speech separation, and significantly reduces \nthe minimum required latency of the output. This makes TasNet suitable for \napplications where low-power, real-time implementation is desirable such as in \nhearable and telecommunication devices. \n</p>"}, "author": "Yi Luo, Nima Mesgarani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669484341", "timestampUsec": "1509669484340792", "id": "tag:google.com,2005:reader/item/000000032b1d2093", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Just ASK: Building an Architecture for Extensible Self-Service Spoken Language Understanding. (arXiv:1711.00549v1 [cs.CL])", "published": 1509669485, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00549"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00549", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents the design of the machine learning architecture that \nunderlies the Alexa Skills Kit (ASK), which was the first Spoken Language \nUnderstanding (SLU) Software Development Kit (SDK) for a virtual digital \nassistant, as far as we are aware. At Amazon, the infrastructure powers more \nthan 20,000 skills built through the ASK, as well as AWS's Amazon Lex SLU \nService. The ASK emphasizes flexibility, predictability and a rapid iteration \ncycle for third party developers. It imposes inductive biases that allow it to \nlearn robust SLU models from extremely small and sparse datasets and, in doing \nso, removes significant barriers to entry for software developers and dialog \nsystems researchers. \n</p>"}, "author": "Anjishnu Kumar, Arpit Gupta, Julian Chan, Sam Tucker, Bjorn Hoffmeister, Markus Dreyer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669484341", "timestampUsec": "1509669484340791", "id": "tag:google.com,2005:reader/item/000000032b1d209b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Performance Evaluation of Channel Decoding With Deep Neural Networks. (arXiv:1711.00727v1 [eess.SP])", "published": 1509669485, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00727"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00727", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the demand of high data rate and low latency in fifth generation (5G), \ndeep neural network decoder (NND) has become a promising candidate due to its \ncapability of one-shot decoding and parallel computing. In this paper, three \ntypes of NND, i.e., multi-layer perceptron (MLP), convolution neural network \n(CNN) and recurrent neural network (RNN), are proposed with the same parameter \nmagnitude. The performance of these deep neural networks are evaluated through \nextensive simulation. Numerical results show that RNN has the best decoding \nperformance, yet at the price of the highest computational overhead. Moreover, \nwe find there exists a saturation length for each type of neural network, which \nis caused by their restricted learning abilities. \n</p>"}, "author": "Wei Lyu, Zhaoyang Zhang, Chunxu Jiao, Kangjian Qin, Huazi Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895487", "id": "tag:google.com,2005:reader/item/000000032b1cf407", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tensor Valued Common and Individual Feature Extraction: Multi-dimensional Perspective. (arXiv:1711.00487v1 [eess.SP])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00487"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00487", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A novel method for common and individual feature analysis from exceedingly \nlarge-scale data is proposed, in order to ensure the tractability of both the \ncomputation and storage and thus mitigate the curse of dimensionality, a major \nbottleneck in modern data science. This is achieved by making use of the \ninherent redundancy in so-called multi-block data structures, which represent \nmultiple observations of the same phenomenon taken at different times, angles \nor recording conditions. Upon providing an intrinsic link between the \nproperties of the outer vector product and extracted features in tensor \ndecompositions (TDs), the proposed common and individual information extraction \nfrom multi-block data is performed through imposing physical meaning to \notherwise unconstrained factorisation approaches. This is shown to dramatically \nreduce the dimensionality of search spaces for subsequent classification \nprocedures and to yield greatly enhanced accuracy. Simulations on a multi-class \nclassification task of large-scale extraction of individual features from a \ncollection of partially related real-world images demonstrate the advantages of \nthe \"blessing of dimensionality\" associated with TDs. \n</p>"}, "author": "Ilia Kisil, Giuseppe G. Calvi, Danilo P. Mandic", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895486", "id": "tag:google.com,2005:reader/item/000000032b1cf44d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Don't Decay the Learning Rate, Increase the Batch Size. (arXiv:1711.00489v1 [cs.LG])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00489"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00489", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is common practice to decay the learning rate. Here we show one can \nusually obtain the same learning curve on both training and test sets by \ninstead increasing the batch size during training. This procedure is successful \nfor stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, \nand Adam. It reaches equivalent test accuracies after the same number of \ntraining epochs, but with fewer parameter updates, leading to greater \nparallelism and shorter training times. We can further reduce the number of \nparameter updates by increasing the learning rate $\\epsilon$ and scaling the \nbatch size $B \\propto \\epsilon$. Finally, one can increase the momentum \ncoefficient $m$ and scale $B \\propto 1/(1-m)$, although this tends to slightly \nreduce the test accuracy. Crucially, our techniques allow us to repurpose \nexisting training schedules for large batch training with no hyper-parameter \ntuning. We train Inception-ResNet-V2 on ImageNet to $77\\%$ validation accuracy \nin under 2500 parameter updates, efficiently utilizing training batches of \n65536 images. \n</p>"}, "author": "Samuel L. Smith, Pieter-Jan Kindermans, Quoc V. Le", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895485", "id": "tag:google.com,2005:reader/item/000000032b1cf4b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning One-hidden-layer Neural Networks with Landscape Design. (arXiv:1711.00501v2 [cs.LG] UPDATED)", "published": 1509959154, "updated": 1509959156, "canonical": [{"href": "http://arxiv.org/abs/1711.00501"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00501", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of learning a one-hidden-layer neural network: we \nassume the input $x\\in \\mathbb{R}^d$ is from Gaussian distribution and the \nlabel $y = a^\\top \\sigma(Bx) + \\xi$, where $a$ is a nonnegative vector in \n$\\mathbb{R}^m$ with $m\\le d$, $B\\in \\mathbb{R}^{m\\times d}$ is a full-rank \nweight matrix, and $\\xi$ is a noise vector. We first give an analytic formula \nfor the population risk of the standard squared loss and demonstrate that it \nimplicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n</p> \n<p>Inspired by the formula, we design a non-convex objective function $G(\\cdot)$ \nwhose landscape is guaranteed to have the following properties: 1. All local \nminima of $G$ are also global minima. \n</p> \n<p>2. All global minima of $G$ correspond to the ground truth parameters. \n</p> \n<p>3. The value and gradient of $G$ can be estimated using samples. \n</p> \n<p>With these properties, stochastic gradient descent on $G$ provably converges \nto the global minimum and learn the ground-truth parameters. We also prove \nfinite sample complexity result and validate the results by simulations. \n</p>"}, "author": "Rong Ge, Jason D. Lee, Tengyu Ma", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895484", "id": "tag:google.com,2005:reader/item/000000032b1cf500", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Sleep Stage Classification Based on Multi-level Feature Learning and Recurrent Neural Networks via Wearable Device. (arXiv:1711.00629v1 [stat.ML])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00629"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00629", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a practical approach for automatic sleep stage \nclassification based on a multi-level feature learning framework and Recurrent \nNeural Network (RNN) classifier using heart rate and wrist actigraphy derived \nfrom a wearable device. The feature learning framework is designed to extract \nlow- and mid-level features. Low-level features capture temporal and frequency \ndomain properties and mid-level features learn compositions and structural \ninformation of signals. Since sleep staging is a sequential problem with \nlong-term dependencies, we take advantage of RNNs with Bidirectional Long \nShort-Term Memory (BLSTM) architectures for sequence data learning. To simulate \nthe actual situation of daily sleep, experiments are conducted with a resting \ngroup in which sleep is recorded in resting state, and a comprehensive group in \nwhich both resting sleep and non-resting sleep are included.We evaluate the \nalgorithm based on an eight-fold cross validation to classify five sleep stages \n(W, N1, N2, N3, and REM). The proposed algorithm achieves weighted precision, \nrecall and F1 score of 58.0%, 60.3%, and 58.2% in the resting group and 58.5%, \n61.1%, and 58.5% in the comprehensive group, respectively. Various comparison \nexperiments demonstrate the effectiveness of feature learning and BLSTM. We \nfurther explore the influence of depth and width of RNNs on performance. Our \nmethod is specially proposed for wearable devices and is expected to be \napplicable for long-term sleep monitoring at home. Without using too much prior \ndomain knowledge, our method has the potential to generalize sleep disorder \ndetection. \n</p>"}, "author": "Xin Zhang, Weixuan Kou, Eric I-Chao Chang, He Gao, Yubo Fan, Yan Xu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895483", "id": "tag:google.com,2005:reader/item/000000032b1cf537", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Recurrent Neural Network Models for Forecasting and Quantifying Uncertainty in Spatial-Temporal Data. (arXiv:1711.00636v1 [stat.ME])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00636"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00636", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural networks (RNNs) are nonlinear dynamical models commonly used \nin the machine learning and dynamical systems literature to represent complex \ndynamical or sequential relationships between variables. More recently, as deep \nlearning models have become more common, RNNs have been used to forecast \nincreasingly complicated systems. Dynamical spatio-temporal processes represent \na class of complex systems that can potentially benefit from these types of \nmodels. Although the RNN literature is expansive and thoroughly developed, \nuncertainty quantification is often ignored. Even when considered, the \nuncertainty is generally quantified without the use of a rigorous framework, \nsuch as a fully Bayesian setting. Here we attempt to quantify uncertainty in a \nmore formal framework while maintaining the forecast ac- curacy that makes \nthese models appealing, by presenting a Bayesian RNN model for nonlinear \nspatio-temporal forecasting. Additionally, we make simple modifications to the \nbasic RNN to help accommodate the unique nature of nonlinear spatio-temporal \ndata. The proposed model is applied to a multiscale Lorenz system and to \nlong-lead forecasting of tropical Pacific sea surface temperature. \n</p>"}, "author": "Patrick L. McDermott, Christopher K. Wikle", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895482", "id": "tag:google.com,2005:reader/item/000000032b1cf55f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Candidates v.s. Noises Estimation for Large Multi-Class Classification Problem. (arXiv:1711.00658v1 [stat.ML])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00658"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00658", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c866d5e7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c866d5e7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper proposes a method for multi-class classification problems, where \nthe number of classes $K$ is large. The method, referred to as {\\em Candidates \nv.s. Noises Estimation} (CANE), selects a small subset of candidate classes and \nsamples the remaining classes. We show that CANE is always consistent and \ncomputationally efficient. Moreover, the resulting estimator has low \nstatistical variance approaching that of the maximum likelihood estimator, when \nthe observed label belongs to the selected candidates with high probability. In \npractice, we use a tree structure with leaves as classes to promote fast beam \nsearch for candidate selection. We also apply the CANE method to estimate word \nprobabilities in neural language models. Experiments show that CANE achieves \nbetter prediction accuracy over the Noise-Contrastive Estimation (NCE), its \nvariants and a number of the state-of-the-art tree classifiers, while it gains \nsignificant speedup compared to the standard $\\mathcal{O}(K)$ methods. \n</p>"}, "author": "Lei Han, Tong Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895481", "id": "tag:google.com,2005:reader/item/000000032b1cf57f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Concave losses for robust dictionary learning. (arXiv:1711.00659v1 [cs.LG])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00659"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00659", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8714e44\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8714e44&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Traditional dictionary learning methods are based on quadratic convex loss \nfunction and thus are sensitive to outliers. In this paper, we propose a \ngeneric framework for robust dictionary learning based on concave losses. We \nprovide results on composition of concave functions, notably regarding \nsuper-gradient computations, that are key for developing generic dictionary \nlearning algorithms applicable to smooth and non-smooth losses. In order to \nimprove identification of outliers, we introduce an initialization heuristic \nbased on undercomplete dictionary learning. Experimental results using \nsynthetic and real data demonstrate that our method is able to better detect \noutliers, is capable of generating better dictionaries, outperforming \nstate-of-the-art methods such as K-SVD and LC-KSVD. \n</p>"}, "author": "Rafael Will M de Araujo (USP), Roberto Hirata (USP), Alain Rakotomamonjy (LITIS)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895480", "id": "tag:google.com,2005:reader/item/000000032b1cf6a1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast Information-theoretic Bayesian Optimisation. (arXiv:1711.00673v1 [stat.ML])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00673"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00673", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Information-theoretic Bayesian optimisation techniques have demonstrated \nstate-of-the-art performance in tackling important global optimisation \nproblems. However, current information-theoretic approaches: require many \napproximations in implementation; limit the choice of kernels available to \nmodel the objective; and introduce often-prohibitive computational overhead. We \ndevelop a fast information-theoretic Bayesian Optimisation method, FITBO, that \ncircumvents the need for sampling the global minimiser, thus significantly \nreducing computational overhead. Moreover, in comparison with existing \napproaches, our method faces fewer constraints on kernel choice and enjoys the \nmerits of dealing with the output space. We demonstrate empirically that FITBO \ninherits the performance associated with information-theoretic Bayesian \noptimisation, while being even faster than simpler Bayesian optimisation \napproaches, such as Expected Improvement. \n</p>"}, "author": "Binxin Ru, Michael Osborne, Mark McLeod", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895479", "id": "tag:google.com,2005:reader/item/000000032b1cf73a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Universal Marginalizer for Amortized Inference in Generative Models. (arXiv:1711.00695v1 [cs.LG])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00695"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00695", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of inference in a causal generative model where the \nset of available observations differs between data instances. We show how \ncombining samples drawn from the graphical model with an appropriate masking \nfunction makes it possible to train a single neural network to approximate all \nthe corresponding conditional marginal distributions and thus amortize the cost \nof inference. We further demonstrate that the efficiency of importance sampling \nmay be improved by basing proposals on the output of the neural network. We \nalso outline how the same network can be used to generate samples from an \napproximate joint posterior via a chain decomposition of the graph. \n</p>"}, "author": "Laura Douglas, Iliyan Zarov, Konstantinos Gourgoulias, Chris Lucas, Chris Hart, Adam Baker, Maneesh Sahani, Yura Perov, Saurabh Johri", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895478", "id": "tag:google.com,2005:reader/item/000000032b1cf7c2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Estimating Historical Hourly Traffic Volumes via Machine Learning and Vehicle Probe Data: A Maryland Case Study. (arXiv:1711.00721v1 [stat.ML])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00721"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00721", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper focuses on the problem of estimating historical traffic volumes \nbetween sparsely-located traffic sensors, which transportation agencies need to \naccurately compute statewide performance measures. To this end, the paper \nexamines applications of vehicle probe data, automatic traffic recorder counts, \nand neural network models to estimate hourly volumes in the Maryland highway \nnetwork, and proposes a novel approach that combines neural networks with an \nexisting profiling method. On average, the proposed approach yields 26% more \naccurate estimates than volume profiles, which are currently used by \ntransportation agencies across the US to compute statewide performance \nmeasures. The paper also quantifies the value of using vehicle probe data in \nestimating hourly traffic volumes, which provides important managerial insights \nto transportation agencies interested in acquiring this type of data. For \nexample, results show that volumes can be estimated with a mean absolute \npercent error of about 20% at locations where average number of observed probes \nis between 30 and 47 vehicles/hr, which provides a useful guideline for \nassessing the value of probe vehicle data from different vendors. \n</p>"}, "author": "Przemys&#x142;aw Seku&#x142;a, Nikola Markovi&#x107;, Zachary Vander Laan, Kaveh Farokhi Sadabadi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895477", "id": "tag:google.com,2005:reader/item/000000032b1cf838", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Network-size independent covering number bounds for deep networks. (arXiv:1711.00753v2 [cs.LG] UPDATED)", "published": 1510319034, "updated": 1510319036, "canonical": [{"href": "http://arxiv.org/abs/1711.00753"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00753", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We give a covering number bound for deep learning networks that is \nindependent of the size of the network. The key for the simple analysis is that \nfor linear classifiers, rotating the data doesn't affect the covering number. \nThus, we can ignore the rotation part of each layer's linear transformation, \nand get the covering number bound by concentrating on the scaling part. \n</p>"}, "author": "Mayank Kabra, Kristin Branson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895476", "id": "tag:google.com,2005:reader/item/000000032b1cf920", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Approximation of Functions over Manifolds: A Moving Least-Squares Approach. (arXiv:1711.00765v1 [stat.ML])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00765"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00765", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present an algorithm for approximating a function defined over a \n$d$-dimensional manifold utilizing only noisy function values at locations \nsampled from the manifold with noise. To produce the approximation we do not \nrequire any knowledge regarding the manifold other than its dimension $d$. The \napproximation scheme is based upon the Manifold Moving Least-Squares (MMLS). \nThe proposed algorithm is resistant to noise in both the domain and function \nvalues. Furthermore, the approximant is shown to be smooth and of approximation \norder of $O(h^{m+1})$ for non-noisy data, where $h$ is the mesh size with \nrespect to the manifold domain, and $m$ is the degree of a local polynomial \napproximation utilized in our algorithm. In addition, the proposed algorithm is \nlinear in time with respect to the ambient-space's dimension. Thus, in case of \nextremely large ambient space dimension, we are able to avoid the curse of \ndimensionality without having to perform non-linear dimension reduction, which \ninevitably introduces distortions to the manifold data. We compare, using \nnumerical experiments, the presented algorithm to state-of-the-art algorithms \nfor regression over manifolds and show its potential. \n</p>"}, "author": "Barak Sober, Yariv Aizenbud, David Levin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895475", "id": "tag:google.com,2005:reader/item/000000032b1cf98e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Partition mixture of 1D wavelets for multi-dimensional data. (arXiv:1711.00789v1 [stat.ME])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00789"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00789", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Traditional statistical wavelet analysis that carries out modeling and \ninference based on wavelet coefficients under a given, predetermined wavelet \ntransform can quickly lose efficiency in multivariate problems, because such \nwavelet transforms, which are typically symmetric with respect to the \ndimensions, cannot adaptively exploit the energy distribution in a \nproblem-specific manner. We introduce a principled probabilistic framework for \nincorporating such adaptivity---by (i) representing multivariate functions \nusing one-dimensional (1D) wavelet transforms applied to a permuted version of \nthe original function, and (ii) placing a prior on the corresponding \npermutation, thereby forming a mixture of permuted 1D wavelet transforms. Such \na representation can achieve substantially better energy concentration in the \nwavelet coefficients. In particular, when combined with the Haar basis, we show \nthat exact Bayesian inference under the model can be achieved analytically \nthrough a recursive message passing algorithm with a computational complexity \nthat scales linearly with sample size. In addition, we propose a sequential \nMonte Carlo (SMC) inference algorithm for other wavelet bases using the exact \nHaar solution as the proposal. We demonstrate that with this framework even \nsimple 1D Haar wavelets can achieve excellent performance in both 2D and 3D \nimage reconstruction via numerical experiments, outperforming state-of-the-art \nmultidimensional wavelet-based methods especially in low signal-to-noise ratio \nsettings, at a fraction of the computational cost. \n</p>"}, "author": "Meng Li, Li Ma", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895474", "id": "tag:google.com,2005:reader/item/000000032b1cfa43", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum Approximation. (arXiv:1711.00799v2 [stat.ML] UPDATED)", "published": 1511308877, "updated": 1511308892, "canonical": [{"href": "http://arxiv.org/abs/1711.00799"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00799", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Modeling sequential data has become more and more important in practice. Some \napplications are autonomous driving, virtual sensors and weather forecasting. \nTo model such systems so called recurrent models are used. In this article we \nintroduce two new Deep Recurrent Gaussian Process (DRGP) models based on the \nSparse Spectrum Gaussian Process (SSGP) and the improved variational version \ncalled Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the \nrecurrent structure given by an existing DRGP based on a specific sparse \nNystr\\\"om approximation. Therefore, we also variationally integrate out the \ninput-space and hence can propagate uncertainty through the layers. We can show \nthat for the resulting lower bound an optimal variational distribution exists. \nTraining is realized through optimizing the variational lower bound. Using \nDistributed Variational Inference (DVI), we can reduce the computational \ncomplexity. We improve over current state of the art methods in prediction \naccuracy for experimental data-sets used for their evaluation and introduce a \nnew data-set for engine control, named Emission. Furthermore, our method can \neasily be adapted for unsupervised learning, e.g. the latent variable model and \nits deep version. \n</p>"}, "author": "Roman F&#xf6;ll, Bernard Haasdonk, Markus Hanselmann, Holger Ulmer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895473", "id": "tag:google.com,2005:reader/item/000000032b1cfad2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Medoids in almost linear time via multi-armed bandits. (arXiv:1711.00817v3 [stat.ML] UPDATED)", "published": 1510139306, "updated": 1510139311, "canonical": [{"href": "http://arxiv.org/abs/1711.00817"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00817", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Computing the medoid of a large number of points in high-dimensional space is \nan increasingly common operation in many data science problems. We present an \nalgorithm Med-dit which uses O(n log n) distance evaluations to compute the \nmedoid with high probability. Med-dit is based on a connection with the \nmulti-armed bandit problem. We evaluate the performance of Med-dit empirically \non the Netflix-prize and the single-cell RNA-Seq datasets, containing hundreds \nof thousands of points living in tens of thousands of dimensions, and observe a \n5-10x improvement in performance over the current state of the art. Med-dit is \navailable at https://github.com/bagavi/Meddit \n</p>"}, "author": "Vivek Bagaria, Govinda M. Kamath, Vasilis Ntranos, Martin J. Zhang, David Tse", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895472", "id": "tag:google.com,2005:reader/item/000000032b1cfb41", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Oversampling for Imbalanced Learning Based on K-Means and SMOTE. (arXiv:1711.00837v1 [cs.LG])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00837"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00837", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c87150a4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c87150a4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Learning from class-imbalanced data continues to be a common and challenging \nproblem in supervised learning as standard classification algorithms are \ndesigned to handle balanced class distributions. While different strategies \nexist to tackle this problem, methods which generate artificial data to achieve \na balanced class distribution are more versatile than modifications to the \nclassification algorithm. Such techniques, called oversamplers, modify the \ntraining data, allowing any classifier to be used with class-imbalanced \ndatasets. Many algorithms have been proposed for this task, but most are \ncomplex and tend to generate unnecessary noise. This work presents a simple and \neffective oversampling method based on k-means clustering and SMOTE \noversampling, which avoids the generation of noise and effectively overcomes \nimbalances between and within classes. Empirical results of extensive \nexperiments with 71 datasets show that training data oversampled with the \nproposed method improves classification results. Moreover, k-means SMOTE \nconsistently outperforms other popular oversampling methods. An implementation \nis made available in the python programming language. \n</p>"}, "author": "Felix Last, Georgios Douzas, Fernando Bacao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895471", "id": "tag:google.com,2005:reader/item/000000032b1cfbb2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalized Probabilistic Bisection for Stochastic Root-Finding. (arXiv:1711.00843v1 [stat.ML])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00843"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00843", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider numerical schemes for root finding of noisy responses through \ngeneralizing the Probabilistic Bisection Algorithm (PBA) to the more practical \ncontext where the sampling distribution is unknown and location-dependent. As \nin standard PBA, we rely on a knowledge state for the approximate posterior of \nthe root location. To implement the corresponding Bayesian updating, we also \ncarry out inference of oracle accuracy, namely learning the probability of \ncorrect response. To this end we utilize batched querying in combination with a \nvariety of frequentist and Bayesian estimators based on majority vote, as well \nas the underlying functional responses, if available. For guiding sampling \nselection we investigate both Information Directed sampling, as well as \nQuantile sampling. Our numerical experiments show that these strategies perform \nquite differently; in particular we demonstrate the efficiency of randomized \nquantile sampling which is reminiscent of Thompson sampling. Our work is \nmotivated by the root-finding sub-routine in pricing of Bermudan financial \nderivatives, illustrated in the last section of the paper. \n</p>"}, "author": "Sergio Rodriguez, Michael Ludkovski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895470", "id": "tag:google.com,2005:reader/item/000000032b1cfc64", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variational Inference of Disentangled Latent Concepts from Unlabeled Observations. (arXiv:1711.00848v1 [cs.LG])", "published": 1509669386, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00848"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00848", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Disentangled representations, where the higher level data generative factors \nare reflected in disjoint latent dimensions, offer several benefits such as \nease of deriving invariant representations, transferability to other tasks, \ninterpretability, etc. We consider the problem of unsupervised learning of \ndisentangled representations from large pool of unlabeled observations, and \npropose a variational inference based approach to infer disentangled latent \nfactors. We introduce a regularizer on the expectation of the approximate \nposterior over observed data that encourages the disentanglement. We evaluate \nthe proposed approach using several quantitative metrics and empirically \nobserve significant gains over existing methods in terms of both \ndisentanglement and data likelihood (reconstruction quality). \n</p>"}, "author": "Abhishek Kumar, Prasanna Sattigeri, Avinash Balakrishnan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895460", "id": "tag:google.com,2005:reader/item/000000032b1cffff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Topology Adaptive Graph Convolutional Networks. (arXiv:1710.10370v3 [cs.LG] UPDATED)", "published": 1511218849, "updated": 1511218854, "canonical": [{"href": "http://arxiv.org/abs/1710.10370"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10370", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolution acts as a local feature extractor in convolutional neural \nnetworks (CNNs). However, the convolution operation is not applicable when the \ninput data is supported on an irregular graph such as with social networks, \ncitation networks, or knowledge graphs. This paper proposes the topology \nadaptive graph convolutional network (TAGCN), a novel graph convolutional \nnetwork that generalizes CNN architectures to graph-structured data and \nprovides a systematic way to design a set of fixed-size learnable filters to \nperform convolutions on graphs. The topologies of these filters are adaptive to \nthe topology of the graph when they scan the graph to perform convolution, \nreplacing the square filter for the grid-structured data in traditional CNNs. \nThe outputs are the weighted sum of these filters' outputs, extraction of both \nvertex features and strength of correlation between vertices. It can be used \nwith both directed and undirected graphs. The proposed TAGCN not only inherits \nthe properties of convolutions in CNN for grid-structured data, but it is also \nconsistent with convolution in traditional signal processing. We apply TAGCN to \nsemi-supervised learning problems for graph vertex classification; experiments \non a number of data sets demonstrate that our method outperforms the existing \ngraph convolutional neural networks and achieves state-of-the-art performance \nfor each data set tested. \n</p>"}, "author": "Jian Du, Shanghang Zhang, Guanhang Wu, Jos&#xe9; M. F. Moura, Soummya Kar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509669386895", "timestampUsec": "1509669386895457", "id": "tag:google.com,2005:reader/item/000000032b1d003f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Orthogonal Machine Learning: Power and Limitations. (arXiv:1711.00342v1 [cs.LG] CROSS LISTED)", "published": 1509669387, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00342"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00342", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Double machine learning provides $\\sqrt{n}$-consistent estimates of \nparameters of interest even when high-dimensional or nonparametric nuisance \nparameters are estimated at an $n^{-1/4}$ rate. The key is to employ \n\\emph{Neyman-orthogonal} moment equations which are first-order insensitive to \nperturbations in the nuisance parameters. We show that the $n^{-1/4}$ \nrequirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order \nnotion of orthogonality that grants robustness to more complex or \nhigher-dimensional nuisance parameters. In the partially linear model setting \npopular in causal inference, we use Stein's lemma to show that we can construct \nsecond-order orthogonal moments if and only if the treatment residual is not \nnormally distributed. We conclude by demonstrating the robustness benefits of \nan explicit doubly-orthogonal estimation procedure for treatment effect. \n</p>"}, "author": "Lester Mackey, Vasilis Syrgkanis, Ilias Zadik", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230801", "id": "tag:google.com,2005:reader/item/000000032b1415bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gene Ontology (GO) Prediction using Machine Learning Methods. (arXiv:1711.00001v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00001"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00001", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We applied machine learning to predict whether a gene is involved in axon \nregeneration. We extracted 31 features from different databases and trained \nfive machine learning models. Our optimal model, a Random Forest Classifier \nwith 50 submodels, yielded a test score of 85.71%, which is 4.1% higher than \nthe baseline score. We concluded that our models have some predictive \ncapability. Similar methodology and features could be applied to predict other \nGene Ontology (GO) terms. \n</p>"}, "author": "Haoze Wu, Yangyu Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230800", "id": "tag:google.com,2005:reader/item/000000032b1415de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Calibration for Stratified Classification Models. (arXiv:1711.00064v1 [stat.ME])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00064"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00064", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In classification problems, sampling bias between training data and testing \ndata is critical to the ranking performance of classification scores. Such bias \ncan be both unintentionally introduced by data collection and intentionally \nintroduced by the algorithm, such as under-sampling or weighting techniques \napplied to imbalanced data. When such sampling bias exists, using the raw \nclassification score to rank observations in the testing data can lead to \nsuboptimal results. In this paper, I investigate the optimal calibration \nstrategy in general settings, and develop a practical solution for one specific \nsampling bias case, where the sampling bias is introduced by stratified \nsampling. The optimal solution is developed by analytically solving the problem \nof optimizing the ROC curve. For practical data, I propose a ranking algorithm \nfor general classification models with stratified data. Numerical experiments \ndemonstrate that the proposed algorithm effectively addresses the stratified \nsampling bias issue. Interestingly, the proposed method shows its potential \napplicability in two other machine learning areas: unsupervised learning and \nmodel ensembling, which can be future research topics. \n</p>"}, "author": "Chandler Zuo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230799", "id": "tag:google.com,2005:reader/item/000000032b141614", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Fraternal Dropout. (arXiv:1711.00066v2 [stat.ML] UPDATED)", "published": 1510882706, "updated": 1510882707, "canonical": [{"href": "http://arxiv.org/abs/1711.00066"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00066", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural networks (RNNs) are important class of architectures among \nneural networks useful for language modeling and sequential prediction. \nHowever, optimizing RNNs is known to be harder compared to feed-forward neural \nnetworks. A number of techniques have been proposed in literature to address \nthis problem. In this paper we propose a simple technique called fraternal \ndropout that takes advantage of dropout to achieve this goal. Specifically, we \npropose to train two identical copies of an RNN (that share parameters) with \ndifferent dropout masks while minimizing the difference between their \n(pre-softmax) predictions. In this way our regularization encourages the \nrepresentations of RNNs to be invariant to dropout mask, thus being robust. We \nshow that our regularization term is upper bounded by the expectation-linear \ndropout objective which has been shown to address the gap due to the difference \nbetween the train and inference phases of dropout. We evaluate our model and \nachieve state-of-the-art results in sequence modeling tasks on two benchmark \ndatasets - Penn Treebank and Wikitext-2. We also show that our approach leads \nto performance improvement by a significant margin in image captioning \n(Microsoft COCO) and semi-supervised (CIFAR-10) tasks. \n</p>"}, "author": "Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230798", "id": "tag:google.com,2005:reader/item/000000032b141660", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Ranking Median Regression: Learning to Order through Local Consensus. (arXiv:1711.00070v1 [math.ST])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00070"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00070", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This article is devoted to the problem of predicting the value taken by a \nrandom permutation $\\Sigma$, describing the preferences of an individual over a \nset of numbered items $\\{1,\\; \\ldots,\\; n\\}$ say, based on the observation of \nan input/explanatory r.v. $X$ e.g. characteristics of the individual), when \nerror is measured by the Kendall $\\tau$ distance. In the probabilistic \nformulation of the 'Learning to Order' problem we propose, which extends the \nframework for statistical Kemeny ranking aggregation developped in \n\\citet{CKS17}, this boils down to recovering conditional Kemeny medians of \n$\\Sigma$ given $X$ from i.i.d. training examples $(X_1, \\Sigma_1),\\; \\ldots,\\; \n(X_N, \\Sigma_N)$. For this reason, this statistical learning problem is \nreferred to as \\textit{ranking median regression} here. Our contribution is \ntwofold. We first propose a probabilistic theory of ranking median regression: \nthe set of optimal elements is characterized, the performance of empirical risk \nminimizers is investigated in this context and situations where fast learning \nrates can be achieved are also exhibited. Next we introduce the concept of \nlocal consensus/median, in order to derive efficient methods for ranking median \nregression. The major advantage of this local learning approach lies in its \nclose connection with the widely studied Kemeny aggregation problem. From an \nalgorithmic perspective, this permits to build predictive rules for ranking \nmedian regression by implementing efficient techniques for (approximate) Kemeny \nmedian computations at a local level in a tractable manner. In particular, \nversions of $k$-nearest neighbor and tree-based methods, tailored to ranking \nmedian regression, are investigated. Accuracy of piecewise constant ranking \nmedian regression rules is studied under a specific smoothness assumption for \n$\\Sigma$'s conditional distribution given $X$. \n</p>"}, "author": "Stephan Cl&#xe9;men&#xe7;on, Anna Korba, Eric Sibony", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230797", "id": "tag:google.com,2005:reader/item/000000032b141695", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Synth-Validation: Selecting the Best Causal Inference Method for a Given Dataset. (arXiv:1711.00083v1 [stat.ML])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00083"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00083", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many decisions in healthcare, business, and other policy domains are made \nwithout the support of rigorous evidence due to the cost and complexity of \nperforming randomized experiments. Using observational data to answer causal \nquestions is risky: subjects who receive different treatments also differ in \nother ways that affect outcomes. Many causal inference methods have been \ndeveloped to mitigate these biases. However, there is no way to know which \nmethod might produce the best estimate of a treatment effect in a given study. \nIn analogy to cross-validation, which estimates the prediction error of \npredictive models applied to a given dataset, we propose synth-validation, a \nprocedure that estimates the estimation error of causal inference methods \napplied to a given dataset. In synth-validation, we use the observed data to \nestimate generative distributions with known treatment effects. We apply each \ncausal inference method to datasets sampled from these distributions and \ncompare the effect estimates with the known effects to estimate error. Using \nsimulations, we show that using synth-validation to select a causal inference \nmethod for each study lowers the expected estimation error relative to \nconsistently using any single method. \n</p>"}, "author": "Alejandro Schuler, Ken Jung, Robert Tibshirani, Trevor Hastie, Nigam Shah", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230796", "id": "tag:google.com,2005:reader/item/000000032b1416de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering. (arXiv:1711.00108v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00108"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00108", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c87152d9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c87152d9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Existing deep multitask learning (MTL) approaches align layers shared between \ntasks in a parallel ordering. Such an organization significantly constricts the \ntypes of shared structure that can be learned. The necessity of parallel \nordering for deep MTL is first tested by comparing it with permuted ordering of \nshared layers. The results indicate that a flexible ordering can enable more \neffective sharing, thus motivating the development of a soft ordering approach, \nwhich learns how shared layers are applied in different ways for different \ntasks. Deep MTL with soft ordering outperforms parallel ordering methods across \na series of domains. These results suggest that the power of deep MTL comes \nfrom learning highly general building blocks that can be assembled to meet the \ndemands of each task. \n</p>"}, "author": "Elliot Meyerson, Risto Miikkulainen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230795", "id": "tag:google.com,2005:reader/item/000000032b14171a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Accelerated Sparse Subspace Clustering. (arXiv:1711.00126v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00126"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00126", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c87a67dd\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c87a67dd&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>State-of-the-art algorithms for sparse subspace clustering perform spectral \nclustering on a similarity matrix typically obtained by representing each data \npoint as a sparse combination of other points using either basis pursuit (BP) \nor orthogonal matching pursuit (OMP). BP-based methods are often prohibitive in \npractice while the performance of OMP-based schemes are unsatisfactory, \nespecially in settings where data points are highly similar. In this paper, we \npropose a novel algorithm that exploits an accelerated variant of orthogonal \nleast-squares to efficiently find the underlying subspaces. We show that under \ncertain conditions the proposed algorithm returns a subspace-preserving \nsolution. Simulation results illustrate that the proposed method compares \nfavorably with BP-based method in terms of running time while being \nsignificantly more accurate than OMP-based schemes. \n</p>"}, "author": "Abolfazl Hashemi, Haris Vikalo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230794", "id": "tag:google.com,2005:reader/item/000000032b141764", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Pomegranate: fast and flexible probabilistic modeling in python. (arXiv:1711.00137v1 [cs.AI])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00137"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00137", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present pomegranate, an open source machine learning package for \nprobabilistic modeling in Python. Probabilistic modeling encompasses a wide \nrange of methods that explicitly describe uncertainty using probability \ndistributions. Three widely used probabilistic models implemented in \npomegranate are general mixture models, hidden Markov models, and Bayesian \nnetworks. A primary focus of pomegranate is to abstract away the complexities \nof training models from their definition. This allows users to focus on \nspecifying the correct model for their application instead of being limited by \ntheir understanding of the underlying algorithms. An aspect of this focus \ninvolves the collection of additive sufficient statistics from data sets as a \nstrategy for training models. This approach trivially enables many useful \nlearning strategies, such as out-of-core learning, minibatch learning, and \nsemi-supervised learning, without requiring the user to consider how to \npartition data or modify the algorithms to handle these tasks themselves. \npomegranate is written in Cython to speed up calculations and releases the \nglobal interpreter lock to allow for built-in multithreaded parallelism, making \nit competitive with---or outperform---other implementations of similar \nalgorithms. This paper presents an overview of the design choices in \npomegranate, and how they have enabled complex features to be supported by \nsimple code. \n</p>"}, "author": "Jacob Schreiber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230793", "id": "tag:google.com,2005:reader/item/000000032b1417ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Training GANs with Optimism. (arXiv:1711.00141v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00141"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00141", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We address the issue of limit cycling behavior in training Generative \nAdversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for \ntraining Wasserstein GANs. Recent theoretical results have shown that \noptimistic mirror decent (OMD) can enjoy faster regret rates in the context of \nzero-sum games. WGANs is exactly a context of solving a zero-sum game with \nsimultaneous no-regret dynamics. Moreover, we show that optimistic mirror \ndecent addresses the limit cycling problem in training WGANs. We formally show \nthat in the case of bi-linear zero-sum games the last iterate of OMD dynamics \nconverges to an equilibrium, in contrast to GD dynamics which are bound to \ncycle. We also portray the huge qualitative difference between GD and OMD \ndynamics with toy examples, even when GD is modified with many adaptations \nproposed in the recent literature, such as gradient penalty or momentum. We \napply OMD WGAN training to a bioinformatics problem of generating DNA \nsequences. We observe that models trained with OMD achieve consistently smaller \nKL divergence with respect to the true underlying distribution, than models \ntrained with GD variants. Finally, we introduce a new algorithm, Optimistic \nAdam, which is an optimistic variant of Adam. We apply it to WGAN training on \nCIFAR10 and observe improved performance in terms of inception score as \ncompared to Adam. \n</p>"}, "author": "Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, Haoyang Zeng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230792", "id": "tag:google.com,2005:reader/item/000000032b1417f2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sampling and Reconstruction of Graph Signals via Weak Submodularity and Semidefinite Relaxation. (arXiv:1711.00142v1 [stat.ML])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00142"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00142", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of sampling a bandlimited graph signal in the presence \nof noise, where the objective is to select a node subset of prescribed \ncardinality that minimizes the signal reconstruction mean squared error (MSE). \nTo that end, we formulate the task at hand as the minimization of MSE subject \nto binary constraints, and approximate the resulting NP-hard problem via \nsemidefinite programming (SDP) relaxation. Moreover, we provide an alternative \nformulation based on maximizing a monotone weak submodular function and propose \na randomized-greedy algorithm to find a sub-optimal subset. We then derive a \nworst-case performance guarantee on the MSE returned by the randomized greedy \nalgorithm for general non-stationary graph signals. The efficacy of the \nproposed methods is illustrated through numerical simulations on synthetic and \nreal-world graphs. Notably, the randomized greedy algorithm yields an \norder-of-magnitude speedup over state-of-the-art greedy sampling schemes, while \nincurring only a marginal MSE performance loss. \n</p>"}, "author": "Abolfazl Hashemi, Rasoul Shafipour, Haris Vikalo, Gonzalo Mateos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230791", "id": "tag:google.com,2005:reader/item/000000032b141845", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Neural Networks as Gaussian Processes. (arXiv:1711.00165v1 [stat.ML])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00165"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00165", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A deep fully-connected neural network with an i.i.d. prior over its \nparameters is equivalent to a Gaussian process (GP) in the limit of infinite \nnetwork width. This correspondence enables exact Bayesian inference for neural \nnetworks on regression tasks by means of straightforward matrix computations. \nFor single hidden-layer networks, the covariance function of this GP has long \nbeen known. Recently, kernel functions for multi-layer random neural networks \nhave been developed, but only outside of a Bayesian framework. As such, \nprevious work has not identified the correspondence between using these kernels \nas the covariance function for a GP and performing fully Bayesian prediction \nwith a deep neural network. In this work, we derive this correspondence and \ndevelop a computationally efficient pipeline to compute the covariance \nfunctions. We then use the resulting GP to perform Bayesian inference for deep \nneural networks on MNIST and CIFAR-10. We find that the GP-based predictions \nare competitive and can outperform neural networks trained with stochastic \ngradient descent. We observe that the trained neural network accuracy \napproaches that of the corresponding GP-based computation with increasing layer \nwidth, and that the GP uncertainty is strongly correlated with prediction \nerror. We connect our observations to the recent development of signal \npropagation in random neural networks. \n</p>"}, "author": "Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230790", "id": "tag:google.com,2005:reader/item/000000032b141885", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Variational Inference for Fully Bayesian Sparse Gaussian Process Regression Models. (arXiv:1711.00221v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00221"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00221", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a novel variational inference framework for deriving a \nfamily of Bayesian sparse Gaussian process regression (SGPR) models whose \napproximations are variationally optimal with respect to the full-rank GPR \nmodel enriched with various corresponding correlation structures of the \nobservation noises. \n</p> \n<p>Our variational Bayesian SGPR (VBSGPR) models jointly treat both the \ndistributions of the inducing variables and hyperparameters as variational \nparameters, which enables the decomposability of the variational lower bound \nthat in turn can be exploited for stochastic optimization. \n</p> \n<p>Such a stochastic optimization involves iteratively following the stochastic \ngradient of the variational lower bound to improve its estimates of the optimal \nvariational distributions of the inducing variables and hyperparameters (and \nhence the predictive distribution) of our VBSGPR models and is guaranteed to \nachieve asymptotic convergence to them. \n</p> \n<p>We show that the stochastic gradient is an unbiased estimator of the exact \ngradient and can be computed in constant time per iteration, hence achieving \nscalability to big data. \n</p> \n<p>We empirically evaluate the performance of our proposed framework on two \nreal-world, massive datasets. \n</p>"}, "author": "Haibin Yu, Trong Nghia Hoang, Kian Hsiang Low, Patrick Jaillet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230789", "id": "tag:google.com,2005:reader/item/000000032b1418c7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Smooth Neighbors on Teacher Graphs for Semi-supervised Learning. (arXiv:1711.00258v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00258"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00258", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The paper proposes an inductive semi-supervised learning method, called \nSmooth Neighbors on Teacher Graphs (SNTG). At each iteration during training, a \ngraph is dynamically constructed based on predictions of the teacher model, \ni.e., the implicit self-ensemble of models. Then the graph serves as a \nsimilarity measure with respect to which the representations of \"similar\" \nneighboring points are learned to be smooth on the low dimensional manifold. We \nachieve state-of-the-art results on semi-supervised learning benchmarks. The \nerror rates are 9.89%, 3.99% for CIFAR-10 with 4000 labels, SVHN with 500 \nlabels, respectively. In particular, the improvements are significant when the \nlabels are scarce. For non-augmented MNIST with only 20 labels, the error rate \nis reduced from previous 4.81% to 1.36%. Our method is also effective under \nnoisy supervision and shows robustness to incorrect labels. \n</p>"}, "author": "Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, Bo Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230788", "id": "tag:google.com,2005:reader/item/000000032b141913", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Avoiding Your Teacher's Mistakes: Training Neural Networks with Controlled Weak Supervision. (arXiv:1711.00313v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00313"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00313", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Training deep neural networks requires massive amounts of training data, but \nfor many tasks only limited labeled data is available. This makes weak \nsupervision attractive, using weak or noisy signals like the output of \nheuristic methods or user click-through data for training. In a semi-supervised \nsetting, we can use a large set of data with weak labels to pretrain a neural \nnetwork and then fine-tune the parameters with a small amount of data with true \nlabels. This feels intuitively sub-optimal as these two independent stages \nleave the model unaware about the varying label quality. What if we could \nsomehow inform the model about the label quality? In this paper, we propose a \nsemi-supervised learning method where we train two neural networks in a \nmulti-task fashion: a \"target network\" and a \"confidence network\". The target \nnetwork is optimized to perform a given task and is trained using a large set \nof unlabeled data that are weakly annotated. We propose to weight the gradient \nupdates to the target network using the scores provided by the second \nconfidence network, which is trained on a small amount of supervised data. Thus \nwe avoid that the weight updates computed from noisy labels harm the quality of \nthe target network model. We evaluate our learning strategy on two different \ntasks: document ranking and sentiment classification. The results demonstrate \nthat our approach not only enhances the performance compared to the baselines \nbut also speeds up the learning process from weak labels. \n</p>"}, "author": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230787", "id": "tag:google.com,2005:reader/item/000000032b141981", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Large Dimensional Analysis of Regularized Discriminant Analysis Classifiers. (arXiv:1711.00382v1 [stat.ML])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00382"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00382", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This article carries out a large dimensional analysis of standard regularized \ndiscriminant analysis classifiers designed on the assumption that data arise \nfrom a Gaussian mixture model with different means and covariances. The \nanalysis relies on fundamental results from random matrix theory (RMT) when \nboth the number of features and the cardinality of the training data within \neach class grow large at the same pace. Under mild assumptions, we show that \nthe asymptotic classification error approaches a deterministic quantity that \ndepends only on the means and covariances associated with each class as well as \nthe problem dimensions. Such a result permits a better understanding of the \nperformance of regularized discriminant analsysis, in practical large but \nfinite dimensions, and can be used to determine and pre-estimate the optimal \nregularization parameter that minimizes the misclassification error \nprobability. Despite being theoretically valid only for Gaussian data, our \nfindings are shown to yield a high accuracy in predicting the performances \nachieved with real data sets drawn from the popular USPS data base, thereby \nmaking an interesting connection between theory and practice. \n</p>"}, "author": "Khalil Elkhalil, Abla Kammoun, Romain Couillet, Tareq Y. Al-Naffouri, Mohamed-Slim Alouini", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230786", "id": "tag:google.com,2005:reader/item/000000032b1419d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Active Tolerant Testing. (arXiv:1711.00388v1 [stat.ML])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00388"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00388", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c87a6ae4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c87a6ae4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this work, we give the first algorithms for tolerant testing of nontrivial \nclasses in the active model: estimating the distance of a target function to a \nhypothesis class C with respect to some arbitrary distribution D, using only a \nsmall number of label queries to a polynomial-sized pool of unlabeled examples \ndrawn from D. Specifically, we show that for the class D of unions of d \nintervals on the line, we can estimate the error rate of the best hypothesis in \nthe class to an additive error epsilon from only $O(\\frac{1}{\\epsilon^6}\\log \n\\frac{1}{\\epsilon})$ label queries to an unlabeled pool of size \n$O(\\frac{d}{\\epsilon^2}\\log \\frac{1}{\\epsilon})$. The key point here is the \nnumber of labels needed is independent of the VC-dimension of the class. This \nextends the work of Balcan et al. [2012] who solved the non-tolerant testing \nproblem for this class (distinguishing the zero-error case from the case that \nthe best hypothesis in the class has error greater than epsilon). \n</p> \n<p>We also consider the related problem of estimating the performance of a given \nlearning algorithm A in this setting. That is, given a large pool of unlabeled \nexamples drawn from distribution D, can we, from only a few label queries, \nestimate how well A would perform if the entire dataset were labeled? We focus \non k-Nearest Neighbor style algorithms, and also show how our results can be \napplied to the problem of hyperparameter tuning (selecting the best value of k \nfor the given learning problem). \n</p>"}, "author": "Avrim Blum, Lunjia Hu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230785", "id": "tag:google.com,2005:reader/item/000000032b141a3e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Minimal Exploration in Structured Stochastic Bandits. (arXiv:1711.00400v1 [stat.ML])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00400"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00400", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper introduces and addresses a wide class of stochastic bandit \nproblems where the function mapping the arm to the corresponding reward \nexhibits some known structural properties. Most existing structures (e.g. \nlinear, Lipschitz, unimodal, combinatorial, dueling, ...) are covered by our \nframework. We derive an asymptotic instance-specific regret lower bound for \nthese problems, and develop OSSB, an algorithm whose regret matches this \nfundamental limit. OSSB is not based on the classical principle of \"optimism in \nthe face of uncertainty\" or on Thompson sampling, and rather aims at matching \nthe minimal exploration rates of sub-optimal arms as characterized in the \nderivation of the regret lower bound. We illustrate the efficiency of OSSB \nusing numerical experiments in the case of the linear bandit problem and show \nthat OSSB outperforms existing algorithms, including Thompson sampling. \n</p>"}, "author": "Richard Combes, Stefan Magureanu, Alexandre Proutiere", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230784", "id": "tag:google.com,2005:reader/item/000000032b141a7b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hierarchical Representations for Efficient Architecture Search. (arXiv:1711.00436v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00436"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00436", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We explore efficient neural architecture search methods and present a simple \nyet powerful evolutionary algorithm that can discover new architectures \nachieving state of the art results. Our approach combines a novel hierarchical \ngenetic representation scheme that imitates the modularized design pattern \ncommonly adopted by human experts, and an expressive search space that supports \ncomplex topologies. Our algorithm efficiently discovers architectures that \noutperform a large number of manually designed models for image classification, \nobtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to \nImageNet, which is competitive with the best existing neural architecture \nsearch approaches and represents the new state of the art for evolutionary \nstrategies on this task. We also present results using random search, achieving \n0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing \nthe architecture search time from 36 hours down to 1 hour. \n</p>"}, "author": "Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230783", "id": "tag:google.com,2005:reader/item/000000032b141ad9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Attacking Binarized Neural Networks. (arXiv:1711.00449v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00449"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00449", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks with low-precision weights and activations offer compelling \nefficiency advantages over their full-precision equivalents. The two most \nfrequently discussed benefits of quantization are reduced memory consumption, \nand a faster forward pass when implemented with efficient bitwise operations. \nWe propose a third benefit of very low-precision neural networks: improved \nrobustness against some adversarial attacks, and in the worst case, performance \nthat is on par with full-precision models. We focus on the very low-precision \ncase where weights and activations are both quantized to $\\pm$1, and note that \nstochastically quantizing weights in just one layer can sharply reduce the \nimpact of iterative attacks. We observe that non-scaled binary neural networks \nexhibit a similar effect to the original defensive distillation procedure that \nled to gradient masking, and a false notion of security. We address this by \nconducting both black-box and white-box experiments with binary models that do \nnot artificially mask gradients. \n</p>"}, "author": "Angus Galloway, Graham W. Taylor, Medhat Moussa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509664066231", "timestampUsec": "1509664066230782", "id": "tag:google.com,2005:reader/item/000000032b141b1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Information-Theoretic Analysis of Deep Latent-Variable Models. (arXiv:1711.00464v1 [cs.LG])", "published": 1509664066, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00464"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00464", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present an information-theoretic framework for understanding trade-offs in \nunsupervised learning of deep latent-variables models using variational \ninference. This framework emphasizes the need to consider latent-variable \nmodels along two dimensions: the ability to reconstruct inputs (distortion) and \nthe communication cost (rate). We derive the optimal frontier of generative \nmodels in the two-dimensional rate-distortion plane, and show how the standard \nevidence lower bound objective is insufficient to select between points along \nthis frontier. However, by performing targeted optimization to learn generative \nmodels with different rates, we are able to learn many models that can achieve \nsimilar generative performance but make vastly different trade-offs in terms of \nthe usage of the latent variable. Through experiments on MNIST and Omniglot \nwith a variety of architectures, we show how our framework sheds light on many \nrecent proposed extensions to the variational autoencoder family. \n</p>"}, "author": "Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, Kevin Murphy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927579", "id": "tag:google.com,2005:reader/item/000000032aaaced3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Unsupervised Machine Translation Using Monolingual Corpora Only. (arXiv:1711.00043v1 [cs.CL])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00043"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00043", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine translation has recently achieved impressive performance thanks to \nrecent advances in deep learning and the availability of large-scale parallel \ncorpora. There have been numerous attempts to extend these successes to \nlow-resource language pairs, yet requiring tens of thousands of parallel \nsentences. In this work, we take this research direction to the extreme and \ninvestigate whether it is possible to learn to translate even without any \nparallel data. We propose a model that takes sentences from monolingual corpora \nin two different languages and maps them into the same latent space. By \nlearning to reconstruct in both languages from this shared feature space, the \nmodel effectively learns to translate without using any labeled data. We \ndemonstrate our model on two widely used datasets and two language pairs, \nreporting BLEU scores up to 32.8, without using even a single parallel sentence \nat training time. \n</p>"}, "author": "Guillaume Lample, Ludovic Denoyer, Marc&#x27;Aurelio Ranzato", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927578", "id": "tag:google.com,2005:reader/item/000000032aaacedc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Abnormal Spatial-Temporal Pattern Analysis for Niagara Frontier Border Wait Times. (arXiv:1711.00054v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00054"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00054", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Border crossing delays cause problems like huge economics loss and heavy \nenvironmental pollutions. To understand more about the nature of border \ncrossing delay, this study applies a dictionary-based compression algorithm to \nprocess the historical Niagara Frontier border wait times data. It can identify \nthe abnormal spatial-temporal patterns for both passenger vehicles and trucks \nat three bridges connecting US and Canada. Furthermore, it provides a \nquantitate anomaly score to rank the wait times patterns across the three \nbridges for each vehicle type and each direction. By analyzing the top three \nmost abnormal patterns, we find that there are at least two factors \ncontributing the anomaly of the patterns. The weekends and holidays may cause \nunusual heave congestions at the three bridges at the same time, and the \nfreight transportation demand may be uneven from Canada to the USA at Peace \nBridge and Lewiston-Queenston Bridge, which may lead to a high anomaly score. \nBy calculating the frequency of the top 5% abnormal patterns by hour of the \nday, the results show that for cars from the USA to Canada, the frequency of \nabnormal waiting time patterns is the highest during noon while for trucks in \nthe same direction, it is the highest during the afternoon peak hours. For \nCanada to US direction, the frequency of abnormal border wait time patterns for \nboth cars and trucks reaches to the peak during the afternoon. The analysis of \nabnormal spatial-temporal wait times patterns is promising to improve the \nborder crossing management \n</p>"}, "author": "Zhenhua Zhang, Lei Lin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927577", "id": "tag:google.com,2005:reader/item/000000032aaacef6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Fraternal Dropout. (arXiv:1711.00066v2 [stat.ML] UPDATED)", "published": 1510995733, "updated": 1510995735, "canonical": [{"href": "http://arxiv.org/abs/1711.00066"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00066", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural networks (RNNs) are important class of architectures among \nneural networks useful for language modeling and sequential prediction. \nHowever, optimizing RNNs is known to be harder compared to feed-forward neural \nnetworks. A number of techniques have been proposed in literature to address \nthis problem. In this paper we propose a simple technique called fraternal \ndropout that takes advantage of dropout to achieve this goal. Specifically, we \npropose to train two identical copies of an RNN (that share parameters) with \ndifferent dropout masks while minimizing the difference between their \n(pre-softmax) predictions. In this way our regularization encourages the \nrepresentations of RNNs to be invariant to dropout mask, thus being robust. We \nshow that our regularization term is upper bounded by the expectation-linear \ndropout objective which has been shown to address the gap due to the difference \nbetween the train and inference phases of dropout. We evaluate our model and \nachieve state-of-the-art results in sequence modeling tasks on two benchmark \ndatasets - Penn Treebank and Wikitext-2. We also show that our approach leads \nto performance improvement by a significant margin in image captioning \n(Microsoft COCO) and semi-supervised (CIFAR-10) tasks. \n</p>"}, "author": "Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927576", "id": "tag:google.com,2005:reader/item/000000032aaacf14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "DCN+: Mixed Objective and Deep Residual Coattention for Question Answering. (arXiv:1711.00106v1 [cs.CL])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00106"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00106", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Traditional models for question answering optimize using cross entropy loss, \nwhich encourages exact answers at the cost of penalizing nearby or overlapping \nanswers that are sometimes equally accurate. We propose a mixed objective that \ncombines cross entropy loss with self-critical policy learning. The objective \nuses rewards derived from word overlap to solve the misalignment between \nevaluation metric and optimization objective. In addition to the mixed \nobjective, we improve dynamic coattention networks (DCN) with a deep residual \ncoattention encoder that is inspired by recent work in deep self-attention and \nresidual networks. Our proposals improve model performance across question \ntypes and input lengths, especially for long questions that requires the \nability to capture long-term dependencies. On the Stanford Question Answering \nDataset, our model achieves state-of-the-art results with 75.1% exact match \naccuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy \nand 86.0% F1. \n</p>"}, "author": "Caiming Xiong, Victor Zhong, Richard Socher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927575", "id": "tag:google.com,2005:reader/item/000000032aaacf20", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering. (arXiv:1711.00108v1 [cs.LG])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00108"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00108", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Existing deep multitask learning (MTL) approaches align layers shared between \ntasks in a parallel ordering. Such an organization significantly constricts the \ntypes of shared structure that can be learned. The necessity of parallel \nordering for deep MTL is first tested by comparing it with permuted ordering of \nshared layers. The results indicate that a flexible ordering can enable more \neffective sharing, thus motivating the development of a soft ordering approach, \nwhich learns how shared layers are applied in different ways for different \ntasks. Deep MTL with soft ordering outperforms parallel ordering methods across \na series of domains. These results suggest that the power of deep MTL comes \nfrom learning highly general building blocks that can be assembled to meet the \ndemands of each task. \n</p>"}, "author": "Elliot Meyerson, Risto Miikkulainen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927574", "id": "tag:google.com,2005:reader/item/000000032aaacf36", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Automata Guided Hierarchical Reinforcement Learning for Zero-shot Skill Composition. (arXiv:1711.00129v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00129"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00129", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c87a6e6d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c87a6e6d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>An obstacle that prevents the wide adoption of (deep) reinforcement learning \n(RL) in control systems is its need for a large amount of interactions with the \nenviron- ment in order to master a skill. The learned skill usually generalizes \npoorly across domains and re-training is often necessary when presented with a \nnew task. We present a framework that combines methods in formal methods with \nhierarchi- cal reinforcement learning (HRL). The set of techniques we provide \nallows for convenient specification of tasks with complex logic, learn \nhierarchical policies (meta-controller and low-level controllers) with \nwell-defined intrinsic rewards us- ing any RL methods and is able to construct \nnew skills from existing ones without additional learning. We evaluate the \nproposed methods in a simple grid world simulation as well as simulation on a \nBaxter robot. \n</p>"}, "author": "Xiao Li, Yao Ma, Calin Belta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927573", "id": "tag:google.com,2005:reader/item/000000032aaacf5d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Pomegranate: fast and flexible probabilistic modeling in python. (arXiv:1711.00137v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00137"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00137", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c88698ac\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c88698ac&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present pomegranate, an open source machine learning package for \nprobabilistic modeling in Python. Probabilistic modeling encompasses a wide \nrange of methods that explicitly describe uncertainty using probability \ndistributions. Three widely used probabilistic models implemented in \npomegranate are general mixture models, hidden Markov models, and Bayesian \nnetworks. A primary focus of pomegranate is to abstract away the complexities \nof training models from their definition. This allows users to focus on \nspecifying the correct model for their application instead of being limited by \ntheir understanding of the underlying algorithms. An aspect of this focus \ninvolves the collection of additive sufficient statistics from data sets as a \nstrategy for training models. This approach trivially enables many useful \nlearning strategies, such as out-of-core learning, minibatch learning, and \nsemi-supervised learning, without requiring the user to consider how to \npartition data or modify the algorithms to handle these tasks themselves. \npomegranate is written in Cython to speed up calculations and releases the \nglobal interpreter lock to allow for built-in multithreaded parallelism, making \nit competitive with---or outperform---other implementations of similar \nalgorithms. This paper presents an overview of the design choices in \npomegranate, and how they have enabled complex features to be supported by \nsimple code. \n</p>"}, "author": "Jacob Schreiber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927572", "id": "tag:google.com,2005:reader/item/000000032aaacf8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Visualizing and Understanding Atari Agents. (arXiv:1711.00138v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00138"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00138", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep reinforcement learning (deep RL) agents have achieved remarkable success \nin a broad range of game-playing and continuous control tasks. While these \nagents are effective at maximizing rewards, it is often unclear what strategies \nthey use to do so. In this paper, we take a step toward explaining deep RL \nagents through a case study in three Atari 2600 environments. In particular, we \nfocus on understanding agents in terms of their visual attentional patterns \nduring decision making. To this end, we introduce a method for generating rich \nsaliency maps and use it to explain 1) what strong agents attend to 2) whether \nagents are making decisions for the right or wrong reasons, and 3) how agents \nevolve during the learning phase. We also test our method on non-expert human \nsubjects and find that it improves their ability to reason about these agents. \nOur techniques are general and, though we focus on Atari, our long-term \nobjective is to produce tools that explain any deep RL policy. \n</p>"}, "author": "Sam Greydanus, Anurag Koul, Jonathan Dodge, Alan Fern", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927571", "id": "tag:google.com,2005:reader/item/000000032aaacf9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Link prediction in drug-target interactions network using similarity indices. (arXiv:1711.00150v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00150"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00150", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Background: In silico drug-target interaction (DTI) prediction plays an \nintegral role in drug repositioning: the discovery of new uses for existing \ndrugs. One popular method of drug repositioning is network-based DTI \nprediction, which uses complex network theory to predict DTIs from a \ndrug-target network. Currently, most network-based DTI prediction is based on \nmachine learning methods such as Restricted Boltzmann Machines (RBM) or Support \nVector Machines (SVM). These methods require additional information about the \ncharacteristics of drugs, targets and DTIs, such as chemical structure, genome \nsequence, binding types, causes of interactions, etc., and do not perform \nsatisfactorily when such information is unavailable. We propose a new, \nalternative method for DTI prediction that makes use of only network topology \ninformation attempting to solve this problem. \n</p> \n<p>Results: We compare our method for DTI prediction against the well-known RBM \napproach. We show that when applied to the MATADOR database, our approach based \non node neighborhoods yield higher precision for high-ranking predictions than \nRBM when no information regarding DTI types is available. \n</p> \n<p>Conclusion: This demonstrates that approaches purely based on network \ntopology provide a more suitable approach to DTI prediction in the many \nreal-life situations where little or no prior knowledge is available about the \ncharacteristics of drugs, targets, or their interactions. \n</p>"}, "author": "Yiding Lu, Yufan Guo, Anna Korhonen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927570", "id": "tag:google.com,2005:reader/item/000000032aaacfb4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Acquiring Target Stacking Skills by Goal-Parameterized Deep Reinforcement Learning. (arXiv:1711.00267v1 [cs.RO])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00267"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00267", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Understanding physical phenomena is a key component of human intelligence and \nenables physical interaction with previously unseen environments. In this \npaper, we study how an artificial agent can autonomously acquire this intuition \nthrough interaction with the environment. We created a synthetic block stacking \nenvironment with physics simulation in which the agent can learn a policy \nend-to-end through trial and error. Thereby, we bypass to explicitly model \nphysical knowledge within the policy. We are specifically interested in tasks \nthat require the agent to reach a given goal state that may be different for \nevery new trial. To this end, we propose a deep reinforcement learning \nframework that learns policies which are parametrized by a goal. We validated \nthe model on a toy example navigating in a grid world with different target \npositions and in a block stacking task with different target structures of the \nfinal tower. In contrast to prior work, our policies show better generalization \nacross different goals. \n</p>"}, "author": "Wenbin Li, Jeannette Bohg, Mario Fritz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927569", "id": "tag:google.com,2005:reader/item/000000032aaacfc1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks. (arXiv:1711.00350v1 [cs.CL])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00350"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00350", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Humans can understand and produce new utterances effortlessly, thanks to \ntheir systematic compositional skills. Once a person learns the meaning of a \nnew verb \"dax,\" he or she can immediately understand the meaning of \"dax twice\" \nor \"sing and dax.\" In this paper, we introduce the SCAN domain, consisting of a \nset of simple compositional navigation commands paired with the corresponding \naction sequences. We then test the zero-shot generalization capabilities of a \nvariety of recurrent neural networks (RNNs) trained on SCAN with \nsequence-to-sequence methods. We find that RNNs can generalize well when the \ndifferences between training and test commands are small, so that they can \napply \"mix-and-match\" strategies to solve the task. However, when \ngeneralization requires systematic compositional skills (as in the \"dax\" \nexample above), RNNs fail spectacularly. We conclude with a proof-of-concept \nexperiment in neural machine translation, supporting the conjecture that lack \nof systematicity is an important factor explaining why neural networks need \nvery large training sets. \n</p>"}, "author": "Brenden M. Lake, Marco Baroni", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927568", "id": "tag:google.com,2005:reader/item/000000032aaacfd7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Servant of Many Masters: Shifting priorities in Pareto-optimal sequential decision-making. (arXiv:1711.00363v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00363"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00363", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is often argued that an agent making decisions on behalf of two or more \nprincipals who have different utility functions should adopt a {\\em \nPareto-optimal} policy, i.e., a policy that cannot be improved upon for one \nagent without making sacrifices for another. A famous theorem of Harsanyi shows \nthat, when the principals have a common prior on the outcome distributions of \nall policies, a Pareto-optimal policy for the agent is one that maximizes a \nfixed, weighted linear combination of the principals' utilities. \n</p> \n<p>In this paper, we show that Harsanyi's theorem does not hold for principals \nwith different priors, and derive a more precise generalization which does \nhold, which constitutes our main result. In this more general case, the \nrelative weight given to each principal's utility should evolve over time \naccording to how well the agent's observations conform with that principal's \nprior. The result has implications for the design of contracts, treaties, joint \nventures, and robots. \n</p>"}, "author": "Andrew Critch, Stuart Russell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927567", "id": "tag:google.com,2005:reader/item/000000032aaacfee", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR. (arXiv:1711.00399v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00399"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00399", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>There has been much discussion of the right to explanation in the EU General \nData Protection Regulation, and its existence, merits, and disadvantages. \nImplementing a right to explanation that opens the black box of algorithmic \ndecision-making faces major legal and technical barriers. Explaining the \nfunctionality of complex algorithmic decision-making systems and their \nrationale in specific cases is a technically challenging problem. Some \nexplanations may offer little meaningful information to data subjects, raising \nquestions around their value. Explanations of automated decisions need not \nhinge on the general public understanding how algorithmic systems function. \nEven though such interpretability is of great importance and should be pursued, \nexplanations can, in principle, be offered without opening the black box. \nLooking at explanations as a means to help a data subject act rather than \nmerely understand, one could gauge the scope and content of explanations \naccording to the specific goal or action they are intended to support. From the \nperspective of individuals affected by automated decision-making, we propose \nthree aims for explanations: (1) to inform and help the individual understand \nwhy a particular decision was reached, (2) to provide grounds to contest the \ndecision if the outcome is undesired, and (3) to understand what would need to \nchange in order to receive a desired result in the future, based on the current \ndecision-making model. We assess how each of these goals finds support in the \nGDPR. We suggest data controllers should offer a particular type of \nexplanation, unconditional counterfactual explanations, to support these three \naims. These counterfactual explanations describe the smallest change to the \nworld that can be made to obtain a desirable outcome, or to arrive at the \nclosest possible world, without needing to explain the internal logic of the \nsystem. \n</p>"}, "author": "Sandra Wachter, Brent Mittelstadt, Chris Russell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927566", "id": "tag:google.com,2005:reader/item/000000032aaacffe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Minimal Exploration in Structured Stochastic Bandits. (arXiv:1711.00400v1 [stat.ML])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00400"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00400", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper introduces and addresses a wide class of stochastic bandit \nproblems where the function mapping the arm to the corresponding reward \nexhibits some known structural properties. Most existing structures (e.g. \nlinear, Lipschitz, unimodal, combinatorial, dueling, ...) are covered by our \nframework. We derive an asymptotic instance-specific regret lower bound for \nthese problems, and develop OSSB, an algorithm whose regret matches this \nfundamental limit. OSSB is not based on the classical principle of \"optimism in \nthe face of uncertainty\" or on Thompson sampling, and rather aims at matching \nthe minimal exploration rates of sub-optimal arms as characterized in the \nderivation of the regret lower bound. We illustrate the efficiency of OSSB \nusing numerical experiments in the case of the linear bandit problem and show \nthat OSSB outperforms existing algorithms, including Thompson sampling. \n</p>"}, "author": "Richard Combes, Stefan Magureanu, Alexandre Proutiere", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927565", "id": "tag:google.com,2005:reader/item/000000032aaad007", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Building Data-driven Models with Microstructural Images: Generalization and Interpretability. (arXiv:1711.00404v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00404"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00404", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>As data-driven methods rise in popularity in materials science applications, \na key question is how these machine learning models can be used to understand \nmicrostructure. Given the importance of process-structure-property relations \nthroughout materials science, it seems logical that models that can leverage \nmicrostructural data would be more capable of predicting property information. \nWhile there have been some recent attempts to use convolutional neural networks \nto understand microstructural images, these early studies have focused only on \nwhich featurizations yield the highest machine learning model accuracy for a \nsingle data set. This paper explores the use of convolutional neural networks \nfor classifying microstructure with a more holistic set of objectives in mind: \ngeneralization between data sets, number of features required, and \ninterpretability. \n</p>"}, "author": "Julia Ling, Maxwell Hutchinson, Erin Antono, Brian DeCost, Elizabeth A. Holm, Bryce Meredig", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927564", "id": "tag:google.com,2005:reader/item/000000032aaad00b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Piecewise Linear Neural Network verification: A comparative study. (arXiv:1711.00455v1 [cs.AI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00455"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00455", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8869c8e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8869c8e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The success of Deep Learning and its potential use in many important safety- \ncritical applications has motivated research on formal verification of Neural \nNetwork (NN) models. Despite the reputation of learned NN models to behave as \nblack boxes and the theoretical hardness of proving their properties, \nresearchers have been successful in verifying some classes of models by \nexploiting their piecewise linear structure. Unfortunately, most of these \napproaches test their algorithms without comparison with other approaches. As a \nresult, the pros and cons of the different algorithms are not well understood. \nMotivated by the need to accelerate progress in this very important area, we \ninvestigate the trade-offs of a number of different approaches based on Mixed \nInteger Programming, Satisfiability Modulo Theory, as well as a novel method \nbased on the Branch-and-Bound framework. We also propose a new data set of \nbenchmarks, in addition to a collection of pre- viously released testcases that \ncan be used to compare existing methods. Our analysis not only allows a \ncomparison to be made between different strategies, the comparison of results \nfrom different solvers also revealed implementation bugs in published methods. \nWe expect that the availability of our benchmark and the analysis of the \ndifferent approaches will allow researchers to develop and evaluate promising \napproaches for making progress on this important topic. \n</p>"}, "author": "Rudy Bunel, Ilker Turkaslan, Philip H.S. Torr, Pushmeet Kohli, M. Pawan Kumar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509617247928", "timestampUsec": "1509617247927563", "id": "tag:google.com,2005:reader/item/000000032aaad01d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Early prediction of the duration of protests using probabilistic Latent Dirichlet Allocation and Decision Trees. (arXiv:1711.00462v1 [cs.SI])", "published": 1509617248, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00462"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00462", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Protests and agitations are an integral part of every democratic civil \nsociety. In recent years, South Africa has seen a large increase in its \nprotests. The objective of this paper is to provide an early prediction of the \nduration of protests from its free flowing English text description. Free \nflowing descriptions of the protests help us in capturing its various nuances \nsuch as multiple causes, courses of actions etc. Next we use a combination of \nunsupervised learning (topic modeling) and supervised learning (decision trees) \nto predict the duration of the protests. Our results show a high degree (close \nto 90%) of accuracy in early prediction of the duration of protests.We expect \nthe work to help police and other security services in planning and managing \ntheir resources in better handling protests in future. \n</p>"}, "author": "Satyakama Paul, Madhur Hasija, Tshilidzi Marwala", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509585685835", "timestampUsec": "1509585685834528", "id": "tag:google.com,2005:reader/item/000000032a72c6b8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Minimum Energy Quantized Neural Networks. (arXiv:1711.00215v1 [cs.NE])", "published": 1509585686, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00215"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00215", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This work targets the automated minimum-energy optimization of Quantized \nNeural Networks (QNNs) - networks using low precision weights and activations. \nThese networks are trained from scratch at an arbitrary fixed point precision. \nAt iso-accuracy, QNNs using fewer bits require deeper and wider network \narchitectures than networks using higher precision operators, while they \nrequire less complex arithmetic and less bits per weights. This fundamental \ntrade-off is analyzed and quantified to find the minimum energy QNN for any \nbenchmark and hence optimize energy-efficiency. To this end, the energy \nconsumption of inference is modeled for a generic hardware platform. This \nallows drawing several conclusions across different benchmarks. First, energy \nconsumption varies orders of magnitude at iso-accuracy depending on the number \nof bits used in the QNN. Second, in a typical system, BinaryNets or int4 \nimplementations lead to the minimum energy solution, outperforming int8 \nnetworks up to 2-10x at iso-accuracy. All code used for QNN training is \navailable from https://github.com/BertMoons. \n</p>"}, "author": "Bert Moons, Koen Goetschalckx, Nick Van Berckelaer, Marian Verhelst", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509585685835", "timestampUsec": "1509585685834527", "id": "tag:google.com,2005:reader/item/000000032a72c6c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Smooth Neighbors on Teacher Graphs for Semi-supervised Learning. (arXiv:1711.00258v1 [cs.LG])", "published": 1509585686, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00258"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00258", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The paper proposes an inductive semi-supervised learning method, called \nSmooth Neighbors on Teacher Graphs (SNTG). At each iteration during training, a \ngraph is dynamically constructed based on predictions of the teacher model, \ni.e., the implicit self-ensemble of models. Then the graph serves as a \nsimilarity measure with respect to which the representations of \"similar\" \nneighboring points are learned to be smooth on the low dimensional manifold. We \nachieve state-of-the-art results on semi-supervised learning benchmarks. The \nerror rates are 9.89%, 3.99% for CIFAR-10 with 4000 labels, SVHN with 500 \nlabels, respectively. In particular, the improvements are significant when the \nlabels are scarce. For non-augmented MNIST with only 20 labels, the error rate \nis reduced from previous 4.81% to 1.36%. Our method is also effective under \nnoisy supervision and shows robustness to incorrect labels. \n</p>"}, "author": "Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, Bo Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509585685835", "timestampUsec": "1509585685834526", "id": "tag:google.com,2005:reader/item/000000032a72c6cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Avoiding Your Teacher's Mistakes: Training Neural Networks with Controlled Weak Supervision. (arXiv:1711.00313v1 [cs.LG])", "published": 1509585686, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00313"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00313", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Training deep neural networks requires massive amounts of training data, but \nfor many tasks only limited labeled data is available. This makes weak \nsupervision attractive, using weak or noisy signals like the output of \nheuristic methods or user click-through data for training. In a semi-supervised \nsetting, we can use a large set of data with weak labels to pretrain a neural \nnetwork and then fine-tune the parameters with a small amount of data with true \nlabels. This feels intuitively sub-optimal as these two independent stages \nleave the model unaware about the varying label quality. What if we could \nsomehow inform the model about the label quality? In this paper, we propose a \nsemi-supervised learning method where we train two neural networks in a \nmulti-task fashion: a \"target network\" and a \"confidence network\". The target \nnetwork is optimized to perform a given task and is trained using a large set \nof unlabeled data that are weakly annotated. We propose to weight the gradient \nupdates to the target network using the scores provided by the second \nconfidence network, which is trained on a small amount of supervised data. Thus \nwe avoid that the weight updates computed from noisy labels harm the quality of \nthe target network model. We evaluate our learning strategy on two different \ntasks: document ranking and sentiment classification. The results demonstrate \nthat our approach not only enhances the performance compared to the baselines \nbut also speeds up the learning process from weak labels. \n</p>"}, "author": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509585685835", "timestampUsec": "1509585685834525", "id": "tag:google.com,2005:reader/item/000000032a72c6d8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hierarchical Representations for Efficient Architecture Search. (arXiv:1711.00436v1 [cs.LG])", "published": 1509585686, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00436"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00436", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We explore efficient neural architecture search methods and present a simple \nyet powerful evolutionary algorithm that can discover new architectures \nachieving state of the art results. Our approach combines a novel hierarchical \ngenetic representation scheme that imitates the modularized design pattern \ncommonly adopted by human experts, and an expressive search space that supports \ncomplex topologies. Our algorithm efficiently discovers architectures that \noutperform a large number of manually designed models for image classification, \nobtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to \nImageNet, which is competitive with the best existing neural architecture \nsearch approaches and represents the new state of the art for evolutionary \nstrategies on this task. We also present results using random search, achieving \n0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing \nthe architecture search time from 36 hours down to 1 hour. \n</p>"}, "author": "Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509585685835", "timestampUsec": "1509585685834522", "id": "tag:google.com,2005:reader/item/000000032a72c6e3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning. (arXiv:1710.11417v1 [cs.AI] CROSS LISTED)", "published": 1509585686, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11417"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11417", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Combining deep model-free reinforcement learning with on-line planning is a \npromising approach to building on the successes of deep RL. On-line planning \nwith look-ahead trees has proven successful in environments where transition \nmodels are known a priori. However, in complex environments where transition \nmodels need to be learned from data, the deficiencies of learned models have \nlimited their utility for planning. To address these challenges, we propose \nTreeQN, a differentiable, recursive, tree-structured model that serves as a \ndrop-in replacement for any value function network in deep RL with discrete \nactions. TreeQN dynamically constructs a tree by recursively applying a \ntransition model in a learned abstract state space and then aggregating \npredicted rewards and state-values using a tree backup to estimate Q-values. We \nalso propose ATreeC, an actor-critic variant that augments TreeQN with a \nsoftmax layer to form a stochastic policy network. Both approaches are trained \nend-to-end, such that the learned model is optimised for its actual use in the \nplanner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a \nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et \nal., 2017) on multiple Atari games, with deeper trees often outperforming \nshallower ones. We also present a qualitative analysis that sheds light on the \ntrees learned by TreeQN. \n</p>"}, "author": "Gregory Farquhar, Tim Rockt&#xe4;schel, Maximilian Igl, Shimon Whiteson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509497758963", "timestampUsec": "1509497758962576", "id": "tag:google.com,2005:reader/item/0000000329c3ad91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Super-polynomial separations for quantum-enhanced reinforcement learning. (arXiv:1710.11160v1 [quant-ph])", "published": 1509497759, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11160"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11160", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent work on quantum machine learning has demonstrated that quantum \ncomputers can offer dramatic improvements over classical devices for data \nmining, prediction and classification. However, less is known about the \nadvantages using quantum computers may bring in the more general setting of \nreinforcement learning, where learning is achieved via interaction with a task \nenvironment that provides occasional rewards. Reinforcement learning can \nincorporate data-analysis-oriented learning settings as special cases, but also \nincludes more complex situations where, e.g., reinforcing feedback is delayed. \nIn a few recent works, Grover-type amplification has been utilized to construct \nquantum agents that achieve up-to-quadratic improvements in learning \nefficiency. These encouraging results have left open the key question of \nwhether super-polynomial improvements in learning times are possible for \ngenuine reinforcement learning problems, that is problems that go beyond the \nother more restricted learning paradigms. In this work, we provide a family of \nsuch genuine reinforcement learning tasks, and we construct quantum-enhanced \nlearners which learn super-polynomially faster than any classical reinforcement \nlearning model. \n</p>"}, "author": "Vedran Dunjko, Yi-Kai Liu, Xingyao Wu, Jacob M. Taylor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509497758963", "timestampUsec": "1509497758962575", "id": "tag:google.com,2005:reader/item/0000000329c3ad9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "ChainerMN: Scalable Distributed Deep Learning Framework. (arXiv:1710.11351v1 [cs.DC])", "published": 1509497759, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11351"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11351", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>One of the keys for deep learning to have made a breakthrough in various \nfields was to utilize high computing powers centering around GPUs. Enabling the \nuse of further computing abilities by distributed processing is essential not \nonly to make the deep learning bigger and faster but also to tackle unsolved \nchallenges. We present the design, implementation, and evaluation of ChainerMN, \nthe distributed deep learning framework we have developed. We demonstrate that \nChainerMN can scale the learning process of the ResNet-50 model to the ImageNet \ndataset up to 128 GPUs with the parallel efficiency of 90%. \n</p>"}, "author": "Takuya Akiba, Keisuke Fukuda, Shuji Suzuki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509497758963", "timestampUsec": "1509497758962574", "id": "tag:google.com,2005:reader/item/0000000329c3ada4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Parametrizing filters of a CNN with a GAN. (arXiv:1710.11386v1 [cs.LG])", "published": 1509497759, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11386"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11386", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is commonly agreed that the use of relevant invariances as a good \nstatistical bias is important in machine-learning. However, most approaches \nthat explicitly incorporate invariances into a model architecture only make use \nof very simple transformations, such as translations and rotations. Hence, \nthere is a need for methods to model and extract richer transformations that \ncapture much higher-level invariances. To that end, we introduce a tool \nallowing to parametrize the set of filters of a trained convolutional neural \nnetwork with the latent space of a generative adversarial network. We then show \nthat the method can capture highly non-linear invariances of the data by \nvisualizing their effect in the data space. \n</p>"}, "author": "Yannic Kilcher, Gary Becigneul, Thomas Hofmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509497758963", "timestampUsec": "1509497758962573", "id": "tag:google.com,2005:reader/item/0000000329c3adac", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning as a Mixed Convex-Combinatorial Optimization Problem. (arXiv:1710.11573v1 [cs.LG])", "published": 1509497759, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11573"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11573", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c886a00e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c886a00e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>As neural networks grow deeper and wider, learning networks with \nhard-threshold activations is becoming increasingly important, both for network \nquantization, which can drastically reduce time and energy requirements, and \nfor creating large integrated systems of deep networks, which may have \nnon-differentiable components and must avoid vanishing and exploding gradients \nfor effective learning. However, since gradient descent is not applicable to \nhard-threshold functions, it is not clear how to learn them in a principled \nway. We address this problem by observing that setting targets for \nhard-threshold hidden units in order to minimize loss is a discrete \noptimization problem, and can be solved as such. The discrete optimization goal \nis to find a set of targets such that each unit, including the output, has a \nlinearly separable problem to solve. Given these targets, the network \ndecomposes into individual perceptrons, which can then be learned with standard \nconvex approaches. Based on this, we develop a recursive mini-batch algorithm \nfor learning deep hard-threshold networks that includes the popular but poorly \njustified straight-through estimator as a special case. Empirically, we show \nthat our algorithm improves classification accuracy in a number of settings, \nincluding for AlexNet and ResNet-18 on ImageNet, when compared to the \nstraight-through estimator. \n</p>"}, "author": "Abram L. Friesen, Pedro Domingos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509497758963", "timestampUsec": "1509497758962572", "id": "tag:google.com,2005:reader/item/0000000329c3adb1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm. (arXiv:1710.11622v1 [cs.LG])", "published": 1509497759, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11622"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11622", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c893bcd1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c893bcd1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Learning to learn is a powerful paradigm for enabling models to learn from \ndata more effectively and efficiently. A popular approach to meta-learning is \nto train a recurrent model to read in a training dataset as input and output \nthe parameters of a learned model, or output predictions for new test inputs. \nAlternatively, a more recent approach to meta-learning aims to acquire deep \nrepresentations that can be effectively fine-tuned, via standard gradient \ndescent, to new tasks. In this paper, we consider the meta-learning problem \nfrom the perspective of universality, formalizing the notion of learning \nalgorithm approximation and comparing the expressive power of the \naforementioned recurrent models to the more recent approaches that embed \ngradient descent into the meta-learner. In particular, we seek to answer the \nfollowing question: does deep representation combined with standard gradient \ndescent have sufficient capacity to approximate any learning algorithm? We find \nthat this is indeed true, and further find, in our experiments, that \ngradient-based meta-learning consistently leads to learning strategies that \ngeneralize more widely compared to those represented by recurrent models. \n</p>"}, "author": "Chelsea Finn, Sergey Levine", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185103", "id": "tag:google.com,2005:reader/item/0000000329c22bb1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Onsets and Frames: Dual-Objective Piano Transcription. (arXiv:1710.11153v1 [cs.SD])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11153"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11153", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of transcribing polyphonic piano music with an \nemphasis on generalizing to unseen instruments. We use deep neural networks and \npropose a novel approach that predicts onsets and frames using both CNNs and \nLSTMs. This model predicts pitch onset events and then uses those predictions \nto condition framewise pitch predictions. During inference, we restrict the \npredictions from the framewise detector by not allowing a new note to start \nunless the onset detector also agrees that an onset for that pitch is present \nin the frame. We focus on improving onsets and offsets together instead of \neither in isolation as we believe it correlates better with human musical \nperception. This technique results in over a 100% relative improvement in note \nwith offset score on the MAPS dataset. \n</p>"}, "author": "Curtis Hawthorne, Erich Elsen, Jialin Song, Adam Roberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev Oore, Douglas Eck", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185102", "id": "tag:google.com,2005:reader/item/0000000329c22bb8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "CrescendoNet: A Simple Deep Convolutional Neural Network with Ensemble Behavior. (arXiv:1710.11176v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11176"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11176", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a new deep convolutional neural network, CrescendoNet, by \nstacking simple building blocks without residual connections. Each Crescendo \nblock contains independent convolution paths with increased depths. The numbers \nof convolution layers and parameters are only increased linearly in Crescendo \nblocks. In experiments, CrescendoNet with only 15 layers outperforms almost all \nnetworks without residual connections on benchmark datasets, CIFAR10, CIFAR100, \nand SVHN. Given sufficient amount of data as in SVHN dataset, CrescendoNet with \n15 layers and 4.1M parameters can match the performance of DenseNet-BC with 250 \nlayers and 15.3M parameters. CrescendoNet provides a new way to construct high \nperformance deep convolutional neural networks without residual connections. \nMoreover, through investigating the behavior and performance of subnetworks in \nCrescendoNet, we note that the high performance of CrescendoNet may come from \nits implicit ensemble behavior, which differs from the FractalNet that is also \na deep convolutional neural network without residual connections. Furthermore, \nthe independence between paths in CrescendoNet allows us to introduce a new \npath-wise training procedure, which can reduce the memory needed for training. \n</p>"}, "author": "Xiang Zhang, Nishant Vishwamitra, Hongxin Hu, Feng Luo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185101", "id": "tag:google.com,2005:reader/item/0000000329c22bbb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sample-efficient Policy Optimization with Stein Control Variate. (arXiv:1710.11198v3 [stat.ML] UPDATED)", "published": 1510588781, "updated": 1510588789, "canonical": [{"href": "http://arxiv.org/abs/1710.11198"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11198", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Policy gradient methods have achieved remarkable successes in solving \nchallenging reinforcement learning problems. However, it still often suffers \nfrom the large variance issue on policy gradient estimation, which leads to \npoor sample efficiency during training. In this work, we propose a control \nvariate method to effectively reduce variance for policy gradient methods. \nMotivated by the Stein's identity, our method extends the previous control \nvariate methods used in REINFORCE and advantage actor-critic by introducing \nmore general action-dependent baseline functions. Empirical studies show that \nour method significantly improves the sample efficiency of the state-of-the-art \npolicy gradient approaches. \n</p>"}, "author": "Hao Liu, Yihao Feng, Yi Mao, Dengyong Zhou, Jian Peng, Qiang Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185100", "id": "tag:google.com,2005:reader/item/0000000329c22bbd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Critical Points of Neural Networks: Analytical Forms and Landscape Properties. (arXiv:1710.11205v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11205"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11205", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Due to the success of deep learning to solving a variety of challenging \nmachine learning tasks, there is a rising interest in understanding loss \nfunctions for training neural networks from a theoretical aspect. Particularly, \nthe properties of critical points and the landscape around them are of \nimportance to determine the convergence performance of optimization algorithms. \nIn this paper, we provide full (necessary and sufficient) characterization of \nthe analytical forms for the critical points (as well as global minimizers) of \nthe square loss functions for various neural networks. We show that the \nanalytical forms of the critical points characterize the values of the \ncorresponding loss functions as well as the necessary and sufficient conditions \nto achieve global minimum. Furthermore, we exploit the analytical forms of the \ncritical points to characterize the landscape properties for the loss functions \nof these neural networks. One particular conclusion is that: The loss function \nof linear networks has no spurious local minimum, while the loss function of \none-hidden-layer nonlinear networks with ReLU activation function does have \nlocal minimum that is not global minimum. \n</p>"}, "author": "Yi Zhou, Yingbin Liang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185099", "id": "tag:google.com,2005:reader/item/0000000329c22bc5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "How Algorithmic Confounding in Recommendation Systems Increases Homogeneity and Decreases Utility. (arXiv:1710.11214v1 [cs.CY])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11214"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11214", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recommendation systems occupy an expanding role in everyday decision making, \nfrom choice of movies and household goods to consequential medical and legal \ndecisions. The data used to train and test these systems is algorithmically \nconfounded in that it is the result of a feedback loop between human choices \nand an existing algorithmic recommendation system. Using simulations, we \ndemonstrate that algorithmic confounding can disadvantage algorithms in \ntraining, bias held-out evaluation, and amplify homogenization of user behavior \nwithout gains in utility. \n</p>"}, "author": "Allison J.B. Chaney, Brandon M. Stewart, Barbara E. Engelhardt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185098", "id": "tag:google.com,2005:reader/item/0000000329c22bca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast and Scalable Learning of Sparse Changes in High-Dimensional Gaussian Graphical Model Structure. (arXiv:1710.11223v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11223"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11223", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We focus on the problem of estimating the change in the dependency structures \nof two $p$-dimensional Gaussian Graphical models (GGMs). Previous studies for \nsparse change estimation in GGMs involve expensive and difficult non-smooth \noptimization. We propose a novel method, DIFFEE for estimating DIFFerential \nnetworks via an Elementary Estimator under a high-dimensional situation. DIFFEE \nis solved through a faster and closed form solution that enables it to work in \nlarge-scale settings. We conduct a rigorous statistical analysis showing that \nsurprisingly DIFFEE achieves the same asymptotic convergence rates as the \nstate-of-the-art estimators that are much more difficult to compute. Our \nexperimental results on multiple synthetic datasets and one real-world data \nabout brain connectivity show strong performance improvements over baselines, \nas well as significant computational benefits. \n</p>"}, "author": "Beilun Wang, Arshdeep Sekhon, Yanjun Qi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185097", "id": "tag:google.com,2005:reader/item/0000000329c22bce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Prototype Matching Networks for Large-Scale Multi-label Genomic Sequence Classification. (arXiv:1710.11238v2 [cs.LG] UPDATED)", "published": 1510588781, "updated": 1510588789, "canonical": [{"href": "http://arxiv.org/abs/1710.11238"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11238", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>One of the fundamental tasks in understanding genomics is the problem of \npredicting Transcription Factor Binding Sites (TFBSs). With more than hundreds \nof Transcription Factors (TFs) as labels, genomic-sequence based TFBS \nprediction is a challenging multi-label classification task. There are two \nmajor biological mechanisms for TF binding: (1) sequence-specific binding \npatterns on genomes known as \"motifs\" and (2) interactions among TFs known as \nco-binding effects. In this paper, we propose a novel deep architecture, the \nPrototype Matching Network (PMN) to mimic the TF binding mechanisms. Our PMN \nmodel automatically extracts prototypes (\"motif\"-like features) for each TF \nthrough a novel prototype-matching loss. Borrowing ideas from few-shot matching \nmodels, we use the notion of support set of prototypes and an LSTM to learn how \nTFs interact and bind to genomic sequences. On a reference TFBS dataset with \n$2.1$ $million$ genomic sequences, PMN significantly outperforms baselines and \nvalidates our design choices empirically. To our knowledge, this is the first \ndeep learning architecture that introduces prototype learning and considers \nTF-TF interactions for large-scale TFBS prediction. Not only is the proposed \narchitecture accurate, but it also models the underlying biology. \n</p>"}, "author": "Jack Lanchantin, Arshdeep Sekhon, Ritambhara Singh, Yanjun Qi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185096", "id": "tag:google.com,2005:reader/item/0000000329c22bd3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Time-lagged autoencoders: Deep learning of slow collective variables for molecular kinetics. (arXiv:1710.11239v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11239"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11239", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Inspired by the success of deep learning techniques in the physical and \nchemical sciences, we apply a modification of an autoencoder type deep neural \nnetwork to the task of dimension reduction of molecular dynamics data. We can \nshow that our time-lagged autoencoder reliably finds low-dimensional embeddings \nfor high-dimensional feature spaces which capture the slow dynamics of the \nunderlying stochastic processes - beyond the capabilities of linear dimension \nreduction techniques. \n</p>"}, "author": "Christoph Wehmeyer, Frank No&#xe9;", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185095", "id": "tag:google.com,2005:reader/item/0000000329c22bd8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Approximation Algorithms for $\\ell_0$-Low Rank Approximation. (arXiv:1710.11253v1 [cs.DS])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11253"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11253", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c893bf5a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c893bf5a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study the $\\ell_0$-Low Rank Approximation Problem, where the goal is, \ngiven an $m \\times n$ matrix $A$, to output a rank-$k$ matrix $A'$ for which \n$\\|A'-A\\|_0$ is minimized. Here, for a matrix $B$, $\\|B\\|_0$ denotes the number \nof its non-zero entries. This NP-hard variant of low rank approximation is \nnatural for problems with no underlying metric, and its goal is to minimize the \nnumber of disagreeing data positions. We provide approximation algorithms which \nsignificantly improve the running time and approximation factor of previous \nwork. For $k &gt; 1$, we show how to find, in poly$(mn)$ time for every $k$, a \nrank $O(k \\log(n/k))$ matrix $A'$ for which $\\|A'-A\\|_0 \\leq O(k^2 \\log(n/k)) \n\\mathrm{OPT}$. To the best of our knowledge, this is the first algorithm with \nprovable guarantees for the $\\ell_0$-Low Rank Approximation Problem for $k &gt; \n1$, even for bicriteria algorithms. For the well-studied case when $k = 1$, we \ngive a $(2+\\epsilon)$-approximation in {\\it sublinear time}, which is \nimpossible for other variants of low rank approximation such as for the \nFrobenius norm. We strengthen this for the well-studied case of binary matrices \nto obtain a $(1+O(\\psi))$-approximation in sublinear time, where $\\psi = \n\\mathrm{OPT}/\\lVert A\\rVert_0$. For small $\\psi$, our approximation factor is \n$1+o(1)$. \n</p>"}, "author": "Karl Bringmann, Pavel Kolev, David P. Woodruff", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185094", "id": "tag:google.com,2005:reader/item/0000000329c22bda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Adaptive Sampling Strategies for Stochastic Optimization. (arXiv:1710.11258v1 [math.OC])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11258"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11258", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a stochastic optimization method that adaptively \ncontrols the sample size used in the computation of gradient approximations. \nUnlike other variance reduction techniques that either require additional \nstorage or the regular computation of full gradients, the proposed method \nreduces variance by increasing the sample size as needed. The decision to \nincrease the sample size is governed by an inner product test that ensures that \nsearch directions are descent directions with high probability. We show that \nthe inner product test improves upon the well known norm test, and can be used \nas a basis for an algorithm that is globally convergent on nonconvex functions \nand enjoys a global linear rate of convergence on strongly convex functions. \nNumerical experiments on logistic regression problems illustrate the \nperformance of the algorithm. \n</p>"}, "author": "Raghu Bollapragada, Richard Byrd, Jorge Nocedal", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185093", "id": "tag:google.com,2005:reader/item/0000000329c22bdd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Implicit Manifold Learning on Generative Adversarial Networks. (arXiv:1710.11260v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11260"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11260", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper raises an implicit manifold learning perspective in Generative \nAdversarial Networks (GANs), by studying how the support of the learned \ndistribution, modelled as a submanifold $\\mathcal{M}_{\\theta}$, perfectly match \nwith $\\mathcal{M}_{r}$, the support of the real data distribution. We show that \noptimizing Jensen-Shannon divergence forces $\\mathcal{M}_{\\theta}$ to perfectly \nmatch with $\\mathcal{M}_{r}$, while optimizing Wasserstein distance does not. \nOn the other hand, by comparing the gradients of the Jensen-Shannon divergence \nand the Wasserstein distances ($W_1$ and $W_2^2$) in their primal forms, we \nconjecture that Wasserstein $W_2^2$ may enjoy desirable properties such as \nreduced mode collapse. It is therefore interesting to design new distances that \ninherit the best from both distances. \n</p>"}, "author": "Kry Yik Chau Lui, Yanshuai Cao, Maxime Gazeau, Kelvin Shuangjian Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185092", "id": "tag:google.com,2005:reader/item/0000000329c22be0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Theoretical and Computational Guarantees of Mean Field Variational Inference for Community Detection. (arXiv:1710.11268v2 [math.ST] UPDATED)", "published": 1511218849, "updated": 1511218854, "canonical": [{"href": "http://arxiv.org/abs/1710.11268"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11268", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The mean field variational Bayes method is becoming increasingly popular in \nstatistics and machine learning. Its iterative Coordinate Ascent Variational \nInference algorithm has been widely applied to large scale Bayesian inference. \nSee Blei et al. (2017) for a recent comprehensive review. Despite the \npopularity of the mean field method there exist remarkably little fundamental \ntheoretical justifications. To the best of our knowledge, the iterative \nalgorithm has never been investigated for any high dimensional and complex \nmodel. In this paper, we study the mean field method for community detection \nunder the Stochastic Block Model. For an iterative Batch Coordinate Ascent \nVariational Inference algorithm, we show that it has a linear convergence rate \nand converges to the minimax rate within $\\log n$ iterations. This complements \nthe results of Bickel et al. (2013) which studied the global minimum of the \nmean field variational Bayes and obtained asymptotic normal estimation of \nglobal model parameters. In addition, we obtain similar optimality results for \nGibbs sampling and an iterative procedure to calculate maximum likelihood \nestimation, which can be of independent interest. \n</p>"}, "author": "Anderson Y. Zhang, Harrison H. Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185091", "id": "tag:google.com,2005:reader/item/0000000329c22be2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Empirical analysis of non-linear activation functions for Deep Neural Networks in classification tasks. (arXiv:1710.11272v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11272"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11272", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We provide an overview of several non-linear activation functions in a neural \nnetwork architecture that have proven successful in many machine learning \napplications. We conduct an empirical analysis on the effectiveness of using \nthese function on the MNIST classification task, with the aim of clarifying \nwhich functions produce the best results overall. Based on this first set of \nresults, we examine the effects of building deeper architectures with an \nincreasing number of hidden layers. We also survey the impact of using, on the \nsame task, different initialisation schemes for the weights of our neural \nnetwork. Using these sets of experiments as a base, we conclude by providing a \noptimal neural network architecture that yields impressive results in accuracy \non the MNIST classification task. \n</p>"}, "author": "Giovanni Alcantara", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185090", "id": "tag:google.com,2005:reader/item/0000000329c22be7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Approximating Continuous Functions by ReLU Nets of Minimal Width. (arXiv:1710.11278v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11278"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11278", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This article concerns the expressive power of depth in deep feed-forward \nneural nets with ReLU activations. Specifically, we answer the following \nquestion: for a fixed $d\\geq 1,$ what is the minimal width $w$ so that neural \nnets with ReLU activations, input dimension $d$, hidden layer widths at most \n$w,$ and arbitrary depth can approximate any continuous function of $d$ \nvariables arbitrarily well. It turns out that this minimal width is exactly \nequal to $d+1.$ That is, if all the hidden layer widths are bounded by $d$, \nthen even in the infinite depth limit, ReLU nets can only express a very \nlimited class of functions. On the other hand, we show that any continuous \nfunction on the $d$-dimensional unit cube can be approximated to arbitrary \nprecision by ReLU nets in which all hidden layers have width exactly $d+1.$ Our \nconstruction gives quantitative depth estimates for such an approximation. \n</p>"}, "author": "Boris Hanin, Mark Sellke", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185089", "id": "tag:google.com,2005:reader/item/0000000329c22bea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Effective Tensor Sketching via Sparsification. (arXiv:1710.11298v3 [stat.ME] UPDATED)", "published": 1510882706, "updated": 1510882707, "canonical": [{"href": "http://arxiv.org/abs/1710.11298"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11298", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we investigate effective sketching schemes via sparsification \nfor high dimensional multilinear arrays or tensors. More specifically, we \npropose a novel tensor sparsification algorithm that retains a subset of the \nentries of a tensor in a judicious way, and prove that it can attain a given \nlevel of approximation accuracy in terms of tensor spectral norm with a much \nsmaller sample complexity when compared with existing approaches. In \nparticular, we show that for a $k$th order $d\\times\\cdots\\times d$ cubic tensor \nof {\\it stable rank} $r_s$, the sample size requirement for achieving a \nrelative error $\\varepsilon$ is, up to a logarithmic factor, of the order \n$r_s^{1/2} d^{k/2} /\\varepsilon$ when $\\varepsilon$ is relatively large, and \n$r_s d /\\varepsilon^2$ and essentially optimal when $\\varepsilon$ is \nsufficiently small. It is especially noteworthy that the sample size \nrequirement for achieving a high accuracy is of an order independent of $k$. To \nfurther demonstrate the utility of our techniques, we also study how higher \norder singular value decomposition (HOSVD) of large tensors can be efficiently \napproximated via sparsification. \n</p>"}, "author": "Dong Xia, Ming Yuan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185088", "id": "tag:google.com,2005:reader/item/0000000329c22bf0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Algorithmic learning of probability distributions from random data in the limit. (arXiv:1710.11303v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11303"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11303", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of identifying a probability distribution for some given \nrandomly sampled data in the limit, in the context of algorithmic learning \ntheory as proposed recently by Vinanyi and Chater. We show that there exists a \ncomputable partial learner for the computable probability measures, while by \nBienvenu, Monin and Shen it is known that there is no computable learner for \nthe computable probability measures. Our main result is the characterization of \nthe oracles that compute explanatory learners for the computable (continuous) \nprobability measures as the high oracles. This provides an analogue of a \nwell-known result of Adleman and Blum in the context of learning computable \nprobability distributions. We also discuss related learning notions such as \nbehaviorally correct learning and orther variations of explanatory learning, in \nthe context of learning probability distributions from data. \n</p>"}, "author": "George Barmpalias, Frank Stephan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185087", "id": "tag:google.com,2005:reader/item/0000000329c22bf9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Characterizing the structural diversity of complex networks across domains. (arXiv:1710.11304v1 [cs.SI])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11304"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11304", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The structure of complex networks has been of interest in many scientific and \nengineering disciplines over the decades. A number of studies in the field have \nbeen focused on finding the common properties among different kinds of networks \nsuch as heavy-tail degree distribution, small-worldness and modular structure \nand they have tried to establish a theory of structural universality in complex \nnetworks. However, there is no comprehensive study of network structure across \na diverse set of domains in order to explain the structural diversity we \nobserve in the real-world networks. In this paper, we study 986 real-world \nnetworks of diverse domains ranging from ecological food webs to online social \nnetworks along with 575 networks generated from four popular network models. \nOur study utilizes a number of machine learning techniques such as random \nforest and confusion matrix in order to show the relationships among network \ndomains in terms of network structure. Our results indicate that there are some \npartitions of network categories in which networks are hard to distinguish \nbased purely on network structure. We have found that these partitions of \nnetwork categories tend to have similar underlying functions, constraints \nand/or generative mechanisms of networks even though networks in the same \npartition have different origins, e.g., biological processes, results of \nengineering by human being, etc. This suggests that the origin of a network, \nwhether it's biological, technological or social, may not necessarily be a \ndecisive factor of the formation of similar network structure. Our findings \nshed light on the possible direction along which we could uncover the hidden \nprinciples for the structural diversity of complex networks. \n</p>"}, "author": "Kansuke Ikehara, Aaron Clauset", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185086", "id": "tag:google.com,2005:reader/item/0000000329c22c04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Exact Solution to Rank-1 L1-norm TUCKER2 Decomposition. (arXiv:1710.11306v1 [cs.DS])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11306"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11306", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study rank-1 {L1-norm-based TUCKER2} (L1-TUCKER2) decomposition of 3-way \ntensors, treated as a collection of $N$ $D \\times M$ matrices that are to be \njointly decomposed. Our contributions are as follows. i) We prove that the \nproblem is equivalent to combinatorial optimization over $N$ antipodal-binary \nvariables. ii) We derive the first two algorithms in the literature for its \nexact solution. The first algorithm has cost exponential in $N$; the second one \nhas cost polynomial in $N$ (under a mild assumption). Our algorithms are \naccompanied by formal complexity analysis. iii) We conduct numerical studies to \ncompare the performance of exact L1-TUCKER2 (proposed) with standard HOSVD, \nHOOI, GLRAM, PCA, L1-PCA, and TPCA-L1. Our studies show that L1-TUCKER2 \noutperforms (in tensor approximation) all the above counterparts when the \nprocessed data are outlier corrupted. \n</p>"}, "author": "Panos P. Markopoulos, Dimitris G. Chachlakis, Evangelos E. Papalexakis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185085", "id": "tag:google.com,2005:reader/item/0000000329c22c0b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Rate-optimal Meta Learning of Classification Error. (arXiv:1710.11315v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11315"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11315", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c893c15f\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c893c15f&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Meta learning of optimal classifier error rates allows an experimenter to \nempirically estimate the intrinsic ability of any estimator to discriminate \nbetween two populations, circumventing the difficult problem of estimating the \noptimal Bayes classifier. To this end we propose a weighted nearest neighbor \n(WNN) graph estimator for a tight bound on the Bayes classification error; the \nHenze-Penrose (HP) divergence. Similar to recently proposed HP estimators \n[berisha2016], the proposed estimator is non-parametric and does not require \ndensity estimation. However, unlike previous approaches the proposed estimator \nis rate-optimal, i.e., its mean squared estimation error (MSEE) decays to zero \nat the fastest possible rate of $O(1/M+1/N)$ where $M,N$ are the sample sizes \nof the respective populations. We illustrate the proposed WNN meta estimator \nfor several simulated and real data sets. \n</p>"}, "author": "Morteza Noshad Iranzad, Alfred O. Hero III", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185084", "id": "tag:google.com,2005:reader/item/0000000329c22c10", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Tensor Regression Meets Gaussian Processes. (arXiv:1710.11345v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11345"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11345", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c89da03d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c89da03d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Low-rank tensor regression, a new model class that learns high-order \ncorrelation from data, has recently received considerable attention. At the \nsame time, Gaussian processes (GP) are well-studied machine learning models for \nstructure learning. In this paper, we demonstrate interesting connections \nbetween the two, especially for multi-way data analysis. We show that low-rank \ntensor regression is essentially learning a multi-linear kernel in Gaussian \nprocesses, and the low-rank assumption translates to the constrained Bayesian \ninference problem. We prove the oracle inequality and derive the average case \nlearning curve for the equivalent GP model. Our finding implies that low-rank \ntensor regression, though empirically successful, is highly dependent on the \neigenvalues of covariance functions as well as variable correlations. \n</p>"}, "author": "Rose Yu, Guangyu Li, Yan Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185083", "id": "tag:google.com,2005:reader/item/0000000329c22c22", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Latent Space Oddity: on the Curvature of Deep Generative Models. (arXiv:1710.11379v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11379"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11379", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep generative models provide a systematic way to learn nonlinear data \ndistributions, through a set of latent variables and a nonlinear \"generator\" \nfunction that maps latent points into the input space. The nonlinearity of the \ngenerator imply that the latent space gives a distorted view of the input \nspace. Under mild conditions, we show that this distortion can be characterized \nby a stochastic Riemannian metric, and demonstrate that distances and \ninterpolants are significantly improved under this metric. This in turn \nimproves probability distributions, sampling algorithms and clustering in the \nlatent space. Our geometric analysis further reveals that current generators \nprovide poor variance estimates and we propose a new generator architecture \nwith vastly improved variance estimates. Results are demonstrated on \nconvolutional and fully connected variational autoencoders, but the formalism \neasily generalize to other deep generative models. \n</p>"}, "author": "Georgios Arvanitidis, Lars Kai Hansen, S&#xf8;ren Hauberg", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185082", "id": "tag:google.com,2005:reader/item/0000000329c22c35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Semantic Interpolation in Implicit Models. (arXiv:1710.11381v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11381"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11381", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In implicit models, one often interpolates between sampled points in latent \nspace. As we show in this paper, care needs to be taken to match-up the \ndistributional assumptions on code vectors with the geometry of the \ninterpolating paths. Otherwise, typical assumptions about the quality and \nsemantics of in-between points may not be justified. Based on our analysis we \npropose to modify the prior code distribution to put significantly more \nprobability mass closer to the origin. As a result, linear interpolation paths \nare not only shortest paths, but they are also guaranteed to pass through \nhigh-density regions, irrespective of the dimensionality of the latent space. \nExperiments on standard benchmark image datasets demonstrate clear visual \nimprovements in the quality of the generated samples and exhibit more \nmeaningful interpolation paths. \n</p>"}, "author": "Yannic Kilcher, Aurelien Lucchi, Thomas Hofmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185081", "id": "tag:google.com,2005:reader/item/0000000329c22c42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Flexible Prior Distributions for Deep Generative Models. (arXiv:1710.11383v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11383"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11383", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of training generative models with deep neural \nnetworks as generators, i.e. to map latent codes to data points. Whereas the \ndominant paradigm combines simple priors over codes with complex deterministic \nmodels, we argue that it might be advantageous to use more flexible code \ndistributions. We demonstrate how these distributions can be induced directly \nfrom the data. The benefits include: more powerful generative models, better \nmodeling of latent structure and explicit control of the degree of \ngeneralization. \n</p>"}, "author": "Yannic Kilcher, Aurelien Lucchi, Thomas Hofmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185080", "id": "tag:google.com,2005:reader/item/0000000329c22c48", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Parametrizing filters of a CNN with a GAN. (arXiv:1710.11386v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11386"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11386", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is commonly agreed that the use of relevant invariances as a good \nstatistical bias is important in machine-learning. However, most approaches \nthat explicitly incorporate invariances into a model architecture only make use \nof very simple transformations, such as translations and rotations. Hence, \nthere is a need for methods to model and extract richer transformations that \ncapture much higher-level invariances. To that end, we introduce a tool \nallowing to parametrize the set of filters of a trained convolutional neural \nnetwork with the latent space of a generative adversarial network. We then show \nthat the method can capture highly non-linear invariances of the data by \nvisualizing their effect in the data space. \n</p>"}, "author": "Yannic Kilcher, Gary Becigneul, Thomas Hofmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185079", "id": "tag:google.com,2005:reader/item/0000000329c22c4d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Updating the VESICLE-CNN Synapse Detector. (arXiv:1710.11397v1 [cs.CV])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11397"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11397", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present an updated version of the VESICLE-CNN algorithm presented by \nRoncal et al. (2014). The original implementation makes use of a patch-based \napproach. This methodology is known to be slow due to repeated computations. We \nupdate this implementation to be fully convolutional through the use of dilated \nconvolutions, recovering the expanded field of view achieved through the use of \nstrided maxpools, but without a degradation of spatial resolution. This updated \nimplementation performs as well as the original implementation, but with a \n$600\\times$ speedup at test time. We release source code and data into the \npublic domain. \n</p>"}, "author": "Andrew Warrington, Frank Wood", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185078", "id": "tag:google.com,2005:reader/item/0000000329c22c55", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning. (arXiv:1710.11417v1 [cs.AI])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11417"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11417", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Combining deep model-free reinforcement learning with on-line planning is a \npromising approach to building on the successes of deep RL. On-line planning \nwith look-ahead trees has proven successful in environments where transition \nmodels are known a priori. However, in complex environments where transition \nmodels need to be learned from data, the deficiencies of learned models have \nlimited their utility for planning. To address these challenges, we propose \nTreeQN, a differentiable, recursive, tree-structured model that serves as a \ndrop-in replacement for any value function network in deep RL with discrete \nactions. TreeQN dynamically constructs a tree by recursively applying a \ntransition model in a learned abstract state space and then aggregating \npredicted rewards and state-values using a tree backup to estimate Q-values. We \nalso propose ATreeC, an actor-critic variant that augments TreeQN with a \nsoftmax layer to form a stochastic policy network. Both approaches are trained \nend-to-end, such that the learned model is optimised for its actual use in the \nplanner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a \nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et \nal., 2017) on multiple Atari games, with deeper trees often outperforming \nshallower ones. We also present a qualitative analysis that sheds light on the \ntrees learned by TreeQN. \n</p>"}, "author": "Gregory Farquhar, Tim Rockt&#xe4;schel, Maximilian Igl, Shimon Whiteson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185077", "id": "tag:google.com,2005:reader/item/0000000329c22c5d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Physics-guided Neural Networks (PGNN): An Application in Lake Temperature Modeling. (arXiv:1710.11431v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11431"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper introduces a novel framework for learning data science models by \nusing the scientific knowledge encoded in physics-based models. This framework, \ntermed as physics-guided neural network (PGNN), leverages the output of \nphysics-based model simulations along with observational features to generate \npredictions using a neural network architecture. Further, we present a novel \nclass of learning objective for training neural networks, which ensures that \nthe model predictions not only show lower errors on the training data but are \nalso \\emph{consistent} with the known physics. We illustrate the effectiveness \nof PGNN for the problem of lake temperature modeling, where physical \nrelationships between the temperature, density, and depth of water are used in \nthe learning of neural network model parameters. By using scientific knowledge \nto guide the construction and learning of neural networks, we are able to show \nthat the proposed framework ensures better generalizability as well as physical \nconsistency of results. \n</p>"}, "author": "Anuj Karpatne, William Watkins, Jordan Read, Vipin Kumar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185076", "id": "tag:google.com,2005:reader/item/0000000329c22c69", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Neural Representations of Human Cognition across Many fMRI Studies. (arXiv:1710.11438v2 [stat.ML] UPDATED)", "published": 1510769311, "updated": 1510769327, "canonical": [{"href": "http://arxiv.org/abs/1710.11438"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11438", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Cognitive neuroscience is enjoying rapid increase in extensive public \nbrain-imaging datasets. It opens the door to large-scale statistical models. \nFinding a unified perspective for all available data calls for scalable and \nautomated solutions to an old challenge: how to aggregate heterogeneous \ninformation on brain function into a universal cognitive system that relates \nmental operations/cognitive processes/psychological tasks to brain networks? We \ncast this challenge in a machine-learning approach to predict conditions from \nstatistical brain maps across different studies. For this, we leverage \nmulti-task learning and multi-scale dimension reduction to learn \nlow-dimensional representations of brain images that carry cognitive \ninformation and can be robustly associated with psychological stimuli. Our \nmulti-dataset classification model achieves the best prediction performance on \nseveral large reference datasets, compared to models without cognitive-aware \nlow-dimension representations, it brings a substantial performance boost to the \nanalysis of small datasets, and can be introspected to identify universal \ntemplate cognitive concepts. \n</p>"}, "author": "Arthur Mensch (PARIETAL, NEUROSPIN), Julien Mairal (Thoth, LJK), Danilo Bzdok, Bertrand Thirion (PARIETAL, NEUROSPIN), Ga&#xeb;l Varoquaux (PARIETAL, NEUROSPIN)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185075", "id": "tag:google.com,2005:reader/item/0000000329c22c72", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Statistical Speech Enhancement Based on Probabilistic Integration of Variational Autoencoder and Non-Negative Matrix Factorization. (arXiv:1710.11439v3 [cs.SD] UPDATED)", "published": 1510859309, "updated": 1510859315, "canonical": [{"href": "http://arxiv.org/abs/1710.11439"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11439", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c89da41d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c89da41d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper presents a statistical method of single-channel speech enhancement \nthat uses a variational autoencoder (VAE) as a prior distribution on clean \nspeech. A standard approach to speech enhancement is to train a deep neural \nnetwork (DNN) to take noisy speech as input and output clean speech. Although \nthis supervised approach requires a very large amount of pair data for \ntraining, it is not robust against unknown environments. Another approach is to \nuse non-negative matrix factorization (NMF) based on basis spectra trained on \nclean speech in advance and those adapted to noise on the fly. This \nsemi-supervised approach, however, causes considerable signal distortion in \nenhanced speech due to the unrealistic assumption that speech spectrograms are \nlinear combinations of the basis spectra. Replacing the poor linear generative \nmodel of clean speech in NMF with a VAE---a powerful nonlinear deep generative \nmodel---trained on clean speech, we formulate a unified probabilistic \ngenerative model of noisy speech. Given noisy speech as observed data, we can \nsample clean speech from its posterior distribution. The proposed method \noutperformed the conventional DNN-based method in unseen noisy environments. \n</p>"}, "author": "Yoshiaki Bando, Masato Mimura, Katsutoshi Itoyama, Kazuyoshi Yoshii, Tatsuya Kawahara", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185074", "id": "tag:google.com,2005:reader/item/0000000329c22c88", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Guarding Against Adversarial Domain Shifts with Counterfactual Regularization. (arXiv:1710.11469v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11469"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11469", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>When training a deep network for image classification, one can broadly \ndistinguish between two types of latent features of images that will drive the \nclassification: (i) \"immutable\" or \"core\" features that are inherent to the \nobject in question and do not change substantially from one instance of the \nobject to another and (ii) \"mutable\" or \"style\" features such as position, \nrotation or image quality but also more complex ones like hair color or posture \nfor images of persons. The distribution of the style features can change in the \nfuture. While transfer learning would try to adapt to a shift in the \ndistribution(s), we here want to protect against future adversarial domain \nshifts, arising through changing style features, by ideally not using the \nmutable style features altogether. \n</p> \n<p>There are two broad scenarios and we show how exploiting grouping information \nin the data helps in both. (a) If the style features are known explicitly (e.g. \nrotation) one usually proceeds by using data augmentation. By exploiting the \ngrouping information about which original image an augmented sample belongs to, \nwe can reduce the sample size required to achieve invariance to the style \nfeature in question. (b) Sometimes the style features are not known explicitly \nbut we still have information about samples that belong to the same underlying \nobject (such as different pictures of the same person). By constraining the \nclassification to give the same forecast for all instances that belong to the \nsame object, we show how using this grouping information leads to invariance to \nsuch implicit style features and helps to protect against adversarial domain \nshifts. \n</p> \n<p>We provide a causal framework for the problem and treat groups of instances \nof the same object as counterfactuals under different interventions on the \nmutable style features. We show links to questions of fairness, transfer \nlearning and adversarial examples. \n</p>"}, "author": "Christina Heinze-Deml, Nicolai Meinshausen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185073", "id": "tag:google.com,2005:reader/item/0000000329c22c92", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Compact Multi-Class Boosted Trees. (arXiv:1710.11547v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11547"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11547", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Gradient boosted decision trees are a popular machine learning technique, in \npart because of their ability to give good accuracy with small models. We \ndescribe two extensions to the standard tree boosting algorithm designed to \nincrease this advantage. The first improvement extends the boosting formalism \nfrom scalar-valued trees to vector-valued trees. This allows individual trees \nto be used as multiclass classifiers, rather than requiring one tree per class, \nand drastically reduces the model size required for multiclass problems. We \nalso show that some other popular vector-valued gradient boosted trees \nmodifications fit into this formulation and can be easily obtained in our \nimplementation. The second extension, layer-by-layer boosting, takes smaller \nsteps in function space, which is empirically shown to lead to a faster \nconvergence and to a more compact ensemble. We have added both improvements to \nthe open-source TensorFlow Boosted trees (TFBT) package, and we demonstrate \ntheir efficacy on a variety of multiclass datasets. We expect these extensions \nwill be of particular interest to boosted tree applications that require small \nmodels, such as embedded devices, applications requiring fast inference, or \napplications desiring more interpretable models. \n</p>"}, "author": "Natalia Ponomareva, Thomas Colthurst, Gilbert Hendry, Salem Haykal, Soroush Radpour", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185072", "id": "tag:google.com,2005:reader/item/0000000329c22c9a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "TF Boosted Trees: A scalable TensorFlow based framework for gradient boosting. (arXiv:1710.11555v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11555"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11555", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>TF Boosted Trees (TFBT) is a new open-sourced frame-work for the distributed \ntraining of gradient boosted trees. It is based on TensorFlow, and its \ndistinguishing features include a novel architecture, automatic loss \ndifferentiation, layer-by-layer boosting that results in smaller ensembles and \nfaster prediction, principled multi-class handling, and a number of \nregularization techniques to prevent overfitting. \n</p>"}, "author": "Natalia Ponomareva, Soroush Radpour, Gilbert Hendry, Salem Haykal, Thomas Colthurst, Petr Mitrichev, Alexander Grushetsky", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185071", "id": "tag:google.com,2005:reader/item/0000000329c22ca1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Graph Convolution Filters from Data Manifold. (arXiv:1710.11577v1 [cs.LG])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11577"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11577", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolution Neural Network (CNN) has gained tremendous success in computer \nvision tasks with its outstanding ability to capture the local latent features. \nRecently, there has been an increasing interest in extending CNNs to the \ngeneral spatial domain. Although various types of graph and geometric \nconvolution methods have been proposed, their connections to traditional \n2D-convolution are not well-understood. In this paper, we show that depthwise \nseparable convolution is the key to close the gap, based on which we derive a \nnovel Depthwise Separable Graph Convolution that subsumes existing graph \nconvolution methods as special cases of our formulation. Experiments show that \nthe proposed approach consistently outperforms other graph and geometric \nconvolution baselines on benchmark datasets in multiple domains. \n</p>"}, "author": "Guokun Lai, Hanxiao Liu, Yiming Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185070", "id": "tag:google.com,2005:reader/item/0000000329c22ca9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Partial Least Squares Random Forest Ensemble Regression as a Soft Sensor. (arXiv:1710.11595v1 [stat.ML])", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11595"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11595", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Six simple, dynamic soft sensor methodologies with two update conditions were \ncompared on two experimentally-obtained datasets and one simulated dataset. The \nsoft sensors investigated were: moving window partial least squares regression \n(and a recursive variant), moving window random forest regression, feedforward \nneural networks, mean moving window, and a novel random forest partial least \nsquares regression ensemble (RF-PLS). We found that, on two of the datasets \nstudied, very small window sizes (4 samples) led to the lowest prediction \nerrors. The RF-PLS method offered the lowest one-step-ahead prediction errors \ncompared to those of the other methods, and demonstrated greater stability at \nlarger time lags than moving window PLS alone. We found that this method most \nadequately modeled the datasets that did not feature purely monotonic increases \nin property values. In general, we observed that linear models deteriorated \nmost rapidly at more delayed model update conditions while nonlinear methods \ntended to provide predictions that approached those from a simple mean moving \nwindow. Other data dependent findings are presented and discussed. \n</p>"}, "author": "Casey Kneale, Steven Brown", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185059", "id": "tag:google.com,2005:reader/item/0000000329c22ceb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Inhomogeneous Hypergraph Clustering with Applications. (arXiv:1709.01249v3 [cs.LG] UPDATED)", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.01249"}], "alternate": [{"href": "http://arxiv.org/abs/1709.01249", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Hypergraph partitioning is an important problem in machine learning, computer \nvision and network analytics. A widely used method for hypergraph partitioning \nrelies on minimizing a normalized sum of the costs of partitioning hyperedges \nacross clusters. Algorithmic solutions based on this approach assume that \ndifferent partitions of a hyperedge incur the same cost. However, this \nassumption fails to leverage the fact that different subsets of vertices within \nthe same hyperedge may have different structural importance. We hence propose a \nnew hypergraph clustering technique, termed inhomogeneous hypergraph \npartitioning, which assigns different costs to different hyperedge cuts. We \nprove that inhomogeneous partitioning produces a quadratic approximation to the \noptimal solution if the inhomogeneous costs satisfy submodularity constraints. \nMoreover, we demonstrate that inhomogenous partitioning offers significant \nperformance improvements in applications such as structure learning of \nrankings, subspace segmentation and motif clustering. \n</p>"}, "author": "Pan Li, Olgica Milenkovic", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185053", "id": "tag:google.com,2005:reader/item/0000000329c22cf6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tracking Tetrahymena Pyriformis Cells using Decision Trees. (arXiv:1207.3127v1 [cs.CV] CROSS LISTED)", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1207.3127"}], "alternate": [{"href": "http://arxiv.org/abs/1207.3127", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Matching cells over time has long been the most difficult step in cell \ntracking. In this paper, we approach this problem by recasting it as a \nclassification problem. We construct a feature set for each cell, and compute a \nfeature difference vector between a cell in the current frame and a cell in a \nprevious frame. Then we determine whether the two cells represent the same cell \nover time by training decision trees as our binary classifiers. With the output \nof decision trees, we are able to formulate an assignment problem for our cell \nassociation task and solve it using a modified version of the Hungarian \nalgorithm. \n</p>"}, "author": "Quan Wang, Yan Ou, A. Agung Julius, Kim L. Boyer, Min Jun Kim", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185052", "id": "tag:google.com,2005:reader/item/0000000329c22cf9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Semantic Context Forests for Learning-Based Knee Cartilage Segmentation in 3D MR Images. (arXiv:1307.2965v2 [cs.CV] CROSS LISTED)", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1307.2965"}], "alternate": [{"href": "http://arxiv.org/abs/1307.2965", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The automatic segmentation of human knee cartilage from 3D MR images is a \nuseful yet challenging task due to the thin sheet structure of the cartilage \nwith diffuse boundaries and inhomogeneous intensities. In this paper, we \npresent an iterative multi-class learning method to segment the femoral, tibial \nand patellar cartilage simultaneously, which effectively exploits the spatial \ncontextual constraints between bone and cartilage, and also between different \ncartilages. First, based on the fact that the cartilage grows in only certain \narea of the corresponding bone surface, we extract the distance features of not \nonly to the surface of the bone, but more informatively, to the densely \nregistered anatomical landmarks on the bone surface. Second, we introduce a set \nof iterative discriminative classifiers that at each iteration, probability \ncomparison features are constructed from the class confidence maps derived by \npreviously learned classifiers. These features automatically embed the semantic \ncontext information between different cartilages of interest. Validated on a \ntotal of 176 volumes from the Osteoarthritis Initiative (OAI) dataset, the \nproposed approach demonstrates high robustness and accuracy of segmentation in \ncomparison with existing state-of-the-art MR cartilage segmentation methods. \n</p>"}, "author": "Quan Wang, Dijia Wu, Le Lu, Meizhu Liu, Kim L. Boyer, Shaohua Kevin Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496837185", "timestampUsec": "1509496837185051", "id": "tag:google.com,2005:reader/item/0000000329c22cfe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "DGM: A deep learning algorithm for solving partial differential equations. (arXiv:1708.07469v1 [q-fin.MF] CROSS LISTED)", "published": 1509496837, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.07469"}], "alternate": [{"href": "http://arxiv.org/abs/1708.07469", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>High-dimensional PDEs have been a longstanding computational challenge. We \npropose a deep learning algorithm similar in spirit to Galerkin methods, using \na deep neural network instead of linear combinations of basis functions. The \nPDE is approximated with a deep neural network, which is trained on random \nbatches of spatial points to satisfy the differential operator and boundary \nconditions. The algorithm is mesh-less, which is key since meshes become \ninfeasible in higher dimensions. Instead of forming a mesh, sequences of \nspatial points are randomly sampled. We implement the approach for American \noptions (a type of free-boundary PDE which is widely used in finance) in up to \n100 dimensions. We call the algorithm a \"Deep Galerkin Method (DGM)\". \n</p>"}, "author": "Justin Sirignano, Konstantinos Spiliopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408105", "id": "tag:google.com,2005:reader/item/0000000329c20aec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Super-polynomial separations for quantum-enhanced reinforcement learning. (arXiv:1710.11160v1 [quant-ph])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11160"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11160", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c89da759\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c89da759&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Recent work on quantum machine learning has demonstrated that quantum \ncomputers can offer dramatic improvements over classical devices for data \nmining, prediction and classification. However, less is known about the \nadvantages using quantum computers may bring in the more general setting of \nreinforcement learning, where learning is achieved via interaction with a task \nenvironment that provides occasional rewards. Reinforcement learning can \nincorporate data-analysis-oriented learning settings as special cases, but also \nincludes more complex situations where, e.g., reinforcing feedback is delayed. \nIn a few recent works, Grover-type amplification has been utilized to construct \nquantum agents that achieve up-to-quadratic improvements in learning \nefficiency. These encouraging results have left open the key question of \nwhether super-polynomial improvements in learning times are possible for \ngenuine reinforcement learning problems, that is problems that go beyond the \nother more restricted learning paradigms. In this work, we provide a family of \nsuch genuine reinforcement learning tasks, and we construct quantum-enhanced \nlearners which learn super-polynomially faster than any classical reinforcement \nlearning model. \n</p>"}, "author": "Vedran Dunjko, Yi-Kai Liu, Xingyao Wu, Jacob M. Taylor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408104", "id": "tag:google.com,2005:reader/item/0000000329c20af6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Indirect Supervision for Relation Extraction using Question-Answer Pairs. (arXiv:1710.11169v1 [cs.CL])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11169"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11169", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8b92560\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8b92560&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Automatic relation extraction (RE) for types of interest is of great \nimportance for interpreting massive text corpora in an efficient manner. \nTraditional RE models have heavily relied on human-annotated corpus for \ntraining, which can be costly in generating labeled data and become obstacles \nwhen dealing with more relation types. Thus, more RE extraction systems have \nshifted to be built upon training data automatically acquired by linking to \nknowledge bases (distant supervision). However, due to the incompleteness of \nknowledge bases and the context-agnostic labeling, the training data collected \nvia distant supervision (DS) can be very noisy. In recent years, as increasing \nattention has been brought to tackling question-answering (QA) tasks, user \nfeedback or datasets of such tasks become more accessible. In this paper, we \npropose a novel framework, ReQuest, to leverage question-answer pairs as an \nindirect source of supervision for relation extraction, and study how to use \nsuch supervision to reduce noise induced from DS. Our model jointly embeds \nrelation mentions, types, QA entity mention pairs and text features in two \nlow-dimensional spaces (RE and QA), where objects with same relation types or \nsemantically similar question-answer pairs have similar representations. Shared \nfeatures connect these two spaces, carrying clearer semantic knowledge from \nboth sources. ReQuest, then use these learned embeddings to estimate the types \nof test relation mentions. We formulate a global objective function and adopt a \nnovel margin-based QA loss to reduce noise in DS by exploiting semantic \nevidence from the QA dataset. Our experimental results achieve an average of \n11% improvement in F1 score on two public RE datasets combined with TREC QA \ndataset. \n</p>"}, "author": "Zeqiu Wu, Xiang Ren, Frank F. Xu, Ji Li, Jiawei Han", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408103", "id": "tag:google.com,2005:reader/item/0000000329c20afc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improve SAT-solving with Machine Learning. (arXiv:1710.11204v1 [cs.AI])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11204"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11204", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this project, we aimed to improve the runtime of Minisat, a \nConflict-Driven Clause Learning (CDCL) solver that solves the Propositional \nBoolean Satisfiability (SAT) problem. We first used a logistic regression model \nto predict the satisfiability of propositional boolean formulae after fixing \nthe values of a certain fraction of the variables in each formula. We then \napplied the logistic model and added a preprocessing period to Minisat to \ndetermine the preferable initial value (either true or false) of each boolean \nvariable using a Monte-Carlo approach. Concretely, for each Monte-Carlo trial, \nwe fixed the values of a certain ratio of randomly selected variables, and \ncalculated the confidence that the resulting sub-formula is satisfiable with \nour logistic regression model. The initial value of each variable was set based \non the mean confidence scores of the trials that started from the literals of \nthat variable. We were particularly interested in setting the initial values of \nthe backbone variables correctly, which are variables that have the same value \nin all solutions of a SAT formula. Our Monte-Carlo method was able to set 78% \nof the backbones correctly. Excluding the preprocessing time, compared with the \ndefault setting of Minisat, the runtime of Minisat for satisfiable formulae \ndecreased by 23%. However, our method did not outperform vanilla Minisat in \nruntime, as the decrease in the conflicts was outweighed by the long runtime of \nthe preprocessing period. \n</p>"}, "author": "Haoze Wu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408102", "id": "tag:google.com,2005:reader/item/0000000329c20afe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast and Scalable Learning of Sparse Changes in High-Dimensional Gaussian Graphical Model Structure. (arXiv:1710.11223v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11223"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11223", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We focus on the problem of estimating the change in the dependency structures \nof two $p$-dimensional Gaussian Graphical models (GGMs). Previous studies for \nsparse change estimation in GGMs involve expensive and difficult non-smooth \noptimization. We propose a novel method, DIFFEE for estimating DIFFerential \nnetworks via an Elementary Estimator under a high-dimensional situation. DIFFEE \nis solved through a faster and closed form solution that enables it to work in \nlarge-scale settings. We conduct a rigorous statistical analysis showing that \nsurprisingly DIFFEE achieves the same asymptotic convergence rates as the \nstate-of-the-art estimators that are much more difficult to compute. Our \nexperimental results on multiple synthetic datasets and one real-world data \nabout brain connectivity show strong performance improvements over baselines, \nas well as significant computational benefits. \n</p>"}, "author": "Beilun Wang, Arshdeep Sekhon, Yanjun Qi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408101", "id": "tag:google.com,2005:reader/item/0000000329c20b07", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Prototype Matching Networks for Large-Scale Multi-label Genomic Sequence Classification. (arXiv:1710.11238v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11238"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11238", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>One of the fundamental tasks in understanding genomics is the problem of \npredicting Transcription Factor Binding Sites (TFBSs). With more than hundreds \nof Transcription Factors (TFs) as labels, genomic-sequence based TFBS \nprediction is a challenging multi-label classification task. There are two \nmajor biological mechanisms for TF binding: (1) sequence-specific binding \npatterns on genomes known as \"motifs\" and (2) interactions among TFs known as \nco-binding effects. In this paper, we propose a novel deep architecture, the \nPrototype Matching Network (PMN) to mimic the TF binding mechanisms. Our PMN \nmodel automatically extracts prototypes (\"motif\"-like features) for each TF \nthrough a novel prototype-matching loss. Borrowing ideas from few-shot matching \nmodels, we use the notion of support set of prototypes and an LSTM to learn how \nTFs interact and bind to genomic sequences. On a reference TFBS dataset with \n$2.1$ $million$ genomic sequences, PMN significantly outperforms baselines and \nvalidates our design choices empirically. To our knowledge, this is the first \ndeep learning architecture that introduces prototype learning and considers \nTF-TF interactions for large-scale TFBS prediction. Not only is the proposed \narchitecture accurate, but it also models the underlying biology. \n</p>"}, "author": "Jack Lanchantin, Arshdeep Sekhon, Ritambhara Singh, Yanjun Qi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408100", "id": "tag:google.com,2005:reader/item/0000000329c20b0e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Adversarial Advantage Actor-Critic Model for Task-Completion Dialogue Policy Learning. (arXiv:1710.11277v1 [cs.CL])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11277"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11277", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a new method --- adversarial advantage actor-critic \n(Adversarial A2C), which significantly improves the efficiency of dialogue \npolicy learning in task-completion dialogue systems. Inspired by generative \nadversarial networks (GAN), we train a discriminator to differentiate \nresponses/actions generated by dialogue agents from responses/actions by \nexperts. Then, we incorporate the discriminator as another critic into the \nadvantage actor-critic (A2C) framework, to encourage the dialogue agent to \nexplore state-action within the regions where the agent takes actions similar \nto those of the experts. Experimental results in a movie-ticket booking domain \nshow that the proposed Adversarial A2C can accelerate policy exploration \nefficiently. \n</p>"}, "author": "Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Yun-Nung Chen, Kam-Fai Wong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408099", "id": "tag:google.com,2005:reader/item/0000000329c20b14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Forward and Inverse Perceptual Models for Tracking and Prediction. (arXiv:1710.11311v1 [cs.RO])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11311"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11311", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problems of learning forward models that map state to \nhigh-dimensional images and inverse models that map high-dimensional images to \nstate in robotics. Specifically, we present a perceptual model for generating \nvideo frames from state with deep networks, and provide a framework for its use \nin tracking and prediction tasks. We show that our proposed model greatly \noutperforms standard deconvolutional methods and GANs for image generation, \nproducing clear, photo-realistic images. We also develop a convolutional neural \nnetwork model for state estimation and compare the result to an Extended Kalman \nFilter to estimate robot trajectories. We validate all models on a real robotic \nsystem. \n</p>"}, "author": "Alexander Lambert, Amirreza Shaban, Amit Raj, Zhen Liu, Byron Boots", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408098", "id": "tag:google.com,2005:reader/item/0000000329c20b17", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Generating Natural Adversarial Examples. (arXiv:1710.11342v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11342"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11342", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Due to their complex nature, it is hard to characterize the ways in which \nmachine learning models can misbehave or be exploited when deployed. Recent \nwork on adversarial examples, i.e. inputs with minor perturbations that result \nin substantially different model predictions, is helpful in evaluating the \nrobustness of these models by exposing the adversarial scenarios where they \nfail. However, these malicious perturbations are often unnatural, not \nsemantically meaningful, and not applicable to complicated domains such as \nlanguage. In this paper, we propose a framework to generate natural and legible \nadversarial examples by searching in semantic space of dense and continuous \ndata representation, utilizing the recent advances in generative adversarial \nnetworks. We present generated adversaries to demonstrate the potential of the \nproposed approach for black-box classifiers in a wide range of applications \nsuch as image classification, textual entailment, and machine translation. We \ninclude experiments to show that the generated adversaries are natural, legible \nto humans, and useful in evaluating and analyzing black-box classifiers. \n</p>"}, "author": "Zhengli Zhao, Dheeru Dua, Sameer Singh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408097", "id": "tag:google.com,2005:reader/item/0000000329c20b1e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Parametrizing filters of a CNN with a GAN. (arXiv:1710.11386v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11386"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11386", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is commonly agreed that the use of relevant invariances as a good \nstatistical bias is important in machine-learning. However, most approaches \nthat explicitly incorporate invariances into a model architecture only make use \nof very simple transformations, such as translations and rotations. Hence, \nthere is a need for methods to model and extract richer transformations that \ncapture much higher-level invariances. To that end, we introduce a tool \nallowing to parametrize the set of filters of a trained convolutional neural \nnetwork with the latent space of a generative adversarial network. We then show \nthat the method can capture highly non-linear invariances of the data by \nvisualizing their effect in the data space. \n</p>"}, "author": "Yannic Kilcher, Gary Becigneul, Thomas Hofmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408096", "id": "tag:google.com,2005:reader/item/0000000329c20b27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "TreeQN and ATreeC: Differentiable Tree Planning for Deep Reinforcement Learning. (arXiv:1710.11417v1 [cs.AI])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11417"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11417", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Combining deep model-free reinforcement learning with on-line planning is a \npromising approach to building on the successes of deep RL. On-line planning \nwith look-ahead trees has proven successful in environments where transition \nmodels are known a priori. However, in complex environments where transition \nmodels need to be learned from data, the deficiencies of learned models have \nlimited their utility for planning. To address these challenges, we propose \nTreeQN, a differentiable, recursive, tree-structured model that serves as a \ndrop-in replacement for any value function network in deep RL with discrete \nactions. TreeQN dynamically constructs a tree by recursively applying a \ntransition model in a learned abstract state space and then aggregating \npredicted rewards and state-values using a tree backup to estimate Q-values. We \nalso propose ATreeC, an actor-critic variant that augments TreeQN with a \nsoftmax layer to form a stochastic policy network. Both approaches are trained \nend-to-end, such that the learned model is optimised for its actual use in the \nplanner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a \nbox-pushing task, as well as n-step DQN and value prediction networks (Oh et \nal., 2017) on multiple Atari games, with deeper trees often outperforming \nshallower ones. We also present a qualitative analysis that sheds light on the \ntrees learned by TreeQN. \n</p>"}, "author": "Gregory Farquhar, Tim Rockt&#xe4;schel, Maximilian Igl, Shimon Whiteson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408095", "id": "tag:google.com,2005:reader/item/0000000329c20b2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Regret Minimization for Partially Observable Deep Reinforcement Learning. (arXiv:1710.11424v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11424"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11424", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8b929d9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8b929d9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep reinforcement learning algorithms that estimate state and state-action \nvalue functions have been shown to be effective in a variety of challenging \ndomains, including learning control strategies from raw image pixels. However, \nalgorithms that estimate state and state-action value functions typically \nassume a fully observed state and must compensate for partial or non-Markovian \nobservations by using finite-length frame-history observations or recurrent \nnetworks. In this work, we propose a new deep reinforcement learning algorithm \nbased on counterfactual regret minimization that iteratively updates an \napproximation to a cumulative clipped advantage function and is robust to \npartially observed state. We demonstrate that on several partially observed \nreinforcement learning tasks, this new class of algorithms can substantially \noutperform strong baseline methods: on Pong with single-frame observations, and \non the challenging Doom (ViZDoom) and Minecraft (Malm\\\"o) first-person \nnavigation benchmarks. \n</p>"}, "author": "Peter H. Jin, Sergey Levine, Kurt Keutzer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408094", "id": "tag:google.com,2005:reader/item/0000000329c20b34", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SemTK: An Ontology-first, Open Source Semantic Toolkit for Managing and Querying Knowledge Graphs. (arXiv:1710.11531v1 [cs.AI])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11531"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11531", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The relatively recent adoption of Knowledge Graphs as an enabling technology \nin multiple high-profile artificial intelligence and cognitive applications has \nled to growing interest in the Semantic Web technology stack. Many \nsemantics-related tools, however, are focused on serving experts with a deep \nunderstanding of semantic technologies. For example, triplification of \nrelational data is available but there is no open source tool that allows a \nuser unfamiliar with OWL/RDF to import data into a semantic triple store in an \nintuitive manner. Further, many tools require users to have a working \nunderstanding of SPARQL to query data. Casual users interested in benefiting \nfrom the power of Knowledge Graphs have few tools available for exploring, \nquerying, and managing semantic data. We present SemTK, the Semantics Toolkit, \na user-friendly suite of tools that allow both expert and non-expert semantics \nusers convenient ingestion of relational data, simplified query generation, and \nmore. The exploration of ontologies and instance data is performed through \nSPARQLgraph, an intuitive web-based user interface in SemTK understandable and \nnavigable by a lay user. The open source version of SemTK is available at \n<a href=\"http://semtk.research.ge.com.\">this http URL</a> \n</p>"}, "author": "Paul Cuddihy, Justin McHugh, Jenny Weisenberg Williams, Varish Mulwad, Kareem S. Aggour", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408093", "id": "tag:google.com,2005:reader/item/0000000329c20b3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Graph Convolution Filters from Data Manifold. (arXiv:1710.11577v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11577"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11577", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolution Neural Network (CNN) has gained tremendous success in computer \nvision tasks with its outstanding ability to capture the local latent features. \nRecently, there has been an increasing interest in extending CNNs to the \ngeneral spatial domain. Although various types of graph and geometric \nconvolution methods have been proposed, their connections to traditional \n2D-convolution are not well-understood. In this paper, we show that depthwise \nseparable convolution is the key to close the gap, based on which we derive a \nnovel Depthwise Separable Graph Convolution that subsumes existing graph \nconvolution methods as special cases of our formulation. Experiments show that \nthe proposed approach consistently outperforms other graph and geometric \nconvolution baselines on benchmark datasets in multiple domains. \n</p>"}, "author": "Guokun Lai, Hanxiao Liu, Yiming Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408092", "id": "tag:google.com,2005:reader/item/0000000329c20b43", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Whodunnit? Crime Drama as a Case for Natural Language Understanding. (arXiv:1710.11601v1 [cs.CL])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11601"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11601", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we argue that crime drama exemplified in television programs \nsuch as CSI:Crime Scene Investigation is an ideal testbed for approximating \nreal-world natural language understanding and the complex inferences associated \nwith it. We propose to treat crime drama as a new inference task, capitalizing \non the fact that each episode poses the same basic question (i.e., who \ncommitted the crime) and naturally provides the answer when the perpetrator is \nrevealed. We develop a new dataset based on CSI episodes, formalize perpetrator \nidentification as a sequence labeling problem, and develop an LSTM-based model \nwhich learns from multi-modal data. Experimental results show that an \nincremental inference strategy is key to making accurate guesses as well as \nlearning from representations fusing textual, visual, and acoustic input. \n</p>"}, "author": "Lea Frermann, Shay B. Cohen, Mirella Lapata", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509496760408", "timestampUsec": "1509496760408091", "id": "tag:google.com,2005:reader/item/0000000329c20b4f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm. (arXiv:1710.11622v1 [cs.LG])", "published": 1509496761, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11622"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11622", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning to learn is a powerful paradigm for enabling models to learn from \ndata more effectively and efficiently. A popular approach to meta-learning is \nto train a recurrent model to read in a training dataset as input and output \nthe parameters of a learned model, or output predictions for new test inputs. \nAlternatively, a more recent approach to meta-learning aims to acquire deep \nrepresentations that can be effectively fine-tuned, via standard gradient \ndescent, to new tasks. In this paper, we consider the meta-learning problem \nfrom the perspective of universality, formalizing the notion of learning \nalgorithm approximation and comparing the expressive power of the \naforementioned recurrent models to the more recent approaches that embed \ngradient descent into the meta-learner. In particular, we seek to answer the \nfollowing question: does deep representation combined with standard gradient \ndescent have sufficient capacity to approximate any learning algorithm? We find \nthat this is indeed true, and further find, in our experiments, that \ngradient-based meta-learning consistently leads to learning strategies that \ngeneralize more widely compared to those represented by recurrent models. \n</p>"}, "author": "Chelsea Finn, Sergey Levine", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582667", "id": "tag:google.com,2005:reader/item/0000000329abb9d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "One-shot and few-shot learning of word embeddings. (arXiv:1710.10280v1 [cs.CL])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10280"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10280", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Standard deep learning systems require thousands or millions of examples to \nlearn a concept, and cannot integrate new concepts easily. By contrast, humans \nhave an incredible ability to do one-shot or few-shot learning. For instance, \nfrom just hearing a word used in a sentence, humans can infer a great deal \nabout it, by leveraging what the syntax and semantics of the surrounding words \ntells us. Here, we draw inspiration from this to highlight a simple technique \nby which deep recurrent networks can similarly exploit their prior knowledge to \nlearn a useful representation for a new word from little data. This could make \nnatural language processing systems much more flexible, by allowing them to \nlearn continually from the new words they encounter. \n</p>"}, "author": "Andrew K. Lampinen, James L. McClelland", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582666", "id": "tag:google.com,2005:reader/item/0000000329abb9de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Probability Series Expansion Classifier that is Interpretable by Design. (arXiv:1710.10301v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10301"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10301", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This work presents a new classifier that is specifically designed to be fully \ninterpretable. This technique determines the probability of a class outcome, \nbased directly on probability assignments measured from the training data. The \naccuracy of the predicted probability can be improved by measuring more \nprobability estimates from the training data to create a series expansion that \nrefines the predicted probability. We use this work to classify four standard \ndatasets and achieve accuracies comparable to that of Random Forests. Because \nthis technique is interpretable by design, it is capable of determining the \ncombinations of features that contribute to a particular classification \nprobability for individual cases as well as the weightings of each of \ncombination of features. \n</p>"}, "author": "Sapan Agarwal, Corey M. Hudson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582665", "id": "tag:google.com,2005:reader/item/0000000329abb9e4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Self-Training Method for Semi-Supervised GANs. (arXiv:1710.10313v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10313"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10313", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Since the creation of Generative Adversarial Networks (GANs), much work has \nbeen done to improve their training stability, their generated image quality, \ntheir range of application but nearly none of them explored their self-training \npotential. Self-training has been used before the advent of deep learning in \norder to allow training on limited labelled training data and has shown \nimpressive results in semi-supervised learning. In this work, we combine these \ntwo ideas and make GANs self-trainable for semi-supervised learning tasks by \nexploiting their infinite data generation potential. Results show that using \neven the simplest form of self-training yields an improvement. We also show \nresults for a more complex self-training scheme that performs at least as well \nas the basic self-training scheme but with significantly less data \naugmentation. \n</p>"}, "author": "Alan Do-Omri, Dalei Wu, Xiaohua Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582664", "id": "tag:google.com,2005:reader/item/0000000329abb9ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Spectral Graph Wavelets for Structural Role Similarity in Networks. (arXiv:1710.10321v1 [cs.SI])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10321"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10321", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Nodes residing in different parts of a graph can have similar structural \nroles within their local network topology. The identification of such roles \nprovides key insight into the organization of networks and can also be used to \ninform machine learning on graphs. However, learning structural representations \nof nodes is a challenging unsupervised-learning task, which typically involves \nmanually specifying and tailoring topological features for each node. Here we \ndevelop GraphWave, a method that represents each node's local network \nneighborhood via a low-dimensional embedding by leveraging spectral graph \nwavelet diffusion patterns. We prove that nodes with similar local network \nneighborhoods will have similar GraphWave embeddings even though these nodes \nmay reside in very different parts of the network. Our method scales linearly \nwith the number of edges and does not require any hand-tailoring of topological \nfeatures. We evaluate performance on both synthetic and real-world datasets, \nobtaining improvements of up to 71% over state-of-the-art baselines. \n</p>"}, "author": "Claire Donnat, Marinka Zitnik, David Hallac, Jure Leskovec", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582663", "id": "tag:google.com,2005:reader/item/0000000329abb9f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Lower Bounds for Higher-Order Convex Optimization. (arXiv:1710.10329v1 [math.OC])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10329"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10329", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>State-of-the-art methods in convex and non-convex optimization employ \nhigher-order derivative information, either implicitly or explicitly. We \nexplore the limitations of higher-order optimization and prove that even for \nconvex optimization, a polynomial dependence on the approximation guarantee and \nhigher-order smoothness parameters is necessary. As a special case, we show \nNesterov's accelerated cubic regularization method to be nearly tight. \n</p>"}, "author": "Naman Agarwal, Elad Hazan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582662", "id": "tag:google.com,2005:reader/item/0000000329abb9fc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Similarity-based Multi-label Learning. (arXiv:1710.10335v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10335"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10335", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8b92d49\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8b92d49&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Multi-label classification is an important learning problem with many \napplications. In this work, we propose a principled similarity-based approach \nfor multi-label learning called SML. We also introduce a similarity-based \napproach for predicting the label set size. The experimental results \ndemonstrate the effectiveness of SML for multi-label classification where it is \nshown to compare favorably with a wide variety of existing algorithms across a \nrange of evaluation criterion. \n</p>"}, "author": "Ryan A. Rossi, Nesreen K. Ahmed, Hoda Eldardiry, Rong Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582661", "id": "tag:google.com,2005:reader/item/0000000329abba01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Implicit Bias of Gradient Descent on Separable Data. (arXiv:1710.10345v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10345"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10345", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8c34192\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8c34192&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We show that gradient descent on an unregularized logistic regression problem \nwith separable data converges to the max-margin solution. The result \ngeneralizes also to other monotone decreasing loss functions with an infimum at \ninfinity, and we also discuss a multi-class generalizations to the cross \nentropy loss. Furthermore, we show this convergence is very slow, and only \nlogarithmic in the convergence of the loss itself. This can help explain the \nbenefit of continuing to optimize the logistic or cross-entropy loss even after \nthe training error is zero and the training loss is extremely small, and, as we \nshow, even if the validation loss increases. Our methodology can also aid in \nunderstanding implicit regularization in more complex models and with other \noptimization methods. \n</p>"}, "author": "Daniel Soudry, Elad Hoffer, Nathan Srebro", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582660", "id": "tag:google.com,2005:reader/item/0000000329abba05", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-level Residual Networks from Dynamical Systems View. (arXiv:1710.10348v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10348"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10348", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep residual networks (ResNets) and their variants are widely used in many \ncomputer vision applications and natural language processing tasks. However, \nthe theoretical principles for designing and training ResNets are still not \nfully understood. Recently, several points of view have emerged to try to \ninterpret ResNet theoretically, such as unraveled view, unrolled iterative \nestimation and dynamical systems view. In this paper, we adopt the dynamical \nsystems point of view, and analyze the lesioning properties of ResNet both \ntheoretically and experimentally. Based on these analyses, we additionally \npropose a novel method for accelerating ResNet training. We apply the proposed \nmethod to train ResNets and Wide ResNets for three image classification \nbenchmarks, reducing training time by more than 40\\% with superior or on-par \naccuracy. \n</p>"}, "author": "Bo Chang, Lili Meng, Eldad Haber, Frederick Tung, David Begert", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582659", "id": "tag:google.com,2005:reader/item/0000000329abba08", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Automated Design using Neural Networks and Gradient Descent. (arXiv:1710.10352v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10352"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10352", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a novel method that makes use of deep neural networks and gradient \ndecent to perform automated design on complex real world engineering tasks. Our \napproach works by training a neural network to mimic the fitness function of a \ndesign optimization task and then, using the differential nature of the neural \nnetwork, perform gradient decent to maximize the fitness. We demonstrate this \nmethods effectiveness by designing an optimized heat sink and both 2D and 3D \nairfoils that maximize the lift drag ratio under steady state flow conditions. \nWe highlight that our method has two distinct benefits over other automated \ndesign approaches. First, evaluating the neural networks prediction of fitness \ncan be orders of magnitude faster then simulating the system of interest. \nSecond, using gradient decent allows the design space to be searched much more \nefficiently then other gradient free methods. These two strengths work together \nto overcome some of the current shortcomings of automated design. \n</p>"}, "author": "Oliver Hennigh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582658", "id": "tag:google.com,2005:reader/item/0000000329abba0d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Diff-DAC: Distributed Actor-Critic for Multitask Deep Reinforcement Learning. (arXiv:1710.10363v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10363"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10363", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a multiagent distributed actor-critic algorithm for multitask \nreinforcement learning (MRL), named \\textit{Diff-DAC}. The agents are \nconnected, forming a (possibly sparse) network. Each agent is assigned a task \nand has access to data from this local task only. During the learning process, \nthe agents are able to communicate some parameters to their neighbors. Since \nthe agents incorporate their neighbors' parameters into their own learning \nrules, the information is diffused across the network, and they can learn a \ncommon policy that generalizes well across all tasks. Diff-DAC is scalable \nsince the computational complexity and communication overhead per agent grow \nwith the number of neighbors, rather than with the total number of agents. \nMoreover, the algorithm is fully distributed in the sense that agents \nself-organize, with no need for coordinator node. Diff-DAC follows an \nactor-critic scheme where the value function and the policy are approximated \nwith deep neural networks, being able to learn expressive policies from raw \ndata. As a by-product of Diff-DAC's derivation from duality theory, we provide \nnovel insights into the standard actor-critic framework, showing that it is \nactually an instance of the dual ascent method to approximate the solution of a \nlinear program. Experiments illustrate the performance of the algorithm in the \ncart-pole, inverted pendulum, and swing-up cart-pole environments. \n</p>"}, "author": "Sergio Valcarcel Macua, Aleksi Tukiainen, Daniel Garc&#xed;a-Oca&#xf1;a Hern&#xe1;ndez, David Baldazo, Enrique Munoz de Cote, Santiago Zazo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582657", "id": "tag:google.com,2005:reader/item/0000000329abba12", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Minimax Rates and Efficient Algorithms for Noisy Sorting. (arXiv:1710.10388v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10388"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10388", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>There has been a recent surge of interest in studying permutation-based \nmodels for ranking from pairwise comparison data. Despite being structurally \nricher and more robust than parametric ranking models, permutation-based models \nare less well understood statistically and generally lack efficient learning \nalgorithms. In this work, we study a prototype of permutation-based ranking \nmodels, namely, the noisy sorting model. We establish the optimal rates of \nlearning the model under two sampling procedures. Furthermore, we provide a \nfast algorithm to achieve near-optimal rates if the observations are sampled \nindependently. Along the way, we discover properties of the symmetric group \nwhich are of theoretical interest. \n</p>"}, "author": "Cheng Mao, Jonathan Weed, Philippe Rigollet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582656", "id": "tag:google.com,2005:reader/item/0000000329abba1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Trainable back-propagated functional transfer matrices. (arXiv:1710.10403v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10403"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10403", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Connections between nodes of fully connected neural networks are usually \nrepresented by weight matrices. In this article, functional transfer matrices \nare introduced as alternatives to the weight matrices: Instead of using real \nweights, a functional transfer matrix uses real functions with trainable \nparameters to represent connections between nodes. Multiple functional transfer \nmatrices are then stacked together with bias vectors and activations to form \ndeep functional transfer neural networks. These neural networks can be trained \nwithin the framework of back-propagation, based on a revision of the delta \nrules and the error transmission rule for functional connections. In \nexperiments, it is demonstrated that the revised rules can be used to train a \nrange of functional connections: 20 different functions are applied to neural \nnetworks with up to 10 hidden layers, and most of them gain high test \naccuracies on the MNIST database. It is also demonstrated that a functional \ntransfer matrix with a memory function can roughly memorise a non-cyclical \nsequence of 400 digits. \n</p>"}, "author": "Cheng-Hao Cai, Yanyan Xu, Dengfeng Ke, Kaile Su, Jing Sun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582655", "id": "tag:google.com,2005:reader/item/0000000329abba27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficient Localized Inference for Large Graphical Models. (arXiv:1710.10404v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10404"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10404", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new localized inference algorithm for answering marginalization \nqueries in large graphical models with the correlation decay property. Given a \nquery variable and a large graphical model, we define a much smaller model in a \nlocal region around the query variable in the target model so that the marginal \ndistribution of the query variable can be accurately approximated. We introduce \ntwo approximation error bounds based on the Dobrushin's comparison theorem and \napply our bounds to derive a greedy expansion algorithm that efficiently guides \nthe selection of neighbor nodes for localized inference. We verify our \ntheoretical bounds on various datasets and demonstrate that our localized \ninference algorithm can provide fast and accurate approximation for large \ngraphical models. \n</p>"}, "author": "Jinglin Chen, Jian Peng, Qiang Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582654", "id": "tag:google.com,2005:reader/item/0000000329abba2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalized End-to-End Loss for Speaker Verification. (arXiv:1710.10467v1 [eess.AS])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10467"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10467", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a new loss function called generalized end-to-end \n(GE2E) loss, which makes the training of speaker verification models more \nefficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike \nTE2E, the GE2E loss function updates the network in a way that emphasizes \nexamples that are difficult to verify at each step of the training process. \nAdditionally, the GE2E loss does not require an initial stage of example \nselection. With these properties, the model with new loss function learns a \nbetter model, by decreasing EER by more than 10%, in shorter period of time, by \nreducing the training time by &gt;60%. We also introduce the MultiReader \ntechnique, which allow us do domain adaptation - training more accurate model \nthat supports multiple keywords (i.e. \"OK Google\" and \"Hey Google\") as well as \nmultiple dialects. \n</p>"}, "author": "Li Wan, Quan Wang, Alan Papir, Ignacio Lopez Moreno", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582653", "id": "tag:google.com,2005:reader/item/0000000329abba36", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Speaker Diarization with LSTM. (arXiv:1710.10468v4 [eess.AS] UPDATED)", "published": 1510882706, "updated": 1510882707, "canonical": [{"href": "http://arxiv.org/abs/1710.10468"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10468", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>For many years, i-vector based speaker embedding techniques were the dominant \napproach for speaker verification and speaker diarization applications. \nHowever, mirroring the rise of deep learning in various domains, neural network \nbased speaker embeddings, also known as d-vectors, have consistently \ndemonstrated superior speaker verification performance. In this paper, we build \non the success of d-vector based speaker verification systems to develop a new \nd-vector based approach to speaker diarization. Specifically, we combine \nLSTM-based d-vector audio embeddings with recent work in non-parametric \nclustering to obtain a state-of-the-art speaker diarization system. Our system \nis evaluated on three standard public datasets, suggesting that d-vector based \ndiarization systems offer significant advantages over traditional i-vector \nbased systems. We achieved a 12.0% diarization error rate on NIST SRE 2000 \nCALLHOME, while our model is trained with out-of-domain data from voice search \nlogs. \n</p>"}, "author": "Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mansfield, Ignacio Lopez Moreno", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582652", "id": "tag:google.com,2005:reader/item/0000000329abba3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Attention-Based Models for Text-Dependent Speaker Verification. (arXiv:1710.10470v1 [eess.AS])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10470"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10470", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8c3458b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8c3458b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Attention-based models have recently shown great performance on a range of \ntasks, such as speech recognition, machine translation, and image captioning \ndue to their ability to summarize relevant information that expands through the \nentire length of an input sequence. In this paper, we analyze the usage of \nattention mechanisms to the problem of sequence summarization in our end-to-end \ntext-dependent speaker recognition system. We explore different topologies and \ntheir variants of the attention layer, and compare different pooling methods on \nthe attention weights. Ultimately, we show that attention-based models can \nimproves the Equal Error Rate (EER) of our speaker verification system by \nrelatively 14% compared to our non-attention LSTM baseline model. \n</p>"}, "author": "F A Rezaur Rahman Chowdhury, Quan Wang, Ignacio Lopez Moreno, Li Wan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582651", "id": "tag:google.com,2005:reader/item/0000000329abba4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Crime incidents embedding using restricted Boltzmann machines. (arXiv:1710.10513v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10513"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10513", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a new approach for detecting related crime series, by unsupervised \nlearning of the latent feature embeddings from narratives of crime record via \nthe Gaussian-Bernoulli Restricted Boltzmann Machines (RBM). This is a \ndrastically different approach from prior work on crime analysis, which \ntypically considers only time and location and at most category information. \nAfter the embedding, related cases are closer to each other in the Euclidean \nfeature space, and the unrelated cases are far apart, which is a good property \ncan enable subsequent analysis such as detection and clustering of related \ncases. Experiments over several series of related crime incidents hand labeled \nby the Atlanta Police Department reveal the promise of our embedding methods. \n</p>"}, "author": "Shixiang Zhu, Yao Xie", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582650", "id": "tag:google.com,2005:reader/item/0000000329abba63", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interpretation of Neural Networks is Fragile. (arXiv:1710.10547v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10547"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10547", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In order for machine learning to be deployed and trusted in many \napplications, it is crucial to be able to reliably explain why the machine \nlearning algorithm makes certain predictions. For example, if an algorithm \nclassifies a given pathology image to be a malignant tumor, then the doctor may \nneed to know which parts of the image led the algorithm to this classification. \nHow to interpret black-box predictors is thus an important and active area of \nresearch. A fundamental question is: how much can we trust the interpretation \nitself? In this paper, we show that interpretation of deep learning predictions \nis extremely fragile in the following sense: two perceptively indistinguishable \ninputs with the same predicted label can be assigned very different \ninterpretations. We systematically characterize the fragility of several \nwidely-used feature-importance interpretation methods (saliency maps, relevance \npropagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that \neven small random perturbation can change the feature importance and new \nsystematic perturbations can lead to dramatically different interpretations \nwithout changing the label. We extend these results to show that \ninterpretations based on exemplars (e.g. influence functions) are similarly \nfragile. Our analysis of the geometry of the Hessian matrix gives insight on \nwhy fragility could be a fundamental challenge to the current interpretation \napproaches. \n</p>"}, "author": "Amirata Ghorbani, Abubakar Abid, James Zou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582649", "id": "tag:google.com,2005:reader/item/0000000329abba77", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Zeroth-order Optimization in High Dimensions. (arXiv:1710.10551v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10551"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10551", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of optimizing a high-dimensional convex function \nusing stochastic zeroth-order query oracles. Such problems arise naturally in a \nvariety of practical applications, including optimizing experimental or \nsimulation parameters with many variables. Under sparsity assumptions on the \ngradients or function values, we present a successive component/feature \nselection algorithm and a noisy mirror descent algorithm with Lasso gradient \nestimates and show that both algorithms have convergence rates depending only \nlogarithmically on the ambient problem dimension. Empirical results verify our \ntheoretical findings and suggest that our designed algorithms outperform \nclassical zeroth-order optimization methods in the high-dimensional setting. \n</p>"}, "author": "Yining Wang, Simon Du, Sivaraman Balakrishnan, Aarti Singh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582648", "id": "tag:google.com,2005:reader/item/0000000329abba8a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Training of Graph Convolutional Networks. (arXiv:1710.10568v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10568"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10568", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Graph convolutional networks (GCNs) are powerful deep neural networks for \ngraph-structured data. However, GCN computes nodes' representation recursively \nfrom their neighbors, making the receptive field size grow exponentially with \nthe number of layers. Previous attempts on reducing the receptive field size by \nsubsampling neighbors do not have any convergence guarantee, and their \nreceptive field size per node is still in the order of hundreds. In this paper, \nwe develop a preprocessing strategy and two control variate based algorithms to \nfurther reduce the receptive field size. Our algorithms are guaranteed to \nconverge to GCN's local optimum regardless of the neighbor sampling size. \nEmpirical results show that our algorithms have a similar convergence speed per \nepoch with the exact algorithm even using only two neighbors per node. The time \nconsumption of our algorithm on the Reddit dataset is only one fifth of \nprevious neighbor sampling algorithms. \n</p>"}, "author": "Jianfei Chen, Jun Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582647", "id": "tag:google.com,2005:reader/item/0000000329abba94", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Weight Initialization of Deep Neural Networks(DNNs) using Data Statistics. (arXiv:1710.10570v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10570"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10570", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks (DNNs) form the backbone of almost every \nstate-of-the-art technique in the fields such as computer vision, speech \nprocessing, and text analysis. The recent advances in computational technology \nhave made the use of DNNs more practical. Despite the overwhelming performances \nby DNN and the advances in computational technology, it is seen that very few \nresearchers try to train their models from the scratch. Training of DNNs still \nremains a difficult and tedious job. The main challenges that researchers face \nduring training of DNNs are the vanishing/exploding gradient problem and the \nhighly non-convex nature of the objective function which has up to million \nvariables. The approaches suggested in He and Xavier solve the vanishing \ngradient problem by providing a sophisticated initialization technique. These \napproaches have been quite effective and have achieved good results on standard \ndatasets, but these same approaches do not work very well on more practical \ndatasets. We think the reason for this is not making use of data statistics for \ninitializing the network weights. Optimizing such a high dimensional loss \nfunction requires careful initialization of network weights. In this work, we \npropose a data dependent initialization and analyze its performance against the \nstandard initialization techniques such as He and Xavier. We performed our \nexperiments on some practical datasets and the results show our algorithm's \nsuperior classification accuracy. \n</p>"}, "author": "Saiprasad Koturwar, Shabbir Merchant", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582646", "id": "tag:google.com,2005:reader/item/0000000329abba9d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Certifiable Distributional Robustness with Principled Adversarial Training. (arXiv:1710.10571v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10571"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10571", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks are vulnerable to adversarial examples and researchers have \nproposed many heuristic attack and defense mechanisms. We take the principled \nview of distributionally robust optimization, which guarantees performance \nunder adversarial input perturbations. By considering a Lagrangian penalty \nformulation of perturbation of the underlying data distribution in a \nWasserstein ball, we provide a training procedure that augments model parameter \nupdates with worst-case perturbations of training data. For smooth losses, our \nprocedure provably achieves moderate levels of robustness with little \ncomputational or statistical cost relative to empirical risk minimization. \nFurthermore, our statistical guarantees allow us to efficiently certify \nrobustness for the population loss. We match or outperform heuristic approaches \non supervised and reinforcement learning tasks. \n</p>"}, "author": "Aman Sinha, Hongseok Namkoong, John Duchi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582645", "id": "tag:google.com,2005:reader/item/0000000329abbaa7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Regularization approaches for support vector machines with applications to biomedical data. (arXiv:1710.10600v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10600"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10600", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The support vector machine (SVM) is a widely used machine learning tool for \nclassification based on statistical learning theory. Given a set of training \ndata, the SVM finds a hyperplane that separates two different classes of data \npoints by the largest distance. While the standard form of SVM uses L2-norm \nregularization, other regularization approaches are particularly attractive for \nbiomedical datasets where, for example, sparsity and interpretability of the \nclassifier's coefficient values are highly desired features. Therefore, in this \npaper we consider different types of regularization approaches for SVMs, and \nexplore them in both synthetic and real biomedical datasets. \n</p>"}, "author": "Daniel Lopez-Martinez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582644", "id": "tag:google.com,2005:reader/item/0000000329abbaad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Variational Continual Learning. (arXiv:1710.10628v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10628"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10628", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper develops variational continual learning (VCL), a simple but \ngeneral framework for continual learning that fuses online variational \ninference (VI) and recent advances in Monte Carlo VI for neural networks. The \nframework can successfully train both deep discriminative models and deep \ngenerative models in complex continual learning settings where existing tasks \nevolve over time and entirely new tasks emerge. Experimental results show that \nvariational continual learning outperforms state-of-the-art continual learning \nmethods on a variety of tasks, avoiding catastrophic forgetting in a fully \nautomatic way. \n</p>"}, "author": "Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, Richard E. Turner", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582643", "id": "tag:google.com,2005:reader/item/0000000329abbab4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dimensionality reduction methods for molecular simulations. (arXiv:1710.10629v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10629"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10629", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Molecular simulations produce very high-dimensional data-sets with millions \nof data points. As analysis methods are often unable to cope with so many \ndimensions, it is common to use dimensionality reduction and clustering methods \nto reach a reduced representation of the data. Yet these methods often fail to \ncapture the most important features necessary for the construction of a Markov \nmodel. Here we demonstrate the results of various dimensionality reduction \nmethods on two simulation data-sets, one of protein folding and another of \nprotein-ligand binding. The methods tested include a k-means clustering \nvariant, a non-linear auto encoder, principal component analysis and tICA. The \ndimension-reduced data is then used to estimate the implied timescales of the \nslowest process by a Markov state model analysis to assess the quality of the \nprojection. The projected dimensions learned from the data are visualized to \ndemonstrate which conformations the various methods choose to represent the \nmolecular process. \n</p>"}, "author": "Stefan Doerr, Igor Ariz, Matthew J. Harvey, Gianni De Fabritiis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582642", "id": "tag:google.com,2005:reader/item/0000000329abbabc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Consistency of Quick Shift. (arXiv:1710.10646v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10646"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10646", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8c3490c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8c3490c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Quick Shift is a popular mode-seeking and clustering algorithm. We present \nfinite sample statistical consistency guarantees for Quick Shift on mode and \ncluster recovery under mild distributional assumptions. We then apply our \nresults to construct a consistent modal regression algorithm. \n</p>"}, "author": "Heinrich Jiang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582641", "id": "tag:google.com,2005:reader/item/0000000329abbac2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "If it ain't broke, don't fix it: Sparse metric repair. (arXiv:1710.10655v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10655"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10655", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8cc52ad\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8cc52ad&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Many modern data-intensive computational problems either require, or benefit \nfrom distance or similarity data that adhere to a metric. The algorithms run \nfaster or have better performance guarantees. Unfortunately, in real \napplications, the data are messy and values are noisy. The distances between \nthe data points are far from satisfying a metric. Indeed, there are a number of \ndifferent algorithms for finding the closest set of distances to the given ones \nthat also satisfy a metric (sometimes with the extra condition of being \nEuclidean). These algorithms can have unintended consequences, they can change \na large number of the original data points, and alter many other features of \nthe data. \n</p> \n<p>The goal of sparse metric repair is to make as few changes as possible to the \noriginal data set or underlying distances so as to ensure the resulting \ndistances satisfy the properties of a metric. In other words, we seek to \nminimize the sparsity (or the $\\ell_0$ \"norm\") of the changes we make to the \ndistances subject to the new distances satisfying a metric. We give three \ndifferent combinatorial algorithms to repair a metric sparsely. In one setting \nthe algorithm is guaranteed to return the sparsest solution and in the other \nsettings, the algorithms repair the metric. Without prior information, the \nalgorithms run in time proportional to the cube of the number of input data \npoints and, with prior information we can reduce the running time considerably. \n</p>"}, "author": "Anna C. Gilbert, Lalit Jain", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582640", "id": "tag:google.com,2005:reader/item/0000000329abbacb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Regularization for Deep Learning: A Taxonomy. (arXiv:1710.10686v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10686"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10686", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Regularization is one of the crucial ingredients of deep learning, yet the \nterm regularization has various definitions, and regularization methods are \noften studied separately from each other. In our work we present a systematic, \nunifying taxonomy to categorize existing methods. We distinguish methods that \naffect data, network architectures, error terms, regularization terms, and \noptimization procedures. We do not provide all details about the listed \nmethods; instead, we present an overview of how the methods can be sorted into \nmeaningful categories and sub-categories. This helps revealing links and \nfundamental similarities between them. Finally, we include practical \nrecommendations both for users and for developers of new regularization \nmethods. \n</p>"}, "author": "Jan Kuka&#x10d;ka, Vladimir Golkov, Daniel Cremers", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582639", "id": "tag:google.com,2005:reader/item/0000000329abbad3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Training Probabilistic Spiking Neural Networks with First-to-spike Decoding. (arXiv:1710.10704v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10704"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10704", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at \nharnessing the energy efficiency of spike-domain processing by building on \ncomputing elements that operate on, and exchange, spikes. In this paper, the \nproblem of training a two-layer SNNs is studied for the purpose of \nclassification, under a Generalized Linear Model (GLM) probabilistic neural \nmodel that was previously considered within the computational neuroscience \nliterature. Conventional classification rules for SNNs operate offline based on \nthe number of output spikes at each output neuron. In contrast, a novel \ntraining method is proposed here for a first-to-spike decoding rule, whereby \nthe SNN can perform an early classification decision once spike firing is \ndetected at an output neuron. Numerical results bring insights into the optimal \nparameter selection for the GLM neuron and on the accuracy-complexity trade-off \nperformance of conventional and first-to-spike decoding. \n</p>"}, "author": "Alireza Bagheri, Osvaldo Simeone, Bipin Rajendran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582638", "id": "tag:google.com,2005:reader/item/0000000329abbae6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Globally Optimal Symbolic Regression. (arXiv:1710.10720v2 [stat.ML] UPDATED)", "published": 1510139306, "updated": 1510139311, "canonical": [{"href": "http://arxiv.org/abs/1710.10720"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10720", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this study we introduce a new technique for symbolic regression that \nguarantees global optimality. This is achieved by formulating a mixed integer \nnon-linear program (MINLP) whose solution is a symbolic mathematical expression \nof minimum complexity that explains the observations. We demonstrate our \napproach by rediscovering Kepler's law on planetary motion using exoplanet data \nand Galileo's pendulum periodicity equation using experimental data. \n</p>"}, "author": "Vernon Austel, Sanjeeb Dash, Oktay Gunluk, Lior Horesh, Leo Liberti, Giacomo Nannicini, Baruch Schieber", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582637", "id": "tag:google.com,2005:reader/item/0000000329abbaeb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Contextual Regression: An Accurate and Conveniently Interpretable Nonlinear Model for Mining Discovery from Scientific Data. (arXiv:1710.10728v1 [q-bio.QM])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10728"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10728", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning algorithms such as linear regression, SVM and neural network \nhave played an increasingly important role in the process of scientific \ndiscovery. However, none of them is both interpretable and accurate on \nnonlinear datasets. Here we present contextual regression, a method that joins \nthese two desirable properties together using a hybrid architecture of neural \nnetwork embedding and dot product layer. We demonstrate its high prediction \naccuracy and sensitivity through the task of predictive feature selection on a \nsimulated dataset and the application of predicting open chromatin sites in the \nhuman genome. On the simulated data, our method achieved high fidelity recovery \nof feature contributions under random noise levels up to 200%. On the open \nchromatin dataset, the application of our method not only outperformed the \nstate of the art method in terms of accuracy, but also unveiled two previously \nunfound open chromatin related histone marks. Our method can fill the blank of \naccurate and interpretable nonlinear modeling in scientific data mining tasks. \n</p>"}, "author": "Chengyu Liu, Wei Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582636", "id": "tag:google.com,2005:reader/item/0000000329abbaf8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Attacking the Madry Defense Model with $L_1$-based Adversarial Examples. (arXiv:1710.10733v2 [stat.ML] UPDATED)", "published": 1509496837, "updated": 1509496839, "canonical": [{"href": "http://arxiv.org/abs/1710.10733"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10733", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Madry Lab recently hosted a competition designed to test the robustness \nof their adversarially trained MNIST model. Attacks were constrained to perturb \neach pixel of the input image by a scaled maximal $L_\\infty$ distortion \n$\\epsilon$ = 0.3. This discourages the use of attacks which are not optimized \non the $L_\\infty$ distortion metric. Our experimental results demonstrate that \nby relaxing the $L_\\infty$ constraint of the competition, the elastic-net \nattack to deep neural networks (EAD) can generate transferable adversarial \nexamples which, despite their high average $L_\\infty$ distortion, have minimal \nvisual distortion. These results call into question the use of $L_\\infty$ as a \nsole measure for visual distortion, and further demonstrate the power of EAD at \ngenerating robust adversarial examples. \n</p>"}, "author": "Yash Sharma, Pin-Yu Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582635", "id": "tag:google.com,2005:reader/item/0000000329abbafe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Linearly convergent stochastic heavy ball method for minimizing generalization error. (arXiv:1710.10737v1 [math.OC])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10737"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10737", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work we establish the first linear convergence result for the \nstochastic heavy ball method. The method performs SGD steps with a fixed \nstepsize, amended by a heavy ball momentum term. In the analysis, we focus on \nminimizing the expected loss and not on finite-sum minimization, which is \ntypically a much harder problem. While in the analysis we constrain ourselves \nto quadratic loss, the overall objective is not necessarily strongly convex. \n</p>"}, "author": "Nicolas Loizou, Peter Richt&#xe1;rik", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582634", "id": "tag:google.com,2005:reader/item/0000000329abbb03", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning neural trans-dimensional random field language models with noise-contrastive estimation. (arXiv:1710.10739v1 [cs.CL])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10739"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10739", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Trans-dimensional random field language models (TRF LMs) where sentences are \nmodeled as a collection of random fields, have shown close performance with \nLSTM LMs in speech recognition and are computationally more efficient in \ninference. However, the training efficiency of neural TRF LMs is not \nsatisfactory, which limits the scalability of TRF LMs on large training corpus. \nIn this paper, several techniques on both model formulation and parameter \nestimation are proposed to improve the training efficiency and the performance \nof neural TRF LMs. First, TRFs are reformulated in the form of exponential \ntilting of a reference distribution. Second, noise-contrastive estimation (NCE) \nis introduced to jointly estimate the model parameters and normalization \nconstants. Third, we extend the neural TRF LMs by marrying the deep \nconvolutional neural network (CNN) and the bidirectional LSTM into the \npotential function to extract the deep hierarchical features and \nbidirectionally sequential features. Utilizing all the above techniques enables \nthe successful and efficient training of neural TRF LMs on a 40x larger \ntraining set with only 1/3 training time and further reduces the WER with \nrelative reduction of 4.7% on top of a strong LSTM LM baseline. \n</p>"}, "author": "Bin Wang, Zhijian Ou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582633", "id": "tag:google.com,2005:reader/item/0000000329abbb1c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Implicit Causal Models for Genome-wide Association Studies. (arXiv:1710.10742v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10742"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10742", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Progress in probabilistic generative models has accelerated, developing \nricher models with neural architectures, implicit densities, and with scalable \nalgorithms for their Bayesian inference. However, there has been limited \nprogress in models that capture causal relationships, for example, how \nindividual genetic factors cause major human diseases. In this work, we focus \non two challenges in particular: How do we build richer causal models, which \ncan capture highly nonlinear relationships and interactions between multiple \ncauses? How do we adjust for latent confounders, which are variables \ninfluencing both cause and effect and which prevent learning of causal \nrelationships? To address these challenges, we synthesize ideas from causality \nand modern probabilistic modeling. For the first, we describe implicit causal \nmodels, a class of causal models that leverages neural architectures with an \nimplicit density. For the second, we describe an implicit causal model that \nadjusts for confounders by sharing strength across examples. In experiments, we \nscale Bayesian inference on up to a billion genetic measurements. We achieve \nstate of the art accuracy for identifying causal factors: we significantly \noutperform existing genetics methods by an absolute difference of 15-45.3%. \n</p>"}, "author": "Dustin Tran, David M. Blei", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582632", "id": "tag:google.com,2005:reader/item/0000000329abbb27", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distance-based classifier by data transformation for high-dimension, strongly spiked eigenvalue models. (arXiv:1710.10768v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10768"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10768", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8cc552a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8cc552a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider classifiers for high-dimensional data under the strongly spiked \neigenvalue (SSE) model. We first show that high-dimensional data often have the \nSSE model. We consider a distance-based classifier using eigenstructures for \nthe SSE model. We apply the noise reduction methodology to estimation of the \neigenvalues and eigenvectors in the SSE model. We create a new distance-based \nclassifier by transforming data from the SSE model to the non-SSE model. We \ngive simulation studies and discuss the performance of the new classifier. \nFinally, we demonstrate the new classifier by using microarray data sets. \n</p>"}, "author": "Makoto Aoshima, Kazuyoshi Yata", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582631", "id": "tag:google.com,2005:reader/item/0000000329abbb2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Communication-Avoiding Optimization Methods for Massive-Scale Graphical Model Structure Learning. (arXiv:1710.10769v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10769"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10769", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Undirected graphical models compactly represent the structure of large, \nhigh-dimensional data sets, which are especially important in interpreting \ncomplex scientific data. Some data sets may run to multiple terabytes, and \ncurrent methods are intractable in both memory size and running time. We \nintroduce HP-CONCORD, a highly scalable optimization algorithm to estimate a \nsparse inverse covariance matrix based on a regularized pseudolikelihood \nframework. Our parallel proximal gradient method runs across a multi-node \ncluster and achieves parallel scalability using a novel communication-avoiding \nlinear algebra algorithm. We demonstrate scalability on problems with 1.28 \nmillion dimensions (over 800 billion parameters) and show that it can \noutperform a previous method on a single node and scales to 1K nodes (24K \ncores). We use HP-CONCORD to estimate the underlying conditional dependency \nstructure of the brain from fMRI data and use the result to automatically \nidentify functional regions. The results show good agreement with a \nstate-of-the-art clustering from the neuroscience literature. \n</p>"}, "author": "Penporn Koanantakool, Alnur Ali, Ariful Azad, Aydin Buluc, Dmitriy Morozov, Sang-Yun Oh, Leonid Oliker, Katherine Yelick", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582630", "id": "tag:google.com,2005:reader/item/0000000329abbb42", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tensorizing Generative Adversarial Nets. (arXiv:1710.10772v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10772"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10772", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative Adversarial Network (GAN) and its variants demonstrate \nstate-of-the-art performance in the class of generative models. To capture \nhigher dimensional distributions, the common learning procedure requires high \ncomputational complexity and large number of parameters. In this paper, we \npresent a new generative adversarial framework by representing each layer as a \ntensor structure connected by multilinear operations, aiming to reduce the \nnumber of model parameters by a large factor while preserving the quality of \ngeneralized performance. To learn the model, we develop an efficient algorithm \nby alternating optimization of the mode connections. Experimental results \ndemonstrate that our model can achieve high compression rate for model \nparameters up to 40 times as compared to the existing GAN. \n</p>"}, "author": "Xingwei Cao, Qibin Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582629", "id": "tag:google.com,2005:reader/item/0000000329abbb4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Transfer Learning to Learn with Multitask Neural Model Search. (arXiv:1710.10776v1 [cs.AI])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10776"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10776", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning models require extensive architecture design exploration and \nhyperparameter optimization to perform well on a given task. The exploration of \nthe model design space is often made by a human expert, and optimized using a \ncombination of grid search and search heuristics over a large space of possible \nchoices. Neural Architecture Search (NAS) is a Reinforcement Learning approach \nthat has been proposed to automate architecture design. NAS has been \nsuccessfully applied to generate Neural Networks that rival the best \nhuman-designed architectures. However, NAS requires sampling, constructing, and \ntraining hundreds to thousands of models to achieve well-performing \narchitectures. This procedure needs to be executed from scratch for each new \ntask. The application of NAS to a wide set of tasks currently lacks a way to \ntransfer generalizable knowledge across tasks. In this paper, we present the \nMultitask Neural Model Search (MNMS) controller. Our goal is to learn a \ngeneralizable framework that can condition model construction on successful \nmodel searches for previously seen tasks, thus significantly speeding up the \nsearch for new tasks. We demonstrate that MNMS can conduct an automated \narchitecture search for multiple tasks simultaneously while still learning \nwell-performing, specialized models for each task. We then show that \npre-trained MNMS controllers can transfer learning to new tasks. By leveraging \nknowledge from previous searches, we find that pre-trained MNMS models start \nfrom a better location in the search space and reduce search time on unseen \ntasks, while still discovering models that outperform published human-designed \nmodels. \n</p>"}, "author": "Catherine Wong, Andrea Gesmundo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582628", "id": "tag:google.com,2005:reader/item/0000000329abbb52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generative Adversarial Source Separation. (arXiv:1710.10779v1 [cs.SD])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10779"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10779", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative source separation methods such as non-negative matrix \nfactorization (NMF) or auto-encoders, rely on the assumption of an output \nprobability density. Generative Adversarial Networks (GANs) can learn data \ndistributions without needing a parametric assumption on the output density. We \nshow on a speech source separation experiment that, a multi-layer perceptron \ntrained with a Wasserstein-GAN formulation outperforms NMF, auto-encoders \ntrained with maximum likelihood, and variational auto-encoders in terms of \nsource to distortion ratio. \n</p>"}, "author": "Cem Subakan, Paris Smaragdis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582627", "id": "tag:google.com,2005:reader/item/0000000329abbb58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic variance reduced multiplicative update for nonnegative matrix factorization. (arXiv:1710.10781v1 [cs.NA])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10781"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10781", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Nonnegative matrix factorization (NMF), a dimensionality reduction and factor \nanalysis method, is a special case in which factor matrices have low-rank \nnonnegative constraints. Considering the stochastic learning in NMF, we \nspecifically address the multiplicative update (MU) rule, which is the most \npopular, but which has slow convergence property. This present paper introduces \non the stochastic MU rule a variance-reduced technique of stochastic gradient. \nNumerical comparisons suggest that our proposed algorithms robustly outperform \nstate-of-the-art algorithms across different synthetic and real-world datasets. \n</p>"}, "author": "Hiroyuki Kasai", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582626", "id": "tag:google.com,2005:reader/item/0000000329abbb5e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "How deep learning works --The geometry of deep learning. (arXiv:1710.10784v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10784"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10784", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Why and how that deep learning works well on different tasks remains a \nmystery from a theoretical perspective. In this paper we draw a geometric \npicture of the deep learning system by finding its analogies with two existing \ngeometric structures, the geometry of quantum computations and the geometry of \nthe diffeomorphic template matching. In this framework, we give the geometric \nstructures of different deep learning systems including convolutional neural \nnetworks, residual networks, recursive neural networks, recurrent neural \nnetworks and the equilibrium prapagation framework. We can also analysis the \nrelationship between the geometrical structures and their performance of \ndifferent networks in an algorithmic level so that the geometric framework may \nguide the design of the structures and algorithms of deep learning systems. \n</p>"}, "author": "Xiao Dong, Jiasong Wu, Ling Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582625", "id": "tag:google.com,2005:reader/item/0000000329abbb62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Understanding GANs: the LQG Setting. (arXiv:1710.10793v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10793"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10793", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative Adversarial Networks (GANs) have become a popular method to learn \na probability model from data. Many GAN architectures with different \noptimization metrics have been introduced recently. Instead of proposing yet \nanother architecture, this paper aims to provide an understanding of some of \nthe basic issues surrounding GANs. First, we propose a natural way of \nspecifying the loss function for GANs by drawing a connection with supervised \nlearning. Second, we shed light on the generalization peformance of GANs \nthrough the analysis of a simple LQG setting: the generator is Linear, the loss \nfunction is Quadratic and the data is drawn from a Gaussian distribution. We \nshow that in this setting: 1) the optimal GAN solution converges to population \nPrincipal Component Analysis (PCA) as the number of training samples increases; \n2) the number of samples required scales exponentially with the dimension of \nthe data; 3) the number of samples scales almost linearly if the discriminator \nis constrained to be quadratic. Thus, linear generators and quadratic \ndiscriminators provide a good balance for fast learning. \n</p>"}, "author": "Soheil Feizi, Changho Suh, Fei Xia, David Tse", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582624", "id": "tag:google.com,2005:reader/item/0000000329abbb68", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Hit Song Prediction for Pop Music by Siamese CNN with Ranking Loss. (arXiv:1710.10814v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10814"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10814", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A model for hit song prediction can be used in the pop music industry to \nidentify emerging trends and potential artists or songs before they are \nmarketed to the public. While most previous work formulates hit song prediction \nas a regression or classification problem, we present in this paper a \nconvolutional neural network (CNN) model that treats it as a ranking problem. \nSpecifically, we use a commercial dataset with daily play-counts to train a \nmulti-objective Siamese CNN model with Euclidean loss and pairwise ranking loss \nto learn from audio the relative ranking relations among songs. Besides, we \ndevise a number of pair sampling methods according to some empirical \nobservation of the data. Our experiment shows that the proposed model with a \nsampling method called A/B sampling leads to much higher accuracy in hit song \nprediction than the baseline regression model. Moreover, we can further improve \nthe accuracy by using a neural attention mechanism to extract the highlights of \nsongs and by using a separate CNN model to offer high-level features of songs. \n</p>"}, "author": "Lang-Chi Yu, Yi-Hsuan Yang, Yun-Ning Hung, Yi-An Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582623", "id": "tag:google.com,2005:reader/item/0000000329abbb6c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Rough extreme learning machine: a new classification method based on uncertainty measure. (arXiv:1710.10824v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10824"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10824", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Extreme learning machine (ELM) is a new single hidden layer feedback neural \nnetwork. The weights of the input layer and the biases of neurons in hidden \nlayer are randomly generated, the weights of the output layer can be \nanalytically determined. ELM has been achieved good results for a large number \nof classification tasks. In this paper, a new extreme learning machine called \nrough extreme learning machine (RELM) was proposed. RELM uses rough set to \ndivide data into upper approximation set and lower approximation set, and the \ntwo approximation sets are utilized to train upper approximation neurons and \nlower approximation neurons. In addition, an attribute reduction is executed in \nthis algorithm to remove redundant attributes. The experimental results showed, \ncomparing with the comparison algorithms, RELM can get a better accuracy and \nrepeatability in most cases, RELM can not only maintain the advantages of fast \nspeed, but also effectively cope with the classification task for \nhigh-dimensional data. \n</p>"}, "author": "Shuliang Xu, Lin Feng, Feilong Wang, Shenglan Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582622", "id": "tag:google.com,2005:reader/item/0000000329abbb75", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Unifying Value Iteration, Advantage Learning, and Dynamic Policy Programming. (arXiv:1710.10866v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10866"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10866", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8cc5768\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8cc5768&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Approximate dynamic programming algorithms, such as approximate value \niteration, have been successfully applied to many complex reinforcement \nlearning tasks, and a better approximate dynamic programming algorithm is \nexpected to further extend the applicability of reinforcement learning to \nvarious tasks. In this paper we propose a new, robust dynamic programming \nalgorithm that unifies value iteration, advantage learning, and dynamic policy \nprogramming. We call it generalized value iteration (GVI) and its approximated \nversion, approximate GVI (AGVI). We show AGVI's performance guarantee, which \nincludes performance guarantees for existing algorithms, as special cases. We \ndiscuss theoretical weaknesses of existing algorithms, and explain the \nadvantages of AGVI. Numerical experiments in a simple environment support \ntheoretical arguments, and suggest that AGVI is a promising alternative to \nprevious algorithms. \n</p>"}, "author": "Tadashi Kozuno, Eiji Uchibe, Kenji Doya", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582621", "id": "tag:google.com,2005:reader/item/0000000329abbb81", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Fast Linear Model for Knowledge Graph Embeddings. (arXiv:1710.10881v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10881"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10881", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8d6c5de\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8d6c5de&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper shows that a simple baseline based on a Bag-of-Words (BoW) \nrepresentation learns surprisingly good knowledge graph embeddings. By casting \nknowledge base completion and question answering as supervised classification \nproblems, we observe that modeling co-occurences of entities and relations \nleads to state-of-the-art performance with a training time of a few minutes \nusing the open sourced library fastText. \n</p>"}, "author": "Armand Joulin, Edouard Grave, Piotr Bojanowski, Maximilian Nickel, Tomas Mikolov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582620", "id": "tag:google.com,2005:reader/item/0000000329abbb9f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Graph Attention Networks. (arXiv:1710.10903v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10903"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10903", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present graph attention networks (GATs), novel neural network \narchitectures that operate on graph-structured data, leveraging masked \nself-attentional layers to address the shortcomings of prior methods based on \ngraph convolutions or their approximations. By stacking layers in which nodes \nare able to attend over their neighborhoods' features, we enable (implicitly) \nspecifying different weights to different nodes in a neighborhood, without \nrequiring any kind of costly matrix operation (such as inversion) or depending \non knowing the graph structure upfront. In this way, we address several key \nchallenges of spectral-based graph neural networks simultaneously, and make our \nmodel readily applicable to inductive as well as transductive problems. Our GAT \nmodels have achieved state-of-the-art results across three established \ntransductive and inductive graph benchmarks: the Cora and Citeseer citation \nnetwork datasets, as well as a protein-protein interaction dataset (wherein \ntest graphs are entirely unseen during training). \n</p>"}, "author": "Petar Veli&#x10d;kovi&#x107;, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li&#xf2;, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582619", "id": "tag:google.com,2005:reader/item/0000000329abbbb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks. (arXiv:1710.10916v1 [cs.CV])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10916"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10916", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Although Generative Adversarial Networks (GANs) have shown remarkable success \nin various tasks, they still face challenges in generating high quality images. \nIn this paper, we propose Stacked Generative Adversarial Networks (StackGAN) \naimed at generating high-resolution photorealistic images. First, we propose a \ntwo-stage generative adversarial network architecture, StackGAN-v1, for \ntext-to-image synthesis. The Stage-I GAN sketches primitive shape and colors of \nthe object based on given text description, yielding low-resolution images. The \nStage-II GAN takes Stage-I results and text descriptions as inputs, and \ngenerates high-resolution images with photo-realistic details. Second, an \nadvanced multi-stage generative adversarial network architecture, StackGAN-v2, \nis proposed for both conditional and unconditional generative tasks. Our \nStackGAN-v2 consists of multiple generators and discriminators in a tree-like \nstructure; images at multiple scales corresponding to the same scene are \ngenerated from different branches of the tree. StackGAN-v2 shows more stable \ntraining behavior than StackGAN-v1 by jointly approximating multiple \ndistributions. Extensive experiments demonstrate that the proposed stacked \ngenerative adversarial networks significantly outperform other state-of-the-art \nmethods in generating photo-realistic images. \n</p>"}, "author": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582618", "id": "tag:google.com,2005:reader/item/0000000329abbbca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal Kernel-Based Dynamic Mode Decomposition. (arXiv:1710.10919v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10919"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10919", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The state-of-the-art algorithm known as kernel-based dynamic mode \ndecomposition (K-DMD) provides a sub-optimal solution to the problem of reduced \nmodeling of a dynamical system based on a finite approximation of the Koopman \noperator. It relies on crude approximations and on restrictive assumptions. The \npurpose of this work is to propose a kernel-based algorithm solving exactly \nthis low-rank approximation problem in a general setting. \n</p>"}, "author": "Patrick H&#xe9;as, C&#xe9;dric Herzet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582617", "id": "tag:google.com,2005:reader/item/0000000329abbbda", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The loss surface and expressivity of deep convolutional neural networks. (arXiv:1710.10928v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10928"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10928", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We analyze the expressiveness and loss surface of practical deep \nconvolutional neural networks (CNNs) with shared weights and max pooling \nlayers. We show that such CNNs produce linearly independent features at a \n\"wide\" layer which has more neurons than the number of training samples. This \ncondition holds e.g. for the VGG network. Furthermore, we provide for such wide \nCNNs necessary and sufficient conditions for global minima with zero training \nerror. For the case where the wide layer is followed by a fully connected \nlayer, we show that almost every critical point of the empirical loss is a \nglobal minimum with zero training error. Our analysis suggests that both depth \nand width are very important in deep learning. While depth brings more \nrepresentational power and allows the network to learn high level features, \nwidth smoothes the optimization landscape of the loss function in the sense \nthat a sufficiently wide network has a well-behaved loss surface with \npotentially no bad local minima. \n</p>"}, "author": "Quynh Nguyen, Matthias Hein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582616", "id": "tag:google.com,2005:reader/item/0000000329abbbf2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SGDLibrary: A MATLAB library for stochastic gradient descent algorithms. (arXiv:1710.10951v1 [cs.MS])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10951"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10951", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of finding the minimizer of a function $f: \n\\mathbb{R}^d \\rightarrow \\mathbb{R}$ of the form $\\min f(w) = \n\\frac{1}{n}\\sum_{i}f_i({w})$. This problem has been studied intensively in \nrecent years in machine learning research field. One typical but promising \napproach for large-scale data is stochastic optimization algorithm. SGDLibrary \nis a flexible, extensible and efficient pure-Matlab library of a collection of \nstochastic optimization algorithms. The purpose of the library is to provide \nresearchers and implementers a comprehensive evaluation environment of those \nalgorithms on various machine learning problems. \n</p>"}, "author": "Hiroyuki Kasai", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582615", "id": "tag:google.com,2005:reader/item/0000000329abbc01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Denoising random forests. (arXiv:1710.11004v1 [cs.CV])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11004"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11004", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a novel type of random forests called a denoising random \nforests that are robust against noises contained in test samples. Such \nnoise-corrupted samples cause serious damage to the estimation performances of \nrandom forests, since unexpected child nodes are often selected and the leaf \nnodes that the input sample reaches are sometimes far from those for a clean \nsample. Our main idea for tackling this problem originates from a binary \nindicator vector that encodes a traversal path of a sample in the forest. Our \nproposed method effectively employs this vector by introducing denoising \nautoencoders into random forests. A denoising autoencoder can be trained with \nindicator vectors produced from clean and noisy input samples, and non-leaf \nnodes where incorrect decisions are made can be identified by comparing the \ninput and output of the trained denoising autoencoder. Multiple traversal paths \nwith respect to the nodes with incorrect decisions caused by the noises can \nthen be considered for the estimation. \n</p>"}, "author": "Masaya Hibino, Akisato Kimura, Takayoshi Yamashita, Yuji Yamauchi, Hironobu Fujiyoshi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582614", "id": "tag:google.com,2005:reader/item/0000000329abbc0f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. (arXiv:1710.11029v1 [cs.LG])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11029"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11029", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stochastic gradient descent (SGD) is widely believed to perform implicit \nregularization when used to train deep neural networks, but the precise manner \nin which this occurs has thus far been elusive. We prove that SGD minimizes an \naverage potential over the posterior distribution of weights along with an \nentropic regularization term. This potential is however not the original loss \nfunction in general. So SGD does perform variational inference, but for a \ndifferent loss than the one used to compute the gradients. Even more \nsurprisingly, SGD does not even converge in the classical sense: we show that \nthe most likely trajectories of SGD for deep networks do not behave like \nBrownian motion around critical points. Instead, they resemble closed loops \nwith deterministic components. We prove that such \"out-of-equilibrium\" behavior \nis a consequence of the fact that the gradient noise in SGD is highly \nnon-isotropic; the covariance matrix of mini-batch gradients has a rank as \nsmall as 1% of its dimension. We provide extensive empirical validation of \nthese claims, proven in the appendix. \n</p>"}, "author": "Pratik Chaudhari, Stefano Soatto", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582613", "id": "tag:google.com,2005:reader/item/0000000329abbc1e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Connection between Feed-Forward Neural Networks and Probabilistic Graphical Models. (arXiv:1710.11052v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11052"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11052", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Two of the most popular modelling paradigms in computer vision are \nfeed-forward neural networks (FFNs) and probabilistic graphical models (GMs). \nVarious connections between the two have been studied in recent works, such as \ne.g. expressing mean-field based inference in a GM as an FFN. This paper \nestablishes a new connection between FFNs and GMs. Our key observation is that \nany FFN implements a certain approximation of a corresponding Bayesian network \n(BN). We characterize various benefits of having this connection. In \nparticular, it results in a new learning algorithm for BNs. We validate the \nproposed methods for a classification problem on CIFAR-10 dataset and for \nbinary image segmentation on Weizmann Horse dataset. We show that statistically \nlearned BNs improve performance, having at the same time essentially better \ngeneralization capability, than their FFN counterparts. \n</p>"}, "author": "Dmitrij Schlesinger", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509484038583", "timestampUsec": "1509484038582612", "id": "tag:google.com,2005:reader/item/0000000329abbc2a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convergence Rates of Latent Topic Models Under Relaxed Identifiability Conditions. (arXiv:1710.11070v1 [stat.ML])", "published": 1509484038, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11070"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11070", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8d6ca5b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8d6ca5b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper we study the frequentist convergence rate for the Latent \nDirichlet Allocation (Blei et al., 2003) topic models. We show that the maximum \nlikelihood estimator converges to one of the finitely many equivalent \nparameters in Wasserstein's distance metric at a rate of $n^{-1/4}$ without \nassuming separability or non-degeneracy of the underlying topics and/or the \nexistence of more than three words per document, thus generalizing the previous \nworks of Anandkumar et al. (2012, 2014) from an information-theoretical \nperspective. We also show that the $n^{-1/4}$ convergence rate is optimal in \nthe worst case. \n</p>"}, "author": "Yining Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852421", "id": "tag:google.com,2005:reader/item/00000003293f86e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Similarity-based Multi-label Learning. (arXiv:1710.10335v1 [stat.ML])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10335"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10335", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multi-label classification is an important learning problem with many \napplications. In this work, we propose a principled similarity-based approach \nfor multi-label learning called SML. We also introduce a similarity-based \napproach for predicting the label set size. The experimental results \ndemonstrate the effectiveness of SML for multi-label classification where it is \nshown to compare favorably with a wide variety of existing algorithms across a \nrange of evaluation criterion. \n</p>"}, "author": "Ryan A. Rossi, Nesreen K. Ahmed, Hoda Eldardiry, Rong Zhou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852420", "id": "tag:google.com,2005:reader/item/00000003293f86ef", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Partitioning Relational Matrices of Similarities or Dissimilarities using the Value of Information. (arXiv:1710.10381v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10381"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10381", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we provide an approach to clustering relational matrices whose \nentries correspond to either similarities or dissimilarities between objects. \nOur approach is based on the value of information, a parameterized, \ninformation-theoretic criterion that measures the change in costs associated \nwith changes in information. Optimizing the value of information yields a \ndeterministic annealing style of clustering with many benefits. For instance, \ninvestigators avoid needing to a priori specify the number of clusters, as the \npartitions naturally undergo phase changes, during the annealing process, \nwhereby the number of clusters changes in a data-driven fashion. The \nglobal-best partition can also often be identified. \n</p>"}, "author": "Isaac J. Sledge, Jose C. Principe", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852419", "id": "tag:google.com,2005:reader/item/00000003293f86f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Left-Right Skip-DenseNets for Coarse-to-Fine Object Categorization. (arXiv:1710.10386v1 [cs.CV])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10386"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10386", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Inspired by the recent neuroscience studies on the left-right asymmetry of \nthe brains in the low and high spatial frequency processing, we introduce a \nnovel type of network -- the left-right skip-densenets for coarse-to-fine \nobject categorization. This network can enable both coarse and fine-grained \nclassification in a single framework. We also for the first time propose the \nlayer-skipping mechanism which learns a gating network to predict whether skip \nsome layers in the testing stage. This layer-skipping mechanism assigns more \nflexibility and capability to our network for the categorization tasks. Our \nnetwork is evaluated on three widely used datasets; the results show that our \nnetwork is more promising in solving the coarse-to-fine object categorization \nthan the competitors. \n</p>"}, "author": "Changmao Cheng, Yanwei Fu, Wenlian Lu, Yu-Gang Jiang, Jianfeng Feng, Xiangyang Xue", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852418", "id": "tag:google.com,2005:reader/item/00000003293f86fb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Ontology to support automated negotiation. (arXiv:1710.10433v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10433"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10433", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work we propose an ontology to support automated negotiation in \nmultiagent systems. The ontology can be connected with some domain-specific \nontologies to facilitate the negotiation in different domains, such as \nIntelligent Transportation Systems (ITS), e-commerce, etc. The specific \nnegotiation rules for each type of negotiation strategy can also be defined as \npart of the ontology, reducing the amount of knowledge hardcoded in the agents \nand ensuring the interoperability. The expressiveness of the ontology was \nproved in a multiagent architecture for the automatic traffic light setting \napplication on ITS. \n</p>"}, "author": "Susel Fernandez, Takayuki Ito", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852417", "id": "tag:google.com,2005:reader/item/00000003293f8707", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Long-Distance Loop Closure Using General Object Landmarks. (arXiv:1710.10466v1 [cs.RO])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10466"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10466", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Visual localization under large changes in scale is an important capability \nin many robotic mapping applications, such as localizing at low altitudes in \nmaps built at high altitudes, or performing loop closure over long distances. \nExisting approaches, however, are robust only up to a 3x difference in scale \nbetween map and query images. We propose a novel combination of \ndeep-learning-based object features and hand-engineered point-features that \nyields improved robustness to scale change, perspective change, and image \nnoise. We conduct experiments in simulation and in real-world outdoor scenes \nexhibiting up to a 7x change in scale, and compare our approach against \nlocalization using state-of-the-art SIFT features. This technique is \ntraining-free and class-agnostic, and in principle can be deployed in any \nenvironment out-of-the-box. \n</p>"}, "author": "Andrew Holliday, Gregory Dudek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852416", "id": "tag:google.com,2005:reader/item/00000003293f870d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exploiting Points and Lines in Regression Forests for RGB-D Camera Relocalization. (arXiv:1710.10519v1 [cs.CV])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10519"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10519", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Camera relocalization plays a vital role in many robotics and computer vision \ntasks, such as global localization, recovery from tracking failure and loop \nclosure detection. Recent random forests based methods exploit randomly sampled \npixel comparison features to predict 3D world locations for 2D image locations \nto guide the camera pose optimization. However, these image features are only \nsampled randomly in the images, without considering the spatial structures or \ngeometric information, leading to large errors or failure cases with the \nexistence of poorly textured areas or in motion blur. Line segment features are \nmore robust in these environments. In this work, we propose to jointly exploit \npoints and lines within the framework of uncertainty driven regression forests. \nThe proposed approach is thoroughly evaluated on three publicly available \ndatasets against several strong state-of-the-art baselines in terms of several \ndifferent error metrics. Experimental results prove the efficacy of our method, \nshowing superior or on-par state-of-the-art performance. \n</p>"}, "author": "Lili Meng, Frederick Tung, James J. Little, Julien Valentin, Clarence W. de Silva", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852415", "id": "tag:google.com,2005:reader/item/00000003293f8716", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interpretable Apprenticship Learning with Temporal Logic Specifications. (arXiv:1710.10532v1 [cs.SY])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10532"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10532", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent work has addressed using formulas in linear temporal logic (LTL) as \nspecifications for agents planning in Markov Decision Processes (MDPs). We \nconsider the inverse problem: inferring an LTL specification from demonstrated \nbehavior trajectories in MDPs. We formulate this as a multiobjective \noptimization problem, and describe state-based (\"what actually happened\") and \naction-based (\"what the agent expected to happen\") objective functions based on \na notion of \"violation cost\". We demonstrate the efficacy of the approach by \nemploying genetic programming to solve this problem in two simple domains. \n</p>"}, "author": "Daniel Kasenberg, Matthias Scheutz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852414", "id": "tag:google.com,2005:reader/item/00000003293f8726", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Partial Knowledge In Embeddings. (arXiv:1710.10538v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10538"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10538", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Representing domain knowledge is crucial for any task. There has been a wide \nrange of techniques developed to represent this knowledge, from older logic \nbased approaches to the more recent deep learning based techniques (i.e. \nembeddings). In this paper, we discuss some of these methods, focusing on the \nrepresentational expressiveness tradeoffs that are often made. In particular, \nwe focus on the the ability of various techniques to encode `partial knowledge' \n- a key component of successful knowledge systems. We introduce and describe \nthe concepts of `ensembles of embeddings' and `aggregate embeddings' and \ndemonstrate how they allow for partial knowledge. \n</p>"}, "author": "Ramanathan V. Guha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852413", "id": "tag:google.com,2005:reader/item/00000003293f8730", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Regularization approaches for support vector machines with applications to biomedical data. (arXiv:1710.10600v1 [cs.LG])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10600"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10600", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The support vector machine (SVM) is a widely used machine learning tool for \nclassification based on statistical learning theory. Given a set of training \ndata, the SVM finds a hyperplane that separates two different classes of data \npoints by the largest distance. While the standard form of SVM uses L2-norm \nregularization, other regularization approaches are particularly attractive for \nbiomedical datasets where, for example, sparsity and interpretability of the \nclassifier's coefficient values are highly desired features. Therefore, in this \npaper we consider different types of regularization approaches for SVMs, and \nexplore them in both synthetic and real biomedical datasets. \n</p>"}, "author": "Daniel Lopez-Martinez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852412", "id": "tag:google.com,2005:reader/item/00000003293f8735", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided Diagnosis of Diabetic Retinopathy. (arXiv:1710.10675v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10675"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10675", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8d6ceb4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8d6ceb4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Objective: Radiomics-driven Computer Aided Diagnosis (CAD) has shown \nconsiderable promise in recent years as a potential tool for improving clinical \ndecision support in medical oncology, particularly those based around the \nconcept of Discovery Radiomics, where radiomic sequencers are discovered \nthrough the analysis of medical imaging data. One of the main limitations with \ncurrent CAD approaches is that it is very difficult to gain insight or \nrationale as to how decisions are made, thus limiting their utility to \nclinicians. Methods: In this study, we propose CLEAR-DR, a novel interpretable \nCAD system based on the notion of CLass-Enhanced Attentive Response Discovery \nRadiomics for the purpose of clinical decision support for diabetic \nretinopathy. Results: In addition to disease grading via the discovered deep \nradiomic sequencer, the CLEAR-DR system also produces a visual interpretation \nof the decision-making process to provide better insight and understanding into \nthe decision-making process of the system. Conclusion: We demonstrate the \neffectiveness and utility of the proposed CLEAR-DR system of enhancing the \ninterpretability of diagnostic grading results for the application of diabetic \nretinopathy grading. Significance: CLEAR-DR can act as a potential powerful \ntool to address the uninterpretability issue of current CAD systems, thus \nimproving their utility to clinicians. \n</p>"}, "author": "Devinder Kumar, Graham W. Taylor, Alexander Wong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852411", "id": "tag:google.com,2005:reader/item/00000003293f8745", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Regularization for Deep Learning: A Taxonomy. (arXiv:1710.10686v1 [cs.LG])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10686"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10686", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8e26640\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8e26640&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Regularization is one of the crucial ingredients of deep learning, yet the \nterm regularization has various definitions, and regularization methods are \noften studied separately from each other. In our work we present a systematic, \nunifying taxonomy to categorize existing methods. We distinguish methods that \naffect data, network architectures, error terms, regularization terms, and \noptimization procedures. We do not provide all details about the listed \nmethods; instead, we present an overview of how the methods can be sorted into \nmeaningful categories and sub-categories. This helps revealing links and \nfundamental similarities between them. Finally, we include practical \nrecommendations both for users and for developers of new regularization \nmethods. \n</p>"}, "author": "Jan Kuka&#x10d;ka, Vladimir Golkov, Daniel Cremers", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852410", "id": "tag:google.com,2005:reader/item/00000003293f874d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Training Probabilistic Spiking Neural Networks with First-to-spike Decoding. (arXiv:1710.10704v1 [stat.ML])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10704"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10704", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at \nharnessing the energy efficiency of spike-domain processing by building on \ncomputing elements that operate on, and exchange, spikes. In this paper, the \nproblem of training a two-layer SNNs is studied for the purpose of \nclassification, under a Generalized Linear Model (GLM) probabilistic neural \nmodel that was previously considered within the computational neuroscience \nliterature. Conventional classification rules for SNNs operate offline based on \nthe number of output spikes at each output neuron. In contrast, a novel \ntraining method is proposed here for a first-to-spike decoding rule, whereby \nthe SNN can perform an early classification decision once spike firing is \ndetected at an output neuron. Numerical results bring insights into the optimal \nparameter selection for the GLM neuron and on the accuracy-complexity trade-off \nperformance of conventional and first-to-spike decoding. \n</p>"}, "author": "Alireza Bagheri, Osvaldo Simeone, Bipin Rajendran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852409", "id": "tag:google.com,2005:reader/item/00000003293f8755", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Computational Social Choice and Computational Complexity: BFFs?. (arXiv:1710.10753v1 [cs.MA])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10753"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10753", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We discuss the connection between computational social choice (comsoc) and \ncomputational complexity. We stress the work so far on, and urge continued \nfocus on, two less-recognized aspects of this connection. Firstly, this is very \nmuch a two-way street: Everyone knows complexity classification is used in \ncomsoc, but we also highlight benefits to complexity that have arisen from its \nuse in comsoc. Secondly, more subtle, less-known complexity tools often can be \nvery productively used in comsoc. \n</p>"}, "author": "Lane A. Hemaspaandra", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852408", "id": "tag:google.com,2005:reader/item/00000003293f8759", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tensorizing Generative Adversarial Nets. (arXiv:1710.10772v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10772"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10772", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative Adversarial Network (GAN) and its variants demonstrate \nstate-of-the-art performance in the class of generative models. To capture \nhigher dimensional distributions, the common learning procedure requires high \ncomputational complexity and large number of parameters. In this paper, we \npresent a new generative adversarial framework by representing each layer as a \ntensor structure connected by multilinear operations, aiming to reduce the \nnumber of model parameters by a large factor while preserving the quality of \ngeneralized performance. To learn the model, we develop an efficient algorithm \nby alternating optimization of the mode connections. Experimental results \ndemonstrate that our model can achieve high compression rate for model \nparameters up to 40 times as compared to the existing GAN. \n</p>"}, "author": "Xingwei Cao, Qibin Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852407", "id": "tag:google.com,2005:reader/item/00000003293f875c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Transfer Learning to Learn with Multitask Neural Model Search. (arXiv:1710.10776v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10776"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10776", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning models require extensive architecture design exploration and \nhyperparameter optimization to perform well on a given task. The exploration of \nthe model design space is often made by a human expert, and optimized using a \ncombination of grid search and search heuristics over a large space of possible \nchoices. Neural Architecture Search (NAS) is a Reinforcement Learning approach \nthat has been proposed to automate architecture design. NAS has been \nsuccessfully applied to generate Neural Networks that rival the best \nhuman-designed architectures. However, NAS requires sampling, constructing, and \ntraining hundreds to thousands of models to achieve well-performing \narchitectures. This procedure needs to be executed from scratch for each new \ntask. The application of NAS to a wide set of tasks currently lacks a way to \ntransfer generalizable knowledge across tasks. In this paper, we present the \nMultitask Neural Model Search (MNMS) controller. Our goal is to learn a \ngeneralizable framework that can condition model construction on successful \nmodel searches for previously seen tasks, thus significantly speeding up the \nsearch for new tasks. We demonstrate that MNMS can conduct an automated \narchitecture search for multiple tasks simultaneously while still learning \nwell-performing, specialized models for each task. We then show that \npre-trained MNMS controllers can transfer learning to new tasks. By leveraging \nknowledge from previous searches, we find that pre-trained MNMS models start \nfrom a better location in the search space and reduce search time on unseen \ntasks, while still discovering models that outperform published human-designed \nmodels. \n</p>"}, "author": "Catherine Wong, Andrea Gesmundo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852406", "id": "tag:google.com,2005:reader/item/00000003293f875f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Understanding Hidden Memories of Recurrent Neural Networks. (arXiv:1710.10777v1 [cs.CL])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10777"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10777", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural networks (RNNs) have been successfully applied to various \nnatural language processing (NLP) tasks and achieved better results than \nconventional methods. However, the lack of understanding of the mechanisms \nbehind their effectiveness limits further improvements on their architectures. \nIn this paper, we present a visual analytics method for understanding and \ncomparing RNN models for NLP tasks. We propose a technique to explain the \nfunction of individual hidden state units based on their expected response to \ninput texts. We then co-cluster hidden state units and words based on the \nexpected response and visualize co-clustering results as memory chips and word \nclouds to provide more structured knowledge on RNNs' hidden states. We also \npropose a glyph-based sequence visualization based on aggregate information to \nanalyze the behavior of an RNN's hidden state at the sentence-level. The \nusability and effectiveness of our method are demonstrated through case studies \nand reviews from domain experts. \n</p>"}, "author": "Yao Ming, Shaozu Cao, Ruixiang Zhang, Zhen Li, Yuanzhe Chen, Yangqiu Song, Huamin Qu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852405", "id": "tag:google.com,2005:reader/item/00000003293f8768", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Rough extreme learning machine: a new classification method based on uncertainty measure. (arXiv:1710.10824v1 [cs.LG])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10824"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10824", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Extreme learning machine (ELM) is a new single hidden layer feedback neural \nnetwork. The weights of the input layer and the biases of neurons in hidden \nlayer are randomly generated, the weights of the output layer can be \nanalytically determined. ELM has been achieved good results for a large number \nof classification tasks. In this paper, a new extreme learning machine called \nrough extreme learning machine (RELM) was proposed. RELM uses rough set to \ndivide data into upper approximation set and lower approximation set, and the \ntwo approximation sets are utilized to train upper approximation neurons and \nlower approximation neurons. In addition, an attribute reduction is executed in \nthis algorithm to remove redundant attributes. The experimental results showed, \ncomparing with the comparison algorithms, RELM can get a better accuracy and \nrepeatability in most cases, RELM can not only maintain the advantages of fast \nspeed, but also effectively cope with the classification task for \nhigh-dimensional data. \n</p>"}, "author": "Shuliang Xu, Lin Feng, Feilong Wang, Shenglan Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852404", "id": "tag:google.com,2005:reader/item/00000003293f8771", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Graph Attention Networks. (arXiv:1710.10903v1 [stat.ML])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10903"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10903", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present graph attention networks (GATs), novel neural network \narchitectures that operate on graph-structured data, leveraging masked \nself-attentional layers to address the shortcomings of prior methods based on \ngraph convolutions or their approximations. By stacking layers in which nodes \nare able to attend over their neighborhoods' features, we enable (implicitly) \nspecifying different weights to different nodes in a neighborhood, without \nrequiring any kind of costly matrix operation (such as inversion) or depending \non knowing the graph structure upfront. In this way, we address several key \nchallenges of spectral-based graph neural networks simultaneously, and make our \nmodel readily applicable to inductive as well as transductive problems. Our GAT \nmodels have achieved state-of-the-art results across three established \ntransductive and inductive graph benchmarks: the Cora and Citeseer citation \nnetwork datasets, as well as a protein-protein interaction dataset (wherein \ntest graphs are entirely unseen during training). \n</p>"}, "author": "Petar Veli&#x10d;kovi&#x107;, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li&#xf2;, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852403", "id": "tag:google.com,2005:reader/item/00000003293f8779", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks. (arXiv:1710.10916v1 [cs.CV])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10916"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10916", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Although Generative Adversarial Networks (GANs) have shown remarkable success \nin various tasks, they still face challenges in generating high quality images. \nIn this paper, we propose Stacked Generative Adversarial Networks (StackGAN) \naimed at generating high-resolution photorealistic images. First, we propose a \ntwo-stage generative adversarial network architecture, StackGAN-v1, for \ntext-to-image synthesis. The Stage-I GAN sketches primitive shape and colors of \nthe object based on given text description, yielding low-resolution images. The \nStage-II GAN takes Stage-I results and text descriptions as inputs, and \ngenerates high-resolution images with photo-realistic details. Second, an \nadvanced multi-stage generative adversarial network architecture, StackGAN-v2, \nis proposed for both conditional and unconditional generative tasks. Our \nStackGAN-v2 consists of multiple generators and discriminators in a tree-like \nstructure; images at multiple scales corresponding to the same scene are \ngenerated from different branches of the tree. StackGAN-v2 shows more stable \ntraining behavior than StackGAN-v1 by jointly approximating multiple \ndistributions. Extensive experiments demonstrate that the proposed stacked \ngenerative adversarial networks significantly outperform other state-of-the-art \nmethods in generating photo-realistic images. \n</p>"}, "author": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852402", "id": "tag:google.com,2005:reader/item/00000003293f878a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The loss surface and expressivity of deep convolutional neural networks. (arXiv:1710.10928v1 [cs.LG])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10928"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10928", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8e26aa9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8e26aa9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We analyze the expressiveness and loss surface of practical deep \nconvolutional neural networks (CNNs) with shared weights and max pooling \nlayers. We show that such CNNs produce linearly independent features at a \n\"wide\" layer which has more neurons than the number of training samples. This \ncondition holds e.g. for the VGG network. Furthermore, we provide for such wide \nCNNs necessary and sufficient conditions for global minima with zero training \nerror. For the case where the wide layer is followed by a fully connected \nlayer, we show that almost every critical point of the empirical loss is a \nglobal minimum with zero training error. Our analysis suggests that both depth \nand width are very important in deep learning. While depth brings more \nrepresentational power and allows the network to learn high level features, \nwidth smoothes the optimization landscape of the loss function in the sense \nthat a sufficiently wide network has a well-behaved loss surface with \npotentially no bad local minima. \n</p>"}, "author": "Quynh Nguyen, Matthias Hein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852401", "id": "tag:google.com,2005:reader/item/00000003293f87a4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "A Supervised STDP-based Training Algorithm for Living Neural Networks. (arXiv:1710.10944v1 [cs.NE])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10944"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10944", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks have shown great potential in many applications like speech \nrecognition, drug discovery, image classification, and object detection. Neural \nnetwork models are inspired by biological neural networks, but they are \noptimized to perform machine learning tasks on digital computers. The proposed \nwork explores the possibilities of using living neural networks in vitro as the \nbasic computational elements for machine learning applications. A supervised \nSTDP-based learning algorithm is proposed in this work, which considers neuron \nengineering constrains. A 75% accuracy is achieved on the MNIST benchmark for \nhandwritten digit recognition. \n</p>"}, "author": "Yuan Zeng, Kevin Devincentis, Yao Xiao, Zubayer Ibne Ferdous, Xiaochen Guo, Zhiyuan Yan, Yevgeny Berdichevsky", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852400", "id": "tag:google.com,2005:reader/item/00000003293f87b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "How Should a Robot Assess Risk? Towards an Axiomatic Theory of Risk in Robotics. (arXiv:1710.11040v1 [cs.RO])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11040"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11040", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Endowing robots with the capability of assessing risk and making risk-aware \ndecisions is widely considered a key step toward ensuring safety for robots \noperating under uncertainty. But, how should a robot quantify risk? A natural \nand common approach is to consider the framework whereby costs are assigned to \nstochastic outcomes - an assignment captured by a cost random variable. \nQuantifying risk then corresponds to evaluating a risk metric, i.e., a mapping \nfrom the cost random variable to a real number. Yet, the question of what \nconstitutes a \"good\" risk metric has received little attention within the \nrobotics community. The goal of this paper is to explore and partially address \nthis question by advocating axioms that risk metrics in robotics applications \nshould satisfy in order to be employed as rational assessments of risk. We \ndiscuss general representation theorems that precisely characterize the class \nof metrics that satisfy these axioms (referred to as distortion risk metrics), \nand provide instantiations that can be used in applications. We further discuss \npitfalls of commonly used risk metrics in robotics, and discuss additional \nproperties that one must consider in sequential decision making tasks. Our hope \nis that the ideas presented here will lead to a foundational framework for \nquantifying risk (and hence safety) in robotics applications. \n</p>"}, "author": "Anirudha Majumdar, Marco Pavone", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852399", "id": "tag:google.com,2005:reader/item/00000003293f87c8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Unsupervised Neural Machine Translation. (arXiv:1710.11041v1 [cs.CL])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11041"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11041", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In spite of the recent success of neural machine translation (NMT) in \nstandard benchmarks, the lack of large parallel corpora poses a major practical \nproblem for many language pairs. There have been several proposals to alleviate \nthis issue with, for instance, triangulation and semi-supervised learning \ntechniques, but they still require a strong cross-lingual signal. In this work, \nwe completely remove the need of parallel data and propose a novel method to \ntrain an NMT system in a completely unsupervised manner, relying on nothing but \nmonolingual corpora. Our model builds upon the recent work on unsupervised \nembedding mappings, and consists of a slightly modified attentional \nencoder-decoder model that can be trained on monolingual corpora alone using a \ncombination of denoising and backtranslation. Despite the simplicity of the \napproach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014 \nFrench-to-English and German-to-English translation. The model can also profit \nfrom small parallel corpora, and attains 21.81 and 15.24 points when combined \nwith 100,000 parallel sentences, respectively. Our approach is a breakthrough \nin unsupervised NMT, and opens exciting opportunities for future research. \n</p>"}, "author": "Mikel Artetxe, Gorka Labaka, Eneko Agirre, Kyunghyun Cho", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852398", "id": "tag:google.com,2005:reader/item/00000003293f87f7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Semantic Code Repair using Neuro-Symbolic Transformation Networks. (arXiv:1710.11054v1 [cs.AI])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11054"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11054", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the problem of semantic code repair, which can be broadly defined as \nautomatically fixing non-syntactic bugs in source code. The majority of past \nwork in semantic code repair assumed access to unit tests against which \ncandidate repairs could be validated. In contrast, the goal here is to develop \na strong statistical model to accurately predict both bug locations and exact \nfixes without access to information about the intended correct behavior of the \nprogram. Achieving such a goal requires a robust contextual repair model, which \nwe train on a large corpus of real-world source code that has been augmented \nwith synthetically injected bugs. Our framework adopts a two-stage approach \nwhere first a large set of repair candidates are generated by rule-based \nprocessors, and then these candidates are scored by a statistical model using a \nnovel neural network architecture which we refer to as Share, Specialize, and \nCompete. Specifically, the architecture (1) generates a shared encoding of the \nsource code using an RNN over the abstract syntax tree, (2) scores each \ncandidate repair using specialized network modules, and (3) then normalizes \nthese scores together so they can compete against one another in comparable \nprobability space. We evaluate our model on a real-world test set gathered from \nGitHub containing four common categories of bugs. Our model is able to predict \nthe exact correct repair 41\\% of the time with a single guess, compared to 13\\% \naccuracy for an attentional sequence-to-sequence model. \n</p>"}, "author": "Jacob Devlin, Jonathan Uesato, Rishabh Singh, Pushmeet Kohli", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852397", "id": "tag:google.com,2005:reader/item/00000003293f880e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Eigenoption Discovery through the Deep Successor Representation. (arXiv:1710.11089v1 [cs.LG])", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.11089"}], "alternate": [{"href": "http://arxiv.org/abs/1710.11089", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Options in reinforcement learning allow agents to hierarchically decompose a \ntask into subtasks, having the potential to speed up learning and planning. \nHowever, autonomously learning effective sets of options is still a major \nchallenge in the field. In this paper we focus on the recently introduced idea \nof using representation learning methods to guide the option discovery process. \nSpecifically, we look at eigenoptions, options obtained from representations \nthat encode diffusive information flow in the environment. We extend the \nexisting algorithms for eigenoption discovery to settings with stochastic \ntransitions and in which handcrafted features are not available. We propose an \nalgorithm that discovers eigenoptions while learning non-linear state \nrepresentations from raw pixels. It exploits recent successes in the deep \nreinforcement learning literature and the equivalence between proto-value \nfunctions and the successor representation. We use traditional tabular domains \nto provide intuition about our approach and Atari 2600 games to demonstrate its \npotential. \n</p>"}, "author": "Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, Murray Campbell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852381", "id": "tag:google.com,2005:reader/item/00000003293f88b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Communicating Semantics: Reference by Description. (arXiv:1511.06341v4 [cs.CL] CROSS LISTED)", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1511.06341"}], "alternate": [{"href": "http://arxiv.org/abs/1511.06341", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Messages often refer to entities such as people, places and events. Correct \nidentification of the intended reference is an essential part of communication. \nLack of shared unique names often complicates entity reference. Shared \nknowledge can be used to construct uniquely identifying descriptive references \nfor entities with ambiguous names. We introduce a mathematical model for \n`Reference by Description', derive results on the conditions under which, with \nhigh probability, programs can construct unambiguous references to most \nentities in the domain of discourse and provide empirical validation of these \nresults. \n</p>"}, "author": "Ramanathan V Guha, Vineet Gupta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509437321852", "timestampUsec": "1509437321852380", "id": "tag:google.com,2005:reader/item/00000003293f88c1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Understanding Grounded Language Learning Agents. (arXiv:1710.09867v1 [cs.CL] CROSS LISTED)", "published": 1509437322, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09867"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09867", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural network-based systems can now learn to locate the referents of words \nand phrases in images, answer questions about visual scenes, and even execute \nsymbolic instructions as first-person actors in partially-observable worlds. To \nachieve this so-called grounded language learning, models must overcome certain \nwell-studied learning challenges that are also fundamental to infants learning \ntheir first words. While it is notable that models with no meaningful prior \nknowledge overcome these learning obstacles, AI researchers and practitioners \ncurrently lack a clear understanding of exactly how they do so. Here we address \nthis question as a way of achieving a clearer general understanding of grounded \nlanguage learning, both to inform future research and to improve confidence in \nmodel predictions. For maximum control and generality, we focus on a simple \nneural network-based language learning agent trained via policy-gradient \nmethods to interpret synthetic linguistic instructions in a simulated 3D world. \nWe apply experimental paradigms from developmental psychology to this agent, \nexploring the conditions under which established human biases and learning \neffects emerge. We further propose a novel way to visualise and analyse \nsemantic representation in grounded language learning agents that yields a \nplausible computational account of the observed effects. \n</p>"}, "author": "Felix Hill, Karl Moritz Hermann, Phil Blunsom, Stephen Clark", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200350", "id": "tag:google.com,2005:reader/item/000000032916de43", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA. (arXiv:1710.10296v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10296"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10296", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recently, FPGA has been increasingly applied to problems such as speech \nrecognition, machine learning, and cloud computation such as the Bing search \nengine used by Microsoft. This is due to FPGAs great parallel computation \ncapacity as well as low power consumption compared to general purpose \nprocessors. However, these applications mainly focus on large scale FPGA \nclusters which have an extreme processing power for executing massive matrix or \nconvolution operations but are unsuitable for portable or mobile applications. \nThis paper describes research on single-FPGA platform to explore the \napplications of FPGAs in these fields. In this project, we design a Deep \nRecurrent Neural Network (DRNN) Language Model (LM) and implement a hardware \naccelerator with AXI Stream interface on a PYNQ board which is equipped with a \nXILINX ZYNQ SOC XC7Z020 1CLG400C. The PYNQ has not only abundant programmable \nlogic resources but also a flexible embedded operation system, which makes it \nsuitable to be applied in the natural language processing field. We design the \nDRNN language model with Python and Theano, train the model on a CPU platform, \nand deploy the model on a PYNQ board to validate the model with Jupyter \nnotebook. Meanwhile, we design the hardware accelerator with Overlay, which is \na kind of hardware library on PYNQ, and verify the acceleration effect on the \nPYNQ board. Finally, we have found that the DRNN language model can be deployed \non the embedded system smoothly and the Overlay accelerator with AXI Stream \ninterface performs at 20 GOPS processing throughput, which constitutes a 70.5X \nand 2.75X speed up compared to the work in Ref.30 and Ref.31 respectively. \n</p>"}, "author": "Yufeng Hao, Steven Quigley", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200349", "id": "tag:google.com,2005:reader/item/000000032916de4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions. (arXiv:1710.10304v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10304"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10304", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep autoregressive models have shown state-of-the-art performance in density \nestimation for natural images on large-scale datasets such as ImageNet. \nHowever, such models require many thousands of gradient-based weight updates \nand unique image examples for training. Ideally, the models would rapidly learn \nvisual concepts from only a handful of examples, similar to the manner in which \nhumans learns across many vision tasks. In this paper, we show how 1) neural \nattention and 2) meta learning techniques can be used in combination with \nautoregressive models to enable effective few-shot density estimation. Our \nproposed modifications to PixelCNN result in state-of-the art few-shot density \nestimation on the Omniglot dataset. Furthermore, we visualize the learned \nattention policy and find that it learns intuitive algorithms for simple tasks \nsuch as image mirroring on ImageNet and handwriting on Omniglot without \nsupervision. Finally, we extend the model to natural images and demonstrate \nfew-shot image generation on the Stanford Online Products dataset. \n</p>"}, "author": "Scott Reed, Yutian Chen, Thomas Paine, A&#xe4;ron van den Oord, S. M. Ali Eslami, Danilo Rezende, Oriol Vinyals, Nando de Freitas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200348", "id": "tag:google.com,2005:reader/item/000000032916de52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convolutional Neural Networks Via Node-Varying Graph Filters. (arXiv:1710.10355v1 [cs.LG])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10355"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10355", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8e26f18\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8e26f18&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Convolutional neural networks (CNNs) are being applied to an increasing \nnumber of problems and fields due to their superior performance in \nclassification and regression tasks. Since two of the key operations that CNNs \nimplement are convolution and pooling, this type of networks is implicitly \ndesigned to act on data described by regular structures such as images. \nMotivated by the recent interest in processing signals defined in irregular \ndomains, we advocate a CNN architecture that operates on signals supported on \ngraphs. The proposed design replaces the classical convolution not with a \nnode-invariant graph filter (GF), which is the natural generalization of \nconvolution to graph domains, but with a node-varying GF. This filter extracts \ndifferent local features without increasing the output dimension of each layer \nand, as a result, bypasses the need for a pooling stage while involving only \nlocal operations. A second contribution is to replace the node-varying GF with \na hybrid node-varying GF, which is a new type of GF introduced in this paper. \nWhile the alternative architecture can still be run locally without requiring a \npooling stage, the number of trainable parameters is smaller and can be \nrendered independent of the data dimension. Tests are run on a synthetic source \nlocalization problem and on the 20NEWS dataset. \n</p>"}, "author": "Fernando Gama, Geert Leus, Antonio Garcia Marques, Alejandro Ribeiro", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200347", "id": "tag:google.com,2005:reader/item/000000032916de60", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning. (arXiv:1710.10380v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10380"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10380", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8edc23d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8edc23d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Context information plays an important role in human language understanding, \nand it is also useful for machines to learn vector representations of language. \nIn this paper, we explore an asymmetric encoder-decoder structure for \nunsupervised context-based sentence representation learning. As a result, we \nbuild an encoder-decoder architecture with an RNN encoder and a CNN decoder. We \nfurther combine a suite of effective designs to significantly improve model \nefficiency while also achieving better performance. Our model is trained on two \ndifferent large unlabeled corpora, and in both cases transferability is \nevaluated on a set of downstream language understanding tasks. We empirically \nshow that our model is simple and fast while producing rich sentence \nrepresentations that excel in downstream tasks. \n</p>"}, "author": "Shuai Tang, Hailin Jin, Chen Fang, Zhaowen Wang, Virginia R. de Sa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200346", "id": "tag:google.com,2005:reader/item/000000032916de62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficient Licence Plate Detection By Unique Edge Detection Algorithm and Smarter Interpretation Through IoT. (arXiv:1710.10418v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10418"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10418", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Vehicles play a vital role in modern day transportation systems. Number plate \nprovides a standard means of identification for any vehicle. To serve this \npurpose, automatic licence plate recognition system was developed. This \nconsisted of four major steps: Pre-processing of the obtained image, extraction \nof licence plate region, segmentation and character recognition. In earlier \nresearch, direct application of Sobel edge detection algorithm or applying \nthreshold were used as key steps to extract the licence plate region, which \ndoes not produce effective results when the captured image is subjected to the \nhigh intensity of light. The use of morphological operations causes deformity \nin the characters during segmentation. We propose a novel algorithm to tackle \nthe mentioned issues through a unique edge detection algorithm. It is also a \ntedious task to create and update the database of required vehicles frequently. \nThis problem is solved by the use of Internet of things(IOT) where an online \ndatabase can be created and updated from any module instantly. Also, through \nIoT, we connect all the cameras in a geographical area to one server to create \na universal eye which drastically increases the probability of tracing a \nvehicle over having manual database attached to each camera for identification \npurpose. \n</p>"}, "author": "Tejas K, Ashok Reddy K, Pradeep Reddy D, Rajesh Kumar M", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200345", "id": "tag:google.com,2005:reader/item/000000032916de6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sample-level CNN Architectures for Music Auto-tagging Using Raw Waveforms. (arXiv:1710.10451v1 [cs.SD])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10451"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10451", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent work has shown that the end-to-end approach using convolutional neural \nnetwork (CNN) is effective in various types of machine learning tasks. For \naudio signals, the approach takes raw waveforms as input using an 1-D \nconvolution layer. In this paper, we improve the 1-D CNN architecture for music \nauto-tagging by adopting building blocks from state-of-the-art image \nclassification models, ResNets and SENets, and adding multi-level feature \naggregation to it. We compare different combinations of the modules in building \nCNN architectures. The results show that they achieve significant improvements \nover previous state-of-the-art models on the MagnaTagATune dataset and \ncomparable results on Million Song Dataset. Furthermore, we analyze and \nvisualize our model to show how the 1-D CNN operates. \n</p>"}, "author": "Taejun Kim, Jongpil Lee, Juhan Nam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200344", "id": "tag:google.com,2005:reader/item/000000032916de78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Toward predictive machine learning for active vision. (arXiv:1710.10460v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10460"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10460", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop a comprehensive description of the active inference framework, as \nproposed by Friston (2010), under a machine-learning compliant perspective. \nStemming from a biological inspiration and the auto-encoding principles, a \nsketch of a cognitive architecture is proposed that should provide ways to \nimplement estimation-oriented control policies under a POMDP perspective. \nComputer simulations illustrate the effectiveness of the approach through a \nfoveated inspection the input data. The pros and cons of the control policy are \nreviewed in details, showing interesting promises in term of processing \ncompression, but also putative risks of a confirmation bias that may degrade \nthe recognition performance if the model is too optimistic about its own \npredictions. The presented formalism is fully compliant with the auto-encoding \nframework and would deserve further developments under variational encoding \narchitectures. \n</p>"}, "author": "Emmanuel Dauc&#xe9;", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200343", "id": "tag:google.com,2005:reader/item/000000032916de7d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided Diagnosis of Diabetic Retinopathy. (arXiv:1710.10675v1 [cs.AI])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10675"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10675", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Objective: Radiomics-driven Computer Aided Diagnosis (CAD) has shown \nconsiderable promise in recent years as a potential tool for improving clinical \ndecision support in medical oncology, particularly those based around the \nconcept of Discovery Radiomics, where radiomic sequencers are discovered \nthrough the analysis of medical imaging data. One of the main limitations with \ncurrent CAD approaches is that it is very difficult to gain insight or \nrationale as to how decisions are made, thus limiting their utility to \nclinicians. Methods: In this study, we propose CLEAR-DR, a novel interpretable \nCAD system based on the notion of CLass-Enhanced Attentive Response Discovery \nRadiomics for the purpose of clinical decision support for diabetic \nretinopathy. Results: In addition to disease grading via the discovered deep \nradiomic sequencer, the CLEAR-DR system also produces a visual interpretation \nof the decision-making process to provide better insight and understanding into \nthe decision-making process of the system. Conclusion: We demonstrate the \neffectiveness and utility of the proposed CLEAR-DR system of enhancing the \ninterpretability of diagnostic grading results for the application of diabetic \nretinopathy grading. Significance: CLEAR-DR can act as a potential powerful \ntool to address the uninterpretability issue of current CAD systems, thus \nimproving their utility to clinicians. \n</p>"}, "author": "Devinder Kumar, Graham W. Taylor, Alexander Wong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200342", "id": "tag:google.com,2005:reader/item/000000032916de83", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Regularization for Deep Learning: A Taxonomy. (arXiv:1710.10686v1 [cs.LG])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10686"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10686", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Regularization is one of the crucial ingredients of deep learning, yet the \nterm regularization has various definitions, and regularization methods are \noften studied separately from each other. In our work we present a systematic, \nunifying taxonomy to categorize existing methods. We distinguish methods that \naffect data, network architectures, error terms, regularization terms, and \noptimization procedures. We do not provide all details about the listed \nmethods; instead, we present an overview of how the methods can be sorted into \nmeaningful categories and sub-categories. This helps revealing links and \nfundamental similarities between them. Finally, we include practical \nrecommendations both for users and for developers of new regularization \nmethods. \n</p>"}, "author": "Jan Kuka&#x10d;ka, Vladimir Golkov, Daniel Cremers", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200341", "id": "tag:google.com,2005:reader/item/000000032916de87", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Training Probabilistic Spiking Neural Networks with First-to-spike Decoding. (arXiv:1710.10704v1 [stat.ML])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10704"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10704", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at \nharnessing the energy efficiency of spike-domain processing by building on \ncomputing elements that operate on, and exchange, spikes. In this paper, the \nproblem of training a two-layer SNNs is studied for the purpose of \nclassification, under a Generalized Linear Model (GLM) probabilistic neural \nmodel that was previously considered within the computational neuroscience \nliterature. Conventional classification rules for SNNs operate offline based on \nthe number of output spikes at each output neuron. In contrast, a novel \ntraining method is proposed here for a first-to-spike decoding rule, whereby \nthe SNN can perform an early classification decision once spike firing is \ndetected at an output neuron. Numerical results bring insights into the optimal \nparameter selection for the GLM neuron and on the accuracy-complexity trade-off \nperformance of conventional and first-to-spike decoding. \n</p>"}, "author": "Alireza Bagheri, Osvaldo Simeone, Bipin Rajendran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200340", "id": "tag:google.com,2005:reader/item/000000032916de8f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "BAS: Beetle Antennae Search Algorithm for Optimization Problems. (arXiv:1710.10724v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10724"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10724", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Meta-heuristic algorithms have become very popular because of powerful \nperformance on the optimization problem. A new algorithm called beetle antennae \nsearch algorithm (BAS) is proposed in the paper inspired by the searching \nbehavior of longhorn beetles. The BAS algorithm imitates the function of \nantennae and the random walking mechanism of beetles in nature, and then two \nmain steps of detecting and searching are implemented. Finally, the algorithm \nis benchmarked on 2 well-known test functions, in which the numerical results \nvalidate the efficacy of the proposed BAS algorithm. \n</p>"}, "author": "Xiangyuan Jiang, Shuai Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200339", "id": "tag:google.com,2005:reader/item/000000032916de96", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Evolving Deep Convolutional Neural Networks for Image Classification. (arXiv:1710.10741v1 [cs.NE])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10741"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10741", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Evolutionary computation methods have been successfully applied to neural \nnetworks since two decades ago, while those methods cannot scale well to the \nmodern deep neural networks due to the complicated architectures and large \nquantities of connection weights. In this paper, we propose a new method using \ngenetic algorithms for evolving the architectures and connection weight \ninitialization values of a deep convolutional neural network to address image \nclassification problems. In the proposed algorithm, an efficient \nvariable-length gene encoding strategy is designed to represent the different \nbuilding blocks and the unpredictable optimal depth in convolutional neural \nnetworks. In addition, a new representation scheme is developed for effectively \ninitializing connection weights of deep convolutional neural networks, which is \nexpected to avoid networks getting stuck into local minima which is typically a \nmajor issue in the backward gradient-based optimization. Furthermore, a novel \nfitness evaluation method is proposed to speed up the heuristic search with \nsubstantially less computational resource. The proposed algorithm is examined \nand compared with 22 existing algorithms on nine widely used image \nclassification tasks, including the state-of-the-art methods. The experimental \nresults demonstrate the remarkable superiority of the proposed algorithm over \nthe state-of-the-art algorithms in terms of classification error rate and the \nnumber of parameters (weights). \n</p>"}, "author": "Yanan Sun, Bing Xue, Mengjie Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200338", "id": "tag:google.com,2005:reader/item/000000032916de9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "How we can control the crack to propagate along the specified path feasibly?. (arXiv:1710.10748v2 [cs.NE] UPDATED)", "published": 1510784959, "updated": 1510784961, "canonical": [{"href": "http://arxiv.org/abs/1710.10748"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10748", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8edc660\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8edc660&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A controllable crack propagation (CCP) strategy is suggested. It is well \nknown that crack always leads the failure by crossing the critical domain in \nengineering structure. Therefore, the CCP method is proposed to control the \ncrack to propagate along the specified path, which is away from the critical \ndomain. To complete this strategy, two optimization methods are engaged. \nFirstly, a back propagation neural network (BPNN) assisted particle swarm \noptimization (PSO) is suggested. In this method, to improve the efficiency of \nCCP, the BPNN is used to build the metamodel instead of the forward evaluation. \nSecondly, the popular PSO is used. Considering the optimization iteration is a \ntime consuming process, an efficient reanalysis based extended finite element \nmethods (X-FEM) is used to substitute the complete X-FEM solver to calculate \nthe crack propagation path. Moreover, an adaptive subdomain partition strategy \nis suggested to improve the fitting accuracy between real crack and specified \npaths. Several typical numerical examples demonstrate that both optimization \nmethods can carry out the CCP. The selection of them should be determined by \nthe tradeoff between efficiency and accuracy. \n</p>"}, "author": "Zhenxing Cheng, Hu Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200337", "id": "tag:google.com,2005:reader/item/000000032916dea5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generative Adversarial Source Separation. (arXiv:1710.10779v1 [cs.SD])", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10779"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10779", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative source separation methods such as non-negative matrix \nfactorization (NMF) or auto-encoders, rely on the assumption of an output \nprobability density. Generative Adversarial Networks (GANs) can learn data \ndistributions without needing a parametric assumption on the output density. We \nshow on a speech source separation experiment that, a multi-layer perceptron \ntrained with a Wasserstein-GAN formulation outperforms NMF, auto-encoders \ntrained with maximum likelihood, and variational auto-encoders in terms of \nsource to distortion ratio. \n</p>"}, "author": "Cem Subakan, Paris Smaragdis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200336", "id": "tag:google.com,2005:reader/item/000000032916deb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Supervised STDP-based Training Algorithm for Living Neural Networks. (arXiv:1710.10944v2 [cs.NE] UPDATED)", "published": 1510063179, "updated": 1510063179, "canonical": [{"href": "http://arxiv.org/abs/1710.10944"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10944", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks have shown great potential in many applications like speech \nrecognition, drug discovery, image classification, and object detection. Neural \nnetwork models are inspired by biological neural networks, but they are \noptimized to perform machine learning tasks on digital computers. The proposed \nwork explores the possibilities of using living neural networks in vitro as \nbasic computational elements for machine learning applications. A new \nsupervised STDP-based learning algorithm is proposed in this work, which \nconsiders neuron engineering constrains. A 74.7% accuracy is achieved on the \nMNIST benchmark for handwritten digit recognition. \n</p>"}, "author": "Yuan Zeng, Kevin Devincentis, Yao Xiao, Zubayer Ibne Ferdous, Xiaochen Guo, Zhiyuan Yan, Yevgeny Berdichevsky", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509413371200", "timestampUsec": "1509413371200329", "id": "tag:google.com,2005:reader/item/000000032916ded7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Understanding Grounded Language Learning Agents. (arXiv:1710.09867v1 [cs.CL] CROSS LISTED)", "published": 1509413371, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09867"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09867", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural network-based systems can now learn to locate the referents of words \nand phrases in images, answer questions about visual scenes, and even execute \nsymbolic instructions as first-person actors in partially-observable worlds. To \nachieve this so-called grounded language learning, models must overcome certain \nwell-studied learning challenges that are also fundamental to infants learning \ntheir first words. While it is notable that models with no meaningful prior \nknowledge overcome these learning obstacles, AI researchers and practitioners \ncurrently lack a clear understanding of exactly how they do so. Here we address \nthis question as a way of achieving a clearer general understanding of grounded \nlanguage learning, both to inform future research and to improve confidence in \nmodel predictions. For maximum control and generality, we focus on a simple \nneural network-based language learning agent trained via policy-gradient \nmethods to interpret synthetic linguistic instructions in a simulated 3D world. \nWe apply experimental paradigms from developmental psychology to this agent, \nexploring the conditions under which established human biases and learning \neffects emerge. We further propose a novel way to visualise and analyse \nsemantic representation in grounded language learning agents that yields a \nplausible computational account of the observed effects. \n</p>"}, "author": "Felix Hill, Karl Moritz Hermann, Phil Blunsom, Stephen Clark", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509325337660", "timestampUsec": "1509325337659589", "id": "tag:google.com,2005:reader/item/000000032869321e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Phase Transitions in Image Denoising via Sparsely Coding Convolutional Neural Networks. (arXiv:1710.09875v1 [cs.NE])", "published": 1509325338, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09875"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09875", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks are analogous in many ways to spin glasses, systems which are \nknown for their rich set of dynamics and equally complex phase diagrams. We \napply well-known techniques in the study of spin glasses to a convolutional \nsparsely encoding neural network and observe power law finite-size scaling \nbehavior in the sparsity and reconstruction error as the network denoises \n32$\\times$32 RGB CIFAR-10 images. This finite-size scaling indicates the \npresence of a continuous phase transition at a critical value of this sparsity. \nBy using the power law scaling relations inherent to finite-size scaling, we \ncan determine the optimal value of sparsity for any network size by tuning the \nsystem to the critical point and operate the system at the minimum denoising \nerror. \n</p>"}, "author": "Jacob Carroll, Nils Carlson, Garrett T. Kenyon", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509325337660", "timestampUsec": "1509325337659588", "id": "tag:google.com,2005:reader/item/000000032869322d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Data-driven Feature Sampling for Deep Hyperspectral Classification and Segmentation. (arXiv:1710.09934v1 [cs.NE])", "published": 1509325338, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09934"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09934", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The high dimensionality of hyperspectral imaging forces unique challenges in \nscope, size and processing requirements. Motivated by the potential for an \nin-the-field cell sorting detector, we examine a $\\textit{Synechocystis sp.}$ \nPCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or \ndeplete cultures. We use deep learning techniques to both successfully classify \ncells and generate a mask segmenting the cells/condition from the background. \nFurther, we use the classification accuracy to guide a data-driven, iterative \nfeature selection method, allowing the design neural networks requiring 90% \nfewer input features with little accuracy degradation. \n</p>"}, "author": "William M. Severa, Jerilyn A. Timlin, Suraj Kholwadwala, Conrad D. James, James B. Aimone", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509325337660", "timestampUsec": "1509325337659587", "id": "tag:google.com,2005:reader/item/000000032869323a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation. (arXiv:1710.10196v1 [cs.NE])", "published": 1509325338, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10196"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10196", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe a new training methodology for generative adversarial networks. \nThe key idea is to grow both the generator and discriminator progressively: \nstarting from a low resolution, we add new layers that model increasingly fine \ndetails as training progresses. This both speeds the training up and greatly \nstabilizes it, allowing us to produce images of unprecedented quality, e.g., \nCelebA images at 1024^2. We also propose a simple way to increase the variation \nin generated images, and achieve a record inception score of 8.80 in \nunsupervised CIFAR10. Additionally, we describe several implementation details \nthat are important for discouraging unhealthy competition between the generator \nand discriminator. Finally, we suggest a new metric for evaluating GAN results, \nboth in terms of image quality and variation. As an additional contribution, we \nconstruct a higher-quality version of the CelebA dataset. \n</p>"}, "author": "Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509325337660", "timestampUsec": "1509325337659586", "id": "tag:google.com,2005:reader/item/0000000328693245", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tensor network language model. (arXiv:1710.10248v2 [cs.CL] UPDATED)", "published": 1509413371, "updated": 1509413373, "canonical": [{"href": "http://arxiv.org/abs/1710.10248"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10248", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new statistical model suitable for machine learning of systems \nwith long distance correlations such as natural languages. The model is based \non directed acyclic graph decorated by multi-linear tensor maps in the vertices \nand vector spaces in the edges, called tensor network. Such tensor networks \nhave been previously employed for effective numerical computation of the \nrenormalization group flow on the space of effective quantum field theories and \nlattice models of statistical mechanics. We provide explicit algebro-geometric \nanalysis of the parameter moduli space for tree graphs, discuss model \nproperties and applications such as statistical translation. \n</p>"}, "author": "Vasily Pestun, Yiannis Vlassopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505092", "id": "tag:google.com,2005:reader/item/000000032866e7af", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gradient Sparsification for Communication-Efficient Distributed Optimization. (arXiv:1710.09854v1 [cs.LG])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09854"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09854", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Modern large scale machine learning applications require stochastic \noptimization algorithms to be implemented on distributed computational \narchitectures. A key bottleneck is the communication overhead for exchanging \ninformation such as stochastic gradients among different workers. In this \npaper, to reduce the communication cost we propose a convex optimization \nformulation to minimize the coding length of stochastic gradients. To solve the \noptimal sparsification efficiently, several simple and fast algorithms are \nproposed for approximate solution, with theoretical guaranteed for sparseness. \nExperiments on $\\ell_2$ regularized logistic regression, support vector \nmachines, and convolutional neural networks validate our sparsification \napproaches. \n</p>"}, "author": "Jianqiao Wangni, Jialei Wang, Ji Liu, Tong Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505091", "id": "tag:google.com,2005:reader/item/000000032866e841", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Energy Clustering. (arXiv:1710.09859v1 [stat.ML])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09859"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09859", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Energy statistics was proposed by Sz\\'{e}kely in the 80's inspired by the \nNewtonian gravitational potential from classical mechanics, and it provides a \nhypothesis test for equality of distributions. It was further generalized from \nEuclidean spaces to metric spaces of strong negative type, and more recently, a \nconnection with reproducing kernel Hilbert spaces (RKHS) was established. Here \nwe consider the clustering problem from an energy statistics theory \nperspective, providing a precise mathematical formulation yielding a \nquadratically constrained quadratic program (QCQP) in the associated RKHS, thus \nestablishing the connection with kernel methods. We show that this QCQP is \nequivalent to kernel $k$-means optimization problem once the kernel is fixed. \nThese results imply a first principles derivation of kernel $k$-means from \nenergy statistics. However, energy statistics fixes a family of standard \nkernels. Furthermore, we also consider a weighted version of energy statistics, \nmaking connection to graph partitioning problems. To find local optimizers of \nsuch QCQP we propose an iterative algorithm based on Hartigan's method, which \nin this case has the same computational cost as kernel $k$-means algorithm, \nbased on Lloyd's heuristic, but usually with better clustering quality. We \nprovide carefully designed numerical experiments showing the superiority of the \nproposed method compared to kernel $k$-means, spectral clustering, standard \n$k$-means, and Gaussian mixture models in a variety of settings. \n</p>"}, "author": "Guilherme Fran&#xe7;a, Joshua T. Vogelstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505090", "id": "tag:google.com,2005:reader/item/000000032866e8b6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Error Probability of Random Fourier Features is Dimensionality Independent. (arXiv:1710.09953v1 [cs.LG])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09953"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09953", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8edca12\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8edca12&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We show that the error probability of reconstructing kernel matrices from \nRandom Fourier Features for any shift-invariant kernel function is at most \n$\\mathcal{O}(\\exp(-D))$, where $D$ is the number of random features. We also \nprovide a matching information-theoretic method-independent lower bound of \n$\\Omega(\\exp(-D))$ for standard Gaussian distributions. Compared to prior work, \nwe are the first to show that the error probability for random Fourier features \nis independent of the dimensionality of data points as well as the size of \ntheir domain. As applications of our theory, we obtain dimension-independent \nbounds for kernel ridge regression and support vector machines. \n</p>"}, "author": "Jean Honorio, Yu-Jun Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505089", "id": "tag:google.com,2005:reader/item/000000032866e90a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Conjugate Gradient Algorithm with Variance Reduction. (arXiv:1710.09979v1 [cs.LG])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09979"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09979", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8fa377e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8fa377e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Conjugate gradient methods are a class of important methods for solving \nlinear equations and nonlinear optimization. In our work, we propose a new \nstochastic conjugate gradient algorithm with variance reduction (CGVR) and \nprove its linear convergence with the Fletcher and Revves method for strongly \nconvex and smooth functions. We experimentally demonstrate that the CGVR \nalgorithm converges faster than its counterparts for six large-scale \noptimization problems that may be convex, non-convex or non-smooth, and its AUC \n(Area Under Curve) performance with $L2$-regularized $L2$-loss is comparable to \nthat of LIBLINEAR but with significant improvement in computational efficiency. \n</p>"}, "author": "Xiao-Bo Jin, Xu-Yao Zhang, Kaizhu Huang, Guang-Gang Geng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505088", "id": "tag:google.com,2005:reader/item/000000032866e95a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Zeroth Order Nonconvex Multi-Agent Optimization over Networks. (arXiv:1710.09997v1 [math.OC])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09997"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09997", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we consider distributed optimization problems over a \nmulti-agent network, where each agent can only partially evaluate the objective \nfunction, and it is allowed to exchange messages with its immediate neighbors. \nDifferently from all existing works on distributed optimization, our focus is \ngiven to optimizing a class of difficult non-convex problems, and under the \nchallenging setting where each agent can only access the zeroth-order \ninformation (i.e., the functional values) of its local functions. For different \ntypes of network topologies such as undirected connected networks or star \nnetworks, we develop efficient distributed algorithms and rigorously analyze \ntheir convergence and rate of convergence (to the set of stationary solutions). \nNumerical results are provided to demonstrate the efficiency of the proposed \nalgorithms. \n</p>"}, "author": "Davood Hajinezhad, Mingyi Hong, Alfredo Garcia", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505087", "id": "tag:google.com,2005:reader/item/000000032866e9b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning for Accelerated Ultrasound Imaging. (arXiv:1710.10006v1 [cs.CV])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10006"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10006", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In portable, 3-D, or ultra-fast ultrasound (US) imaging systems, there is an \nincreasing demand to reconstruct high quality images from limited number of \ndata. However, the existing solutions require either hardware changes or \ncomputationally expansive algorithms. To overcome these limitations, here we \npropose a novel deep learning approach that interpolates the missing RF data by \nutilizing the sparsity of the RF data in the Fourier domain. Extensive \nexperimental results from sub-sampled RF data from a real US system confirmed \nthat the proposed method can effectively reduce the data rate without \nsacrificing the image quality. \n</p>"}, "author": "Yeo Hun Yoon, Jong Chul Ye", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505086", "id": "tag:google.com,2005:reader/item/000000032866e9fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Regularization via Mass Transportation. (arXiv:1710.10016v1 [math.OC])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10016"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10016", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The goal of regression and classification methods in supervised learning is \nto minimize the empirical risk, that is, the expectation of some loss function \nquantifying the prediction error under the empirical distribution. When facing \nscarce training data, overfitting is typically mitigated by adding \nregularization terms to the objective that penalize hypothesis complexity. In \nthis paper we introduce new regularization techniques using ideas from \ndistributionally robust optimization, and we give new probabilistic \ninterpretations to existing techniques. Specifically, we propose to minimize \nthe worst-case expected loss, where the worst case is taken over the ball of \nall (continuous or discrete) distributions that have a bounded transportation \ndistance from the (discrete) empirical distribution. By choosing the radius of \nthis ball judiciously, we can guarantee that the worst-case expected loss \nprovides an upper confidence bound on the loss on test data, thus offering new \ngeneralization bounds. We prove that the resulting regularized learning \nproblems are tractable and can be tractably kernelized for many popular loss \nfunctions. We validate our theoretical out-of-sample guarantees through \nsimulated and empirical experiments. \n</p>"}, "author": "Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, Peyman Mohajerin Esfahani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505085", "id": "tag:google.com,2005:reader/item/000000032866ea59", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Online Learning of Power Transmission Dynamics. (arXiv:1710.10021v1 [cs.SY])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10021"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10021", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of reconstructing the dynamic state matrix of \ntransmission power grids from time-stamped PMU measurements in the regime of \nambient fluctuations. Using a maximum likelihood based approach, we construct a \nfamily of convex estimators that adapt to the structure of the problem \ndepending on the available prior information. The proposed method is fully \ndata-driven and does not assume any knowledge of system parameters. It can be \nimplemented in near real-time and requires a small amount of data. Our learning \nalgorithms can be used for model validation and calibration, and can also be \napplied to related problems of system stability, detection of forced \noscillations, generation re-dispatch, as well as to the estimation of the \nsystem state. \n</p>"}, "author": "Andrey Y. Lokhov, Marc Vuffray, Dmitry Shemetov, Deepjyoti Deka, Michael Chertkov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505084", "id": "tag:google.com,2005:reader/item/000000032866eaed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalization Tower Network: A Novel Deep Neural Network Architecture for Multi-Task Learning. (arXiv:1710.10036v1 [cs.AI])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10036"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10036", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by \nincorporating deep neural networks in learning representations from the input \nto RL. However, the conventional deep neural network architecture is limited in \nlearning representations for multi-task RL (MT-RL), as multiple tasks can refer \nto different kinds of representations. In this paper, we thus propose a novel \ndeep neural network architecture, namely generalization tower network (GTN), \nwhich can achieve MT-RL within a single learned model. Specifically, the \narchitecture of GTN is composed of both horizontal and vertical streams. In our \nGTN architecture, horizontal streams are used to learn representation shared in \nsimilar tasks. In contrast, the vertical streams are introduced to be more \nsuitable for handling diverse tasks, which encodes hierarchical shared \nknowledge of these tasks. The effectiveness of the introduced vertical stream \nis validated by experimental results. Experimental results further verify that \nour GTN architecture is able to advance the state-of-the-art MT-RL, via being \ntested on 51 Atari games. \n</p>"}, "author": "Yuhang Song, Main Xu, Songyang Zhang, Liangyu Huo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505083", "id": "tag:google.com,2005:reader/item/000000032866eb3e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distributional Reinforcement Learning with Quantile Regression. (arXiv:1710.10044v1 [cs.AI])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10044"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10044", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In reinforcement learning an agent interacts with the environment by taking \nactions and observing the next state and reward. When sampled \nprobabilistically, these state transitions, rewards, and actions can all induce \nrandomness in the observed long-term return. Traditionally, reinforcement \nlearning algorithms average over this randomness to estimate the value \nfunction. In this paper, we build on recent work advocating a distributional \napproach to reinforcement learning in which the distribution over returns is \nmodeled explicitly instead of only estimating the mean. That is, we examine \nmethods of learning the value distribution instead of the value function. We \ngive results that close a number of gaps between the theoretical and \nalgorithmic results given by Bellemare, Dabney, and Munos (2017). First, we \nextend existing results to the approximate distribution setting. Second, we \npresent a novel distributional reinforcement learning algorithm consistent with \nour theoretical formulation. Finally, we evaluate this new algorithm on the \nAtari 2600 games, observing that it significantly outperforms many of the \nrecent improvements on DQN, including the related distributional algorithm C51. \n</p>"}, "author": "Will Dabney, Mark Rowland, Marc G. Bellemare, R&#xe9;mi Munos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505082", "id": "tag:google.com,2005:reader/item/000000032866eb88", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations. (arXiv:1710.10121v1 [cs.CV])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10121"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10121", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In our work, we bridge deep neural network design with numerical differential \nequations. We show that many effective networks, such as ResNet, PolyNet, \nFractalNet and RevNet, can be interpreted as different numerical \ndiscretizations of differential equations. This finding brings us a brand new \nperspective on the design of effective deep architectures. We can take \nadvantage of the rich knowledge in numerical analysis to guide us in designing \nnew and potentially more effective deep networks. As an example, we propose a \nlinear multi-step architecture (LM-architecture) which is inspired by the \nlinear multi-step method solving ordinary differential equations. The \nLM-architecture is an effective structure that can be used on any ResNet-like \nnetworks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the \nnetworks obtained by applying the LM-architecture on ResNet and ResNeXt \nrespectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on \nboth CIFAR and ImageNet with comparable numbers of trainable parameters. In \nparticular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly \ncompress ($&gt;50$\\%) the original networks while maintaining a similar \nperformance. This can be explained mathematically using the concept of modified \nequation from numerical analysis. Last but not least, we also establish a \nconnection between stochastic control and noise injection in the training \nprocess which helps to improve generalization of the networks. Furthermore, by \nrelating stochastic training strategy with stochastic dynamic system, we can \neasily apply stochastic training to the networks with the LM-architecture. As \nan example, we introduced stochastic depth to LM-ResNet and achieve significant \nimprovement over the original LM-ResNet on CIFAR10. \n</p>"}, "author": "Yiping Lu, Aoxiao Zhong, Quanzheng Li, Bin Dong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505081", "id": "tag:google.com,2005:reader/item/000000032866ec0a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Development and analysis of a Bayesian water balance model for large lake systems. (arXiv:1710.10161v1 [stat.AP])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10161"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10161", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Water balance models are often employed to improve understanding of drivers \nof change in regional hydrologic cycles. Most of these models, however, are \nphysically-based, and few employ state-of-the-art statistical methods to \nreconcile measurement uncertainty and bias. Here, we introduce a framework for \ndeveloping, analyzing, and selecting among alternative formulations of a \nstatistical water balance model for large lake systems that addresses this \nresearch gap. We demonstrate our new analytical framework using a model \ncustomized for Lakes Superior and Michigan-Huron, the two largest lakes on \nEarth by surface area. The selected model (from among 26 alternatives) closed \nthe water balance across both lakes and had a computation time of roughly 95 \nminutes - an order of magnitude less than prototype versions of the same model. \nWe expect our new framework will be used to improve computational efficiency \nand skill of water balance models for other lakes around the world. \n</p>"}, "author": "Joeseph P. Smith, Andrew D. Gronewold", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505080", "id": "tag:google.com,2005:reader/item/000000032866ec71", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation. (arXiv:1710.10196v1 [cs.NE])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10196"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10196", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8fa39f6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8fa39f6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We describe a new training methodology for generative adversarial networks. \nThe key idea is to grow both the generator and discriminator progressively: \nstarting from a low resolution, we add new layers that model increasingly fine \ndetails as training progresses. This both speeds the training up and greatly \nstabilizes it, allowing us to produce images of unprecedented quality, e.g., \nCelebA images at 1024^2. We also propose a simple way to increase the variation \nin generated images, and achieve a record inception score of 8.80 in \nunsupervised CIFAR10. Additionally, we describe several implementation details \nthat are important for discouraging unhealthy competition between the generator \nand discriminator. Finally, we suggest a new metric for evaluating GAN results, \nboth in terms of image quality and variation. As an additional contribution, we \nconstruct a higher-quality version of the CelebA dataset. \n</p>"}, "author": "Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505079", "id": "tag:google.com,2005:reader/item/000000032866ecc2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On denoising modulo 1 samples of a function. (arXiv:1710.10210v2 [stat.ML] UPDATED)", "published": 1510049224, "updated": 1510049278, "canonical": [{"href": "http://arxiv.org/abs/1710.10210"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10210", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Consider an unknown smooth function $f: [0,1] \\rightarrow \\mathbb{R}$, and \nsay we are given $n$ noisy mod 1 samples of $f$, i.e., $y_i = (f(x_i) + \n\\eta_i)\\mod 1$ for $x_i \\in [0,1]$, where $\\eta_i$ denotes noise. Given the \nsamples $(x_i,y_i)_{i=1}^{n}$, our goal is to recover smooth, robust estimates \nof the clean samples $f(x_i) \\bmod 1$. We formulate a natural approach for \nsolving this problem which works with angular embeddings of the noisy mod 1 \nsamples over the unit complex circle, inspired by the angular synchronization \nframework. Our approach amounts to solving a quadratically constrained \nquadratic program (QCQP) which is NP-hard in its basic form, and therefore we \nconsider its relaxation which is a trust region sub-problem and hence solvable \nefficiently. We demonstrate its robustness to noise via extensive numerical \nsimulations on several synthetic examples, along with a detailed theoretical \nanalysis. To the best of our knowledge, we provide the first algorithm for \ndenoising mod 1 samples of a smooth function, which comes with robustness \nguarantees. \n</p>"}, "author": "Mihai Cucuringu, Hemant Tyagi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505078", "id": "tag:google.com,2005:reader/item/000000032866ed2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Not-So-Random Features. (arXiv:1710.10230v1 [cs.LG])", "published": 1509323746, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10230"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10230", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a principled method for kernel learning, which relies on a \nFourier-analytic characterization of translation-invariant or \nrotation-invariant kernels. Our method produces a sequence of feature maps, \niteratively refining the SVM margin. We provide rigorous guarantees for \noptimality and generalization, interpreting our algorithm as online \nequilibrium-finding dynamics in a certain two-player min-max game. Evaluations \non synthetic and real-world datasets demonstrate scalability and consistent \nimprovements over related random features-based methods. \n</p>"}, "author": "Brian Bullins, Cyril Zhang, Yi Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323746505", "timestampUsec": "1509323746505077", "id": "tag:google.com,2005:reader/item/000000032866ed54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tensor network language model. (arXiv:1710.10248v2 [cs.CL] UPDATED)", "published": 1509484038, "updated": 1509484044, "canonical": [{"href": "http://arxiv.org/abs/1710.10248"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10248", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new statistical model suitable for machine learning of systems \nwith long distance correlations such as natural languages. The model is based \non directed acyclic graph decorated by multi-linear tensor maps in the vertices \nand vector spaces in the edges, called tensor network. Such tensor networks \nhave been previously employed for effective numerical computation of the \nrenormalization group flow on the space of effective quantum field theories and \nlattice models of statistical mechanics. We provide explicit algebro-geometric \nanalysis of the parameter moduli space for tree graphs, discuss model \nproperties and applications such as statistical translation. \n</p>"}, "author": "Vasily Pestun, Yiannis Vlassopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097875", "id": "tag:google.com,2005:reader/item/000000032866b89b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gradient Sparsification for Communication-Efficient Distributed Optimization. (arXiv:1710.09854v1 [cs.LG])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09854"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09854", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Modern large scale machine learning applications require stochastic \noptimization algorithms to be implemented on distributed computational \narchitectures. A key bottleneck is the communication overhead for exchanging \ninformation such as stochastic gradients among different workers. In this \npaper, to reduce the communication cost we propose a convex optimization \nformulation to minimize the coding length of stochastic gradients. To solve the \noptimal sparsification efficiently, several simple and fast algorithms are \nproposed for approximate solution, with theoretical guaranteed for sparseness. \nExperiments on $\\ell_2$ regularized logistic regression, support vector \nmachines, and convolutional neural networks validate our sparsification \napproaches. \n</p>"}, "author": "Jianqiao Wangni, Jialei Wang, Ji Liu, Tong Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097874", "id": "tag:google.com,2005:reader/item/000000032866b89f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Enhancements of linked data expressiveness for ontologies. (arXiv:1710.09952v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09952"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09952", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The semantic web has received many contributions of researchers as ontologies \nwhich, in this context, i.e. within RDF linked data, are formalized \nconceptualizations that might use different protocols, such as RDFS, OWL DL and \nOWL FULL. In this article, we describe new expressive techniques which were \nfound necessary after elaborating dozens of OWL ontologies for the scientific \nacademy, the State and the civil society. They consist in: 1) stating possible \nuses a property might have without incurring into axioms or restrictions; 2) \nassigning a level of priority for an element (class, property, triple); 3) \ncorrect depiction in diagrams of relations between classes, between individuals \nwhich are imperative, and between individuals which are optional; 4) a \nconvenient association between OWL classes and SKOS concepts. We propose \nspecific rules to accomplish these enhancements and exemplify both its use and \nthe difficulties that arise because these techniques are currently not \nestablished as standards to the ontology designer. \n</p>"}, "author": "Renato Fabbri", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097873", "id": "tag:google.com,2005:reader/item/000000032866b8a5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Audiovisual Analytics Vocabulary and Ontology (AAVO): initial core and example expansion. (arXiv:1710.09954v1 [cs.CY])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09954"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09954", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Visual Analytics might be defined as data mining assisted by interactive \nvisual interfaces. The field has been receiving prominent consideration by \nresearchers, developers and the industry. The literature, however, is complex \nbecause it involves multiple fields of knowledge and is considerably recent. In \nthis article we describe an initial tentative organization of the knowledge in \nthe field as an OWL ontology and a SKOS vocabulary. This effort might be useful \nin many ways that include conceptual considerations and software \nimplementations. Within the results and discussions, we expose a core and an \nexample expansion of the conceptualization, and incorporate design issues that \nenhance the expressive power of the abstraction. \n</p>"}, "author": "Renato Fabbri, Maria Cristina Ferreira de Oliveira", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097872", "id": "tag:google.com,2005:reader/item/000000032866b8aa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning for Accelerated Ultrasound Imaging. (arXiv:1710.10006v1 [cs.CV])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10006"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10006", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In portable, 3-D, or ultra-fast ultrasound (US) imaging systems, there is an \nincreasing demand to reconstruct high quality images from limited number of \ndata. However, the existing solutions require either hardware changes or \ncomputationally expansive algorithms. To overcome these limitations, here we \npropose a novel deep learning approach that interpolates the missing RF data by \nutilizing the sparsity of the RF data in the Fourier domain. Extensive \nexperimental results from sub-sampled RF data from a real US system confirmed \nthat the proposed method can effectively reduce the data rate without \nsacrificing the image quality. \n</p>"}, "author": "Yeo Hun Yoon, Jong Chul Ye", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097871", "id": "tag:google.com,2005:reader/item/000000032866b8ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Online Learning of Power Transmission Dynamics. (arXiv:1710.10021v1 [cs.SY])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10021"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10021", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of reconstructing the dynamic state matrix of \ntransmission power grids from time-stamped PMU measurements in the regime of \nambient fluctuations. Using a maximum likelihood based approach, we construct a \nfamily of convex estimators that adapt to the structure of the problem \ndepending on the available prior information. The proposed method is fully \ndata-driven and does not assume any knowledge of system parameters. It can be \nimplemented in near real-time and requires a small amount of data. Our learning \nalgorithms can be used for model validation and calibration, and can also be \napplied to related problems of system stability, detection of forced \noscillations, generation re-dispatch, as well as to the estimation of the \nsystem state. \n</p>"}, "author": "Andrey Y. Lokhov, Marc Vuffray, Dmitry Shemetov, Deepjyoti Deka, Michael Chertkov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097870", "id": "tag:google.com,2005:reader/item/000000032866b8b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convolutional neural networks on irregular domains through approximate translations on inferred graphs. (arXiv:1710.10035v1 [cs.DM])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10035"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10035", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a generalization of convolutional neural networks (CNNs) to \nirregular domains, through the use of an inferred graph structure. In more \ndetails, we introduce a three-step methodology to create convolutional layers \nthat are adapted to the signals to process: 1) From a training set of signals, \ninfer a graph representing the topology on which they evolve; 2) Identify \ntranslation operators in the vertex domain; 3) Emulate a convolution operator \nby translating a localized kernel on the graph. Using these layers, a \nconvolutional neural network is built, and is trained on the initial signals to \nperform a classification task. Contributions are twofold. First, we adapt a \ndefinition of translations on graphs to make them more robust to \nirregularities, and to take into account locality of the kernel. Second, we \nintroduce a procedure to build CNNs from data. We apply our methodology on a \nscrambled version of the CIFAR-10 and Haxby datasets. Without using any \nknowledge on the signals, we significantly outperform existing methods. \nMoreover, our approach extends classical CNNs on images in the sense that such \nnetworks are a particular case of our approach when the inferred graph is a \ngrid. \n</p>"}, "author": "Bastien Pasdeloup, Vincent Gripon, Jean-Charles Vialatte, Dominique Pastor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097869", "id": "tag:google.com,2005:reader/item/000000032866b8b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Generalization Tower Network: A Novel Deep Neural Network Architecture for Multi-Task Learning. (arXiv:1710.10036v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10036"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10036", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c8fa3c29\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c8fa3c29&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by \nincorporating deep neural networks in learning representations from the input \nto RL. However, the conventional deep neural network architecture is limited in \nlearning representations for multi-task RL (MT-RL), as multiple tasks can refer \nto different kinds of representations. In this paper, we thus propose a novel \ndeep neural network architecture, namely generalization tower network (GTN), \nwhich can achieve MT-RL within a single learned model. Specifically, the \narchitecture of GTN is composed of both horizontal and vertical streams. In our \nGTN architecture, horizontal streams are used to learn representation shared in \nsimilar tasks. In contrast, the vertical streams are introduced to be more \nsuitable for handling diverse tasks, which encodes hierarchical shared \nknowledge of these tasks. The effectiveness of the introduced vertical stream \nis validated by experimental results. Experimental results further verify that \nour GTN architecture is able to advance the state-of-the-art MT-RL, via being \ntested on 51 Atari games. \n</p>"}, "author": "Yuhang Song, Main Xu, Songyang Zhang, Liangyu Huo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097868", "id": "tag:google.com,2005:reader/item/000000032866b8bd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distributional Reinforcement Learning with Quantile Regression. (arXiv:1710.10044v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10044"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10044", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c906f4f6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c906f4f6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In reinforcement learning an agent interacts with the environment by taking \nactions and observing the next state and reward. When sampled \nprobabilistically, these state transitions, rewards, and actions can all induce \nrandomness in the observed long-term return. Traditionally, reinforcement \nlearning algorithms average over this randomness to estimate the value \nfunction. In this paper, we build on recent work advocating a distributional \napproach to reinforcement learning in which the distribution over returns is \nmodeled explicitly instead of only estimating the mean. That is, we examine \nmethods of learning the value distribution instead of the value function. We \ngive results that close a number of gaps between the theoretical and \nalgorithmic results given by Bellemare, Dabney, and Munos (2017). First, we \nextend existing results to the approximate distribution setting. Second, we \npresent a novel distributional reinforcement learning algorithm consistent with \nour theoretical formulation. Finally, we evaluate this new algorithm on the \nAtari 2600 games, observing that it significantly outperforms many of the \nrecent improvements on DQN, including the related distributional algorithm C51. \n</p>"}, "author": "Will Dabney, Mark Rowland, Marc G. Bellemare, R&#xe9;mi Munos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097867", "id": "tag:google.com,2005:reader/item/000000032866b8c2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Group Fairness in Multiwinner Voting. (arXiv:1710.10057v1 [cs.CY])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10057"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10057", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study multiwinner voting problems when there is an additional requirement \nthat the selected committee should be fair with respect to attributes such as \ngender, ethnicity, or political parties. Every setting of an attribute gives \nrise to a group, and the goal is to ensure that each group is neither over nor \nunder represented in the selected committee. Prior work has largely focused on \ndesigning specialized score functions that lead to a precise level of \nrepresentation with respect to disjoint attributes (e.g., only political \naffiliation). Here we propose a general algorithmic framework that allows the \nuse of any score function and can guarantee flexible notions of fairness with \nrespect to multiple, non-disjoint attributes (e.g., political affiliation and \ngender). Technically, we study the complexity of this constrained multiwinner \nvoting problem subject to group-fairness constraints for monotone submodular \nscore functions. We present approximation algorithms and hardness of \napproximation results for various attribute set structures and score functions. \n</p>"}, "author": "L. Elisa Celis, Lingxiao Huang, Nisheeth K. Vishnoi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097866", "id": "tag:google.com,2005:reader/item/000000032866b8ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network. (arXiv:1710.10059v1 [cs.SD])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10059"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10059", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper proposes a deep neural network for estimating the directions of \narrival (DOA) of multiple sound sources. The proposed stacked convolutional and \nrecurrent neural network (DOAnet) generates a spatial pseudo-spectrum along \nwith the DOA estimates in both azimuth and elevation. We avoid any explicit \nfeature extraction step by using the magnitude and phase of the spectrogram as \ninput to the network. The proposed DOAnet is evaluated by estimating the DOAs \nof multiple concurrently present sources in anechoic, matched and unmatched \nreverberant conditions. The results show that the proposed DOAnet is capable of \nestimating the number of sources and their respective DOAs with good precision \nand generate SPS with high signal-to-noise ratio. \n</p>"}, "author": "Sharath Adavanne, Archontis Politis, Tuomas Virtanen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097865", "id": "tag:google.com,2005:reader/item/000000032866b8d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On modeling vagueness and uncertainty in data-to-text systems through fuzzy sets. (arXiv:1710.10093v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10093"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10093", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Vagueness and uncertainty management is counted among one of the challenges \nthat remain unresolved in systems that generate texts from non-linguistic data, \nknown as data-to-text systems. In the last decade, work in fuzzy linguistic \nsummarization and description of data has raised the interest of using fuzzy \nsets to model and manage the imprecision of human language in data-to-text \nsystems. However, despite some research in this direction, there has not been \nan actual clear discussion and justification on how fuzzy sets can contribute \nto data-to-text for modeling vagueness and uncertainty in words and \nexpressions. This paper intends to bridge this gap by answering the following \nquestions: What does vagueness mean in fuzzy sets theory? What does vagueness \nmean in data-to-text contexts? In what ways can fuzzy sets theory contribute to \nimprove data-to-text systems? What are the challenges that researchers from \nboth disciplines need to address for a successful integration of fuzzy sets \ninto data-to-text systems? In what cases should the use of fuzzy sets be \navoided in D2T? For this, we review and discuss the state of the art of \nvagueness modeling in natural language generation and data-to-text, describe \npotential and actual usages of fuzzy sets in data-to-text contexts, and provide \nsome additional insights about the engineering of data-to-text systems that \nmake use of fuzzy set-based techniques. \n</p>"}, "author": "A. Ramos-Soto, M. Pereira-Fari&#xf1;a", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097864", "id": "tag:google.com,2005:reader/item/000000032866b8db", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An efficient SAT formulation for learning multiple criteria non-compensatory sorting rules from examples. (arXiv:1710.10098v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10098"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10098", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The literature on Multiple Criteria Decision Analysis (MCDA) proposes several \nmethods in order to sort alternatives evaluated on several attributes into \nordered classes. Non Compensatory Sorting models (NCS) assign alternatives to \nclasses based on the way they compare to multicriteria profiles separating the \nconsecutive classes. Previous works have proposed approaches to learn the \nparameters of a NCS model based on a learning set. Exact approaches based on \nmixed integer linear programming ensures that the learning set is best \nrestored, but can only handle datasets of limited size. Heuristic approaches \ncan handle large learning sets, but do not provide any guarantee about the \ninferred model. In this paper, we propose an alternative formulation to learn a \nNCS model. This formulation, based on a SAT problem, guarantees to find a model \nfully consistent with the learning set (whenever it exists), and is \ncomputationally much more efficient than existing exact MIP approaches. \n</p>"}, "author": "K. Belahc&#xe8;ne, C. Labreuche, N. Maudet, V. Mousseau, W. Ouerdane", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097863", "id": "tag:google.com,2005:reader/item/000000032866b8ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Inverse Reinforcement Learning Under Noisy Observations. (arXiv:1710.10116v1 [cs.RO])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10116"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10116", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of performing inverse reinforcement learning when the \ntrajectory of the expert is not perfectly observed by the learner. Instead, a \nnoisy continuous-time observation of the trajectory is provided to the learner. \nThis problem exhibits wide-ranging applications and the specific application we \nconsider here is the scenario in which the learner seeks to penetrate a \nperimeter patrolled by a robot. The learner's field of view is limited due to \nwhich it cannot observe the patroller's complete trajectory. Instead, we allow \nthe learner to listen to the expert's movement sound, which it can also use to \nestimate the expert's state and action using an observation model. We treat the \nexpert's state and action as hidden data and present an algorithm based on \nexpectation maximization and maximum entropy principle to solve the non-linear, \nnon-convex problem. Related work considers discrete-time observations and an \nobservation model that does not include actions. In contrast, our technique \ntakes expectations over both state and action of the expert, enabling learning \neven in the presence of extreme noise and broader applications. \n</p>"}, "author": "Shervin Shahryari, Prashant Doshi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097862", "id": "tag:google.com,2005:reader/item/000000032866b8f9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards a new paradigm for assistive technology at home: research challenges, design issues and performance assessment. (arXiv:1710.10164v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10164"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10164", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Providing elderly and people with special needs, including those suffering \nfrom physical disabilities and chronic diseases, with the possibility of \nretaining their independence at best is one of the most important challenges \nour society is expected to face. Assistance models based on the home care \nparadigm are being adopted rapidly in almost all industrialized and emerging \ncountries. Such paradigms hypothesize that it is necessary to ensure that the \nso-called Activities of Daily Living are correctly and regularly performed by \nthe assisted person to increase the perception of an improved quality of life. \nThis chapter describes the computational inference engine at the core of \nArianna, a system able to understand whether an assisted person performs a \ngiven set of ADL and to motivate him/her in performing them through a \nspeech-mediated motivational dialogue, using a set of nearables to be installed \nin an apartment, plus a wearable to be worn or fit in garments. \n</p>"}, "author": "Luca Buoncompagni, Barbara Bruno, Antonella Giuni, Fulvio Mastrogiovanni, Renato Zaccaria", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097861", "id": "tag:google.com,2005:reader/item/000000032866b904", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Advanced LSTM: A Study about Better Time Dependency Modeling in Emotion Recognition. (arXiv:1710.10197v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10197"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10197", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Long short-term memory (LSTM) is normally used in recurrent neural network \n(RNN) as basic recurrent unit. However,conventional LSTM assumes that the state \nat current time step depends on previous time step. This assumption constraints \nthe time dependency modeling capability. In this study, we propose a new \nvariation of LSTM, advanced LSTM (A-LSTM), for better temporal context \nmodeling. We employ A-LSTM in weighted pooling RNN for emotion recognition. The \nA-LSTM outperforms the conventional LSTM by 5.5% relatively. The A-LSTM based \nweighted pooling RNN can also complement the state-of-the-art emotion \nclassification framework. This shows the advantage of A-LSTM. \n</p>"}, "author": "Fei Tao, Gang Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509323622098", "timestampUsec": "1509323622097860", "id": "tag:google.com,2005:reader/item/000000032866b90b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "BridgeNets: Student-Teacher Transfer Learning Based on Recursive Neural Networks and its Application to Distant Speech Recognition. (arXiv:1710.10224v1 [cs.AI])", "published": 1509323622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10224"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10224", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Despite the remarkable progress achieved on automatic speech recognition, \nrecognizing far-field speeches mixed with various noise sources is still a \nchallenging task. In this paper, we introduce novel student-teacher transfer \nlearning, BridgeNet which can provide a solution to improve distant speech \nrecognition. There are two key features in BridgeNet. First, BridgeNet extends \ntraditional student-teacher frameworks by providing multiple hints from a \nteacher network. Hints are not limited to the soft labels from a teacher \nnetwork. Teacher's intermediate feature representations can better guide a \nstudent network to learn how to denoise or dereverberate noisy input. Second, \nthe proposed recursive architecture in the BridgeNet can iteratively improve \ndenoising and recognition performance. The experimental results of BridgeNet \nshowed significant improvements in tackling the distant speech recognition \nproblem, where it achieved up to 13.24% relative WER reductions on AMI corpus \ncompared to a baseline neural network without teacher's hints. \n</p>"}, "author": "Jaeyoung Kim, Mostafa El-Khamy, Jungwon Lee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509066299539", "timestampUsec": "1509066299538883", "id": "tag:google.com,2005:reader/item/0000000326bd5bdd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Benefits of Depth for Long-Term Memory of Recurrent Networks. (arXiv:1710.09431v1 [cs.LG])", "published": 1509066300, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09431"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c906f8e6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c906f8e6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The key attribute that drives the unprecedented success of modern Recurrent \nNeural Networks (RNNs) on learning tasks which involve sequential data, is \ntheir ever-improving ability to model intricate long-term temporal \ndependencies. However, an adequate measure of RNNs long-term memory capacity is \nlacking, and thus formal understanding of their ability to correlate data \nthroughout time is limited. Though depth efficiency in convolutional networks \nis well established, it does not suffice in order to account for the success of \ndeep RNNs on data of varying lengths, and the need to address their \n`time-series expressive power' arises. In this paper, we analyze the effect of \ndepth on the ability of recurrent networks to express correlations ranging over \nlong time-scales. To meet the above need, we introduce a measure of the \ninformation flow across time supported by the network, referred to as the \nStart-End separation rank. This measure essentially reflects the distance of \nthe function realized by the recurrent network from a function that models no \ninteraction whatsoever between the beginning and end of the input sequence. We \nprove that deep recurrent networks support Start-End separation ranks which are \nexponentially higher than those supported by their shallow counterparts. Thus, \nwe establish that depth brings forth an overwhelming advantage in the ability \nof recurrent networks to model long-term dependencies. Such analyses may be \nreadily extended to other RNN architectures of interest, e.g. variants of LSTM \nnetworks. We obtain our results by considering a class of recurrent networks \nreferred to as Recurrent Arithmetic Circuits (RACs), which merge the hidden \nstate with the input via the Multiplicative Integration operation. Finally, we \nmake use of the tool of quantum Tensor Networks to gain additional graphic \ninsight regarding the complexity brought forth by depth in recurrent networks. \n</p>"}, "author": "Yoav Levine, Or Sharir, Amnon Shashua", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509066299539", "timestampUsec": "1509066299538882", "id": "tag:google.com,2005:reader/item/0000000326bd5bf1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Rotational Unit of Memory. (arXiv:1710.09537v1 [cs.LG])", "published": 1509066300, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09537"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09537", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The concepts of unitary evolution matrices and associative memory have \nboosted the field of Recurrent Neural Networks (RNN) to state-of-the-art \nperformance in a variety of sequential tasks. However, RNN still have a limited \ncapacity to manipulate long-term memory. To bypass this weakness the most \nsuccessful applications of RNN use external techniques such as attention \nmechanisms. In this paper we propose a novel RNN model that unifies the \nstate-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM \nis its rotational operation, which is, naturally, a unitary matrix, providing \narchitectures with the power to learn long-term dependencies by overcoming the \nvanishing and exploding gradients problem. Moreover, the rotational unit also \nserves as associative memory. We evaluate our model on synthetic memorization, \nquestion answering and language modeling tasks. RUM learns the Copying Memory \ntask completely and improves the state-of-the-art result in the Recall task. \nRUM's performance in the bAbI Question Answering task is comparable to that of \nmodels with attention mechanism. We also improve the state-of-the-art result to \n1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB) \ntask, which is to signify the applications of RUM to real-world sequential \ndata. The universality of our construction, at the core of RNN, establishes RUM \nas a promising approach to language modeling, speech recognition and machine \ntranslation. \n</p>"}, "author": "Rumen Dangovski, Li Jing, Marin Soljacic", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509066299539", "timestampUsec": "1509066299538881", "id": "tag:google.com,2005:reader/item/0000000326bd5bfb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Biologically Inspired Feedforward Supervised Learning for Deep Self-Organizing Map Networks. (arXiv:1710.09574v1 [stat.ML])", "published": 1509066300, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09574"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09574", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this study, we propose a novel deep neural network and its supervised \nlearning method that uses a feedforward supervisory signal. The method is \ninspired by the human visual system and performs human-like association-based \nlearning without any backward error propagation. The feedforward supervisory \nsignal that produces the correct result is preceded by the target signal and \nassociates its confirmed label with the classification result of the target \nsignal. It effectively uses a large amount of information from the feedforward \nsignal, and forms a continuous and rich learning representation. The method is \nvalidated using visual recognition tasks on the MNIST handwritten dataset. \n</p>"}, "author": "Takashi Shinozaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509066299539", "timestampUsec": "1509066299538880", "id": "tag:google.com,2005:reader/item/0000000326bd5c04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "PDE-Net: Learning PDEs from Data. (arXiv:1710.09668v1 [math.NA])", "published": 1509066300, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09668"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09668", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present an initial attempt to learn evolution PDEs from \ndata. Inspired by the latest development of neural network designs in deep \nlearning, we propose a new feed-forward deep network, called PDE-Net, to \nfulfill two objectives at the same time: to accurately predict dynamics of \ncomplex systems and to uncover the underlying hidden PDE models. The basic idea \nof the proposed PDE-Net is to learn differential operators by learning \nconvolution kernels (filters), and apply neural networks or other machine \nlearning methods to approximate the unknown nonlinear responses. Comparing with \nexisting approaches, which either assume the form of the nonlinear response is \nknown or fix certain finite difference approximations of differential \noperators, our approach has the most flexibility by learning both differential \noperators and the nonlinear responses. A special feature of the proposed \nPDE-Net is that all filters are properly constrained, which enables us to \neasily identify the governing PDE models while still maintaining the expressive \nand predictive power of the network. These constrains are carefully designed by \nfully exploiting the relation between the orders of differential operators and \nthe orders of sum rules of filters (an important concept originated from \nwavelet theory). We also discuss relations of the PDE-Net with some existing \nnetworks in computer vision such as Network-In-Network (NIN) and Residual \nNeural Network (ResNet). Numerical experiments show that the PDE-Net has the \npotential to uncover the hidden PDE of the observed dynamics, and predict the \ndynamical behavior for a relatively long time, even in a noisy environment. \n</p>"}, "author": "Zichao Long, Yiping Lu, Xianzhong Ma, Bin Dong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509066299539", "timestampUsec": "1509066299538879", "id": "tag:google.com,2005:reader/item/0000000326bd5c19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the role of synaptic stochasticity in training low-precision neural networks. (arXiv:1710.09825v1 [cond-mat.dis-nn])", "published": 1509066300, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09825"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09825", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stochasticity and limited precision of synaptic weights in neural network \nmodels is a key aspect of both biological and hardware modeling of learning \nprocesses. Here we show that a neural network model with stochastic binary \nweights naturally gives prominence to exponentially rare dense regions of \nsolutions with a number of desirable properties such as robustness and good \ngeneralization per- formance, while typical solutions are isolated and hard to \nfind. Binary solutions of the standard perceptron problem are obtained from a \nsimple gradient descent procedure on a set of real values parametrizing a \nprobability distribution over the binary synapses. Both analytical and \nnumerical results are presented. An algorithmic extension aimed at training \ndiscrete deep neural networks is also investigated. \n</p>"}, "author": "Carlo Baldassi, Federica Gerace, Hilbert J. Kappen, Carlo Lucibello, Luca Saglietti, Enzo Tartaglione, Riccardo Zecchina", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858293", "id": "tag:google.com,2005:reader/item/0000000326bb094a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "mixup: Beyond Empirical Risk Minimization. (arXiv:1710.09412v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09412"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09412", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Large deep neural networks are powerful, but exhibit undesirable behaviors \nsuch as memorization and sensitivity to adversarial examples. In this work, we \npropose mixup, a simple learning principle to alleviate these issues. In \nessence, mixup trains a neural network on convex combinations of pairs of \nexamples and their labels. By doing so, mixup regularizes the neural network to \nfavor simple linear behavior in-between training examples. Our experiments on \nthe ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show \nthat mixup improves the generalization of state-of-the-art neural network \narchitectures. We also find that mixup reduces the memorization of corrupt \nlabels, increases the robustness to adversarial examples, and stabilizes the \ntraining of generative adversarial networks. \n</p>"}, "author": "Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858292", "id": "tag:google.com,2005:reader/item/0000000326bb0974", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "DPCA: Dimensionality Reduction for Discriminative Analytics of Multiple Large-Scale Datasets. (arXiv:1710.09429v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09429"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09429", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Principal component analysis (PCA) has well-documented merits for data \nextraction and dimensionality reduction. PCA deals with a single dataset at a \ntime, and it is challenged when it comes to analyzing multiple datasets. Yet in \ncertain setups, one wishes to extract the most significant information of one \ndataset relative to other datasets. Specifically, the interest may be on \nidentifying, namely extracting features that are specific to a single target \ndataset but not the others. This paper develops a novel approach for such \nso-termed discriminative data analysis, and establishes its optimality in the \nleast-squares (LS) sense under suitable data modeling assumptions. The \ncriterion reveals linear combinations of variables by maximizing the ratio of \nthe variance of the target data to that of the remainders. The novel approach \nsolves a generalized eigenvalue problem by performing SVD just once. Numerical \ntests using synthetic and real datasets showcase the merits of the proposed \napproach relative to its competing alternatives. \n</p>"}, "author": "Gang Wang, Jia Chen, Georgios B. Giannakis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858291", "id": "tag:google.com,2005:reader/item/0000000326bb0981", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Markov Chain Theory Approach to Characterizing the Minimax Optimality of Stochastic Gradient Descent (for Least Squares). (arXiv:1710.09430v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09430"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09430", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This work provides a simplified proof of the statistical minimax optimality \nof (iterate averaged) stochastic gradient descent (SGD), for the special case \nof least squares. This result is obtained by analyzing SGD as a stochastic \nprocess and by sharply characterizing the stationary covariance matrix of this \nprocess. The finite rate optimality characterization captures the constant \nfactors and addresses model mis-specification. \n</p>"}, "author": "Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, Venkata Krishna Pillutla, Aaron Sidford", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858290", "id": "tag:google.com,2005:reader/item/0000000326bb098d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Malware Detection by Eating a Whole EXE. (arXiv:1710.09435v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09435"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09435", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work we introduce malware detection from raw byte sequences as a \nfruitful research area to the larger machine learning community. Building a \nneural network for such a problem presents a number of interesting challenges \nthat have not occurred in tasks such as image processing or NLP. In particular, \nwe note that detection from raw bytes presents a sequence problem with over two \nmillion time steps and a problem where batch normalization appear to hinder the \nlearning process. We present our initial work in building a solution to tackle \nthis problem, which has linear complexity dependence on the sequence length, \nand allows for interpretable sub-regions of the binary to be identified. In \ndoing so we will discuss the many challenges in building a neural network to \nprocess data at this scale, and the methods we used to work around them. \n</p>"}, "author": "Edward Raff, Jon Barker, Jared Sylvester, Robert Brandon, Bryan Catanzaro, Charles Nicholas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858289", "id": "tag:google.com,2005:reader/item/0000000326bb0998", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "General Bayesian Inference over the Stiefel Manifold via the Givens Transform. (arXiv:1710.09443v2 [stat.ML] UPDATED)", "published": 1511218849, "updated": 1511218853, "canonical": [{"href": "http://arxiv.org/abs/1710.09443"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09443", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce the Givens Transform, a novel transform between the space of \northonormal matrices and $\\mathbb{R}^D$. The Givens Transform allows for the \napplication of any general Bayesian inference algorithm to probabilistic models \ncontaining constrained unit-vectors or orthonormal matrix parameters. This \nincludes a variety of matrix factorizations and dimensionality reduction models \nsuch as Probabilistic PCA (PPCA), Exponential Family PPCA (BXPCA), and \nCanonical Correlation Analysis (CCA). While previous Bayesian approaches to \nthese models relied on separate sampling update rules for constrained and \nunconstrained parameters, the Givens Transform enables the treatment of \nunit-vectors and orthonormal matrices agnostically as unconstrained parameters. \nThus any Bayesian inference algorithm can be used on these models without \nmodification. This opens the door to not just sampling algorithms, but \nVariational Inference (VI) as well. We illustrate with several examples and \nsupplied code, how the Givens Transform allows end-users to easily build \ncomplex models in their favorite Bayesian modeling framework such as Stan, \nEdward, or PyMC3, a task that was previously intractable due to technical \nconstraints. \n</p>"}, "author": "Arya A Pourzanjani, Richard M Jiang, Brian Mitchell, Paul J Atzberger, Linda R Petzold", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858288", "id": "tag:google.com,2005:reader/item/0000000326bb09b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Non-convex Optimization with Strong High Probability Second-order Convergence. (arXiv:1710.09447v1 [math.OC])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09447"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09447", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c906fca2\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c906fca2&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we study stochastic non-convex optimization with non-convex \nrandom functions. Recent studies on non-convex optimization revolve around \nestablishing second-order convergence, i.e., converging to a nearly \nsecond-order optimal stationary points. However, existing results on stochastic \nnon-convex optimization are limited, especially with a high probability \nsecond-order convergence. We propose a novel updating step (named NCG-S) by \nleveraging a stochastic gradient and a noisy negative curvature of a stochastic \nHessian, where the stochastic gradient and Hessian are based on a proper \nmini-batch of random functions. Building on this step, we develop two \nalgorithms and establish their high probability second-order convergence. To \nthe best of our knowledge, the proposed stochastic algorithms are the first \nwith a second-order convergence in {\\it high probability} and a time complexity \nthat is {\\it almost linear} in the problem's dimensionality. \n</p>"}, "author": "Mingrui Liu, Tianbao Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858287", "id": "tag:google.com,2005:reader/item/0000000326bb09b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Representation Learning in Large Attributed Graphs. (arXiv:1710.09471v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09471"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09471", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c915091a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c915091a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Graphs (networks) are ubiquitous and allow us to model entities (nodes) and \nthe dependencies (edges) between them. Learning a useful feature representation \nfrom graph data lies at the heart and success of many machine learning tasks \nsuch as classification, anomaly detection, link prediction, among many others. \nMany existing techniques use random walks as a basis for learning features or \nestimating the parameters of a graph model for a downstream prediction task. \nExamples include recent node embedding methods such as DeepWalk, node2vec, as \nwell as graph-based deep learning algorithms. However, the simple random walk \nused by these methods is fundamentally tied to the identity of the node. This \nhas three main disadvantages. First, these approaches are inherently \ntransductive and do not generalize to unseen nodes and other graphs. Second, \nthey are not space-efficient as a feature vector is learned for each node which \nis impractical for large graphs. Third, most of these approaches lack support \nfor attributed graphs. \n</p> \n<p>To make these methods more generally applicable, we propose a framework based \non the notion of attributed random walk that is not tied to node identity and \nis instead based on learning a function $\\Phi : \\mathrm{\\rm \\bf x} \\rightarrow \nw$ that maps a node attribute vector $\\mathrm{\\rm \\bf x}$ to a type $w$. This \nframework serves as a basis for generalizing existing methods such as DeepWalk, \nnode2vec, and many other previous methods that leverage traditional random \nwalks. \n</p>"}, "author": "Nesreen K. Ahmed, Ryan A. Rossi, Rong Zhou, John Boaz Lee, Xiangnan Kong, Theodore L. Willke, Hoda Eldardiry", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858286", "id": "tag:google.com,2005:reader/item/0000000326bb09ba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reparameterizing the Birkhoff Polytope for Variational Permutation Inference. (arXiv:1710.09508v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09508"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09508", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many matching, tracking, sorting, and ranking problems require probabilistic \nreasoning about possible permutations, a set that grows factorially with \ndimension. Combinatorial optimization algorithms may enable efficient point \nestimation, but fully Bayesian inference poses a severe challenge in this \nhigh-dimensional, discrete space. To surmount this challenge, we start with the \nusual step of relaxing a discrete set (here, of permutation matrices) to its \nconvex hull, which here is the Birkhoff polytope: the set of all \ndoubly-stochastic matrices. We then introduce two novel transformations: first, \nan invertible and differentiable stick-breaking procedure that maps \nunconstrained space to the Birkhoff polytope; second, a map that rounds points \ntoward the vertices of the polytope. Both transformations include a temperature \nparameter that, in the limit, concentrates the densities on permutation \nmatrices. We then exploit these transformations and reparameterization \ngradients to introduce variational inference over permutation matrices, and we \ndemonstrate its utility in a series of experiments. \n</p>"}, "author": "Scott W. Linderman, Gonzalo E. Mena, Hal Cooper, Liam Paninski, John P. Cunningham", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858285", "id": "tag:google.com,2005:reader/item/0000000326bb09c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "InterpNET: Neural Introspection for Interpretable Deep Learning. (arXiv:1710.09511v2 [stat.ML] UPDATED)", "published": 1511218849, "updated": 1511218854, "canonical": [{"href": "http://arxiv.org/abs/1710.09511"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09511", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Humans are able to explain their reasoning. On the contrary, deep neural \nnetworks are not. This paper attempts to bridge this gap by introducing a new \nway to design interpretable neural networks for classification, inspired by \nphysiological evidence of the human visual system's inner-workings. This paper \nproposes a neural network design paradigm, termed InterpNET, which can be \ncombined with any existing classification architecture to generate natural \nlanguage explanations of the classifications. The success of the module relies \non the assumption that the network's computation and reasoning is represented \nin its internal layer activations. While in principle InterpNET could be \napplied to any existing classification architecture, it is evaluated via an \nimage classification and explanation task. Experiments on a CUB bird \nclassification and explanation dataset show qualitatively and quantitatively \nthat the model is able to generate high-quality explanations. While the current \nstate-of-the-art METEOR score on this dataset is 29.2, InterpNET achieves a \nmuch higher METEOR score of 37.9. \n</p>"}, "author": "Shane Barratt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858284", "id": "tag:google.com,2005:reader/item/0000000326bb09cf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Maximum Principle Based Algorithms for Deep Learning. (arXiv:1710.09513v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09513"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09513", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The continuous dynamical system approach to deep learning is explored in \norder to devise alternative frameworks for training algorithms. Training is \nrecast as a control problem and this allows us to formulate necessary \noptimality conditions in continuous time using the Pontryagin's maximum \nprinciple (PMP). A modification of the method of successive approximations is \nthen used to solve the PMP, giving rise to an alternative training algorithm \nfor deep learning. This approach has the advantage that rigorous error \nestimates and convergence results can be established. We also show that it may \navoid some pitfalls of gradient-based methods, such as slow convergence on flat \nlandscapes near saddle points. Furthermore, we demonstrate that it obtains \nfavorable initial convergence rate per-iteration, provided Hamiltonian \nmaximization can be efficiently carried out - a step which is still in need of \nimprovement. Overall, the approach opens up new avenues to attack problems \nassociated with deep learning, such as trapping in slow manifolds and \ninapplicability of gradient-based methods for discrete trainable variables. \n</p>"}, "author": "Qianxiao Li, Long Chen, Cheng Tai, E Weinan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858283", "id": "tag:google.com,2005:reader/item/0000000326bb09d8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Laplacian Prior Variational Automatic Relevance Determination for Transmission Tomography. (arXiv:1710.09522v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09522"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09522", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In the classic sparsity-driven problems, the fundamental L-1 penalty method \nhas been shown to have good performance in reconstructing signals for a wide \nrange of problems. However this performance relies on a good choice of penalty \nweight which is often found from empirical experiments. We propose an algorithm \ncalled the Laplacian variational automatic relevance determination (Lap-VARD) \nthat takes this penalty weight as a parameter of a prior Laplace distribution. \nOptimization of this parameter using an automatic relevance determination \nframework results in a balance between the sparsity and accuracy of signal \nreconstruction. Our algorithm is implemented in a transmission tomography model \nwith sparsity constraint in wavelet domain. \n</p>"}, "author": "Jingwei Lu, David G. Politte, Joseph A. O&#x27;Sullivan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858282", "id": "tag:google.com,2005:reader/item/0000000326bb09e2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Rotational Unit of Memory. (arXiv:1710.09537v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09537"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09537", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The concepts of unitary evolution matrices and associative memory have \nboosted the field of Recurrent Neural Networks (RNN) to state-of-the-art \nperformance in a variety of sequential tasks. However, RNN still have a limited \ncapacity to manipulate long-term memory. To bypass this weakness the most \nsuccessful applications of RNN use external techniques such as attention \nmechanisms. In this paper we propose a novel RNN model that unifies the \nstate-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM \nis its rotational operation, which is, naturally, a unitary matrix, providing \narchitectures with the power to learn long-term dependencies by overcoming the \nvanishing and exploding gradients problem. Moreover, the rotational unit also \nserves as associative memory. We evaluate our model on synthetic memorization, \nquestion answering and language modeling tasks. RUM learns the Copying Memory \ntask completely and improves the state-of-the-art result in the Recall task. \nRUM's performance in the bAbI Question Answering task is comparable to that of \nmodels with attention mechanism. We also improve the state-of-the-art result to \n1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB) \ntask, which is to signify the applications of RUM to real-world sequential \ndata. The universality of our construction, at the core of RNN, establishes RUM \nas a promising approach to language modeling, speech recognition and machine \ntranslation. \n</p>"}, "author": "Rumen Dangovski, Li Jing, Marin Soljacic", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858281", "id": "tag:google.com,2005:reader/item/0000000326bb09e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior. (arXiv:1710.09553v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09553"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09553", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe an approach to understand the peculiar and counterintuitive \ngeneralization properties of deep neural networks. The approach involves going \nbeyond worst-case theoretical capacity control frameworks that have been \npopular in machine learning in recent years to revisit old ideas in the \nstatistical mechanics of neural networks. Within this approach, we present a \nprototypical Very Simple Deep Learning (VSDL) model, whose behavior is \ncontrolled by two control parameters, one describing an effective amount of \ndata, or load, on the network (that decreases when noise is added to the \ninput), and one with an effective temperature interpretation (that increases \nwhen algorithms are early stopped). Using this model, we describe how a very \nsimple application of ideas from the statistical mechanics theory of \ngeneralization provides a strong qualitative description of recently-observed \nempirical results regarding the inability of deep neural networks not to \noverfit training data, discontinuous learning and sharp transitions in the \ngeneralization properties of learning algorithms, etc. \n</p>"}, "author": "Charles H. Martin, Michael W. Mahoney", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858280", "id": "tag:google.com,2005:reader/item/0000000326bb0a00", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Duality-free Methods for Stochastic Composition Optimization. (arXiv:1710.09554v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09554"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09554", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the composition optimization with two expected-value functions in \nthe form of $\\frac{1}{n}\\sum\\nolimits_{i = 1}^n F_i(\\frac{1}{m}\\sum\\nolimits_{j \n= 1}^m G_j(x))+R(x)$, { which formulates many important problems in statistical \nlearning and machine learning such as solving Bellman equations in \nreinforcement learning and nonlinear embedding}. Full Gradient or classical \nstochastic gradient descent based optimization algorithms are unsuitable or \ncomputationally expensive to solve this problem due to the inner expectation \n$\\frac{1}{m}\\sum\\nolimits_{j = 1}^m G_j(x)$. We propose a duality-free based \nstochastic composition method that combines variance reduction methods to \naddress the stochastic composition problem. We apply SVRG and SAGA based \nmethods to estimate the inner function, and duality-free method to estimate the \nouter function. We prove the linear convergence rate not only for the convex \ncomposition problem, but also for the case that the individual outer functions \nare non-convex while the objective function is strongly-convex. We also provide \nthe results of experiments that show the effectiveness of our proposed methods. \n</p>"}, "author": "Liu Liu, Ji Liu, Dacheng Tao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858279", "id": "tag:google.com,2005:reader/item/0000000326bb0a04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Big Data Classification Using Augmented Decision Trees. (arXiv:1710.09567v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09567"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09567", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present an algorithm for classification tasks on big data. Experiments \nconducted as part of this study indicate that the algorithm can be as accurate \nas ensemble methods such as random forests or gradient boosted trees. Unlike \nensemble methods, the models produced by the algorithm can be easily \ninterpreted. The algorithm is based on a divide and conquer strategy and \nconsists of two steps. The first step consists of using a decision tree to \nsegment the large dataset. By construction, decision trees attempt to create \nhomogeneous class distributions in their leaf nodes. However, non-homogeneous \nleaf nodes are usually produced. The second step of the algorithm consists of \nusing a suitable classifier to determine the class labels for the \nnon-homogeneous leaf nodes. The decision tree segment provides a coarse segment \nprofile while the leaf level classifier can provide information about the \nattributes that affect the label within a segment. \n</p>"}, "author": "Rajiv Sambasivan, Sourish Das", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858278", "id": "tag:google.com,2005:reader/item/0000000326bb0a11", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Biologically Inspired Feedforward Supervised Learning for Deep Self-Organizing Map Networks. (arXiv:1710.09574v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09574"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09574", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9150c9d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9150c9d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this study, we propose a novel deep neural network and its supervised \nlearning method that uses a feedforward supervisory signal. The method is \ninspired by the human visual system and performs human-like association-based \nlearning without any backward error propagation. The feedforward supervisory \nsignal that produces the correct result is preceded by the target signal and \nassociates its confirmed label with the classification result of the target \nsignal. It effectively uses a large amount of information from the feedforward \nsignal, and forms a continuous and rich learning representation. The method is \nvalidated using visual recognition tasks on the MNIST handwritten dataset. \n</p>"}, "author": "Takashi Shinozaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858277", "id": "tag:google.com,2005:reader/item/0000000326bb0a1a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Watch Your Step: Learning Graph Embeddings Through Attention. (arXiv:1710.09599v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09599"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09599", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Graph embedding methods represent nodes in a continuous vector space, \npreserving information from the graph (e.g. by sampling random walks). There \nare many hyper-parameters to these methods (such as random walk length) which \nhave to be manually tuned for every graph. In this paper, we replace random \nwalk hyper-parameters with trainable parameters that we automatically learn via \nbackpropagation. In particular, we learn a novel attention model on the power \nseries of the transition matrix, which guides the random walk to optimize an \nupstream objective. Unlike previous approaches to attention models, the method \nthat we propose utilizes attention parameters exclusively on the data (e.g. on \nthe random walk), and not used by the model for inference. We experiment on \nlink prediction tasks, as we aim to produce embeddings that best-preserve the \ngraph structure, generalizing to unseen information. We improve \nstate-of-the-art on a comprehensive suite of real world datasets including \nsocial, collaboration, and biological networks. Adding attention to random \nwalks can reduce the error by 20% to 45% on datasets we attempted. Further, our \nlearned attention parameters are different for every graph, and our \nautomatically-found values agree with the optimal choice of hyper-parameter if \nwe manually tune existing methods. \n</p>"}, "author": "Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, Alex Alemi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858276", "id": "tag:google.com,2005:reader/item/0000000326bb0a23", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Segment Parameter Labelling in MCMC Mean-Shift Change Detection. (arXiv:1710.09657v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09657"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09657", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This work addresses the problem of segmentation in time series data with \nrespect to a statistical parameter of interest in Bayesian models. It is common \nto assume that the parameters are distinct within each segment. As such, many \nBayesian change point detection models do not exploit the segment parameter \npatterns, which can improve performance. This work proposes a Bayesian \nmean-shift change point detection algorithm that makes use of repetition in \nsegment parameters, by introducing segment class labels that utilise a \nDirichlet process prior. The performance of the proposed approach was assessed \non both synthetic and real world data, highlighting the enhanced performance \nwhen using parameter labelling. \n</p>"}, "author": "Alireza Ahrabian, Shirin Enshaeifar, Clive Cheong-Took, Payam Barnaghi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858275", "id": "tag:google.com,2005:reader/item/0000000326bb0a28", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "PDE-Net: Learning PDEs from Data. (arXiv:1710.09668v1 [math.NA])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09668"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09668", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present an initial attempt to learn evolution PDEs from \ndata. Inspired by the latest development of neural network designs in deep \nlearning, we propose a new feed-forward deep network, called PDE-Net, to \nfulfill two objectives at the same time: to accurately predict dynamics of \ncomplex systems and to uncover the underlying hidden PDE models. The basic idea \nof the proposed PDE-Net is to learn differential operators by learning \nconvolution kernels (filters), and apply neural networks or other machine \nlearning methods to approximate the unknown nonlinear responses. Comparing with \nexisting approaches, which either assume the form of the nonlinear response is \nknown or fix certain finite difference approximations of differential \noperators, our approach has the most flexibility by learning both differential \noperators and the nonlinear responses. A special feature of the proposed \nPDE-Net is that all filters are properly constrained, which enables us to \neasily identify the governing PDE models while still maintaining the expressive \nand predictive power of the network. These constrains are carefully designed by \nfully exploiting the relation between the orders of differential operators and \nthe orders of sum rules of filters (an important concept originated from \nwavelet theory). We also discuss relations of the PDE-Net with some existing \nnetworks in computer vision such as Network-In-Network (NIN) and Residual \nNeural Network (ResNet). Numerical experiments show that the PDE-Net has the \npotential to uncover the hidden PDE of the observed dynamics, and predict the \ndynamical behavior for a relatively long time, even in a noisy environment. \n</p>"}, "author": "Zichao Long, Yiping Lu, Xianzhong Ma, Bin Dong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858274", "id": "tag:google.com,2005:reader/item/0000000326bb0a2f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Weighting Scheme for a Pairwise Multi-label Classifier Based on the Fuzzy Confusion Matrix. (arXiv:1710.09710v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09710"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09710", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work we addressed the issue of applying a stochastic classifier and a \nlocal, fuzzy confusion matrix under the framework of multi-label \nclassification. We proposed a novel solution to the problem of correcting label \npairwise ensembles. The main step of the correction procedure is to compute \nclassifier-specific competence and cross-competence measures, which estimates \nerror pattern of the underlying classifier. At the fusion phase we employed two \nweighting approaches based on information theory. The classifier weights \npromote base classifiers which are the most susceptible to the correction based \non the fuzzy confusion matrix. During the experimental study, the proposed \napproach was compared against two reference methods. The comparison was made in \nterms of six different quality criteria. The conducted experiments reveals that \nthe proposed approach eliminates one of main drawbacks of the original \nFCM-based approach i.e. the original approach is vulnerable to the imbalanced \nclass/label distribution. What is more, the obtained results shows that the \nintroduced method achieves satisfying classification quality under all \nconsidered quality criteria. Additionally, the impact of fluctuations of data \nset characteristics is reduced. \n</p>"}, "author": "Pawel Trajdos, Marek Kurzynski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858273", "id": "tag:google.com,2005:reader/item/0000000326bb0a34", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "From Distance Correlation to Multiscale Generalized Correlation. (arXiv:1710.09768v1 [stat.ML])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09768"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09768", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Understanding and developing a correlation measure that can detect general \ndependencies is not only imperative to statistics and machine learning, but \nalso crucial to general scientific discovery in the big data age. We proposed \nthe Multiscale Generalized Correlation (MGC) in Shen et al. 2017 as a novel \ncorrelation measure, which worked well empirically and helped a number of real \ndata discoveries. But there is a wide gap with respect to the theoretical side, \ne.g., the population statistic, the convergence from sample to population, how \nwell does the algorithmic Sample MGC perform, etc. To better understand its \nunderlying mechanism, in this paper we formalize the population version of \nlocal distance correlations, MGC, and the optimal local scale between the \nunderlying random variables, by utilizing the characteristic functions and \nincorporating the nearest-neighbor machinery. The population version enables a \nseamless connection with, and significant improvement to, the algorithmic \nSample MGC, both theoretically and in practice, which further allows a number \nof desirable asymptotic and finite-sample properties to be proved and explored \nfor MGC. The advantages of MGC are further illustrated via a comprehensive set \nof simulations with linear, nonlinear, univariate, multivariate, and noisy \ndependencies, where it loses almost no power against monotone dependencies \nwhile achieving superior performance against general dependencies. \n</p>"}, "author": "Cencheng Shen, Carey E. Priebe, Joshua T. Vogelstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858272", "id": "tag:google.com,2005:reader/item/0000000326bb0a3a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal Shrinkage of Singular Values Under Random Data Contamination. (arXiv:1710.09787v2 [cs.IT] UPDATED)", "published": 1511308877, "updated": 1511308892, "canonical": [{"href": "http://arxiv.org/abs/1710.09787"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09787", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A low rank matrix X has been contaminated by uniformly distributed noise, \nmissing values, outliers and corrupt entries. Reconstruction of X from the \nsingular values and singular vectors of the contaminated matrix Y is a key \nproblem in machine learning, computer vision and data science. In this paper we \nshow that common contamination models (including arbitrary combinations of \nuniform noise,missing values, outliers and corrupt entries) can be described \nefficiently using a single framework. We develop an asymptotically optimal \nalgorithm that estimates X by manipulation of the singular values of Y , which \napplies to any of the contamination models considered. Finally, we find an \nexplicit signal-to-noise cutoff, below which estimation of X from the singular \nvalue decomposition of Y must fail, in a well-defined sense. \n</p>"}, "author": "Danny Barash, Matan Gavish", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858271", "id": "tag:google.com,2005:reader/item/0000000326bb0a43", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improving Negative Sampling for Word Representation using Self-embedded Features. (arXiv:1710.09805v1 [cs.LG])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09805"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09805", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Although the word-popularity based negative sampler has shown superb \nperformance in the skip-gram model, the theoretical motivation behind \noversampling popular (non-observed) words as negative samples is still not well \nunderstood. In this paper, we start from an investigation of the gradient \nvanishing issue in the skip-gram model without a proper negative sampler. By \nperforming an insightful analysis from the stochastic gradient descent (SGD) \nlearning perspective, we demonstrate that, both theoretically and intuitively, \nnegative samples with larger inner product scores are more informative than \nthose with lower scores for the SGD learner in terms of both convergence rate \nand accuracy. Understanding this, we propose an alternative sampling algorithm \nthat dynamically selects informative negative samples during each SGD update. \nMore importantly, the proposed sampler accounts for multi-dimensional \nself-embedded features during the sampling process, which essentially makes it \nmore effective than the original popularity-based (one-dimensional) sampler. \nEmpirical experiments further verify our observations, and show that our \nfine-grained samplers gain significant improvement over the existing ones \nwithout increasing computational complexity. \n</p>"}, "author": "Long Chen, Fajie Yuan, Joemon M. Jose, Weinan Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858270", "id": "tag:google.com,2005:reader/item/0000000326bb0a47", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Joint Screening Tests for LASSO. (arXiv:1710.09809v2 [cs.LG] UPDATED)", "published": 1510319034, "updated": 1510319036, "canonical": [{"href": "http://arxiv.org/abs/1710.09809"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09809", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper focusses on \"safe\" screening techniques for the LASSO problem. \nMotivated by the need for low-complexity algorithms, we propose a new approach, \ndubbed \"joint\" screening test, allowing to screen a set of atoms by carrying \nout one single test. The approach is particularized to two different sets of \natoms, respectively expressed as sphere and dome regions. After presenting the \nmathematical derivations of the tests, we elaborate on their relative \neffectiveness and discuss the practical use of such procedures. \n</p>"}, "author": "C. Herzet, A. Dr&#xe9;meau", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064798858", "timestampUsec": "1509064798858269", "id": "tag:google.com,2005:reader/item/0000000326bb0a4b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the role of synaptic stochasticity in training low-precision neural networks. (arXiv:1710.09825v1 [cond-mat.dis-nn])", "published": 1509064799, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09825"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09825", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stochasticity and limited precision of synaptic weights in neural network \nmodels is a key aspect of both biological and hardware modeling of learning \nprocesses. Here we show that a neural network model with stochastic binary \nweights naturally gives prominence to exponentially rare dense regions of \nsolutions with a number of desirable properties such as robustness and good \ngeneralization per- formance, while typical solutions are isolated and hard to \nfind. Binary solutions of the standard perceptron problem are obtained from a \nsimple gradient descent procedure on a set of real values parametrizing a \nprobability distribution over the binary synapses. Both analytical and \nnumerical results are presented. An algorithmic extension aimed at training \ndiscrete deep neural networks is also investigated. \n</p>"}, "author": "Carlo Baldassi, Federica Gerace, Hilbert J. Kappen, Carlo Lucibello, Luca Saglietti, Enzo Tartaglione, Riccardo Zecchina", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064764149", "timestampUsec": "1509064764149170", "id": "tag:google.com,2005:reader/item/0000000326bafaf5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Representation Learning in Large Attributed Graphs. (arXiv:1710.09471v1 [stat.ML])", "published": 1509064764, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09471"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09471", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c915101d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c915101d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Graphs (networks) are ubiquitous and allow us to model entities (nodes) and \nthe dependencies (edges) between them. Learning a useful feature representation \nfrom graph data lies at the heart and success of many machine learning tasks \nsuch as classification, anomaly detection, link prediction, among many others. \nMany existing techniques use random walks as a basis for learning features or \nestimating the parameters of a graph model for a downstream prediction task. \nExamples include recent node embedding methods such as DeepWalk, node2vec, as \nwell as graph-based deep learning algorithms. However, the simple random walk \nused by these methods is fundamentally tied to the identity of the node. This \nhas three main disadvantages. First, these approaches are inherently \ntransductive and do not generalize to unseen nodes and other graphs. Second, \nthey are not space-efficient as a feature vector is learned for each node which \nis impractical for large graphs. Third, most of these approaches lack support \nfor attributed graphs. \n</p> \n<p>To make these methods more generally applicable, we propose a framework based \non the notion of attributed random walk that is not tied to node identity and \nis instead based on learning a function $\\Phi : \\mathrm{\\rm \\bf x} \\rightarrow \nw$ that maps a node attribute vector $\\mathrm{\\rm \\bf x}$ to a type $w$. This \nframework serves as a basis for generalizing existing methods such as DeepWalk, \nnode2vec, and many other previous methods that leverage traditional random \nwalks. \n</p>"}, "author": "Nesreen K. Ahmed, Ryan A. Rossi, Rong Zhou, John Boaz Lee, Xiangnan Kong, Theodore L. Willke, Hoda Eldardiry", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064764149", "timestampUsec": "1509064764149169", "id": "tag:google.com,2005:reader/item/0000000326bafafb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Context-Aware Generative Adversarial Privacy. (arXiv:1710.09549v2 [cs.LG] UPDATED)", "published": 1509437322, "updated": 1509437326, "canonical": [{"href": "http://arxiv.org/abs/1710.09549"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09549", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c92156f1\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c92156f1&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Preserving the utility of published datasets while simultaneously providing \nprovable privacy guarantees is a well-known challenge. On the one hand, \ncontext-free privacy solutions, such as differential privacy, provide strong \nprivacy guarantees, but often lead to a significant reduction in utility. On \nthe other hand, context-aware privacy solutions, such as information theoretic \nprivacy, achieve an improved privacy-utility tradeoff, but assume that the data \nholder has access to dataset statistics. We circumvent these limitations by \nintroducing a novel context-aware privacy framework called generative \nadversarial privacy (GAP). GAP leverages recent advancements in generative \nadversarial networks (GANs) to allow the data holder to learn privatization \nschemes from the dataset itself. Under GAP, learning the privacy mechanism is \nformulated as a constrained minimax game between two players: a privatizer that \nsanitizes the dataset in a way that limits the risk of inference attacks on the \nindividuals' private variables, and an adversary that tries to infer the \nprivate variables from the sanitized dataset. To evaluate GAP's performance, we \ninvestigate two simple (yet canonical) statistical dataset models: (a) the \nbinary data model, and (b) the binary Gaussian mixture model. For both models, \nwe derive game-theoretically optimal minimax privacy mechanisms, and show that \nthe privacy mechanisms learned from data (in a generative adversarial fashion) \nmatch the theoretically optimal ones. This demonstrates that our framework can \nbe easily applied in practice, even in the absence of dataset statistics. \n</p>"}, "author": "Chong Huang, Peter Kairouz, Xiao Chen, Lalitha Sankar, Ram Rajagopal", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064764149", "timestampUsec": "1509064764149168", "id": "tag:google.com,2005:reader/item/0000000326bafb04", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Duality-free Methods for Stochastic Composition Optimization. (arXiv:1710.09554v1 [stat.ML])", "published": 1509064764, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09554"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09554", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the composition optimization with two expected-value functions in \nthe form of $\\frac{1}{n}\\sum\\nolimits_{i = 1}^n F_i(\\frac{1}{m}\\sum\\nolimits_{j \n= 1}^m G_j(x))+R(x)$, { which formulates many important problems in statistical \nlearning and machine learning such as solving Bellman equations in \nreinforcement learning and nonlinear embedding}. Full Gradient or classical \nstochastic gradient descent based optimization algorithms are unsuitable or \ncomputationally expensive to solve this problem due to the inner expectation \n$\\frac{1}{m}\\sum\\nolimits_{j = 1}^m G_j(x)$. We propose a duality-free based \nstochastic composition method that combines variance reduction methods to \naddress the stochastic composition problem. We apply SVRG and SAGA based \nmethods to estimate the inner function, and duality-free method to estimate the \nouter function. We prove the linear convergence rate not only for the convex \ncomposition problem, but also for the case that the individual outer functions \nare non-convex while the objective function is strongly-convex. We also provide \nthe results of experiments that show the effectiveness of our proposed methods. \n</p>"}, "author": "Liu Liu, Ji Liu, Dacheng Tao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064764149", "timestampUsec": "1509064764149167", "id": "tag:google.com,2005:reader/item/0000000326bafb0b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SRE: Semantic Rules Engine For the Industrial Internet-Of-Things Gateways. (arXiv:1710.09627v1 [cs.AI])", "published": 1509064764, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09627"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09627", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Advent of the Internet-of-Things (IoT) paradigm has brought opportunities \nto solve many real-world problems. Energy management, for example, has \nattracted huge interest from academia, industries, governments and regulatory \nbodies. It involves collecting energy usage data, analyzing it, and optimizing \nthe energy consumption by applying control strategies. However, in industrial \nenvironments, performing such optimization is not trivial. The changes in \nbusiness rules, process control, and customer requirements make it much more \nchallenging. In this paper, a Semantic Rules Engine (SRE) for industrial \ngateways is presented that allows implementing dynamic and flexible rule-based \ncontrol strategies. It is simple, expressive, and allows managing rules \non-the-fly without causing any service interruption. Additionally, it can \nhandle semantic queries and provide results by inferring additional knowledge \nfrom previously defined concepts in ontologies. SRE has been validated and \ntested on different hardware platforms and in commercial products. Performance \nevaluations are also presented to validate its conformance to the customer \nrequirements. \n</p>"}, "author": "Charbel El Kaed, Imran Khan, Andre Van Den Berg, Hicham Hossayni, Christophe Saint-Marcel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064764149", "timestampUsec": "1509064764149166", "id": "tag:google.com,2005:reader/item/0000000326bafb10", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "FashionBrain Project: A Vision for Understanding Europe's Fashion Data Universe. (arXiv:1710.09788v1 [cs.AI])", "published": 1509064764, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09788"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09788", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A core business in the fashion industry is the understanding and prediction \nof customer needs and trends. Search engines and social networks are at the \nsame time a fundamental bridge and a costly middleman between the customer's \npurchase intention and the retailer. To better exploit Europe's distinctive \ncharacteristics e.g., multiple languages, fashion and cultural differences, it \nis pivotal to reduce retailers' dependence to search engines. This goal can be \nachieved by harnessing various data channels (manufacturers and distribution \nnetworks, online shops, large retailers, social media, market observers, call \ncenters, press/magazines etc.) that retailers can leverage in order to gain \nmore insight about potential buyers, and on the industry trends as a whole. \nThis can enable the creation of novel on-line shopping experiences, the \ndetection of influencers, and the prediction of upcoming fashion trends. \n</p> \n<p>In this paper, we provide an overview of the main research challenges and an \nanalysis of the most promising technological solutions that we are \ninvestigating in the FashionBrain project. \n</p>"}, "author": "Alessandro Checco, Gianluca Demartini, Alexander Loeser, Ines Arous, Mourad Khayati, Matthias Dantone, Richard Koopmanschap, Svetlin Stalinov, Martin Kersten, Ying Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1509064764149", "timestampUsec": "1509064764149165", "id": "tag:google.com,2005:reader/item/0000000326bafb15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Klout Topics for Modeling Interests and Expertise of Users Across Social Networks. (arXiv:1710.09824v1 [cs.IR])", "published": 1509064764, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09824"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09824", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents Klout Topics, a lightweight ontology to describe social \nmedia users' topics of interest and expertise. Klout Topics is designed to: be \nhuman-readable and consumer-friendly; cover multiple domains of knowledge in \ndepth; and promote data extensibility via knowledge base entities. We discuss \nwhy this ontology is well-suited for text labeling and interest modeling \napplications, and how it compares to available alternatives. We show its \ncoverage against common social media interest sets, and examples of how it is \nused to model the interests of over 780M social media users on Klout.com. \nFinally, we open the ontology for external use. \n</p>"}, "author": "Sarah Ellinger, Prantik Bhattacharyya, Preeti Bhargava, Nemanja Spasojevic", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508980048582", "timestampUsec": "1508980048582302", "id": "tag:google.com,2005:reader/item/00000003260e5194", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI Big Data Analytics. (arXiv:1710.08961v1 [cs.DC])", "published": 1508980049, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08961"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08961", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years, analyzing task-based fMRI (tfMRI) data has become an \nessential tool for understanding brain function and networks. However, due to \nthe sheer size of tfMRI data, its intrinsic complex structure, and lack of \nground truth of underlying neural activities, modeling tfMRI data is hard and \nchallenging. Previously proposed data-modeling methods including Independent \nComponent Analysis (ICA) and Sparse Dictionary Learning only provided a weakly \nestablished model based on blind source separation under the strong assumption \nthat original fMRI signals could be linearly decomposed into time series \ncomponents with corresponding spatial maps. Meanwhile, analyzing and learning a \nlarge amount of tfMRI data from a variety of subjects has been shown to be very \ndemanding but yet challenging even with technological advances in computational \nhardware. Given the Convolutional Neural Network (CNN), a robust method for \nlearning high-level abstractions from low-level data such as tfMRI time series, \nin this work we propose a fast and scalable novel framework for distributed \ndeep Convolutional Autoencoder model. This model aims to both learn the complex \nhierarchical structure of the tfMRI data and to leverage the processing power \nof multiple GPUs in a distributed fashion. To implement such a model, we have \ncreated an enhanced processing pipeline on the top of Apache Spark and \nTensorflow library, leveraging from a very large cluster of GPU machines. \nExperimental data from applying the model on the Human Connectome Project (HCP) \nshow that the proposed model is efficient and scalable toward tfMRI big data \nanalytics, thus enabling data-driven extraction of hierarchical neuroscientific \ninformation from massive fMRI big data in the future. \n</p>"}, "author": "Milad Makkie, Heng Huang, Yu Zhao, Athanasios V. Vasilakos, Tianming Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508980048582", "timestampUsec": "1508980048582301", "id": "tag:google.com,2005:reader/item/00000003260e5196", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adversarial Deep Structured Nets for Mass Segmentation from Mammograms. (arXiv:1710.09288v1 [cs.CV])", "published": 1508980049, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09288"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09288", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Mass segmentation provides effective morphological features which are \nimportant for mass diagnosis. In this work, we propose a novel end-to-end \nnetwork for mammographic mass segmentation which employs a fully convolutional \nnetwork (FCN) to model a potential function, followed by a CRF to perform \nstructured learning. Because the mass distribution varies greatly with pixel \nposition, the FCN is combined with a position priori. Further, we employ \nadversarial training to eliminate over-fitting due to the small sizes of \nmammogram datasets. Multi-scale FCN is employed to improve the segmentation \nperformance. Experimental results on two public datasets, INbreast and \nDDSM-BCRP, demonstrate that our end-to-end network achieves better performance \nthan state-of-the-art approaches. \n\\footnote{https://github.com/wentaozhu/adversarial-deep-structural-networks.git} \n</p>"}, "author": "Wentao Zhu, Xiang Xiang, Trac D. Tran, Gregory D. Hager, Xiaohui Xie", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508980048582", "timestampUsec": "1508980048582300", "id": "tag:google.com,2005:reader/item/00000003260e5199", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Feature learning in feature-sample networks using multi-objective optimization. (arXiv:1710.09300v1 [cs.AI])", "published": 1508980049, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09300"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09300", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Data and knowledge representation are fundamental concepts in machine \nlearning. The quality of the representation impacts the performance of the \nlearning model directly. Feature learning transforms or enhances raw data to \nstructures that are effectively exploited by those models. In recent years, \nseveral works have been using complex networks for data representation and \nanalysis. However, no feature learning method has been proposed for such \ncategory of techniques. Here, we present an unsupervised feature learning \nmechanism that works on datasets with binary features. First, the dataset is \nmapped into a feature--sample network. Then, a multi-objective optimization \nprocess selects a set of new vertices to produce an enhanced version of the \nnetwork. The new features depend on a nonlinear function of a combination of \npreexisting features. Effectively, the process projects the input data into a \nhigher-dimensional space. To solve the optimization problem, we design two \nmetaheuristics based on the lexicographic genetic algorithm and the improved \nstrength Pareto evolutionary algorithm (SPEA2). We show that the enhanced \nnetwork contains more information and can be exploited to improve the \nperformance of machine learning methods. The advantages and disadvantages of \neach optimization strategy are discussed. \n</p>"}, "author": "Filipe Alves Neto Verri, Renato Tin&#xf3;s, Liang Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978243979", "timestampUsec": "1508978243979485", "id": "tag:google.com,2005:reader/item/00000003260b690f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention. (arXiv:1710.08969v1 [cs.SD])", "published": 1508978244, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08969"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08969", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper describes a novel text-to-speech (TTS) technique based on deep \nconvolutional neural networks (CNN), without any recurrent units. Recurrent \nneural network (RNN) has been a standard technique to model sequential data \nrecently, and this technique has been used in some cutting-edge neural TTS \ntechniques. However, training RNN component often requires a very powerful \ncomputer, or very long time typically several days or weeks. Recent other \nstudies, on the other hand, have shown that CNN-based sequence synthesis can be \nmuch faster than RNN-based techniques, because of high parallelizability. The \nobjective of this paper is to show an alternative neural TTS system, based only \non CNN, that can alleviate these economic costs of training. In our experiment, \nthe proposed Deep Convolutional TTS can be sufficiently trained only in a night \n(15 hours), using an ordinary gaming PC equipped with two GPUs, while the \nquality of the synthesized speech was almost acceptable. \n</p>"}, "author": "Hideyuki Tachibana, Katsuya Uenoyama, Shunsuke Aihara", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978243979", "timestampUsec": "1508978243979484", "id": "tag:google.com,2005:reader/item/00000003260b6933", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-Objective Approaches to Markov Decision Processes with Uncertain Transition Parameters. (arXiv:1710.08986v1 [cs.AI])", "published": 1508978244, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08986"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08986", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9215bd3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9215bd3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Markov decision processes (MDPs) are a popular model for performance analysis \nand optimization of stochastic systems. The parameters of stochastic behavior \nof MDPs are estimates from empirical observations of a system; their values are \nnot known precisely. Different types of MDPs with uncertain, imprecise or \nbounded transition rates or probabilities and rewards exist in the literature. \n</p> \n<p>Commonly, analysis of models with uncertainties amounts to searching for the \nmost robust policy which means that the goal is to generate a policy with the \ngreatest lower bound on performance (or, symmetrically, the lowest upper bound \non costs). However, hedging against an unlikely worst case may lead to losses \nin other situations. In general, one is interested in policies that behave well \nin all situations which results in a multi-objective view on decision making. \n</p> \n<p>In this paper, we consider policies for the expected discounted reward \nmeasure of MDPs with uncertain parameters. In particular, the approach is \ndefined for bounded-parameter MDPs (BMDPs) [8]. In this setting the worst, best \nand average case performances of a policy are analyzed simultaneously, which \nyields a multi-scenario multi-objective optimization problem. The paper \npresents and evaluates approaches to compute the pure Pareto optimal policies \nin the value vector space. \n</p>"}, "author": "Dimitri Scheftelowitsch, Peter Buchholz, Vahid Hashemi, Holger Hermanns", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978243979", "timestampUsec": "1508978243979483", "id": "tag:google.com,2005:reader/item/00000003260b694b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sufficient and necessary causation are dual. (arXiv:1710.09102v1 [cs.AI])", "published": 1508978244, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09102"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09102", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Causation has been the issue of philosophic debate since Hippocrates. Recent \nwork defines actual causation in terms of Pearl/Halpern's causality framework, \nformalizing necessary causes (IJCAI'15). This has inspired causality notions in \nthe security domain (CSF'15), which, perhaps surprisingly, formalize sufficient \ncauses instead. We provide an explicit relation between necessary and \nsufficient causes. \n</p>"}, "author": "Robert K&#xfc;nnemann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978243979", "timestampUsec": "1508978243979482", "id": "tag:google.com,2005:reader/item/00000003260b695d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Evidence of an exponential speed-up in the solution of hard optimization problems. (arXiv:1710.09278v1 [cs.AI])", "published": 1508978244, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09278"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09278", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Optimization problems pervade essentially every scientific discipline and \nindustry. Many such problems require finding a solution that maximizes the \nnumber of constraints satisfied. Often, these problems are particularly \ndifficult to solve because they belong to the NP-hard class, namely algorithms \nthat always find a solution in polynomial time are not known. Over the past \ndecades, research has focused on developing heuristic approaches that attempt \nto find an approximation to the solution. However, despite numerous research \nefforts, in many cases even approximations to the optimal solution are hard to \nfind, as the computational time for further refining a candidate solution grows \nexponentially with input size. Here, we show a non-combinatorial approach to \nhard optimization problems that achieves an exponential speed-up and finds \nbetter approximations than the current state-of-the-art. First, we map the \noptimization problem into a boolean circuit made of specially designed, \nself-organizing logic gates, which can be built with (non-quantum) electronic \ncomponents; the equilibrium points of the circuit represent the approximation \nto the problem at hand. Then, we solve its associated non-linear ordinary \ndifferential equations numerically, towards the equilibrium points. We \ndemonstrate this exponential gain by comparing a sequential MatLab \nimplementation of our solver with the winners of the 2016 Max-SAT competition \non a variety of hard optimization instances. We show empirical evidence that \nour solver scales linearly with the size of the problem, both in time and \nmemory, and argue that this property derives from the collective behavior of \nthe simulated physical circuit. Our approach can be applied to other types of \noptimization problems and the results presented here have far-reaching \nconsequences in many fields. \n</p>"}, "author": "Fabio L. Traversa, Pietro Cicotti, Forrest Sheldon, Massimiliano Di Ventra", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978243979", "timestampUsec": "1508978243979481", "id": "tag:google.com,2005:reader/item/00000003260b696f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Feature learning in feature-sample networks using multi-objective optimization. (arXiv:1710.09300v1 [cs.AI])", "published": 1508978244, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09300"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09300", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Data and knowledge representation are fundamental concepts in machine \nlearning. The quality of the representation impacts the performance of the \nlearning model directly. Feature learning transforms or enhances raw data to \nstructures that are effectively exploited by those models. In recent years, \nseveral works have been using complex networks for data representation and \nanalysis. However, no feature learning method has been proposed for such \ncategory of techniques. Here, we present an unsupervised feature learning \nmechanism that works on datasets with binary features. First, the dataset is \nmapped into a feature--sample network. Then, a multi-objective optimization \nprocess selects a set of new vertices to produce an enhanced version of the \nnetwork. The new features depend on a nonlinear function of a combination of \npreexisting features. Effectively, the process projects the input data into a \nhigher-dimensional space. To solve the optimization problem, we design two \nmetaheuristics based on the lexicographic genetic algorithm and the improved \nstrength Pareto evolutionary algorithm (SPEA2). We show that the enhanced \nnetwork contains more information and can be exploited to improve the \nperformance of machine learning methods. The advantages and disadvantages of \neach optimization strategy are discussed. \n</p>"}, "author": "Filipe Alves Neto Verri, Renato Tin&#xf3;s, Liang Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539635", "id": "tag:google.com,2005:reader/item/00000003260b2a6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Curvature-aided Incremental Aggregated Gradient Method. (arXiv:1710.08936v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08936"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08936", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a new algorithm for finite sum optimization which we call the \ncurvature-aided incremental aggregated gradient (CIAG) method. Motivated by the \nproblem of training a classifier for a d-dimensional problem, where the number \nof training data is $m$ and $m \\gg d \\gg 1$, the CIAG method seeks to \naccelerate incremental aggregated gradient (IAG) methods using aids from the \ncurvature (or Hessian) information, while avoiding the evaluation of matrix \ninverses required by the incremental Newton (IN) method. Specifically, our idea \nis to exploit the incrementally aggregated Hessian matrix to trace the full \ngradient vector at every incremental step, therefore achieving an improved \nlinear convergence rate over the state-of-the-art IAG methods. For strongly \nconvex problems, the fast linear convergence rate requires the objective \nfunction to be close to quadratic, or the initial point to be close to optimal \nsolution. Importantly, we show that running one iteration of the CIAG method \nyields the same improvement to the optimality gap as running one iteration of \nthe full gradient method, while the complexity is $O(d^2)$ for CIAG and $O(md)$ \nfor the full gradient. Overall, the CIAG method strikes a balance between the \nhigh computation complexity incremental Newton-type methods and the slow IAG \nmethod. Our numerical results support the theoretical findings and show that \nthe CIAG method often converges with much fewer iterations than IAG, and \nrequires much shorter running time than IN when the problem dimension is high. \n</p>"}, "author": "Hoi-To Wai, Wei Shi, Angelia Nedic, Anna Scaglione", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539634", "id": "tag:google.com,2005:reader/item/00000003260b2a75", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Estimating the Operating Characteristics of Ensemble Methods. (arXiv:1710.08952v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08952"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08952", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we present a technique for using the bootstrap to estimate the \noperating characteristics and their variability for certain types of ensemble \nmethods. Bootstrapping a model can require a huge amount of work if the \ntraining data set is large. Fortunately in many cases the technique lets us \ndetermine the effect of infinite resampling without actually refitting a single \nmodel. We apply the technique to the study of meta-parameter selection for \nrandom forests. We demonstrate that alternatives to bootstrap aggregation and \nto considering \\sqrt{d} features to split each node, where d is the number of \nfeatures, can produce improvements in predictive accuracy. \n</p>"}, "author": "Anthony Gamst, Jay-Calvin Reyes, Alden Walker", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539633", "id": "tag:google.com,2005:reader/item/00000003260b2a7a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI Big Data Analytics. (arXiv:1710.08961v1 [cs.DC])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08961"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08961", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years, analyzing task-based fMRI (tfMRI) data has become an \nessential tool for understanding brain function and networks. However, due to \nthe sheer size of tfMRI data, its intrinsic complex structure, and lack of \nground truth of underlying neural activities, modeling tfMRI data is hard and \nchallenging. Previously proposed data-modeling methods including Independent \nComponent Analysis (ICA) and Sparse Dictionary Learning only provided a weakly \nestablished model based on blind source separation under the strong assumption \nthat original fMRI signals could be linearly decomposed into time series \ncomponents with corresponding spatial maps. Meanwhile, analyzing and learning a \nlarge amount of tfMRI data from a variety of subjects has been shown to be very \ndemanding but yet challenging even with technological advances in computational \nhardware. Given the Convolutional Neural Network (CNN), a robust method for \nlearning high-level abstractions from low-level data such as tfMRI time series, \nin this work we propose a fast and scalable novel framework for distributed \ndeep Convolutional Autoencoder model. This model aims to both learn the complex \nhierarchical structure of the tfMRI data and to leverage the processing power \nof multiple GPUs in a distributed fashion. To implement such a model, we have \ncreated an enhanced processing pipeline on the top of Apache Spark and \nTensorflow library, leveraging from a very large cluster of GPU machines. \nExperimental data from applying the model on the Human Connectome Project (HCP) \nshow that the proposed model is efficient and scalable toward tfMRI big data \nanalytics, thus enabling data-driven extraction of hierarchical neuroscientific \ninformation from massive fMRI big data in the future. \n</p>"}, "author": "Milad Makkie, Heng Huang, Yu Zhao, Athanasios V. Vasilakos, Tianming Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539632", "id": "tag:google.com,2005:reader/item/00000003260b2a98", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Scaling Text with the Class Affinity Model. (arXiv:1710.08963v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08963"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08963", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Probabilistic methods for classifying text form a rich tradition in machine \nlearning and natural language processing. For many important problems, however, \nclass prediction is uninteresting because the class is known, and instead the \nfocus shifts to estimating latent quantities related to the text, such as \naffect or ideology. We focus on one such problem of interest, estimating the \nideological positions of 55 Irish legislators in the 1991 D\\'ail confidence \nvote. To solve the D\\'ail scaling problem and others like it, we develop a text \nmodeling framework that allows actors to take latent positions on a \"gray\" \nspectrum between \"black\" and \"white\" polar opposites. We are able to validate \nresults from this model by measuring the influences exhibited by individual \nwords, and we are able to quantify the uncertainty in the scaling estimates by \nusing a sentence-level block bootstrap. Applying our method to the D\\'ail \ndebate, we are able to scale the legislators between extreme pro-government and \npro-opposition in a way that reveals nuances in their speeches not captured by \ntheir votes or party affiliations. \n</p>"}, "author": "Patrick O. Perry, Kenneth Benoit", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539631", "id": "tag:google.com,2005:reader/item/00000003260b2aa9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Hidden Quantum Markov Models. (arXiv:1710.09016v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09016"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09016", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Hidden Quantum Markov Models (HQMMs) can be thought of as quantum \nprobabilistic graphical models that can model sequential data. We extend \nprevious work on HQMMs with three contributions: (1) we show how classical \nhidden Markov models (HMMs) can be simulated on a quantum circuit, (2) we \nreformulate HQMMs by relaxing the constraints for modeling HMMs on quantum \ncircuits, and (3) we present a learning algorithm to estimate the parameters of \nan HQMM from data. While our algorithm requires further optimization to handle \nlarger datasets, we are able to evaluate our algorithm using several synthetic \ndatasets. We show that on HQMM generated data, our algorithm learns HQMMs with \nthe same number of hidden states and predictive accuracy as the true HQMMs, \nwhile HMMs learned with the Baum-Welch algorithm require more states to match \nthe predictive accuracy. \n</p>"}, "author": "Siddarth Srinivasan, Geoff Gordon, Byron Boots", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539630", "id": "tag:google.com,2005:reader/item/00000003260b2ac0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Trace norm regularization and faster inference for embedded speech recognition RNNs. (arXiv:1710.09026v1 [cs.LG])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09026"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09026", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose and evaluate new techniques for compressing and speeding up dense \nmatrix multiplications as found in the fully connected and recurrent layers of \nneural networks for embedded large vocabulary continuous speech recognition \n(LVCSR). For compression, we introduce and study a trace norm regularization \ntechnique for training low rank factored versions of matrix multiplications. \nCompared to standard low rank training, we show that our method more \nconsistently leads to good accuracy versus number of parameter trade-offs and \ncan be used to speed up training of large models. For speedup, we enable faster \ninference on ARM processors through new open sourced kernels optimized for \nsmall batch sizes, resulting in 3x to 7x speed ups over the widely used \ngemmlowp library. Beyond LVCSR, we expect our techniques and kernels to be more \ngenerally applicable to embedded neural networks with large fully connected or \nrecurrent layers. \n</p>"}, "author": "Markus Kliegl, Siddharth Goyal, Kexin Zhao, Kavya Srinet, Mohammad Shoeybi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539629", "id": "tag:google.com,2005:reader/item/00000003260b2ac8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Nonparametric estimation of the fragmentation kernel based on a PDE stationary distribution approximation. (arXiv:1710.09172v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09172"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09172", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c92160ff\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c92160ff&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We consider a stochastic individual-based model in continuous time to \ndescribe a size-structured population for cell divisions. This model is \nmotivated by the detection of cellular aging in biology. We address here the \nproblem of nonparametric estimation of the kernel ruling the divisions based on \nthe eigenvalue problem related to the asymptotic behavior in large population. \nThis inverse problem involves a multiplicative deconvolution operator. Using \nFourier technics we derive a nonparametric estimator whose consistency is \nstudied. The main difficulty comes from the non-standard equations connecting \nthe Fourier transforms of the kernel and the parameters of the model. A \nnumerical study is carried out and we pay special attention to the derivation \nof bandwidths by using resampling. \n</p>"}, "author": "Van Ha Hoang (1), Thanh Mai Pham Ngoc (1), Vincent Rivoirard (2), Viet Chi Tran (3) ((1) LM-Orsay, (2) CEREMADE, (3) LPP)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539628", "id": "tag:google.com,2005:reader/item/00000003260b2ad6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Anatomical labeling of brain CT scan anomalies using multi-context nearest neighbor relation networks. (arXiv:1710.09180v1 [cs.CV])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09180"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09180", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9389a2a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9389a2a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This work is an endeavor to develop a deep learning methodology for automated \nanatomical labeling of a given region of interest (ROI) in brain computed \ntomography (CT) scans. We combine both local and global context to obtain a \nrepresentation of the ROI. We then use Relation Networks (RNs) to predict the \ncorresponding anatomy of the ROI based on its relationship score for each \nclass. Further, we propose a novel strategy employing nearest neighbors \napproach for training RNs. We train RNs to learn the relationship of the target \nROI with the joint representation of its nearest neighbors in each class \ninstead of all data-points in each class. The proposed strategy leads to better \ntraining of RNs along with increased performance as compared to training \nbaseline RN network. \n</p>"}, "author": "Srikrishna Varadarajan, Muktabh Mayank Srivastava, Monika Grewal, Pulkit Kumar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539627", "id": "tag:google.com,2005:reader/item/00000003260b2add", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Inversion using a new low-dimensional representation of complex binary geological media based on a deep neural network. (arXiv:1710.09196v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09196"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09196", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Efficient and high-fidelity prior sampling and inversion for complex \ngeological media is still a largely unsolved challenge. Here, we use a deep \nneural network of the variational autoencoder type to construct a parametric \nlow-dimensional base model parameterization of complex binary geological media. \nFor inversion purposes, it has the attractive feature that random draws from an \nuncorrelated standard normal distribution yield model realizations with spatial \ncharacteristics that are in agreement with the training set. In comparison with \nthe most commonly used parametric representations in probabilistic inversion, \nwe find that our dimensionality reduction (DR) approach outperforms principle \ncomponent analysis (PCA), optimization-PCA (OPCA) and discrete cosine transform \n(DCT) DR techniques for unconditional geostatistical simulation of a \nchannelized prior model. For the considered examples, important compression \nratios (200 - 500) are achieved. Given that the construction of our \nparameterization requires a training set of several tens of thousands of prior \nmodel realizations, our DR approach is more suited for probabilistic (or \ndeterministic) inversion than for unconditional (or point-conditioned) \ngeostatistical simulation. Probabilistic inversions of 2D steady-state and 3D \ntransient hydraulic tomography data are used to demonstrate the DR-based \ninversion. For the 2D case study, the performance is superior compared to \ncurrent state-of-the-art multiple-point statistics inversion by sequential \ngeostatistical resampling (SGR). Inversion results for the 3D application are \nalso encouraging. \n</p>"}, "author": "Eric Laloy, Romain H&#xe9;rault, John Lee, Diederik Jacques, Niklas Linde", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539626", "id": "tag:google.com,2005:reader/item/00000003260b2ae1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Unsupervised and Semi-supervised Anomaly Detection with LSTM Neural Networks. (arXiv:1710.09207v1 [eess.SP])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09207"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09207", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We investigate anomaly detection in an unsupervised framework and introduce \nLong Short Term Memory (LSTM) neural network based algorithms. In particular, \ngiven variable length data sequences, we first pass these sequences through our \nLSTM based structure and obtain fixed length sequences. We then find a decision \nfunction for our anomaly detectors based on the One Class Support Vector \nMachines (OC-SVM) and Support Vector Data Description (SVDD) algorithms. As the \nfirst time in the literature, we jointly train and optimize the parameters of \nthe LSTM architecture and the OC-SVM (or SVDD) algorithm using highly effective \ngradient and quadratic programming based training methods. To apply the \ngradient based training method, we modify the original objective criteria of \nthe OC-SVM and SVDD algorithms, where we prove the convergence of the modified \nobjective criteria to the original criteria. We also provide extensions of our \nunsupervised formulation to the semi-supervised and fully supervised \nframeworks. Thus, we obtain anomaly detection algorithms that can process \nvariable length data sequences while providing high performance, especially for \ntime series data. Our approach is generic so that we also apply this approach \nto the Gated Recurrent Unit (GRU) architecture by directly replacing our LSTM \nbased structure with the GRU based structure. In our experiments, we illustrate \nsignificant performance gains achieved by our algorithms with respect to the \nconventional methods. \n</p>"}, "author": "Tolga Ergen, Ali Hassan Mirza, Suleyman Serdar Kozat", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539625", "id": "tag:google.com,2005:reader/item/00000003260b2ae5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "The Heterogeneous Ensembles of Standard Classification Algorithms (HESCA): the Whole is Greater than the Sum of its Parts. (arXiv:1710.09220v1 [cs.LG])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09220"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09220", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Building classification models is an intrinsically practical exercise that \nrequires many design decisions prior to deployment. We aim to provide some \nguidance in this decision making process. Specifically, given a classification \nproblem with real valued attributes, we consider which classifier or family of \nclassifiers should one use. Strong contenders are tree based homogeneous \nensembles, support vector machines or deep neural networks. All three families \nof model could claim to be state-of-the-art, and yet it is not clear when one \nis preferable to the others. Our extensive experiments with over 200 data sets \nfrom two distinct archives demonstrate that, rather than choose a single family \nand expend computing resources on optimising that model, it is significantly \nbetter to build simpler versions of classifiers from each family and ensemble. \nWe show that the Heterogeneous Ensembles of Standard Classification Algorithms \n(HESCA), which ensembles based on error estimates formed on the train data, is \nsignificantly better (in terms of error, balanced error, negative log \nlikelihood and area under the ROC curve) than its individual components, \npicking the component that is best on train data, and a support vector machine \ntuned over 1089 different parameter configurations. We demonstrate HESCA+, \nwhich contains a deep neural network, a support vector machine and two decision \ntree forests, is significantly better than its components, picking the best \ncomponent, and HESCA. We analyse the results further and find that HESCA and \nHESCA+ are of particular value when the train set size is relatively small and \nthe problem has multiple classes. HESCA is a fast approach that is, on average, \nas good as state-of-the-art classifiers, whereas HESCA+ is significantly better \nthan average and represents a strong benchmark for future research. \n</p>"}, "author": "James Large, Jason Lines, Anthony Bagnall", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539624", "id": "tag:google.com,2005:reader/item/00000003260b2af0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Supervised Classification: Quite a Brief Overview. (arXiv:1710.09230v1 [cs.LG])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09230"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09230", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The original problem of supervised classification considers the task of \nautomatically assigning objects to their respective classes on the basis of \nnumerical measurements derived from these objects. Classifiers are the tools \nthat implement the actual functional mapping from these measurements---also \ncalled features or inputs---to the so-called class label---or output. The \nfields of pattern recognition and machine learning study ways of constructing \nsuch classifiers. The main idea behind supervised methods is that of learning \nfrom examples: given a number of example input-output relations, to what extent \ncan the general mapping be learned that takes any new and unseen feature vector \nto its correct class? This chapter provides a basic introduction to the \nunderlying ideas of how to come to a supervised classification problem. In \naddition, it provides an overview of some specific classification techniques, \ndelves into the issues of object representation and classifier evaluation, and \n(very) briefly covers some variations on the basic supervised classification \ntask that may also be of interest to the practitioner. \n</p>"}, "author": "Marco Loog", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539623", "id": "tag:google.com,2005:reader/item/00000003260b2af7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Deep Neural Networks. (arXiv:1710.09302v3 [stat.ML] UPDATED)", "published": 1510139306, "updated": 1510139311, "canonical": [{"href": "http://arxiv.org/abs/1710.09302"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09302", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep Neural Networks (DNNs) are universal function approximators providing \nstate-of- the-art solutions on wide range of applications. Common perceptual \ntasks such as speech recognition, image classification, and object tracking are \nnow commonly tackled via DNNs. Some fundamental problems remain: (1) the lack \nof a mathematical framework providing an explicit and interpretable \ninput-output formula for any topology, (2) quantification of DNNs stability \nregarding adversarial examples (i.e. modified inputs fooling DNN predictions \nwhilst undetectable to humans), (3) absence of generalization guarantees and \ncontrollable behaviors for ambiguous patterns, (4) leverage unlabeled data to \napply DNNs to domains where expert labeling is scarce as in the medical field. \nAnswering those points would provide theoretical perspectives for further \ndevelopments based on a common ground. Furthermore, DNNs are now deployed in \ntremendous societal applications, pushing the need to fill this theoretical gap \nto ensure control, reliability, and interpretability. \n</p>"}, "author": "Randall Balestriero, Richard Baraniuk", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539622", "id": "tag:google.com,2005:reader/item/00000003260b2aff", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "GeoSeq2Seq: Information Geometric Sequence-to-Sequence Networks. (arXiv:1710.09363v1 [stat.ML])", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.09363"}], "alternate": [{"href": "http://arxiv.org/abs/1710.09363", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Fisher information metric is an important foundation of information \ngeometry, wherein it allows us to approximate the local geometry of a \nprobability distribution. Recurrent neural networks such as the \nSequence-to-Sequence (Seq2Seq) networks that have lately been used to yield \nstate-of-the-art performance on speech translation or image captioning have so \nfar ignored the geometry of the latent embedding, that they iteratively learn. \nWe propose the information geometric Seq2Seq network which abridges the gap \nbetween deep recurrent neural networks and information geometry. Specifically, \nthe latent embedding offered by a recurrent network is encoded as a Fisher \nkernel of a parametric Gaussian Mixture Model, a formalism common in computer \nvision. We utilise such a network to predict the shortest routes between two \nnodes of a graph by learning the adjacency matrix using the information \ngeometric Seq2Seq model; our results show that for such a problem the \nprobabilistic representation of the latent embedding supersedes the \nnon-probabilistic embedding by 10-15%. \n</p>"}, "author": "Alessandro Bay, Biswa Sengupta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508978103540", "timestampUsec": "1508978103539619", "id": "tag:google.com,2005:reader/item/00000003260b2b0a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Minimax Optimal Algorithm for Crowdsourcing. (arXiv:1606.00226v2 [stat.ML] UPDATED)", "published": 1508978103, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1606.00226"}], "alternate": [{"href": "http://arxiv.org/abs/1606.00226", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the problem of accurately estimating the reliability of workers \nbased on noisy labels they provide, which is a fundamental question in \ncrowdsourcing. We propose a novel lower bound on the minimax estimation error \nwhich applies to any estimation procedure. We further propose Triangular \nEstimation (TE), an algorithm for estimating the reliability of workers. TE has \nlow complexity, may be implemented in a streaming setting when labels are \nprovided by workers in real time, and does not rely on an iterative procedure. \nWe further prove that TE is minimax optimal and matches our lower bound. We \nconclude by assessing the performance of TE and other state-of-the-art \nalgorithms on both synthetic and real-world data sets. \n</p>"}, "author": "Thomas Bonald, Richard Combes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508893682617", "timestampUsec": "1508893682617052", "id": "tag:google.com,2005:reader/item/00000003255e1bc4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Pre-Processing-Free Gear Fault Diagnosis Using Small Datasets with Deep Convolutional Neural Network-Based Transfer Learning. (arXiv:1710.08904v1 [cs.NE])", "published": 1508893683, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08904"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08904", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Early fault diagnosis in complex mechanical systems such as gearbox has \nalways been a great challenge, even with the recent development in deep neural \nnetworks. The performance of a classic fault diagnosis system predominantly \ndepends on the features extracted and the classifier subsequently applied. \nAlthough a large number of attempts have been made regarding feature extraction \ntechniques, the methods require great human involvements are heavily depend on \ndomain expertise and may thus be non-representative and biased from application \nto application. On the other hand, while the deep neural networks based \napproaches feature adaptive feature extractions and inherent classifications, \nthey usually require a substantial set of training data and thus hinder their \nusage for engineering applications with limited training data such as gearbox \nfault diagnosis. This paper develops a deep convolutional neural network-based \ntransfer learning approach that not only entertains pre-processing free \nadaptive feature extractions, but also requires only a small set of training \ndata. The proposed approach performs gear fault diagnosis using pre-processing \nfree raw accelerometer data and experiments with various sizes of training data \nwere conducted. The superiority of the proposed approach is revealed by \ncomparing the performance with other methods such as locally trained \nconvolution neural network and angle-frequency analysis based support vector \nmachine. The achieved accuracy indicates that the proposed approach is not only \nviable and robust, but also has the potential to be readily applicable to other \nfault diagnosis practices. \n</p>"}, "author": "Pei Cao, Shengli Zhang, Jiong Tang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891849586", "timestampUsec": "1508891849586136", "id": "tag:google.com,2005:reader/item/00000003255b32a6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neural Stain-Style Transfer Learning using GAN for Histopathological Images. (arXiv:1710.08543v1 [cs.CV])", "published": 1508891850, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08543"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08543", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9389dfd\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9389dfd&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Performance of data-driven network for tumor classification varies with \nstain-style of histopathological images. This article proposes the stain-style \ntransfer (SST) model based on conditional generative adversarial networks \n(GANs) which is to learn not only the certain color distribution but also the \ncorresponding histopathological pattern. Our model considers feature-preserving \nloss in addition to well-known GAN loss. Consequently our model does not only \ntransfers initial stain-styles to the desired one but also prevent the \ndegradation of tumor classifier on transferred images. The model is examined \nusing the CAMELYON16 dataset. \n</p>"}, "author": "Hyungjoo Cho, Sungbin Lim, Gunho Choi, Hyunseok Min", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891849586", "timestampUsec": "1508891849586135", "id": "tag:google.com,2005:reader/item/00000003255b32b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Max-Margin Invariant Features from Transformed Unlabeled Data. (arXiv:1710.08585v1 [cs.LG])", "published": 1508891850, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08585"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08585", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The study of representations invariant to common transformations of the data \nis important to learning. Most techniques have focused on local approximate \ninvariance implemented within expensive optimization frameworks lacking \nexplicit theoretical guarantees. In this paper, we study kernels that are \ninvariant to a unitary group while having theoretical guarantees in addressing \nthe important practical issue of unavailability of transformed versions of \nlabelled data. A problem we call the Unlabeled Transformation Problem which is \na special form of semi-supervised learning and one-shot learning. We present a \ntheoretically motivated alternate approach to the invariant kernel SVM based on \nwhich we propose Max-Margin Invariant Features (MMIF) to solve this problem. As \nan illustration, we design an framework for face recognition and demonstrate \nthe efficacy of our approach on a large scale semi-synthetic dataset with \n153,000 images and a new challenging protocol on Labelled Faces in the Wild \n(LFW) while out-performing strong baselines. \n</p>"}, "author": "Dipan K. Pal, Ashwin A. Kannan, Gautam Arakalgud, Marios Savvides", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891849586", "timestampUsec": "1508891849586134", "id": "tag:google.com,2005:reader/item/00000003255b32bd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Auto-Differentiating Linear Algebra. (arXiv:1710.08717v1 [cs.MS])", "published": 1508891850, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08717"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08717", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Development systems for deep learning, such as Theano, Torch, TensorFlow, or \nMXNet, are easy-to-use tools for creating complex neural network models. Since \ngradient computations are automatically baked in, and execution is mapped to \nhigh performance hardware, these models can be trained end-to-end on large \namounts of data. However, it is currently not easy to implement many basic \nmachine learning primitives in these systems (such as Gaussian processes, least \nsquares estimation, principal components analysis, Kalman smoothing), mainly \nbecause they lack efficient support of linear algebra primitives as \ndifferentiable operators. We detail how a number of matrix decompositions \n(Cholesky, LQ, symmetric eigen) can be implemented as differentiable operators. \nWe have implemented these primitives in MXNet, running on CPU and GPU in single \nand double precision. We sketch use cases of these new operators, learning \nGaussian process and Bayesian linear regression models. Our implementation is \nbased on BLAS/LAPACK APIs, for which highly tuned implementations are available \non all major CPUs and GPUs. \n</p>"}, "author": "Matthias Seeger, Asmus Hetzel, Zhenwen Dai, Neil D. Lawrence", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891849586", "timestampUsec": "1508891849586133", "id": "tag:google.com,2005:reader/item/00000003255b32c5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "One pixel attack for fooling deep neural networks. (arXiv:1710.08864v1 [cs.AI])", "published": 1508891850, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08864"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08864", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent research has revealed that the output of Deep neural networks(DNN) is \nnot continuous and very sensitive to tiny perturbation on the input vectors and \naccordingly several methods have been proposed for crafting effective \nperturbation against the networks. In this paper, we propose a novel method for \noptically calculating extremely small adversarial perturbation (few-pixels \nattack), based on differential evolution. It requires much less adversarial \ninformation and works with a broader classes of DNN models. The results show \nthat 73.8$\\%$ of the test images can be crafted to adversarial images with \nmodification just on one pixel with 98.7$\\%$ confidence on average. In \naddition, it is known that investigating the robustness problem of DNN can \nbring critical clues for understanding the geometrical features of the DNN \ndecision map in high dimensional input space. The results of conducting \nfew-pixels attack contribute quantitative measurements and analysis to the \ngeometrical understanding from a different perspective compared to previous \nworks. \n</p>"}, "author": "Jiawei Su, Danilo Vasconcellos Vargas, Sakurai Kouichi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891849586", "timestampUsec": "1508891849586132", "id": "tag:google.com,2005:reader/item/00000003255b32cc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Model Identification via Physics Engines for Improved Policy Search. (arXiv:1710.08893v1 [cs.RO])", "published": 1508891850, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08893"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08893", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a practical approach for identifying unknown mechanical \nparameters, such as mass and friction models of manipulated rigid objects or \nactuated robotic links, in a succinct manner that aims to improve the \nperformance of policy search algorithms. Key features of this approach are the \nuse of off-the-shelf physics engines and the adaptation of a black-box Bayesian \noptimization framework for this purpose. The physics engine is used to \nreproduce in simulation experiments that are performed on a real robot, and the \nmechanical parameters of the simulated system are automatically fine-tuned so \nthat the simulated trajectories match with the real ones. The optimized model \nis then used for learning a policy in simulation, before safely deploying it on \nthe real robot. Given the well-known limitations of physics engines in modeling \nreal-world objects, it is generally not possible to find a mechanical model \nthat reproduces in simulation the real trajectories exactly. Moreover, there \nare many scenarios where a near-optimal policy can be found without having a \nperfect knowledge of the system. Therefore, searching for a perfect model may \nnot be worth the computational effort in practice. The proposed approach aims \nthen to identify a model that is good enough to approximate the value of a \nlocally optimal policy with a certain confidence, instead of spending all the \ncomputational resources on searching for the most accurate model. Empirical \nevaluations, performed in simulation and on a real robotic manipulation task, \nshow that model identification via physics engines can significantly boost the \nperformance of policy search algorithms that are popular in robotics, such as \nTRPO, PoWER and PILCO, with no additional real-world data. \n</p>"}, "author": "Shaojun Zhu, Andrew Kimmel, Kostas E. Bekris, Abdeslam Boularias", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891849586", "timestampUsec": "1508891849586121", "id": "tag:google.com,2005:reader/item/00000003255b335d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep reinforcement learning from human preferences. (arXiv:1706.03741v3 [stat.ML] CROSS LISTED)", "published": 1508891850, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.03741"}], "alternate": [{"href": "http://arxiv.org/abs/1706.03741", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>For sophisticated reinforcement learning (RL) systems to interact usefully \nwith real-world environments, we need to communicate complex goals to these \nsystems. In this work, we explore goals defined in terms of (non-expert) human \npreferences between pairs of trajectory segments. We show that this approach \ncan effectively solve complex RL tasks without access to the reward function, \nincluding Atari games and simulated robot locomotion, while providing feedback \non less than one percent of our agent's interactions with the environment. This \nreduces the cost of human oversight far enough that it can be practically \napplied to state-of-the-art RL systems. To demonstrate the flexibility of our \napproach, we show that we can successfully train complex novel behaviors with \nabout an hour of human time. These behaviors and environments are considerably \nmore complex than any that have been previously learned from human feedback. \n</p>"}, "author": "Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882736", "id": "tag:google.com,2005:reader/item/00000003255ab3de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease aDivergence At Every Step. (arXiv:1710.08446v1 [stat.ML])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08446"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08446", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative adversarial networks (GANs) are a family of generative models that \ndo not minimize a single training criterion. Unlike other generative models, \nthe data distribution is learned via a game between a generator (the generative \nmodel) and a discriminator (a teacher providing training signal) that each \nminimize their own cost. GANs are designed to reach a Nash equilibrium at which \neach player cannot reduce their cost without changing the other players' \nparameters. One useful approach for the theory of GANs is to show that a \ndivergence between the training distribution and the model distribution obtains \nits minimum value at equilibrium. Several recent research directions have been \nmotivated by the idea that this divergence is the primary guide for the \nlearning process and that every step of learning should decrease the \ndivergence. We show that this view is overly restrictive. During GAN training, \nthe discriminator provides learning signal in situations where the gradients of \nthe divergences between distributions would not be useful. We provide empirical \ncounterexamples to the view of GAN training as divergence minimization. \nSpecifically, we demonstrate that GANs are able to learn distributions in \nsituations where the divergence minimization point of view predicts they would \nfail. We also show that gradient penalties motivated from the divergence \nminimization perspective are equally helpful when applied in other contexts in \nwhich the divergence minimization perspective does not predict they would be \nhelpful. This contributes to a growing body of evidence that GAN training may \nbe more usefully viewed as approaching Nash equilibria via trajectories that do \nnot necessarily minimize a specific divergence at each step. \n</p>"}, "author": "William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M. Dai, Shakir Mohamed, Ian Goodfellow", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882735", "id": "tag:google.com,2005:reader/item/00000003255ab401", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interpretable Machine Learning for Privacy-Preserving IoT and Pervasive Systems. (arXiv:1710.08464v1 [stat.ML])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08464"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08464", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The presence of pervasive computing in our everyday lives and emergence of \nthe Internet of Things, such as the interaction of users with connected devices \nlike smartphones or home appliances generate increasing amounts of traces that \nreflect users' behavior. A plethora of machine learning techniques enable \nservice providers to process these traces to extract latent information about \nthe users. While most of the existing projects have focused on the accuracy of \nthese techniques, little work has been done on the interpretation of the \ninference and identification algorithms based on them. In this paper, we \npropose a machine learning interpretability framework for inference algorithms \nbased on data collected through IoT and pervasive systems and we outline the \nopen challenges in this research area. Our interpretability framework enable \nusers to understand how the traces they generate could expose their privacy, \nwhile allowing for usable and personalized services at the same time. \n</p>"}, "author": "Benjamin Baron, Mirco Musolesi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882734", "id": "tag:google.com,2005:reader/item/00000003255ab44c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Unified Framework for Long Range and Cold Start Forecasting of Seasonal Profiles in Time Series. (arXiv:1710.08473v1 [stat.ML])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08473"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08473", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Providing long-range forecasts is a fundamental challenge in time series \nmodeling, which is only compounded by the challenge of having to form such \nforecasts when a time series has never previously been observed. The latter \nchallenge is the time series version of the cold-start problem seen in \nrecommender systems which, to our knowledge, has not been directly addressed in \nprevious work. In addition, modern time series datasets are often plagued by \nmissing data. We focus on forecasting seasonal profiles---or baseline \ndemand---for periods on the order of a year long, even in the cold-start \nsetting or with otherwise missing data. Traditional time series approaches that \nperform iterated step-ahead methods struggle to provide accurate forecasts on \nsuch problems, let alone in the missing data regime. We present a \ncomputationally efficient framework which combines ideas from high-dimensional \nregression and matrix factorization on a carefully constructed data matrix. Key \nto our formulation and resulting performance is (1) leveraging repeated \npatterns over fixed periods of time and across series, and (2) metadata \nassociated with the individual series. We provide analyses of our framework on \nlarge messy real-world datasets. \n</p>"}, "author": "Christopher Xie, Alex Tank, Alec Greaves-Tunnell, Emily Fox", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882733", "id": "tag:google.com,2005:reader/item/00000003255ab49e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convolutional Neural Knowledge Graph Learning. (arXiv:1710.08502v1 [cs.LG])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08502"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08502", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Previous models for learning entity and relationship embeddings of knowledge \ngraphs such as TransE, TransH, and TransR aim to explore new links based on \nlearned representations. However, these models interpret relationships as \nsimple translations on entity embeddings. In this paper, we try to learn more \ncomplex connections between entities and relationships. In particular, we use a \nConvolutional Neural Network (CNN) to learn entity and relationship \nrepresentations in knowledge graphs. In our model, we treat entities and \nrelationships as one-dimensional numerical sequences with the same length. \nAfter that, we combine each triplet of head, relationship, and tail together as \na matrix with height 3. CNN is applied to the triplets to get confidence \nscores. Positive and manually corrupted negative triplets are used to train the \nembeddings and the CNN model simultaneously. Experimental results on public \nbenchmark datasets show that the proposed model outperforms state-of-the-art \nmodels on exploring unseen relationships, which proves that CNN is effective to \nlearn complex interactive patterns between entities and relationships. \n</p>"}, "author": "Feipeng Zhao, Martin Renqiang Min, Chen Shen, Amit Chakraborty", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882732", "id": "tag:google.com,2005:reader/item/00000003255ab4ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Expectation Maximization Framework for Preferential Attachment Models. (arXiv:1710.08511v1 [stat.CO])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08511"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08511", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c938a137\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c938a137&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper we develop an Expectation Maximization(EM) algorithm to \nestimate the parameter of a Yule-Simon distribution. The Yule-Simon \ndistribution exhibits the \"rich get richer\" effect whereby an 80-20 type of \nrule tends to dominate. These distributions are ubiquitous in industrial \nsettings. The EM algorithm presented provides both frequentist and Bayesian \nestimates of the $\\lambda$ parameter. By placing the estimation method within \nthe EM framework we are able to derive Standard errors of the resulting \nestimate. Additionally, we prove convergence of the Yule-Simon EM algorithm and \nstudy the rate of convergence. An explicit, closed form solution for the rate \nof convergence of the algorithm is given. \n</p>"}, "author": "Lucas Roberts, Denisa Roberts", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882731", "id": "tag:google.com,2005:reader/item/00000003255ab528", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stability Analysis of Optimal Adaptive Control using Value Iteration with Approximation Errors. (arXiv:1710.08530v1 [math.OC])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08530"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08530", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c948d19c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c948d19c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Adaptive optimal control using value iteration initiated from a stabilizing \ncontrol policy is theoretically analyzed in terms of stability of the system \nduring the learning stage without ignoring the effects of approximation errors. \nThis analysis includes the system operated using any single/constant resulting \ncontrol policy and also using an evolving/time-varying control policy. A \nfeature of the presented results is providing estimations of the \\textit{region \nof attraction} so that if the initial condition is within the region, the whole \ntrajectory will remain inside it and hence, the function approximation results \nremain valid. \n</p>"}, "author": "Ali Heydari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882730", "id": "tag:google.com,2005:reader/item/00000003255ab585", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Benchmark of Deep Learning Models on Large Healthcare MIMIC Datasets. (arXiv:1710.08531v1 [cs.LG])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08531"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08531", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning models (aka Deep Neural Networks) have revolutionized many \nfields including computer vision, natural language processing, speech \nrecognition, and is being increasingly used in clinical healthcare \napplications. However, few works exist which have benchmarked the performance \nof the deep learning models with respect to the state-of-the-art machine \nlearning models and prognostic scoring systems on publicly available healthcare \ndatasets. In this paper, we present the benchmarking results for several \nclinical prediction tasks such as mortality prediction, length of stay \nprediction, and ICD-9 code group prediction using Deep Learning models, \nensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA \nscores. We used the Medical Information Mart for Intensive Care III (MIMIC-III) \n(v1.4) publicly available dataset, which includes all patients admitted to an \nICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the \nbenchmarking tasks. Our results show that deep learning models consistently \noutperform all the other approaches especially when the `raw' clinical time \nseries data is used as input features to the models. \n</p>"}, "author": "Sanjay Purushotham, Chuizheng Meng, Zhengping Che, Yan Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882729", "id": "tag:google.com,2005:reader/item/00000003255ab5c5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Display advertising: Estimating conversion probability efficiently. (arXiv:1710.08583v1 [stat.ML])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08583"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08583", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The goal of online display advertising is to entice users to \"convert\" (i.e., \ntake a pre-defined action such as making a purchase) after clicking on the ad. \nAn important measure of the value of an ad is the probability of conversion. \nThe focus of this paper is the development of a computationally efficient, \naccurate, and precise estimator of conversion probability. The challenges \nassociated with this estimation problem are the delays in observing conversions \nand the size of the data set (both number of observations and number of \npredictors). Two models have previously been considered as a basis for \nestimation: A logistic regression model and a joint model for observed \nconversion statuses and delay times. Fitting the former is simple, but ignoring \nthe delays in conversion leads to an under-estimate of conversion probability. \nOn the other hand, the latter is less biased but computationally expensive to \nfit. Our proposed estimator is a compromise between these two estimators. We \napply our results to a data set from Criteo, a commerce marketing company that \npersonalizes online display advertisements for users. \n</p>"}, "author": "Abdollah Safari, Rachel MacKay Altman, Thomas M. Loughin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882728", "id": "tag:google.com,2005:reader/item/00000003255ab5fb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interpretable Deep Learning applied to Plant Stress Phenotyping. (arXiv:1710.08619v1 [stat.ML])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08619"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08619", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Availability of an explainable deep learning model that can be applied to \npractical real world scenarios and in turn, can consistently, rapidly and \naccurately identify specific and minute traits in applicable fields of \nbiological sciences, is scarce. Here we consider one such real world example \nviz., accurate identification, classification and quantification of biotic and \nabiotic stresses in crop research and production. Up until now, this has been \npredominantly done manually by visual inspection and require specialized \ntraining. However, such techniques are hindered by subjectivity resulting from \ninter- and intra-rater cognitive variability. Here, we demonstrate the ability \nof a machine learning framework to identify and classify a diverse set of \nfoliar stresses in the soybean plant with remarkable accuracy. We also present \nan explanation mechanism using gradient-weighted class activation mapping that \nisolates the visual symptoms used by the model to make predictions. This \nunsupervised identification of unique visual symptoms for each stress provides \na quantitative measure of stress severity, allowing for identification, \nclassification and quantification in one framework. The learnt model appears to \nbe agnostic to species and make good predictions for other (non-soybean) \nspecies, demonstrating an ability of transfer learning. \n</p>"}, "author": "Sambuddha Ghosal, David Blystone, Asheesh K. Singh, Baskar Ganapathysubramanian, Arti Singh, Soumik Sarkar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882727", "id": "tag:google.com,2005:reader/item/00000003255ab65c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improving Accuracy of Nonparametric Transfer Learning via Vector Segmentation. (arXiv:1710.08637v1 [cs.LG])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08637"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08637", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Transfer learning using deep neural networks as feature extractors has become \nincreasingly popular over the past few years. It allows to obtain \nstate-of-the-art accuracy on datasets too small to train a deep neural network \non its own, and it provides cutting edge descriptors that, combined with \nnonparametric learning methods, allow rapid and flexible deployment of \nperforming solutions in computationally restricted settings. In this paper, we \nare interested in showing that the features extracted using deep neural \nnetworks have specific properties which can be used to improve accuracy of \ndownstream nonparametric learning methods. Namely, we demonstrate that for some \ndistributions where information is embedded in a few coordinates, segmenting \nfeature vectors can lead to better accuracy. We show how this model can be \napplied to real datasets by performing experiments using three mainstream deep \nneural network feature extractors and four databases, in vision and audio. \n</p>"}, "author": "Vincent Gripon, Ghouthi B. Hacene, Matthias L&#xf6;we, Franck Vermet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882726", "id": "tag:google.com,2005:reader/item/00000003255ab691", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Auto-Differentiating Linear Algebra. (arXiv:1710.08717v1 [cs.MS])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08717"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08717", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Development systems for deep learning, such as Theano, Torch, TensorFlow, or \nMXNet, are easy-to-use tools for creating complex neural network models. Since \ngradient computations are automatically baked in, and execution is mapped to \nhigh performance hardware, these models can be trained end-to-end on large \namounts of data. However, it is currently not easy to implement many basic \nmachine learning primitives in these systems (such as Gaussian processes, least \nsquares estimation, principal components analysis, Kalman smoothing), mainly \nbecause they lack efficient support of linear algebra primitives as \ndifferentiable operators. We detail how a number of matrix decompositions \n(Cholesky, LQ, symmetric eigen) can be implemented as differentiable operators. \nWe have implemented these primitives in MXNet, running on CPU and GPU in single \nand double precision. We sketch use cases of these new operators, learning \nGaussian process and Bayesian linear regression models. Our implementation is \nbased on BLAS/LAPACK APIs, for which highly tuned implementations are available \non all major CPUs and GPUs. \n</p>"}, "author": "Matthias Seeger, Asmus Hetzel, Zhenwen Dai, Neil D. Lawrence", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882725", "id": "tag:google.com,2005:reader/item/00000003255ab6d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Greater data science at baccalaureate institutions. (arXiv:1710.08728v1 [stat.OT])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08728"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08728", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Donoho's JCGS (in press) paper is a spirited call to action for \nstatisticians, who he points out are losing ground in the field of data science \nby refusing to accept that data science is its own domain. (Or, at least, a \ndomain that is becoming distinctly defined.) He calls on writings by John \nTukey, Bill Cleveland, and Leo Breiman, among others, to remind us that \nstatisticians have been dealing with data science for years, and encourages \nacceptance of the direction of the field while also ensuring that statistics is \ntightly integrated. \n</p> \n<p>As faculty at baccalaureate institutions (where the growth of undergraduate \nstatistics programs has been dramatic), we are keen to ensure statistics has a \nplace in data science and data science education. In his paper, Donoho is \nprimarily focused on graduate education. At our undergraduate institutions, we \nare considering many of the same questions. \n</p>"}, "author": "Amelia McNamara, Nicholas J. Horton, Benjamin S. Baumer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882724", "id": "tag:google.com,2005:reader/item/00000003255ab703", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Correction Method of a Binary Classifier Applied to Multi-label Pairwise Models. (arXiv:1710.08729v1 [cs.LG])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08729"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08729", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we addressed the issue of applying a stochastic classifier and \na local, fuzzy confusion matrix under the framework of multi-label \nclassification. We proposed a novel solution to the problem of correcting label \npairwise ensembles. The main step of the correction procedure is to compute \nclassifier- specific competence and cross-competence measures, which estimates \nerror pattern of the underlying classifier. We considered two improvements of \nthe method of obtaining confusion matrices. The first one is aimed to deal with \nimbalanced labels. The other utilizes double labelled instances which are \nusually removed during the pairwise transformation. The proposed methods were \nevaluated using 29 benchmark datasets. In order to assess the efficiency of the \nintroduced models, they were compared against 1 state-of-the-art approach and \nthe correction scheme based on the original method of confusion matrix \nestimation. The comparison was performed using four different multi-label \nevaluation measures: macro and micro-averaged F1 loss, zero-one loss and \nHamming loss. Additionally, we investigated relations between classification \nquality, which is expressed in terms of different quality criteria, and \ncharacteristics of multi-label datasets such as average imbalance ratio or \nlabel density. The experimental study reveals that the correction approaches \nsignificantly outperforms the reference method only in terms of zero-one loss. \n</p>"}, "author": "Pawel Trajdos, Marek Kurzynski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882723", "id": "tag:google.com,2005:reader/item/00000003255ab770", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Markov Properties for Graphical Models with Cycles and Latent Variables. (arXiv:1710.08775v1 [math.ST])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08775"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08775", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We investigate probabilistic graphical models that allow for both cycles and \nlatent variables. For this we introduce directed graphs with hyperedges \n(HEDGes), generalizing and combining both marginalized directed acyclic graphs \n(mDAGs) that can model latent (dependent) variables, and directed mixed graphs \n(DMGs) that can model cycles. We define and analyse several different Markov \nproperties that relate the graphical structure of a HEDG with a probability \ndistribution on a corresponding product space over the set of nodes, for \nexample factorization properties, structural equations properties, \nordered/local/global Markov properties, and marginal versions of these. The \nvarious Markov properties for HEDGes are in general not equivalent to each \nother when cycles or hyperedges are present, in contrast with the simpler case \nof directed acyclic graphical (DAG) models (also known as Bayesian networks). \nWe show how the Markov properties for HEDGes - and thus the corresponding \ngraphical Markov models - are logically related to each other. \n</p>"}, "author": "Patrick Forr&#xe9;, Joris M. Mooij", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882722", "id": "tag:google.com,2005:reader/item/00000003255ab79b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Algorithmic infeasibility of community detection in higher-order networks. (arXiv:1710.08816v1 [cs.SI])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08816"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08816", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c948d42e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c948d42e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In principle, higher-order networks that have multiple edge types are more \ninformative than their lower-order counterparts. In practice, however, \nexcessively rich information may be algorithmically infeasible to extract. It \nrequires an algorithm that assumes a high-dimensional model and such an \nalgorithm may perform poorly or be extremely sensitive to the initial estimate \nof the model parameters. Herein, we address this problem of community detection \nthrough a detectability analysis. We focus on the expectation-maximization (EM) \nalgorithm with belief propagation (BP), and analytically derive its algorithmic \ndetectability threshold, i.e., the limit of the modular structure strength \nbelow which the algorithm can no longer detect any modular structures. The \nresults indicate the existence of a phase in which the community detection of a \nlower-order network outperforms its higher-order counterpart. \n</p>"}, "author": "Tatsuro Kawamoto", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882721", "id": "tag:google.com,2005:reader/item/00000003255ab7c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Algorithmic detectability threshold of the stochastic blockmodel. (arXiv:1710.08841v1 [cs.SI])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08841"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08841", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The assumption that the values of model parameters are known or correctly \nlearned, i.e., the Nishimori condition, is one of the requirements for the \ndetectability analysis of the stochastic blockmodel in statistical inference. \nIn practice, however, there is no example demonstrating that we can know the \nmodel parameters beforehand, and there is no guarantee that the model \nparameters can be learned accurately. In this study, we consider the \nexpectation-maximization (EM) algorithm with belief propagation (BP) and derive \nits algorithmic detectability threshold. Our analysis is not restricted to the \ncommunity structure, but includes general modular structures. Because the \nalgorithm cannot always learn the planted model parameters correctly, the \nalgorithmic detectability threshold is qualitatively different from the one \nwith the Nishimori condition. \n</p>"}, "author": "Tatsuro Kawamoto", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882720", "id": "tag:google.com,2005:reader/item/00000003255ab856", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Bayesian Method for Joint Clustering of Vectorial Data and Network Data. (arXiv:1710.08846v1 [stat.ML])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08846"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08846", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a new model-based integrative method for clustering objects given \nboth vectorial data, which describes the feature of each object, and network \ndata, which indicates the similarity of connected objects. The proposed general \nmodel is able to cluster the two types of data simultaneously within one \nintegrative probabilistic model, while traditional methods can only handle one \ndata type or depend on transforming one data type to another. Bayesian \ninference of the clustering is conducted based on a Markov chain Monte Carlo \nalgorithm. A special case of the general model combining the Gaussian mixture \nmodel and the stochastic block model is extensively studied. We used both \nsynthetic data and real data to evaluate this new method and compare it with \nalternative methods. The results show that our simultaneous clustering method \nperforms much better. This improvement is due to the power of the model-based \nprobabilistic approach for efficiently integrating information. \n</p>"}, "author": "Yunchuan Kong, Xiaodan Fan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882719", "id": "tag:google.com,2005:reader/item/00000003255ab8c0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Robust Photometric Stereo via Dictionary Learning. (arXiv:1710.08873v1 [cs.CV])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08873"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08873", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Photometric stereo is a method that seeks to reconstruct the normal vectors \nof an object from a set of images of the object illuminated under different \nlight sources. While effective in some situations, classical photometric stereo \nrelies on a diffuses surface model that cannot handle objects with complex \nreflectance patterns, and it is sensitive to non-idealities in the images. In \nthis work, we propose a novel approach to photometric stereo that relies on \ndictionary learning to produce robust normal vector reconstructions. \nSpecifically, we develop three formulations for applying dictionary learning to \nphotometric stereo. We propose a preprocessing step that utilizes dictionary \nlearning to denoise the images. We also present a model that applies dictionary \nlearning to regularize and reconstruct the normal vectors from the images under \nthe classic Lambertian reflectance model. Finally, we generalize the latter \nmodel to explicitly model non-Lambertian objects. We investigate all three \napproaches through extensive experimentation on synthetic and real benchmark \ndatasets and observe state-of-the-art performance compared to existing robust \nphotometric stereo methods. \n</p>"}, "author": "Andrew J. Wagenmaker, Brian E. Moore, Raj Rao Nadakuditi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882718", "id": "tag:google.com,2005:reader/item/00000003255abafb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Classification on Large Networks: A Quantitative Bound via Motifs and Graphons. (arXiv:1710.08878v1 [cs.LG])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08878"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08878", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>When each data point is a large graph, graph statistics such as densities of \ncertain subgraphs (motifs) can be used as feature vectors for machine learning. \nWhile intuitive, motif counts are expensive to compute and difficult to work \nwith theoretically. Via graphon theory, we give an explicit quantitative bound \nfor the ability of motif homomorphisms to distinguish large networks under both \ngenerative and sampling noise. Furthermore, we give similar bounds for the \ngraph spectrum and connect it to homomorphism densities of cycles. This results \nin an easily computable classifier on graph data with theoretical performance \nguarantee. Our method yields competitive results on classification tasks for \nthe autoimmune disease Lupus Erythematosus. \n</p>"}, "author": "Andreas Haupt, Mohammad Khatami, Thomas Schultz, Ngoc Mai Tran", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882717", "id": "tag:google.com,2005:reader/item/00000003255abd19", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Memristor-Based Optimization Framework for AI Applications. (arXiv:1710.08882v1 [cs.ET])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08882"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08882", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Memristors have recently received significant attention as ubiquitous \ndevice-level components for building a novel generation of computing systems. \nThese devices have many promising features, such as non-volatility, low power \nconsumption, high density, and excellent scalability. The ability to control \nand modify biasing voltages at the two terminals of memristors make them \npromising candidates to perform matrix-vector multiplications and solve systems \nof linear equations. In this article, we discuss how networks of memristors \narranged in crossbar arrays can be used for efficiently solving optimization \nand machine learning problems. We introduce a new memristor-based optimization \nframework that combines the computational merit of memristor crossbars with the \nadvantages of an operator splitting method, alternating direction method of \nmultipliers (ADMM). Here, ADMM helps in splitting a complex optimization \nproblem into subproblems that involve the solution of systems of linear \nequations. The capability of this framework is shown by applying it to linear \nprogramming, quadratic programming, and sparse optimization. In addition to \nADMM, implementation of a customized power iteration (PI) method for \neigenvalue/eigenvector computation using memristor crossbars is discussed. The \nmemristor-based PI method can further be applied to principal component \nanalysis (PCA). The use of memristor crossbars yields a significant speed-up in \ncomputation, and thus, we believe, has the potential to advance optimization \nand machine learning research in artificial intelligence (AI). \n</p>"}, "author": "Sijia Liu, Yanzhi Wang, Makan Fardad, Pramod K. Varshney", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882716", "id": "tag:google.com,2005:reader/item/00000003255abd59", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Conformal predictive distributions with kernels. (arXiv:1710.08894v1 [cs.LG])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08894"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08894", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper reviews the checkered history of predictive distributions in \nstatistics and discusses two developments, one from recent literature and the \nother new. The first development is bringing predictive distributions into \nmachine learning, whose early development was so deeply influenced by two \nremarkable groups at the Institute of Automation and Remote Control. The second \ndevelopment is combining predictive distributions with kernel methods, which \nwere originated by one of those groups, including Emmanuel Braverman. \n</p>"}, "author": "Vladimir Vovk, Ilia Nouretdinov, Valery Manokhin, Alex Gammerman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508891555883", "timestampUsec": "1508891555882715", "id": "tag:google.com,2005:reader/item/00000003255abd8e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Calibration of Machine Learning Classifiers for Probability of Default Modelling. (arXiv:1710.08901v1 [econ.EM])", "published": 1508891556, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08901"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08901", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Binary classification is highly used in credit scoring in the estimation of \nprobability of default. The validation of such predictive models is based both \non rank ability, and also on calibration (i.e. how accurately the probabilities \noutput by the model map to the observed probabilities). In this study we cover \nthe current best practices regarding calibration for binary classification, and \nexplore how different approaches yield different results on real world credit \nscoring data. The limitations of evaluating credit scoring models using only \nrank ability metrics are explored. A benchmark is run on 18 real world \ndatasets, and results compared. The calibration techniques used are Platt \nScaling and Isotonic Regression. Also, different machine learning models are \nused: Logistic Regression, Random Forest Classifiers, and Gradient Boosting \nClassifiers. Results show that when the dataset is treated as a time series, \nthe use of re-calibration with Isotonic Regression is able to improve the long \nterm calibration better than the alternative methods. Using re-calibration, the \nnon-parametric models are able to outperform the Logistic Regression on Brier \nScore Loss. \n</p>"}, "author": "Pedro G. Fonseca, Hugo D. Lopes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890291", "id": "tag:google.com,2005:reader/item/00000003253733dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Voice 3: 2000-Speaker Neural Text-to-Speech. (arXiv:1710.07654v1 [cs.SD])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07654"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07654", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present Deep Voice 3, a fully-convolutional attention-based neural \ntext-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural \nspeech synthesis systems in naturalness while training ten times faster. We \nscale Deep Voice 3 to data set sizes unprecedented for TTS, training on more \nthan eight hundred hours of audio from over two thousand speakers. In addition, \nwe identify common error modes of attention-based speech synthesis networks, \ndemonstrate how to mitigate them, and compare several different waveform \nsynthesis methods. We also describe how to scale inference to ten million \nqueries per day on one single-GPU server. \n</p>"}, "author": "Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, John Miller", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890290", "id": "tag:google.com,2005:reader/item/00000003253734f1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Point Neurons with Conductance-Based Synapses in the Neural Engineering Framework. (arXiv:1710.07659v1 [q-bio.NC])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07659"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07659", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The mathematical model underlying the Neural Engineering Framework (NEF) \nexpresses neuronal input as a linear combination of synaptic currents. However, \nin biology, synapses are not perfect current sources and are thus nonlinear. \nDetailed synapse models are based on channel conductances instead of currents, \nwhich require independent handling of excitatory and inhibitory synapses. This, \nin particular, significantly affects the influence of inhibitory signals on the \nneuronal dynamics. In this technical report we first summarize the relevant \nportions of the NEF and conductance-based synapse models. We then discuss a \nna\\\"ive translation between populations of LIF neurons with current- and \nconductance-based synapses based on an estimation of an average membrane \npotential. Experiments show that this simple approach works relatively well for \nfeed-forward communication channels, yet performance degrades for NEF networks \ndescribing more complex dynamics, such as integration. \n</p>"}, "author": "Andreas St&#xf6;ckel, Aaron R. Voelker, Chris Eliasmith", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890289", "id": "tag:google.com,2005:reader/item/000000032537359d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Low Precision RNNs: Quantizing RNNs Without Losing Accuracy. (arXiv:1710.07706v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07706"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07706", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c948d661\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c948d661&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Similar to convolution neural networks, recurrent neural networks (RNNs) \ntypically suffer from over-parameterization. Quantizing bit-widths of weights \nand activations results in runtime efficiency on hardware, yet it often comes \nat the cost of reduced accuracy. This paper proposes a quantization approach \nthat increases model size with bit-width reduction. This approach will allow \nnetworks to perform at their baseline accuracy while still maintaining the \nbenefits of reduced precision and overall model size reduction. \n</p>"}, "author": "Supriya Kapur, Asit Mishra, Debbie Marr", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890288", "id": "tag:google.com,2005:reader/item/0000000325373677", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Solving the \"false positives\" problem in fraud prediction. (arXiv:1710.07709v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07709"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07709", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9535454\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9535454&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we present an automated feature engineering based approach to \ndramatically reduce false positives in fraud prediction. False positives plague \nthe fraud prediction industry. It is estimated that only 1 in 5 declared as \nfraud are actually fraud and roughly 1 in every 6 customers have had a valid \ntransaction declined in the past year. To address this problem, we use the Deep \nFeature Synthesis algorithm to automatically derive behavioral features based \non the historical data of the card associated with a transaction. We generate \n237 features (&gt;100 behavioral patterns) for each transaction, and use a random \nforest to learn a classifier. We tested our machine learning model on data from \na large multinational bank and compared it to their existing solution. On an \nunseen data of 1.852 million transactions, we were able to reduce the false \npositives by 54% and provide a savings of 190K euros. We also assess how to \ndeploy this solution, and whether it necessitates streaming computation for \nreal time scoring. We found that our solution can maintain similar benefits \neven when historical features are computed once every 7 days. \n</p>"}, "author": "Roy Wedge, James Max Kanter, Santiago Moral Rubio, Sergio Iglesias Perez, Kalyan Veeramachaneni", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890287", "id": "tag:google.com,2005:reader/item/00000003253736dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adversarially Optimizing Intersection over Union for Object Localization Tasks. (arXiv:1710.07735v1 [cs.CV])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07735"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07735", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>An implicit uncertainty exists in the annotations of computer visions \ndatasets due to annotator disagreement and the high-dimensional space that \nannotations must be selected from.Rather than attempting to remove all \nannotation uncertainty,which we view as hopeless, or ignoring it, which can be \ndetrimental, we choose to embrace uncertainty in the design of our learning \napproach. Specifically, we address uncertainty adversarially by approximating \nprovided datasets annotations within a game-theoretic formulation of prediction \ntasks. The adversarial approximator is constrained to resemble the training \ndata annotations according to a set of specified features.This induces a \nlearned feature-based potential function that we then apply to new test cases. \nWe demonstrate the efficiency and predictive performance of our approach on the \nILSVRC2012 image dataset, showing significant improvements over existing \nmethods. \n</p>"}, "author": "Sima Behpour, Kris M. Kitani, Brian D. Ziebart", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890286", "id": "tag:google.com,2005:reader/item/0000000325373729", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Novel Stochastic Stratified Average Gradient Method: Convergence Rate and Its Complexity. (arXiv:1710.07783v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07783"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07783", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>SGD (Stochastic Gradient Descent) is a popular algorithm for large scale \noptimization problems due to its low iterative cost. However, SGD can not \nachieve linear convergence rate as FGD (Full Gradient Descent) because of the \ninherent gradient variance. To attack the problem, mini-batch SGD was proposed \nto get a trade-off in terms of convergence rate and iteration cost. In this \npaper, a general CVI (Convergence-Variance Inequality) equation is presented to \nstate formally the interaction of convergence rate and gradient variance. Then \na novel algorithm named SSAG (Stochastic Stratified Average Gradient) is \nintroduced to reduce gradient variance based on two techniques, stratified \nsampling and averaging over iterations that is a key idea in SAG (Stochastic \nAverage Gradient). Furthermore, SSAG can achieve linear convergence rate of \n$\\mathcal {O}((1-\\frac{\\mu}{8CL})^k)$ at smaller storage and iterative costs, \nwhere $C\\geq 2$ is the category number of training data. This convergence rate \ndepends mainly on the variance between classes, but not on the variance within \nthe classes. In the case of $C\\ll N$ ($N$ is the training data size), SSAG's \nconvergence rate is much better than SAG's convergence rate of $\\mathcal \n{O}((1-\\frac{\\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG \nand many other algorithms. \n</p>"}, "author": "Aixiang Chen, Bingchuan Chen, Xiaolong Chai, Rui Bian, Hengguang Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890285", "id": "tag:google.com,2005:reader/item/00000003253737a0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Learning-to-Infer Method for Real-Time Power Grid Topology Identification. (arXiv:1710.07818v1 [cs.LG])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07818"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07818", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Identifying arbitrary topologies of power networks in real time is a \ncomputationally hard problem due to the number of hypotheses that grows \nexponentially with the network size. A new \"Learning-to-Infer\" variational \ninference method is developed for efficient inference of every line status in \nthe network. Optimizing the variational model is transformed to and solved as a \ndiscriminative learning problem based on Monte Carlo samples generated with \npower flow simulations. A major advantage of the developed Learning-to-Infer \nmethod is that the labeled data used for training can be generated in an \narbitrarily large amount fast and at very little cost. As a result, the power \nof offline training is fully exploited to learn very complex classifiers for \neffective real-time topology identification. The proposed methods are evaluated \nin the IEEE 30, 118 and 300 bus systems. Excellent performance in identifying \narbitrary power network topologies in real time is achieved even with \nrelatively simple variational models and a reasonably small amount of data. \n</p>"}, "author": "Yue Zhao, Jianshu Chen, H. Vincent Poor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890284", "id": "tag:google.com,2005:reader/item/000000032537380a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Incomplete Dot Products for Dynamic Computation Scaling in Neural Network Inference. (arXiv:1710.07830v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07830"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07830", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose the use of incomplete dot products (IDP) to dynamically adjust the \nnumber of input channels used in each layer of a convolutional neural network \nduring feedforward inference. IDP adds monotonically non-increasing \ncoefficients, referred to as a \"profile\", to the channels during training. The \nprofile orders the contribution of each channel in non-increasing order. At \ninference time, the number of channels used can be dynamically adjusted to \ntrade off accuracy for lowered power consumption and reduced latency by \nselecting only a beginning subset of channels. This approach allows for a \nsingle network to dynamically scale over a computation range, as opposed to \ntraining and deploying multiple networks to support different levels of \ncomputation scaling. Additionally, we extend the notion to multiple profiles, \neach optimized for some specific range of computation scaling. We present \nexperiments on the computation and accuracy trade-offs of IDP for popular image \nclassification models and datasets. We demonstrate that, for MNIST and \nCIFAR-10, IDP reduces computation significantly, e.g., by 75%, without \nsignificantly compromising accuracy. We argue that IDP provides a convenient \nand effective means for devices to lower computation costs dynamically to \nreflect the current computation budget of the system. For example, VGG-16 with \n50% IDP (using only the first 50% of channels) achieves 70% in accuracy on the \nCIFAR-10 dataset compared to the standard network which achieves only 35% \naccuracy when using the reduced channel set. \n</p>"}, "author": "Bradley McDanel, Surat Teerapittayanon, H.T. Kung", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890283", "id": "tag:google.com,2005:reader/item/0000000325373865", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Neural Network Approximation using Tensor Sketching. (arXiv:1710.07850v1 [stat.ML])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07850"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07850", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks are powerful learning models that achieve \nstate-of-the-art performance on many computer vision, speech, and language \nprocessing tasks. In this paper, we study a fundamental question that arises \nwhen designing deep network architectures: Given a target network architecture \ncan we design a smaller network architecture that approximates the operation of \nthe target network? The question is, in part, motivated by the challenge of \nparameter reduction (compression) in modern deep neural networks, as the ever \nincreasing storage and memory requirements of these networks pose a problem in \nresource constrained environments. \n</p> \n<p>In this work, we focus on deep convolutional neural network architectures, \nand propose a novel randomized tensor sketching technique that we utilize to \ndevelop a unified framework for approximating the operation of both the \nconvolutional and fully connected layers. By applying the sketching technique \nalong different tensor dimensions, we design changes to the convolutional and \nfully connected layers that substantially reduce the number of effective \nparameters in a network. We show that the resulting smaller network can be \ntrained directly, and has a classification accuracy that is comparable to the \noriginal network. \n</p>"}, "author": "Shiva Prasad Kasiviswanathan, Nina Narodytska, Hongxia Jin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890282", "id": "tag:google.com,2005:reader/item/00000003253738bf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Triphone Embedding Improves Phoneme Recognition. (arXiv:1710.07868v1 [cs.SD])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07868"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07868", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present a novel Deep Triphone Embedding (DTE) \nrepresentation derived from Deep Neural Network (DNN) to encapsulate the \ndiscriminative information present in the adjoining speech frames. DTEs are \ngenerated using a four hidden layer DNN with 3000 nodes in each hidden layer at \nthe first-stage. This DNN is trained with the tied-triphone classification \naccuracy as an optimization criterion. Thereafter, we retain the activation \nvectors (3000) of the last hidden layer, for each speech MFCC frame, and \nperform dimension reduction to further obtain a 300 dimensional representation, \nwhich we termed as DTE. DTEs along with MFCC features are fed into a \nsecond-stage four hidden layer DNN, which is subsequently trained for the task \nof tied-triphone classification. Both DNNs are trained using tri-phone labels \ngenerated from a tied-state triphone HMM-GMM system, by performing a \nforced-alignment between the transcriptions and MFCC feature frames. We conduct \nthe experiments on publicly available TED-LIUM speech corpus. The results show \nthat the proposed DTE method provides an improvement of absolute 2.11% in \nphoneme recognition, when compared with a competitive hybrid tied-state \ntriphone HMM-DNN system. \n</p>"}, "author": "Mohit Yadav, Vivek Tyagi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890281", "id": "tag:google.com,2005:reader/item/0000000325373947", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Complexity of Graph-Based Reductions for Reachability in Markov Decision Processes. (arXiv:1710.07903v1 [cs.LO])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07903"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07903", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the never-worse relation (NWR) for Markov decision processes with an \ninfinite-horizon reachability objective. A state q is never worse than a state \np if the maximal probability of reaching the target set of states from p is at \nmost the same value from q, regardless of the probabilities labelling the \ntransitions. Extremal-probability states, end components, and essential states \nare all special cases of the equivalence relation induced by the NWR. Using the \nNWR, states in the same equivalence class can be collapsed. Then, actions \nleading to sub-optimal states can be removed. We show the natural decision \nproblem associated to computing the NWR is coNP-complete. Finally, we describe \nan incomplete polynomial-time iterative algorithm to under-approximate the NWR. \n</p>"}, "author": "Stephane Le Roux, Guillermo A. Perez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890280", "id": "tag:google.com,2005:reader/item/000000032537399e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Safety-Aware Apprenticeship Learning. (arXiv:1710.07983v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07983"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07983", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Apprenticeship learning (AL) is a class of \"learning from demonstrations\" \ntechniques where the reward function of a Markov Decision Process (MDP) is \nunknown to the learning agent and the agent has to derive a good policy by \nobserving an expert's demonstrations. In this paper, we study the problem of \nhow to make AL algorithms inherently safe while still meeting its learning \nobjective. We consider a setting where the unknown reward function is assumed \nto be a linear combination of a set of state features, and the safety property \nis specified in Probabilistic Computation Tree Logic (PCTL). By embedding \nprobabilistic model checking inside AL, we propose a novel \ncounterexample-guided approach that can ensure both safety and performance of \nthe learned policy. We demonstrate the effectiveness of our approach on several \nchallenging AL scenarios where safety is essential. \n</p>"}, "author": "Weichao Zhou, Wenchao Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890279", "id": "tag:google.com,2005:reader/item/00000003253739fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hierarchical State Abstractions for Decision-Making Problems with Computational Constraints. (arXiv:1710.07990v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07990"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07990", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c953577b\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c953577b&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this semi-tutorial paper, we first review the information-theoretic \napproach to account for the computational costs incurred during the search for \noptimal actions in a sequential decision-making problem. The traditional (MDP) \nframework ignores computational limitations while searching for optimal \npolicies, essentially assuming that the acting agent is perfectly rational and \naims for exact optimality. Using the free-energy, a variational principle is \nintroduced that accounts not only for the value of a policy alone, but also \nconsiders the cost of finding this optimal policy. The solution of the \nvariational equations arising from this formulation can be obtained using \nfamiliar Bellman-like value iterations from dynamic programming (DP) and the \nBlahut-Arimoto (BA) algorithm from rate distortion theory. Finally, we \ndemonstrate the utility of the approach for generating hierarchies of state \nabstractions that can be used to best exploit the available computational \nresources. A numerical example showcases these concepts for a path-planning \nproblem in a grid world environment. \n</p>"}, "author": "Daniel T. Larsson, Daniel Braun, Panagiotis Tsiotras", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890278", "id": "tag:google.com,2005:reader/item/0000000325373a56", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Probabilistic Pursuits on Graphs. (arXiv:1710.08107v1 [cs.DM])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08107"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08107", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider discrete dynamical systems of \"ant-like\" agents engaged in a \nsequence of pursuits on a graph environment. The agents emerge one by one at \nequal time intervals from a source vertex $s$ and pursue each other by greedily \nattempting to close the distance to their immediate predecessor, the agent that \nemerged just before them from $s$, until they arrive at the destination point \n$t$. Such pursuits have been investigated before in the continuous setting and \nin discrete time when the underlying environment is a regular grid. In both \nthese settings the agents' walks provably converge to a shortest path from $s$ \nto $t$. Furthermore, assuming a certain natural probability distribution over \nthe move choices of the agents on the grid (in case there are multiple shortest \npaths between an agent and its predecessor), the walks converge to the uniform \ndistribution over all shortest paths from $s$ to $t$. \n</p> \n<p>We study the evolution of agent walks over a general finite graph environment \n$G$. Our model is a natural generalization of the pursuit rule proposed for the \ncase of the grid. The main results are as follows. We show that \"convergence\" \nto the shortest paths in the sense of previous work extends to all \npseudo-modular graphs (i.e. graphs in which every three pairwise intersecting \ndisks have a nonempty intersection), and also to environments obtained by \ntaking graph products, generalizing previous results in two different ways. We \nshow that convergence to the shortest paths is also obtained by chordal graphs, \nand discuss some further positive and negative results for planar graphs. In \nthe most general case, convergence to the shortest paths is not guaranteed, and \nthe agents may get stuck on sets of recurrent, non-optimal walks from $s$ to \n$t$. However, we show that the limiting distributions of the agents' walks will \nalways be uniform distributions over some set of walks of equal length. \n</p>"}, "author": "Michael Amir, Alfred M. Bruckstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890277", "id": "tag:google.com,2005:reader/item/0000000325373aa9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Human-in-the-loop Artificial Intelligence. (arXiv:1710.08191v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08191"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08191", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Little by little, newspapers are revealing the bright future that Artificial \nIntelligence (AI) is building. Intelligent machines will help everywhere. \nHowever, this bright future has a dark side: a dramatic job market contraction \nbefore its unpredictable transformation. Hence, in a near future, large numbers \nof job seekers will need financial support while catching up with these novel \nunpredictable jobs. This possible job market crisis has an antidote inside. In \nfact, the rise of AI is sustained by the biggest knowledge theft of the recent \nyears. Learning AI machines are extracting knowledge from unaware skilled or \nunskilled workers by analyzing their interactions. By passionately doing their \njobs, these workers are digging their own graves. \n</p> \n<p>In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI) \nas a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward \naware and unaware knowledge producers with a different scheme: decisions of AI \nsystems generating revenues will repay the legitimate owners of the knowledge \nused for taking those decisions. As modern Robin Hoods, HIT-AI researchers \nshould fight for a fairer Artificial Intelligence that gives back what it \nsteals. \n</p>"}, "author": "Fabio Massimo Zanzotto", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890276", "id": "tag:google.com,2005:reader/item/0000000325373afc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Investigating the feature collection for semantic segmentation via single skip connection. (arXiv:1710.08192v1 [cs.CV])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08192"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08192", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Since the study of deep convolutional neural network became prevalent, one of \nthe important discoveries is that a feature map from a convolutional network \ncan be extracted before going into the fully connected layer and can be used as \na saliency map for object detection. Furthermore, the model can use features \nfrom each different layer for accurate object detection: the features from \ndifferent layers can have different properties. As the model goes deeper, it \nhas many latent skip connections and feature maps to elaborate object \ndetection. Although there are many intermediate layers that we can use for \nsemantic segmentation through skip connection, still the characteristics of \neach skip connection and the best skip connection for this task are uncertain. \nTherefore, in this study, we exhaustively research skip connections of \nstate-of-the-art deep convolutional networks and investigate the \ncharacteristics of the features from each intermediate layer. In addition, this \nstudy would suggest how to use a recent deep neural network model for semantic \nsegmentation and it would therefore become a cornerstone for later studies with \nthe state-of-the-art network models. \n</p>"}, "author": "Jonghwa Yim, Kyung-Ah Sohn", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890275", "id": "tag:google.com,2005:reader/item/0000000325373b37", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Autoencoder Feature Selector. (arXiv:1710.08310v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08310"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08310", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>High-dimensional data in many areas such as computer vision and machine \nlearning brings in computational and analytical difficulty. Feature selection \nwhich select a subset of features from original ones has been proven to be \neffective and efficient to deal with high-dimensional data. In this paper, we \npropose a novel AutoEncoder Feature Selector (AEFS) for unsupervised feature \nselection. AEFS is based on the autoencoder and the group lasso regularization. \nCompared to traditional feature selection methods, AEFS can select the most \nimportant features in spite of nonlinear and complex correlation among \nfeatures. It can be viewed as a nonlinear extension of the linear method \nregularized self-representation (RSR) for unsupervised feature selection. In \norder to deal with noise and corruption, we also propose robust AEFS. An \nefficient iterative algorithm is designed for model optimization and \nexperimental results verify the effectiveness and superiority of the proposed \nmethod. \n</p>"}, "author": "Kai Han, Chao Li, Xin Shi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890274", "id": "tag:google.com,2005:reader/item/0000000325373b6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "BENCHIP: Benchmarking Intelligence Processors. (arXiv:1710.08315v1 [cs.AI])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08315"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08315", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The increasing attention on deep learning has tremendously spurred the design \nof intelligence processing hardware. The variety of emerging intelligence \nprocessors requires standard benchmarks for fair comparison and system \noptimization (in both software and hardware). However, existing benchmarks are \nunsuitable for benchmarking intelligence processors due to their non-diversity \nand nonrepresentativeness. Also, the lack of a standard benchmarking \nmethodology further exacerbates this problem. In this paper, we propose \nBENCHIP, a benchmark suite and benchmarking methodology for intelligence \nprocessors. The benchmark suite in BENCHIP consists of two sets of benchmarks: \nmicrobenchmarks and macrobenchmarks. The microbenchmarks consist of \nsingle-layer networks. They are mainly designed for bottleneck analysis and \nsystem optimization. The macrobenchmarks contain state-of-the-art industrial \nnetworks, so as to offer a realistic comparison of different platforms. We also \npropose a standard benchmarking methodology built upon an industrial software \nstack and evaluation metrics that comprehensively reflect the various \ncharacteristics of the evaluated intelligence processors. BENCHIP is utilized \nfor evaluating various hardware platforms, including CPUs, GPUs, and \naccelerators. BENCHIP will be open-sourced soon. \n</p>"}, "author": "Jin-Hua Tao, Zi-Dong Du, Qi Guo, Hui-Ying Lan, Lei Zhang, Sheng-Yuan Zhou, Cong Liu, Hai-Feng Liu, Shan Tang, Allen Rush, Willian Chen, Shao-Li Liu, Yun-Ji Chen, Tian-Shi Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508872067890", "timestampUsec": "1508872067890273", "id": "tag:google.com,2005:reader/item/0000000325373bbd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Listening to the World Improves Speech Command Recognition. (arXiv:1710.08377v1 [cs.SD])", "published": 1508872068, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08377"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08377", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study transfer learning in convolutional network architectures applied to \nthe task of recognizing audio, such as environmental sound events and speech \ncommands. Our key finding is that not only is it possible to transfer \nrepresentations from an unrelated task like environmental sound classification \nto a voice-focused task like speech command recognition, but also that doing so \nimproves accuracies significantly. We also investigate the effect of increased \nmodel capacity for transfer learning audio, by first validating known results \nfrom the field of Computer Vision of achieving better accuracies with \nincreasingly deeper networks on two audio datasets: UrbanSound8k and the newly \nreleased Google Speech Commands dataset. Then we propose a simple multiscale \ninput representation using dilated convolutions and show that it is able to \naggregate larger contexts and increase classification performance. Further, the \nmodels trained using a combination of transfer learning and multiscale input \nrepresentations need only 40% of the training data to achieve similar \naccuracies as a freshly trained model with 100% of the training data. Finally, \nwe demonstrate a positive interaction effect for the multiscale input and \ntransfer learning, making a case for the joint application of the two \ntechniques. \n</p>"}, "author": "Brian McMahan, Delip Rao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068177", "id": "tag:google.com,2005:reader/item/00000003250c2f1a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Consistency of Graph-based Bayesian Learning and the Scalability of Sampling Algorithms. (arXiv:1710.07702v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07702"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07702", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A popular approach to semi-supervised learning proceeds by endowing the input \ndata with a graph structure in order to extract geometric information and \nincorporate it into a Bayesian framework. We introduce new theory that gives \nappropriate scalings of graph parameters that provably lead to a well-defined \nlimiting posterior as the size of the unlabeled data set grows. Furthermore, we \nshow that these consistency results have profound algorithmic implications. \nWhen consistency holds, carefully designed graph-based Markov chain Monte Carlo \nalgorithms are proved to have a uniform spectral gap, independent of the number \nof unlabeled inputs. Several numerical experiments corroborate both the \nstatistical consistency and the algorithmic scalability established by the \ntheory. \n</p>"}, "author": "Nicolas Garcia Trillos, Zachary Kaplan, Thabo Samakhoana, Daniel Sanz-Alonso", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068176", "id": "tag:google.com,2005:reader/item/00000003250c2fab", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Tight Excess Risk Bound via a Unified PAC-Bayesian-Rademacher-Shtarkov-MDL Complexity. (arXiv:1710.07732v1 [cs.LG])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07732"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07732", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a novel notion of complexity that interpolates between and \ngeneralizes some classic existing complexity notions in learning theory: for \nestimators like empirical risk minimization (ERM) with arbitrary bounded \nlosses, it is upper bounded in terms of data-independent Rademacher complexity; \nfor generalized Bayesian estimators, it is upper bounded by the data-dependent \ninformation complexity (also known as stochastic or PAC-Bayesian, \n$\\mathrm{KL}(\\text{posterior} \\operatorname{\\|} \\text{prior})$ complexity. For \n(penalized) ERM, the new complexity reduces to (generalized) normalized maximum \nlikelihood (NML) complexity, i.e. a minimax log-loss individual-sequence \nregret. Our first main result bounds excess risk in terms of the new \ncomplexity. Our second main result links the new complexity via Rademacher \ncomplexity to $L_2(P)$ entropy, thereby generalizing earlier results of Opper, \nHaussler, Lugosi, and Cesa-Bianchi who did the log-loss case with $L_\\infty$. \nTogether, these results recover optimal bounds for VC- and large (polynomial \nentropy) classes, replacing localized Rademacher complexity by a simpler \nanalysis which almost completely separates the two aspects that determine the \nachievable rates: 'easiness' (Bernstein) conditions and model complexity. \n</p>"}, "author": "Peter D. Gr&#xfc;nwald, Nishant A. Mehta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068175", "id": "tag:google.com,2005:reader/item/00000003250c3017", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Discrete Weights Using the Local Reparameterization Trick. (arXiv:1710.07739v2 [cs.LG] UPDATED)", "published": 1509484038, "updated": 1509484044, "canonical": [{"href": "http://arxiv.org/abs/1710.07739"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07739", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent breakthroughs in computer vision make use of large deep neural \nnetworks, utilizing the substantial speedup offered by GPUs. For applications \nrunning on limited hardware, however, high precision real-time processing can \nstill be a challenge. One approach to solving this problem is training networks \nwith binary or ternary weights, thus removing the need to calculate \nmultiplications and significantly reducing memory size. In this work, we \nintroduce LR-nets (Local reparameterization networks), a new method for \ntraining neural networks with discrete weights using stochastic parameters. We \nshow how a simple modification to the local reparameterization trick, \npreviously used to train Gaussian distributed weights, enables the training of \ndiscrete weights. Using the proposed training we test both binary and ternary \nmodels on MNIST, CIFAR-10 and ImageNet benchmarks and reach state-of-the-art \nresults on most experiments. \n</p>"}, "author": "Oran Shayer, Dan Levi, Ethan Fetaya", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068174", "id": "tag:google.com,2005:reader/item/00000003250c30d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Black-box Iterative Machine Teaching. (arXiv:1710.07742v2 [stat.ML] UPDATED)", "published": 1511308877, "updated": 1511308892, "canonical": [{"href": "http://arxiv.org/abs/1710.07742"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07742", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c95359b6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c95359b6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we make an important step towards the black-box machine \nteaching by considering the cross-space teaching setting, where the teacher and \nthe learner use different feature representations and the teacher can not fully \nobserve the learner's model. In such scenario, we study how the teacher is \nstill able to teach the learner to achieve a faster convergence rate than the \ntraditional passive learning. We propose an active teacher model that can \nactively query the learner (i.e., make the learner take exams) for estimating \nthe learner's status, and provide the sample complexity for both teaching and \nquery, respectively. In the experiments, we compare the proposed active teacher \nwith the omniscient teacher and verify the effectiveness of the active teacher \nmodel. \n</p>"}, "author": "Weiyang Liu, Bo Dai, Xingguo Li, James M. Rehg, Le Song", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068173", "id": "tag:google.com,2005:reader/item/00000003250c3123", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Novel Stochastic Stratified Average Gradient Method: Convergence Rate and Its Complexity. (arXiv:1710.07783v1 [cs.AI])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07783"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07783", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c95e04eb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c95e04eb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>SGD (Stochastic Gradient Descent) is a popular algorithm for large scale \noptimization problems due to its low iterative cost. However, SGD can not \nachieve linear convergence rate as FGD (Full Gradient Descent) because of the \ninherent gradient variance. To attack the problem, mini-batch SGD was proposed \nto get a trade-off in terms of convergence rate and iteration cost. In this \npaper, a general CVI (Convergence-Variance Inequality) equation is presented to \nstate formally the interaction of convergence rate and gradient variance. Then \na novel algorithm named SSAG (Stochastic Stratified Average Gradient) is \nintroduced to reduce gradient variance based on two techniques, stratified \nsampling and averaging over iterations that is a key idea in SAG (Stochastic \nAverage Gradient). Furthermore, SSAG can achieve linear convergence rate of \n$\\mathcal {O}((1-\\frac{\\mu}{8CL})^k)$ at smaller storage and iterative costs, \nwhere $C\\geq 2$ is the category number of training data. This convergence rate \ndepends mainly on the variance between classes, but not on the variance within \nthe classes. In the case of $C\\ll N$ ($N$ is the training data size), SSAG's \nconvergence rate is much better than SAG's convergence rate of $\\mathcal \n{O}((1-\\frac{\\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG \nand many other algorithms. \n</p>"}, "author": "Aixiang Chen, Bingchuan Chen, Xiaolong Chai, Rui Bian, Hengguang Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068172", "id": "tag:google.com,2005:reader/item/00000003250c3157", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal Rates for Learning with Nystr\\\"om Stochastic Gradient Methods. (arXiv:1710.07797v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07797"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07797", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In the setting of nonparametric regression, we propose and study a \ncombination of stochastic gradient methods with Nystr\\\"om subsampling, allowing \nmultiple passes over the data and mini-batches. Generalization error bounds for \nthe studied algorithm are provided. Particularly, optimal learning rates are \nderived considering different possible choices of the step-size, the mini-batch \nsize, the number of iterations/passes, and the subsampling level. In comparison \nwith state-of-the-art algorithms such as the classic stochastic gradient \nmethods and kernel ridge regression with Nystr\\\"om, the studied algorithm has \nadvantages on the computational complexity, while achieving the same optimal \nlearning rates. Moreover, our results indicate that using mini-batches can \nreduce the total computational cost while achieving the same optimal \nstatistical results. \n</p>"}, "author": "Junhong Lin, Lorenzo Rosasco", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068171", "id": "tag:google.com,2005:reader/item/00000003250c31c2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Zeroth-Order Online Alternating Direction Method of Multipliers: Convergence Analysis and Applications. (arXiv:1710.07804v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07804"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07804", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we design and analyze a new zeroth-order online algorithm, \nnamely, the zeroth-order online alternating direction method of multipliers \n(ZOO-ADMM), which enjoys dual advantages of being gradient-free operation and \nemploying the ADMM to accommodate complex structured regularizers. Compared to \nthe first-order gradient-based online algorithm, we show that ZOO-ADMM requires \n$\\sqrt{m}$ times more iterations, leading to a convergence rate of \n$O(\\sqrt{m}/\\sqrt{T})$, where $m$ is the number of optimization variables, and \n$T$ is the number of iterations. To accelerate ZOO-ADMM, we propose two \nminibatch strategies: gradient sample averaging and observation averaging, \nresulting in an improved convergence rate of $O(\\sqrt{1+q^{-1}m}/\\sqrt{T})$, \nwhere $q$ is the minibatch size. In addition to convergence analysis, we also \ndemonstrate ZOO-ADMM to applications in signal processing, statistics, and \nmachine learning. \n</p>"}, "author": "Sijia Liu, Jie Chen, Pin-Yu Chen, Alfred O. Hero", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068170", "id": "tag:google.com,2005:reader/item/00000003250c323f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Learning-to-Infer Method for Real-Time Power Grid Topology Identification. (arXiv:1710.07818v1 [cs.LG])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07818"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07818", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Identifying arbitrary topologies of power networks in real time is a \ncomputationally hard problem due to the number of hypotheses that grows \nexponentially with the network size. A new \"Learning-to-Infer\" variational \ninference method is developed for efficient inference of every line status in \nthe network. Optimizing the variational model is transformed to and solved as a \ndiscriminative learning problem based on Monte Carlo samples generated with \npower flow simulations. A major advantage of the developed Learning-to-Infer \nmethod is that the labeled data used for training can be generated in an \narbitrarily large amount fast and at very little cost. As a result, the power \nof offline training is fully exploited to learn very complex classifiers for \neffective real-time topology identification. The proposed methods are evaluated \nin the IEEE 30, 118 and 300 bus systems. Excellent performance in identifying \narbitrary power network topologies in real time is achieved even with \nrelatively simple variational models and a reasonably small amount of data. \n</p>"}, "author": "Yue Zhao, Jianshu Chen, H. Vincent Poor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068169", "id": "tag:google.com,2005:reader/item/00000003250c3295", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Neural Network Approximation using Tensor Sketching. (arXiv:1710.07850v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07850"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07850", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks are powerful learning models that achieve \nstate-of-the-art performance on many computer vision, speech, and language \nprocessing tasks. In this paper, we study a fundamental question that arises \nwhen designing deep network architectures: Given a target network architecture \ncan we design a smaller network architecture that approximates the operation of \nthe target network? The question is, in part, motivated by the challenge of \nparameter reduction (compression) in modern deep neural networks, as the ever \nincreasing storage and memory requirements of these networks pose a problem in \nresource constrained environments. \n</p> \n<p>In this work, we focus on deep convolutional neural network architectures, \nand propose a novel randomized tensor sketching technique that we utilize to \ndevelop a unified framework for approximating the operation of both the \nconvolutional and fully connected layers. By applying the sketching technique \nalong different tensor dimensions, we design changes to the convolutional and \nfully connected layers that substantially reduce the number of effective \nparameters in a network. We show that the resulting smaller network can be \ntrained directly, and has a classification accuracy that is comparable to the \noriginal network. \n</p>"}, "author": "Shiva Prasad Kasiviswanathan, Nina Narodytska, Hongxia Jin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068168", "id": "tag:google.com,2005:reader/item/00000003250c32de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Insulin Regimen ML-based control for T2DM patients. (arXiv:1710.07855v1 [q-bio.QM])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07855"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07855", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>\\begin{abstract} We model individual T2DM patient blood glucose level (BGL) \nby stochastic process with discrete number of states mainly but not solely \ngoverned by medication regimen (e.g. insulin injections). BGL states change \notherwise according to various physiological triggers which render a \nstochastic, statistically unknown, yet assumed to be quasi-stationary, nature \nof the process. In order to express incentive for being in desired healthy BGL \nwe heuristically define a reward function which returns positive values for \ndesirable BG levels and negative values for undesirable BG levels. The state \nspace consists of sufficient number of states in order to allow for memoryless \nassumption. This, in turn, allows to formulate Markov Decision Process (MDP), \nwith an objective to maximize the total reward, summarized over a long run. The \nprobability law is found by model-based reinforcement learning (RL) and the \noptimal insulin treatment policy is retrieved from MDP solution. \n</p>"}, "author": "Mark Shifrin, Hava Siegelmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068167", "id": "tag:google.com,2005:reader/item/00000003250c3326", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Iteratively reweighted $\\ell_1$ algorithms with extrapolation. (arXiv:1710.07886v2 [math.OC] UPDATED)", "published": 1511308877, "updated": 1511308892, "canonical": [{"href": "http://arxiv.org/abs/1710.07886"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07886", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Iteratively reweighted $\\ell_1$ algorithm is a popular algorithm for solving \na large class of optimization problems whose objective is the sum of a \nLipschitz differentiable loss function and a possibly nonconvex sparsity \ninducing regularizer. In this paper, motivated by the success of extrapolation \ntechniques in accelerating first-order methods, we study how widely used \nextrapolation techniques such as those in [4,5,22,28] can be incorporated to \npossibly accelerate the iteratively reweighted $\\ell_1$ algorithm. We consider \nthree versions of such algorithms. For each version, we exhibit an explicitly \ncheckable condition on the extrapolation parameters so that the sequence \ngenerated provably clusters at a stationary point of the optimization problem. \nWe also investigate global convergence under additional Kurdyka-$\\L$ojasiewicz \nassumptions on certain potential functions. Our numerical experiments show that \nour algorithms usually outperform the general iterative shrinkage and \nthresholding algorithm in [21] and an adaptation of the iteratively reweighted \n$\\ell_1$ algorithm in [23, Algorithm 7] with nonmonotone line-search for \nsolving random instances of log penalty regularized least squares problems in \nterms of both CPU time and solution quality. \n</p>"}, "author": "Peiran Yu, Ting Kei Pong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068166", "id": "tag:google.com,2005:reader/item/00000003250c337f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Elliptical modeling and pattern analysis for perturbation models and classfication. (arXiv:1710.07939v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07939"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07939", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The characteristics (or numerical patterns) of a feature vector in the \ntransform domain of a perturbation model differ significantly from those of its \ncorresponding feature vector in the input domain. These differences - caused by \nthe perturbation techniques used for the transformation of feature patterns - \ndegrade the performance of machine learning techniques in the transform domain. \nIn this paper, we proposed a nonlinear parametric perturbation model that \ntransforms the input feature patterns to a set of elliptical patterns, and \nstudied the performance degradation issues associated with random forest \nclassification technique using both the input and transform domain features. \nCompared with the linear transformation such as Principal Component Analysis \n(PCA), the proposed method requires less statistical assumptions and is highly \nsuitable for the applications such as data privacy and security due to the \ndifficulty of inverting the elliptical patterns from the transform domain to \nthe input domain. In addition, we adopted a flexible block-wise dimensionality \nreduction step in the proposed method to accommodate the possible \nhigh-dimensional data in modern applications. We evaluated the empirical \nperformance of the proposed method on a network intrusion data set and a \nbiological data set, and compared the results with PCA in terms of \nclassification performance and data privacy protection (measured by the blind \nsource separation attack and signal interference ratio). Both results confirmed \nthe superior performance of the proposed elliptical transformation. \n</p>"}, "author": "Shan Suthaharan, Weining Shen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068165", "id": "tag:google.com,2005:reader/item/00000003250c33c2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "WristAuthen: A Dynamic Time Wrapping Approach for User Authentication by Hand-Interaction through Wrist-Worn Devices. (arXiv:1710.07941v1 [cs.HC])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07941"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07941", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The growing trend of using wearable devices for context-aware computing and \npervasive sensing systems has raised its potentials for quick and reliable \nauthentication techniques. Since personal writing habitats differ from each \nother, it is possible to realize user authentication through writing. This is \nof great significance as sensible information is easily collected by these \ndevices. This paper presents a novel user authentication system through \nwrist-worn devices by analyzing the interaction behavior with users, which is \nboth accurate and efficient for future usage. The key feature of our approach \nlies in using much more effective Savitzky-Golay filter and Dynamic Time \nWrapping method to obtain fine-grained writing metrics for user authentication. \nThese new metrics are relatively unique from person to person and independent \nof the computing platform. Analyses are conducted on the wristband-interaction \ndata collected from 50 users with diversity in gender, age, and height. \nExtensive experimental results show that the proposed approach can identify \nusers in a timely and accurate manner, with a false-negative rate of 1.78\\%, \nfalse-positive rate of 6.7\\%, and Area Under ROC Curve of 0.983 . Additional \nexamination on robustness to various mimic attacks, tolerance to training data, \nand comparisons to further analyze the applicability. \n</p>"}, "author": "Qi Lyu, Zhifeng Kong, Chao Shen, Tianwei Yue", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068164", "id": "tag:google.com,2005:reader/item/00000003250c343f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Approach to One-Bit Compressed Sensing Based on Probably Approximately Correct Learning Theory. (arXiv:1710.07973v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07973"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07973", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c95e08c0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c95e08c0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, the problem of one-bit compressed sensing (OBCS) is formulated \nas a problem in probably approximately correct (PAC) learning. It is shown that \nthe Vapnik-Chervonenkis (VC-) dimension of the set of half-spaces in \n$\\mathbb{R}^n$ generated by $k$-sparse vectors is bounded below by $k \\lg \n(n/k)$ and above by $2k \\lg (n/k)$, plus some round-off terms. By coupling this \nestimate with well-established results in PAC learning theory, we show that a \nconsistent algorithm can recover a $k$-sparse vector with $O(k \\lg (n/k))$ \nmeasurements, given only the signs of the measurement vector. This result holds \nfor \\textit{all} probability measures on $\\mathbb{R}^n$. It is further shown \nthat random sign-flipping errors result only in an increase in the constant in \nthe $O(k \\lg (n/k))$ estimate. Because constructing a consistent algorithm is \nnot straight-forward, we present a heuristic based on the $\\ell_1$-norm support \nvector machine, and illustrate that its computational performance is superior \nto a currently popular method. \n</p>"}, "author": "Mehmet Eren Ahsen, Mathukumalli Vidyasagar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068163", "id": "tag:google.com,2005:reader/item/00000003250c34b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hierarchical State Abstractions for Decision-Making Problems with Computational Constraints. (arXiv:1710.07990v1 [cs.AI])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07990"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07990", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this semi-tutorial paper, we first review the information-theoretic \napproach to account for the computational costs incurred during the search for \noptimal actions in a sequential decision-making problem. The traditional (MDP) \nframework ignores computational limitations while searching for optimal \npolicies, essentially assuming that the acting agent is perfectly rational and \naims for exact optimality. Using the free-energy, a variational principle is \nintroduced that accounts not only for the value of a policy alone, but also \nconsiders the cost of finding this optimal policy. The solution of the \nvariational equations arising from this formulation can be obtained using \nfamiliar Bellman-like value iterations from dynamic programming (DP) and the \nBlahut-Arimoto (BA) algorithm from rate distortion theory. Finally, we \ndemonstrate the utility of the approach for generating hierarchies of state \nabstractions that can be used to best exploit the available computational \nresources. A numerical example showcases these concepts for a path-planning \nproblem in a grid world environment. \n</p>"}, "author": "Daniel T. Larsson, Daniel Braun, Panagiotis Tsiotras", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068162", "id": "tag:google.com,2005:reader/item/00000003250c3517", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Rethinking Convolutional Semantic Segmentation Learning. (arXiv:1710.07991v1 [cs.LG])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07991"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07991", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep convolutional semantic segmentation (DCSS) learning doesn't converge to \nan optimal local minimum with random parameters initializations; a pre-trained \nmodel on the same domain becomes necessary to achieve convergence.In this work, \nwe propose a joint cooperative end-to-end learning method for DCSS. It \naddresses many drawbacks with existing deep semantic segmentation learning; the \nproposed approach simultaneously learn both segmentation and classification; \ntaking away the essential need of the pre-trained model for learning \nconvergence. We present an improved inception based architecture with partial \nattention gating (PAG) over encoder information. The PAG also adds to achieve \nfaster convergence and better accuracy for segmentation task. We will show the \neffectiveness of this learning on a diabetic retinopathy classification and \nsegmentation dataset. \n</p>"}, "author": "Mrinal Haloi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068161", "id": "tag:google.com,2005:reader/item/00000003250c3577", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Smart \"Predict, then Optimize\". (arXiv:1710.08005v1 [math.OC])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08005"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08005", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many real-world analytics problems involve two significant challenges: \nprediction and optimization. Due to the typically complex nature of each \nchallenge, the standard paradigm is to predict, then optimize. By and large, \nmachine learning tools are intended to minimize prediction error and do not \naccount for how the predictions will be used in a downstream optimization \nproblem. In contrast, we propose a new and very general framework, called Smart \n\"Predict, then Optimize\" (SPO), which directly leverages the optimization \nproblem structure, i.e., its objective and constraints, for designing \nsuccessful analytics tools. A key component of our framework is the SPO loss \nfunction, which measures the quality of a prediction by comparing the objective \nvalues of the solutions generated using the predicted and observed parameters, \nrespectively. Training a model with respect to the SPO loss is computationally \nchallenging, and therefore we also develop a surrogate loss function, called \nthe SPO+ loss, which upper bounds the SPO loss, has desirable convexity \nproperties, and is statistically consistent under mild conditions. We also \npropose a stochastic gradient descent algorithm which allows for situations in \nwhich the number of training samples is large, model regularization is desired, \nand/or the optimization problem of interest is nonlinear or integer. Finally, \nwe perform computational experiments to empirically verify the success of our \nSPO framework in comparison to the standard predict-then-optimize approach. \n</p>"}, "author": "Adam N. Elmachtoub, Paul Grigas", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068160", "id": "tag:google.com,2005:reader/item/00000003250c35ae", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exploiting generalization in the subspaces for faster model-based learning. (arXiv:1710.08012v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08012"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08012", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Due to the lack of enough generalization in the state-space, common methods \nin Reinforcement Learning (RL) suffer from slow learning speed especially in \nthe early learning trials. This paper introduces a model-based method in \ndiscrete state-spaces for increasing learning speed in terms of required \nexperience (but not required computational time) by exploiting generalization \nin the experiences of the subspaces. A subspace is formed by choosing a subset \nof features in the original state representation (full-space). Generalization \nand faster learning in a subspace are due to many-to-one mapping of experiences \nfrom the full-space to each state in the subspace. Nevertheless, due to \ninherent perceptual aliasing in the subspaces, the policy suggested by each \nsubspace does not generally converge to the optimal policy. Our approach, \ncalled Model Based Learning with Subspaces (MoBLeS), calculates confidence \nintervals of the estimated Q-values in the full-space and in the subspaces. \nThese confidence intervals are used in the decision making, such that the agent \nbenefits the most from the possible generalization while avoiding from \ndetriment of the perceptual aliasing in the subspaces. Convergence of MoBLeS to \nthe optimal policy is theoretically investigated. Additionally, we show through \nseveral experiments that MoBLeS improves the learning speed in the early \ntrials. \n</p>"}, "author": "Maryam Hashemzadeh, Reshad Hosseini, Majid Nili Ahmadabadi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068159", "id": "tag:google.com,2005:reader/item/00000003250c3609", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sequential Matrix Completion. (arXiv:1710.08045v1 [cs.IR])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08045"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08045", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a novel algorithm for sequential matrix completion in a \nrecommender system setting, where the $(i,j)$th entry of the matrix corresponds \nto a user $i$'s rating of product $j$. The objective of the algorithm is to \nprovide a sequential policy for user-product pair recommendation which will \nyield the highest possible ratings after a finite time horizon. The algorithm \nuses a Gamma process factor model with two posterior-focused bandit policies, \nThompson Sampling and Information-Directed Sampling. While Thompson Sampling \nshows competitive performance in simulations, state-of-the-art performance is \nobtained from Information-Directed Sampling, which makes its recommendations \nbased off a ratio between the expected reward and a measure of information \ngain. To our knowledge, this is the first implementation of Information \nDirected Sampling on large real datasets. \n</p> \n<p>This approach contributes to a recent line of research on bandit approaches \nto collaborative filtering including Kawale et al. (2015), Li et al. (2010), \nBresler et al. (2014), Li et al. (2016), Deshpande &amp; Montanari (2012), and Zhao \net al. (2013). The setting of this paper, as has been noted in Kawale et al. \n(2015) and Zhao et al. (2013), presents significant challenges to bounding \nregret after finite horizons. We discuss these challenges in relation to \nsimpler models for bandits with side information, such as linear or gaussian \nprocess bandits, and hope the experiments presented here motivate further \nresearch toward theoretical guarantees. \n</p>"}, "author": "Annie Marsden, Sergio Bacallado", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068158", "id": "tag:google.com,2005:reader/item/00000003250c36a2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Online Boosting Algorithms for Multi-label Ranking. (arXiv:1710.08079v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08079"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08079", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the multi-label ranking approach to multi-label learning. \nBoosting is a natural method for multi-label ranking as it aggregates weak \npredictions through majority votes, which can be directly used as scores to \nproduce a ranking of the labels. We design online boosting algorithms with \nprovable loss bounds for multi-label ranking. We show that our first algorithm \nis optimal in terms of the number of learners required to attain a desired \naccuracy, but it requires knowledge of the edge of the weak learners. We also \ndesign an adaptive algorithm that does not require this knowledge and is hence \nmore practical. Experimental results on real data sets demonstrate that our \nalgorithms are at least as good as existing batch boosting algorithms. \n</p>"}, "author": "Young Hun Jung, Ambuj Tewari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068157", "id": "tag:google.com,2005:reader/item/00000003250c36fc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SMSSVD - SubMatrix Selection Singular Value Decomposition. (arXiv:1710.08144v1 [stat.AP])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08144"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08144", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>High throughput biomedical measurements normally capture multiple overlaid \nbiologically relevant signals and often also signals representing different \ntypes of technical artefacts like e.g. batch effects. Signal identification and \ndecomposition are accordingly main objectives in statistical biomedical \nmodeling and data analysis. Existing methods, aimed at signal reconstruction \nand deconvolution, in general, are either supervised, contain parameters that \nneed to be estimated or present other types of ad hoc features. We here \nintroduce SubMatrix Selection SingularValue Decomposition (SMSSVD), a \nparameter-free unsupervised signal decomposition and dimension reduction \nmethod, designed to reduce noise, adaptively for each low-rank-signal in a \ngiven data matrix, and represent the signals in the data in a way that enable \nunbiased exploratory analysis and reconstruction of multiple overlaid signals, \nincluding identifying groups of variables that drive different signals. \n</p> \n<p>The Submatrix Selection Singular Value Decomposition (SMSSVD) method produces \na denoised signal decomposition from a given data matrix. The SMSSVD method \nguarantees orthogonality between signal components in a straightforward manner \nand it is designed to make automation possible. We illustrate SMSSVD by \napplying it to several real and synthetic datasets and compare its performance \nto golden standard methods like PCA (Principal Component Analysis) and SPC \n(Sparse Principal Components, using Lasso constraints). The SMSSVD is \ncomputationally efficient and despite being a parameter-free method, in \ngeneral, outperforms existing statistical learning methods. \n</p> \n<p>A Julia implementation of SMSSVD is openly available on GitHub \n(https://github.com/rasmushenningsson/SMSSVD.jl). \n</p>"}, "author": "Rasmus Henningsson (1,2), Magnus Fontes (1,2,3,4) ((1) The Centre for Mathematical Sciences, Lund University, Sweden, (2) The International Group for Data Analysis, Institut Pasteur, Paris, France, (3) The Center for Genomic Medicine, Rigshospitalet, Copenhagen, Denmark, (4) Persimune, The Centre of Excellence for Personalized Medicine, Copenhagen, Denmark)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068156", "id": "tag:google.com,2005:reader/item/00000003250c37a0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast MCMC sampling algorithms on polytopes. (arXiv:1710.08165v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08165"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08165", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose and analyze two new MCMC sampling algorithms, the Vaidya walk and \nthe John walk, for generating samples from the uniform distribution over a \npolytope. Both random walks are sampling algorithms derived from interior point \nmethods. The former is based on volumetric-logarithmic barrier introduced by \nVaidya whereas the latter uses John's ellipsoids. We show that the Vaidya walk \nmixes in significantly fewer steps than the logarithmic-barrier based Dikin \nwalk studied in past work. For a polytope in $\\mathbb{R}^d$ defined by $n &gt;d$ \nlinear constraints, we show that the mixing time from a warm start is bounded \nas $\\mathcal{O}(n^{0.5}d^{1.5})$, compared to the $\\mathcal{O}(nd)$ mixing time \nbound for the Dikin walk. The cost of each step of the Vaidya walk is of the \nsame order as the Dikin walk, and at most twice as large in terms of constant \npre-factors. For the John walk, we prove an \n$\\mathcal{O}(d^{2.5}\\cdot\\log^4(n/d))$ bound on its mixing time and conjecture \nthat an improved variant of it could achieve a mixing time of \n$\\mathcal{O}(d^2\\cdot\\text{polylog}(n/d))$. Additionally, we propose variants \nof the Vaidya and John walks that mix in polynomial time from a deterministic \nstarting point. We illustrate the speed-up of the Vaidya walk over the Dikin \nwalk via several numerical examples. \n</p>"}, "author": "Yuansi Chen, Raaz Dwivedi, Martin J. Wainwright, Bin Yu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068155", "id": "tag:google.com,2005:reader/item/00000003250c3815", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Interactive Visual Data Exploration with Subjective Feedback: An Information-Theoretic Approach. (arXiv:1710.08167v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08167"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08167", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Visual exploration of high-dimensional real-valued datasets is a fundamental \ntask in exploratory data analysis (EDA). Existing methods use predefined \ncriteria to choose the representation of data. There is a lack of methods that \n(i) elicit from the user what she has learned from the data and (ii) show \npatterns that she does not know yet. We construct a theoretical model where \nidentified patterns can be input as knowledge to the system. The knowledge \nsyntax here is intuitive, such as \"this set of points forms a cluster\", and \nrequires no knowledge of maths. This background knowledge is used to find a \nMaximum Entropy distribution of the data, after which the system provides the \nuser data projections in which the data and the Maximum Entropy distribution \ndiffer the most, hence showing the user aspects of the data that are maximally \ninformative given the user's current knowledge. We provide an open source EDA \nsystem with tailored interactive visualizations to demonstrate these concepts. \nWe study the performance of the system and present use cases on both synthetic \nand real data. We find that the model and the prototype system allow the user \nto learn information efficiently from various data sources and the system works \nsufficiently fast in practice. We conclude that the information theoretic \napproach to exploratory data analysis where patterns observed by a user are \nformalized as constraints provides a principled, intuitive, and efficient basis \nfor constructing an EDA system. \n</p>"}, "author": "Kai Puolam&#xe4;ki, Emilia Oikarinen, Bo Kang, Jefrey Lijffijt, Tijl De Bie", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068154", "id": "tag:google.com,2005:reader/item/00000003250c3884", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Progressive Learning for Systematic Design of Large Neural Networks. (arXiv:1710.08177v1 [cs.NE])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08177"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08177", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c95e0c5e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c95e0c5e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We develop an algorithm for systematic design of a large artificial neural \nnetwork using a progression property. We find that some non-linear functions, \nsuch as the rectifier linear unit and its derivatives, hold the property. The \nsystematic design addresses the choice of network size and regularization of \nparameters. The number of nodes and layers in network increases in progression \nwith the objective of consistently reducing an appropriate cost. Each layer is \noptimized at a time, where appropriate parameters are learned using convex \noptimization. Regularization parameters for convex optimization do not need a \nsignificant manual effort for tuning. We also use random instances for some \nweight matrices, and that helps to reduce the number of parameters we learn. \nThe developed network is expected to show good generalization power due to \nappropriate regularization and use of random weights in the layers. This \nexpectation is verified by extensive experiments for classification and \nregression problems, using standard databases. \n</p>"}, "author": "Saikat Chatterjee, Alireza M. Javid, Mostafa Sadeghi, Partha P. Mitra, Mikael Skoglund", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068153", "id": "tag:google.com,2005:reader/item/00000003250c38ed", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Autoencoder Feature Selector. (arXiv:1710.08310v1 [cs.AI])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08310"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08310", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c96886fe\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c96886fe&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>High-dimensional data in many areas such as computer vision and machine \nlearning brings in computational and analytical difficulty. Feature selection \nwhich select a subset of features from original ones has been proven to be \neffective and efficient to deal with high-dimensional data. In this paper, we \npropose a novel AutoEncoder Feature Selector (AEFS) for unsupervised feature \nselection. AEFS is based on the autoencoder and the group lasso regularization. \nCompared to traditional feature selection methods, AEFS can select the most \nimportant features in spite of nonlinear and complex correlation among \nfeatures. It can be viewed as a nonlinear extension of the linear method \nregularized self-representation (RSR) for unsupervised feature selection. In \norder to deal with noise and corruption, we also propose robust AEFS. An \nefficient iterative algorithm is designed for model optimization and \nexperimental results verify the effectiveness and superiority of the proposed \nmethod. \n</p>"}, "author": "Kai Han, Chao Li, Xin Shi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068152", "id": "tag:google.com,2005:reader/item/00000003250c3949", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stability and Generalization of Learning Algorithms that Converge to Global Optima. (arXiv:1710.08402v1 [stat.ML])", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08402"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08402", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We establish novel generalization bounds for learning algorithms that \nconverge to global minima. We do so by deriving black-box stability results \nthat only depend on the convergence of a learning algorithm and the geometry \naround the minimizers of the loss function. The results are shown for nonconvex \nloss functions satisfying the Polyak-{\\L}ojasiewicz (PL) and the quadratic \ngrowth (QG) conditions. We further show that these conditions arise for some \nneural networks with linear activations. We use our black-box results to \nestablish the stability of optimization algorithms such as stochastic gradient \ndescent (SGD), gradient descent (GD), randomized coordinate descent (RCD), and \nthe stochastic variance reduced gradient method (SVRG), in both the PL and the \nstrongly convex setting. Our results match or improve state-of-the-art \ngeneralization bounds and can easily be extended to similar optimization \nalgorithms. Finally, we show that although our results imply comparable \nstability for SGD and GD in the PL setting, there exist simple neural networks \nwith multiple local minima where SGD is stable but GD is not. \n</p>"}, "author": "Zachary Charles, Dimitris Papailiopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068149", "id": "tag:google.com,2005:reader/item/00000003250c399c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal Rates for Multi-pass Stochastic Gradient Methods. (arXiv:1605.08882v2 [cs.LG] UPDATED)", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1605.08882"}], "alternate": [{"href": "http://arxiv.org/abs/1605.08882", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We analyze the learning properties of the stochastic gradient method when \nmultiple passes over the data and mini-batches are allowed. We study how \nregularization properties are controlled by the step-size, the number of passes \nand the mini-batch size. In particular, we consider the square loss and show \nthat for a universal step-size choice, the number of passes acts as a \nregularization parameter, and optimal finite sample bounds can be achieved by \nearly-stopping. Moreover, we show that larger step-sizes are allowed when \nconsidering mini-batches. Our analysis is based on a unifying approach, \nencompassing both batch and stochastic gradient methods as special cases. As a \nbyproduct, we derive optimal convergence results for batch gradient methods \n(even in the non-attainable cases). \n</p>"}, "author": "Junhong Lin, Lorenzo Rosasco", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508854086068", "timestampUsec": "1508854086068131", "id": "tag:google.com,2005:reader/item/00000003250c3e87", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Feeling of Success: Does Touch Sensing Help Predict Grasp Outcomes?. (arXiv:1710.05512v1 [cs.RO] CROSS LISTED)", "published": 1508854086, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05512"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05512", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A successful grasp requires careful balancing of the contact forces. Deducing \nwhether a particular grasp will be successful from indirect measurements, such \nas vision, is therefore quite challenging, and direct sensing of contacts \nthrough touch sensing provides an appealing avenue toward more successful and \nconsistent robotic grasping. However, in order to fully evaluate the value of \ntouch sensing for grasp outcome prediction, we must understand how touch \nsensing can influence outcome prediction accuracy when combined with other \nmodalities. Doing so using conventional model-based techniques is exceptionally \ndifficult. In this work, we investigate the question of whether touch sensing \naids in predicting grasp outcomes within a multimodal sensing framework that \ncombines vision and touch. To that end, we collected more than 9,000 grasping \ntrials using a two-finger gripper equipped with GelSight high-resolution \ntactile sensors on each finger, and evaluated visuo-tactile deep neural network \nmodels to directly predict grasp outcomes from either modality individually, \nand from both modalities together. Our experimental results indicate that \nincorporating tactile readings substantially improve grasping performance. \n</p>"}, "author": "Roberto Calandra, Andrew Owens, Manu Upadhyaya, Wenzhen Yuan, Justin Lin, Edward H. Adelson, Sergey Levine", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328718", "id": "tag:google.com,2005:reader/item/0000000324a850b1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Point Neurons with Conductance-Based Synapses in the Neural Engineering Framework. (arXiv:1710.07659v1 [q-bio.NC])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07659"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07659", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The mathematical model underlying the Neural Engineering Framework (NEF) \nexpresses neuronal input as a linear combination of synaptic currents. However, \nin biology, synapses are not perfect current sources and are thus nonlinear. \nDetailed synapse models are based on channel conductances instead of currents, \nwhich require independent handling of excitatory and inhibitory synapses. This, \nin particular, significantly affects the influence of inhibitory signals on the \nneuronal dynamics. In this technical report we first summarize the relevant \nportions of the NEF and conductance-based synapse models. We then discuss a \nna\\\"ive translation between populations of LIF neurons with current- and \nconductance-based synapses based on an estimation of an average membrane \npotential. Experiments show that this simple approach works relatively well for \nfeed-forward communication channels, yet performance degrades for NEF networks \ndescribing more complex dynamics, such as integration. \n</p>"}, "author": "Andreas St&#xf6;ckel, Aaron R. Voelker, Chris Eliasmith", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328717", "id": "tag:google.com,2005:reader/item/0000000324a850c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Superposed Episodic and Semantic Memory via Sparse Distributed Representation. (arXiv:1710.07829v1 [cs.NE])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07829"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07829", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The abilities to perceive, learn, and use generalities, similarities, \nclasses, i.e., semantic memory (SM), is central to cognition. Machine learning \n(ML), neural network, and AI research has been primarily driven by tasks \nrequiring such abilities. However, another central facet of cognition, \nsingle-trial formation of permanent memories of experiences, i.e., episodic \nmemory (EM), has had relatively little focus. Only recently has EM-like \nfunctionality been added to Deep Learning (DL) models, e.g., Neural Turing \nMachine, Memory Networks. However, in these cases: a) EM is implemented as a \nseparate module, which entails substantial data movement (and so, time and \npower) between the DL net itself and EM; and b) individual items are stored \nlocalistically within the EM, precluding realizing the exponential \nrepresentational efficiency of distributed over localist coding. We describe \nSparsey, an unsupervised, hierarchical, spatial/spatiotemporal associative \nmemory model differing fundamentally from mainstream ML models, most crucially, \nin its use of sparse distributed representations (SDRs), or, cell assemblies, \nwhich admits an extremely efficient, single-trial learning algorithm that maps \ninput similarity into code space similarity (measured as intersection). SDRs of \nindividual inputs are stored in superposition and because similarity is \npreserved, the patterns of intersections over the assigned codes reflect the \nsimilarity, i.e., statistical, structure, of all orders, not simply pairwise, \nover the inputs. Thus, SM, i.e., a generative model, is built as a \ncomputationally free side effect of the act of storing episodic memory traces \nof individual inputs, either spatial patterns or sequences. We report initial \nresults on MNIST and on the Weizmann video event recognition benchmarks. While \nwe have not yet attained SOTA class accuracy, learning takes only minutes on a \nsingle CPU. \n</p>"}, "author": "Rod Rinkus, Jasmin Leveille", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328716", "id": "tag:google.com,2005:reader/item/0000000324a850e3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Moderate Environmental Variation Promotes Adaptation in Artificial Evolution. (arXiv:1710.07913v1 [cs.NE])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07913"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07913", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we analyze the role of environmental variations in the \nevolution of artificial agents situated in an external environment and we \ndemonstrate how environmental variations promote the evolution of better \nagents. The beneficial effect is maximized at intermediate rates of variations, \ni.e. when the dynamics of the environment displays a sufficient level of \nstability and variability. The analysis of the obtained results indicate that \nthe adaptive advantage provided by environmental variations is due to the fact \nthat it increases the rate with which evolving agents change phylogenetically. \nThe performance of the adaptive agents and the rate with which agents change \nphylogenetically are maximized at moderate rate of variations of the \nenvironment which provide a good tradeoff between environmental variation and \nstability. \n</p>"}, "author": "Nicola Milano, J&#xf4;nata Tyska Carvalho, Stefano Nolfi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328715", "id": "tag:google.com,2005:reader/item/0000000324a850f8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Progressive Learning for Systematic Design of Large Neural Networks. (arXiv:1710.08177v1 [cs.NE])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08177"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08177", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop an algorithm for systematic design of a large artificial neural \nnetwork using a progression property. We find that some non-linear functions, \nsuch as the rectifier linear unit and its derivatives, hold the property. The \nsystematic design addresses the choice of network size and regularization of \nparameters. The number of nodes and layers in network increases in progression \nwith the objective of consistently reducing an appropriate cost. Each layer is \noptimized at a time, where appropriate parameters are learned using convex \noptimization. Regularization parameters for convex optimization do not need a \nsignificant manual effort for tuning. We also use random instances for some \nweight matrices, and that helps to reduce the number of parameters we learn. \nThe developed network is expected to show good generalization power due to \nappropriate regularization and use of random weights in the layers. This \nexpectation is verified by extensive experiments for classification and \nregression problems, using standard databases. \n</p>"}, "author": "Saikat Chatterjee, Alireza M. Javid, Mostafa Sadeghi, Partha P. Mitra, Mikael Skoglund", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328714", "id": "tag:google.com,2005:reader/item/0000000324a85105", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generic 3D Representation via Pose Estimation and Matching. (arXiv:1710.08247v1 [cs.CV])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08247"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08247", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Though a large body of computer vision research has investigated developing \ngeneric semantic representations, efforts towards developing a similar \nrepresentation for 3D has been limited. In this paper, we learn a generic 3D \nrepresentation through solving a set of foundational proxy 3D tasks: \nobject-centric camera pose estimation and wide baseline feature matching. Our \nmethod is based upon the premise that by providing supervision over a set of \ncarefully selected foundational tasks, generalization to novel tasks and \nabstraction capabilities can be achieved. We empirically show that the internal \nrepresentation of a multi-task ConvNet trained to solve the above core problems \ngeneralizes to novel 3D tasks (e.g., scene layout estimation, object pose \nestimation, surface normal estimation) without the need for fine-tuning and \nshows traits of abstraction abilities (e.g., cross-modality pose estimation). \nIn the context of the core supervised tasks, we demonstrate our representation \nachieves state-of-the-art wide baseline feature matching results without \nrequiring apriori rectification (unlike SIFT and the majority of learned \nfeatures). We also show 6DOF camera pose estimation given a pair local image \npatches. The accuracy of both supervised tasks come comparable to humans. \nFinally, we contribute a large-scale dataset composed of object-centric street \nview scenes along with point correspondences and camera pose information, and \nconclude with a discussion on the learned representation and open research \nquestions. \n</p>"}, "author": "Amir R. Zamir, Tilman Wekel, Pulkit Argrawal, Colin Weil, Jitendra Malik, Silvio Savarese", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328713", "id": "tag:google.com,2005:reader/item/0000000324a85124", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning applied to Road Traffic Speed forecasting. (arXiv:1710.08266v1 [stat.AP])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08266"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08266", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9688972\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9688972&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we propose deep learning architectures (FNN, CNN and LSTM) to \nforecast a regression model for time dependent data. These algorithm's are \ndesigned to handle Floating Car Data (FCD) historic speeds to predict road \ntraffic data. For this we aggregate the speeds into the network inputs in an \ninnovative way. We compare the RMSE thus obtained with the results of a simpler \nphysical model, and show that the latter achieves better RMSE accuracy. We also \npropose a new indicator, which evaluates the algorithms improvement when \ncompared to a benchmark prediction. We conclude by questioning the interest of \nusing deep learning methods for this specific regression task. \n</p>"}, "author": "Thomas Epelbaum (IPHT), Fabrice Gamboa (IMT), Jean-Michel Loubes (IMT), Jessica Martin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508807308329", "timestampUsec": "1508807308328712", "id": "tag:google.com,2005:reader/item/0000000324a85137", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adversarial Domain Adaptation for Identifying Phase Transitions. (arXiv:1710.08382v1 [cond-mat.stat-mech])", "published": 1508807308, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.08382"}], "alternate": [{"href": "http://arxiv.org/abs/1710.08382", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The identification of phases of matter is a challenging task, especially in \nquantum mechanics, where the complexity of the ground state appears to grow \nexponentially with the size of the system. We address this problem with \nstate-of-the-art deep learning techniques: adversarial domain adaptation. We \nderive the phase diagram of the whole parameter space starting from a fixed and \nknown subspace using unsupervised learning. The input data set contains both \nlabeled and unlabeled data instances. The first kind is a system that admits an \naccurate analytical or numerical solution, and one can recover its phase \ndiagram. The second type is the physical system with an unknown phase diagram. \nAdversarial domain adaptation uses both types of data to create invariant \nfeature extracting layers in a deep learning architecture. Once these layers \nare trained, we can attach an unsupervised learner to the network to find phase \ntransitions. We show the success of this technique by applying it on several \nparadigmatic models: the Ising model with different temperatures, the \nBose-Hubbard model, and the SSH model with disorder. The input is the ground \nstate without any manual feature engineering, and the dimension of the \nparameter space is unrestricted. The method finds unknown transitions \nsuccessfully and predicts transition points in close agreement with standard \nmethods. This study opens the door to the classification of physical systems \nwhere the phases boundaries are complex such as the many-body localization \nproblem or the Bose glass phase. \n</p>"}, "author": "Patrick Huembeli, Alexandre Dauphin, Peter Wittek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508720882022", "timestampUsec": "1508720882022227", "id": "tag:google.com,2005:reader/item/0000000323fd2419", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to Recognize Actions from Limited Training Examples Using a Recurrent Spiking Neural Model. (arXiv:1710.07354v1 [cs.NE])", "published": 1508720882, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07354"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07354", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A fundamental challenge in machine learning today is to build a model that \ncan learn from few examples. Here, we describe a reservoir based spiking neural \nmodel for learning to recognize actions with a limited number of labeled \nvideos. First, we propose a novel encoding, inspired by how microsaccades \ninfluence visual perception, to extract spike information from raw video data \nwhile preserving the temporal correlation across different frames. Using this \nencoding, we show that the reservoir generalizes its rich dynamical activity \ntoward signature action/movements enabling it to learn from few training \nexamples. We evaluate our approach on the UCF-101 dataset. Our experiments \ndemonstrate that our proposed reservoir achieves 81.3%/87% Top-1/Top-5 \naccuracy, respectively, on the 101-class data while requiring just 8 video \nexamples per class for training. Our results establish a new benchmark for \naction recognition from limited video examples for spiking neural models while \nyielding competetive accuracy with respect to state-of-the-art non-spiking \nneural models. \n</p>"}, "author": "Priyadarshini Panda, Narayan Srinivasa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508720882022", "timestampUsec": "1508720882022226", "id": "tag:google.com,2005:reader/item/0000000323fd241f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning compressed representations of blood samples time series with missing data. (arXiv:1710.07547v1 [cs.NE])", "published": 1508720882, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07547"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07547", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Clinical measurements collected over time are naturally represented as \nmultivariate time series (MTS), which often contain missing data. An \nautoencoder can learn low dimensional vectorial representations of MTS that \npreserve important data characteristics, but cannot deal explicitly with \nmissing data. In this work, we propose a new framework that combines an \nautoencoder with the Time series Cluster Kernel (TCK), a kernel that accounts \nfor missingness patterns in MTS. Via kernel alignment, we incorporate TCK in \nthe autoencoder to improve the learned representations in presence of missing \ndata. We consider a classification problem of MTS with missing values, \nrepresenting blood samples of patients with surgical site infection. With our \napproach, rather than with a standard autoencoder, we learn representations in \nlow dimensions that can be classified better. \n</p>"}, "author": "Filippo Maria Bianchi, Karl &#xd8;yvind Mikalsen, Robert Jenssen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508719250276", "timestampUsec": "1508719250275721", "id": "tag:google.com,2005:reader/item/0000000323fac489", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Go game formal revealing by Ising model. (arXiv:1710.07360v1 [cs.AI])", "published": 1508719250, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07360"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07360", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Go gaming is a struggle for territory control between rival, black and white, \nstones on a board. We model the Go dynamics in a game by means of the Ising \nmodel whose interaction coefficients reflect essential rules and tactics \nemployed in Go to build long-term strategies. At any step of the game, the \nenergy functional of the model provides the control degree (strength) of a \nplayer over the board. A close fit between predictions of the model with actual \ngames is obtained. \n</p>"}, "author": "Mat&#xed;as Alvarado, Arturo Yee, Carlos Villarreal", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508719250276", "timestampUsec": "1508719250275720", "id": "tag:google.com,2005:reader/item/0000000323fac48f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Spoken Language Biomarkers for Detecting Cognitive Impairment. (arXiv:1710.07551v1 [cs.AI])", "published": 1508719250, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07551"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07551", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this study we developed an automated system that evaluates speech and \nlanguage features from audio recordings of neuropsychological examinations of \n92 subjects in the Framingham Heart Study. A total of 265 features were used in \nan elastic-net regularized binomial logistic regression model to classify the \npresence of cognitive impairment, and to select the most predictive features. \nWe compared performance with a demographic model from 6,258 subjects in the \ngreater study cohort (0.79 AUC), and found that a system that incorporated both \naudio and text features performed the best (0.92 AUC), with a True Positive \nRate of 29% (at 0% False Positive Rate) and a good model fit (Hosmer-Lemeshow \ntest &gt; 0.05). We also found that decreasing pitch and jitter, shorter segments \nof speech, and responses phrased as questions were positively associated with \ncognitive impairment. \n</p>"}, "author": "Tuka Alhanai, Rhoda Au, James Glass", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508719250276", "timestampUsec": "1508719250275719", "id": "tag:google.com,2005:reader/item/0000000323fac494", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Classification Driven Dynamic Image Enhancement. (arXiv:1710.07558v2 [cs.CV] UPDATED)", "published": 1511265636, "updated": 1511265658, "canonical": [{"href": "http://arxiv.org/abs/1710.07558"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07558", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolutional neural networks rely on image texture and structure to serve as \ndiscriminative features to classify the image content. Image enhancement \ntechniques can be used as preprocessing steps to help improve the overall image \nquality and in turn improve the overall effectiveness of a CNN. Existing image \nenhancement methods, however, are designed to improve the perceptual quality of \nan image for a human observer. In this paper, we are interested in learning \nCNNs that can emulate image enhancement and restoration, but with the overall \ngoal to improve image classification and not necessarily human perception. To \nthis end, we present a unified CNN architecture that uses a range of \nenhancement filters that can enhance image-specific details via end-to-end \ndynamic filter learning. We demonstrate the effectiveness of this strategy on \nfour challenging benchmark datasets for fine-grained, object, scene and texture \nclassification: CUB-200-2011, PASCAL-VOC2007, MIT-Indoor, and DTD. Experiments \nusing our proposed enhancement shows promising results on all the datasets. In \naddition, our approach is capable of improving the performance of all generic \nCNN architectures. \n</p>"}, "author": "Vivek Sharma, Ali Diba, Davy Neven, Michael S. Brown, Luc Van Gool, Rainer Stiefelhagen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288529", "id": "tag:google.com,2005:reader/item/0000000323fa7133", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Machine Learning as Statistical Data Assimilation. (arXiv:1710.07276v1 [cs.LG])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07276"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07276", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We identify a strong equivalence between neural network based machine \nlearning (ML) methods and the formulation of statistical data assimilation \n(DA), known to be a problem in statistical physics. DA, as used widely in \nphysical and biological sciences, systematically transfers information in \nobservations to a model of the processes producing the observations. The \ncorrespondence is that layer label in the ML setting is the analog of time in \nthe data assimilation setting. Utilizing aspects of this equivalence we discuss \nhow to establish the global minimum of the cost functions in the ML context, \nusing a variational annealing method from DA. This provides a design method for \noptimal networks for ML applications and may serve as the basis for \nunderstanding the success of \"deep learning\". Results from an ML example are \npresented. \n</p> \n<p>When the layer label is taken to be continuous, the Euler-Lagrange equation \nfor the ML optimization problem is an ordinary differential equation, and we \nsee that the problem being solved is a two point boundary value problem. The \nuse of continuous layers is denoted \"deepest learning\". The Hamiltonian version \nprovides a direct rationale for back propagation as a solution method for the \ncanonical momentum; however, it suggests other solution methods are to be \npreferred. \n</p>"}, "author": "H. D. I. Abarbanel, P. J. Rozdeba, S. Shirman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288528", "id": "tag:google.com,2005:reader/item/0000000323fa713f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Decomposition of Uncertainty for Active Learning and Reliable Reinforcement Learning in Stochastic Systems. (arXiv:1710.07283v2 [stat.ML] UPDATED)", "published": 1510769311, "updated": 1510769327, "canonical": [{"href": "http://arxiv.org/abs/1710.07283"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07283", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Bayesian neural networks (BNNs) with latent variables are probabilistic \nmodels which can automatically identify complex stochastic patterns in the \ndata. We study in these models a decomposition of predictive uncertainty into \nits epistemic and aleatoric components. We show how such a decomposition arises \nnaturally in a Bayesian active learning scenario and develop a new objective \nfor reliable reinforcement learning (RL) with an epistemic and aleatoric risk \nelement. Our experiments illustrate the usefulness of the resulting \ndecomposition in active learning and reliable RL. \n</p>"}, "author": "Stefan Depeweg, Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato, Finale Doshi-Velez, Steffen Udluft", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288527", "id": "tag:google.com,2005:reader/item/0000000323fa7144", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Power Plant Performance Modeling with Concept Drift. (arXiv:1710.07314v1 [cs.LG])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07314"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07314", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Power plant is a complex and nonstationary system for which the traditional \nmachine learning modeling approaches fall short of expectations. The \nensemble-based online learning methods provide an effective way to continuously \nlearn from the dynamic environment and autonomously update models to respond to \nenvironmental changes. This paper proposes such an online ensemble regression \napproach to model power plant performance, which is critically important for \noperation optimization. The experimental results on both simulated and real \ndata show that the proposed method can achieve performance with less than 1% \nmean average percentage error, which meets the general expectations in field \noperations. \n</p>"}, "author": "Rui Xu, Yunwen Xu, Weizhong Yan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288526", "id": "tag:google.com,2005:reader/item/0000000323fa714f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Visual Integration of Data and Model Space in Ensemble Learning. (arXiv:1710.07322v1 [cs.HC])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07322"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07322", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9688b83\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9688b83&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Ensembles of classifier models typically deliver superior performance and can \noutperform single classifier models given a dataset and classification task at \nhand. However, the gain in performance comes together with the lack in \ncomprehensibility, posing a challenge to understand how each model affects the \nclassification outputs and where the errors come from. We propose a tight \nvisual integration of the data and the model space for exploring and combining \nclassifier models. We introduce a workflow that builds upon the visual \nintegration and enables the effective exploration of classification outputs and \nmodels. We then present a use case in which we start with an ensemble \nautomatically selected by a standard ensemble selection algorithm, and show how \nwe can manipulate models and alternative combinations. \n</p>"}, "author": "Bruno Schneider, Dominik J&#xe4;ckle, Florian Stoffel, Alexandra Diehl, Johannes Fuchs, Daniel Keim", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288525", "id": "tag:google.com,2005:reader/item/0000000323fa7156", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition. (arXiv:1710.07324v1 [cs.LG])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07324"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07324", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c982b4ac\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c982b4ac&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a method (TT-GP) for approximate inference in Gaussian Process \n(GP) models. We build on previous scalable GP research including stochastic \nvariational inference based on inducing inputs, kernel interpolation, and \nstructure exploiting algebra. The key idea of our method is to use Tensor Train \ndecomposition for variational parameters, which allows us to train GPs with \nbillions of inducing inputs and achieve state-of-the-art results on several \nbenchmarks. Further, our approach allows for training kernels based on deep \nneural networks without any modifications to the underlying GP model. A neural \nnetwork learns a multidimensional embedding for the data, which is used by the \nGP to make the final prediction. We train GP and neural network parameters \nend-to-end without pretraining, through maximization of GP marginal likelihood. \nWe show the efficiency of the proposed approach on several regression and \nclassification benchmark datasets including MNIST, CIFAR-10, and Airline. \n</p>"}, "author": "Pavel Izmailov, Alexander Novikov, Dmitry Kropotov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288524", "id": "tag:google.com,2005:reader/item/0000000323fa7167", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Frequency Based Index Estimating the Subclusters' Connection Strength. (arXiv:1710.07340v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07340"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07340", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, a frequency coefficient based on the Sen-Shorrocks-Thon (SST) \npoverty index notion is proposed. The clustering SST index can be used as the \nmethod for determination of the connection between similar neighbor \nsub-clusters. Consequently, connections can reveal existence of natural \nhomogeneous. Through estimation of the connection strength, we can also verify \ninformation about the estimated number of natural clusters that is necessary \nassumption of efficient market segmentation and campaign management and \nfinancial decisions. The index can be used as the complementary tool for the \nU-matrix visualization. The index is tested on an artificial dataset with known \nparameters and compared with results obtained by the Unified-distance matrix \nmethod. \n</p>"}, "author": "Lukas Pastorek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288523", "id": "tag:google.com,2005:reader/item/0000000323fa716c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Linear-Time Algorithm in Bayesian Image Denoising based on Gaussian Markov Random Field. (arXiv:1710.07393v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07393"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07393", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we consider Bayesian image denoising based on a Gaussian \nMarkov random field (GMRF) model, for which we propose an new algorithm. Our \nmethod can solve Bayesian image denoising problems, including hyperparameter \nestimation, in $O(n)$-time, where $n$ is the number of pixels in a given image. \nFrom the perspective of the order of the computational time, this is a \nstate-of-the-art algorithm for the present problem setting. Moreover, the \nresults of our numerical experiments we show our method is in fact effective in \npractice. \n</p>"}, "author": "Muneki Yasuda, Junpei Watanabe, Shun Kataoka, kazuyuki Tanaka", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288522", "id": "tag:google.com,2005:reader/item/0000000323fa7170", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Ligand Pose Optimization with Atomic Grid-Based Convolutional Neural Networks. (arXiv:1710.07400v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07400"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07400", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Docking is an important tool in computational drug discovery that aims to \npredict the binding pose of a ligand to a target protein through a combination \nof pose scoring and optimization. A scoring function that is differentiable \nwith respect to atom positions can be used for both scoring and gradient-based \noptimization of poses for docking. Using a differentiable grid-based atomic \nrepresentation as input, we demonstrate that a scoring function learned by \ntraining a convolutional neural network (CNN) to identify binding poses can \nalso be applied to pose optimization. We also show that an iteratively-trained \nCNN that includes poses optimized by the first CNN in its training set performs \neven better at optimizing randomly initialized poses than either the first CNN \nscoring function or AutoDock Vina. \n</p>"}, "author": "Matthew Ragoza, Lillian Turner, David Ryan Koes", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288521", "id": "tag:google.com,2005:reader/item/0000000323fa7173", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "First-order Methods Almost Always Avoid Saddle Points. (arXiv:1710.07406v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07406"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07406", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We establish that first-order methods avoid saddle points for almost all \ninitializations. Our results apply to a wide variety of first-order methods, \nincluding gradient descent, block coordinate descent, mirror descent and \nvariants thereof. The connecting thread is that such algorithms can be studied \nfrom a dynamical systems perspective in which appropriate instantiations of the \nStable Manifold Theorem allow for a global stability analysis. Thus, neither \naccess to second-order derivative information nor randomness beyond \ninitialization is necessary to provably avoid saddle points. \n</p>"}, "author": "Jason D. Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I. Jordan, Benjamin Recht", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288520", "id": "tag:google.com,2005:reader/item/0000000323fa7177", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Differentially Private Empirical Risk Minimization with Input Perturbation. (arXiv:1710.07425v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07425"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07425", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a novel framework for the differentially private ERM, input \nperturbation. Existing differentially private ERM implicitly assumed that the \ndata contributors submit their private data to a database expecting that the \ndatabase invokes a differentially private mechanism for publication of the \nlearned model. In input perturbation, each data contributor independently \nrandomizes her/his data by itself and submits the perturbed data to the \ndatabase. We show that the input perturbation framework theoretically \nguarantees that the model learned with the randomized data eventually satisfies \ndifferential privacy with the prescribed privacy parameters. At the same time, \ninput perturbation guarantees that local differential privacy is guaranteed to \nthe server. We also show that the excess risk bound of the model learned with \ninput perturbation is $O(1/n)$ under a certain condition, where $n$ is the \nsample size. This is the same as the excess risk bound of the state-of-the-art. \n</p>"}, "author": "Kazuto Fukuchi, Quang Khai Tran, Jun Sakuma", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288519", "id": "tag:google.com,2005:reader/item/0000000323fa717a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distributed Deep Transfer Learning by Basic Probability Assignment. (arXiv:1710.07437v1 [cs.LG])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07437"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07437", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Transfer learning is a popular practice in deep neural networks, but \nfine-tuning of large number of parameters is a hard task due to the complex \nwiring of neurons between splitting layers and imbalance distributions of data \nin pretrained and transferred domains. The reconstruction of the original \nwiring for the target domain is a heavy burden due to the size of \ninterconnections across neurons. We propose a distributed scheme that tunes the \nconvolutional filters individually while backpropagates them jointly by means \nof basic probability assignment. Some of the most recent advances in evidence \ntheory show that in a vast variety of the imbalanced regimes, optimizing of \nsome proper objective functions derived from contingency matrices prevents \nbiases towards high-prior class distributions. Therefore, the original filters \nget gradually transferred based on individual contributions to overall \nperformance of the target domain. This largely reduces the expected complexity \nof transfer learning whilst highly improves precision. Our experiments on \nstandard benchmarks and scenarios confirm the consistent improvement of our \ndistributed deep transfer learning strategy. \n</p>"}, "author": "Arash Shahriari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288518", "id": "tag:google.com,2005:reader/item/0000000323fa717b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Unified Backpropagation for Multi-Objective Deep Learning. (arXiv:1710.07438v1 [cs.LG])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07438"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07438", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A common practice in most of deep convolutional neural architectures is to \nemploy fully-connected layers followed by Softmax activation to minimize \ncross-entropy loss for the sake of classification. Recent studies show that \nsubstitution or addition of the Softmax objective to the cost functions of \nsupport vector machines or linear discriminant analysis is highly beneficial to \nimprove the classification performance in hybrid neural networks. We propose a \nnovel paradigm to link the optimization of several hybrid objectives through \nunified backpropagation. This highly alleviates the burden of extensive \nboosting for independent objective functions or complex formulation of \nmultiobjective gradients. Hybrid loss functions are linked by basic probability \nassignment from evidence theory. We conduct our experiments for a variety of \nscenarios and standard datasets to evaluate the advantage of our proposed \nunification approach to deliver consistent improvements into the classification \nperformance of deep convolutional neural networks. \n</p>"}, "author": "Arash Shahriari", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288517", "id": "tag:google.com,2005:reader/item/0000000323fa717e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Finite-dimensional Gaussian approximation with linear inequality constraints. (arXiv:1710.07453v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07453"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07453", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Introducing inequality constraints in Gaussian process (GP) models can lead \nto more realistic uncertainties in learning a great variety of real-world \nproblems. We consider the finite-dimensional Gaussian approach from Maatouk and \nBay (2017) which can satisfy inequality conditions everywhere (either \nboundedness, monotonicity or convexity). Our contributions are threefold. \nFirst, we extend their approach in order to deal with general sets of linear \ninequalities. Second, we explore several Markov Chain Monte Carlo (MCMC) \ntechniques to approximate the posterior distribution. Third, we investigate \ntheoretical and numerical properties of the constrained likelihood for \ncovariance parameter estimation. According to experiments on both artificial \nand real data, our full framework together with a Hamiltonian Monte Carlo-based \nsampler provides efficient results on both data fitting and uncertainty \nquantification. \n</p>"}, "author": "Andr&#xe9;s F. L&#xf3;pez-Lopera, Fran&#xe7;ois Bachoc, Nicolas Durrande, Olivier Roustant", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288516", "id": "tag:google.com,2005:reader/item/0000000323fa7183", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning Wasserstein Embeddings. (arXiv:1710.07457v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07457"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07457", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c982b807\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c982b807&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The Wasserstein distance received a lot of attention recently in the \ncommunity of machine learning, especially for its principled way of comparing \ndistributions. It has found numerous applications in several hard problems, \nsuch as domain adaptation, dimensionality reduction or generative models. \nHowever, its use is still limited by a heavy computational cost. Our goal is to \nalleviate this problem by providing an approximation mechanism that allows to \nbreak its inherent complexity. It relies on the search of an embedding where \nthe Euclidean distance mimics the Wasserstein distance. We show that such an \nembedding can be found with a siamese architecture associated with a decoder \nnetwork that allows to move from the embedding space back to the original input \nspace. Once this embedding has been found, computing optimization problems in \nthe Wasserstein space (e.g. barycenters, principal directions or even \narchetypes) can be conducted extremely fast. Numerical experiments supporting \nthis idea are conducted on image datasets, and show the wide potential benefits \nof our method. \n</p>"}, "author": "Nicolas Courty, R&#xe9;mi Flamary, M&#xe9;lanie Ducoffe", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288515", "id": "tag:google.com,2005:reader/item/0000000323fa718b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Tracking the gradients using the Hessian: A new look at variance reducing stochastic methods. (arXiv:1710.07462v1 [math.OC])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07462"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07462", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Our goal is to improve variance reducing stochastic methods through better \ncontrol variates. We first propose a modification of SVRG which uses the \nHessian to track gradients over time, rather than to recondition, increasing \nthe correlation of the control variates and leading to faster theoretical \nconvergence close to the optimum. We then propose accurate and computationally \nefficient approximations to the Hessian, both using a diagonal and a low-rank \nmatrix. Finally, we demonstrate the effectiveness of our method on a wide range \nof problems. \n</p>"}, "author": "Robert M. Gower, Nicolas Le Roux, Francis Bach", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288514", "id": "tag:google.com,2005:reader/item/0000000323fa7190", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Dynamic classifier chains for multi-label learning. (arXiv:1710.07491v1 [cs.LG])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07491"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07491", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we deal with the task of building a dynamic ensemble of chain \nclassifiers for multi-label classification. To do so, we proposed two concepts \nof classifier chains algorithms that are able to change label order of the \nchain without rebuilding the entire model. Such modes allows anticipating the \ninstance-specific chain order without a significant increase in computational \nburden. The proposed chain models are built using the Naive Bayes classifier \nand nearest neighbour approach as a base single-label classifiers. To take the \nbenefits of the proposed algorithms, we developed a simple heuristic that \nallows the system to find relatively good label order. The heuristic sort \nlabels according to the label-specific classification quality gained during the \nvalidation phase. The heuristic tries to minimise the phenomenon of error \npropagation in the chain. The experimental results showed that the proposed \nmodel based on Naive Bayes classifier the above-mentioned heuristic is an \nefficient tool for building dynamic chain classifiers. \n</p>"}, "author": "Pawel Trajdos, Marek Kurzynski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288513", "id": "tag:google.com,2005:reader/item/0000000323fa7197", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning compressed representations of blood samples time series with missing data. (arXiv:1710.07547v1 [cs.NE])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07547"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07547", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Clinical measurements collected over time are naturally represented as \nmultivariate time series (MTS), which often contain missing data. An \nautoencoder can learn low dimensional vectorial representations of MTS that \npreserve important data characteristics, but cannot deal explicitly with \nmissing data. In this work, we propose a new framework that combines an \nautoencoder with the Time series Cluster Kernel (TCK), a kernel that accounts \nfor missingness patterns in MTS. Via kernel alignment, we incorporate TCK in \nthe autoencoder to improve the learned representations in presence of missing \ndata. We consider a classification problem of MTS with missing values, \nrepresenting blood samples of patients with surgical site infection. With our \napproach, rather than with a standard autoencoder, we learn representations in \nlow dimensions that can be classified better. \n</p>"}, "author": "Filippo Maria Bianchi, Karl &#xd8;yvind Mikalsen, Robert Jenssen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508718990289", "timestampUsec": "1508718990288512", "id": "tag:google.com,2005:reader/item/0000000323fa719a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Belief Propagation Min-Sum Algorithm for Generalized Min-Cost Network Flow. (arXiv:1710.07600v1 [stat.ML])", "published": 1508718990, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07600"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07600", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Belief Propagation algorithms are instruments used broadly to solve graphical \nmodel optimization and statistical inference problems. In the general case of a \nloopy Graphical Model, Belief Propagation is a heuristic which is quite \nsuccessful in practice, even though its empirical success, typically, lacks \ntheoretical guarantees. This paper extends the short list of special cases \nwhere correctness and/or convergence of a Belief Propagation algorithm is \nproven. We generalize formulation of Min-Sum Network Flow problem by relaxing \nthe flow conservation (balance) constraints and then proving that the Belief \nPropagation algorithm converges to the exact result. \n</p>"}, "author": "Andrii Riazanov, Yury Maximov, Michael Chertkov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508461685574", "timestampUsec": "1508461685573759", "id": "tag:google.com,2005:reader/item/00000003224e8cc3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Protein Folding Optimization using Differential Evolution Extended with Local Search and Component Reinitialization. (arXiv:1710.07031v1 [cs.AI])", "published": 1508461686, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07031"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07031", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper presents a novel differential evolution algorithm for protein \nfolding optimization that is applied to a three-dimensional AB off-lattice \nmodel. The proposed algorithm includes two new mechanisms. A local search is \nused to improve convergence speed and to reduce the runtime complexity of the \nenergy calculation. For this purpose, a local movement is introduced within the \nlocal search. The designed evolutionary algorithm has fast convergence and, \ntherefore, when it is trapped into local optimum or a relatively good solution \nis located, it is hard to locate a better similar solution. The similar \nsolution is different from the good solution in only a few components. A \ncomponent reinitialization method is designed to mitigate this problem. Both \nthe new mechanisms and the proposed algorithm were analyzed on well-known \namino-acid sequences that are used frequently in the literature. Experimental \nresults show that the employed new mechanisms improve the efficiency of our \nalgorithm and the proposed algorithm is superior to other state-of-the-art \nalgorithms. It obtained a hit ratio of 100 % for sequences up to 18 monomers \nwithin a budget of $10^{11}$ solution evaluations. New best-known solutions \nwere obtained for most of the sequences. The existence of the symmetric \nbest-known solutions is also demonstrated in the paper. \n</p>"}, "author": "Borko Bo&#x161;kovi&#x107;, Janez Brest", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361826", "id": "tag:google.com,2005:reader/item/00000003224b3326", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Graph Embedding with Rich Information through Bipartite Heterogeneous Network. (arXiv:1710.06879v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06879"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06879", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Graph embedding has attracted increasing attention due to its critical \napplication in social network analysis. Most existing algorithms for graph \nembedding only rely on the typology information and fail to use the copious \ninformation in nodes as well as edges. As a result, their performance for many \ntasks may not be satisfactory. In this paper, we proposed a novel and general \nframework of representation learning for graph with rich text information \nthrough constructing a bipartite heterogeneous network. Specially, we designed \na biased random walk to explore the constructed heterogeneous network with the \nnotion of flexible neighborhood. The efficacy of our method is demonstrated by \nextensive comparison experiments with several baselines on various datasets. It \nimproves the Micro-F1 and Macro-F1 of node classification by 10% and 7% on Cora \ndataset. \n</p>"}, "author": "Guolei Sun, Xiangliang Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361825", "id": "tag:google.com,2005:reader/item/00000003224b3328", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Emergent Translation in Multi-Agent Communication. (arXiv:1710.06922v1 [cs.CL])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06922"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06922", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>While most machine translation systems to date are trained on large parallel \ncorpora, humans learn language in a different way: by being grounded in an \nenvironment and interacting with other humans. In this work, we propose a \ncommunication game where two agents, native speakers of their own respective \nlanguages, jointly learn to solve a visual referential task. We find that the \nability to understand and translate a foreign language emerges as a means to \nachieve shared goals. The emergent translation is interactive and multimodal, \nand crucially does not require parallel corpora, but only monolingual, \nindependent text and corresponding images. Our proposed translation model \nachieves this by grounding the source and target languages into a shared visual \nmodality, and outperforms several baselines on both word-level and \nsentence-level translation tasks. Furthermore, we show that agents in a \nmultilingual community learn to translate better and faster than in a bilingual \ncommunication setting. \n</p>"}, "author": "Jason Lee, Kyunghyun Cho, Jason Weston, Douwe Kiela", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361824", "id": "tag:google.com,2005:reader/item/00000003224b332b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Adapting general-purpose speech recognition engine output for domain-specific natural language question answering. (arXiv:1710.06923v1 [cs.CL])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06923"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06923", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Speech-based natural language question-answering interfaces to enterprise \nsystems are gaining a lot of attention. General-purpose speech engines can be \nintegrated with NLP systems to provide such interfaces. Usually, \ngeneral-purpose speech engines are trained on large `general' corpus. However, \nwhen such engines are used for specific domains, they may not recognize \ndomain-specific words well, and may produce erroneous output. Further, the \naccent and the environmental conditions in which the speaker speaks a sentence \nmay induce the speech engine to inaccurately recognize certain words. The \nsubsequent natural language question-answering does not produce the requisite \nresults as the question does not accurately represent what the speaker \nintended. Thus, the speech engine's output may need to be adapted for a domain \nbefore further natural language processing is carried out. We present two \nmechanisms for such an adaptation, one based on evolutionary development and \nthe other based on machine learning, and show how we can repair the \nspeech-output to make the subsequent natural language question-answering \nbetter. \n</p>"}, "author": "C. Anantaram, Sunil Kumar Kopparapu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361823", "id": "tag:google.com,2005:reader/item/00000003224b332f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Consequentialist conditional cooperation in social dilemmas with imperfect information. (arXiv:1710.06975v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06975"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06975", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Social dilemmas, where mutual cooperation can lead to high payoffs but \nparticipants face incentives to cheat, are ubiquitous in multi-agent \ninteraction. We wish to construct agents that cooperate with pure cooperators, \navoid exploitation by pure defectors, and incentivize cooperation from the \nrest. However, often the actions taken by a partner are (partially) unobserved \nor the consequences of individual actions are hard to predict. We show that in \na large class of games good strategies can be constructed by conditioning one's \nbehavior solely on outcomes (ie. one's past rewards). We call this \nconsequentialist conditional cooperation. We show how to construct such \nstrategies using deep reinforcement learning techniques and demonstrate, both \nanalytically and experimentally, that they are effective in social dilemmas \nbeyond simple matrix games. We also show the limitations of relying purely on \nconsequences and discuss the need for understanding both the consequences of \nand the intentions behind an action. \n</p>"}, "author": "Alexander Peysakhovich, Adam Lerer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361822", "id": "tag:google.com,2005:reader/item/00000003224b3333", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Protein Folding Optimization using Differential Evolution Extended with Local Search and Component Reinitialization. (arXiv:1710.07031v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07031"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07031", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c982bb0c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c982bb0c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper presents a novel differential evolution algorithm for protein \nfolding optimization that is applied to a three-dimensional AB off-lattice \nmodel. The proposed algorithm includes two new mechanisms. A local search is \nused to improve convergence speed and to reduce the runtime complexity of the \nenergy calculation. For this purpose, a local movement is introduced within the \nlocal search. The designed evolutionary algorithm has fast convergence and, \ntherefore, when it is trapped into local optimum or a relatively good solution \nis located, it is hard to locate a better similar solution. The similar \nsolution is different from the good solution in only a few components. A \ncomponent reinitialization method is designed to mitigate this problem. Both \nthe new mechanisms and the proposed algorithm were analyzed on well-known \namino-acid sequences that are used frequently in the literature. Experimental \nresults show that the employed new mechanisms improve the efficiency of our \nalgorithm and the proposed algorithm is superior to other state-of-the-art \nalgorithms. It obtained a hit ratio of 100 % for sequences up to 18 monomers \nwithin a budget of $10^{11}$ solution evaluations. New best-known solutions \nwere obtained for most of the sequences. The existence of the symmetric \nbest-known solutions is also demonstrated in the paper. \n</p>"}, "author": "Borko Bo&#x161;kovi&#x107;, Janez Brest", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361821", "id": "tag:google.com,2005:reader/item/00000003224b3336", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Decision Trees for Helpdesk Advisor Graphs. (arXiv:1710.07075v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07075"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07075", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c98c6625\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c98c6625&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We use decision trees to build a helpdesk agent reference network to \nfacilitate the on-the-job advising of junior or less experienced staff on how \nto better address telecommunication customer fault reports. Such reports \ngenerate field measurements and remote measurements which, when coupled with \nlocation data and client attributes, and fused with organization-level \nstatistics, can produce models of how support should be provided. Beyond \ndecision support, these models can help identify staff who can act as advisors, \nbased on the quality, consistency and predictability of dealing with complex \ntroubleshooting reports. Advisor staff models are then used to guide less \nexperienced staff in their decision making; thus, we advocate the deployment of \na simple mechanism which exploits the availability of staff with a sound track \nrecord at the helpdesk to act as dormant tutors. \n</p>"}, "author": "Spyros Gkezerlis, Dimitris Kalles", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361820", "id": "tag:google.com,2005:reader/item/00000003224b333b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Swift Linked Data Miner: Mining OWL 2 EL class expressions directly from online RDF datasets. (arXiv:1710.07114v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07114"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07114", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this study, we present Swift Linked Data Miner, an interruptible algorithm \nthat can directly mine an online Linked Data source (e.g., a SPARQL endpoint) \nfor OWL 2 EL class expressions to extend an ontology with new SubClassOf: \naxioms. The algorithm works by downloading only a small part of the Linked Data \nsource at a time, building a smart index in the memory and swiftly iterating \nover the index to mine axioms. We propose a transformation function from mined \naxioms to RDF Data Shapes. We show, by means of a crowdsourcing experiment, \nthat most of the axioms mined by Swift Linked Data Miner are correct and can be \nadded to an ontology. We provide a ready to use Prot\\'eg\\'e plugin implementing \nthe algorithm, to support ontology engineers in their daily modeling work. \n</p>"}, "author": "Jedrzej Potoniec, Piotr Jakubowski, Agnieszka &#x141;awrynowicz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361819", "id": "tag:google.com,2005:reader/item/00000003224b3340", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Two-Phase Safe Vehicle Routing and Scheduling Problem: Formulations and Solution Algorithms. (arXiv:1710.07147v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07147"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07147", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a two phase time dependent vehicle routing and scheduling \noptimization model that identifies the safest routes, as a substitute for the \nclassical objectives given in the literature such as shortest distance or \ntravel time, through (1) avoiding recurring congestions, and (2) selecting \nroutes that have a lower probability of crash occurrences and non-recurring \ncongestion caused by those crashes. In the first phase, we solve a \nmixed-integer programming model which takes the dynamic speed variations into \naccount on a graph of roadway networks according to the time of day, and \nidentify the routing of a fleet and sequence of nodes on the safest feasible \npaths. Second phase considers each route as an independent transit path (fixed \nroute with fixed node sequences), and tries to avoid congestion by rescheduling \nthe departure times of each vehicle from each node, and by adjusting the \nsub-optimal speed on each arc. A modified simulated annealing (SA) algorithm is \nformulated to solve both complex models iteratively, which is found to be \ncapable of providing solutions in a considerably short amount of time. \n</p>"}, "author": "Aschkan Omidvar, Eren Erman Ozguven, O. Arda Vanli, R. Tavakkoli-Moghaddam", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361818", "id": "tag:google.com,2005:reader/item/00000003224b3342", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On Using Linear Diophantine Equations to Tune the extent of Look Ahead while Hiding Decision Tree Rules. (arXiv:1710.07214v1 [cs.AI])", "published": 1508459581, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07214"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07214", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper focuses on preserving the privacy of sensitive pat-terns when \ninducing decision trees. We adopt a record aug-mentation approach for hiding \nsensitive classification rules in binary datasets. Such a hiding methodology is \npreferred over other heuristic solutions like output perturbation or \ncrypto-graphic techniques - which restrict the usability of the data - since \nthe raw data itself is readily available for public use. In this paper, we \npropose a look ahead approach using linear Diophantine equations in order to \nadd the appropriate number of instances while minimally disturbing the initial \nentropy of the nodes. \n</p>"}, "author": "Georgios Feretzakis, Dimitris Kalles, Vassilios S. Verykios", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459580362", "timestampUsec": "1508459580361815", "id": "tag:google.com,2005:reader/item/00000003224b3347", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Detecting Bias in Black-Box Models Using Transparent Model Distillation. (arXiv:1710.06169v2 [stat.ML] UPDATED)", "published": 1511265636, "updated": 1511265658, "canonical": [{"href": "http://arxiv.org/abs/1710.06169"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06169", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Black-box risk scoring models permeate our lives, yet are typically \nproprietary and opaque. We propose a transparent model distillation approach to \ndetect bias in such models. Model distillation was originally designed to \ndistill knowledge from a large, complex teacher model to a faster, simpler \nstudent model without significant loss in prediction accuracy. We add a third \nrestriction - transparency. In this paper we use data sets that contain two \nlabels to train on: the risk score predicted by a black-box model, as well as \nthe actual outcome the risk score was intended to predict. This allows us to \ncompare models that predict each label. For a particular class of student \nmodels - interpretable tree additive models with pairwise interactions (GA2Ms) \n- we provide confidence intervals for the difference between the risk score and \nactual outcome models. This presents a new method for detecting bias in \nblack-box risk scores by assessing if contributions of protected features to \nthe risk score are statistically different from their contributions to the \nactual outcome. \n</p>"}, "author": "Sarah Tan, Rich Caruana, Giles Hooker, Yin Lou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393688", "id": "tag:google.com,2005:reader/item/00000003224b08ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Bayesian Nonparametric Method for Clustering Imputation, and Forecasting in Multivariate Time Series. (arXiv:1710.06900v1 [stat.ME])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06900"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06900", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This article proposes a Bayesian nonparametric method for forecasting, \nimputation, and clustering in sparsely observed, multivariate time series. The \nmethod is appropriate for jointly modeling hundreds of time series with widely \nvarying, non-stationary dynamics. Given a collection of $N$ time series, the \nBayesian model first partitions them into independent clusters using a Chinese \nrestaurant process prior. Within a cluster, all time series are modeled jointly \nusing a novel \"temporally-coupled\" extension of the Chinese restaurant process \nmixture. Markov chain Monte Carlo techniques are used to obtain samples from \nthe posterior distribution, which are then used to form predictive inferences. \nWe apply the technique to challenging prediction and imputation tasks using \nseasonal flu data from the US Center for Disease Control and Prevention, \ndemonstrating competitive imputation performance and improved forecasting \naccuracy as compared to several state-of-the art baselines. We also show that \nthe model discovers interpretable clusters in datasets with hundreds of time \nseries using macroeconomic data from the Gapminder Foundation. \n</p>"}, "author": "Feras A. Saad, Vikash K. Mansinghka", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393687", "id": "tag:google.com,2005:reader/item/00000003224b08b5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Characterization of Gradient Dominance and Regularity Conditions for Neural Networks. (arXiv:1710.06910v1 [stat.ML])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06910"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06910", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The past decade has witnessed a successful application of deep learning to \nsolving many challenging problems in machine learning and artificial \nintelligence. However, the loss functions of deep neural networks (especially \nnonlinear networks) are still far from being well understood from a theoretical \naspect. In this paper, we enrich the current understanding of the landscape of \nthe square loss functions for three types of neural networks. Specifically, \nwhen the parameter matrices are square, we provide an explicit characterization \nof the global minimizers for linear networks, linear residual networks, and \nnonlinear networks with one hidden layer. Then, we establish two quadratic \ntypes of landscape properties for the square loss of these neural networks, \ni.e., the gradient dominance condition within the neighborhood of their full \nrank global minimizers, and the regularity condition along certain directions \nand within the neighborhood of their global minimizers. These two landscape \nproperties are desirable for the optimization around the global minimizers of \nthe loss function for these neural networks. \n</p>"}, "author": "Yi Zhou, Yingbin Liang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393686", "id": "tag:google.com,2005:reader/item/00000003224b08b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Concept Drift Learning with Alternating Learners. (arXiv:1710.06940v1 [cs.LG])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06940"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06940", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Data-driven predictive analytics are in use today across a number of \nindustrial applications, but further integration is hindered by the requirement \nof similarity among model training and test data distributions. This paper \naddresses the need of learning from possibly nonstationary data streams, or \nunder concept drift, a commonly seen phenomenon in practical applications. A \nsimple dual-learner ensemble strategy, alternating learners framework, is \nproposed. A long-memory model learns stable concepts from a long relevant time \nwindow, while a short-memory model learns transient concepts from a small \nrecent window. The difference in prediction performance of these two models is \nmonitored and induces an alternating policy to select, update and reset the two \nmodels. The method features an online updating mechanism to maintain the \nensemble accuracy, and a concept-dependent trigger to focus on relevant data. \nThrough empirical studies the method demonstrates effective tracking and \nprediction when the steaming data carry abrupt and/or gradual changes. \n</p>"}, "author": "Yunwen Xu, Rui Xu, Weizhong Yan, Paul Ardis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393685", "id": "tag:google.com,2005:reader/item/00000003224b08bd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Asynchronous Decentralized Parallel Stochastic Gradient Descent. (arXiv:1710.06952v1 [math.OC])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06952"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06952", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent work shows that decentralized parallel stochastic gradient decent \n(D-PSGD) can outperform its centralized counterpart both theoretically and \npractically. While asynchronous parallelism is a powerful technology to improve \nthe efficiency of parallelism in distributed machine learning platforms and has \nbeen widely used in many popular machine learning softwares and solvers based \non centralized parallel protocols such as Tensorflow, it still remains unclear \nhow to apply the asynchronous parallelism to improve the efficiency of \ndecentralized parallel algorithms. This paper proposes an asynchronous \ndecentralize parallel stochastic gradient descent algorithm to apply the \nasynchronous parallelism technology to decentralized algorithms. Our \ntheoretical analysis provides the convergence rate or equivalently the \ncomputational complexity, which is consistent with many special cases and \nindicates we can achieve nice linear speedup when we increase the number of \nnodes or the batchsize. Extensive experiments in deep learning validate the \nproposed algorithm. \n</p>"}, "author": "Xiangru Lian, Wei Zhang, Ce Zhang, Ji Liu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393684", "id": "tag:google.com,2005:reader/item/00000003224b08c6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Minimax Estimation of Bandable Precision Matrices. (arXiv:1710.07006v1 [stat.ML])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07006"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07006", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c98c6a32\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c98c6a32&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The inverse covariance matrix provides considerable insight for understanding \nstatistical models in the multivariate setting. In particular, when the \ndistribution over variables is assumed to be multivariate normal, the sparsity \npattern in the inverse covariance matrix, commonly referred to as the precision \nmatrix, corresponds to the adjacency matrix representation of the Gauss-Markov \ngraph, which encodes conditional independence statements between variables. \nMinimax results under the spectral norm have previously been established for \ncovariance matrices, both sparse and banded, and for sparse precision matrices. \nWe establish minimax estimation bounds for estimating banded precision matrices \nunder the spectral norm. Our results greatly improve upon the existing bounds; \nin particular, we find that the minimax rate for estimating banded precision \nmatrices matches that of estimating banded covariance matrices. The key insight \nin our analysis is that we are able to obtain barely-noisy estimates of $k \n\\times k$ subblocks of the precision matrix by inverting slightly wider blocks \nof the empirical covariance matrix along the diagonal. Our theoretical results \nare complemented by experiments demonstrating the sharpness of our bounds. \n</p>"}, "author": "Addison Hu, Sahand Negahban", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393683", "id": "tag:google.com,2005:reader/item/00000003224b08cd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reti bayesiane per lo studio del fenomeno degli incidenti stradali tra i giovani in Toscana. (arXiv:1710.07066v1 [stat.AP])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07066"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07066", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper aims to analyse adolescents' road accidents in Tuscany. The \nanalysis is based on the Database Edit of Osservatorio di Epidemiologia della \nToscana. Complexity and heterogeneity of Edit's data represet an interesting \nscope to apply Machine Learning methods. In particular, in this paper is \nproposed an analysis based on a Bayesian probabilistic network, used to \ndiscover relationships between adolescents' characteristics and behaviours that \nare more often associated with an audacious driving style. The probabilistic \nnetwork developed by this study can be considered a useful starting point for \nfollow up reasearches, aiming to develop a causal network, a tool to limit this \nphenomenon. \n</p>"}, "author": "Filippo Elba, Lisa Gnaulati, Fabio Voeller", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393682", "id": "tag:google.com,2005:reader/item/00000003224b08d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Meta-Learning via Feature-Label Memory Network. (arXiv:1710.07110v1 [cs.LG])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07110"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07110", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep learning typically requires training a very capable architecture using \nlarge datasets. However, many important learning problems demand an ability to \ndraw valid inferences from small size datasets, and such problems pose a \nparticular challenge for deep learning. In this regard, various researches on \n\"meta-learning\" are being actively conducted. Recent work has suggested a \nMemory Augmented Neural Network (MANN) for meta-learning. MANN is an \nimplementation of a Neural Turing Machine (NTM) with the ability to rapidly \nassimilate new data in its memory, and use this data to make accurate \npredictions. In models such as MANN, the input data samples and their \nappropriate labels from previous step are bound together in the same memory \nlocations. This often leads to memory interference when performing a task as \nthese models have to retrieve a feature of an input from a certain memory \nlocation and read only the label information bound to that location. In this \npaper, we tried to address this issue by presenting a more robust MANN. We \nrevisited the idea of meta-learning and proposed a new memory augmented neural \nnetwork by explicitly splitting the external memory into feature and label \nmemories. The feature memory is used to store the features of input data \nsamples and the label memory stores their labels. Hence, when predicting the \nlabel of a given input, our model uses its feature memory unit as a reference \nto extract the stored feature of the input, and based on that feature, it \nretrieves the label information of the input from the label memory unit. In \norder for the network to function in this framework, a new memory-writingmodule \nto encode label information into the label memory in accordance with the \nmeta-learning task structure is designed. Here, we demonstrate that our model \noutperforms MANN by a large margin in supervised one-shot classification tasks \nusing Omniglot and MNIST datasets. \n</p>"}, "author": "Dawit Mureja, Hyunsin Park, Chang D. Yoo", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508459465394", "timestampUsec": "1508459465393681", "id": "tag:google.com,2005:reader/item/00000003224b08d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Binary Classification from Positive-Confidence Data. (arXiv:1710.07138v1 [stat.ML])", "published": 1508459465, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.07138"}], "alternate": [{"href": "http://arxiv.org/abs/1710.07138", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Reducing labeling costs in supervised learning is a critical issue in many \npractical machine learning applications. In this paper, we consider \npositive-confidence (Pconf) classification, the problem of training a binary \nclassifier only from positive data equipped with confidence. Pconf \nclassification can be regarded as a discriminative extension of one-class \nclassification (which is aimed at \"describing\" the positive class), with \nability to tune hyper-parameters for \"classifying\" positive and negative \nsamples. Pconf classification is also related to positive-unlabeled (PU) \nclassification (which uses hard-labeled positive data and unlabeled data), \nallowing us to avoid estimating the class priors, which is a critical \nbottleneck in typical PU classification methods. For the Pconf classification \nproblem, we provide a simple empirical risk minimization framework and give a \nformulation for linear-in-parameter models that can be implemented easily and \ncomputationally efficiently. We also theoretically establish the consistency \nand generalization error bounds for Pconf classification, and demonstrate the \npractical usefulness of the proposed method through experiments. \n</p>"}, "author": "Takashi Ishida, Gang Niu, Masashi Sugiyama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373889130", "timestampUsec": "1508373889130374", "id": "tag:google.com,2005:reader/item/0000000321a0cbea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Classification and Geometry of General Perceptual Manifolds. (arXiv:1710.06487v1 [cond-mat.dis-nn])", "published": 1508373890, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06487"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06487", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Perceptual manifolds arise when a neural population responds to an ensemble \nof sensory signals associated with different physical features (e.g., \norientation, pose, scale, location, and intensity) of the same perceptual \nobject. Object recognition and discrimination requires classifying the \nmanifolds in a manner that is insensitive to variability within a manifold. How \nneuronal systems give rise to invariant object classification and recognition \nis a fundamental problem in brain theory as well as in machine learning. Here \nwe study the ability of a readout network to classify objects from their \nperceptual manifold representations. We develop a statistical mechanical theory \nfor the linear classification of manifolds with arbitrary geometry revealing a \nremarkable relation to the mathematics of conic decomposition. Novel \ngeometrical measures of manifold radius and manifold dimension are introduced \nwhich can explain the classification capacity for manifolds of various \ngeometries. The general theory is demonstrated on a number of representative \nmanifolds, including L2 ellipsoids prototypical of strictly convex manifolds, \nL1 balls representing polytopes consisting of finite sample points, and \norientation manifolds which arise from neurons tuned to respond to a continuous \nangle variable, such as object orientation. The effects of label sparsity on \nthe classification capacity of manifolds are elucidated, revealing a scaling \nrelation between label sparsity and manifold radius. Theoretical predictions \nare corroborated by numerical simulations using recently developed algorithms \nto compute maximum margin solutions for manifold dichotomies. Our theory and \nits extensions provide a powerful and rich framework for applying statistical \nmechanics of linear classification to data arising from neuronal responses to \nobject stimuli, as well as to artificial deep networks trained for object \nrecognition tasks. \n</p>"}, "author": "SueYeon Chung, Daniel D. Lee, Haim Sompolinsky", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373889130", "timestampUsec": "1508373889130373", "id": "tag:google.com,2005:reader/item/0000000321a0cbf3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "SQG-Differential Evolution for difficult optimization problems under a tight function evaluation budget. (arXiv:1710.06770v1 [cs.NE])", "published": 1508373890, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06770"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06770", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In the context of industrial engineering it is important to integrate \nefficient computational optimization methods in the product development \nprocess. Some of the most challenging simulation based engineering design \noptimization problems are characterized by: a large number of design variables, \nthe absence of analytical gradient information, highly non-linear objectives \nand a limited function evaluation budget. Although a huge variety of different \noptimization algorithms is available, the development and selection of \nefficient algorithms for problems with these industrial relevant \ncharacteristics, remains a challenge. In this communication a hybrid variant of \nDifferential Evolution (DE) is introduced which combines aspects of Stochastic \nQuasi-Gradient (SQG) methods within the framework of DE, in order to improve \noptimization efficiency on problems with the previously mentioned \ncharacteristics. The performance of the resulting method is compared with other \nstate-of-the-art DE variants on 25 commonly used test functions, under tight \nfunction evaluation budget constraints of 1000 evaluations. The experimental \nresults indicate that the proposed method performs particularly good on the \n\"difficult\" (high dimensional, multi-modal, inseparable) test functions. The \noperations used in the proposed mutation scheme, are computationally \ninexpensive, and can be easily implemented in existing differential evolution \nor other optimization algorithms by a few lines of program code as an \nnon-invasive optional setting. Besides the applicability of the presented \nalgorithm by itself, the described concepts can serve as a useful and \ninteresting addition to the algorithmic operators in the frameworks of \nheuristics and evolutionary optimization and computing. \n</p>"}, "author": "Ramses Sala, Niccolo Baldanzini, Marco Pierini", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373889130", "timestampUsec": "1508373889130372", "id": "tag:google.com,2005:reader/item/0000000321a0cbfb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reverse Curriculum Generation for Reinforcement Learning. (arXiv:1707.05300v2 [cs.AI] CROSS LISTED)", "published": 1508373890, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.05300"}], "alternate": [{"href": "http://arxiv.org/abs/1707.05300", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many relevant tasks require an agent to reach a certain state, or to \nmanipulate objects into a desired configuration. For example, we might want a \nrobot to align and assemble a gear onto an axle or insert and turn a key in a \nlock. These goal-oriented tasks present a considerable challenge for \nreinforcement learning, since their natural reward function is sparse and \nprohibitive amounts of exploration are required to reach the goal and receive \nsome learning signal. Past approaches tackle these problems by exploiting \nexpert demonstrations or by manually designing a task-specific reward shaping \nfunction to guide the learning agent. Instead, we propose a method to learn \nthese tasks without requiring any prior knowledge other than obtaining a single \nstate in which the task is achieved. The robot is trained in reverse, gradually \nlearning to reach the goal from a set of start states increasingly far from the \ngoal. Our method automatically generates a curriculum of start states that \nadapts to the agent's performance, leading to efficient training on \ngoal-oriented tasks. We demonstrate our approach on difficult simulated \nnavigation and fine-grained manipulation problems, not solvable by \nstate-of-the-art reinforcement learning methods. \n</p>"}, "author": "Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, Pieter Abbeel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855689", "id": "tag:google.com,2005:reader/item/0000000321a054d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Bayesian Perspective on Generalization and Stochastic Gradient Descent. (arXiv:1710.06451v2 [cs.LG] UPDATED)", "published": 1509064764, "updated": 1509064765, "canonical": [{"href": "http://arxiv.org/abs/1710.06451"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06451", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper tackles two related questions at the heart of machine learning; \nhow can we predict if a minimum will generalize to the test set, and why does \nstochastic gradient descent find minima that generalize well? Our work is \ninspired by Zhang et al. (2017), who showed deep networks can easily memorize \nrandomly labeled training data, despite generalizing well when shown real \nlabels of the same inputs. We show here that the same phenomenon occurs in \nsmall linear models. These observations are explained by evaluating the \nBayesian evidence, which penalizes sharp minima but is invariant to model \nparameterization. We also explore the \"generalization gap\" between small and \nlarge batch training, identifying an optimum batch size which maximizes the \ntest set accuracy. Interpreting stochastic gradient descent as a stochastic \ndifferential equation, we identify a \"noise scale\" $g = \\epsilon (\\frac{N}{B} - \n1) \\approx \\epsilon N/B$, where $\\epsilon$ is the learning rate, $N$ training \nset size and $B$ batch size. Consequently the optimum batch size is \nproportional to the learning rate and the training set size, $B_{opt} \\propto \n\\epsilon N$. We verify these predictions empirically. \n</p>"}, "author": "Samuel L. Smith, Quoc V. Le", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855688", "id": "tag:google.com,2005:reader/item/0000000321a054e3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents. (arXiv:1710.06481v1 [cs.CL])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06481"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06481", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Most Reading Comprehension methods limit themselves to queries which can be \nanswered using a single sentence, paragraph, or document. Enabling models to \ncombine disjoint pieces of textual evidence would extend the scope of machine \ncomprehension methods, but currently there exist no resources to train and test \nthis capability. We propose a novel task to encourage the development of models \nfor text understanding across multiple documents and to investigate the limits \nof existing methods. In our task, a model learns to seek and combine evidence - \neffectively performing multi-hop (alias multi-step) inference. We devise a \nmethodology to produce datasets for this task, given a collection of \nquery-answer pairs and thematically linked documents. Two datasets from \ndifferent domains are induced, and we identify potential pitfalls and devise \ncircumvention strategies. We evaluate two previously proposed competitive \nmodels and find that one can integrate information across documents. However, \nboth models struggle to select relevant information, as providing documents \nguaranteed to be relevant greatly improves their performance. While the models \noutperform several strong baselines, their best accuracy reaches 42.9% compared \nto human performance at 74.0% - leaving ample room for improvement. \n</p>"}, "author": "Johannes Welbl, Pontus Stenetorp, Sebastian Riedel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855687", "id": "tag:google.com,2005:reader/item/0000000321a054f0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Knowledge-guided Pose Grammar Machine for 3D Human Pose Estimation. (arXiv:1710.06513v1 [cs.CV])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06513"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06513", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a knowledge-guided pose grammar network to tackle \nthe problem of 3D human pose estimation. Our model directly takes 2D poses as \ninputs and learns the generalized 2D-3D mapping function, which renders high \napplicability. The proposed network consists of a base network which \nefficiently captures pose-aligned features and a hierarchy of Bidirectional \nRNNs on top of it to explicitly incorporate a set of knowledge (e.g., \nkinematics, symmetry, motor coordination) and thus enforce high-level \nconstraints over human poses. In learning, we develop a pose-guided sample \nsimulator to augment training samples in virtual camera views, which further \nimproves the generalization ability of our model. We validate our method on \npublic 3D human pose benchmarks and propose a new evaluation protocol working \non cross-view setting to verify the generalization ability of different \nmethods. We empirically observe that most state-of-the-arts face difficulty \nunder such setting while our method obtains superior performance. \n</p>"}, "author": "Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, Song-Chun Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855686", "id": "tag:google.com,2005:reader/item/0000000321a054f7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Near-Optimal Adversarial Policy Switching for Decentralized Asynchronous Multi-Agent Systems. (arXiv:1710.06525v1 [cs.AI])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06525"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06525", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c98c6ea7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c98c6ea7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A key challenge in multi-robot and multi-agent systems is generating \nsolutions that are robust to other self-interested or even adversarial parties \nwho actively try to prevent the agents from achieving their goals. The \npracticality of existing works addressing this challenge is limited to only \nsmall-scale synchronous decision-making scenarios or a single agent planning \nits best response against a single adversary with fixed, procedurally \ncharacterized strategies. In contrast this paper considers a more realistic \nclass of problems where a team of asynchronous agents with limited observation \nand communication capabilities need to compete against multiple strategic \nadversaries with changing strategies. This problem necessitates agents that can \ncoordinate to detect changes in adversary strategies and plan the best response \naccordingly. Our approach first optimizes a set of stratagems that represent \nthese best responses. These optimized stratagems are then integrated into a \nunified policy that can detect and respond when the adversaries change their \nstrategies. The near-optimality of the proposed framework is established \ntheoretically as well as demonstrated empirically in simulation and hardware. \n</p>"}, "author": "Trong Nghia Hoang, Yuchen Xiao, Kavinayan Sivakumar, Christopher Amato, Jonathan How", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855685", "id": "tag:google.com,2005:reader/item/0000000321a054fd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Asymmetric Actor Critic for Image-Based Robot Learning. (arXiv:1710.06542v1 [cs.RO])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06542"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06542", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9972837\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9972837&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deep reinforcement learning (RL) has proven a powerful technique in many \nsequential decision making domains. However, Robotics poses many challenges for \nRL, most notably training on a physical system can be expensive and dangerous, \nwhich has sparked significant interest in learning control policies using a \nphysics simulator. While several recent works have shown promising results in \ntransferring policies trained in simulation to the real world, they often do \nnot fully utilize the advantage of working with a simulator. In this work, we \nexploit the full state observability in the simulator to train better policies \nwhich take as input only partial observations (RGBD images). We do this by \nemploying an actor-critic training algorithm in which the critic is trained on \nfull states while the actor (or policy) gets rendered images as input. We show \nexperimentally on a range of simulated tasks that using these asymmetric inputs \nsignificantly improves performance. Finally, we combine this method with domain \nrandomization and show real robot experiments for several tasks like picking, \npushing, and moving a block. We achieve this simulation to real world transfer \nwithout training on any real world data. \n</p>"}, "author": "Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, Pieter Abbeel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855684", "id": "tag:google.com,2005:reader/item/0000000321a05506", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Effects of Memory Replay in Reinforcement Learning. (arXiv:1710.06574v1 [cs.AI])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06574"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06574", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Experience replay is a key technique behind many recent advances in deep \nreinforcement learning. Allowing the agent to learn from earlier memories can \nspeed up learning and break undesirable temporal correlations. Despite its \nwide-spread application, very little is understood about the properties of \nexperience replay. How does the amount of memory kept affect learning dynamics? \nDoes it help to prioritize certain experiences? In this paper, we address these \nquestions by formulating a dynamical systems ODE model of Q-learning with \nexperience replay. We derive analytic solutions of the ODE for a simple \nsetting. We show that even in this very simple setting, the amount of memory \nkept can substantially affect the agent's performance. Too much or too little \nmemory both slow down learning. Moreover, we characterize regimes where \nprioritized replay harms the agent's learning. We show that our analytic \nsolutions have excellent agreement with experiments. Finally, we propose a \nsimple algorithm for adaptively changing the memory buffer size which achieves \nconsistently good empirical performance. \n</p>"}, "author": "Ruishan Liu, James Zou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855683", "id": "tag:google.com,2005:reader/item/0000000321a0550a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deceased Organ Matching in Australia. (arXiv:1710.06636v1 [cs.GT])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06636"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06636", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Despite efforts to increase the supply of organs from living donors, most \nkidney transplants performed in Australia still come from deceased donors. The \nage of these donated organs has increased substantially in recent decades as \nthe rate of fatal accidents on roads has fallen. The Organ and Tissue Authority \nin Australia is therefore looking to design a new mechanism that better matches \nthe age of the organ to the age of the patient. I discuss the design, \naxiomatics and performance of several candidate mechanisms that respect the \nspecial online nature of this fair division problem. \n</p>"}, "author": "Toby Walsh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373576856", "timestampUsec": "1508373576855682", "id": "tag:google.com,2005:reader/item/0000000321a05510", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Photo-Guided Exploration of Volume Data Features. (arXiv:1710.06815v1 [cs.GR])", "published": 1508373577, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06815"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06815", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we pose the question of whether, by considering qualitative \ninformation such as a sample target image as input, one can produce a rendered \nimage of scientific data that is similar to the target. The algorithm resulting \nfrom our research allows one to ask the question of whether features like those \nin the target image exists in a given dataset. In that way, our method is one \nof imagery query or reverse engineering, as opposed to manual parameter \ntweaking of the full visualization pipeline. For target images, we can use \nreal-world photographs of physical phenomena. Our method leverages deep neural \nnetworks and evolutionary optimization. Using a trained similarity function \nthat measures the difference between renderings of a phenomenon and real-world \nphotographs, our method optimizes rendering parameters. We demonstrate the \nefficacy of our method using a superstorm simulation dataset and images found \nonline. We also discuss a parallel implementation of our method, which was run \non NCSA's Blue Waters. \n</p>"}, "author": "Mohammad Raji, Alok Hota, Robert Sisneros, Peter Messmer, Jian Huang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707203", "id": "tag:google.com,2005:reader/item/00000003219fce9b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Bayesian Perspective on Generalization and Stochastic Gradient Descent. (arXiv:1710.06451v2 [cs.LG] UPDATED)", "published": 1509064799, "updated": 1509064802, "canonical": [{"href": "http://arxiv.org/abs/1710.06451"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06451", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper tackles two related questions at the heart of machine learning; \nhow can we predict if a minimum will generalize to the test set, and why does \nstochastic gradient descent find minima that generalize well? Our work is \ninspired by Zhang et al. (2017), who showed deep networks can easily memorize \nrandomly labeled training data, despite generalizing well when shown real \nlabels of the same inputs. We show here that the same phenomenon occurs in \nsmall linear models. These observations are explained by evaluating the \nBayesian evidence, which penalizes sharp minima but is invariant to model \nparameterization. We also explore the \"generalization gap\" between small and \nlarge batch training, identifying an optimum batch size which maximizes the \ntest set accuracy. Interpreting stochastic gradient descent as a stochastic \ndifferential equation, we identify a \"noise scale\" $g = \\epsilon (\\frac{N}{B} - \n1) \\approx \\epsilon N/B$, where $\\epsilon$ is the learning rate, $N$ training \nset size and $B$ batch size. Consequently the optimum batch size is \nproportional to the learning rate and the training set size, $B_{opt} \\propto \n\\epsilon N$. We verify these predictions empirically. \n</p>"}, "author": "Samuel L. Smith, Quoc V. Le", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707202", "id": "tag:google.com,2005:reader/item/00000003219fcea2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "S-Isomap++: Multi Manifold Learning from Streaming Data. (arXiv:1710.06462v1 [stat.ML])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06462"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06462", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Manifold learning based methods have been widely used for non-linear \ndimensionality reduction (NLDR). However, in many practical settings, the need \nto process streaming data is a challenge for such methods, owing to the high \ncomputational complexity involved. Moreover, most methods operate under the \nassumption that the input data is sampled from a single manifold, embedded in a \nhigh dimensional space. We propose a method for streaming NLDR when the \nobserved data is either sampled from multiple manifolds or irregularly sampled \nfrom a single manifold. We show that existing NLDR methods, such as Isomap, \nfail in such situations, primarily because they rely on smoothness and \ncontinuity of the underlying manifold, which is violated in the scenarios \nexplored in this paper. However, the proposed algorithm is able to learn \neffectively in presence of multiple, and potentially intersecting, manifolds, \nwhile allowing for the input data to arrive as a massive stream. \n</p>"}, "author": "Suchismit Mahapatra, Varun Chandola", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707201", "id": "tag:google.com,2005:reader/item/00000003219fceaa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Classification and Geometry of General Perceptual Manifolds. (arXiv:1710.06487v1 [cond-mat.dis-nn])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06487"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06487", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Perceptual manifolds arise when a neural population responds to an ensemble \nof sensory signals associated with different physical features (e.g., \norientation, pose, scale, location, and intensity) of the same perceptual \nobject. Object recognition and discrimination requires classifying the \nmanifolds in a manner that is insensitive to variability within a manifold. How \nneuronal systems give rise to invariant object classification and recognition \nis a fundamental problem in brain theory as well as in machine learning. Here \nwe study the ability of a readout network to classify objects from their \nperceptual manifold representations. We develop a statistical mechanical theory \nfor the linear classification of manifolds with arbitrary geometry revealing a \nremarkable relation to the mathematics of conic decomposition. Novel \ngeometrical measures of manifold radius and manifold dimension are introduced \nwhich can explain the classification capacity for manifolds of various \ngeometries. The general theory is demonstrated on a number of representative \nmanifolds, including L2 ellipsoids prototypical of strictly convex manifolds, \nL1 balls representing polytopes consisting of finite sample points, and \norientation manifolds which arise from neurons tuned to respond to a continuous \nangle variable, such as object orientation. The effects of label sparsity on \nthe classification capacity of manifolds are elucidated, revealing a scaling \nrelation between label sparsity and manifold radius. Theoretical predictions \nare corroborated by numerical simulations using recently developed algorithms \nto compute maximum margin solutions for manifold dichotomies. Our theory and \nits extensions provide a powerful and rich framework for applying statistical \nmechanics of linear classification to data arising from neuronal responses to \nobject stimuli, as well as to artificial deep networks trained for object \nrecognition tasks. \n</p>"}, "author": "SueYeon Chung, Daniel D. Lee, Haim Sompolinsky", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707200", "id": "tag:google.com,2005:reader/item/00000003219fceb3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On reducing sampling variance in covariate shift using control variates. (arXiv:1710.06514v1 [cs.LG])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06514"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06514", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Covariate shift classification problems can in principle be tackled by \nimportance-weighting training samples. However, the sampling variance of the \nrisk estimator is often scaled up dramatically by the weights. This means that \nduring cross-validation - when the importance-weighted risk is repeatedly \nevaluated - suboptimal hyperparameter estimates are produced. We study the \nsampling variances of the importance-weighted versus the oracle estimator as a \nfunction of the relative scale of the training data. We show that introducing a \ncontrol variate can reduce the variance of the importance-weighted risk \nestimator, which leads to superior regularization parameter estimates when the \ntraining data is much smaller in scale than the test data. \n</p>"}, "author": "Wouter Kouw, Marco Loog", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707199", "id": "tag:google.com,2005:reader/item/00000003219fcec3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Exploiting oddsmaker bias to improve the prediction of NFL outcomes. (arXiv:1710.06551v1 [stat.AP])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06551"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06551", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Accurately predicting the outcome of sporting events has been a goal for many \ngroups who seek to maximize profit. What makes this challenging is that the \noutcome of an event can be influenced by many factors that dynamically change \nacross time. Oddsmakers attempt to estimate these factors by using both \nalgorithmic and subjective methods to set the spread. However, it is well-known \nthat both human and algorithmic decision-making can be biased, so this paper \nexplores if oddsmaker biases can be used in an exploitative manner, in order to \nimprove the prediction of NFL game outcomes. Real-world gambling data was used \nto train and test different predictive models under varying assumptions. The \nresults show that methods that leverage oddsmaker biases in an exploitative \nmanner perform best under the conditions tested in this paper. These findings \nsuggest that leveraging human and algorithmic decision biases in an \nexploitative manner may be useful for predicting the outcomes of competitive \nevents, and could lead to increased profit for those who have financial \ninterest in the outcomes. \n</p>"}, "author": "Erik J. Schlicht", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707198", "id": "tag:google.com,2005:reader/item/00000003219fcecb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Revenue-based Attribution Modeling for Online Advertising. (arXiv:1710.06561v1 [econ.EM])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06561"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06561", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9972b01\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9972b01&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper examines and proposes several attribution modeling methods that \nquantify how revenue should be attributed to online advertising inputs. We \nadopt and further develop relative importance method, which is based on \nregression models that have been extensively studied and utilized to \ninvestigate the relationship between advertising efforts and market reaction \n(revenue). Relative importance method aims at decomposing and allocating \nmarginal contributions to the coefficient of determination (R^2) of regression \nmodels as attribution values. In particular, we adopt two alternative \nsubmethods to perform this decomposition: dominance analysis and relative \nweight analysis. Moreover, we demonstrate an extension of the decomposition \nmethods from standard linear model to additive model. We claim that our new \napproaches are more flexible and accurate in modeling the underlying \nrelationship and calculating the attribution values. We use simulation examples \nto demonstrate the superior performance of our new approaches over traditional \nmethods. We further illustrate the value of our proposed approaches using a \nreal advertising campaign dataset. \n</p>"}, "author": "Kaifeng Zhao, Seyed Hanif Mahboobi, Saeed Bagheri", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707197", "id": "tag:google.com,2005:reader/item/00000003219fced1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Replacement AutoEncoder: A Privacy-Preserving Algorithm for Sensory Data Analysis. (arXiv:1710.06564v1 [cs.LG])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06564"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06564", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>An increasing number of sensors on mobile, Internet of things (IoT), and \nwearable devices generate time-series measurements of physical activities. \nThough access to the sensory data is critical to the success of many beneficial \napplications such as health monitoring or activity recognition, a wide range of \npotentially sensitive information about the individuals can also be discovered \nthrough these datasets and this cannot easily be protected using traditional \nprivacy approaches. \n</p> \n<p>In this paper, we propose an integrated sensing framework for managing access \nto personal time-series data in order to provide utility while protecting \nindividuals' privacy. We introduce \\textit{Replacement AutoEncoder}, a novel \nfeature-learning algorithm which learns how to transform discriminative \nfeatures of multidimensional time-series that correspond to sensitive \ninferences, into some features that have been more observed in non-sensitive \ninferences, to protect users' privacy. The main advantage of Replacement \nAutoEncoder is its ability to keep important features of desired inferences \nunchanged to preserve the utility of the data. We evaluate the efficacy of the \nalgorithm with an activity recognition task in a multi-sensing environment \nusing extensive experiments on three benchmark datasets. We show that it can \nretain the recognition accuracy of state-of-the-art techniques while \nsimultaneously preserving the privacy of sensitive information. We use a \nGenerative Adversarial Network to attempt to detect the replacement of \nsensitive data with fake non-sensitive data. We show that this approach does \nnot detect the replacement unless the network can train using the users' \noriginal unmodified data. \n</p>"}, "author": "Mohammad Malekzadeh, Richard G. Clegg, Hamed Haddadi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707196", "id": "tag:google.com,2005:reader/item/00000003219fced7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Correspondence Between Random Neural Networks and Statistical Field Theory. (arXiv:1710.06570v1 [stat.ML])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06570"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06570", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A number of recent papers have provided evidence that practical design \nquestions about neural networks may be tackled theoretically by studying the \nbehavior of random networks. However, until now the tools available for \nanalyzing random neural networks have been relatively ad-hoc. In this work, we \nshow that the distribution of pre-activations in random neural networks can be \nexactly mapped onto lattice models in statistical physics. We argue that \nseveral previous investigations of stochastic networks actually studied a \nparticular factorial approximation to the full lattice model. For random linear \nnetworks and random rectified linear networks we show that the corresponding \nlattice models in the wide network limit may be systematically approximated by \na Gaussian distribution with covariance between the layers of the network. In \neach case, the approximate distribution can be diagonalized by Fourier \ntransformation. We show that this approximation accurately describes the \nresults of numerical simulations of wide random neural networks. Finally, we \ndemonstrate that in each case the large scale behavior of the random networks \ncan be approximated by an effective field theory. \n</p>"}, "author": "Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707195", "id": "tag:google.com,2005:reader/item/00000003219fcee1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Effects of Memory Replay in Reinforcement Learning. (arXiv:1710.06574v1 [cs.AI])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06574"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06574", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Experience replay is a key technique behind many recent advances in deep \nreinforcement learning. Allowing the agent to learn from earlier memories can \nspeed up learning and break undesirable temporal correlations. Despite its \nwide-spread application, very little is understood about the properties of \nexperience replay. How does the amount of memory kept affect learning dynamics? \nDoes it help to prioritize certain experiences? In this paper, we address these \nquestions by formulating a dynamical systems ODE model of Q-learning with \nexperience replay. We derive analytic solutions of the ODE for a simple \nsetting. We show that even in this very simple setting, the amount of memory \nkept can substantially affect the agent's performance. Too much or too little \nmemory both slow down learning. Moreover, we characterize regimes where \nprioritized replay harms the agent's learning. We show that our analytic \nsolutions have excellent agreement with experiments. Finally, we propose a \nsimple algorithm for adaptively changing the memory buffer size which achieves \nconsistently good empirical performance. \n</p>"}, "author": "Ruishan Liu, James Zou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707194", "id": "tag:google.com,2005:reader/item/00000003219fcee7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Social Image Embedding with Deep Multimodal Attention Networks. (arXiv:1710.06582v1 [cs.MM])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06582"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06582", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning social media data embedding by deep models has attracted extensive \nresearch interest as well as boomed a lot of applications, such as link \nprediction, classification, and cross-modal search. However, for social images \nwhich contain both link information and multimodal contents (e.g., text \ndescription, and visual content), simply employing the embedding learnt from \nnetwork structure or data content results in sub-optimal social image \nrepresentation. In this paper, we propose a novel social image embedding \napproach called Deep Multimodal Attention Networks (DMAN), which employs a deep \nmodel to jointly embed multimodal contents and link information. Specifically, \nto effectively capture the correlations between multimodal contents, we propose \na multimodal attention network to encode the fine-granularity relation between \nimage regions and textual words. To leverage the network structure for \nembedding learning, a novel Siamese-Triplet neural network is proposed to model \nthe links among images. With the joint deep model, the learnt embedding can \ncapture both the multimodal contents and the nonlinear network information. \nExtensive experiments are conducted to investigate the effectiveness of our \napproach in the applications of multi-label classification and cross-modal \nsearch. Compared to state-of-the-art image embeddings, our proposed DMAN \nachieves significant improvement in the tasks of multi-label classification and \ncross-modal search. \n</p>"}, "author": "Feiran Huang, Xiaoming Zhang, Zhoujun Li, Tao Mei, Yueying He, Zhonghua Zhao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707193", "id": "tag:google.com,2005:reader/item/00000003219fceef", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Variational Inference based on Robust Divergences. (arXiv:1710.06595v1 [stat.ML])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06595"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06595", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Robustness to outliers is a central issue in real-world machine learning \napplications. While replacing a model to a heavy-tailed one (e.g., from \nGaussian to Student-t) is a standard approach for robustification, it can only \nbe applied to simple models. In this paper, based on Zellner's optimization and \nvariational formulation of Bayesian inference, we propose an outlier-robust \npseudo-Bayesian variational method by replacing the Kullback-Leibler divergence \nused for data fitting to a robust divergence such as the beta and \ngamma-divergences. An advantage of our approach is that complex models such as \ndeep networks can be handled. We theoretically prove that, for deep networks \nwith ReLU activation functions, the influence function in our proposed method \nis bounded, while it is unbounded in the ordinary variational inference. This \nimplies that our proposed method is robust to both of input and output \noutliers, while the ordinary variational method is not. We experimentally \ndemonstrate that our robust variational method outperforms ordinary variational \ninference in regression and classification with deep networks. \n</p>"}, "author": "Futoshi Futami, Issei Sato, Masashi Sugiyama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707192", "id": "tag:google.com,2005:reader/item/00000003219fcef5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Sinkhorn-Newton method for entropic optimal transport. (arXiv:1710.06635v1 [math.OC])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06635"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06635", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the entropic regularization of discretized optimal transport and \npropose to solve its optimality conditions via a logarithmic Newton iteration. \nWe show a quadratic convergence rate and validate numerically that the method \ncompares favorably with the more commonly used Sinkhorn--Knopp algorithm for \nsmall regularization strength. We further investigate numerically the \nrobustness of the proposed method with respect to parameters such as the mesh \nsize of the discretization. \n</p>"}, "author": "Christoph Brauer, Christian Clason, Dirk Lorenz, Benedikt Wirth", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707191", "id": "tag:google.com,2005:reader/item/00000003219fcefc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Weighted Function Norm Regularization. (arXiv:1710.06703v1 [cs.LG])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06703"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06703", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks (DNNs) have become increasingly important due to their \nexcellent empirical performance on a wide range of problems. However, \nregularization is generally achieved by indirect means, largely due to the \ncomplex set of functions defined by a network and the difficulty in measuring \nfunction complexity. There exists no method in the literature for additive \nregularization based on a norm of the function, as is classically considered in \nstatistical learning theory. In this work, we propose sampling-based \napproximations to weighted function norms as regularizers for deep neural \nnetworks. We provide, to the best of our knowledge, the first proof in the \nliterature of the NP-hardness of computing function norms of DNNs, motivating \nthe necessity of a stochastic optimization strategy. Based on our proposed \nregularization scheme, stability-based bounds yield a \n$\\mathcal{O}(N^{-\\frac{1}{2}})$ generalization error for our proposed \nregularizer when applied to convex function sets. We demonstrate broad \nconditions for the convergence of stochastic gradient descent on our objective, \nincluding for non-convex function sets such as those defined by DNNs. Finally, \nwe empirically validate the improved performance of the proposed regularization \nstrategy for both convex function sets as well as DNNs on real-world \nclassification and segmentation tasks. \n</p>"}, "author": "Amal Rannen Triki, Maxim Berman, Matthew B. Blaschko", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707190", "id": "tag:google.com,2005:reader/item/00000003219fcf01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A complete characterization of optimal dictionaries for least squares representation. (arXiv:1710.06763v1 [math.OC])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06763"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06763", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Dictionaries are collections of vectors used for representations of elements \nin Euclidean spaces. While recent research on optimal dictionaries is focussed \non providing sparse (i.e., $\\ell_0$-optimal,) representations, here we consider \nthe problem of finding optimal dictionaries such that representations of \nsamples of a random vector are optimal in an $\\ell_2$-sense. For us, optimality \nof representation is equivalent to minimization of the average $\\ell_2$-norm of \nthe coefficients used to represent the random vector, with the lengths of the \ndictionary vectors being specified a priori. With the help of recent results on \nrank-$1$ decompositions of symmetric positive semidefinite matrices and the \ntheory of majorization, we provide a complete characterization of \n$\\ell_2$-optimal dictionaries. Our results are accompanied by polynomial time \nalgorithms that construct $\\ell_2$-optimal dictionaries from given data. \n</p>"}, "author": "Mohammed Rayyan Sheriff, Debasish Chatterjee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707189", "id": "tag:google.com,2005:reader/item/00000003219fcf05", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Phase Transitions in the Pooled Data Problem. (arXiv:1710.06766v1 [stat.ML])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06766"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06766", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study the pooled data problem of identifying the labels \nassociated with a large collection of items, based on a sequence of pooled \ntests revealing the counts of each label within the pool. In the noiseless \nsetting, we identify an exact asymptotic threshold on the required number of \ntests with optimal decoding, and prove a phase transition between complete \nsuccess and complete failure. In addition, we present a novel noisy variation \nof the problem, and provide an information-theoretic framework for \ncharacterizing the required number of tests for general random noise models. \nOur results reveal that noise can make the problem considerably more difficult, \nwith strict increases in the scaling laws even at low noise levels. Finally, we \ndemonstrate similar behavior in an approximate recovery setting, where a given \nnumber of errors is allowed in the decoded labels. \n</p>"}, "author": "Jonathan Scarlett, Volkan Cevher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707188", "id": "tag:google.com,2005:reader/item/00000003219fcf08", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Weighted Tensor Decomposition for Learning Latent Variables with Partial Data. (arXiv:1710.06818v1 [stat.ML])", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06818"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06818", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9972cf3\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9972cf3&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Tensor decomposition methods are popular tools for learning latent variables \ngiven only lower-order moments of the data. However, the standard assumption is \nthat we have sufficient data to estimate these moments to high accuracy. In \nthis work, we consider the case in which certain dimensions of the data are not \nalways observed---common in applied settings, where not all measurements may be \ntaken for all observations---resulting in moment estimates of varying quality. \nWe derive a weighted tensor decomposition approach that is computationally as \nefficient as the non-weighted approach, and demonstrate that it outperforms \nmethods that do not appropriately leverage these less-observed dimensions. \n</p>"}, "author": "Omer Gottesman, Weiwei Pan, Finale Doshi-Velez", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707178", "id": "tag:google.com,2005:reader/item/00000003219fcf35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neural Networks Quantum States, String-Bond States and chiral topological states. (arXiv:1710.04045v2 [quant-ph] CROSS LISTED)", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04045"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04045", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9a41787\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9a41787&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Neural Networks Quantum States have been recently introduced as an Ansatz for \ndescribing the wave function of quantum many-body systems. We show that there \nare strong connections between Neural Networks Quantum States in the form of \nRestricted Boltzmann Machines and some classes of Tensor Network states in \narbitrary dimension. In particular we demonstrate that short-range Restricted \nBoltzmann Machines are Entangled Plaquette States, while fully connected \nRestricted Boltzmann Machines are String-Bond States with a non-local geometry \nand low bond dimension. These results shed light on the underlying architecture \nof Restricted Boltzmann Machines and their efficiency at representing many-body \nquantum states. String-Bond States also provide a generic way of enhancing the \npower of Neural Networks Quantum States and a natural generalization to systems \nwith larger local Hilbert space. We compare the advantages and drawbacks of \nthese different classes of states and present a method to combine them \ntogether. This allows us to benefit from both the entanglement structure of \nTensor Networks and the efficiency of Neural Network Quantum States into a \nsingle Ansatz capable of targeting the wave function of strongly correlated \nsystems. While it remains a challenge to describe states with chiral \ntopological order using traditional Tensor Networks, we show that Neural \nNetworks Quantum States and their String-Bond States extension can describe a \nlattice Fractional Quantum Hall state exactly. In addition, we provide \nnumerical evidence that Neural Networks Quantum States can approximate a chiral \nspin liquid with better accuracy than Entangled Plaquette States and local \nString-Bond States. Our results demonstrate the efficiency of neural networks \nto describe complex quantum wave functions and pave the way towards the use of \nString-Bond States as a tool in more traditional machine learning applications. \n</p>"}, "author": "Ivan Glasser, Nicola Pancotti, Moritz August, Ivan D. Rodriguez, J. Ignacio Cirac", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508373235707", "timestampUsec": "1508373235707177", "id": "tag:google.com,2005:reader/item/00000003219fcf39", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "User Modelling for Avoiding Overfitting in Interactive Knowledge Elicitation for Prediction. (arXiv:1710.04881v1 [cs.HC] CROSS LISTED)", "published": 1508373235, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04881"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04881", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In human-in-the-loop machine learning, the user provides information beyond \nthat in the training data. Many algorithms and user interfaces have been \ndesigned to optimize and facilitate this human--machine interaction; however, \nfewer studies have addressed the potential defects the designs can cause. \nEffective interaction often requires exposing the user to the training data or \nits statistics. The design of the system is then critical, as this can lead to \ndouble use of data and overfitting, if the user reinforces noisy patterns in \nthe data. We propose a user modelling methodology, by assuming simple rational \nbehaviour, to correct the problem. We show, in a user study with 48 \nparticipants, that the method improves predictive performance in a sparse \nlinear regression sentiment analysis task, where graded user knowledge on \nfeature relevance is elicited. We believe that the key idea of inferring user \nknowledge with probabilistic user models has general applicability in guarding \nagainst overfitting and improving interactive machine learning. \n</p>"}, "author": "Pedram Daee, Tomi Peltola, Aki Vehtari, Samuel Kaski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976761", "id": "tag:google.com,2005:reader/item/00000003214c0854", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Neuro Fuzzy Modelling for Prediction of Consumer Price Index. (arXiv:1710.05944v1 [cs.CY])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05944"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05944", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Economic indicators such as Consumer Price Index (CPI) have frequently used \nin predicting future economic wealth for financial policy makers of respective \ncountry. Most central banks, on guidelines of research studies, have recently \nadopted an inflation targeting monetary policy regime, which accounts for high \nrequirement for effective prediction model of consumer price index. However, \nprediction accuracy by numerous studies is still low, which raises a need for \nimprovement. This manuscript presents findings of study that use neuro fuzzy \ntechnique to design a machine-learning model that train and test data to \npredict a univariate time series CPI. The study establishes a matrix of monthly \nCPI data from secondary data source of Tanzania National Bureau of Statistics \nfrom January 2000 to December 2015 as case study and thereafter conducted \nsimulation experiments on MATLAB whereby ninety five percent (95%) of data used \nto train the model and five percent (5%) for testing. Furthermore, the study \nuse root mean square error (RMSE) and mean absolute percentage error (MAPE) as \nerror metrics for model evaluation. The results show that the neuro fuzzy model \nhave an architecture of 5:74:1 with Gaussian membership functions (2, 2, 2, 2, \n2), provides RMSE of 0.44886 and MAPE 0.23384, which is far better compared to \nexisting research studies. \n</p>"}, "author": "Godwin Ambukege, Godfrey Justo, Joseph Mushi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976760", "id": "tag:google.com,2005:reader/item/00000003214c0860", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gradient-free Policy Architecture Search and Adaptation. (arXiv:1710.05958v1 [cs.LG])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05958"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05958", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop a method for policy architecture search and adaptation via \ngradient-free optimization which can learn to perform autonomous driving tasks. \nBy learning from both demonstration and environmental reward we develop a model \nthat can learn with relatively few early catastrophic failures. We first learn \nan architecture of appropriate complexity to perceive aspects of world state \nrelevant to the expert demonstration, and then mitigate the effect of \ndomain-shift during deployment by adapting a policy demonstrated in a source \ndomain to rewards obtained in a target environment. We show that our approach \nallows safer learning than baseline methods, offering a reduced cumulative \ncrash metric over the agent's lifetime as it learns to drive in a realistic \nsimulated environment. \n</p>"}, "author": "Sayna Ebrahimi, Anna Rohrbach, Trevor Darrell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976759", "id": "tag:google.com,2005:reader/item/00000003214c0879", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Safe Medicine Recommendation via Medical Knowledge Graph Embedding. (arXiv:1710.05980v1 [cs.IR])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05980"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05980", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Most of the existing medicine recommendation systems that are mainly based on \nelectronic medical records (EMRs) are significantly assisting doctors to make \nbetter clinical decisions benefiting both patients and caregivers. Even though \nthe growth of EMRs is at a lighting fast speed in the era of big data, content \nlimitations in EMRs restrain the existed recommendation systems to reflect \nrelevant medical facts, such as drug-drug interactions. Many medical knowledge \ngraphs that contain drug-related information, such as DrugBank, may give hope \nfor the recommendation systems. However, the direct use of these knowledge \ngraphs in the systems suffers from robustness caused by the incompleteness of \nthe graphs. To address these challenges, we stand on recent advances in graph \nembedding learning techniques and propose a novel framework, called Safe \nMedicine Recommendation (SMR), in this paper. Specifically, SMR first \nconstructs a high-quality heterogeneous graph by bridging EMRs (MIMIC-III) and \nmedical knowledge graphs (ICD-9 ontology and DrugBank). Then, SMR jointly \nembeds diseases, medicines, patients, and their corresponding relations into a \nshared lower dimensional space. Finally, SMR uses the embeddings to decompose \nthe medicine recommendation into a link prediction process while considering \nthe patient's diagnoses and adverse drug reactions. To our best knowledge, SMR \nis the first to learn embeddings of a patient-disease-medicine graph for \nmedicine recommendation in the world. Extensive experiments on real datasets \nare conducted to evaluate the effectiveness of proposed framework. \n</p>"}, "author": "Meng Wang, Mengyue Liu, Jun Liu, Sen Wang, Guodong Long, Buyue Qian", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976758", "id": "tag:google.com,2005:reader/item/00000003214c088b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Reply With: Proactive Recommendation of Email Attachments. (arXiv:1710.06061v2 [cs.IR] UPDATED)", "published": 1510995733, "updated": 1510995735, "canonical": [{"href": "http://arxiv.org/abs/1710.06061"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06061", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Email responses often contain items-such as a file or a hyperlink to an \nexternal document-that are attached to or included inline in the body of the \nmessage. Analysis of an enterprise email corpus reveals that 35% of the time \nwhen users include these items as part of their response, the attachable item \nis already present in their inbox or sent folder. A modern email client can \nproactively retrieve relevant attachable items from the user's past emails \nbased on the context of the current conversation, and recommend them for \ninclusion, to reduce the time and effort involved in composing the response. In \nthis paper, we propose a weakly supervised learning framework for recommending \nattachable items to the user. As email search systems are commonly available, \nwe constrain the recommendation task to formulating effective search queries \nfrom the context of the conversations. The query is submitted to an existing IR \nsystem to retrieve relevant items for attachment. We also present a novel \nstrategy for generating labels from an email corpus---without the need for \nmanual annotations---that can be used to train and evaluate the query \nformulation model. In addition, we describe a deep convolutional neural network \nthat demonstrates satisfactory performance on this query formulation task when \nevaluated on the publicly available Avocado dataset and a proprietary dataset \nof internal emails obtained through an employee participation program. \n</p>"}, "author": "Christophe Van Gysel, Bhaskar Mitra, Matteo Venanzi, Roy Rosemarin, Grzegorz Kukla, Piotr Grudzien, Nicola Cancedda", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976757", "id": "tag:google.com,2005:reader/item/00000003214c089d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts. (arXiv:1710.06071v1 [cs.CL])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06071"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06071", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present PubMed 200k RCT, a new dataset based on PubMed for sequential \nsentence classification. The dataset consists of approximately 200,000 \nabstracts of randomized controlled trials, totaling 2.3 million sentences. Each \nsentence of each abstract is labeled with their role in the abstract using one \nof the following classes: background, objective, method, result, or conclusion. \nThe purpose of releasing this dataset is twofold. First, the majority of \ndatasets for sequential short-text classification (i.e., classification of \nshort texts that appear in sequences) are small: we hope that releasing a new \nlarge dataset will help develop more accurate algorithms for this task. Second, \nfrom an application perspective, researchers need better tools to efficiently \nskim through the literature. Automatically classifying each sentence in an \nabstract would help researchers read abstracts more efficiently, especially in \nfields where abstracts may be long, such as the medical field. \n</p>"}, "author": "Franck Dernoncourt, Ji Young Lee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976756", "id": "tag:google.com,2005:reader/item/00000003214c08ad", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Spontaneous Symmetry Breaking in Neural Networks. (arXiv:1710.06096v1 [stat.CO])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06096"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06096", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a framework to understand the unprecedented performance and \nrobustness of deep neural networks using field theory. Correlations between the \nweights within the same layer can be described by symmetries in that layer, and \nnetworks generalize better if such symmetries are broken to reduce the \nredundancies of the weights. Using a two parameter field theory, we find that \nthe network can break such symmetries itself towards the end of training in a \nprocess commonly known in physics as spontaneous symmetry breaking. This \ncorresponds to a network generalizing itself without any user input layers to \nbreak the symmetry, but by communication with adjacent layers. In the layer \ndecoupling limit applicable to residual networks (He et al., 2015), we show \nthat the remnant symmetries that survive the non-linear layers are \nspontaneously broken. The Lagrangian for the non-linear and weight layers \ntogether has striking similarities with the one in quantum field theory of a \nscalar. Using results from quantum field theory we show that our framework is \nable to explain many experimentally observed phenomena,such as training on \nrandom labels with zero error (Zhang et al., 2017), the information bottleneck, \nthe phase transition out of it and gradient variance explosion (Shwartz-Ziv &amp; \nTishby, 2017), shattered gradients (Balduzzi et al., 2017), and many more. \n</p>"}, "author": "Ricky Fok, Aijun An, Xiaogang Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976755", "id": "tag:google.com,2005:reader/item/00000003214c08c1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Map-based Multi-Policy Reinforcement Learning: Enhancing Adaptability of Robots by Deep Reinforcement Learning. (arXiv:1710.06117v2 [cs.RO] UPDATED)", "published": 1508373577, "updated": 1508373578, "canonical": [{"href": "http://arxiv.org/abs/1710.06117"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06117", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In order for robots to perform mission-critical tasks, it is essential that \nthey are able to quickly adapt to changes in their environment as well as to \ninjuries and or other bodily changes. Deep reinforcement learning has been \nshown to be successful in training robot control policies for operation in \ncomplex environments. However, existing methods typically employ only a single \npolicy. This can limit the adaptability since a large environmental \nmodification might require a completely different behavior compared to the \nlearning environment. To solve this problem, we propose Map-based Multi-Policy \nReinforcement Learning (MMPRL), which aims to search and store multiple \npolicies that encode different behavioral features while maximizing the \nexpected reward in advance of the environment change. Thanks to these policies, \nwhich are stored into a multi-dimensional discrete map according to its \nbehavioral feature, adaptation can be performed within reasonable time without \nretraining the robot. An appropriate pre-trained policy from the map can be \nrecalled using Bayesian optimization. Our experiments show that MMPRL enables \nrobots to quickly adapt to large changes without requiring any prior knowledge \non the type of injuries that could occur. A highlight of the learned behaviors \ncan be found here: https://youtu.be/QwInbilXNOE . \n</p>"}, "author": "Ayaka Kume, Eiichi Matsumoto, Kuniyuki Takahashi, Wilson Ko, Jethro Tan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976754", "id": "tag:google.com,2005:reader/item/00000003214c08d9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Distributed algorithm for empty vehicles management in personal rapid transit (PRT) network. (arXiv:1710.06331v1 [cs.DC])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06331"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06331", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9a41a1c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9a41a1c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, an original heuristic algorithm of empty vehicles management \nin personal rapid transit network is presented. The algorithm is used for the \ndelivery of empty vehicles for waiting passengers, for balancing the \ndistribution of empty vehicles within the network, and for providing an empty \nspace for vehicles approaching a station. Each of these tasks involves a \ndecision on the trip that has to be done by a selected empty vehicle from its \nactual location to some determined destination. The decisions are based on a \nmulti-parameter function involving a set of factors and thresholds. An \nimportant feature of the algorithm is that it does not use any central database \nof passenger input (demand) and locations of free vehicles. Instead, it is \nbased on the local exchange of data between stations: on their states and on \nthe vehicles they expect. Therefore, it seems well-tailored for a distributed \nimplementation. The algorithm is uniform, meaning that the same basic procedure \nis used for multiple tasks using a task-specific set of parameters. \n</p>"}, "author": "Wiktor B. Daszczuk, Jerzy Mie&#x15b;cicki, Waldemar Grabski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976753", "id": "tag:google.com,2005:reader/item/00000003214c08f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Laying Down the Yellow Brick Road: Development of a Wizard-of-Oz Interface for Collecting Human-Robot Dialogue. (arXiv:1710.06406v1 [cs.CL])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06406"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06406", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe the adaptation and refinement of a graphical user interface \ndesigned to facilitate a Wizard-of-Oz (WoZ) approach to collecting human-robot \ndialogue data. The data collected will be used to develop a dialogue system for \nrobot navigation. Building on an interface previously used in the development \nof dialogue systems for virtual agents and video playback, we add templates \nwith open parameters which allow the wizard to quickly produce a wide variety \nof utterances. Our research demonstrates that this approach to data collection \nis viable as an intermediate step in developing a dialogue system for physical \nrobots in remote locations from their users - a domain in which the human and \nrobot need to regularly verify and update a shared understanding of the \nphysical environment. We show that our WoZ interface and the fixed set of \nutterances and templates therein provide for a natural pace of dialogue with \ngood coverage of the navigation domain. \n</p>"}, "author": "Claire Bonial, Matthew Marge, Ron artstein, Ashley Foots, Felix Gervits, Cory J. Hayes, Cassidy Henry, Susan G. Hill, Anton Leuski, Stephanie M. Lukin, Pooja Moolchandani, Kimberly A. Pollard, David Traum, Clare R. Voss", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976752", "id": "tag:google.com,2005:reader/item/00000003214c0910", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-task Domain Adaptation for Deep Learning of Instance Grasping from Simulation. (arXiv:1710.06422v1 [cs.LG])", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06422"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06422", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning-based approaches to robotic manipulation are limited by the \nscalability of data collection and accessibility of labels. In this paper, we \npresent a multi-task domain adaptation framework for instance grasping in \ncluttered scenes by utilizing simulated robot experiments. Our neural network \ntakes monocular RGB images and the instance segmentation mask of a specified \ntarget object as inputs, and predicts the probability of successfully grasping \nthe specified object for each candidate motor command. The proposed transfer \nlearning framework trains a model for instance grasping in simulation and uses \na domain-adversarial loss to transfer the trained model to real robots using \nindiscriminate grasping data, which is available both in simulation and the \nreal world. We evaluate our model in real-world robot experiments, comparing it \nwith alternative model architectures as well as an indiscriminate grasping \nbaseline. \n</p>"}, "author": "Kuan Fang, Yunfei Bai, Stefan Hinterstoisser, Mrinal Kalakrishnan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508332026977", "timestampUsec": "1508332026976742", "id": "tag:google.com,2005:reader/item/00000003214c0989", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Self-Supervised Visual Planning with Temporal Skip Connections. (arXiv:1710.05268v1 [cs.RO] CROSS LISTED)", "published": 1508332027, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05268"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05268", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In order to autonomously learn wide repertoires of complex skills, robots \nmust be able to learn from their own autonomously collected data, without human \nsupervision. One learning signal that is always available for autonomously \ncollected data is prediction: if a robot can learn to predict the future, it \ncan use this predictive model to take actions to produce desired outcomes, such \nas moving an object to a particular location. However, in complex open-world \nscenarios, designing a representation for prediction is difficult. In this \nwork, we instead aim to enable self-supervised robotic learning through direct \nvideo prediction: instead of attempting to design a good representation, we \ndirectly predict what the robot will see next, and then use this model to \nachieve desired goals. A key challenge in video prediction for robotic \nmanipulation is handling complex spatial arrangements such as occlusions. To \nthat end, we introduce a video prediction model that can keep track of objects \nthrough occlusion by incorporating temporal skip-connections. Together with a \nnovel planning criterion and action space formulation, we demonstrate that this \nmodel substantially outperforms prior work on video prediction-based control. \nOur results show manipulation of objects not seen during training, handling \nmultiple objects, and pushing objects around obstructions. These results \nrepresent a significant advance in the range and complexity of skills that can \nbe performed entirely with self-supervised robotic learning. \n</p>"}, "author": "Frederik Ebert, Chelsea Finn, Alex X. Lee, Sergey Levine", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715974", "id": "tag:google.com,2005:reader/item/0000000321219be4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convolutional neural networks for structured omics: OmicsCNN and the OmicsConv layer. (arXiv:1710.05918v1 [q-bio.QM])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05918"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05918", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Convolutional Neural Networks (CNNs) are a popular deep learning architecture \nwidely applied in different domains, in particular in classifying over images, \nfor which the concept of convolution with a filter comes naturally. \nUnfortunately, the requirement of a distance (or, at least, of a neighbourhood \nfunction) in the input feature space has so far prevented its direct use on \ndata types such as omics data. However, a number of omics data are metrizable, \ni.e., they can be endowed with a metric structure, enabling to adopt a \nconvolutional based deep learning framework, e.g., for prediction. We propose a \ngeneralized solution for CNNs on omics data, implemented through a dedicated \nKeras layer. In particular, for metagenomics data, a metric can be derived from \nthe patristic distance on the phylogenetic tree. For transcriptomics data, we \ncombine Gene Ontology semantic similarity and gene co-expression to define a \ndistance; the function is defined through a multilayer network where 3 layers \nare defined by the GO mutual semantic similarity while the fourth one by gene \nco-expression. As a general tool, feature distance on omics data is enabled by \nOmicsConv, a novel Keras layer, obtaining OmicsCNN, a dedicated deep learning \nframework. Here we demonstrate OmicsCNN on gut microbiota sequencing data, for \nInflammatory Bowel Disease (IBD) 16S data, first on synthetic data and then a \nmetagenomics collection of gut microbiota of 222 IBD patients. \n</p>"}, "author": "Giuseppe Jurman, Valerio Maggio, Diego Fioravanti, Ylenia Giarratano, Isotta Landi, Margherita Francescatto, Claudio Agostinelli, Marco Chierici, Manlio De Domenico, Cesare Furlanello", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715973", "id": "tag:google.com,2005:reader/item/0000000321219bea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sparse Linear Isotonic Models. (arXiv:1710.05989v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05989"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05989", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In machine learning and data mining, linear models have been widely used to \nmodel the response as parametric linear functions of the predictors. To relax \nsuch stringent assumptions made by parametric linear models, additive models \nconsider the response to be a summation of unknown transformations applied on \nthe predictors; in particular, additive isotonic models (AIMs) assume the \nunknown transformations to be monotone. In this paper, we introduce sparse \nlinear isotonic models (SLIMs) for highdimensional problems by hybridizing \nideas in parametric sparse linear models and AIMs, which enjoy a few appealing \nadvantages over both. In the high-dimensional setting, a two-step algorithm is \nproposed for estimating the sparse parameters as well as the monotone functions \nover predictors. Under mild statistical assumptions, we show that the algorithm \ncan accurately estimate the parameters. Promising preliminary experiments are \npresented to support the theoretical results. \n</p>"}, "author": "Sheng Chen, Arindam Banerjee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715972", "id": "tag:google.com,2005:reader/item/0000000321219bf2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "VAMPnets: Deep learning of molecular kinetics. (arXiv:1710.06012v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06012"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06012", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Here we develop a deep learning framework for molecular kinetics from \nmolecular dynamics (MD) simulation data. There is an increasing demand for \ncomputing the relevant structures, equilibria and long-timescale kinetics of \ncomplex biomolecular processes, such as protein-drug binding, from \nhigh-throughput MD simulations. State-of-the art methods employ a handcrafted \ndata processing pipeline, involving (i) transformation of simulated coordinates \ninto a set of features characterizing the molecular structure, (ii) dimension \nreduction to collective variables, (iii) clustering the dimension-reduced data, \nand (iv) estimation of a Markov state model (MSM) or related model of the \ninterconversion rates between molecular structures. This approach demands a \nsubstantial amount of modeling expertise, as poor decisions at every step will \nlead to large modeling errors. Here we employ the recently developed \nvariational approach for Markov processes (VAMP) to develop a deep learning \nframework for molecular kinetics using neural networks, dubbed VAMPnets. A \nVAMPnet encodes the entire mapping from molecular coordinates to Markov states \nand learns optimal feature transformations, nonlinear dimension reduction, \ncluster discretization and MSM estimation within a single end-to-end learning \nframework. Our results, ranging from toy models to protein folding, are \ncompetitive or outperform state-of-the art Markov modeling methods and readily \nprovide easily interpretable few-state kinetic models. \n</p>"}, "author": "Andreas Mardt, Luca Pasquali, Hao Wu, Frank No&#xe9;", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715971", "id": "tag:google.com,2005:reader/item/0000000321219bf9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Linear Regression with Sparsely Permuted Data. (arXiv:1710.06030v2 [math.ST] UPDATED)", "published": 1510882706, "updated": 1510882707, "canonical": [{"href": "http://arxiv.org/abs/1710.06030"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06030", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In regression analysis of multivariate data, it is tacitly assumed that \nresponse and predictor variables in each observed response-predictor pair \ncorrespond to the same entity or unit. In this paper, we consider the situation \nof \"permuted data\" in which this basic correspondence has been lost. Several \nrecent papers have considered this situation without further assumptions on the \nunderlying permutation. In applications, the latter is often to known to have \nadditional structure that can be leveraged. Specifically, we herein consider \nthe common scenario of \"sparsely permuted data\" in which only a small fraction \nof the data is affected by a mismatch between response and predictors. However, \nan adverse effect already observed for sparsely permuted data is that the least \nsquares estimator as well as other estimators not accounting for such partial \nmismatch are inconsistent. One approach studied in detail herein is to treat \npermuted data as outliers which motivates the use of robust regression \nformulations to estimate the regression parameter. The resulting estimate can \nsubsequently be used to recover the permutation. A notable benefit of the \nproposed approach is its computational simplicity given the general lack of \nprocedures for the above problem that are both statistically sound and \ncomputationally appealing. \n</p>"}, "author": "Martin Slawski, Emanuel Ben-David", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715970", "id": "tag:google.com,2005:reader/item/0000000321219bfd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Variance Reduction for Policy Gradient Estimation. (arXiv:1710.06034v1 [cs.LG])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06034"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06034", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent advances in policy gradient methods and deep learning have \ndemonstrated their applicability for complex reinforcement learning problems. \nHowever, the variance of the performance gradient estimates obtained from the \nsimulation is often excessive, leading to poor sample efficiency. In this \npaper, we apply the stochastic variance reduced gradient descent (SVRG) to \nmodel-free policy gradient to significantly improve the sample-efficiency. The \nSVRG estimation is incorporated into a trust-region Newton conjugate gradient \nframework for the policy optimization. On several Mujoco tasks, our method \nachieves significantly better performance compared to the state-of-the-art \nmodel-free policy gradient methods in robotic continuous control such as trust \nregion policy optimization (TRPO) \n</p>"}, "author": "Tianbing Xu, Qiang Liu, Jian Peng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715969", "id": "tag:google.com,2005:reader/item/0000000321219c12", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts. (arXiv:1710.06071v1 [cs.CL])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06071"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06071", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present PubMed 200k RCT, a new dataset based on PubMed for sequential \nsentence classification. The dataset consists of approximately 200,000 \nabstracts of randomized controlled trials, totaling 2.3 million sentences. Each \nsentence of each abstract is labeled with their role in the abstract using one \nof the following classes: background, objective, method, result, or conclusion. \nThe purpose of releasing this dataset is twofold. First, the majority of \ndatasets for sequential short-text classification (i.e., classification of \nshort texts that appear in sequences) are small: we hope that releasing a new \nlarge dataset will help develop more accurate algorithms for this task. Second, \nfrom an application perspective, researchers need better tools to efficiently \nskim through the literature. Automatically classifying each sentence in an \nabstract would help researchers read abstracts more efficiently, especially in \nfields where abstracts may be long, such as the medical field. \n</p>"}, "author": "Franck Dernoncourt, Ji Young Lee", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715968", "id": "tag:google.com,2005:reader/item/0000000321219c14", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Estimate exponential memory decay in Hidden Markov Model and its applications. (arXiv:1710.06078v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06078"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06078", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9a41c70\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9a41c70&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Inference in hidden Markov model has been challenging in terms of scalability \ndue to dependencies in the observation data. In this paper, we utilize the \ninherent memory decay in hidden Markov models, such that the forward and \nbackward probabilities can be carried out with subsequences, enabling efficient \ninference over long sequences of observations. We formulate this forward \nfiltering process in the setting of the random dynamical system and there exist \nLyapunov exponents in the i.i.d random matrices production. And the rate of the \nmemory decay is known as $\\lambda_2-\\lambda_1$, the gap of the top two Lyapunov \nexponents almost surely. An efficient and accurate algorithm is proposed to \nnumerically estimate the gap after the soft-max parametrization. The length of \nsubsequences $B$ given the controlled error $\\epsilon$ is \n$B=\\log(\\epsilon)/(\\lambda_2-\\lambda_1)$. We theoretically prove the validity \nof the algorithm and demonstrate the effectiveness with numerical examples. The \nmethod developed here can be applied to widely used algorithms, such as \nmini-batch stochastic gradient method. Moreover, the continuity of Lyapunov \nspectrum ensures the estimated $B$ could be reused for the nearby parameter \nduring the inference. \n</p>"}, "author": "Felix X.-F. Ye, Yi-an Ma, Hong Qian", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715967", "id": "tag:google.com,2005:reader/item/0000000321219c18", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Discovering Adversarial Examples with Momentum. (arXiv:1710.06081v1 [cs.LG])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06081"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06081", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9ae60fb\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9ae60fb&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Machine learning models, especially Deep Neural Networks, are vulnerable to \nadversarial examples---malicious inputs crafted by adding small noises to real \nexamples, but fool the models. Adversarial examples transfer from one model to \nanother, enabling black-box attacks to real-world applications. In this paper, \nwe propose a strong attack algorithm named momentum iterative fast gradient \nsign method (MI-FGSM) to discover adversarial examples. MI-FGSM is an extension \nof iterative fast gradient sign method (I-FGSM) but improves the \ntransferability significantly. Besides, we study how to attack an ensemble of \nmodels efficiently. Experiments demonstrate the effectiveness of the proposed \nalgorithm. We hope that MI-FGSM can serve as a benchmark attack algorithm for \nevaluating the robustness of various models and defense methods. \n</p>"}, "author": "Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Xiaolin Hu, Jun Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715966", "id": "tag:google.com,2005:reader/item/0000000321219c22", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the challenges of learning with inference networks on sparse, high-dimensional data. (arXiv:1710.06085v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06085"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06085", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study parameter estimation in Nonlinear Factor Analysis (NFA) where the \ngenerative model is parameterized by a deep neural network. Recent work has \nfocused on learning such models using inference (or recognition) networks; we \nidentify a crucial problem when modeling large, sparse, high-dimensional \ndatasets -- underfitting. We study the extent of underfitting, highlighting \nthat its severity increases with the sparsity of the data. We propose methods \nto tackle it via iterative optimization inspired by stochastic variational \ninference \\citep{hoffman2013stochastic} and improvements in the sparse data \nrepresentation used for inference. The proposed techniques drastically improve \nthe ability of these powerful models to fit sparse data, achieving \nstate-of-the-art results on a benchmark text-count dataset and excellent \nresults on the task of top-N recommendation. \n</p>"}, "author": "Rahul G. Krishnan, Dawen Liang, Matthew Hoffman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715965", "id": "tag:google.com,2005:reader/item/0000000321219c35", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Detecting Bias in Black-Box Models Using Transparent Model Distillation. (arXiv:1710.06169v2 [stat.ML] UPDATED)", "published": 1511308877, "updated": 1511308892, "canonical": [{"href": "http://arxiv.org/abs/1710.06169"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06169", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Black-box risk scoring models permeate our lives, yet are typically \nproprietary and opaque. We propose a transparent model distillation approach to \ndetect bias in such models. Model distillation was originally designed to \ndistill knowledge from a large, complex teacher model to a faster, simpler \nstudent model without significant loss in prediction accuracy. We add a third \nrestriction - transparency. In this paper we use data sets that contain two \nlabels to train on: the risk score predicted by a black-box model, as well as \nthe actual outcome the risk score was intended to predict. This allows us to \ncompare models that predict each label. For a particular class of student \nmodels - interpretable tree additive models with pairwise interactions (GA2Ms) \n- we provide confidence intervals for the difference between the risk score and \nactual outcome models. This presents a new method for detecting bias in \nblack-box risk scores by assessing if contributions of protected features to \nthe risk score are statistically different from their contributions to the \nactual outcome. \n</p>"}, "author": "Sarah Tan, Rich Caruana, Giles Hooker, Yin Lou", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715964", "id": "tag:google.com,2005:reader/item/0000000321219c52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Gaussian Covariance Network. (arXiv:1710.06202v2 [cs.LG] UPDATED)", "published": 1509323746, "updated": 1509323757, "canonical": [{"href": "http://arxiv.org/abs/1710.06202"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06202", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The correlation length-scale next to the noise variance are the most used \nhyperparameters for the Gaussian processes. Typically, stationary covariance \nfunctions are used, which are only dependent on the distances between input \npoints and thus invariant to the translations in the input space. The \noptimization of the hyperparameters is commonly done by maximizing the log \nmarginal likelihood. This works quite well, if the distances are uniform \ndistributed. In the case of a locally adapted or even sparse input space, the \nprediction of a test point can be worse dependent of its position. A possible \nsolution to this, is the usage of a non-stationary covariance function, where \nthe hyperparameters are calculated by a deep neural network. So that the \ncorrelation length scales and possibly the noise variance are dependent on the \ntest point. Furthermore, different types of covariance functions are trained \nsimultaneously, so that the Gaussian process prediction is an additive overlay \nof different covariance matrices. The right covariance functions combination \nand its hyperparameters are learned by the deep neural network. Additional, the \nGaussian process will be able to be trained by batches or online and so it can \nhandle arbitrarily large data sets. We call this framework Deep Gaussian \nCovariance Network (DGCP). There are also further extensions to this framework \npossible, for example sequentially dependent problems like time series or the \nlocal mixture of experts. The basic framework and some extension possibilities \nwill be presented in this work. Moreover, a comparison to some recent state of \nthe art surrogate model methods will be performed, also for a time dependent \nproblem. \n</p>"}, "author": "Kevin Cremanns, Dirk Roos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715963", "id": "tag:google.com,2005:reader/item/0000000321219c62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning to Transfer Initializations for Bayesian Hyperparameter Optimization. (arXiv:1710.06219v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06219"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06219", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Hyperparameter optimization undergoes extensive evaluations of validation \nerrors in order to find the best configuration of hyperparameters. Bayesian \noptimization is now popular for hyperparameter optimization, since it reduces \nthe number of validation error evaluations required. Suppose that we are given \na collection of datasets on which hyperparameters are already tuned by either \nhumans with domain expertise or extensive trials of cross-validation. When a \nmodel is applied to a new dataset, it is desirable to let Bayesian \nhyperparameter optimzation start from configurations that were successful on \nsimilar datasets. To this end, we construct a Siamese network with \nconvolutional layers followed by bi-directional LSTM layers, to learn {\\em \nmeta-features} over datasets. Learned meta-features are used to select a few \ndatasets that are similar to the new dataset, so that a set of configurations \nin similar datasets is adopted as initializations for Bayesian hyperparameter \noptimization. Experiments on image datasets demonstrate that our learned \nmeta-features are useful in optimizing several hyperparameters in deep residual \nnetworks for image classification. \n</p>"}, "author": "Jungtaek Kim, Saehoon Kim, Seungjin Choi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715962", "id": "tag:google.com,2005:reader/item/0000000321219c6b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Nonlinear Interference Mitigation via Deep Neural Networks. (arXiv:1710.06234v1 [cs.IT])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06234"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06234", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A neural-network-based approach is presented to efficiently implement digital \nbackpropagation (DBP). For a 32x100 km fiber-optic link, the resulting \n\"learned\" DBP significantly reduces the complexity compared to conventional DBP \nimplementations. \n</p>"}, "author": "Christian H&#xe4;ger, Henry D. Pfister", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715961", "id": "tag:google.com,2005:reader/item/0000000321219c7b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convergence Rate of Riemannian Hamiltonian Monte Carlo and Faster Polytope Volume Computation. (arXiv:1710.06261v1 [cs.DS])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06261"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06261", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We give the first rigorous proof of the convergence of Riemannian Hamiltonian \nMonte Carlo, a general (and practical) method for sampling Gibbs distributions. \nOur analysis shows that the rate of convergence is bounded in terms of natural \nsmoothness parameters of an associated Riemannian manifold. We then apply the \nmethod with the manifold defined by the log barrier function to the problems of \n(1) uniformly sampling a polytope and (2) computing its volume, the latter by \nextending Gaussian cooling to the manifold setting. In both cases, the total \nnumber of steps needed is O^{*}(mn^{\\frac{2}{3}}), improving the state of the \nart. A key ingredient of our analysis is a proof of an analog of the KLS \nconjecture for Gibbs distributions over manifolds. \n</p>"}, "author": "Yin Tat Lee, Santosh S. Vempala", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715960", "id": "tag:google.com,2005:reader/item/0000000321219c88", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Combinatorial Penalties: Which structures are preserved by convex relaxations?. (arXiv:1710.06273v1 [cs.LG])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06273"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06273", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the homogeneous and the non-homogeneous convex relaxations for \ncombinatorial penalty functions defined on support sets. Our study identifies \nkey differences in the tightness of the resulting relaxations through the \nnotion of the lower combinatorial envelope of a set-function along with new \nnecessary conditions for support identification. We then propose a general \nadaptive estimator for convex monotone regularizers, and derive new sufficient \nconditions for support recovery in the asymptotic setting. \n</p>"}, "author": "Marwa El Halabi, Francis Bach, Volkan Cevher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715959", "id": "tag:google.com,2005:reader/item/0000000321219c91", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Smooth and Sparse Optimal Transport. (arXiv:1710.06276v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06276"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06276", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Entropic regularization is quickly emerging as a new standard in optimal \ntransport (OT). It enables to cast the OT computation as a differentiable and \nunconstrained convex optimization problem, which can be efficiently solved \nusing the Sinkhorn algorithm. However, the entropy term keeps the \ntransportation plan strictly positive and therefore completely dense, unlike \nunregularized OT. This lack of sparsity can be problematic for applications \nwhere the transportation plan itself is of interest. In this paper, we explore \nregularizing both the primal and dual original formulations with an arbitrary \nstrongly convex term. We show that this corresponds to relaxing dual and primal \nconstraints with approximate smooth constraints. We show how to incorporate \nsquared 2-norm and group lasso regularizations within that framework, leading \nto sparse and group-sparse transportation plans. On the theoretical side, we \nare able to bound the approximation error introduced by smoothing the original \nprimal and dual formulations. Our results suggest that, for the smoothed dual, \nthe approximation error can often be smaller with squared 2-norm regularization \nthan with entropic regularization. We showcase our proposed framework on the \ntask of color transfer. \n</p>"}, "author": "Mathieu Blondel, Vivien Seguy, Antoine Rolet", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715958", "id": "tag:google.com,2005:reader/item/0000000321219ca8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Good Arm Identification via Bandit Feedback. (arXiv:1710.06360v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06360"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06360", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9ae64f9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9ae64f9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this paper, we consider and discuss a new stochastic multi-armed bandit \nproblem called {\\em good arm identification} (GAI), where a good arm is an arm \nwith expected reward greater than or equal to a given threshold. GAI is a \npure-exploration problem that an agent repeats a process of outputting an arm \nas soon as it is identified as a good one before confirming the other arms are \nactually not good. The objective of GAI is to minimize the number of samples \nfor each process. We find that GAI faces a new kind of dilemma, the {\\em \nexploration-exploitation dilemma of confidence}, while best arm identification \ndoes not. Therefore, GAI is not just an extension of the best arm \nidentification. Actually, an efficient design of algorithms for GAI is quite \ndifferent from that for best arm identification. We derive a lower bound on the \nsample complexity for GAI and develop an algorithm whose sample complexity \nalmost matches the lower bound. We also confirm experimentally that the \nproposed algorithm outperforms a naive algorithm and a thresholding-bandit-like \nalgorithm in synthetic settings and in settings based on medical data. \n</p>"}, "author": "Hideaki Kano, Junya Honda, Kentaro Sakamaki, Kentaro Matsuura, Atsuyoshi Nakamura, Masashi Sugiyama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508313918716", "timestampUsec": "1508313918715957", "id": "tag:google.com,2005:reader/item/0000000321219cc6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Convergence diagnostics for stochastic gradient descent with constant step size. (arXiv:1710.06382v1 [stat.ML])", "published": 1508313918, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06382"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06382", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Iterative procedures in stochastic optimization are typically comprised of a \ntransient phase and a stationary phase. During the transient phase the \nprocedure converges towards a region of interest, and during the stationary \nphase the procedure oscillates in a convergence region, commonly around a \nsingle point. In this paper, we develop a statistical diagnostic test to detect \nsuch phase transition in the context of stochastic gradient descent with \nconstant step size. We present theoretical and experimental results suggesting \nthat the diagnostic behaves as intended, and the region where the diagnostic is \nactivated coincides with the convergence region. For a class of loss functions, \nwe derive a closed-form solution describing such region, and support this \ntheoretical result with simulated experiments. Finally, we suggest an \napplication to speed up convergence of stochastic gradient descent by halving \nthe learning rate each time convergence is detected. This leads to remarkable \nspeed gains that are empirically comparable to state-of-art procedures. \n</p>"}, "author": "Jerry Chee, Panos Toulis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508289709375", "timestampUsec": "1508289709374727", "id": "tag:google.com,2005:reader/item/0000000320f681e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Searching for Activation Functions. (arXiv:1710.05941v2 [cs.NE] UPDATED)", "published": 1509325338, "updated": 1509325338, "canonical": [{"href": "http://arxiv.org/abs/1710.05941"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05941", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The choice of activation functions in deep networks has a significant effect \non the training dynamics and task performance. Currently, the most successful \nand widely-used activation function is the Rectified Linear Unit (ReLU). \nAlthough various hand-designed alternatives to ReLU have been proposed, none \nhave managed to replace it due to inconsistent gains. In this work, we propose \nto leverage automatic search techniques to discover new activation functions. \nUsing a combination of exhaustive and reinforcement learning-based search, we \ndiscover multiple novel activation functions. We verify the effectiveness of \nthe searches by conducting an empirical evaluation with the best discovered \nactivation function. Our experiments show that the best discovered activation \nfunction, $f(x) = x \\cdot \\text{sigmoid}(\\beta x)$, which we name Swish, tends \nto work better than ReLU on deeper models across a number of challenging \ndatasets. For example, simply replacing ReLUs with Swish units improves top-1 \nclassification accuracy on ImageNet by 0.9\\% for Mobile NASNet-A and 0.6\\% for \nInception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it \neasy for practitioners to replace ReLUs with Swish units in any neural network. \n</p>"}, "author": "Prajit Ramachandran, Barret Zoph, Quoc V. Le", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508289709375", "timestampUsec": "1508289709374726", "id": "tag:google.com,2005:reader/item/0000000320f681ec", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Evolution in Virtual Worlds. (arXiv:1710.06055v1 [cs.NE])", "published": 1508289710, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.06055"}], "alternate": [{"href": "http://arxiv.org/abs/1710.06055", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This chapter discusses the possibility of instilling a virtual world with \nmechanisms for evolution and natural selection in order to generate rich \necosystems of complex organisms in a process akin to biological evolution. Some \nprevious work in the area is described, and successes and failures are \ndiscussed. The components of a more comprehensive framework for designing such \nworlds are mapped out, including the design of the individual organisms, the \nproperties and dynamics of the environmental medium in which they are evolving, \nand the representational relationship between organism and environment. Some of \nthe key issues discussed include how to allow organisms to evolve new \nstructures and functions with few restrictions, and how to create an \ninterconnectedness between organisms in order to generate drives for continuing \nevolutionary activity. \n</p>"}, "author": "Tim Taylor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893785", "id": "tag:google.com,2005:reader/item/00000003208f6155", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Functional Decision Theory: A New Theory of Instrumental Rationality. (arXiv:1710.05060v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05060"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05060", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper describes and motivates a new decision theory known as functional \ndecision theory (FDT), as distinct from causal decision theory and evidential \ndecision theory. Functional decision theorists hold that the normative \nprinciple for action is to treat one's decision as the output of a fixed \nmathematical function that answers the question, \"Which output of this very \nfunction would yield the best outcome?\" Adhering to this principle delivers a \nnumber of benefits, including the ability to maximize wealth in an array of \ntraditional decision-theoretic and game-theoretic problems where CDT and EDT \nperform poorly. Using one simple and coherent decision rule, functional \ndecision theorists (for example) achieve more utility than CDT on Newcomb's \nproblem, more utility than EDT on the smoking lesion problem, and more utility \nthan both in Parfit's hitchhiker problem. In this paper, we define FDT, explore \nits prescriptions in a number of different decision problems, compare it to CDT \nand EDT, and give philosophical justifications for FDT as a normative theory of \ndecision-making. \n</p>"}, "author": "Eliezer Yudkowsky, Nate Soares", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893784", "id": "tag:google.com,2005:reader/item/00000003208f615e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Ontological Modeling of Trees. (arXiv:1710.05096v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05096"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05096", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Trees -- i.e., the type of data structure known under this name -- are \ncentral to many aspects of knowledge organization. We investigate some central \ndesign choices concerning the ontological modeling of such trees. In \nparticular, we consider the limits of what is expressible in the Web Ontology \nLanguage, and provide a reusable ontology design pattern for trees. \n</p>"}, "author": "David Carral, Pascal Hitzler, Hilmar Lapp, Sebastian Rudolph", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893783", "id": "tag:google.com,2005:reader/item/00000003208f6168", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Community Aware Random Walk for Network Embedding. (arXiv:1710.05199v1 [cs.SI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05199"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05199", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Social network analysis provides meaningful information about behavior of \nnetwork members that can be used in diverse applications such as \nclassification, link prediction, etc. however, network analysis is \ncomputationally expensive because of feature learning for different \napplications. In recent years, many researches have focused on feature learning \nmethods in social networks. Network embedding represents the network in a lower \ndimensional representation space with the same properties which presents a \ncompressed representation of the input network. In this paper, we introduce a \nnovel algorithm named \"CARE\" for network embedding that can be used for \ndifferent types of networks including weighted, directed and complex. While \ncurrent methods try to preserve local neighborhood information of nodes, we \nutilize local neighborhood and community information of network nodes to cover \nboth local and global structure of social networks. CARE builds customized \npaths, which are consisted of local and global structure of network nodes, as a \nbasis for network embedding and uses skip-gram model to learn representation \nvector of nodes. Then, stochastic gradient descent is used to optimize our \nobjective function and learn the final representation of nodes. Our method can \nbe scalable when new nodes are appended to network without information loss. \nParallelize generation of customized random walks is also used for speeding up \nCARE. We evaluate the performance of CARE on multi label classification and \nlink prediction tasks. Experimental results on different networks indicate that \nthe proposed method outperforms others in both Micro-f1 and Macro-f1 measures \nfor different size of training data. \n</p>"}, "author": "Mohammad Mehdi Keikha, Maseud Rahgozar, Masoud Asadpour", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893782", "id": "tag:google.com,2005:reader/item/00000003208f6178", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Network Model Selection Using Task-Focused Minimum Description Length. (arXiv:1710.05207v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05207"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05207", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Networks are fundamental models for data used in practically every \napplication domain. In most instances, several implicit or explicit choices \nabout the network definition impact the translation of underlying data to a \nnetwork representation, and the subsequent question(s) about the underlying \nsystem being represented. Users of downstream network data may not even be \naware of these choices or their impacts. We propose a task-focused network \nmodel selection methodology which addresses several key challenges. Our \napproach constructs network models from underlying data and uses minimum \ndescription length (MDL) criteria for selection. Our methodology measures \nefficiency, a general and comparable measure of the network's performance of a \nlocal (i.e. node-level) predictive task of interest. Selection on efficiency \nfavors parsimonious (e.g. sparse) models to avoid overfitting and can be \napplied across arbitrary tasks and representations (including networks and \nnon-network models). We show stability, sensitivity, and significance testing \nin our methodology. \n</p>"}, "author": "Ivan Brugere, Tanya Y. Berger-Wolf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893781", "id": "tag:google.com,2005:reader/item/00000003208f6181", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Mental Sampling in Multimodal Representations. (arXiv:1710.05219v1 [cs.LG])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05219"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05219", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Both resources in the natural environment and concepts in a semantic space \nare distributed \"patchily\", with large gaps in between the patches. To describe \npeople's internal and external foraging behavior, various random walk models \nhave been proposed. In particular, internal foraging has been modeled as \nsampling: in order to gather relevant information for making a decision, people \ndraw samples from a mental representation using random-walk algorithms such as \nMarkov chain Monte Carlo (MCMC). However, two common empirical observations \nargue against simple sampling algorithms such as MCMC. First, the spatial \nstructure is often best described by a L\\'evy flight distribution: the \nprobability of the distance between two successive locations follows a \npower-law on the distances. Second, the temporal structure of the sampling that \nhumans and other animals produce have long-range, slowly decaying serial \ncorrelations characterized as $1/f$-like fluctuations. We propose that mental \nsampling is not done by simple MCMC, but is instead adapted to multimodal \nrepresentations and is implemented by Metropolis-coupled Markov chain Monte \nCarlo (MC$^3$), one of the first algorithms developed for sampling from \nmultimodal distributions. MC$^3$ involves running multiple Markov chains in \nparallel but with target distributions of different temperatures, and it swaps \nthe states of the chains whenever a better location is found. Heated chains \nmore readily traverse valleys in the probability landscape to propose moves to \nfar-away peaks, while the colder chains make the local steps that explore the \ncurrent peak or patch. We show that MC$^3$ generates distances between \nsuccessive samples that follow a L\\'evy flight distribution and $1/f$-like \nserial correlations, providing a single mechanistic account of these two \npuzzling empirical phenomena. \n</p>"}, "author": "Jian-Qiao Zhu, Adam N. Sanborn, Nick Chater", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893780", "id": "tag:google.com,2005:reader/item/00000003208f619a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learners that Leak Little Information. (arXiv:1710.05233v1 [cs.LG])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05233"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05233", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study learning algorithms that are restricted to revealing little \ninformation about their input sample. Various manifestations of this notion \nhave been recently studied. A central theme in these works, and in ours, is \nthat such algorithms generalize. We study a category of learning algorithms, \nwhich we term d-bit information learners}. These are algorithms whose output \nconveys at most d bits of information on their input. \n</p> \n<p>We focus on the learning capacity of such algorithms: we prove generalization \nbounds with tight dependencies on the confidence and error parameters. We \nobserve connections with well studied notions such as PAC-Bayes and \ndifferential privacy. For example, it is known that pure differentially private \nalgorithms leak little information. We complement this fact with a separation \nbetween bounded information and pure differential privacy in the setting of \nproper learning, showing that differential privacy is strictly more \nrestrictive. \n</p> \n<p>We also demonstrate limitations by exhibiting simple concept classes for \nwhich every (possibly randomized) empirical risk minimizer must leak a lot of \ninformation. On the other hand, we show that in the distribution-dependent \nsetting every VC class has empirical risk minimizers that do not leak a lot of \ninformation. \n</p>"}, "author": "Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, Amir Yehudayoff", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893779", "id": "tag:google.com,2005:reader/item/00000003208f61a5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On Hashing-Based Approaches to Approximate DNF-Counting. (arXiv:1710.05247v1 [cs.LO])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05247"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05247", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9ae686c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9ae686c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Propositional model counting is a fundamental problem in artificial \nintelligence with a wide variety of applications, such as probabilistic \ninference, decision making under uncertainty, and probabilistic databases. \nConsequently, the problem is of theoretical as well as practical interest. When \nthe constraints are expressed as DNF formulas, Monte Carlo-based techniques \nhave been shown to provide a fully polynomial randomized approximation scheme \n(FPRAS). For CNF constraints, hashing-based approximation techniques have been \ndemonstrated to be highly successful. Furthermore, it was shown that \nhashing-based techniques also yield an FPRAS for DNF counting without usage of \nMonte Carlo sampling. Our analysis, however, shows that the proposed \nhashing-based approach to DNF counting provides poor time complexity compared \nto the Monte Carlo-based DNF counting techniques. Given the success of \nhashing-based techniques for CNF constraints, it is natural to ask: Can \nhashing-based techniques provide an efficient FPRAS for DNF counting? In this \npaper, we provide a positive answer to this question. To this end, we introduce \ntwo novel algorithmic techniques: \\emph{Symbolic Hashing} and \\emph{Stochastic \nCell Counting}, along with a new hash family of \\emph{Row-Echelon hash \nfunctions}. These innovations allow us to design a hashing-based FPRAS for DNF \ncounting of similar complexity (up to polylog factors) as that of prior works. \nFurthermore, we expect these techniques to have potential applications beyond \nDNF counting. \n</p>"}, "author": "Kuldeep S. Meel (1), Aditya A. Shrotri (2), Moshe Y. Vardi (2) ((1) National University of Singapore, (2) Rice University)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893778", "id": "tag:google.com,2005:reader/item/00000003208f61bb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-Value Rule Sets. (arXiv:1710.05257v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05257"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05257", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9b98714\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9b98714&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We present the Multi-vAlue Rule Set (MARS) model for interpretable \nclassification with feature efficient presentations. MARS introduces a more \ngeneralized form of association rules that allows multiple values in a \ncondition. Rules of this form are more concise than traditional single-valued \nrules in capturing and describing patterns in data. MARS mitigates the problem \nof dealing with continuous features and high-cardinality categorical features \nfaced by rule-based models. Our formulation also pursues a higher efficiency of \nfeature utilization, which reduces the cognitive load to understand the \ndecision process. We propose an efficient inference method for learning a \nmaximum a posteriori model, incorporating theoretically grounded bounds to \niteratively reduce the search space to improve search efficiency. Experiments \nwith synthetic and real-world data demonstrate that MARS models have \nsignificantly smaller complexity and fewer features, providing better \ninterpretability while being competitive in predictive accuracy. We conducted a \nusability study with human subjects and results show that MARS is the easiest \nto use compared with other competing rule-based models, in terms of the correct \nrate and response time. Overall, MARS introduces a new approach to rule-based \nmodels that balance accuracy and interpretability with feature-efficient \nrepresentations. \n</p>"}, "author": "Tong Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893777", "id": "tag:google.com,2005:reader/item/00000003208f61c5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Infinite RBMs with Frank-Wolfe. (arXiv:1710.05270v1 [cs.LG])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05270"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05270", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we propose an infinite restricted Boltzmann machine~(RBM), \nwhose maximum likelihood estimation~(MLE) corresponds to a constrained convex \noptimization. We consider the Frank-Wolfe algorithm to solve the program, which \nprovides a sparse solution that can be interpreted as inserting a hidden unit \nat each iteration, so that the optimization process takes the form of a \nsequence of finite models of increasing complexity. As a side benefit, this can \nbe used to easily and efficiently identify an appropriate number of hidden \nunits during the optimization. The resulting model can also be used as an \ninitialization for typical state-of-the-art RBM training algorithms such as \ncontrastive divergence, leading to models with consistently higher test \nlikelihood than random initialization. \n</p>"}, "author": "Wei Ping, Qiang Liu, Alexander Ihler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893776", "id": "tag:google.com,2005:reader/item/00000003208f61e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Complete Extensions do not form a Complete Semilattice. (arXiv:1710.05341v2 [cs.AI] UPDATED)", "published": 1509064764, "updated": 1509064765, "canonical": [{"href": "http://arxiv.org/abs/1710.05341"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05341", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In his seminal paper that inaugurated abstract argumentation, Dung proved \nthat the set of complete extensions forms a complete semilattice with respect \nto set inclusion. In this note we demonstrate that this proof is incorrect with \ncounterexamples. We then trace the error in the proof and explain why it arose. \nWe then examine the implications for the grounded extension. \n</p> \n<p>[Reason for withdrawal continued] Page 4, Example 2 is not a counterexample \nto Dung 1995 Theorem 25(3). It was believed to be a counter-example because the \nauthor misunderstood ``glb'' to be set-theoretic intersection. But in this \ncase, ``glb'' is defined to be other than set-theoretic intersection such that \nTheorem 25(3) is true. \n</p> \n<p>The author was motivated to fully understand the lattice-theoretic claims of \nDung 1995 in writing this note and was not aware that this issue is probably \nfolklore; the author bears full responsibility for this error. \n</p>"}, "author": "Anthony P. Young", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893775", "id": "tag:google.com,2005:reader/item/00000003208f61f7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A systematic study of the class imbalance problem in convolutional neural networks. (arXiv:1710.05381v1 [cs.CV])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05381"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05381", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this study, we systematically investigate the impact of class imbalance on \nclassification performance of convolutional neural networks (CNNs) and compare \nfrequently used methods to address the issue. Class imbalance is a common \nproblem that has been comprehensively studied in classical machine learning, \nyet very limited systematic research is available in the context of deep \nlearning. In our study, we use three benchmark datasets of increasing \ncomplexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of \nimbalance on classification and perform an extensive comparison of several \nmethods to address the issue: oversampling, undersampling, two-phase training, \nand thresholding that compensates for prior class probabilities. Our main \nevaluation metric is area under the receiver operating characteristic curve \n(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is \nassociated with notable difficulties in the context of imbalanced data. Based \non results from our experiments we conclude that (i) the effect of class \nimbalance on classification performance is detrimental; (ii) the method of \naddressing class imbalance that emerged as dominant in almost all analyzed \nscenarios was oversampling; (iii) oversampling should be applied to the level \nthat totally eliminates the imbalance, whereas undersampling can perform better \nwhen the imbalance is only removed to some extent; (iv) as opposed to some \nclassical machine learning models, oversampling does not necessarily cause \noverfitting of CNNs; (v) thresholding should be applied to compensate for prior \nclass probabilities when overall number of properly classified cases is of \ninterest. \n</p>"}, "author": "Mateusz Buda, Atsuto Maki, Maciej A. Mazurowski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893774", "id": "tag:google.com,2005:reader/item/00000003208f620c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Manifold Regularization for Kernelized LSTD. (arXiv:1710.05387v1 [cs.LG])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05387"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05387", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Policy evaluation or value function or Q-function approximation is a key \nprocedure in reinforcement learning (RL). It is a necessary component of policy \niteration and can be used for variance reduction in policy gradient methods. \nTherefore its quality has a significant impact on most RL algorithms. Motivated \nby manifold regularized learning, we propose a novel kernelized policy \nevaluation method that takes advantage of the intrinsic geometry of the state \nspace learned from data, in order to achieve better sample efficiency and \nhigher accuracy in Q-function approximation. Applying the proposed method in \nthe Least-Squares Policy Iteration (LSPI) framework, we observe superior \nperformance compared to widely used parametric basis functions on two standard \nbenchmarks in terms of policy quality. \n</p>"}, "author": "Xinyan Yan, Krzysztof Choromanski, Byron Boots, Vikas Sindhwani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893773", "id": "tag:google.com,2005:reader/item/00000003208f6224", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Causal Rule Sets for Identifying Subgroups with Enhanced Treatment Effect. (arXiv:1710.05426v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05426"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05426", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a novel generative model for interpretable subgroup analysis for \ncausal inference applications, Causal Rule Sets (CRS). A CRS model uses a small \nset of short rules to capture a subgroup where the average treatment effect is \nelevated compared to the entire population. We present a Bayesian framework for \nlearning a causal rule set. The Bayesian framework consists of a prior that \nfavors simpler models and a Bayesian logistic regression that characterizes the \nrelation between outcomes, attributes and subgroup membership. We find maximum \na posteriori models using discrete Monte Carlo steps in the joint solution \nspace of rules sets and parameters. We provide theoretically grounded \nheuristics and bounding strategies to improve search efficiency. Experiments \nshow that the search algorithm can efficiently recover a true underlying \nsubgroup and CRS shows consistently competitive performance compared to other \nstate-of-the-art baseline methods. \n</p>"}, "author": "Tong Wang, Cynthia Rudin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893772", "id": "tag:google.com,2005:reader/item/00000003208f6237", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Flow: Architecture and Benchmarking for Reinforcement Learning in Traffic Control. (arXiv:1710.05465v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05465"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05465", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Flow is a new computational framework, built to support a key need triggered \nby the rapid growth of autonomy in ground traffic: controllers for autonomous \nvehicles in the presence of complex nonlinear dynamics in traffic. Leveraging \nrecent advances in deep Reinforcement Learning (RL), Flow enables the use of RL \nmethods such as policy gradient for traffic control and enables benchmarking \nthe performance of classical (including hand-designed) controllers with learned \npolicies (control laws). Flow integrates traffic microsimulator SUMO with deep \nreinforcement learning library rllab and enables the easy design of traffic \ntasks, including different networks configurations and vehicle dynamics. We use \nFlow to develop reliable controllers for complex problems, such as controlling \nmixed-autonomy traffic (involving both autonomous and human-driven vehicles) in \na ring road. For this, we first show that state-of-the-art hand-designed \ncontrollers excel when in-distribution, but fail to generalize; then, we show \nthat even simple neural network policies can solve the stabilization task \nacross density settings and generalize to out-of-distribution settings. \n</p>"}, "author": "Cathy Wu, Aboudy Kreidieh, Kanaad Parvate, Eugene Vinitsky, Alexandre M Bayen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893771", "id": "tag:google.com,2005:reader/item/00000003208f623e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalization in Deep Learning. (arXiv:1710.05468v1 [stat.ML])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05468"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05468", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper explains why deep learning can generalize well, despite large \ncapacity and possible algorithmic instability, nonrobustness, and sharp minima, \neffectively addressing an open problem in the literature. Based on our \ntheoretical insight, this paper also proposes a family of new regularization \nmethods. Its simplest member was empirically shown to improve base models and \nachieve state-of-the-art performance on MNIST and CIFAR-10 benchmarks. \nMoreover, this paper presents both data-dependent and data-independent \ngeneralization guarantees with improved convergence rates. Our results suggest \nseveral new open areas of research. \n</p>"}, "author": "Kenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893770", "id": "tag:google.com,2005:reader/item/00000003208f6244", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Toward Crowd-Sensitive Path Planning. (arXiv:1710.05503v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05503"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05503", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>If a robot can predict crowds in parts of its environment that are \ninaccessible to its sensors, then it can plan to avoid them. This paper \nproposes a fast, online algorithm that learns average crowd densities in \ndifferent areas. It also describes how these densities can be incorporated into \nexisting navigation architectures. In simulation across multiple challenging \ncrowd scenarios, the robot reaches its target faster, travels less, and risks \nfewer collisions than if it were to plan with the traditional A* algorithm. \n</p>"}, "author": "Anoop Aroor, Susan L. Epstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893769", "id": "tag:google.com,2005:reader/item/00000003208f624d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Intention-Net: Integrating Planning and Deep Learning for Goal-Directed Autonomous Navigation. (arXiv:1710.05627v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05627"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05627", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9b98a0d\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9b98a0d&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>How can a delivery robot navigate reliably to a destination in a new office \nbuilding, with minimal prior information? To tackle this challenge, this paper \nintroduces a two-level hierarchical approach, which integrates model-free deep \nlearning and model-based path planning. At the low level, a neural-network \nmotion controller, called the intention-net, is trained end-to-end to provide \nrobust local navigation. The intention-net maps images from a single monocular \ncamera and \"intentions\" directly to robot controls. At the high level, a path \nplanner uses a crude map, e.g., a 2-D floor plan, to compute a path from the \nrobot's current location to the goal. The planned path provides intentions to \nthe intention-net. Preliminary experiments suggest that the learned motion \ncontroller is robust against perceptual uncertainty and by integrating with a \npath planner, it generalizes effectively to new environments and goals. \n</p>"}, "author": "Gao Wei, David Hus, Wee Sun Lee, Shengmei Shen, Karthikk Subramanian", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893768", "id": "tag:google.com,2005:reader/item/00000003208f625e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Mining Frequent Patterns in Process Models. (arXiv:1710.05693v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05693"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05693", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Process mining has emerged as a way to analyze the behavior of an \norganization by extracting knowledge from event logs and by offering techniques \nto discover, monitor and enhance real processes. In the discovery of process \nmodels, retrieving a complex one, i.e., a hardly readable process model, can \nhinder the extraction of information. Even in well-structured process models, \nthere is information that cannot be obtained with the current techniques. In \nthis paper, we present WoMine, an algorithm to retrieve frequent behavioural \npatterns from the model. Our approach searches in process models extracting \nstructures with sequences, selections, parallels and loops, which are \nfrequently executed in the logs. This proposal has been validated with a set of \nprocess models, including some from BPI Challenges, and compared with the state \nof the art techniques. Experiments have validated that WoMine can find all \ntypes of patterns, extracting information that cannot be mined with the state \nof the art techniques. \n</p>"}, "author": "David Chapela-Campa, Manuel Mucientes, Manuel Lama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893767", "id": "tag:google.com,2005:reader/item/00000003208f626e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Survey on Optical Character Recognition System. (arXiv:1710.05703v1 [cs.CV])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05703"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05703", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Optical Character Recognition (OCR) has been a topic of interest for many \nyears. It is defined as the process of digitizing a document image into its \nconstituent characters. Despite decades of intense research, developing OCR \nwith capabilities comparable to that of human still remains an open challenge. \nDue to this challenging nature, researchers from industry and academic circles \nhave directed their attentions towards Optical Character Recognition. Over the \nlast few years, the number of academic laboratories and companies involved in \nresearch on Character Recognition has increased dramatically. This research \naims at summarizing the research so far done in the field of OCR. It provides \nan overview of different aspects of OCR and discusses corresponding proposals \naimed at resolving issues of OCR. \n</p>"}, "author": "Noman Islam, Zeeshan Islam, Nazia Noor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893766", "id": "tag:google.com,2005:reader/item/00000003208f627f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "ACCBench: A Framework for Comparing Causality Algorithms. (arXiv:1710.05720v1 [cs.AI])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05720"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05720", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Modern socio-technical systems are increasingly complex. A fundamental \nproblem is that the borders of such systems are often not well-defined \na-priori, which among other problems can lead to unwanted behavior during \nruntime. Ideally, unwanted behavior should be prevented. If this is not \npossible the system shall at least be able to help determine potential cause(s) \na-posterori, identify responsible parties and make them accountable for their \nbehavior. Recently, several algorithms addressing these concepts have been \nproposed. However, the applicability of the corresponding approaches, \nspecifically their effectiveness and performance, is mostly unknown. Therefore, \nin this paper, we propose ACCBench, a benchmark tool that allows to compare and \nevaluate causality algorithms under a consistent setting. Furthermore, we \ncontribute an implementation of the two causality algorithms by G\\\"o{\\ss}ler \nand Metayer and G\\\"o{\\ss}ler and Astefanoaei as well as of a policy compliance \napproach based on some concepts of Main et al. Lastly, we conduct a case study \nof an Intelligent Door Control System, which exposes concrete strengths and \nweaknesses of all algorithms under different aspects. In the course of this, we \nshow that the effectiveness of the algorithms in terms of cause detection as \nwell as their performance differ to some extent. In addition, our analysis \nreports on some qualitative aspects that should be considered when evaluating \neach algorithm. For example, the human effort needed to configure the algorithm \nand model the use case is analyzed. \n</p>"}, "author": "Simon Rehwald, Amjad Ibrahim, Kristian Beckers, Alexander Pretschner", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893765", "id": "tag:google.com,2005:reader/item/00000003208f62a9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Characterizing Driving Context from Driver Behavior. (arXiv:1710.05733v2 [cs.AI] UPDATED)", "published": 1511265636, "updated": 1511265658, "canonical": [{"href": "http://arxiv.org/abs/1710.05733"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05733", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Because of the increasing availability of spatiotemporal data, a variety of \ndata-analytic applications have become possible. Characterizing driving \ncontext, where context may be thought of as a combination of location and time, \nis a new challenging application. An example of such a characterization is \nfinding the correlation between driving behavior and traffic conditions. This \ncontextual information enables analysts to validate observation-based \nhypotheses about the driving of an individual. In this paper, we present \nDriveContext, a novel framework to find the characteristics of a context, by \nextracting significant driving patterns (e.g., a slow-down), and then \nidentifying the set of potential causes behind patterns (e.g., traffic \ncongestion). Our experimental results confirm the feasibility of the framework \nin identifying meaningful driving patterns, with improvements in comparison \nwith the state-of-the-art. We also demonstrate how the framework derives \ninteresting characteristics for different contexts, through real-world \nexamples. \n</p>"}, "author": "Sobhan Moosavi, Behrooz Omidvar-Tehrani, R. Bruce Craig, Arnab Nandi, Rajiv Ramnath", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893764", "id": "tag:google.com,2005:reader/item/00000003208f62b7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "A retrieval-based dialogue system utilizing utterance and context embeddings. (arXiv:1710.05780v1 [cs.CL])", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05780"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05780", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Finding semantically rich and computer-understandable representations for \ntextual dialogues, utterances and words is crucial for dialogue systems (or \nconversational agents), as their performance mostly depends on understanding \nthe context of conversations. Recent research aims at finding distributed \nvector representations (embeddings) for words, such that semantically similar \nwords are relatively close within the vector-space. Encoding the \"meaning\" of \ntext into vectors is a current trend, and text can range from words, phrases \nand documents to actual human-to-human conversations. In recent research \napproaches, responses have been generated utilizing a decoder architecture, \ngiven the vector representation of the current conversation. In this paper, the \nutilization of embeddings for answer retrieval is explored by using \nLocality-Sensitive Hashing Forest (LSH Forest), an Approximate Nearest Neighbor \n(ANN) model, to find similar conversations in a corpus and rank possible \ncandidates. Experimental results on the well-known Ubuntu Corpus (in English) \nand a customer service chat dataset (in Dutch) show that, in combination with a \ncandidate selection method, retrieval-based approaches outperform generative \nones and reveal promising future research directions towards the usability of \nsuch a system. \n</p>"}, "author": "Alexander Bartl, Gerasimos Spanakis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893757", "id": "tag:google.com,2005:reader/item/00000003208f6318", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sparse Depth Sensing for Resource-Constrained Robots. (arXiv:1703.01398v3 [cs.RO] CROSS LISTED)", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.01398"}], "alternate": [{"href": "http://arxiv.org/abs/1703.01398", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider the case in which a robot has to navigate in an unknown \nenvironment but does not have enough on-board power or payload to carry a \ntraditional depth sensor (e.g., a 3D lidar) and thus can only acquire a few \n(point-wise) depth measurements. We address the following question: is it \npossible to reconstruct the geometry of an unknown environment using sparse and \nincomplete depth measurements? Reconstruction from incomplete data is not \npossible in general, but when the robot operates in man-made environments, the \ndepth exhibits some regularity (e.g., many planar surfaces with only a few \nedges); we leverage this regularity to infer depth from a small number of \nmeasurements. Our first contribution is a formulation of the depth \nreconstruction problem that bridges robot perception with the compressive \nsensing literature in signal processing. The second contribution includes a set \nof formal results that ascertain the exactness and stability of the depth \nreconstruction in 2D and 3D problems, and completely characterize the geometry \nof the profiles that we can reconstruct. Our third contribution is a set of \npractical algorithms for depth reconstruction: our formulation directly \ntranslates into algorithms for depth estimation based on convex programming. In \nreal-world problems, these convex programs are very large and general-purpose \nsolvers are relatively slow. For this reason, we discuss ad-hoc solvers that \nenable fast depth reconstruction in real problems. The last contribution is an \nextensive experimental evaluation in 2D and 3D problems, including Monte Carlo \nruns on simulated instances and testing on multiple real datasets. Empirical \nresults confirm that the proposed approach ensures accurate depth \nreconstruction, outperforms interpolation-based strategies, and performs well \neven when the assumption of structured environment is violated. \n</p>"}, "author": "Fangchang Ma, Luca Carlone, Ulas Ayaz, Sertac Karaman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508242044894", "timestampUsec": "1508242044893756", "id": "tag:google.com,2005:reader/item/00000003208f6324", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Influence of Personal Preferences on Link Dynamics in Social Networks. (arXiv:1709.07401v1 [cs.SI] CROSS LISTED)", "published": 1508242045, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.07401"}], "alternate": [{"href": "http://arxiv.org/abs/1709.07401", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study a unique network dataset including periodic surveys and electronic \nlogs of dyadic contacts via smartphones. The participants were a sample of \nfreshmen entering university in the Fall 2011. Their opinions on a variety of \npolitical and social issues and lists of activities on campus were regularly \nrecorded at the beginning and end of each semester for the first three years of \nstudy. We identify a behavioral network defined by call and text data, and a \ncognitive network based on friendship nominations in ego-network surveys. Both \nnetworks are limited to study participants. Since a wide range of attributes on \neach node were collected in self-reports, we refer to these networks as \nattribute-rich networks. We study whether student preferences for certain \nattributes of friends can predict formation and dissolution of edges in both \nnetworks. We introduce a method for computing student preferences for different \nattributes which we use to predict link formation and dissolution. We then rank \nthese attributes according to their importance for making predictions. We find \nthat personal preferences, in particular political views, and preferences for \ncommon activities help predict link formation and dissolution in both the \nbehavioral and cognitive networks. \n</p>"}, "author": "Ashwin Bahulkar, Boleslaw K. Szymanski, Nitesh Chawla, Omar Lizardo, Kevin Chan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508233034373", "timestampUsec": "1508233034372876", "id": "tag:google.com,2005:reader/item/000000032079b1c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A graphical, scalable and intuitive method for the placement and the connection of biological cells. (arXiv:1710.05189v1 [cs.NE])", "published": 1508233035, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05189"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05189", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a graphical method originating from the computer graphics domain \nthat is used for the arbitrary and intuitive placement of cells over a \ntwo-dimensional manifold. Using a bitmap image as input, where the color \nindicates the identity of the different structures and the alpha channel \nindicates the local cell density, this method guarantees a discrete \ndistribution of cell position respecting the local density function. This \nmethod scales to any number of cells, allows to specify several different \nstructures at once with arbitrary shapes and provides a scalable and versatile \nalternative to the more classical assumption of a uniform non-spatial \ndistribution. Furthermore, several connection schemes can be derived from the \npaired distances between cells using either an automatic mapping or a \nuser-defined local reference frame, providing new computational properties for \nthe underlying model. The method is illustrated on a discrete homogeneous \nneural field, on the distribution of cones and rods in the retina and on a \ncoronal view of the basal ganglia. \n</p>"}, "author": "Nicolas P. Rougier", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508233034373", "timestampUsec": "1508233034372875", "id": "tag:google.com,2005:reader/item/000000032079b1e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Vector Quantization using the Improved Differential Evolution Algorithm for Image Compression. (arXiv:1710.05311v1 [cs.CV])", "published": 1508233035, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05311"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05311", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Vector Quantization, VQ is a popular image compression technique with a \nsimple decoding architecture and high compression ratio. Codebook designing is \nthe most essential part in Vector Quantization. LindeBuzoGray, LBG is a \ntraditional method of generation of VQ Codebook which results in lower PSNR \nvalue. A Codebook affects the quality of image compression, so the choice of an \nappropriate codebook is a must. Several optimization techniques have been \nproposed for global codebook generation to enhance the quality of image \ncompression. In this paper, a novel algorithm called IDE-LBG is proposed which \nuses Improved Differential Evolution Algorithm coupled with LBG for generating \noptimum VQ Codebooks. The proposed IDE works better than the traditional DE \nwith modifications in the scaling factor and the boundary control mechanism. \nThe IDE generates better solutions by efficient exploration and exploitation of \nthe search space. Then the best optimal solution obtained by the IDE is \nprovided as the initial Codebook for the LBG. This approach produces an \nefficient Codebook with less computational time and the consequences include \nexcellent PSNR values and superior quality reconstructed images. It is observed \nthat the proposed IDE-LBG find better VQ Codebooks as compared to IPSO-LBG, \nBA-LBG and FA-LBG. \n</p>"}, "author": "Sayan Nag", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508233034373", "timestampUsec": "1508233034372874", "id": "tag:google.com,2005:reader/item/000000032079b214", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A systematic study of the class imbalance problem in convolutional neural networks. (arXiv:1710.05381v1 [cs.CV])", "published": 1508233035, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05381"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05381", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9b98c82\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9b98c82&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this study, we systematically investigate the impact of class imbalance on \nclassification performance of convolutional neural networks (CNNs) and compare \nfrequently used methods to address the issue. Class imbalance is a common \nproblem that has been comprehensively studied in classical machine learning, \nyet very limited systematic research is available in the context of deep \nlearning. In our study, we use three benchmark datasets of increasing \ncomplexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of \nimbalance on classification and perform an extensive comparison of several \nmethods to address the issue: oversampling, undersampling, two-phase training, \nand thresholding that compensates for prior class probabilities. Our main \nevaluation metric is area under the receiver operating characteristic curve \n(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is \nassociated with notable difficulties in the context of imbalanced data. Based \non results from our experiments we conclude that (i) the effect of class \nimbalance on classification performance is detrimental; (ii) the method of \naddressing class imbalance that emerged as dominant in almost all analyzed \nscenarios was oversampling; (iii) oversampling should be applied to the level \nthat totally eliminates the imbalance, whereas undersampling can perform better \nwhen the imbalance is only removed to some extent; (iv) as opposed to some \nclassical machine learning models, oversampling does not necessarily cause \noverfitting of CNNs; (v) thresholding should be applied to compensate for prior \nclass probabilities when overall number of properly classified cases is of \ninterest. \n</p>"}, "author": "Mateusz Buda, Atsuto Maki, Maciej A. Mazurowski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508233034373", "timestampUsec": "1508233034372873", "id": "tag:google.com,2005:reader/item/000000032079b229", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalization in Deep Learning. (arXiv:1710.05468v1 [stat.ML])", "published": 1508233035, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05468"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05468", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9c69cf5\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9c69cf5&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>This paper explains why deep learning can generalize well, despite large \ncapacity and possible algorithmic instability, nonrobustness, and sharp minima, \neffectively addressing an open problem in the literature. Based on our \ntheoretical insight, this paper also proposes a family of new regularization \nmethods. Its simplest member was empirically shown to improve base models and \nachieve state-of-the-art performance on MNIST and CIFAR-10 benchmarks. \nMoreover, this paper presents both data-dependent and data-independent \ngeneralization guarantees with improved convergence rates. Our results suggest \nseveral new open areas of research. \n</p>"}, "author": "Kenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489109", "id": "tag:google.com,2005:reader/item/000000032065be68", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Independent Features with Adversarial Nets for Non-linear ICA. (arXiv:1710.05050v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05050"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05050", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Reliable measures of statistical dependence could be useful tools for \nlearning independent features and performing tasks like source separation using \nIndependent Component Analysis (ICA). Unfortunately, many of such measures, \nlike the mutual information, are hard to estimate and optimize directly. We \npropose to learn independent features with adversarial objectives which \noptimize such measures implicitly. These objectives compare samples from the \njoint distribution and the product of the marginals without the need to compute \nany probability densities. We also propose two methods for obtaining samples \nfrom the product of the marginals using either a simple resampling trick or a \nseparate parametric distribution. Our experiments show that this strategy can \neasily be applied to different types of model architectures and solve both \nlinear and non-linear ICA problems. \n</p>"}, "author": "Philemon Brakel, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489108", "id": "tag:google.com,2005:reader/item/000000032065bf4a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Automated Scalable Bayesian Inference via Hilbert Coresets. (arXiv:1710.05053v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05053"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05053", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The automation of posterior inference in Bayesian data analysis has enabled \nexperts and nonexperts alike to use more sophisticated models, engage in faster \nexploratory modeling and analysis, and ensure experimental reproducibility. \nHowever, standard automated posterior inference algorithms are not tractable at \nthe scale of massive modern datasets, and modifications to make them so are \ntypically model-specific, require expert tuning, and can break theoretical \nguarantees on inferential quality. Building on the Bayesian coresets framework, \nthis work instead takes advantage of data redundancy to shrink the dataset \nitself as a preprocessing step, providing fully-automated, scalable Bayesian \ninference with theoretical guarantees. We begin with an intuitive reformulation \nof Bayesian coreset construction as sparse vector sum approximation, and \ndemonstrate that its automation and performance-based shortcomings arise from \nthe use of the supremum norm. To address these shortcomings we develop Hilbert \ncoresets, i.e., Bayesian coresets constructed under a norm induced by an \ninner-product on the log-likelihood function space. We propose two Hilbert \ncoreset construction algorithms---one based on importance sampling, and one \nbased on the Frank-Wolfe algorithm---along with theoretical guarantees on \napproximation quality as a function of coreset size. Since the exact \ncomputation of the proposed inner-products is model-specific, we automate the \nconstruction with a random finite-dimensional projection of the log-likelihood \nfunctions. The resulting automated coreset construction algorithm is simple to \nimplement, and experiments on a variety of models with real and synthetic \ndatasets show that it provides high-quality posterior approximations and a \nsignificant reduction in the computational cost of inference. \n</p>"}, "author": "Trevor Campbell, Tamara Broderick", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489107", "id": "tag:google.com,2005:reader/item/000000032065bfc3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "DSCOVR: Randomized Primal-Dual Block Coordinate Algorithms for Asynchronous Distributed Optimization. (arXiv:1710.05080v1 [math.OC])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05080"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05080", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning with big data often involves large optimization models. For \ndistributed optimization over a cluster of machines, frequent communication and \nsynchronization of all model parameters (optimization variables) can be very \ncostly. A promising solution is to use parameter servers to store different \nsubsets of the model parameters, and update them asynchronously at different \nmachines using local datasets. In this paper, we focus on distributed \noptimization of large linear models with convex loss functions, and propose a \nfamily of randomized primal-dual block coordinate algorithms that are \nespecially suitable for asynchronous distributed implementation with parameter \nservers. In particular, we work with the saddle-point formulation of such \nproblems which allows simultaneous data and model partitioning, and exploit its \nstructure by doubly stochastic coordinate optimization with variance reduction \n(DSCOVR). Compared with other first-order distributed algorithms, we show that \nDSCOVR may require less amount of overall computation and communication, and \nless or no synchronization. We discuss the implementation details of the DSCOVR \nalgorithms, and present numerical experiments on an industrial distributed \ncomputing system. \n</p>"}, "author": "Lin Xiao, Adams Wei Yu, Qihang Lin, Weizhu Chen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489106", "id": "tag:google.com,2005:reader/item/000000032065c014", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A deep generative model for single-cell RNA sequencing with application to detecting differentially expressed genes. (arXiv:1710.05086v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05086"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05086", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose a probabilistic model for interpreting gene expression levels that \nare observed through single-cell RNA sequencing. In the model, each cell has a \nlow-dimensional latent representation. Additional latent variables account for \ntechnical effects that may erroneously set some observations of gene expression \nlevels to zero. Conditional distributions are specified by neural networks, \ngiving the proposed model enough flexibility to fit the data well. We use \nvariational inference and stochastic optimization to approximate the posterior \ndistribution. The inference procedure scales to over one million cells, whereas \ncompeting algorithms do not. Even for smaller datasets, for several tasks, the \nproposed procedure outperforms state-of-the-art methods like ZIFA and \nZINB-WaVE. We also extend our framework to take into account batch effects and \nother confounding factors and propose a natural Bayesian hypothesis framework \nfor differential expression that outperforms tradition DESeq2. \n</p>"}, "author": "Romain Lopez, Jeffrey Regier, Michael Cole, Michael Jordan, Nir Yosef", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489105", "id": "tag:google.com,2005:reader/item/000000032065c07e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Burn-In Demonstrations for Multi-Modal Imitation Learning. (arXiv:1710.05090v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05090"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05090", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent work on imitation learning has generated policies that reproduce \nexpert behavior from multi-modal data. However, past approaches have focused \nonly on recreating a small number of distinct, expert maneuvers, or have relied \non supervised learning techniques that produce unstable policies. This work \nextends InfoGAIL, an algorithm for multi-modal imitation learning, to reproduce \nbehavior over an extended period of time. Our approach involves reformulating \nthe typical imitation learning setting to include \"burn-in demonstrations\" upon \nwhich policies are conditioned at test time. We demonstrate that our approach \noutperforms standard InfoGAIL in maximizing the mutual information between \npredicted and unseen style labels in road scene simulations, and we show that \nour method leads to policies that imitate expert autonomous driving systems \nover long time horizons. \n</p>"}, "author": "Alex Kuefler, Mykel J. Kochenderfer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489104", "id": "tag:google.com,2005:reader/item/000000032065c0e8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A simple data discretizer. (arXiv:1710.05091v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05091"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05091", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Data discretization is an important step in the process of machine learning, \nsince it is easier for classifiers to deal with discrete attributes rather than \ncontinuous attributes. Over the years, several methods of performing \ndiscretization such as Boolean Reasoning, Equal Frequency Binning, Entropy have \nbeen proposed, explored, and implemented. In this article, a simple supervised \ndiscretization approach is introduced. The prime goal of MIL is to maximize \nclassification accuracy of classifier, minimizing loss of information while \ndiscretization of continuous attributes. The performance of the suggested \napproach is compared with the supervised discretization algorithm Minimum \nInformation Loss (MIL), using the state-of-the-art rule inductive algorithms- \nJ48 (Java implementation of C4.5 classifier). The presented approach is, \nindeed, the modified version of MIL. The empirical results show that the \nmodified approach performs better in several cases in comparison to the \noriginal MIL algorithm and Minimum Description Length Principle (MDLP) . \n</p>"}, "author": "Gourab Mitra, Shashidhar Sundareisan, Bikash Kanti Sarkar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489103", "id": "tag:google.com,2005:reader/item/000000032065c133", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dropout as a Low-Rank Regularizer for Matrix Factorization. (arXiv:1710.05092v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05092"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05092", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Regularization for matrix factorization (MF) and approximation problems has \nbeen carried out in many different ways. Due to its popularity in deep \nlearning, dropout has been applied also for this class of problems. Despite its \nsolid empirical performance, the theoretical properties of dropout as a \nregularizer remain quite elusive for this class of problems. In this paper, we \npresent a theoretical analysis of dropout for MF, where Bernoulli random \nvariables are used to drop columns of the factors. We demonstrate the \nequivalence between dropout and a fully deterministic model for MF in which the \nfactors are regularized by the sum of the product of squared Euclidean norms of \nthe columns. Additionally, we inspect the case of a variable sized \nfactorization and we prove that dropout achieves the global minimum of a convex \napproximation problem with (squared) nuclear norm regularization. As a result, \nwe conclude that dropout can be used as a low-rank regularizer with data \ndependent singular-value thresholding. \n</p>"}, "author": "Jacopo Cavazza, Pietro Morerio, Benjamin Haeffele, Connor Lane, Vittorio Murino, Rene Vidal", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489102", "id": "tag:google.com,2005:reader/item/000000032065c183", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Unsupervised Real-Time Control through Variational Empowerment. (arXiv:1710.05101v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05101"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05101", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We introduce a methodology for efficiently computing a lower bound to \nempowerment, allowing it to be used as an unsupervised cost function for policy \nlearning in real-time control. Empowerment, being the channel capacity between \nactions and states, maximises the influence of an agent on its near future. It \nhas been shown to be a good model of biological behaviour in the absence of an \nextrinsic goal. But empowerment is also prohibitively hard to compute, \nespecially in nonlinear continuous spaces. We introduce an efficient, amortised \nmethod for learning empowerment-maximising policies. We demonstrate that our \nalgorithm can reliably handle continuous dynamical systems using system \ndynamics learned from raw data. The resulting policies consistently drive the \nagents into states where they can use their full potential. \n</p>"}, "author": "Maximilian Karl, Maximilian Soelch, Philip Becker-Ehmck, Djalel Benbouzid, Patrick van der Smagt, Justin Bayer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489101", "id": "tag:google.com,2005:reader/item/000000032065c1cf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Arbitrage-Free Regularization. (arXiv:1710.05114v1 [q-fin.MF])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05114"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05114", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9c6a1f4\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9c6a1f4&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce a path-dependent geometric framework which generalizes the HJM \nmodeling approach to a wide variety of other asset classes. A machine learning \nregularization framework is developed with the objective of removing arbitrage \nopportunities from models within this general framework. The regularization \nmethod relies on minimal deformations of a model subject to a path-dependent \npenalty that detects arbitrage opportunities. We prove that the solution of \nthis regularization problem is independent of the arbitrage-penalty chosen, \nsubject to a fixed information loss functional. In addition to the general \nproperties of the minimal deformation, we also consider several explicit \nexamples. This paper is focused on placing machine learning methods in finance \non a sound theoretical basis and the techniques developed to achieve this \nobjective may be of interest in other areas of application. \n</p>"}, "author": "Anastasia Kratsios, Cody B. Hyndman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489100", "id": "tag:google.com,2005:reader/item/000000032065c241", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Benefits from Superposed Hawkes Processes. (arXiv:1710.05115v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05115"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05115", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The superposition of temporal point processes has been studied for many \nyears, although the usefulness of such models for practical applications has \nnot be fully developed. We investigate superposed Hawkes process as an \nimportant class of such models, with properties studied in the framework of \nleast squares estimation. The superposition of Hawkes processes is demonstrated \nto be beneficial for tightening the upper bound of excess risk under certain \nconditions, and we show the feasibility of the benefit in typical situations. \nThe usefulness of superposed Hawkes processes is verified on synthetic data, \nand its potential to solve the cold-start problem of recommendation systems is \ndemonstrated on real-world data. \n</p>"}, "author": "Hongteng Xu, Dixin Luo, Xu Chen, Lawrence Carin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489099", "id": "tag:google.com,2005:reader/item/000000032065c280", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "When Point Process Meets RNNs: Predicting Fine-Grained User Interests with Mutual Behavioral Infectivity. (arXiv:1710.05135v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05135"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05135", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Predicting fine-grained interests of users with temporal behavior is \nimportant to personalization and information filtering applications. However, \nexisting interest prediction methods are incapable of capturing the subtle \ndegreed user interests towards particular items, and the internal time-varying \ndrifting attention of individuals is not studied yet. Moreover, the prediction \nprocess can also be affected by inter-personal influence, known as behavioral \nmutual infectivity. Inspired by point process in modeling temporal point \nprocess, in this paper we present a deep prediction method based on two \nrecurrent neural networks (RNNs) to jointly model each user's continuous \nbrowsing history and asynchronous event sequences in the context of inter-user \nbehavioral mutual infectivity. Our model is able to predict the fine-grained \ninterest from a user regarding a particular item and corresponding timestamps \nwhen an occurrence of event takes place. The proposed approach is more flexible \nto capture the dynamic characteristic of event sequences by using the temporal \npoint process to model event data and timely update its intensity function by \nRNNs. Furthermore, to improve the interpretability of the model, the attention \nmechanism is introduced to emphasize both intra-personal and inter-personal \nbehavior influence over time. Experiments on real datasets demonstrate that our \nmodel outperforms the state-of-the-art methods in fine-grained user interest \nprediction. \n</p>"}, "author": "Tong Chen, Lin Wu, Yang Wang, Jun Zhang, Hongxu Chen, Xue Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489098", "id": "tag:google.com,2005:reader/item/000000032065c2c7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Improved Modified Cholesky Decomposition Method for Inverse Covariance Matrix Estimation. (arXiv:1710.05163v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05163"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05163", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The modified Cholesky decomposition is commonly used for inverse covariance \nmatrix estimation given a specified order of random variables. However, the \norder of variables is often not available or cannot be pre-determined. Hence, \nwe propose a novel estimator to address the variable order issue in the \nmodified Cholesky decomposition to estimate the sparse inverse covariance \nmatrix. The key idea is to effectively combine a set of estimates obtained from \nmultiple permutations of variable orders, and to efficiently encourage the \nsparse structure for the resultant estimate by the use of thresholding \ntechnique on the combined Cholesky factor matrix. The consistent property of \nthe proposed estimate is established under some weak regularity conditions. \nSimulation studies show the superior performance of the proposed method in \ncomparison with several existing approaches. We also apply the proposed method \ninto the linear discriminant analysis for analyzing real-data examples for \nclassification. \n</p>"}, "author": "Xiaoning Kang, Xinwei Deng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489097", "id": "tag:google.com,2005:reader/item/000000032065c2ea", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Simultaneous Matrix Diagonalization for Structural Brain Networks Classification. (arXiv:1710.05213v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05213"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05213", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper considers the problem of brain disease classification based on \nconnectome data. A connectome is a network representation of a human brain. The \ntypical connectome classification problem is very challenging because of the \nsmall sample size and high dimensionality of the data. We propose to use \nsimultaneous approximate diagonalization of adjacency matrices in order to \ncompute their eigenstructures in more stable way. The obtained approximate \neigenvalues are further used as features for classification. The proposed \napproach is demonstrated to be efficient for detection of Alzheimer's disease, \noutperforming simple baselines and competing with state-of-the-art approaches \nto brain disease classification. \n</p>"}, "author": "Nikita Mokrov, Maxim Panov, Boris A. Gutman, Joshua I. Faskowitz, Neda Jahanshad, Paul M. Thompson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489096", "id": "tag:google.com,2005:reader/item/000000032065c32f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Robust Federated Learning Using ADMM in the Presence of Data Falsifying Byzantines. (arXiv:1710.05241v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05241"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05241", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we consider the problem of federated (or decentralized) \nlearning using ADMM with multiple agents. We consider a scenario where a \ncertain fraction of agents (referred to as Byzantines) provide falsified data \nto the system. In this context, we study the convergence behavior of the \ndecentralized ADMM algorithm. We show that ADMM converges linearly to a \nneighborhood of the solution to the problem under certain conditions. We next \nprovide guidelines for network structure design to achieve faster convergence. \nNext, we provide necessary conditions on the falsified updates for exact \nconvergence to the true solution. To tackle the data falsification problem, we \npropose a robust variant of ADMM. We also provide simulation results to \nvalidate the analysis and show the resilience of the proposed algorithm to \nByzantines. \n</p>"}, "author": "Qunwei Li, Bhavya Kailkhura, Ryan Goldhahn, Priyadip Ray, Pramod K. Varshney", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489095", "id": "tag:google.com,2005:reader/item/000000032065c391", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Infinite RBMs with Frank-Wolfe. (arXiv:1710.05270v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05270"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05270", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we propose an infinite restricted Boltzmann machine~(RBM), \nwhose maximum likelihood estimation~(MLE) corresponds to a constrained convex \noptimization. We consider the Frank-Wolfe algorithm to solve the program, which \nprovides a sparse solution that can be interpreted as inserting a hidden unit \nat each iteration, so that the optimization process takes the form of a \nsequence of finite models of increasing complexity. As a side benefit, this can \nbe used to easily and efficiently identify an appropriate number of hidden \nunits during the optimization. The resulting model can also be used as an \ninitialization for typical state-of-the-art RBM training algorithms such as \ncontrastive divergence, leading to models with consistently higher test \nlikelihood than random initialization. \n</p>"}, "author": "Wei Ping, Qiang Liu, Alexander Ihler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489094", "id": "tag:google.com,2005:reader/item/000000032065c3ba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Facial Keypoints Detection. (arXiv:1710.05279v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05279"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05279", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Detect facial keypoints is a critical element in face recognition. However, \nthere is difficulty to catch keypoints on the face due to complex influences \nfrom original images, and there is no guidance to suitable algorithms. In this \npaper, we study different algorithms that can be applied to locate keyponits. \nSpecifically: our framework (1)prepare the data for further investigation \n(2)Using PCA and LBP to process the data (3) Apply different algorithms to \nanalysis data, including linear regression models, tree based model, neural \nnetwork and convolutional neural network, etc. Finally we will give our \nconclusion and further research topic. A comprehensive set of experiments on \ndataset demonstrates the effectiveness of our framework. \n</p>"}, "author": "Shenghao Shi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489093", "id": "tag:google.com,2005:reader/item/000000032065c3d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Accelerated Block Coordinate Proximal Gradients with Applications in High Dimensional Statistics. (arXiv:1710.05338v6 [math.OC] UPDATED)", "published": 1511308877, "updated": 1511308892, "canonical": [{"href": "http://arxiv.org/abs/1710.05338"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05338", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Nonconvex optimization problems arise in different research fields and arouse \nlots of attention in signal processing, statistics and machine learning. In \nthis work, we explore the accelerated proximal gradient method and some of its \nvariants which have been shown to converge under nonconvex context recently. We \nshow that a novel variant proposed here, which exploits adaptive momentum and \nblock coordinate update with specific update rules, further improves the \nperformance of a broad class of nonconvex problems. In applications to sparse \nlinear regression with regularizations like Lasso, grouped Lasso, capped \n$\\ell_1$ and SCAP, the proposed scheme enjoys provable local linear \nconvergence, with experimental justification. \n</p>"}, "author": "Tsz Kit Lau, Yuan Yao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489092", "id": "tag:google.com,2005:reader/item/000000032065c412", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Estimation of Squared-Loss Mutual Information from Positive and Unlabeled Data. (arXiv:1710.05359v2 [stat.ML] UPDATED)", "published": 1510139306, "updated": 1510139311, "canonical": [{"href": "http://arxiv.org/abs/1710.05359"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05359", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Capturing input-output dependency is an important task in statistical data \nanalysis. Mutual information (MI) is a vital tool for this purpose, but it is \nknown to be sensitive to outliers. To cope with this problem, a squared-loss \nvariant of MI (SMI) was proposed, and its supervised estimator has been \ndeveloped. On the other hand, in real-world classification problems, it is \nconceivable that only positive and unlabeled (PU) data are available. In this \npaper, we propose a novel estimator of SMI only from PU data, and prove its \noptimal convergence to true SMI. Based on the PU-SMI estimator, we further \npropose a dimension reduction method which can be executed without estimating \nthe class-prior probabilities of unlabeled data. Such PU class-prior estimation \nis often required in PU classification algorithms, but it is unreliable \nparticularly in high-dimensional problems, yielding a biased classifier. Our \ndimension reduction method significantly boosts the accuracy of PU class-prior \nestimation, as demonstrated through experiments. We also develop a method of \nindependent testing based on our PU-SMI estimator and experimentally show its \nsuperiority. \n</p>"}, "author": "Tomoya Sakai, Gang Niu, Masashi Sugiyama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489091", "id": "tag:google.com,2005:reader/item/000000032065c453", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A systematic study of the class imbalance problem in convolutional neural networks. (arXiv:1710.05381v1 [cs.CV])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05381"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05381", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9c6a734\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9c6a734&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In this study, we systematically investigate the impact of class imbalance on \nclassification performance of convolutional neural networks (CNNs) and compare \nfrequently used methods to address the issue. Class imbalance is a common \nproblem that has been comprehensively studied in classical machine learning, \nyet very limited systematic research is available in the context of deep \nlearning. In our study, we use three benchmark datasets of increasing \ncomplexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of \nimbalance on classification and perform an extensive comparison of several \nmethods to address the issue: oversampling, undersampling, two-phase training, \nand thresholding that compensates for prior class probabilities. Our main \nevaluation metric is area under the receiver operating characteristic curve \n(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is \nassociated with notable difficulties in the context of imbalanced data. Based \non results from our experiments we conclude that (i) the effect of class \nimbalance on classification performance is detrimental; (ii) the method of \naddressing class imbalance that emerged as dominant in almost all analyzed \nscenarios was oversampling; (iii) oversampling should be applied to the level \nthat totally eliminates the imbalance, whereas undersampling can perform better \nwhen the imbalance is only removed to some extent; (iv) as opposed to some \nclassical machine learning models, oversampling does not necessarily cause \noverfitting of CNNs; (v) thresholding should be applied to compensate for prior \nclass probabilities when overall number of properly classified cases is of \ninterest. \n</p>"}, "author": "Mateusz Buda, Atsuto Maki, Maciej A. Mazurowski", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489090", "id": "tag:google.com,2005:reader/item/000000032065c47a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Scaling Limit of High-Dimensional Online Independent Component Analysis. (arXiv:1710.05384v2 [cs.LG] UPDATED)", "published": 1510139306, "updated": 1510139311, "canonical": [{"href": "http://arxiv.org/abs/1710.05384"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05384", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9d7a9f9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9d7a9f9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We analyze the dynamics of an online algorithm for independent component \nanalysis in the high-dimensional scaling limit. As the ambient dimension tends \nto infinity, and with proper time scaling, we show that the time-varying joint \nempirical measure of the target feature vector and the estimates provided by \nthe algorithm will converge weakly to a deterministic measured-valued process \nthat can be characterized as the unique solution of a nonlinear PDE. Numerical \nsolutions of this PDE, which involves two spatial variables and one time \nvariable, can be efficiently obtained. These solutions provide detailed \ninformation about the performance of the ICA algorithm, as many practical \nperformance metrics are functionals of the joint empirical measures. Numerical \nsimulations show that our asymptotic analysis is accurate even for moderate \ndimensions. In addition to providing a tool for understanding the performance \nof the algorithm, our PDE analysis also provides useful insight. In particular, \nin the high-dimensional limit, the original coupled dynamics associated with \nthe algorithm will be asymptotically \"decoupled\", with each coordinate \nindependently solving a 1-D effective minimization problem via stochastic \ngradient descent. Exploiting this insight to design new algorithms for \nachieving optimal trade-offs between computational and statistical efficiency \nmay prove an interesting line of future research. \n</p>"}, "author": "Chuang Wang, Yue M. Lu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489089", "id": "tag:google.com,2005:reader/item/000000032065c490", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Manifold Regularization for Kernelized LSTD. (arXiv:1710.05387v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05387"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05387", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Policy evaluation or value function or Q-function approximation is a key \nprocedure in reinforcement learning (RL). It is a necessary component of policy \niteration and can be used for variance reduction in policy gradient methods. \nTherefore its quality has a significant impact on most RL algorithms. Motivated \nby manifold regularized learning, we propose a novel kernelized policy \nevaluation method that takes advantage of the intrinsic geometry of the state \nspace learned from data, in order to achieve better sample efficiency and \nhigher accuracy in Q-function approximation. Applying the proposed method in \nthe Least-Squares Policy Iteration (LSPI) framework, we observe superior \nperformance compared to widely used parametric basis functions on two standard \nbenchmarks in terms of policy quality. \n</p>"}, "author": "Xinyan Yan, Krzysztof Choromanski, Byron Boots, Vikas Sindhwani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489088", "id": "tag:google.com,2005:reader/item/000000032065c4e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "NeuralPower: Predict and Deploy Energy-Efficient Convolutional Neural Networks. (arXiv:1710.05420v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05420"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05420", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>\"How much energy is consumed for an inference made by a convolutional neural \nnetwork (CNN)?\" With the increased popularity of CNNs deployed on the \nwide-spectrum of platforms (from mobile devices to workstations), the answer to \nthis question has drawn significant attention. From lengthening battery life of \nmobile devices to reducing the energy bill of a datacenter, it is important to \nunderstand the energy efficiency of CNNs during serving for making an \ninference, before actually training the model. In this work, we propose \nNeuralPower: a layer-wise predictive framework based on sparse polynomial \nregression, for predicting the serving energy consumption of a CNN deployed on \nany GPU platform. Given the architecture of a CNN, NeuralPower provides an \naccurate prediction and breakdown for power and runtime across all layers in \nthe whole network, helping machine learners quickly identify the power, \nruntime, or energy bottlenecks. We also propose the \"energy-precision ratio\" \n(EPR) metric to guide machine learners in selecting an energy-efficient CNN \narchitecture that better trades off the energy consumption and prediction \naccuracy. The experimental results show that the prediction accuracy of the \nproposed NeuralPower outperforms the best published model to date, yielding an \nimprovement in accuracy of up to 68.5%. We also assess the accuracy of \npredictions at the network level, by predicting the runtime, power, and energy \nof state-of-the-art CNN architectures, achieving an average accuracy of 88.24% \nin runtime, 88.34% in power, and 97.21% in energy. We comprehensively \ncorroborate the effectiveness of NeuralPower as a powerful framework for \nmachine learners by testing it on different GPU platforms and Deep Learning \nsoftware tools. \n</p>"}, "author": "Ermao Cai, Da-Cheng Juan, Dimitrios Stamoulis, Diana Marculescu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489087", "id": "tag:google.com,2005:reader/item/000000032065c515", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalization in Deep Learning. (arXiv:1710.05468v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05468"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05468", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper explains why deep learning can generalize well, despite large \ncapacity and possible algorithmic instability, nonrobustness, and sharp minima, \neffectively addressing an open problem in the literature. Based on our \ntheoretical insight, this paper also proposes a family of new regularization \nmethods. Its simplest member was empirically shown to improve base models and \nachieve state-of-the-art performance on MNIST and CIFAR-10 benchmarks. \nMoreover, this paper presents both data-dependent and data-independent \ngeneralization guarantees with improved convergence rates. Our results suggest \nseveral new open areas of research. \n</p>"}, "author": "Kenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489086", "id": "tag:google.com,2005:reader/item/000000032065c546", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Calibrated Boosting-Forest. (arXiv:1710.05476v3 [stat.ML] UPDATED)", "published": 1510769311, "updated": 1510769327, "canonical": [{"href": "http://arxiv.org/abs/1710.05476"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05476", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Excellent ranking power along with well calibrated probability estimates are \nneeded in many classification tasks. In this paper, we introduce a technique, \nCalibrated Boosting-Forest that captures both. This novel technique is an \nensemble of gradient boosting machines that can support both continuous and \nbinary labels. While offering superior ranking power over any individual \nregression or classification model, Calibrated Boosting-Forest is able to \npreserve well calibrated posterior probabilities. Along with these benefits, we \nprovide an alternative to the tedious step of tuning gradient boosting \nmachines. We demonstrate that tuning Calibrated Boosting-Forest can be reduced \nto a simple hyper-parameter selection. We further establish that increasing \nthis hyper-parameter improves the ranking performance under a diminishing \nreturn. We examine the effectiveness of Calibrated Boosting-Forest on \nligand-based virtual screening where both continuous and binary labels are \navailable and compare the performance of Calibrated Boosting-Forest with \nlogistic regression, gradient boosting machine and deep learning. Calibrated \nBoosting-Forest achieved an approximately 48% improvement compared to a \nstate-of-art deep learning model. Moreover, it achieved around 95% improvement \non probability quality measurement compared to the best individual gradient \nboosting machine. Calibrated Boosting-Forest offers a benchmark demonstration \nthat in the field of ligand-based virtual screening, deep learning is not the \nuniversally dominant machine learning model and good calibrated probabilities \ncan better facilitate virtual screening process. \n</p>"}, "author": "Haozhen Wu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489085", "id": "tag:google.com,2005:reader/item/000000032065c573", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Geometric View of Optimal Transportation and Generative Model. (arXiv:1710.05488v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05488"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05488", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this work, we show the intrinsic relations between optimal transportation \nand convex geometry, especially the variational approach to solve Alexandrov \nproblem: constructing a convex polytope with prescribed face normals and \nvolumes. This leads to a geometric interpretation to generative models, and \nleads to a novel framework for generative models. By using the optimal \ntransportation view of GAN model, we show that the discriminator computes the \nKantorovich potential, the generator calculates the transportation map. For a \nlarge class of transportation costs, the Kantorovich potential can give the \noptimal transportation map by a close-form formula. Therefore, it is sufficient \nto solely optimize the discriminator. This shows the adversarial competition \ncan be avoided, and the computational architecture can be simplified. \nPreliminary experimental results show the geometric method outperforms WGAN for \napproximating probability measures with multiple clusters in low dimensional \nspace. \n</p>"}, "author": "Na Lei, Kehua Su, Li Cui, Shing-Tung Yau, David Xianfeng Gu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489084", "id": "tag:google.com,2005:reader/item/000000032065c5b3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Robust Maximum Likelihood Estimation of Sparse Vector Error Correction Model. (arXiv:1710.05513v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05513"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05513", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In econometrics and finance, the vector error correction model (VECM) is an \nimportant time series model for cointegration analysis, which is used to \nestimate the long-run equilibrium variable relationships. The traditional \nanalysis and estimation methodologies assume the underlying Gaussian \ndistribution but, in practice, heavy-tailed data and outliers can lead to the \ninapplicability of these methods. In this paper, we propose a robust model \nestimation method based on the Cauchy distribution to tackle this issue. In \naddition, sparse cointegration relations are considered to realize feature \nselection and dimension reduction. An efficient algorithm based on the \nmajorization-minimization (MM) method is applied to solve the proposed \nnonconvex problem. The performance of this algorithm is shown through numerical \nsimulations. \n</p>"}, "author": "Ziping Zhao, Daniel P. Palomar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489083", "id": "tag:google.com,2005:reader/item/000000032065c5d5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fully adaptive algorithm for pure exploration in linear bandits. (arXiv:1710.05552v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05552"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05552", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose the first fully-adaptive algorithm for pure exploration in linear \nbandits---the task to find the arm with the largest expected reward, which \ndepends on an unknown parameter linearly. While existing methods partially or \nentirely fix sequences of arm selections before observing rewards, our method \nadaptively changes the arm selection strategy based on past observations at \neach round. We show our sample complexity matches the achievable lower bound up \nto a constant factor in an extreme case. Furthermore, we evaluate the \nperformance of the methods by simulations based on both synthetic setting and \nreal-world data, in which our method shows vast improvement over existing \nmethods. \n</p>"}, "author": "Liyuan Xu, Junya Honda, Masashi Sugiyama", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489082", "id": "tag:google.com,2005:reader/item/000000032065c5f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fair Kernel Learning. (arXiv:1710.05578v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05578"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05578", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>New social and economic activities massively exploit big data and machine \nlearning algorithms to do inference on people's lives. Applications include \nautomatic curricula evaluation, wage determination, and risk assessment for \ncredits and loans. Recently, many governments and institutions have raised \nconcerns about the lack of fairness, equity and ethics in machine learning to \ntreat these problems. It has been shown that not including sensitive features \nthat bias fairness, such as gender or race, is not enough to mitigate the \ndiscrimination when other related features are included. Instead, including \nfairness in the objective function has been shown to be more efficient. \n</p> \n<p>We present novel fair regression and dimensionality reduction methods built \non a previously proposed fair classification framework. Both methods rely on \nusing the Hilbert Schmidt independence criterion as the fairness term. Unlike \nprevious approaches, this allows us to simplify the problem and to use multiple \nsensitive variables simultaneously. Replacing the linear formulation by kernel \nfunctions allows the methods to deal with nonlinear problems. For both linear \nand nonlinear formulations the solution reduces to solving simple matrix \ninversions or generalized eigenvalue problems. This simplifies the evaluation \nof the solutions for different trade-off values between the predictive error \nand fairness terms. We illustrate the usefulness of the proposed methods in toy \nexamples, and evaluate their performance on real world datasets to predict \nincome using gender and/or race discrimination as sensitive variables, and \ncontraceptive method prediction under demographic and socio-economic sensitive \ndescriptors. \n</p>"}, "author": "Adri&#xe1;n P&#xe9;rez-Suay, Valero Laparra, Gonzalo Mateo-Garc&#xed;a, Jordi Mu&#xf1;oz-Mar&#xed;, Luis G&#xf3;mez-Chova, Gustau Camps-Valls", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489081", "id": "tag:google.com,2005:reader/item/000000032065c625", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning from Incomplete Ratings using Nonlinear Multi-layer Semi-Nonnegative Matrix Factorization. (arXiv:1710.05613v2 [cs.LG] UPDATED)", "published": 1510229029, "updated": 1510229038, "canonical": [{"href": "http://arxiv.org/abs/1710.05613"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05613", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9d7ae1a\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9d7ae1a&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Recommender systems problems witness a growing interest for finding better \nlearning algorithms for personalized information. Matrix factorization that \nestimates the user liking for an item by taking an inner product on the latent \nfeatures of users and item have been widely studied owing to its better \naccuracy and scalability. However, it is possible that the mapping between the \nlatent features learned from these and the original features contains rather \ncomplex nonlinear hierarchical information, that classical linear matrix \nfactorization can not capture. In this paper, we aim to propose a novel \nmultilayer non-linear approach to a variant of nonnegative matrix factorization \n(NMF) to learn such factors from the incomplete ratings matrix. Firstly, we \nconstruct a user-item matrix with explicit ratings, secondly we learn latent \nfactors for representations of users and items from the designed nonlinear \nmulti-layer approach. Further, the architecture is built with different \nnonlinearities using adaptive gradient optimizer to better learn the latent \nfactors in this space. We show that by doing so, our model is able to learn \nlow-dimensional representations that are better suited for recommender systems \non several benchmark datasets. \n</p>"}, "author": "Vaibhav Krishna, Nino Antulov-Fantulin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489080", "id": "tag:google.com,2005:reader/item/000000032065c659", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Large Scale Graph Learning from Smooth Signals. (arXiv:1710.05654v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05654"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05654", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Graphs are a prevalent tool in data science, as they model the inherent \nstructure of the data. They have been used successfully in unsupervised and \nsemi-supervised learning. Typically they are constructed either by connecting \nnearest samples, or by learning them from data, solving an optimization \nproblem. While graph learning does achieve a better quality, it also comes with \na higher computational cost. In particular, the current state-of-the-art model \ncost is $\\mathcal{O}(n^2)$ for $n$ samples. In this paper, we show how to scale \nit, obtaining an approximation with leading cost of $\\mathcal{O}(n\\log(n))$, \nwith quality that approaches the exact graph learning model. Our algorithm uses \nknown approximate nearest neighbor techniques to reduce the number of \nvariables, and automatically selects the correct parameters of the model, \nrequiring a single intuitive input: the desired edge density. \n</p>"}, "author": "Vassilis Kalofolias, Nathana&#xeb;l Perraudin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489079", "id": "tag:google.com,2005:reader/item/000000032065c683", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Hardness of Inventory Management with Censored Demand Data. (arXiv:1710.05739v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05739"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05739", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider a repeated newsvendor problem where the inventory manager has no \nprior information about the demand, and can access only censored/sales data. In \nanalogy to multi-armed bandit problems, the manager needs to simultaneously \n\"explore\" and \"exploit\" with her inventory decisions, in order to minimize the \ncumulative cost. We make no probabilistic assumptions---importantly, \nindependence or time stationarity---regarding the mechanism that creates the \ndemand sequence. Our goal is to shed light on the hardness of the problem, and \nto develop policies that perform well with respect to the regret criterion, \nthat is, the difference between the cumulative cost of a policy and that of the \nbest fixed action/static inventory decision in hindsight, uniformly over all \nfeasible demand sequences. We show that a simple randomized policy, termed the \nExponentially Weighted Forecaster, combined with a carefully designed cost \nestimator, achieves optimal scaling of the expected regret (up to logarithmic \nfactors) with respect to all three key primitives: the number of time periods, \nthe number of inventory decisions available, and the demand support. Through \nthis result, we derive an important insight: the benefit from \"information \nstalking\" as well as the cost of censoring are both negligible in this dynamic \nlearning problem, at least with respect to the regret criterion. Furthermore, \nwe modify the proposed policy in order to perform well in terms of the tracking \nregret, that is, using as benchmark the best sequence of inventory decisions \nthat switches a limited number of times. Numerical experiments suggest that the \nproposed approach outperforms existing ones (that are tailored to, or \nfacilitated by, time stationarity) on nonstationary demand models. Finally, we \nextend the proposed approach and its analysis to a \"combinatorial\" version of \nthe repeated newsvendor problem. \n</p>"}, "author": "G&#xe1;bor Lugosi, Mihalis G. Markakis, Gergely Neu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489078", "id": "tag:google.com,2005:reader/item/000000032065c6a5", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning. (arXiv:1710.05741v2 [stat.ML] UPDATED)", "published": 1509484038, "updated": 1509484044, "canonical": [{"href": "http://arxiv.org/abs/1710.05741"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05741", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper takes a step towards temporal reasoning in a dynamically changing \nvideo, not in the pixel space that constitutes its frames, but in a latent \nspace that describes the non-linear dynamics of the objects in its world. We \nintroduce the Kalman variational auto-encoder, a framework for unsupervised \nlearning of sequential data that disentangles two latent representations: an \nobject's representation, coming from a recognition model, and a latent state \ndescribing its dynamics. As a result, the evolution of the world can be \nimagined and missing data imputed, both without the need to generate high \ndimensional frames at each time step. The model is trained end-to-end on videos \nof a variety of simulated physical systems, and outperforms competing methods \nin generative and missing data imputation tasks. \n</p>"}, "author": "Marco Fraccaro, Simon Kamronn, Ulrich Paquet, Ole Winther", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489077", "id": "tag:google.com,2005:reader/item/000000032065c6b9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Time Series Prediction : Predicting Stock Price. (arXiv:1710.05751v2 [stat.ML] UPDATED)", "published": 1508718990, "updated": 1508718993, "canonical": [{"href": "http://arxiv.org/abs/1710.05751"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05751", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Time series forecasting is widely used in a multitude of domains. In this \npaper, we present four models to predict the stock price using the SPX index as \ninput time series data. The martingale and ordinary linear models require the \nstrongest assumption in stationarity which we use as baseline models. The \ngeneralized linear model requires lesser assumptions but is unable to \noutperform the martingale. In empirical testing, the RNN model performs the \nbest comparing to other two models, because it will update the input through \nLSTM instantaneously, but also does not beat the martingale. In addition, we \nintroduce an online to batch algorithm and discrepancy measure to inform \nreaders the newest research in time series predicting method, which doesn't \nrequire any stationarity or non mixing assumptions in time series data. \nFinally, to apply these forecasting to practice, we introduce basic trading \nstrategies that can create Win win and Zero sum situations. \n</p>"}, "author": "Aaron Elliot, Cheng Hua Hsu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489076", "id": "tag:google.com,2005:reader/item/000000032065c6c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "TensorQuant - A Simulation Toolbox for Deep Neural Network Quantization. (arXiv:1710.05758v1 [cs.CV])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05758"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05758", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent research implies that training and inference of deep neural networks \n(DNN) can be computed with low precision numerical representations of the \ntraining/test data, weights and gradients without a general loss in accuracy. \nThe benefit of such compact representations is twofold: they allow a \nsignificant reduction of the communication bottleneck in distributed DNN \ntraining and faster neural network implementations on hardware accelerators \nlike FPGAs. Several quantization methods have been proposed to map the original \n32-bit floating point problem to low-bit representations. While most related \npublications validate the proposed approach on a single DNN topology, it \nappears to be evident, that the optimal choice of the quantization method and \nnumber of coding bits is topology dependent. To this end, there is no general \ntheory available, which would allow users to derive the optimal quantization \nduring the design of a DNN topology. In this paper, we present a quantization \ntool box for the TensorFlow framework. TensorQuant allows a transparent \nquantization simulation of existing DNN topologies during training and \ninference. TensorQuant supports generic quantization methods and allows \nexperimental evaluation of the impact of the quantization on single layers as \nwell as on the full topology. In a first series of experiments with \nTensorQuant, we show an analysis of fix-point quantizations of popular CNN \ntopologies. \n</p>"}, "author": "Dominik Marek Loroch, Norbert Wehn, Franz-Josef Pfreundt, Janis Keuper", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489075", "id": "tag:google.com,2005:reader/item/000000032065c6e6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Nonsmooth Frank-Wolfe using Uniform Affine Approximations. (arXiv:1710.05776v1 [stat.ML])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05776"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05776", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Frank-Wolfe methods (FW) have gained significant interest in the machine \nlearning community due to its ability to efficiently solve large problems that \nadmit a sparse structure (e.g. sparse vectors and low-rank matrices). However \nthe performance of the existing FW method hinges on the quality of the linear \napproximation. This typically restricts FW to smooth functions for which the \napproximation quality, indicated by a global curvature measure, is reasonably \ngood. \n</p> \n<p>In this paper, we propose a modified FW algorithm amenable to nonsmooth \nfunctions by optimizing for approximation quality over all affine \napproximations given a neighborhood of interest. We analyze theoretical \nproperties of the proposed algorithm and demonstrate that it overcomes many \nissues associated with existing methods in the context of nonsmooth low-rank \nmatrix estimation. \n</p>"}, "author": "Edward Cheung, Yuying Li", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489074", "id": "tag:google.com,2005:reader/item/000000032065c70c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A successive difference-of-convex approximation method for a class of nonconvex nonsmooth optimization problems. (arXiv:1710.05778v1 [math.OC])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05778"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05778", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We consider a class of nonconvex nonsmooth optimization problems whose \nobjective is the sum of a nonnegative smooth function and a bunch of \nnonnegative proper closed possibly nonsmooth functions (whose proximal mappings \nare easy to compute), some of which are further composed with linear maps. This \nkind of problems arises naturally in various applications when different \nregularizers are introduced for inducing simultaneous structures in the \nsolutions. Solving these problems, however, can be challenging because of the \ncoupled nonsmooth functions: the corresponding proximal mapping can be hard to \ncompute so that standard first-order methods such as the proximal gradient \nalgorithm cannot be applied efficiently. In this paper, we propose a successive \ndifference-of-convex approximation method for solving this kind of problems. In \nthis algorithm, we approximate the nonsmooth functions by their Moreau \nenvelopes in each iteration. Making use of the simple observation that Moreau \nenvelopes of nonnegative proper closed functions are continuous \ndifference-of-convex functions, we can then approximately minimize the \napproximation function by first-order methods with suitable majorization \ntechniques. These first-order methods can be implemented efficiently thanks to \nthe fact that the proximal mapping of each nonsmooth function is easy to \ncompute. Under suitable assumptions, we prove that the sequence generated by \nour method is bounded and clusters at a stationary point of the objective. We \nalso discuss how our method can be applied to concrete applications such as \nnonconvex fused regularized optimization problems and simultaneously structured \nmatrix optimization problems, and illustrate the performance numerically for \nthese two specific applications. \n</p>"}, "author": "Tianxiang Liu, Ting Kei Pong, Akiko Takeda", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489073", "id": "tag:google.com,2005:reader/item/000000032065c71f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Densely Connected Convolutional Networks and Signal Quality Analysis to Detect Atrial Fibrillation Using Short Single-Lead ECG Recordings. (arXiv:1710.05817v1 [eess.SP])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05817"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05817", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The development of new technology such as wearables that record high-quality \nsingle channel ECG, provides an opportunity for ECG screening in a larger \npopulation, especially for atrial fibrillation screening. The main goal of this \nstudy is to develop an automatic classification algorithm for normal sinus \nrhythm (NSR), atrial fibrillation (AF), other rhythms (O), and noise from a \nsingle channel short ECG segment (9-60 seconds). For this purpose, signal \nquality index (SQI) along with dense convolutional neural networks was used. \nTwo convolutional neural network (CNN) models (main model that accepts 15 \nseconds ECG and secondary model that processes 9 seconds shorter ECG) were \ntrained using the training data set. If the recording is determined to be of \nlow quality by SQI, it is immediately classified as noisy. Otherwise, it is \ntransformed to a time-frequency representation and classified with the CNN as \nNSR, AF, O, or noise. At the final step, a feature-based post-processing \nalgorithm classifies the rhythm as either NSR or O in case the CNN model's \ndiscrimination between the two is indeterminate. The best result achieved at \nthe official phase of the PhysioNet/CinC challenge on the blind test set was \n0.80 (F1 for NSR, AF, and O were 0.90, 0.80, and 0.70, respectively). \n</p>"}, "author": "Jonathan Rubin, Saman Parvaneh, Asif Rahman, Bryan Conroy, Saeed Babaeizadeh", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489072", "id": "tag:google.com,2005:reader/item/000000032065c73c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Geometric Learning and Filtering in Finance. (arXiv:1710.05829v1 [q-fin.MF])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05829"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05829", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We develop a method for incorporating relevant non-Euclidean geometric \ninformation into a broad range of classical filtering and statistical or \nmachine learning algorithms. We apply these techniques to approximate the \nsolution of the non-Euclidean filtering problem to arbitrary precision. We then \nextend the particle filtering algorithm to compute our asymptotic solution to \narbitrary precision. Moreover, we find explicit error bounds measuring the \ndiscrepancy between our locally triangulated filter and the true theoretical \nnon-Euclidean filter. Our methods are motivated by certain fundamental problems \nin mathematical finance. In particular we apply these filtering techniques to \nincorporate the non-Euclidean geometry present in stochastic volatility models \nand optimal Markowitz portfolios. We also extend Euclidean statistical or \nmachine learning algorithms to non-Euclidean problems by using the local \ntriangulation technique, which we show improves the accuracy of the original \nalgorithm. We apply the local triangulation method to obtain improvements of \nthe (sparse) principal component analysis and the principal geodesic analysis \nalgorithms and show how these improved algorithms can be used to parsimoniously \nestimate the evolution of the shape of forward-rate curves. While focused on \nfinancial applications, the non-Euclidean geometric techniques presented in \nthis paper can be employed to provide improvements to a range of other \nstatistical or machine learning algorithms and may be useful in other areas of \napplication. \n</p>"}, "author": "Anastasia Kratsios, Cody B. Hyndman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489071", "id": "tag:google.com,2005:reader/item/000000032065c75e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Spectral Algorithms for Computing Fair Support Vector Machines. (arXiv:1710.05895v1 [cs.LG])", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05895"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05895", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9d7b6f6\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9d7b6f6&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Classifiers and rating scores are prone to implicitly codifying biases, which \nmay be present in the training data, against protected classes (i.e., age, \ngender, or race). So it is important to understand how to design classifiers \nand scores that prevent discrimination in predictions. This paper develops \ncomputationally tractable algorithms for designing accurate but fair support \nvector machines (SVM's). Our approach imposes a constraint on the covariance \nmatrices conditioned on each protected class, which leads to a nonconvex \nquadratic constraint in the SVM formulation. We develop iterative algorithms to \ncompute fair linear and kernel SVM's, which solve a sequence of relaxations \nconstructed using a spectral decomposition of the nonconvex constraint. Its \neffectiveness in achieving high prediction accuracy while ensuring fairness is \nshown through numerical experiments on several data sets. \n</p>"}, "author": "Matt Olfat, Anil Aswani", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508224100489", "timestampUsec": "1508224100489044", "id": "tag:google.com,2005:reader/item/000000032065cb40", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Stochastic Replica Approach to Machine Learning: Stability and Parameter Optimization. (arXiv:1708.05715v2 [stat.ML] UPDATED)", "published": 1508224100, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.05715"}], "alternate": [{"href": "http://arxiv.org/abs/1708.05715", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9e34d16\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9e34d16&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We introduce a statistical physics inspired supervised machine learning \nalgorithm for classification and regression problems. The method is based on \nthe invariances or stability of predicted results when known data is \nrepresented as expansions in terms of various stochastic functions. The \nalgorithm predicts the classification/regression values of new data by \ncombining (via voting) the outputs of these numerous linear expansions in \nrandomly chosen functions. The few parameters (typically only one parameter is \nused in all studied examples) that this model has may be automatically \noptimized. The algorithm has been tested on 10 diverse training data sets of \nvarious types and feature space dimensions. It has been shown to consistently \nexhibit high accuracy and readily allow for optimization of parameters, while \nsimultaneously avoiding pitfalls of existing algorithms such as those \nassociated with class imbalance. We very briefly speculate on whether spatial \ncoordinates in physical theories may be viewed as emergent \"features\" that \nenable a robust machine learning type description of data with generic low \norder smooth functions. \n</p>"}, "author": "Patrick Chao, Tahereh Mazaheri, Bo Sun, Nicholas B. Weingartner, Zohar Nussinov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508115936014", "timestampUsec": "1508115936014402", "id": "tag:google.com,2005:reader/item/000000031f99e80f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "STDP Based Pruning of Connections and Weight Quantization in Spiking Neural Networks for Energy Efficient Recognition. (arXiv:1710.04734v1 [cs.NE])", "published": 1508115936, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04734"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04734", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Spiking Neural Networks (SNNs) with a large number of weights and varied \nweight distribution can be difficult to implement in emerging in-memory \ncomputing hardware due to the limitations on crossbar size (implementing dot \nproduct), the constrained number of conductance levels in non-CMOS devices and \nthe power budget. We present a sparse SNN topology where non-critical \nconnections are pruned to reduce the network size and the remaining critical \nsynapses are weight quantized to accommodate for limited conductance levels. \nPruning is based on the power law weight-dependent Spike Timing Dependent \nPlasticity (STDP) model; synapses between pre- and post-neuron with high spike \ncorrelation are retained, whereas synapses with low correlation or uncorrelated \nspiking activity are pruned. The weights of the retained connections are \nquantized to the available number of conductance levels. The process of pruning \nnon-critical connections and quantizing the weights of critical synapses is \nperformed at regular intervals during training. We evaluated our sparse and \nquantized network on MNIST dataset and on a subset of images from Caltech-101 \ndataset. The compressed topology achieved a classification accuracy of 90.1% \n(91.6%) on the MNIST (Caltech-101) dataset with 3.1x (2.2x) and 4x (2.6x) \nimprovement in energy and area, respectively. The compressed topology is energy \nand area efficient while maintaining the same classification accuracy of a \n2-layer fully connected SNN topology. \n</p>"}, "author": "Nitin Rathi, Priyadarshini Panda, Kaushik Roy", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508115936014", "timestampUsec": "1508115936014401", "id": "tag:google.com,2005:reader/item/000000031f99e814", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "HyperENTM: Evolving Scalable Neural Turing Machines through HyperNEAT. (arXiv:1710.04748v1 [cs.AI])", "published": 1508115936, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04748"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04748", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent developments within memory-augmented neural networks have solved \nsequential problems requiring long-term memory, which are intractable for \ntraditional neural networks. However, current approaches still struggle to \nscale to large memory sizes and sequence lengths. In this paper we show how \naccess to memory can be encoded geometrically through a HyperNEAT-based Neural \nTuring Machine (HyperENTM). We demonstrate that using the indirect HyperNEAT \nencoding allows for training on small memory vectors in a bit-vector copy task \nand then applying the knowledge gained from such training to speed up training \non larger size memory vectors. Additionally, we demonstrate that in some \ninstances, networks trained to copy bit-vectors of size 9 can be scaled to \nsizes of 1,000 without further training. While the task in this paper is \nsimple, these results could open up the problems amendable to networks with \nexternal memories to problems with larger memory vectors and theoretically \nunbounded memory sizes. \n</p>"}, "author": "Jakob Merrild, Mikkel Angaju Rasmussen, Sebastian Risi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508115936014", "timestampUsec": "1508115936014400", "id": "tag:google.com,2005:reader/item/000000031f99e816", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficient Computation in Adaptive Artificial Spiking Neural Networks. (arXiv:1710.04838v1 [cs.NE])", "published": 1508115936, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04838"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04838", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Artificial Neural Networks (ANNs) are bio-inspired models of neural \ncomputation that have proven highly effective. Still, ANNs lack a natural \nnotion of time, and neural units in ANNs exchange analog values in a \nframe-based manner, a computationally and energetically inefficient form of \ncommunication. This contrasts sharply with biological neurons that communicate \nsparingly and efficiently using binary spikes. While artificial Spiking Neural \nNetworks (SNNs) can be constructed by replacing the units of an ANN with \nspiking neurons, the current performance is far from that of deep ANNs on hard \nbenchmarks and these SNNs use much higher firing rates compared to their \nbiological counterparts, limiting their efficiency. Here we show how spiking \nneurons that employ an efficient form of neural coding can be used to construct \nSNNs that match high-performance ANNs and exceed state-of-the-art in SNNs on \nimportant benchmarks, while requiring much lower average firing rates. For \nthis, we use spike-time coding based on the firing rate limiting adaptation \nphenomenon observed in biological spiking neurons. This phenomenon can be \ncaptured in adapting spiking neuron models, for which we derive the effective \ntransfer function. Neural units in ANNs trained with this transfer function can \nbe substituted directly with adaptive spiking neurons, and the resulting \nAdaptive SNNs (AdSNNs) can carry out inference in deep neural networks using up \nto an order of magnitude fewer spikes compared to previous SNNs. Adaptive \nspike-time coding additionally allows for the dynamic control of neural coding \nprecision: we show how a simple model of arousal in AdSNNs further halves the \naverage required firing rate and this notion naturally extends to other forms \nof attention. AdSNNs thus hold promise as a novel and efficient model for \nneural computation that naturally fits to temporally continuous and \nasynchronous applications. \n</p>"}, "author": "Davide Zambrano, Roeland Nusselder, H. Steven Scholte, Sander Bohte", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508115936014", "timestampUsec": "1508115936014399", "id": "tag:google.com,2005:reader/item/000000031f99e81a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Method of Generating Random Weights and Biases in Feedforward Neural Networks with Random Hidden Nodes. (arXiv:1710.04874v1 [cs.NE])", "published": 1508115936, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04874"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04874", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks with random hidden nodes have gained increasing interest from \nresearchers and practical applications. This is due to their unique features \nsuch as very fast training and universal approximation property. In these \nnetworks the weights and biases of hidden nodes determining the nonlinear \nfeature mapping are set randomly and are not learned. Appropriate selection of \nthe intervals from which weights and biases are selected is extremely \nimportant. This topic has not yet been sufficiently explored in the literature. \nIn this work a method of generating random weights and biases is proposed. This \nmethod generates the parameters of the hidden nodes in such a way that \nnonlinear fragments of the activation functions are located in the input space \nregions with data and can be used to construct the surface approximating a \nnonlinear target function. The weights and biases are dependent on the input \ndata range and activation function type. The proposed methods allows us to \ncontrol the generalization degree of the model. These all lead to improvement \nin approximation performance of the network. Several experiments show very \npromising results. \n</p>"}, "author": "Grzegorz Dudek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361619", "id": "tag:google.com,2005:reader/item/000000031f973003", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Game-Theoretic Design of Secure and Resilient Distributed Support Vector Machines with Adversaries. (arXiv:1710.04677v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04677"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04677", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With a large number of sensors and control units in networked systems, \ndistributed support vector machines (DSVMs) play a fundamental role in scalable \nand efficient multi-sensor classification and prediction tasks. However, DSVMs \nare vulnerable to adversaries who can modify and generate data to deceive the \nsystem to misclassification and misprediction. This work aims to design defense \nstrategies for DSVM learner against a potential adversary. We establish a \ngame-theoretic framework to capture the conflicting interests between the DSVM \nlearner and the attacker. The Nash equilibrium of the game allows predicting \nthe outcome of learning algorithms in adversarial environments, and enhancing \nthe resilience of the machine learning through dynamic distributed learning \nalgorithms. We show that the DSVM learner is less vulnerable when he uses a \nbalanced network with fewer nodes and higher degree. We also show that adding \nmore training samples is an efficient defense strategy against an attacker. We \npresent secure and resilient DSVM algorithms with verification method and \nrejection method, and show their resiliency against adversary with numerical \nexperiments. \n</p>"}, "author": "Rui Zhang, Quanyan Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361618", "id": "tag:google.com,2005:reader/item/000000031f973012", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Hyperparameter Importance Across Datasets. (arXiv:1710.04725v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04725"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04725", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the advent of automated machine learning, automated hyperparameter \noptimization methods are by now routinely used. However, this progress is not \nyet matched by equal progress on automatic analyses that yield information \nbeyond performance-optimizing hyperparameter settings. In this work, we aim to \nanswer the following two questions: Given an algorithm, what are generally its \nmost important hyperparameters, and what are good priors over their \nhyperparameters' ranges to draw values from? We present methodology and a \nframework to answer these questions based on meta-learning across many \ndatasets. We apply this methodology using the experimental meta-data available \non OpenML to determine the most important hyperparameters of support vector \nmachines, random forests and Adaboost, and to infer priors for all their \nhyperparameters. Our results, obtained fully automatically, provide a \nquantitative basis to focus efforts in both manual algorithm design and in \nautomated hyperparameter optimization. Our experiments confirm that the \nselected hyperparameters are indeed the most important ones and that our \nobtained priors also lead to improvements in hyperparameter optimization. \n</p>"}, "author": "J. N. van Rijn, F. Hutter", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361617", "id": "tag:google.com,2005:reader/item/000000031f97301b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Runtime-Efficacy Trade-off of Anomaly Detection Techniques for Real-Time Streaming Data. (arXiv:1710.04735v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04735"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04735", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Ever growing volume and velocity of data coupled with decreasing attention \nspan of end users underscore the critical need for real-time analytics. In this \nregard, anomaly detection plays a key role as an application as well as a means \nto verify data fidelity. Although the subject of anomaly detection has been \nresearched for over 100 years in a multitude of disciplines such as, but not \nlimited to, astronomy, statistics, manufacturing, econometrics, marketing, most \nof the existing techniques cannot be used as is on real-time data streams. \nFurther, the lack of characterization of performance -- both with respect to \nreal-timeliness and accuracy -- on production data sets makes model selection \nvery challenging. To this end, we present an in-depth analysis, geared towards \nreal-time streaming data, of anomaly detection techniques. Given the \nrequirements with respect to real-timeliness and accuracy, the analysis \npresented in this paper should serve as a guide for selection of the \"best\" \nanomaly detection technique. To the best of our knowledge, this is the first \ncharacterization of anomaly detection techniques proposed in very diverse set \nof fields, using production data sets corresponding to a wide set of \napplication domains. \n</p>"}, "author": "Dhruv Choudhary, Arun Kejariwal, Francois Orsini", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361616", "id": "tag:google.com,2005:reader/item/000000031f973021", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Explaining Aviation Safety Incidents Using Deep Learned Precursors. (arXiv:1710.04749v1 [cs.CV])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04749"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04749", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Although aviation accidents are rare, safety incidents occur more frequently \nand require careful analysis for providing actionable recommendations to \nimprove safety. Automatically analyzing safety incidents using flight data is \nchallenging because of the absence of labels on timestep-wise events in a \nflight, complexity of multi-dimensional data, and lack of scalable tools to \nperform analysis over large number of events. In this work, we propose a \nprecursor mining algorithm that identifies correlated patterns in \nmultidimensional time series to explain an adverse event. Precursors are \nvaluable to systems health and safety monitoring in explaining and forecasting \nanomalies. Current precursor mining methods suffer from poor scalability to \nhigh dimensional time series data and in capturing long-term memory. We propose \nan approach by combining multiple-instance learning (MIL) and deep recurrent \nneural networks (DRNN) to take advantage of MIL's ability to model \nweakly-supervised data and DRNN's ability to model long term memory processes, \nto scale well to high dimensional data and to large volumes of data using GPU \nparallelism. We apply the proposed method to find precursors and offer \nexplanations to high speed exceedance safety incidents using commercial flight \ndata. \n</p>"}, "author": "Vijay Manikandan Janakiraman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361615", "id": "tag:google.com,2005:reader/item/000000031f973029", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Hypernetworks. (arXiv:1710.04759v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04759"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04759", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9e34f8c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9e34f8c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose Bayesian hypernetworks: a framework for approximate Bayesian \ninference in neural networks. A Bayesian hypernetwork, $h$, is a neural network \nwhich learns to transform a simple noise distribution, $p(\\epsilon) = \n\\mathcal{N}(0,I)$, to a distribution $q(\\theta) \\doteq q(h(\\epsilon))$ over the \nparameters $\\theta$ of another neural network (the \"primary network\"). We train \n$q$ with variational inference, using an invertible $h$ to enable efficient \nestimation of the variational lower bound on the posterior $p(\\theta | \n\\mathcal{D})$ via sampling. In contrast to most methods for Bayesian deep \nlearning, Bayesian hypernets can represent a complex multimodal approximate \nposterior with correlations between parameters, while enabling cheap i.i.d. \nsampling of $q(\\theta)$. We demonstrate these qualitative advantages of \nBayesian hypernets, which also achieve competitive performance on a suite of \ntasks that demonstrate the advantage of estimating model uncertainty, including \nactive learning and anomaly detection. \n</p>"}, "author": "David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, Aaron Courville", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361614", "id": "tag:google.com,2005:reader/item/000000031f973036", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sparse Weighted Canonical Correlation Analysis. (arXiv:1710.04792v1 [cs.LG])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04792"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04792", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Given two data matrices $X$ and $Y$, sparse canonical correlation analysis \n(SCCA) is to seek two sparse canonical vectors $u$ and $v$ to maximize the \ncorrelation between $Xu$ and $Yv$. However, classical and sparse CCA models \nconsider the contribution of all the samples of data matrices and thus cannot \nidentify an underlying specific subset of samples. To this end, we propose a \nnovel sparse weighted canonical correlation analysis (SWCCA), where weights are \nused for regularizing different samples. We solve the $L_0$-regularized SWCCA \n($L_0$-SWCCA) using an alternating iterative algorithm. We apply $L_0$-SWCCA to \nsynthetic data and real-world data to demonstrate its effectiveness and \nsuperiority compared to related methods. Lastly, we consider also SWCCA with \ndifferent penalties like LASSO (Least absolute shrinkage and selection \noperator) and Group LASSO, and extend it for integrating more than three data \nmatrices. \n</p>"}, "author": "Wenwen Min, Juan Liu, Shihua Zhang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361613", "id": "tag:google.com,2005:reader/item/000000031f973042", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning for Case-based Reasoning through Prototypes: A Neural Network that Explains its Predictions. (arXiv:1710.04806v1 [cs.AI])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04806"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04806", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks are widely used for classification. These deep models \noften suffer from a lack of interpretability -- they are particularly difficult \nto understand because of their non-linear nature. As a result, neural networks \nare often treated as \"black box\" models, and in the past, have been trained \npurely to optimize the accuracy of predictions. In this work, we create a novel \nnetwork architecture for deep learning that naturally explains its own \nreasoning for each prediction. This architecture contains an autoencoder and a \nspecial prototype layer, where each unit of that layer stores a weight vector \nthat resembles an encoded training input. The encoder of the autoencoder allows \nus to do comparisons within the latent space, while the decoder allows us to \nvisualize the learned prototypes. The training objective has four terms: an \naccuracy term, a term that encourages every prototype to be similar to at least \none encoded input, a term that encourages every encoded input to be close to at \nleast one prototype, and a term that encourages faithful reconstruction by the \nautoencoder. The distances computed in the prototype layer are used as part of \nthe classification process. Since the prototypes are learned during training, \nthe learned network naturally comes with explanations for each prediction, and \nthe explanations are loyal to what the network actually computes. \n</p>"}, "author": "Oscar Li, Hao Liu, Chaofan Chen, Cynthia Rudin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361612", "id": "tag:google.com,2005:reader/item/000000031f97304a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Machine Learning by Two-Dimensional Hierarchical Tensor Networks: A Quantum Information Theoretic Perspective on Deep Architectures. (arXiv:1710.04833v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04833"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04833", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The resemblance between the methods used in studying quantum-many body \nphysics and in machine learning has drawn considerable attention. In \nparticular, tensor networks (TNs) and deep learning architectures bear striking \nsimilarities to the extent that TNs can be used for machine learning. Previous \nresults used one-dimensional TNs in image recognition, showing limited \nscalability and a high bond dimension. In this work, we train two-dimensional \nhierarchical TNs to solve image recognition problems, using a training \nalgorithm derived from the multipartite entanglement renormalization ansatz \n(MERA). This approach overcomes scalability issues and implies novel \nmathematical connections among quantum many-body physics, quantum information \ntheory, and machine learning. While keeping the TN unitary in the training \nphase, TN states can be defined, which optimally encodes each class of the \nimages into a quantum many-body state. We study the quantum features of the TN \nstates, including quantum entanglement and fidelity. We suggest these \nquantities could be novel properties that characterize the image classes, as \nwell as the machine learning tasks. Our work could be further applied to \nidentifying possible quantum properties of certain artificial intelligence \nmethods. \n</p>"}, "author": "Ding Liu, Shi-Ju Ran, Peter Wittek, Cheng Peng, Raul Bl&#xe1;zquez Garc&#xed;a, Gang Su, Maciej Lewenstein", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361611", "id": "tag:google.com,2005:reader/item/000000031f973057", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Recent Advances in Zero-shot Recognition. (arXiv:1710.04837v1 [cs.CV])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04837"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04837", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the recent renaissance of deep convolution neural networks, encouraging \nbreakthroughs have been achieved on the supervised recognition tasks, where \neach class has sufficient training data and fully annotated training data. \nHowever, to scale the recognition to a large number of classes with few or now \ntraining samples for each class remains an unsolved problem. One approach to \nscaling up the recognition is to develop models capable of recognizing unseen \ncategories without any training instances, or zero-shot recognition/ learning. \nThis article provides a comprehensive review of existing zero-shot recognition \ntechniques covering various aspects ranging from representations of models, and \nfrom datasets and evaluation settings. We also overview related recognition \ntasks including one-shot and open set recognition which can be used as natural \nextensions of zero-shot recognition when limited number of class samples become \navailable or when zero-shot recognition is implemented in a real-world setting. \nImportantly, we highlight the limitations of existing approaches and point out \nfuture research directions in this existing new research area. \n</p>"}, "author": "Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue, Leonid Sigal, Shaogang Gong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361610", "id": "tag:google.com,2005:reader/item/000000031f973060", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Manifold regularization based on Nystr{\\&quot;o}m type subsampling. (arXiv:1710.04872v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04872"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04872", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we study the Nystr{\\\"o}m type subsampling for large scale \nkernel methods to reduce the computational complexities of big data. We discuss \nthe multi-penalty regularization scheme based on Nystr{\\\"o}m type subsampling \nwhich is motivated from well-studied manifold regularization schemes. We \ndevelop a theoretical analysis of multi-penalty least-square regularization \nscheme under the general source condition in vector-valued function setting, \ntherefore the results can also be applied to multi-task learning problems. We \nachieve the optimal minimax convergence rates of multi-penalty regularization \nusing the concept of effective dimension for the appropriate subsampling size. \nWe discuss an aggregation approach based on linear function strategy to combine \nvarious Nystr{\\\"o}m approximants. Finally, we demonstrate the performance of \nmulti-penalty regularization based on Nystr{\\\"o}m type subsampling on \nCaltech-101 data set for multi-class image classification and NSL-KDD benchmark \ndata set for intrusion detection problem. \n</p>"}, "author": "Abhishake Rastogi, Sivananthan Sampath", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361609", "id": "tag:google.com,2005:reader/item/000000031f973064", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Method of Generating Random Weights and Biases in Feedforward Neural Networks with Random Hidden Nodes. (arXiv:1710.04874v1 [cs.NE])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04874"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04874", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks with random hidden nodes have gained increasing interest from \nresearchers and practical applications. This is due to their unique features \nsuch as very fast training and universal approximation property. In these \nnetworks the weights and biases of hidden nodes determining the nonlinear \nfeature mapping are set randomly and are not learned. Appropriate selection of \nthe intervals from which weights and biases are selected is extremely \nimportant. This topic has not yet been sufficiently explored in the literature. \nIn this work a method of generating random weights and biases is proposed. This \nmethod generates the parameters of the hidden nodes in such a way that \nnonlinear fragments of the activation functions are located in the input space \nregions with data and can be used to construct the surface approximating a \nnonlinear target function. The weights and biases are dependent on the input \ndata range and activation function type. The proposed methods allows us to \ncontrol the generalization degree of the model. These all lead to improvement \nin approximation performance of the network. Several experiments show very \npromising results. \n</p>"}, "author": "Grzegorz Dudek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361608", "id": "tag:google.com,2005:reader/item/000000031f973067", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Graph Convolutional Networks for Classification with a Structured Label Space. (arXiv:1710.04908v1 [cs.LG])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04908"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04908", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is a usual practice to ignore any structural information underlying \nclasses in multi-class classification. In this paper, we propose a graph \nconvolutional network (GCN) augmented neural network classifier to exploit a \nknown, underlying graph structure of labels. The proposed approach resembles an \n(approximate) inference procedure in, for instance, a conditional random field \n(CRF), however without losing any modelling flexibility. The proposed method \ncan easily scale up to thousands of labels. We evaluate the proposed approach \non the problems of document classification and object recognition and report \nboth accuracies and graph-theoretic metrics that correspond to the consistency \nof the model's prediction. The experiment results reveal that the proposed \nmodel outperforms a baseline method which ignores the graph structures of a \nlabel space. \n</p>"}, "author": "Meihao Chen, Zhuoru Lin, Kyunghyun Cho", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361607", "id": "tag:google.com,2005:reader/item/000000031f973075", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Two-stage Algorithm for Fairness-aware Machine Learning. (arXiv:1710.04924v1 [stat.ML])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04924"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04924", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Algorithmic decision making process now affects many aspects of our lives. \nStandard tools for machine learning, such as classification and regression, are \nsubject to the bias in data, and thus direct application of such off-the-shelf \ntools could lead to a specific group being unfairly discriminated. Removing \nsensitive attributes of data does not solve this problem because a \n\\textit{disparate impact} can arise when non-sensitive attributes and sensitive \nattributes are correlated. Here, we study a fair machine learning algorithm \nthat avoids such a disparate impact when making a decision. Inspired by the \ntwo-stage least squares method that is widely used in the field of economics, \nwe propose a two-stage algorithm that removes bias in the training data. The \nproposed algorithm is conceptually simple. Unlike most of existing fair \nalgorithms that are designed for classification tasks, the proposed method is \nable to (i) deal with regression tasks, (ii) combine explanatory attributes to \nremove reverse discrimination, and (iii) deal with numerical sensitive \nattributes. The performance and fairness of the proposed algorithm are \nevaluated in simulations with synthetic and real-world datasets. \n</p>"}, "author": "Junpei Komiyama, Hajime Shimao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361606", "id": "tag:google.com,2005:reader/item/000000031f97307a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "RADNET: Radiologist Level Accuracy using Deep Learning for HEMORRHAGE detection in CT Scans. (arXiv:1710.04934v1 [cs.CV])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04934"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04934", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We describe a deep learning approach for automated brain hemorrhage detection \nfrom computed tomography (CT) scans. Our model emulates the procedure followed \nby radiologists to analyse a 3D CT scan in real-world. Similar to radiologists, \nthe model sifts through 2D cross-sectional slices while paying close attention \nto potential hemorrhagic regions. Further, the model utilizes 3D context from \nneighboring slices to improve predictions at each slice and subsequently, \naggregates the slice-level predictions to provide diagnosis at CT level. We \nrefer to our proposed approach as Recurrent Attention DenseNet (RADnet) as it \nemploys original DenseNet architecture along with adding the components of \nattention for slice level predictions and recurrent neural network layer for \nincorporating 3D context. The real-world performance of RADnet has been \nbenchmarked against independent analysis performed by three senior radiologists \nfor 77 brain CTs. RADnet demonstrates 81.82% hemorrhage prediction accuracy at \nCT level that is comparable to radiologists. Further, RADnet achieves higher \nrecall than two of the three radiologists, which is remarkable. \n</p>"}, "author": "Monika Grewal, Muktabh Mayank Srivastava, Pulkit Kumar, Srikrishna Varadarajan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113977362", "timestampUsec": "1508113977361605", "id": "tag:google.com,2005:reader/item/000000031f97307d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Potential Conditional Mutual Information: Estimators, Properties and Applications. (arXiv:1710.05012v1 [cs.IT])", "published": 1508113977, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.05012"}], "alternate": [{"href": "http://arxiv.org/abs/1710.05012", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9e351bd\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9e351bd&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The conditional mutual information I(X;Y|Z) measures the average information \nthat X and Y contain about each other given Z. This is an important primitive \nin many learning problems including conditional independence testing, graphical \nmodel inference, causal strength estimation and time-series problems. In \nseveral applications, it is desirable to have a functional purely of the \nconditional distribution p_{Y|X,Z} rather than of the joint distribution \np_{X,Y,Z}. We define the potential conditional mutual information as the \nconditional mutual information calculated with a modified joint distribution \np_{Y|X,Z} q_{X,Z}, where q_{X,Z} is a potential distribution, fixed airport. We \ndevelop K nearest neighbor based estimators for this functional, employing \nimportance sampling, and a coupling trick, and prove the finite k consistency \nof such an estimator. We demonstrate that the estimator has excellent practical \nperformance and show an application in dynamical system inference. \n</p>"}, "author": "Arman Rahimzamani, Sreeram Kannan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708618", "id": "tag:google.com,2005:reader/item/000000031f9729ca", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Identifying On-time Reward Delivery Projects with Estimating Delivery Duration on Kickstarter. (arXiv:1710.04743v1 [cs.CY])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04743"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04743", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9ee1b9c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9ee1b9c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>In Crowdfunding platforms, people turn their prototype ideas into real \nproducts by raising money from the crowd, or invest in someone else's projects. \nIn reward-based crowdfunding platforms such as Kickstarter and Indiegogo, \nselecting accurate reward delivery duration becomes crucial for creators, \nbackers, and platform providers to keep the trust between the creators and the \nbackers, and the trust between the platform providers and users. According to \nKickstarter, 35% backers did not receive rewards on time. Unfortunately, little \nis known about on-time and late reward delivery projects, and there is no prior \nwork to estimate reward delivery duration. To fill the gap, in this paper, we \n(i) extract novel features that reveal latent difficulty levels of project \nrewards; (ii) build predictive models to identify whether a creator will \ndeliver all rewards in a project on time or not; and (iii) build a regression \nmodel to estimate accurate reward delivery duration (i.e., how long it will \ntake to produce and deliver all the rewards). Experimental results show that \nour models achieve good performance -- 82.5% accuracy, 78.1 RMSE, and 0.108 \nNRMSE at the first 5% of the longest reward delivery duration. \n</p>"}, "author": "Thanh Tran, Kyumin Lee, Nguyen Vo, Hongkyu Choi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708617", "id": "tag:google.com,2005:reader/item/000000031f9729d1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "HyperENTM: Evolving Scalable Neural Turing Machines through HyperNEAT. (arXiv:1710.04748v1 [cs.AI])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04748"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04748", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recent developments within memory-augmented neural networks have solved \nsequential problems requiring long-term memory, which are intractable for \ntraditional neural networks. However, current approaches still struggle to \nscale to large memory sizes and sequence lengths. In this paper we show how \naccess to memory can be encoded geometrically through a HyperNEAT-based Neural \nTuring Machine (HyperENTM). We demonstrate that using the indirect HyperNEAT \nencoding allows for training on small memory vectors in a bit-vector copy task \nand then applying the knowledge gained from such training to speed up training \non larger size memory vectors. Additionally, we demonstrate that in some \ninstances, networks trained to copy bit-vectors of size 9 can be scaled to \nsizes of 1,000 without further training. While the task in this paper is \nsimple, these results could open up the problems amendable to networks with \nexternal memories to problems with larger memory vectors and theoretically \nunbounded memory sizes. \n</p>"}, "author": "Jakob Merrild, Mikkel Angaju Rasmussen, Sebastian Risi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708616", "id": "tag:google.com,2005:reader/item/000000031f9729d7", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Explaining Aviation Safety Incidents Using Deep Learned Precursors. (arXiv:1710.04749v1 [cs.CV])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04749"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04749", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Although aviation accidents are rare, safety incidents occur more frequently \nand require careful analysis for providing actionable recommendations to \nimprove safety. Automatically analyzing safety incidents using flight data is \nchallenging because of the absence of labels on timestep-wise events in a \nflight, complexity of multi-dimensional data, and lack of scalable tools to \nperform analysis over large number of events. In this work, we propose a \nprecursor mining algorithm that identifies correlated patterns in \nmultidimensional time series to explain an adverse event. Precursors are \nvaluable to systems health and safety monitoring in explaining and forecasting \nanomalies. Current precursor mining methods suffer from poor scalability to \nhigh dimensional time series data and in capturing long-term memory. We propose \nan approach by combining multiple-instance learning (MIL) and deep recurrent \nneural networks (DRNN) to take advantage of MIL's ability to model \nweakly-supervised data and DRNN's ability to model long term memory processes, \nto scale well to high dimensional data and to large volumes of data using GPU \nparallelism. We apply the proposed method to find precursors and offer \nexplanations to high speed exceedance safety incidents using commercial flight \ndata. \n</p>"}, "author": "Vijay Manikandan Janakiraman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708615", "id": "tag:google.com,2005:reader/item/000000031f9729de", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Hypernetworks. (arXiv:1710.04759v1 [stat.ML])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04759"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04759", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose Bayesian hypernetworks: a framework for approximate Bayesian \ninference in neural networks. A Bayesian hypernetwork, $h$, is a neural network \nwhich learns to transform a simple noise distribution, $p(\\epsilon) = \n\\mathcal{N}(0,I)$, to a distribution $q(\\theta) \\doteq q(h(\\epsilon))$ over the \nparameters $\\theta$ of another neural network (the \"primary network\"). We train \n$q$ with variational inference, using an invertible $h$ to enable efficient \nestimation of the variational lower bound on the posterior $p(\\theta | \n\\mathcal{D})$ via sampling. In contrast to most methods for Bayesian deep \nlearning, Bayesian hypernets can represent a complex multimodal approximate \nposterior with correlations between parameters, while enabling cheap i.i.d. \nsampling of $q(\\theta)$. We demonstrate these qualitative advantages of \nBayesian hypernets, which also achieve competitive performance on a suite of \ntasks that demonstrate the advantage of estimating model uncertainty, including \nactive learning and anomaly detection. \n</p>"}, "author": "David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, Aaron Courville", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708614", "id": "tag:google.com,2005:reader/item/000000031f9729eb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Combinatorial Multi-armed Bandits for Real-Time Strategy Games. (arXiv:1710.04805v1 [cs.AI])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04805"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04805", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Games with large branching factors pose a significant challenge for game tree \nsearch algorithms. In this paper, we address this problem with a sampling \nstrategy for Monte Carlo Tree Search (MCTS) algorithms called {\\em na\\\"{i}ve \nsampling}, based on a variant of the Multi-armed Bandit problem called {\\em \nCombinatorial Multi-armed Bandits} (CMAB). We analyze the theoretical \nproperties of several variants of {\\em na\\\"{i}ve sampling}, and empirically \ncompare it against the other existing strategies in the literature for CMABs. \nWe then evaluate these strategies in the context of real-time strategy (RTS) \ngames, a genre of computer games characterized by their very large branching \nfactors. Our results show that as the branching factor grows, {\\em na\\\"{i}ve \nsampling} outperforms the other sampling strategies. \n</p>"}, "author": "Santiago Onta&#xf1;&#xf3;n", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708613", "id": "tag:google.com,2005:reader/item/000000031f9729f2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning for Case-based Reasoning through Prototypes: A Neural Network that Explains its Predictions. (arXiv:1710.04806v1 [cs.AI])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04806"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04806", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks are widely used for classification. These deep models \noften suffer from a lack of interpretability -- they are particularly difficult \nto understand because of their non-linear nature. As a result, neural networks \nare often treated as \"black box\" models, and in the past, have been trained \npurely to optimize the accuracy of predictions. In this work, we create a novel \nnetwork architecture for deep learning that naturally explains its own \nreasoning for each prediction. This architecture contains an autoencoder and a \nspecial prototype layer, where each unit of that layer stores a weight vector \nthat resembles an encoded training input. The encoder of the autoencoder allows \nus to do comparisons within the latent space, while the decoder allows us to \nvisualize the learned prototypes. The training objective has four terms: an \naccuracy term, a term that encourages every prototype to be similar to at least \none encoded input, a term that encourages every encoded input to be close to at \nleast one prototype, and a term that encourages faithful reconstruction by the \nautoencoder. The distances computed in the prototype layer are used as part of \nthe classification process. Since the prototypes are learned during training, \nthe learned network naturally comes with explanations for each prediction, and \nthe explanations are loyal to what the network actually computes. \n</p>"}, "author": "Oscar Li, Hao Liu, Chaofan Chen, Cynthia Rudin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708612", "id": "tag:google.com,2005:reader/item/000000031f9729fc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast Top-$\\boldsymbol{k}$ Area Topics Extraction with Knowledge Base. (arXiv:1710.04822v1 [cs.AI])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04822"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04822", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>What are the most popular research topics in Artificial Intelligence (AI)? We \nformulate the problem as extracting top-$k$ topics that can best represent a \ngiven area with the help of knowledge base. We theoretically prove that the \nproblem is NP-hard and propose an optimization model, FastKATE, to address this \nproblem by combining both explicit and latent representations for each topic. \nWe leverage a large-scale knowledge base (Wikipedia) to generate topic \nembeddings using neural networks and use this kind of representations to help \ncapture the representativeness of topics for given areas. We develop a fast \nheuristic algorithm to efficiently solve the problem with a provable error \nbound. We evaluate the proposed model on three real-world datasets. \nExperimental results demonstrate our model's effectiveness, robustness, \nreal-timeness (return results in $&lt;1$s), and its superiority over several \nalternative methods. \n</p>"}, "author": "Fang Zhang, Xiaochen Wang, Jingfei Han, Jie Tang, Shiyin Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708611", "id": "tag:google.com,2005:reader/item/000000031f972a03", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Recent Advances in Zero-shot Recognition. (arXiv:1710.04837v1 [cs.CV])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04837"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04837", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>With the recent renaissance of deep convolution neural networks, encouraging \nbreakthroughs have been achieved on the supervised recognition tasks, where \neach class has sufficient training data and fully annotated training data. \nHowever, to scale the recognition to a large number of classes with few or now \ntraining samples for each class remains an unsolved problem. One approach to \nscaling up the recognition is to develop models capable of recognizing unseen \ncategories without any training instances, or zero-shot recognition/ learning. \nThis article provides a comprehensive review of existing zero-shot recognition \ntechniques covering various aspects ranging from representations of models, and \nfrom datasets and evaluation settings. We also overview related recognition \ntasks including one-shot and open set recognition which can be used as natural \nextensions of zero-shot recognition when limited number of class samples become \navailable or when zero-shot recognition is implemented in a real-world setting. \nImportantly, we highlight the limitations of existing approaches and point out \nfuture research directions in this existing new research area. \n</p>"}, "author": "Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue, Leonid Sigal, Shaogang Gong", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1508113962709", "timestampUsec": "1508113962708610", "id": "tag:google.com,2005:reader/item/000000031f972a0d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Two-stage Algorithm for Fairness-aware Machine Learning. (arXiv:1710.04924v1 [stat.ML])", "published": 1508113963, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04924"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04924", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Algorithmic decision making process now affects many aspects of our lives. \nStandard tools for machine learning, such as classification and regression, are \nsubject to the bias in data, and thus direct application of such off-the-shelf \ntools could lead to a specific group being unfairly discriminated. Removing \nsensitive attributes of data does not solve this problem because a \n\\textit{disparate impact} can arise when non-sensitive attributes and sensitive \nattributes are correlated. Here, we study a fair machine learning algorithm \nthat avoids such a disparate impact when making a decision. Inspired by the \ntwo-stage least squares method that is widely used in the field of economics, \nwe propose a two-stage algorithm that removes bias in the training data. The \nproposed algorithm is conceptually simple. Unlike most of existing fair \nalgorithms that are designed for classification tasks, the proposed method is \nable to (i) deal with regression tasks, (ii) combine explanatory attributes to \nremove reverse discrimination, and (iii) deal with numerical sensitive \nattributes. The performance and fairness of the proposed algorithm are \nevaluated in simulations with synthetic and real-world datasets. \n</p>"}, "author": "Junpei Komiyama, Hajime Shimao", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507857538695", "timestampUsec": "1507857538695388", "id": "tag:google.com,2005:reader/item/000000031dfa2647", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sequence stacking using dual encoder Seq2Seq recurrent networks. (arXiv:1710.04211v1 [cs.LG])", "published": 1507857539, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04211"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04211", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9ee1f9e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9ee1f9e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>A widely studied non-polynomial (NP) hard problem lies in finding a route \nbetween the two nodes of a graph. Often meta-heuristics algorithms such as \n$A^{*}$ are employed on graphs with a large number of nodes. Here, we propose a \ndeep recurrent neural network architecture based on the Sequence-2-Sequence \nmodel, widely used, for instance in text translation. Particularly, we \nillustrate that utilising a context vector that has been learned from two \ndifferent recurrent networks enables increased accuracies in learning the \nshortest route of a graph. Additionally, we show that one can boost the \nperformance of the Seq2Seq network by smoothing the loss function using a \nhomotopy continuation of the decoder's loss function. \n</p>"}, "author": "Alessandro Bay, Biswa Sengupta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507857538695", "timestampUsec": "1507857538695387", "id": "tag:google.com,2005:reader/item/000000031dfa264c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sum-Product-Quotient Networks. (arXiv:1710.04404v1 [cs.LG])", "published": 1507857539, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04404"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04404", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a novel tractable generative model that extends Sum-Product \nNetworks (SPNs) and significantly boost their power. We call it \nSum-Product-Quotient Networks (SPQNs), whose core concept is to incorporate \nconditional distributions into the model by direct computation using quotient \nnodes, e.g. $P(A|B){=}\\frac{P(A,B)}{P(B)}$. We provide sufficient conditions \nfor the tractability of SPQNs that generalize and relax the decomposable and \ncomplete tractability conditions of SPNs. These relaxed conditions give rise to \nan exponential boost to the expressive efficiency of our model, i.e. we prove \nthat there are distributions which SPQNs can compute efficiently but require \nSPNs to be of exponential size. Thus, we narrow the gap in expressivity between \ntractable graphical models and other Neural Network-based generative models. \n</p>"}, "author": "Or Sharir, Amnon Shashua", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.NE", "title": "Neural &amp; Evolutionary", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505867", "id": "tag:google.com,2005:reader/item/000000031df63b5c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Measurement Context Extraction from Text: Discovering Opportunities and Gaps in Earth Science. (arXiv:1710.04312v1 [cs.IR])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04312"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04312", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose Marve, a system for extracting measurement values, units, and \nrelated words from natural language text. Marve uses conditional random fields \n(CRF) to identify measurement values and units, followed by a rule-based system \nto find related entities, descriptors and modifiers within a sentence. Sentence \ntokens are represented by an undirected graphical model, and rules are based on \npart-of-speech and word dependency patterns connecting values and units to \ncontextual words. Marve is unique in its focus on measurement context and early \nexperimentation demonstrates Marve's ability to generate high-precision \nextractions with strong recall. We also discuss Marve's role in refining \nmeasurement requirements for NASA's proposed HyspIRI mission, a hyperspectral \ninfrared imaging satellite that will study the world's ecosystems. In general, \nour work with HyspIRI demonstrates the value of semantic measurement \nextractions in characterizing quantitative discussion contained in large \ncorpuses of natural language text. These extractions accelerate broad, \ncross-cutting research and expose scientists new algorithmic approaches and \nexperimental nuances. They also facilitate identification of scientific \nopportunities enabled by HyspIRI leading to more efficient scientific \ninvestment and research. \n</p>"}, "author": "Kyle Hundman, Chris A. Mattmann", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505866", "id": "tag:google.com,2005:reader/item/000000031df63b65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Explaining Trained Neural Networks with Semantic Web Technologies: First Steps. (arXiv:1710.04324v1 [cs.AI])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04324"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04324", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The ever increasing prevalence of publicly available structured data on the \nWorld Wide Web enables new applications in a variety of domains. In this paper, \nwe provide a conceptual approach that leverages such data in order to explain \nthe input-output behavior of trained artificial neural networks. We apply \nexisting Semantic Web technologies in order to provide an experimental proof of \nconcept. \n</p>"}, "author": "Md Kamruzzaman Sarker, Ning Xie, Derek Doran, Michael Raymer, Pascal Hitzler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505865", "id": "tag:google.com,2005:reader/item/000000031df63b71", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "DisSent: Sentence Representation Learning from Explicit Discourse Relations. (arXiv:1710.04334v1 [cs.CL])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04334"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04334", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sentence vectors represent an appealing approach to meaning: learn an \nembedding that encompasses the meaning of a sentence in a single vector, that \ncan be used for a variety of semantic tasks. Existing models for learning \nsentence embeddings either require extensive computational resources to train \non large corpora, or are trained on costly, manually curated datasets of \nsentence relations. We observe that humans naturally annotate the relations \nbetween their sentences with discourse markers like \"but\" and \"because\". These \nwords are deeply linked to the meanings of the sentences they connect. Using \nthis natural signal, we automatically collect a classification dataset from \nunannotated text. Training a model to predict these discourse markers yields \nhigh quality sentence embeddings. Our model captures complementary information \nto existing models and achieves comparable generalization performance to state \nof the art models. \n</p>"}, "author": "Allen Nie, Erin D. Bennett, Noah D. Goodman", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505864", "id": "tag:google.com,2005:reader/item/000000031df63b79", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sign-Constrained Regularized Loss Minimization. (arXiv:1710.04380v1 [cs.LG])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04380"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04380", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In practical analysis, domain knowledge about analysis target has often been \naccumulated, although, typically, such knowledge has been discarded in the \nstatistical analysis stage, and the statistical tool has been applied as a \nblack box. In this paper, we introduce sign constraints that are a handy and \nsimple representation for non-experts in generic learning problems. We have \ndeveloped two new optimization algorithms for the sign-constrained regularized \nloss minimization, called the sign-constrained Pegasos (SC-Pega) and the \nsign-constrained SDCA (SC-SDCA), by simply inserting the sign correction step \ninto the original Pegasos and SDCA, respectively. We present theoretical \nanalyses that guarantee that insertion of the sign correction step does not \ndegrade the convergence rate for both algorithms. Two applications, where the \nsign-constrained learning is effective, are presented. The one is exploitation \nof prior information about correlation between explanatory variables and a \ntarget variable. The other is introduction of the sign-constrained to \nSVM-Pairwise method. Experimental results demonstrate significant improvement \nof generalization performance by introducing sign constraints in both \napplications. \n</p>"}, "author": "Tsuyoshi Kato, Misato Kobayashi, Daisuke Sano", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505863", "id": "tag:google.com,2005:reader/item/000000031df63b87", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Marginal sequential Monte Carlo for doubly intractable models. (arXiv:1710.04382v1 [stat.CO])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04382"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04382", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Bayesian inference for models that have an intractable partition function is \nknown as a doubly intractable problem, where standard Monte Carlo methods are \nnot applicable. The past decade has seen the development of auxiliary variable \nMonte Carlo techniques (M{\\o}ller et al., 2006; Murray et al., 2006) for \ntackling this problem; these approaches being members of the more general class \nof pseudo-marginal, or exact-approximate, Monte Carlo algorithms (Andrieu and \nRoberts, 2009), which make use of unbiased estimates of intractable posteriors. \nEveritt et al. (2017) investigated the use of exact-approximate importance \nsampling (IS) and sequential Monte Carlo (SMC) in doubly intractable problems, \nbut focussed only on SMC algorithms that used data-point tempering. This paper \ndescribes SMC samplers that may use alternative sequences of distributions, and \ndescribes ways in which likelihood estimates may be improved adaptively as the \nalgorithm progresses, building on ideas from Moores et al. (2015). This \napproach is compared with a number of alternative algorithms for doubly \nintractable problems, including approximate Bayesian computation (ABC), which \nwe show is closely related to the method of M{\\o}ller et al. (2006). \n</p>"}, "author": "Richard G. Everitt, Dennis Prangle, Philip Maybank, Mark Bell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505862", "id": "tag:google.com,2005:reader/item/000000031df63b93", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Arguing Machines: Perception-Control System Redundancy and Edge Case Discovery in Real-World Autonomous Driving. (arXiv:1710.04459v1 [cs.AI])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04459"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04459", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Safe autonomous driving may be one of the most difficult engineering \nchallenges that any artificial intelligence system has been asked to do since \nthe birth of AI over sixty years ago. The difficulty is not within the task \nitself, but rather in the extremely small margin of allowable error given the \nhuman life at stake and the extremely large number of edge cases that have to \nbe accounted for. In other words, we task these systems to expect the \nunexpected with near 100% accuracy, which is a technical challenge for machine \nlearning methods that to date have generally been better at memorizing the \nexpected than predicting the unexpected. In fact, the process of efficiently \nand automatically discovering the edge cases of driving may be the key to \nsolving this engineering challenge. In this work, we propose and evaluate a \nmethod for discovering edge cases by monitoring the disagreement between two \nmonocular-vision-based automated steering systems. The first is a proprietary \nTesla Autopilot system equipped in the first generation of Autopilot-capable \nvehicles. The second is a end-to-end neural network trained on a large-scale \nnaturalistic dataset of 420 hours or 45 million frames of autonomous driving in \nTesla vehicles. \n</p>"}, "author": "Lex Fridman, Benedikt Jenik, Bryan Reimer", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505861", "id": "tag:google.com,2005:reader/item/000000031df63b9e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Clusters of Driving Behavior from Observational Smartphone Data. (arXiv:1710.04502v2 [cs.AI] UPDATED)", "published": 1509669621, "updated": 1509669622, "canonical": [{"href": "http://arxiv.org/abs/1710.04502"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04502", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Understanding driving behaviors is essential for improving safety and \nmobility of our transportation systems. Data is usually collected via \nsimulator-based studies or naturalistic driving studies. Those techniques allow \nfor understanding relations between demographics, road conditions and safety. \nOn the other hand, they are very costly and time consuming. Thanks to the \nsmartphone data, we have an opportunity to substantially complement more \ntraditional data collection techniques with data extracted from phone sensors, \nsuch as GPS, accelerometer gyroscope and camera. We developed statistical \nmodels that provided insight into driver behavior in the San Francisco metro \narea based on tens of thousands of driver logs. We used a novel data source to \nsupport our work. We used cell phone sensor data drawn from five hundred \ndrivers in San Francisco to understand the speed of traffic across the city as \nwell as the maneuvers of drivers in different areas. Specifically we clustered \ndrivers based on the way they drove around the city. We looked at driver norms \nby street and flagged driving behaviors that deviated from the norm. \n</p>"}, "author": "Josh Warren, Jeff Lipkowitz, Vadim Sokolov", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505860", "id": "tag:google.com,2005:reader/item/000000031df63ba4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Is Epicurus the father of Reinforcement Learning?. (arXiv:1710.04582v1 [cs.LG])", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04582"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04582", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Epicurean Philosophy is commonly thought as simplistic and hedonistic. \nHere I discuss how this is a misconception and explore its link to \nReinforcement Learning. Based on the letters of Epicurus, I construct an \nobjective function for hedonism which turns out to be equivalent of the \nReinforcement Learning objective function when omitting the discount factor. I \nthen discuss how Plato and Aristotle 's views that can be also loosely linked \nto Reinforcement Learning, as well as their weaknesses in relationship to it. \nFinally, I emphasise the close affinity of the Epicurean views and the Bellman \nequation. \n</p>"}, "author": "Eleni Vasilaki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505859", "id": "tag:google.com,2005:reader/item/000000031df63baa", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Scalable Spectral Clustering via Spectrum-Preserving Sparsification. (arXiv:1710.04584v2 [cs.LG] UPDATED)", "published": 1510067299, "updated": 1510067302, "canonical": [{"href": "http://arxiv.org/abs/1710.04584"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04584", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9ee2321\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9ee2321&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The eigendeomposition of nearest-neighbor (NN) graph Laplacian matrices is \nthe main computational bottleneck in spectral clustering. In this work, we \nintroduce a highly-scalable, spectrum-preserving graph sparsification algorithm \nthat enables to build ultra-sparse NN (u-NN) graphs with guaranteed \npreservation of the original graph spectrums, such as the first few \neigenvectors of the original graph Laplacian. Our approach can immediately lead \nto scalable spectral clustering of large data networks without sacrificing \nsolution quality. The proposed method starts from constructing low-stretch \nspanning trees (LSSTs) from the original graphs, which is followed by \niteratively recovering small portions of \"spectrally critical\" off-tree edges \nto the LSSTs by leveraging a spectral off-tree embedding scheme. To determine \nthe suitable amount of off-tree edges to be recovered to the LSSTs, an \neigenvalue stability checking scheme is proposed, which enables to robustly \npreserve the first few Laplacian eigenvectors within the sparsified graph. \nAdditionally, an incremental graph densification scheme is proposed for \nidentifying extra edges that have been missing in the original NN graphs but \ncan still play important roles in spectral clustering tasks. Our experimental \nresults for a variety of well-known data sets show that the proposed method can \ndramatically reduce the complexity of NN graphs, leading to significant \nspeedups in spectral clustering. \n</p>"}, "author": "Yongyu Wang, Zhuo Feng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505854", "id": "tag:google.com,2005:reader/item/000000031df63bba", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Construction of FuzzyFind Dictionary using Golay Coding Transformation for Searching Applications. (arXiv:1503.06483v1 [cs.DB] CROSS LISTED)", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1503.06483"}], "alternate": [{"href": "http://arxiv.org/abs/1503.06483", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9fa2e44\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9fa2e44&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Searching through a large volume of data is very critical for companies, \nscientists, and searching engines applications due to time complexity and \nmemory complexity. In this paper, a new technique of generating FuzzyFind \nDictionary for text mining was introduced. We simply mapped the 23 bits of the \nEnglish alphabet into a FuzzyFind Dictionary or more than 23 bits by using more \nFuzzyFind Dictionary, and reflecting the presence or absence of particular \nletters. This representation preserves closeness of word distortions in terms \nof closeness of the created binary vectors within Hamming distance of 2 \ndeviations. This paper talks about the Golay Coding Transformation Hash Table \nand how it can be used on a FuzzyFind Dictionary as a new technology for using \nin searching through big data. This method is introduced by linear time \ncomplexity for generating the dictionary and constant time complexity to access \nthe data and update by new data sets, also updating for new data sets is linear \ntime depends on new data points. This technique is based on searching only for \nletters of English that each segment has 23 bits, and also we have more than \n23-bit and also it could work with more segments as reference table. \n</p>"}, "author": "Kamran Kowsari, Maryam Yammahi, Nima Bari, Roman Vichr, Faisal Alsaby, Simon Y. Berkovich", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854909506", "timestampUsec": "1507854909505853", "id": "tag:google.com,2005:reader/item/000000031df63bc2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Synkhronos: a Multi-GPU Theano Extension for Data Parallelism. (arXiv:1710.04162v1 [cs.DC] CROSS LISTED)", "published": 1507854910, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04162"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04162", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present Synkhronos, an extension to Theano for multi-GPU computations \nleveraging data parallelism. Our framework provides automated execution and \nsynchronization across devices, allowing users to continue to write serial \nprograms without risk of race conditions. The NVIDIA Collective Communication \nLibrary is used for high-bandwidth inter-GPU communication. Further \nenhancements to the Theano function interface include input slicing (with \naggregation) and input indexing, which perform common data-parallel computation \npatterns efficiently. One example use case is synchronous SGD, which has \nrecently been shown to scale well for a growing set of deep learning problems. \nWhen training ResNet-50, we achieve a near-linear speedup of 7.5x on an NVIDIA \nDGX-1 using 8 GPUs, relative to Theano-only code running a single GPU in \nisolation. Yet Synkhronos remains general to any data-parallel computation \nprogrammable in Theano. By implementing parallelism at the level of individual \nTheano functions, our framework uniquely addresses a niche between manual \nmulti-device programming and prescribed multi-GPU training routines. \n</p>"}, "author": "Adam Stooke, Pieter Abbeel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/cs.AI", "title": "Artificial Intelligence", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714504", "id": "tag:google.com,2005:reader/item/000000031df6107d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sequence stacking using dual encoder Seq2Seq recurrent networks. (arXiv:1710.04211v1 [cs.LG])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04211"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04211", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A widely studied non-polynomial (NP) hard problem lies in finding a route \nbetween the two nodes of a graph. Often meta-heuristics algorithms such as \n$A^{*}$ are employed on graphs with a large number of nodes. Here, we propose a \ndeep recurrent neural network architecture based on the Sequence-2-Sequence \nmodel, widely used, for instance in text translation. Particularly, we \nillustrate that utilising a context vector that has been learned from two \ndifferent recurrent networks enables increased accuracies in learning the \nshortest route of a graph. Additionally, we show that one can boost the \nperformance of the Seq2Seq network by smoothing the loss function using a \nhomotopy continuation of the decoder's loss function. \n</p>"}, "author": "Alessandro Bay, Biswa Sengupta", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714503", "id": "tag:google.com,2005:reader/item/000000031df610a0", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Maximum Margin Interval Trees. (arXiv:1710.04234v2 [stat.ML] UPDATED)", "published": 1509323746, "updated": 1509323757, "canonical": [{"href": "http://arxiv.org/abs/1710.04234"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04234", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning a regression function using censored or interval-valued output data \nis an important problem in fields such as genomics and medicine. The goal is to \nlearn a real-valued prediction function, and the training output labels \nindicate an interval of possible values. Whereas most existing algorithms for \nthis task are linear models, in this paper we investigate learning nonlinear \ntree models. We propose to learn a tree by minimizing a margin-based \ndiscriminative objective function, and we provide a dynamic programming \nalgorithm for computing the optimal solution in log-linear time. We show \nempirically that this algorithm achieves state-of-the-art speed and prediction \naccuracy in a benchmark of several data sets. \n</p>"}, "author": "Alexandre Drouin, Toby Dylan Hocking, Fran&#xe7;ois Laviolette", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714502", "id": "tag:google.com,2005:reader/item/000000031df61101", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Local Convergence of Proximal Splitting Methods for Rank Constrained Problems. (arXiv:1710.04248v1 [math.OC])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04248"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04248", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We analyze the local convergence of proximal splitting algorithms to solve \noptimization problems that are convex besides a rank constraint. For this, we \nshow conditions under which the proximal operator of a function involving the \nrank constraint is locally identical to the proximal operator of its convex \nenvelope, hence implying local convergence. The conditions imply that the \nnon-convex algorithms locally converge to a solution whenever a convex \nrelaxation involving the convex envelope can be expected to solve the \nnon-convex problem. \n</p>"}, "author": "Christian Grussler, Pontus Giselsson", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714501", "id": "tag:google.com,2005:reader/item/000000031df61131", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Gradient Descent in Continuous Time: A Central Limit Theorem. (arXiv:1710.04273v2 [math.PR] UPDATED)", "published": 1509669387, "updated": 1509669413, "canonical": [{"href": "http://arxiv.org/abs/1710.04273"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04273", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Stochastic gradient descent in continuous time (SGDCT) provides a \ncomputationally efficient method for the statistical learning of \ncontinuous-time models, which are widely used in science, engineering, and \nfinance. The SGDCT algorithm follows a (noisy) descent direction along a \ncontinuous stream of data. The parameter updates occur in continuous time and \nsatisfy a stochastic differential equation. This paper analyzes the asymptotic \nconvergence rate of the SGDCT algorithm by proving a central limit theorem for \nstrongly convex objective functions and, under slightly stronger conditions, \nfor non-convex objective functions as well. An L$^p$ convergence rate is also \nproven for the algorithm in the strongly convex case. \n</p>"}, "author": "Justin Sirignano, Konstantinos Spiliopoulos", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714500", "id": "tag:google.com,2005:reader/item/000000031df61162", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improved Coresets for Kernel Density Estimates. (arXiv:1710.04325v1 [cs.LG])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04325"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04325", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the construction of coresets for kernel density estimates. That is \nwe show how to approximate the kernel density estimate described by a large \npoint set with another kernel density estimate with a much smaller point set. \nFor characteristic kernels (including Gaussian and Laplace kernels), our \napproximation preserves the $L_\\infty$ error between kernel density estimates \nwithin error $\\epsilon$, with coreset size $2/\\epsilon^2$, but no other aspects \nof the data, including the dimension, the diameter of the point set, or the \nbandwidth of the kernel common to other approximations. When the dimension is \nunrestricted, we show this bound is tight for these kernels as well as a much \nbroader set. \n</p> \n<p>This work provides a careful analysis of the iterative Frank-Wolfe algorithm \nadapted to this context, an algorithm called \\emph{kernel herding}. This \nanalysis unites a broad line of work that spans statistics, machine learning, \nand geometry. \n</p> \n<p>When the dimension $d$ is constant, we demonstrate much tighter bounds on the \nsize of the coreset specifically for Gaussian kernels, showing that it is \nbounded by the size of the coreset for axis-aligned rectangles. Currently the \nbest known constructive bound is $O(\\frac{1}{\\epsilon} \\log^d \n\\frac{1}{\\epsilon})$, and non-constructively, this can be improved by \n$\\sqrt{\\log \\frac{1}{\\epsilon}}$. This improves the best constant dimension \nbounds polynomially for $d \\geq 3$. \n</p>"}, "author": "Jeff M. Phillips, Wai Ming Tai", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714499", "id": "tag:google.com,2005:reader/item/000000031df6119c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "What Would a Graph Look Like in This Layout? A Machine Learning Approach to Large Graph Visualization. (arXiv:1710.04328v1 [cs.SI])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04328"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04328", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Using different methods for laying out a graph can lead to very different \nvisual appearances, with which the viewer perceives different information. \nSelecting a \"good\" layout method is thus important for visualizing a graph. The \nselection can be highly subjective and dependent on the given task. A common \napproach to selecting a good layout is to use aesthetic criteria and visual \ninspection. However, fully calculating various layouts and their associated \naesthetic metrics is computationally expensive. In this paper, we present a \nmachine learning approach to large graph visualization based on computing the \ntopological similarity of graphs using graph kernels. For a given graph, our \napproach can show what the graph would look like in different layouts and \nestimate their corresponding aesthetic metrics. An important contribution of \nour work is the development of a new framework to design graph kernels. Our \nexperimental study shows that our estimation calculation is considerably faster \nthan computing the actual layouts and their aesthetic metrics. Also, our graph \nkernels outperform the state-of-the-art ones in both time and accuracy. In \naddition, we conducted a user study to demonstrate that the topological \nsimilarity computed with our graph kernel matches perceptual similarity \nassessed by human users. \n</p>"}, "author": "Oh-Hyun Kwon, Tarik Crnovrsanin, Kwan-Liu Ma", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714498", "id": "tag:google.com,2005:reader/item/000000031df611f4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Efficient Data-Driven Geologic Feature Detection from Pre-stack Seismic Measurements using Randomized Machine-Learning Algorithm. (arXiv:1710.04329v1 [cs.LG])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04329"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04329", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Conventional seismic techniques for detecting the subsurface geologic \nfeatures are challenged by limited data coverage, computational inefficiency, \nand subjective human factors. We developed a novel data-driven geological \nfeature detection approach based on pre-stack seismic measurements. Our \ndetection method employs an efficient and accurate machine-learning detection \napproach to extract useful subsurface geologic features automatically. \nSpecifically, our method is based on kernel ridge regression model. The \nconventional kernel ridge regression can be computationally prohibited because \nof the large volume of seismic measurements. We employ a data reduction \ntechnique in combination with the conventional kernel ridge regression method \nto improve the computational efficiency and reduce memory usage. In particular, \nwe utilize a randomized numerical linear algebra technique, named Nystr\\\"om \nmethod, to effectively reduce the dimensionality of the feature space without \ncompromising the information content required for accurate detection. We \nprovide thorough computational cost analysis to show efficiency of our new \ngeological feature detection methods. We further validate the performance of \nour new subsurface geologic feature detection method using synthetic surface \nseismic data for 2D acoustic and elastic velocity models. Our numerical \nexamples demonstrate that our new detection method significantly improves the \ncomputational efficiency while maintaining comparable accuracy. Interestingly, \nwe show that our method yields a speed-up ratio on the order of $\\sim10^2$ to \n$\\sim 10^3$ in a multi-core computational environment. \n</p>"}, "author": "Youzuo Lin, Shusen Wang, Jayaraman Thiagarajan, George Guthrie, David Coblentz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714497", "id": "tag:google.com,2005:reader/item/000000031df612ce", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition. (arXiv:1710.04340v1 [cs.LG])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04340"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04340", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9fa32a9\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9fa32a9&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Spectral decomposition of the Koopman operator is attracting attention as a \ntool for the analysis of nonlinear dynamical systems. Dynamic mode \ndecomposition is a popular numerical algorithm for Koopman spectral analysis; \nhowever, we often need to prepare nonlinear observables manually according to \nthe underlying dynamics, which is not always possible since we may not have any \na priori knowledge about them. In this paper, we propose a fully data-driven \nmethod for Koopman spectral analysis based on the principle of learning Koopman \ninvariant subspaces from observed data. To this end, we propose minimization of \nthe residual sum of squares of linear least-squares regression to estimate a \nset of functions that transforms data into a form in which the linear \nregression fits well. We introduce an implementation with neural networks and \nevaluate performance empirically using nonlinear dynamical systems and \napplications. \n</p>"}, "author": "Naoya Takeishi, Yoshinobu Kawahara, Takehisa Yairi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714496", "id": "tag:google.com,2005:reader/item/000000031df61327", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Unified Neural Network Approach for Estimating Travel Time and Distance for a Taxi Trip. (arXiv:1710.04350v1 [stat.ML])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04350"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04350", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In building intelligent transportation systems such as taxi or rideshare \nservices, accurate prediction of travel time and distance is crucial for \ncustomer experience and resource management. Using the NYC taxi dataset, which \ncontains taxi trips data collected from GPS-enabled taxis [23], this paper \ninvestigates the use of deep neural networks to jointly predict taxi trip time \nand distance. We propose a model, called ST-NN (Spatio-Temporal Neural \nNetwork), which first predicts the travel distance between an origin and a \ndestination GPS coordinate, then combines this prediction with the time of day \nto predict the travel time. The beauty of ST-NN is that it uses only the raw \ntrips data without requiring further feature engineering and provides a joint \nestimate of travel time and distance. We compare the performance of ST-NN to \nthat of state-of-the-art travel time estimation methods, and we observe that \nthe proposed approach generalizes better than state-of-the-art methods. We show \nthat ST-NN approach significantly reduces the mean absolute error for both \npredicted travel time and distance, about 17% for travel time prediction. We \nalso observe that the proposed approach is more robust to outliers present in \nthe dataset by testing the performance of ST-NN on the datasets with and \nwithout outliers. \n</p>"}, "author": "Ishan Jindal, Tony (Zhiwei)Qin, Xuewen Chen, Matthew Nokleby, Jieping Ye", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804715", "timestampUsec": "1507854804714495", "id": "tag:google.com,2005:reader/item/000000031df61396", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning in Multiple Multistep Time Series Prediction. (arXiv:1710.04373v1 [stat.ML])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04373"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04373", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The project aims to research on combining deep learning specifically \nLong-Short Memory (LSTM) and basic statistics in multiple multistep time series \nprediction. LSTM can dive into all the pages and learn the general trends of \nvariation in a large scope, while the well selected medians for each page can \nkeep the special seasonality of different pages so that the future trend will \nnot fluctuate too much from the reality. A recent Kaggle competition on 145K \nWeb Traffic Time Series Forecasting [1] is used to thoroughly illustrate and \ntest this idea. \n</p>"}, "author": "Chuanyun Zang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714494", "id": "tag:google.com,2005:reader/item/000000031df613d8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Marginal sequential Monte Carlo for doubly intractable models. (arXiv:1710.04382v1 [stat.CO])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04382"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04382", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Bayesian inference for models that have an intractable partition function is \nknown as a doubly intractable problem, where standard Monte Carlo methods are \nnot applicable. The past decade has seen the development of auxiliary variable \nMonte Carlo techniques (M{\\o}ller et al., 2006; Murray et al., 2006) for \ntackling this problem; these approaches being members of the more general class \nof pseudo-marginal, or exact-approximate, Monte Carlo algorithms (Andrieu and \nRoberts, 2009), which make use of unbiased estimates of intractable posteriors. \nEveritt et al. (2017) investigated the use of exact-approximate importance \nsampling (IS) and sequential Monte Carlo (SMC) in doubly intractable problems, \nbut focussed only on SMC algorithms that used data-point tempering. This paper \ndescribes SMC samplers that may use alternative sequences of distributions, and \ndescribes ways in which likelihood estimates may be improved adaptively as the \nalgorithm progresses, building on ideas from Moores et al. (2015). This \napproach is compared with a number of alternative algorithms for doubly \nintractable problems, including approximate Bayesian computation (ABC), which \nwe show is closely related to the method of M{\\o}ller et al. (2006). \n</p>"}, "author": "Richard G. Everitt, Dennis Prangle, Philip Maybank, Mark Bell", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714493", "id": "tag:google.com,2005:reader/item/000000031df61434", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Sum-Product-Quotient Networks. (arXiv:1710.04404v1 [cs.LG])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04404"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04404", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We present a novel tractable generative model that extends Sum-Product \nNetworks (SPNs) and significantly boost their power. We call it \nSum-Product-Quotient Networks (SPQNs), whose core concept is to incorporate \nconditional distributions into the model by direct computation using quotient \nnodes, e.g. $P(A|B){=}\\frac{P(A,B)}{P(B)}$. We provide sufficient conditions \nfor the tractability of SPQNs that generalize and relax the decomposable and \ncomplete tractability conditions of SPNs. These relaxed conditions give rise to \nan exponential boost to the expressive efficiency of our model, i.e. we prove \nthat there are distributions which SPQNs can compute efficiently but require \nSPNs to be of exponential size. Thus, we narrow the gap in expressivity between \ntractable graphical models and other Neural Network-based generative models. \n</p>"}, "author": "Or Sharir, Amnon Shashua", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714492", "id": "tag:google.com,2005:reader/item/000000031df6146a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Self-Taught Support Vector Machine. (arXiv:1710.04450v1 [cs.CV])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04450"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04450", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, a new approach for classification of target task using limited \nlabeled target data as well as enormous unlabeled source data is proposed which \nis called self-taught learning. The target and source data can be drawn from \ndifferent distributions. In the previous approaches, covariate shift assumption \nis considered where the marginal distributions p(x) change over domains and the \nconditional distributions p(y|x) remain the same. In our approach, we propose a \nnew objective function which simultaneously learns a common space T(.) where \nthe conditional distributions over domains p(T(x)|y) remain the same and learns \nrobust SVM classifiers for target task using both source and target data in the \nnew representation. Hence, in the proposed objective function, the hidden label \nof the source data is also incorporated. We applied the proposed approach on \nCaltech-256, MSRC+LMO datasets and compared the performance of our algorithm to \nthe available competing methods. Our method has a superior performance to the \nsuccessful existing algorithms. \n</p>"}, "author": "Parvin Razzaghi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714491", "id": "tag:google.com,2005:reader/item/000000031df61498", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "An Improved Naive Bayes Classifier-based Noise Detection Technique for Classifying User Phone Call Behavior. (arXiv:1710.04461v1 [cs.LG])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04461"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04461", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The presence of noisy instances in mobile phone data is a fundamental issue \nfor classifying user phone call behavior (i.e., accept, reject, missed and \noutgoing), with many potential negative consequences. The classification \naccuracy may decrease and the complexity of the classifiers may increase due to \nthe number of redundant training samples. To detect such noisy instances from a \ntraining dataset, researchers use naive Bayes classifier (NBC) as it identifies \nmisclassified instances by taking into account independence assumption and \nconditional probabilities of the attributes. However, some of these \nmisclassified instances might indicate usages behavioral patterns of individual \nmobile phone users. Existing naive Bayes classifier based noise detection \ntechniques have not considered this issue and, thus, are lacking in \nclassification accuracy. \n</p> \n<p>In this paper, we propose an improved noise detection technique based on \nnaive Bayes classifier for effectively classifying users' phone call behaviors. \nIn order to improve the classification accuracy, we effectively identify noisy \ninstances from the training dataset by analyzing the behavioral patterns of \nindividuals. We dynamically determine a noise threshold according to \nindividual's unique behavioral patterns by using both the naive Bayes \nclassifier and Laplace estimator. We use this noise threshold to identify noisy \ninstances. To measure the effectiveness of our technique in classifying user \nphone call behavior, we employ the most popular classification algorithm (e.g., \ndecision tree). Experimental results on the real phone call log dataset show \nthat our proposed technique more accurately identifies the noisy instances from \nthe training datasets that leads to better classification accuracy. \n</p>"}, "author": "Iqbal H. Sarker, Muhammad Ashad Kabir, Alan Colman, Jun Han", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714490", "id": "tag:google.com,2005:reader/item/000000031df614e8", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Effects of Images with Different Levels of Familiarity on EEG. (arXiv:1710.04462v1 [stat.ML])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04462"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04462", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Evaluating human brain potentials during watching different images can be \nused for memory evaluation, information retrieving, guilty-innocent \nidentification and examining the brain response. In this study, the effects of \nwatching images, with different levels of familiarity, on subjects' \nElectroencephalogram (EEG) have been studied. Three different groups of images \nwith three familiarity levels of \"unfamiliar\", \"familiar\" and \"very familiar\" \nhave been considered for this study. EEG signals of 21 subjects (14 men) were \nrecorded. After signal acquisition, pre-processing, including noise and \nartifact removal, were performed on epochs of data. Features, including \nspatial-statistical, wavelet, frequency and harmonic parameters, and also \ncorrelation between recording channels, were extracted from the data. Then, we \nevaluated the efficiency of the extracted features by using p-value and also an \northogonal feature selection method (combination of Gram-Schmitt method and \nFisher discriminant ratio) for feature dimensional reduction. As the final step \nof feature selection, we used 'add-r take-away l' method for choosing the most \ndiscriminative features. For data classification, including all two-class and \nthree-class cases, we applied Support Vector Machine (SVM) on the extracted \nfeatures. The correct classification rates (CCR) for \"unfamiliar-familiar\", \n\"unfamiliar-very familiar\" and \"familiar-very familiar\" cases were 85.6%, \n92.6%, and 70.6%, respectively. The best results of classifications were \nobtained in pre-frontal and frontal regions of brain. Also, wavelet, frequency \nand harmonic features were among the most discriminative features. Finally, in \nthree-class case, the best CCR was 86.8%. \n</p>"}, "author": "Ali Saeedi, Ehsan Arbabi", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714489", "id": "tag:google.com,2005:reader/item/000000031df61525", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Dimensionality Reduction Ensembles. (arXiv:1710.04484v1 [stat.ML])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04484"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04484", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Ensemble learning has had many successes in supervised learning, but it has \nbeen rare in unsupervised learning and dimensionality reduction. This study \nexplores dimensionality reduction ensembles, using principal component analysis \nand manifold learning techniques to capture linear, nonlinear, local, and \nglobal features in the original dataset. Dimensionality reduction ensembles are \ntested first on simulation data and then on two real medical datasets using \nrandom forest classifiers; results suggest the efficacy of this approach, with \naccuracies approaching that of the full dataset. Limitations include \ncomputational cost of some algorithms with strong performance, which may be \nameliorated through distributed computing and the development of more efficient \nversions of these algorithms. \n</p>"}, "author": "Colleen M. Farrelly", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714488", "id": "tag:google.com,2005:reader/item/000000031df6156a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multimodal Observation and Interpretation of Subjects Engaged in Problem Solving. (arXiv:1710.04486v1 [cs.HC])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04486"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04486", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper we present the first results of a pilot experiment in the \ncapture and interpretation of multimodal signals of human experts engaged in \nsolving challenging chess problems. Our goal is to investigate the extent to \nwhich observations of eye-gaze, posture, emotion and other physiological \nsignals can be used to model the cognitive state of subjects, and to explore \nthe integration of multiple sensor modalities to improve the reliability of \ndetection of human displays of awareness and emotion. We observed chess players \nengaged in problems of increasing difficulty while recording their behavior. \nSuch recordings can be used to estimate a participant's awareness of the \ncurrent situation and to predict ability to respond effectively to challenging \nsituations. Results show that a multimodal approach is more accurate than a \nunimodal one. By combining body posture, visual attention and emotion, the \nmultimodal approach can reach up to 93% of accuracy when determining player's \nchess expertise while unimodal approach reaches 86%. Finally this experiment \nvalidates the use of our equipment as a general and reproducible tool for the \nstudy of participants engaged in screen-based interaction and/or problem \nsolving. \n</p>"}, "author": "Thomas Guntz (LIG), Raffaella Balzarini (LIG), Dominique Vaufreydaz (LIG, UGA), James L. Crowley (Grenoble INP, LIG)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1507854804714", "timestampUsec": "1507854804714487", "id": "tag:google.com,2005:reader/item/000000031df615a3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Subjectively Interesting Subgroup Discovery on Real-valued Targets. (arXiv:1710.04521v1 [stat.ML])", "published": 1507854804, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.04521"}], "alternate": [{"href": "http://arxiv.org/abs/1710.04521", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a1b3c9fa3722\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a1b3c9fa3722&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deriving insights from high-dimensional data is one of the core problems in \ndata mining. The difficulty mainly stems from the fact that there are \nexponentially many variable combinations to potentially consider, and there are \ninfinitely many if we consider weighted combinations, even for linear \ncombinations. Hence, an obvious question is whether we can automate the search \nfor interesting patterns and visualizations. In this paper, we consider the \nsetting where a user wants to learn as efficiently as possible about \nreal-valued attributes. For example, to understand the distribution of crime \nrates in different geographic areas in terms of other (numerical, ordinal \nand/or categorical) variables that describe the areas. We introduce a method to \nfind subgroups in the data that are maximally informative (in the formal \nInformation Theoretic sense) with respect to a single or set of real-valued \ntarget attributes. The subgroup descriptions are in terms of a succinct set of \narbitrarily-typed other attributes. The approach is based on the Subjective \nInterestingness framework FORSIED to enable the use of prior knowledge when \nfinding most informative non-redundant patterns, and hence the method also \nsupports iterative data mining. \n</p>"}, "author": "Jefrey Lijffijt, Bo Kang, Wouter Duivesteijn, Kai Puolam&#xe4;ki, Emilia Oikarinen, Tijl De Bie", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/http://arxiv.org/rss/stat.ML", "title": "Machine Learning", "htmlUrl": "http://arxiv.org/"}}]}