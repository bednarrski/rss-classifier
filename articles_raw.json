{"all_articles": [{"crawlTimeMsec": "1517575665641", "timestampUsec": "1517575665641110", "id": "tag:google.com,2005:reader/item/0000000368815327", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Machine learning and evolutionary techniques in interplanetary trajectory design. (arXiv:1802.00180v1 [cs.NE])", "published": 1517575665, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00180"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00180", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a746af567170\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a746af567170&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>After providing a brief historical overview on the synergies between \nartificial intelligence research, in the areas of evolutionary computations and \nmachine learning, and the optimal design of interplanetary trajectories, we \npropose and study the use of deep artificial neural networks to represent, \non-board, the optimal guidance profile of an interplanetary mission. The \nresults, limited to the chosen test case of an Earth-Mars orbital transfer, \nextend the findings made previously for landing scenarios and quadcopter \ndynamics, opening a new research area in interplanetary trajectory planning. \n</p>"}, "author": "Dario Izzo, Christopher Sprague, Dharmesh Tailor", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517575665641", "timestampUsec": "1517575665641106", "id": "tag:google.com,2005:reader/item/0000000368815331", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Classifying medical notes into standard disease codes using Machine Learning. (arXiv:1802.00382v1 [cs.AI])", "published": 1517575665, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00382"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00382", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We investigate the automatic classification of patient discharge notes into \nstandard disease labels. We find that Convolutional Neural Networks with \nAttention outperform previous algorithms used in this task, and suggest further \nareas for improvement. \n</p>"}, "author": "Amitabha Karmakar", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517575665641", "timestampUsec": "1517575665641105", "id": "tag:google.com,2005:reader/item/0000000368815345", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. (arXiv:1802.00420v1 [cs.LG])", "published": 1517575665, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00420"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00420", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We identify obfuscated gradients as a phenomenon that leads to a false sense \nof security in defenses against adversarial examples. While defenses that cause \nobfuscated gradients appear to defeat optimization-based attacks, we find \ndefenses relying on this effect can be circumvented. \n</p> \n<p>For each of the three types of obfuscated gradients we discover, we describe \nindicators of defenses exhibiting this effect and develop attack techniques to \novercome it. In a case study, examining all defenses accepted to ICLR 2018, we \nfind obfuscated gradients are a common occurrence, with 7 of 8 defenses relying \non obfuscated gradients. Using our new attack techniques, we successfully \ncircumvent all 7 of them. \n</p>"}, "author": "Anish Athalye, Nicholas Carlini, David Wagner", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517575665641", "timestampUsec": "1517575665641102", "id": "tag:google.com,2005:reader/item/0000000368815356", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "PRNN: Recurrent Neural Network with Persistent Memory. (arXiv:1801.08094v2 [cs.LG] UPDATED)", "published": 1517575665, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.08094"}], "alternate": [{"href": "http://arxiv.org/abs/1801.08094", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Although Recurrent Neural Network (RNN) has been a powerful tool for modeling \nsequential data, its performance is inadequate when processing sequences with \nmultiple patterns. In this paper, we address this challenge by introducing an \nexternal memory and constructing a novel persistent memory augmented RNN \n(termed as PRNN). The PRNN captures the principle patterns in training \nsequences and stores them in an external memory. By leveraging the persistent \nmemory, the proposed method can adaptively update states according to the \nsimilarities between encoded inputs and memory slots, leading to a stronger \ncapacity in assimilating sequences with multiple patterns. Content-based \naddressing is suggested in memory accessing, and gradient descent is utilized \nfor implicitly updating the memory. Our approach can be further extended by \ncombining the prior knowledge of data. Experiments on several datasets \ndemonstrate the effectiveness of the proposed method. \n</p>"}, "author": "Kui Zhao, Yuechuan Li, Chi Zhang, Cheng Yang, Shenghuo Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517575665641", "timestampUsec": "1517575665641101", "id": "tag:google.com,2005:reader/item/000000036881536c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "DxNAT - Deep Neural Networks for Explaining Non-Recurring Traffic Congestion. (arXiv:1802.00002v1 [cs.LG])", "published": 1517575665, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00002"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00002", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Non-recurring traffic congestion is caused by temporary disruptions, such as \naccidents, sports games, adverse weather, etc. We use data related to real-time \ntraffic speed, jam factors (a traffic congestion indicator), and events \ncollected over a year from Nashville, TN to train a multi-layered deep neural \nnetwork. The traffic dataset contains over 900 million data records. The \nnetwork is thereafter used to classify the real-time data and identify \nanomalous operations. Compared with traditional approaches of using statistical \nor machine learning techniques, our model reaches an accuracy of 98.73 percent \nwhen identifying traffic congestion caused by football games. Our approach \nfirst encodes the traffic across a region as a scaled image. After that the \nimage data from different timestamps is fused with event- and time-related \ndata. Then a crossover operator is used as a data augmentation method to \ngenerate training datasets with more balanced classes. Finally, we use the \nreceiver operating characteristic (ROC) analysis to tune the sensitivity of the \nclassifier. We present the analysis of the training time and the inference time \nseparately. \n</p>"}, "author": "Fangzhou sun, Abhishek Dubey, Jules White", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517575665641", "timestampUsec": "1517575665641099", "id": "tag:google.com,2005:reader/item/0000000368815377", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Alternating Multi-bit Quantization for Recurrent Neural Networks. (arXiv:1802.00150v1 [cs.LG])", "published": 1517575665, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00150"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00150", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural networks have achieved excellent performance in many \napplications. However, on portable devices with limited resources, the models \nare often too large to deploy. For applications on the server with large scale \nconcurrent requests, the latency during inference can also be very critical for \ncostly computing resources. In this work, we address these problems by \nquantizing the network, both weights and activations, into multiple binary \ncodes {-1,+1}. We formulate the quantization as an optimization problem. Under \nthe key observation that once the quantization coefficients are fixed the \nbinary codes can be derived efficiently by binary search tree, alternating \nminimization is then applied. We test the quantization for two well-known RNNs, \ni.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the \nlanguage models. Compared with the full-precision counter part, by 2-bit \nquantization we can achieve ~16x memory saving and ~6x real inference \nacceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit \nquantization, we can achieve almost no loss in the accuracy or even surpass the \noriginal model, with ~10.5x memory saving and ~3x real inference acceleration. \nBoth results beat the exiting quantization works with large margins. We extend \nour alternating quantization to image classification tasks. In both RNNs and \nfeedforward neural networks, the method also achieves excellent performance. \n</p>"}, "author": "Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, Hongbin Zha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517575665641", "timestampUsec": "1517575665641097", "id": "tag:google.com,2005:reader/item/0000000368815385", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Global optimality conditions for deep neural networks. (arXiv:1707.02444v2 [cs.LG] UPDATED)", "published": 1517575665, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1707.02444"}], "alternate": [{"href": "http://arxiv.org/abs/1707.02444", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We study the error landscape of deep linear and nonlinear neural networks \nwith the squared error loss. Minimizing the loss of a deep linear neural \nnetwork is a nonconvex problem, and despite recent progress, our understanding \nof this loss surface is still incomplete. For deep linear networks, we present \nnecessary and sufficient conditions for a critical point of the risk function \nto be a global minimum. Our conditions provide an efficiently checkable test \nfor global optimality, which is remarkable because such tests are typically \nintractable in nonconvex optimization. We further extend these results to deep \nnonlinear neural networks and prove similar sufficient conditions for global \noptimality, albeit in a more limited function space setting. \n</p>"}, "author": "Chulhee Yun, Suvrit Sra, Ali Jadbabaie", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517575665641", "timestampUsec": "1517575665641095", "id": "tag:google.com,2005:reader/item/000000036881538f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Generalized End-to-End Loss for Speaker Verification. (arXiv:1710.10467v2 [eess.AS] UPDATED)", "published": 1517575665, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1710.10467"}], "alternate": [{"href": "http://arxiv.org/abs/1710.10467", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a new loss function called generalized end-to-end \n(GE2E) loss, which makes the training of speaker verification models more \nefficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike \nTE2E, the GE2E loss function updates the network in a way that emphasizes \nexamples that are difficult to verify at each step of the training process. \nAdditionally, the GE2E loss does not require an initial stage of example \nselection. With these properties, our model with the new loss function \ndecreases speaker verification EER by more than 10%, while reducing the \ntraining time by 60% at the same time. We also introduce the MultiReader \ntechnique, which allows us to do domain adaptation - training a more accurate \nmodel that supports multiple keywords (i.e. \"OK Google\" and \"Hey Google\") as \nwell as multiple dialects. \n</p>"}, "author": "Li Wan, Quan Wang, Alan Papir, Ignacio Lopez Moreno", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224163", "id": "tag:google.com,2005:reader/item/00000003684d3dbd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Dual Recurrent Attention Units for Visual Question Answering. (arXiv:1802.00209v1 [cs.AI])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00209"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00209", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose an architecture for VQA which utilizes recurrent layers to \ngenerate visual and textual attention. The memory characteristic of the \nproposed recurrent attention units offers a rich joint embedding of visual and \ntextual features and enables the model to reason relations between several \nparts of the image and question. Our single model outperforms the first place \nwinner on the VQA 1.0 dataset, performs within margin to the current \nstate-of-the-art ensemble model. We also experiment with replacing attention \nmechanisms in other state-of-the-art models with our implementation and show \nincreased accuracy. In both cases, our recurrent attention mechanism improves \nperformance in tasks requiring sequential or relational reasoning on the VQA \ndataset. \n</p>"}, "author": "Ahmed Osman, Wojciech Samek", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224161", "id": "tag:google.com,2005:reader/item/00000003684d3ddf", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deceptive Games. (arXiv:1802.00048v1 [cs.AI])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00048"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00048", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a746af567397\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a746af567397&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Deceptive games are games where the reward structure or other aspects of the \ngame are designed to lead the agent away from a globally optimal policy. While \nmany games are already deceptive to some extent, we designed a series of games \nin the Video Game Description Language (VGDL) implementing specific types of \ndeception, classified by the cognitive biases they exploit. VGDL games can be \nrun in the General Video Game Artificial Intelligence (GVGAI) Framework, making \nit possible to test a variety of existing AI agents that have been submitted to \nthe GVGAI Competition on these deceptive games. Our results show that all \ntested agents are vulnerable to several kinds of deception, but that different \nagents have different weaknesses. This suggests that we can use deception to \nunderstand the capabilities of a game-playing algorithm, and game-playing \nalgorithms to characterize the deception displayed by a game. \n</p>"}, "author": "Damien Anderson, Matthew Stephenson, Julian Togelius, Christian Salge, John Levine, Jochen Renz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224160", "id": "tag:google.com,2005:reader/item/00000003684d3dfb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Recursive Feature Generation for Knowledge-based Learning. (arXiv:1802.00050v1 [cs.AI])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00050"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00050", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>When humans perform inductive learning, they often enhance the process with \nbackground knowledge. With the increasing availability of well-formed \ncollaborative knowledge bases, the performance of learning algorithms could be \nsignificantly enhanced if a way were found to exploit these knowledge bases. In \nthis work, we present a novel algorithm for injecting external knowledge into \ninduction algorithms using feature generation. Given a feature, the algorithm \ndefines a new learning task over its set of values, and uses the knowledge base \nto solve the constructed learning task. The resulting classifier is then used \nas a new feature for the original problem. We have applied our algorithm to the \ndomain of text classification using large semantic knowledge bases. We have \nshown that the generated features significantly improve the performance of \nexisting learning algorithms. \n</p>"}, "author": "Lior Friedman, Shaul Markovitch", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224159", "id": "tag:google.com,2005:reader/item/00000003684d3e1a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Crowd Flow Prediction by Deep Spatio-Temporal Transfer Learning. (arXiv:1802.00386v1 [cs.AI])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00386"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00386", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Crowd flow prediction is a fundamental urban computing problem. Recently, \ndeep learning has been successfully applied to solve this problem, but it \nrelies on rich historical data. In reality, many cities may suffer from data \nscarcity issue when their targeted service or infrastructure is new. To \novercome this issue, this paper proposes a novel deep spatio-temporal transfer \nlearning framework, called RegionTrans, which can predict future crowd flow in \na data-scarce (target) city by transferring knowledge from a data-rich (source) \ncity. Leveraging social network check-ins, RegionTrans first links a region in \nthe target city to certain regions in the source city, expecting that these \ninter-city region pairs will share similar crowd flow dynamics. Then, we \npropose a deep spatio-temporal neural network structure, in which a hidden \nlayer is dedicated to keeping the region representation. A source city model is \nthen trained on its rich historical data with this network structure. Finally, \nwe propose a region-based cross-city transfer learning algorithm to learn the \ntarget city model from the source city model by minimizing the hidden \nrepresentation discrepancy between the inter-city region pairs previously \nlinked by check-ins. With experiments on real crowd flow, RegionTrans can \noutperform state-of-the-arts by reducing up to 10.7% prediction error. \n</p>"}, "author": "Leye Wang, Xu Geng, Xiaojuan Ma, Feng Liu, Qiang Yang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224158", "id": "tag:google.com,2005:reader/item/00000003684d3e47", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "3D Object Dense Reconstruction from a Single Depth View. (arXiv:1802.00411v1 [cs.CV])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00411"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00411", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs \nthe complete 3D structure of a given object from a single arbitrary depth view \nusing generative adversarial networks. Unlike existing work which typically \nrequires multiple views of the same object or class labels to recover the full \n3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation \nof a depth view of the object as input, and is able to generate the complete 3D \noccupancy grid with a high resolution of 256^3 by recovering the \noccluded/missing regions. The key idea is to combine the generative \ncapabilities of autoencoders and the conditional Generative Adversarial \nNetworks (GAN) framework, to infer accurate and fine-grained 3D structures of \nobjects in high-dimensional voxel space. Extensive experiments on large \nsynthetic datasets and real-world Kinect datasets show that the proposed \n3D-RecGAN++ significantly outperforms the state of the art in single view 3D \nobject reconstruction, and is able to reconstruct unseen types of objects. \n</p>"}, "author": "Bo Yang, Stefano Rosa, Andrew Markham, Niki Trigoni, Hongkai Wen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224157", "id": "tag:google.com,2005:reader/item/00000003684d3e7a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "On Understanding and Machine Understanding. (arXiv:1103.5034v2 [cs.AI] UPDATED)", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1103.5034"}], "alternate": [{"href": "http://arxiv.org/abs/1103.5034", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In the present paper, we try to propose a self-similar network theory for the \nbasic understanding. By extending the natural languages to a kind of so called \nidealy sufficient language, we can proceed a few steps to the investigation of \nthe language searching and the language understanding of AI. \n</p> \n<p>Image understanding, and the familiarity of the brain to the surrounding \nenvironment are also discussed. Group effects are discussed by addressing the \nessense of the power of influences, and constructing the influence network of a \nsociety. We also give a discussion of inspirations. \n</p>"}, "author": "Tong Chern", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224156", "id": "tag:google.com,2005:reader/item/00000003684d3ea3", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Reliable Decision Support using Counterfactual Models. (arXiv:1703.10651v4 [stat.ML] UPDATED)", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.10651"}], "alternate": [{"href": "http://arxiv.org/abs/1703.10651", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Decision-makers are faced with the challenge of estimating what is likely to \nhappen when they take an action. For instance, if I choose not to treat this \npatient, are they likely to die? Practitioners commonly use supervised learning \nalgorithms to fit predictive models that help decision-makers reason about \nlikely future outcomes, but we show that this approach is unreliable, and \nsometimes even dangerous. The key issue is that supervised learning algorithms \nare highly sensitive to the policy used to choose actions in the training data, \nwhich causes the model to capture relationships that do not generalize. We \npropose using a different learning objective that predicts counterfactuals \ninstead of predicting outcomes under an existing action policy as in supervised \nlearning. To support decision-making in temporal settings, we introduce the \nCounterfactual Gaussian Process (CGP) to predict the counterfactual future \nprogression of continuous-time trajectories under sequences of future actions. \nWe demonstrate the benefits of the CGP on two important decision-support tasks: \nrisk prediction and \"what if?\" reasoning for individualized treatment planning. \n</p>"}, "author": "Peter Schulam, Suchi Saria", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224154", "id": "tag:google.com,2005:reader/item/00000003684d3efb", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "JointDNN: An Efficient Training and Inference Engine for Intelligent Mobile Cloud Computing Services. (arXiv:1801.08618v1 [cs.DC] CROSS LISTED)", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.08618"}], "alternate": [{"href": "http://arxiv.org/abs/1801.08618", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks are among the most influential architectures of deep \nlearning algorithms, being deployed in many mobile intelligent applications. \nEnd-side services, such as intelligent personal assistants (IPAs), autonomous \ncars, and smart home services often employ either simple local models or \ncomplex remote models on the cloud. Mobile-only and cloud-only computations are \ncurrently the status quo approaches. In this paper, we propose an efficient, \nadaptive, and practical engine, JointDNN, for collaborative computation between \na mobile device and cloud for DNNs in both inference and training phase. \nJointDNN not only provides an energy and performance efficient method of \nquerying DNNs for the mobile side, but also benefits the cloud server by \nreducing the amount of its workload and communications compared to the \ncloud-only approach. Given the DNN architecture, we investigate the efficiency \nof processing some layers on the mobile device and some layers on the cloud \nserver. We provide optimization formulations at layer granularity for forward \nand backward propagation in DNNs, which can adapt to mobile battery limitations \nand cloud server load constraints and quality of service. JointDNN achieves up \nto 18X and 32X reductions on the latency and mobile energy consumption of \nquerying DNNs, respectively. \n</p>"}, "author": "Amir Erfan Eshratifar, Mohammad Saeed Abrishami, Massoud Pedram", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224153", "id": "tag:google.com,2005:reader/item/00000003684d3f1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fusarium Damaged Kernels Detection Using Transfer Learning on Deep Neural Network Architecture. (arXiv:1802.00030v1 [cs.LG])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00030"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00030", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The present work shows the application of transfer learning for a pre-trained \ndeep neural network (DNN), using a small image dataset ($\\approx$ 12,000) on a \nsingle workstation with enabled NVIDIA GPU card that takes up to 1 hour to \ncomplete the training task and archive an overall average accuracy of $94.7\\%$. \nThe DNN presents a $20\\%$ score of misclassification for an external test \ndataset. The accuracy of the proposed methodology is equivalent to ones using \nHSI methodology $(81\\%-91\\%)$ used for the same task, but with the advantage of \nbeing independent on special equipment to classify wheat kernel for FHB \nsymptoms. \n</p>"}, "author": "M&#xe1;rcio Nicolau, M&#xe1;rcia Barrocas Moreira Pimentel, Casiane Salete Tibola, Jos&#xe9; Mauricio Cunha Fernandes, Willingthon Pavan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224152", "id": "tag:google.com,2005:reader/item/00000003684d3f30", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Incremental kernel PCA and the Nystr\\\"om method. (arXiv:1802.00043v1 [stat.ML])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00043"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00043", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Incremental versions of batch algorithms are often desired, for increased \ntime efficiency in the streaming data setting, or increased memory efficiency \nin general. In this paper we present a novel algorithm for incremental kernel \nPCA, based on rank one updates to the eigendecomposition of the kernel matrix, \nwhich is more computationally efficient than comparable existing algorithms. We \nextend our algorithm to incremental calculation of the Nystr\\\"om approximation \nto the kernel matrix, the first such algorithm proposed. Incremental \ncalculation of the Nystr\\\"om approximation leads to further gains in memory \nefficiency, and allows for empirical evaluation of when a subset of sufficient \nsize has been obtained. \n</p>"}, "author": "Fredrik Hallgren, Paul Northrop", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224151", "id": "tag:google.com,2005:reader/item/00000003684d3f57", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "A Modified Sigma-Pi-Sigma Neural Network with Adaptive Choice of Multinomials. (arXiv:1802.00123v1 [cs.LG])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00123"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00123", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Sigma-Pi-Sigma neural networks (SPSNNs) as a kind of high-order neural \nnetworks can provide more powerful mapping capability than the traditional \nfeedforward neural networks (Sigma-Sigma neural networks). In the existing \nliterature, in order to reduce the number of the Pi nodes in the Pi layer, a \nspecial multinomial P_s is used in SPSNNs. Each monomial in P_s is linear with \nrespect to each particular variable sigma_i when the other variables are taken \nas constants. Therefore, the monomials like sigma_i^n or sigma_i^n sigma_j with \nn&gt;1 are not included. This choice may be somehow intuitive, but is not \nnecessarily the best. We propose in this paper a modified Sigma-Pi-Sigma neural \nnetwork (MSPSNN) with an adaptive approach to find a better multinomial for a \ngiven problem. To elaborate, we start from a complete multinomial with a given \norder. Then we employ a regularization technique in the learning process for \nthe given problem to reduce the number of monomials used in the multinomial, \nand end up with a new SPSNN involving the same number of monomials (= the \nnumber of nodes in the Pi-layer) as in P_s. Numerical experiments on some \nbenchmark problems show that our MSPSNN behaves better than the traditional \nSPSNN with P_s. \n</p>"}, "author": "Feng Li, Yan Liu, Khidir Shaib Mohamed, Wei Wu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224150", "id": "tag:google.com,2005:reader/item/00000003684d3f6d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Learning with Data Dependent Implicit Activation Function. (arXiv:1802.00168v1 [cs.LG])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00168"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00168", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a746af56757e\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a746af56757e&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Though deep neural networks (DNNs) achieve remarkable performances in many \nartificial intelligence tasks, the lack of training instances remains a \nnotorious challenge. As the network goes deeper, the generalization accuracy \ndecays rapidly in the situation of lacking massive amounts of training data. In \nthis paper, we propose novel deep neural network structures that can be \ninherited from all existing DNNs with almost the same level of complexity, and \ndevelop simple training algorithms. We show our paradigm successfully resolves \nthe lack of data issue. Tests on the CIFAR10 and CIFAR100 image recognition \ndatasets show that the new paradigm leads to 20$\\%$ to $30\\%$ relative error \nrate reduction compared to their base DNNs. The intuition of our algorithms for \ndeep residual network stems from theories of the partial differential equation \n(PDE) control problems. Code will be made available. \n</p>"}, "author": "Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi, Stanley J. Osher", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224149", "id": "tag:google.com,2005:reader/item/00000003684d3f8f", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "One-class Collective Anomaly Detection based on Long Short-Term Memory Recurrent Neural Networks. (arXiv:1802.00324v1 [cs.LG])", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1802.00324"}], "alternate": [{"href": "http://arxiv.org/abs/1802.00324", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a746af604293\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a746af604293&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Intrusion detection for computer network systems has been becoming one of the \nmost critical tasks for network administrators today. It has an important role \nfor organizations, governments and our society due to the valuable resources \nhosted on computer networks. Traditional misuse detection strategies are unable \nto detect new and unknown intrusion types. In contrast, anomaly detection in \nnetwork security aims to distinguish between illegal or malicious events and \nnormal behavior of network systems. Anomaly detection can be considered as a \nclassification problem where it builds models of normal network behavior, of \nwhich it uses to detect new patterns that significantly deviate from the model. \nMost of the current approaches on anomaly detection is based on the learning of \nnormal behavior and anomalous actions. They do not include memory that is they \ndo not take into account previous events classify new ones. In this paper, we \npropose a one class collective anomaly detection model based on neural network \nlearning. Normally a Long Short Term Memory Recurrent Neural Network (LSTM RNN) \nis trained only on normal data, and it is capable of predicting several time \nsteps ahead of an input. In our approach, a LSTM RNN is trained on normal time \nseries data before performing a prediction for each time step. Instead of \nconsidering each time-step separately, the observation of prediction errors \nfrom a certain number of time-steps is now proposed as a new idea for detecting \ncollective anomalies. The prediction errors of a certain number of the latest \ntime-steps above a threshold will indicate a collective anomaly. The model is \nevaluated on a time series version of the KDD 1999 dataset. The experiments \ndemonstrate that the proposed model is capable to detect collective anomaly \nefficiently \n</p>"}, "author": "Nga Nguyen Thi, Van Loi Cao, Nhien-An Le-Khac", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224148", "id": "tag:google.com,2005:reader/item/00000003684d3f99", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Boundary-Seeking Generative Adversarial Networks. (arXiv:1702.08431v3 [stat.ML] UPDATED)", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1702.08431"}], "alternate": [{"href": "http://arxiv.org/abs/1702.08431", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative adversarial networks (GANs) are a learning framework that rely on \ntraining a discriminator to estimate a measure of difference between a target \nand generated distributions. GANs, as normally formulated, rely on the \ngenerated samples being completely differentiable w.r.t. the generative \nparameters, and thus do not work for discrete data. We introduce a method for \ntraining GANs with discrete data that uses the estimated difference measure \nfrom the discriminator to compute importance weights for generated samples, \nthus providing a policy gradient for training the generator. The importance \nweights have a strong connection to the decision boundary of the discriminator, \nand we call our method boundary-seeking GANs (BGANs). We demonstrate the \neffectiveness of the proposed algorithm with discrete image and character-based \nnatural language generation. In addition, the boundary-seeking objective \nextends to continuous data, which can be used to improve stability of training. \n</p>"}, "author": "R Devon Hjelm, Athul Paul Jacob, Tong Che, Adam Trischler, Kyunghyun Cho, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224147", "id": "tag:google.com,2005:reader/item/00000003684d3fa1", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting. (arXiv:1709.01604v3 [cs.CR] UPDATED)", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.01604"}], "alternate": [{"href": "http://arxiv.org/abs/1709.01604", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Machine learning algorithms, when applied to sensitive data, pose a distinct \nthreat to privacy. A growing body of prior work demonstrates that models \nproduced by these algorithms may leak specific private information in the \ntraining data to an attacker, either through the models' structure or their \nobservable behavior. However, the underlying cause of this privacy risk is not \nwell understood beyond a handful of anecdotal accounts that suggest overfitting \nand influence might play a role. \n</p> \n<p>This paper examines the effect that overfitting and influence have on the \nability of an attacker to learn information about the training data from \nmachine learning models, either through training set membership inference or \nattribute inference attacks. Using both formal and empirical analyses, we \nillustrate a clear relationship between these factors and the privacy risk that \narises in several popular machine learning algorithms. We find that overfitting \nis sufficient to allow an attacker to perform membership inference and, when \nthe target attribute meets certain conditions about its influence, attribute \ninference attacks. Interestingly, our formal analysis also shows that \noverfitting is not necessary for these attacks and begins to shed light on what \nother factors may be in play. Finally, we explore the connection between \nmembership inference and attribute inference, showing that there are deep \nconnections between the two that lead to effective new attacks. \n</p>"}, "author": "Samuel Yeom, Matt Fredrikson, Irene Giacomelli, Somesh Jha", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517554275224", "timestampUsec": "1517554275224146", "id": "tag:google.com,2005:reader/item/00000003684d3fcd", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting. (arXiv:1709.04875v3 [cs.LG] UPDATED)", "published": 1517554275, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1709.04875"}], "alternate": [{"href": "http://arxiv.org/abs/1709.04875", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Timely accurate traffic forecast is crucial for urban traffic control and \nguidance. Due to the high nonlinearity and complexity of traffic flow, \ntraditional methods cannot satisfy the requirements of mid-and-long term \nprediction tasks and often neglect spatial and temporal dependencies. In this \npaper, we propose a novel deep learning framework, Spatio-Temporal Graph \nConvolutional Networks (STGCN), to tackle the time series prediction problem in \ntraffic domain. Instead of applying regular convolutional and recurrent units, \nwe formulate the problem on graphs and build the model with complete \nconvolutional structures, which enable much faster training speed with fewer \nparameters. Experiments show that our STGCN model effectively captures \ncomprehensive spatio-temporal correlations through modeling multi-scale traffic \nnetworks and consistently outperforms state-of-the-art baselines on various \nreal-world traffic datasets. \n</p>"}, "author": "Bing Yu, Haoteng Yin, Zhanxing Zhu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166253", "id": "tag:google.com,2005:reader/item/00000003678215c9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-Layer Competitive-Cooperative Framework for Performance Enhancement of Differential Evolution. (arXiv:1801.10546v1 [cs.NE])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10546"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10546", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Differential Evolution (DE) is one of the most powerful optimizers in the \nevolutionary algorithm (EA) family. In recent years, many DE variants have been \nproposed to enhance performance. However, when compared with each other, \nsignificant differences in performances are seldomly observed. To meet this \nchallenge of a more significant improvement, this paper proposes a multi-layer \ncompetitive-cooperative (MLCC) framework to combine the advantages of multiple \nDEs. Existing multi-method strategies commonly use a multi-population based \nstructure, which classifies the entire population into several subpopulations \nand evolve individuals only in their corresponding subgroups. MLCC proposes to \nimplement a parallel structure with the entire population simultaneously \nmonitored by multiple DEs assigned in multiple layers. Each individual can \nstore, utilize and update its evolution information in different layers by \nusing a novel individual preference based layer selecting (IPLS) mechanism and \na computational resource allocation bias (RAB) mechanism. In IPLS, individuals \nonly connect to one favorite layer. While in RAB, high quality solutions are \nevolved by considering all the layers. In this way, the multiple layers work in \na competitive and cooperative manner. The proposed MLCC framework has been \nimplemented on several highly competitive DEs. Experimental studies show that \nMLCC variants significantly outperform the baseline DEs as well as several \nstate-of-the-art and up-to-date DEs on the CEC benchmark functions. \n</p>"}, "author": "Sheng Xin Zhang, Li Ming Zheng, Kit Sang Tang, Shao Yong Zheng, Wing Shing Chan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166252", "id": "tag:google.com,2005:reader/item/00000003678215d2", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Reinforcement Learning for Programming Language Correction. (arXiv:1801.10467v1 [cs.AI])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10467"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10467", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Novice programmers often struggle with the formal syntax of programming \nlanguages. To assist them, we design a novel programming language correction \nframework amenable to reinforcement learning. The framework allows an agent to \nmimic human actions for text navigation and editing. We demonstrate that the \nagent can be trained through self-exploration directly from the raw input, that \nis, program text itself, without any knowledge of the formal syntax of the \nprogramming language. We leverage expert demonstrations for one tenth of the \ntraining data to accelerate training. The proposed technique is evaluated on \n6975 erroneous C programs with typographic errors, written by students during \nan introductory programming course. Our technique fixes 14% more programs and \n29% more compiler error messages relative to those fixed by a state-of-the-art \ntool, DeepFix, which uses a fully supervised neural machine translation \napproach. \n</p>"}, "author": "Rahul Gupta, Aditya Kanade, Shirish Shevade", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166251", "id": "tag:google.com,2005:reader/item/00000003678215dc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Fraternal Dropout. (arXiv:1711.00066v3 [stat.ML] UPDATED)", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00066"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00066", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Recurrent neural networks (RNNs) are important class of architectures among \nneural networks useful for language modeling and sequential prediction. \nHowever, optimizing RNNs is known to be harder compared to feed-forward neural \nnetworks. A number of techniques have been proposed in literature to address \nthis problem. In this paper we propose a simple technique called fraternal \ndropout that takes advantage of dropout to achieve this goal. Specifically, we \npropose to train two identical copies of an RNN (that share parameters) with \ndifferent dropout masks while minimizing the difference between their \n(pre-softmax) predictions. In this way our regularization encourages the \nrepresentations of RNNs to be invariant to dropout mask, thus being robust. We \nshow that our regularization term is upper bounded by the expectation-linear \ndropout objective which has been shown to address the gap due to the difference \nbetween the train and inference phases of dropout. We evaluate our model and \nachieve state-of-the-art results in sequence modeling tasks on two benchmark \ndatasets - Penn Treebank and Wikitext-2. We also show that our approach leads \nto performance improvement by a significant margin in image captioning \n(Microsoft COCO) and semi-supervised (CIFAR-10) tasks. \n</p>"}, "author": "Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166250", "id": "tag:google.com,2005:reader/item/00000003678215e9", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Learning to Classify from Impure Samples. (arXiv:1801.10158v1 [hep-ph])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10158"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10158", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A persistent challenge in practical classification tasks is that labelled \ntraining sets are not always available. In particle physics, this challenge is \nsurmounted by the use of simulations. These simulations accurately reproduce \nmost features of data, but cannot be trusted to capture all of the complex \ncorrelations exploitable by modern machine learning methods. Recent work in \nweakly supervised learning has shown that simple, low-dimensional classifiers \ncan be trained using only the impure mixtures present in data. Here, we \ndemonstrate that complex, high-dimensional classifiers can also be trained on \nimpure mixtures using weak supervision techniques, with performance comparable \nto what could be achieved with pure samples. Using weak supervision will \ntherefore allow us to avoid relying exclusively on simulations for \nhigh-dimensional classification. This work opens the door to a new regime \nwhereby complex models are trained directly on data, providing direct access to \nprobe the underlying physics. \n</p>"}, "author": "Patrick T. Komiske, Eric M. Metodiev, Benjamin Nachman, Matthew D. Schwartz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166249", "id": "tag:google.com,2005:reader/item/00000003678215ef", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Kernel Distillation for Gaussian Processes. (arXiv:1801.10273v1 [stat.ML])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10273"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10273", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Gaussian processes (GPs) are flexible models that can capture complex \nstructure in large-scale dataset due to their non-parametric nature. However, \nthe usage of GPs in real-world application is limited due to their high \ncomputational cost at inference time. In this paper, we introduce a new \nframework, \\textit{kernel distillation}, for kernel matrix approximation. The \nidea adopts from knowledge distillation in deep learning community, where we \napproximate a fully trained teacher kernel matrix of size $n\\times n$ with a \nstudent kernel matrix. We combine inducing points method with sparse low-rank \napproximation in the distillation procedure. The distilled student kernel \nmatrix only cost $\\mathcal{O}(m^2)$ storage where $m$ is the number of inducing \npoints and $m \\ll n$. We also show that one application of kernel distillation \nis for fast GP prediction, where we demonstrate empirically that our \napproximation provide better balance between the prediction time and the \npredictive performance compared to the alternatives. \n</p>"}, "author": "Congzheng Song, Yiming Sun", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166248", "id": "tag:google.com,2005:reader/item/00000003678215f4", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Multi-view Learning to Rank. (arXiv:1801.10402v1 [cs.LG])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10402"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10402", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a746af6045f7\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a746af6045f7&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We study the problem of learning to rank from multiple sources. Though \nmulti-view learning and learning to rank have been studied extensively leading \nto a wide range of applications, multi-view learning to rank as a synergy of \nboth topics has received little attention. The aim of the paper is to propose a \ncomposite ranking method while keeping a close correlation with the individual \nrankings simultaneously. We propose a multi-objective solution to ranking by \ncapturing the information of the feature mapping from both within each view as \nwell as across views using autoencoder-like networks. Moreover, a novel \nend-to-end solution is introduced to enhance the joint ranking with minimum \nview-specific ranking loss, so that we can achieve the maximum global view \nagreements within a single optimization process. The proposed method is \nvalidated on a wide variety of ranking problems, including university ranking, \nmulti-view lingual text ranking and image data ranking, providing superior \nresults. \n</p>"}, "author": "Guanqun Cao, Alexandros Iosifidis, Moncef Gabbouj, Vijay Raghavan, Raju Gottumukkala", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166247", "id": "tag:google.com,2005:reader/item/00000003678215fe", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Model compression for faster structural separation of macromolecules captured by Cellular Electron Cryo-Tomography. (arXiv:1801.10597v1 [q-bio.QM])", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10597"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10597", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Electron Cryo-Tomography (ECT) enables 3D visualization of macromolecule \nstructure inside single cells. Macromolecule classification approaches based on \nconvolutional neural networks (CNN) were developed to separate millions of \nmacromolecules captured from ECT systematically. However, given the fast \naccumulation of ECT data, it will soon become necessary to use CNN models to \nefficiently and accurately separate substantially more macromolecules at the \nprediction stage, which requires additional computational costs. To speed up \nthe prediction, we compress classification models into compact neural networks \nwith little in accuracy for deployment. Specifically, we propose to perform \nmodel compression through knowledge distillation. Firstly, a complex teacher \nnetwork is trained to generate soft labels with better classification \nfeasibility followed by training of customized student networks with simple \narchitectures using the soft label to compress model complexity. Our tests \ndemonstrate that our compressed models significantly reduce the number of \nparameters and time cost while maintaining similar classification accuracy. \n</p>"}, "author": "Jialiang Guo, Bo Zhou, Xiangrui Zeng, Zachary Freyberg, Min Xu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517463224166", "timestampUsec": "1517463224166246", "id": "tag:google.com,2005:reader/item/0000000367821606", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Attacking Binarized Neural Networks. (arXiv:1711.00449v2 [cs.LG] UPDATED)", "published": 1517463225, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.00449"}], "alternate": [{"href": "http://arxiv.org/abs/1711.00449", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Neural networks with low-precision weights and activations offer compelling \nefficiency advantages over their full-precision equivalents. The two most \nfrequently discussed benefits of quantization are reduced memory consumption, \nand a faster forward pass when implemented with efficient bitwise operations. \nWe propose a third benefit of very low-precision neural networks: improved \nrobustness against some adversarial attacks, and in the worst case, performance \nthat is on par with full-precision models. We focus on the very low-precision \ncase where weights and activations are both quantized to $\\pm$1, and note that \nstochastically quantizing weights in just one layer can sharply reduce the \nimpact of iterative attacks. We observe that non-scaled binary neural networks \nexhibit a similar effect to the original defensive distillation procedure that \nled to gradient masking, and a false notion of security. We address this by \nconducting both black-box and white-box experiments with binary models that do \nnot artificially mask gradients. \n</p>"}, "author": "Angus Galloway, Graham W. Taylor, Medhat Moussa", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893436", "id": "tag:google.com,2005:reader/item/0000000366c10d01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "ReNN: Rule-embedded Neural Networks. (arXiv:1801.09856v1 [cs.LG])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09856"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09856", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The artificial neural network shows powerful ability of inference, but it is \nstill criticized for lack of interpretability and prerequisite needs of big \ndataset. This paper proposes the Rule-embedded Neural Network (ReNN) to \novercome the shortages. ReNN first makes local-based inferences to detect local \npatterns, and then uses rules based on domain knowledge about the local \npatterns to generate rule-modulated map. After that, ReNN makes global-based \ninferences that synthesizes the local patterns and the rule-modulated map. To \nsolve the optimization problem caused by rules, we use a two-stage optimization \nstrategy to train the ReNN model. By introducing rules into ReNN, we can \nstrengthen traditional neural networks with long-term dependencies which are \ndifficult to learn with limited empirical dataset, thus improving inference \naccuracy. The complexity of neural networks can be reduced since long-term \ndependencies are not modeled with neural connections, and thus the amount of \ndata needed to optimize the neural networks can be reduced. Besides, inferences \nfrom ReNN can be analyzed with both local patterns and rules, and thus have \nbetter interpretability. In this paper, ReNN has been validated with a \ntime-series detection problem. \n</p>"}, "author": "Hu Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893433", "id": "tag:google.com,2005:reader/item/0000000366c10d0c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Over-representation of Extreme Events in Decision-Making: A Rational Metacognitive Account. (arXiv:1801.09848v1 [q-bio.NC])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09848"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09848", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Availability bias, manifested in the over-representation of extreme \neventualities in decision-making, is a well-known cognitive bias, and is \ngenerally taken as evidence of human irrationality. In this work, we present \nthe first rational, metacognitive account of the Availability bias, formally \narticulated at Marr's algorithmic level of analysis. Concretely, we present a \nnormative, metacognitive model of how a cognitive system should over-represent \nextreme eventualities, depending on the amount of time available at its \ndisposal for decision-making. Our model also accounts for two well-known \nframing effects in human decision-making under risk---the fourfold pattern of \nrisk preferences in outcome probability (Tversky &amp; Kahneman, 1992) and in \noutcome magnitude (Markovitz, 1952)---thereby providing the first \nmetacognitively-rational basis for those effects. Empirical evidence, \nfurthermore, confirms an important prediction of our model. Surprisingly, our \nmodel is unimaginably robust with respect to its focal parameter. We discuss \nthe implications of our work for studies on human decision-making, and conclude \nby presenting a counterintuitive prediction of our model, which, if confirmed, \nwould have intriguing implications for human decision-making under risk. To our \nknowledge, our model is the first metacognitive, resource-rational process \nmodel of cognitive biases in decision-making. \n</p>"}, "author": "Ardavan S. Nobandegani, Kevin da Silva Castanheira, A. Ross Otto, Thomas R. Shultz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893432", "id": "tag:google.com,2005:reader/item/0000000366c10d15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal Number of Choices in Rating Contexts. (arXiv:1605.06588v6 [cs.AI] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1605.06588"}], "alternate": [{"href": "http://arxiv.org/abs/1605.06588", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In many settings people must give numerical scores to entities from a small \ndiscrete set. For instance, rating physical attractiveness from 1--5 on dating \nsites, or papers from 1--10 for conference reviewing. We study the problem of \nunderstanding when using a different number of options is optimal. For \nconcreteness we assume the true underlying scores are integers from 1--100. We \nconsider the case when scores are uniform random and Gaussian. We study when \nusing 2, 3, 4, 5, and 10 options is optimal in these models. One may expect \nthat using more options would always improve performance in this model, but we \nshow that this is not necessarily the case, and that using fewer choices---even \njust two---can surprisingly be optimal in certain situations. While in theory \nfor this setting it would be optimal to use all 100 options, in practice this \nis prohibitive, and it is preferable to utilize a smaller number of options due \nto humans' limited computational resources. Our results suggest that using a \nsmaller number of options than is typical could be optimal in certain \nsituations. This would have many potential applications, as settings requiring \nentities to be ranked by humans are ubiquitous. \n</p>"}, "author": "Sam Ganzfried, Farzana Yusuf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893431", "id": "tag:google.com,2005:reader/item/0000000366c10d1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Towards a Quantum World Wide Web. (arXiv:1703.06642v2 [cs.AI] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.06642"}], "alternate": [{"href": "http://arxiv.org/abs/1703.06642", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We elaborate a quantum model for the meaning associated with corpora of \nwritten documents, like the pages forming the World Wide Web. To that end, we \nare guided by how physicists constructed quantum theory for microscopic \nentities, which unlike classical objects cannot be fully represented in our \nspatial theater. We suggest that a similar construction needs to be carried out \nby linguists and computational scientists, to capture the full meaning carried \nby collections of documental entities. More precisely, we show how to associate \na quantum-like 'entity of meaning' to a 'language entity formed by printed \ndocuments', considering the latter as the collection of traces that are left by \nthe former, in specific results of search actions that we describe as \nmeasurements. In other words, we offer a perspective where a collection of \ndocuments, like the Web, is described as the space of manifestation of a more \ncomplex entity - the QWeb - which is the object of our modeling, drawing its \ninspiration from previous studies on operational-realistic approaches to \nquantum physics and quantum modeling of human cognition and decision-making. We \nemphasize that a consistent QWeb model needs to account for the observed \ncorrelations between words appearing in printed documents, e.g., \nco-occurrences, as the latter would depend on the 'meaning connections' \nexisting between the concepts that are associated with these words. In that \nrespect, we show that both 'context and interference (quantum) effects' are \nrequired to explain the probabilities calculated by counting the relative \nnumber of documents containing certain words and co-ocurrrences of words. \n</p>"}, "author": "Diederik Aerts, Jonito Aerts Arguelles, Lester Beltran, Lyneth Beltran, Isaac Distrito, Massimiliano Sassoli de Bianchi, Sandro Sozzo, Tomas Veloz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893430", "id": "tag:google.com,2005:reader/item/0000000366c10d22", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Towards Synthesizing Complex Programs from Input-Output Examples. (arXiv:1706.01284v2 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.01284"}], "alternate": [{"href": "http://arxiv.org/abs/1706.01284", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years, deep learning techniques have been developed to improve the \nperformance of program synthesis from input-output examples. Albeit its \nsignificant progress, the programs that can be synthesized by state-of-the-art \napproaches are still simple in terms of their complexity. In this work, we move \na significant step forward along this direction by proposing a new class of \nchallenging tasks in the domain of program synthesis from input-output \nexamples: learning a context-free parser from pairs of input programs and their \nparse trees. We show that this class of tasks are much more challenging than \npreviously studied tasks, and the test accuracy of existing approaches is \nalmost 0%. \n</p> \n<p>We tackle the challenges by developing three novel techniques inspired by \nthree novel observations, which reveal the key ingredients of using deep \nlearning to synthesize a complex program. First, the use of a \nnon-differentiable machine is the key to effectively restrict the search space. \nThus our proposed approach learns a neural program operating a domain-specific \nnon-differentiable machine. Second, recursion is the key to achieve \ngeneralizability. Thus, we bake-in the notion of recursion in the design of our \nnon-differentiable machine. Third, reinforcement learning is the key to learn \nhow to operate the non-differentiable machine, but it is also hard to train the \nmodel effectively with existing reinforcement learning algorithms from a cold \nboot. We develop a novel two-phase reinforcement learning-based search \nalgorithm to overcome this issue. In our evaluation, we show that using our \nnovel approach, neural parsing programs can be learned to achieve 100% test \naccuracy on test inputs that are 500x longer than the training samples. \n</p>"}, "author": "Xinyun Chen, Chang Liu, Dawn Song", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893429", "id": "tag:google.com,2005:reader/item/0000000366c10d2c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Bayesian Neural Networks. (arXiv:1801.07710v2 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.07710"}], "alternate": [{"href": "http://arxiv.org/abs/1801.07710", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper describes and discusses Bayesian Neural Network (BNN). The paper \nshowcases a few different applications of them for classification and \nregression problems. BNNs are comprised of a Probabilistic Model and a Neural \nNetwork. The intent of such a design is to combine the strengths of Neural \nNetworks and Stochastic modeling. Neural Networks exhibit continuous function \napproximator capabilities. Stochastic models allow direct specification of a \nmodel with known interaction between parameters to generate data. During the \nprediction phase, stochastic models generate a complete posterior distribution \nand produce probabilistic guarantees on the predictions. Thus BNNs are a unique \ncombination of neural network and stochastic models with the stochastic model \nforming the core of this integration. BNNs can then produce probabilistic \nguarantees on it's predictions and also generate the distribution of parameters \nthat it has learnt from the observations. That means, in the parameter space, \none can deduce the nature and shape of the neural network's learnt parameters. \nThese two characteristics makes them highly attractive to theoreticians as well \nas practitioners. Recently there has been a lot of activity in this area, with \nthe advent of numerous probabilistic programming libraries such as: PyMC3, \nEdward, Stan etc. Further this area is rapidly gaining ground as a standard \nmachine learning approach for numerous problems \n</p>"}, "author": "Vikram Mullachery, Aniruddh Khera, Amir Husain", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893428", "id": "tag:google.com,2005:reader/item/0000000366c10d3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Cross-type Biomedical Named Entity Recognition with Deep Multi-Task Learning. (arXiv:1801.09851v1 [cs.IR])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09851"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09851", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Motivation: Biomedical named entity recognition (BioNER) is the most \nfundamental task in biomedical text mining. State-of-the-art BioNER systems \noften require handcrafted features specifically designed for each type of \nbiomedical entities. This feature generation process requires intensive labors \nfrom biomedical and linguistic experts, and makes it difficult to adapt these \nsystems to new biomedical entity types. Although recent studies explored using \nneural network models for BioNER to free experts from manual feature \ngeneration, these models still require substantial human efforts to annotate \nmassive training data. \n</p> \n<p>Results: We propose a multi-task learning framework for BioNER that is based \non neural network models to save human efforts. We build a global model by \ncollectively training multiple models that share parameters, each model \ncapturing the characteristics of a different biomedical entity type. In \nexperiments on five BioNER benchmark datasets covering four major biomedical \nentity types, our model outperforms state-of-the-art systems and other neural \nnetwork models by a large margin, even when only limited training data are \navailable. Further analysis shows that the large performance gains come from \nsharing character- and word-level information between different biomedical \nentities. The approach creates new opportunities for text-mining approaches to \nhelp biomedical scientists better exploit knowledge in biomedical literature. \n</p>"}, "author": "Xuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang, Marinka Zitnik, Jingbo Shang, Curtis Langlotz, Jiawei Han", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893427", "id": "tag:google.com,2005:reader/item/0000000366c10d4b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Cardiac Arrhythmia Detection from ECG Combining Convolutional and Long Short-Term Memory Networks. (arXiv:1801.10033v1 [eess.SP])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10033"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10033", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a746af6049d0\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a746af6049d0&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Objectives: Atrial fibrillation (AF) is a common heart rhythm disorder \nassociated with deadly and debilitating consequences including heart failure, \nstroke, poor mental health, reduced quality of life and death. Having an \nautomatic system that diagnoses various types of cardiac arrhythmias would \nassist cardiologists to initiate appropriate preventive measures and to improve \nthe analysis of cardiac disease. To this end, this paper introduces a new \napproach to detect and classify automatically cardiac arrhythmias in \nelectrocardiograms (ECG) recordings. \n</p> \n<p>Methods: The proposed approach used a combination of Convolution Neural \nNetworks (CNNs) and a sequence of Long Short-Term Memory (LSTM) units, with \npooling, dropout and normalization techniques to improve their accuracy. The \nnetwork predicted a classification at every 18th input sample and we selected \nthe final prediction for classification. Results were cross-validated on the \nPhysionet Challenge 2017 training dataset, which contains 8,528 single lead ECG \nrecordings lasting from 9s to just over 60s. \n</p> \n<p>Results: Using the proposed structure and no explicit feature selection, \n10-fold stratified cross-validation gave an overall F-measure of 0.83.10-0.015 \non the held-out test data (mean-standard deviation over all folds) and 0.80 on \nthe hidden dataset of the Challenge entry server. \n</p>"}, "author": "Philip Warrick (1), Masun Nabhan Homsi (2) ((1) PeriGen. Inc., Montreal, Canada, (2) Simon Bolivar University, Caracas, Venezuela)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893426", "id": "tag:google.com,2005:reader/item/0000000366c10d54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Nonparametric Kernel-Learning. (arXiv:1506.08776v2 [stat.ML] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1506.08776"}], "alternate": [{"href": "http://arxiv.org/abs/1506.08776", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a746af64acde\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a746af64acde&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Kernel methods are ubiquitous tools in machine learning. However, there is \noften little reason for the common practice of selecting a kernel a priori. \nEven if a universal approximating kernel is selected, the quality of the finite \nsample estimator may be greatly affected by the choice of kernel. Furthermore, \nwhen directly applying kernel methods, one typically needs to compute a $N \n\\times N$ Gram matrix of pairwise kernel evaluations to work with a dataset of \n$N$ instances. The computation of this Gram matrix precludes the direct \napplication of kernel methods on large datasets, and makes kernel learning \nespecially difficult. In this paper we introduce Bayesian nonparmetric \nkernel-learning (BaNK), a generic, data-driven framework for scalable learning \nof kernels. BaNK places a nonparametric prior on the spectral distribution of \nrandom frequencies allowing it to both learn kernels and scale to large \ndatasets. We show that this framework can be used for large scale regression \nand classification tasks. Furthermore, we show that BaNK outperforms several \nother scalable approaches for kernel learning on a variety of real world \ndatasets. \n</p>"}, "author": "Junier Oliva, Avinava Dubey, Andrew G. Wilson, Barnabas Poczos, Jeff Schneider, Eric P. Xing", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893425", "id": "tag:google.com,2005:reader/item/0000000366c10d5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Ensemble Adversarial Training: Attacks and Defenses. (arXiv:1705.07204v3 [stat.ML] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.07204"}], "alternate": [{"href": "http://arxiv.org/abs/1705.07204", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Adversarial examples are perturbed inputs designed to fool machine learning \nmodels. Adversarial training injects such examples into training data to \nincrease robustness. To scale this technique to large datasets, perturbations \nare crafted using fast single-step methods that maximize a linear approximation \nof the model's loss. We show that this form of adversarial training converges \nto a degenerate global minimum, wherein small curvature artifacts near the data \npoints obfuscate a linear approximation of the loss. The model thus learns to \ngenerate weak perturbations, rather than defend against strong ones. As a \nresult, we find that adversarial training remains vulnerable to black-box \nattacks, where we transfer perturbations computed on undefended models, as well \nas to a powerful novel single-step attack that escapes the non-smooth vicinity \nof the input data via a small random step. We further introduce Ensemble \nAdversarial Training, a technique that augments training data with \nperturbations transferred from other models. On ImageNet, Ensemble Adversarial \nTraining yields models with strong robustness to black-box attacks. In \nparticular, our most robust model won the first round of the NIPS 2017 \ncompetition on Defenses against Adversarial Attacks. \n</p>"}, "author": "Florian Tram&#xe8;r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893424", "id": "tag:google.com,2005:reader/item/0000000366c10d5e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "SEARNN: Training RNNs with Global-Local Losses. (arXiv:1706.04499v2 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.04499"}], "alternate": [{"href": "http://arxiv.org/abs/1706.04499", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose SEARNN, a novel training algorithm for recurrent neural networks \n(RNNs) inspired by the \"learning to search\" (L2S) approach to structured \nprediction. RNNs have been widely successful in structured prediction \napplications such as machine translation or parsing, and are commonly trained \nusing maximum likelihood estimation (MLE). Unfortunately, this training loss is \nnot always an appropriate surrogate for the test error: by only maximizing the \nground truth probability, it fails to exploit the wealth of information offered \nby structured losses. Further, it introduces discrepancies between training and \npredicting (such as exposure bias) that may hurt test performance. Instead, \nSEARNN leverages test-alike search space exploration to introduce global-local \nlosses that are closer to the test error. We first demonstrate improved \nperformance over MLE on two different tasks: OCR and spelling correction. Then, \nwe propose a subsampling strategy to enable SEARNN to scale to large vocabulary \nsizes. This allows us to validate the benefits of our approach on a machine \ntranslation task. \n</p>"}, "author": "R&#xe9;mi Leblond, Jean-Baptiste Alayrac, Anton Osokin, Simon Lacoste-Julien", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893423", "id": "tag:google.com,2005:reader/item/0000000366c10d62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Neural Phrase-based Machine Translation. (arXiv:1706.05565v5 [cs.CL] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.05565"}], "alternate": [{"href": "http://arxiv.org/abs/1706.05565", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our \nmethod explicitly models the phrase structures in output sequences using \nSleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence \nmodeling method. To mitigate the monotonic alignment requirement of SWAN, we \nintroduce a new layer to perform (soft) local reordering of input sequences. \nDifferent from existing neural machine translation (NMT) approaches, NPMT does \nnot use attention-based decoding mechanisms. Instead, it directly outputs \nphrases in a sequential order and can decode in linear time. Our experiments \nshow that NPMT achieves superior performances on IWSLT 2014 \nGerman-English/English-German and IWSLT 2015 English-Vietnamese machine \ntranslation tasks compared with strong NMT baselines. We also observe that our \nmethod produces meaningful phrases in output languages. \n</p>"}, "author": "Po-Sen Huang, Chong Wang, Sitao Huang, Dengyong Zhou, Li Deng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893422", "id": "tag:google.com,2005:reader/item/0000000366c10d65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multiresolution Kernel Approximation for Gaussian Process Regression. (arXiv:1708.02183v3 [stat.ML] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.02183"}], "alternate": [{"href": "http://arxiv.org/abs/1708.02183", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Gaussian process regression generally does not scale to beyond a few \nthousands data points without applying some sort of kernel approximation \nmethod. Most approximations focus on the high eigenvalue part of the spectrum \nof the kernel matrix, $K$, which leads to bad performance when the length scale \nof the kernel is small. In this paper we introduce Multiresolution Kernel \nApproximation (MKA), the first true broad bandwidth kernel approximation \nalgorithm. Important points about MKA are that it is memory efficient, and it \nis a direct method, which means that it also makes it easy to approximate \n$K^{-1}$ and $\\mathop{\\textrm{det}}(K)$. \n</p>"}, "author": "Yi Ding, Risi Kondor, Jonathan Eskreis-Winkler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893421", "id": "tag:google.com,2005:reader/item/0000000366c10d6a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields. (arXiv:1708.08819v3 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.08819"}], "alternate": [{"href": "http://arxiv.org/abs/1708.08819", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative adversarial networks (GANs) evolved into one of the most \nsuccessful unsupervised techniques for generating realistic images. Even though \nit has recently been shown that GAN training converges, GAN models often end up \nin local Nash equilibria that are associated with mode collapse or otherwise \nfail to model the target distribution. We introduce Coulomb GANs, which pose \nthe GAN learning problem as a potential field of charged particles, where \ngenerated samples are attracted to training set samples but repel each other. \nThe discriminator learns a potential field while the generator decreases the \nenergy by moving its samples along the vector (force) field determined by the \ngradient of the potential field. Through decreasing the energy, the GAN model \nlearns to generate samples according to the whole target distribution and does \nnot only cover some of its modes. We prove that Coulomb GANs possess only one \nNash equilibrium which is optimal in the sense that the model distribution \nequals the target distribution. We show the efficacy of Coulomb GANs on a \nvariety of image datasets. On LSUN and celebA, Coulomb GANs set a new state of \nthe art and produce a previously unseen variety of different samples. \n</p>"}, "author": "Thomas Unterthiner, Bernhard Nessler, Calvin Seward, G&#xfc;nter Klambauer, Martin Heusel, Hubert Ramsauer, Sepp Hochreiter", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066053", "id": "tag:google.com,2005:reader/item/0000000365f48c52", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks. (arXiv:1801.09335v1 [cs.LG])", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09335"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09335", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>It is desirable to train convolutional networks (CNNs) to run more \nefficiently during inference. In many cases however, the computational budget \nthat the system has for inference cannot be known beforehand during training, \nor the inference budget is dependent on the changing real-time resource \navailability. Thus, it is inadequate to train just inference-efficient CNNs, \nwhose inference costs are not adjustable and cannot adapt to varied inference \nbudgets. We propose a novel approach for cost-adjustable inference in CNNs - \nStochastic Downsampling Point (SDPoint). During training, SDPoint applies \nfeature map downsampling to a random point in the layer hierarchy, with a \nrandom downsampling ratio. The different stochastic downsampling configurations \nknown as SDPoint instances (of the same model) have computational costs \ndifferent from each other, while being trained to minimize the same prediction \nloss. Sharing network parameters across different instances provides \nsignificant regularization boost. During inference, one may handpick a SDPoint \ninstance that best fits the inference budget. The effectiveness of SDPoint, as \nboth a cost-adjustable inference approach and a regularizer, is validated \nthrough extensive experiments on image classification. \n</p>"}, "author": "Jason Kuen, Xiangfei Kong, Zhe Lin, Gang Wang, Jianxiong Yin, Simon See, Yap-Peng Tan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066052", "id": "tag:google.com,2005:reader/item/0000000365f48c58", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "On the Behavior of Convolutional Nets for Feature Extraction. (arXiv:1703.01127v4 [cs.NE] UPDATED)", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.01127"}], "alternate": [{"href": "http://arxiv.org/abs/1703.01127", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Deep neural networks are representation learning techniques. During training, \na deep net is capable of generating a descriptive language of unprecedented \nsize and detail in machine learning. Extracting the descriptive language coded \nwithin a trained CNN model (in the case of image data), and reusing it for \nother purposes is a field of interest, as it provides access to the visual \ndescriptors previously learnt by the CNN after processing millions of images, \nwithout requiring an expensive training phase. Contributions to this field \n(commonly known as feature representation transfer or transfer learning) have \nbeen purely empirical so far, extracting all CNN features from a single layer \nclose to the output and testing their performance by feeding them to a \nclassifier. This approach has provided consistent results, although its \nrelevance is limited to classification tasks. In a completely different \napproach, in this paper we statistically measure the discriminative power of \nevery single feature found within a deep CNN, when used for characterizing \nevery class of 11 datasets. We seek to provide new insights into the behavior \nof CNN features, particularly the ones from convolutional layers, as this can \nbe relevant for their application to knowledge representation and reasoning. \nOur results confirm that low and middle level features may behave differently \nto high level features, but only under certain conditions. We find that all CNN \nfeatures can be used for knowledge representation purposes both by their \npresence or by their absence, doubling the information a single CNN feature may \nprovide. We also study how much noise these features may include, and propose a \nthresholding approach to discard most of it. All these insights have a direct \napplication to the generation of CNN embedding spaces. \n</p>"}, "author": "Dario Garcia-Gasulla, Ferran Par&#xe9;s, Armand Vilalta, Jonatan Moreno, Eduard Ayguad&#xe9;, Jes&#xfa;s Labarta, Ulises Cort&#xe9;s, Toyotaro Suzumura", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066051", "id": "tag:google.com,2005:reader/item/0000000365f48c5d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multi-Pointer Co-Attention Networks for Recommendation. (arXiv:1801.09251v1 [cs.CL])", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09251"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09251", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Many recent state-of-the-art recommender systems such as D-ATT, TransNet and \nDeepCoNN exploit reviews for representation learning. This paper proposes a new \nneural architecture for recommendation with reviews. Our model operates on a \nmulti-hierarchical paradigm and is based on the intuition that not all reviews \nare created equal, i.e., only a select few are important. The importance, \nhowever, should be dynamically inferred depending on the current target. To \nthis end, we propose a review-by-review pointer-based learning scheme that \nextracts important reviews, subsequently matching them in a word-by-word \nfashion. This enables not only the most informative reviews to be utilized for \nprediction but also a deeper word-level interaction. Our pointer-based method \noperates with a novel gumbel-softmax based pointer mechanism that enables the \nincorporation of discrete vectors within differentiable neural architectures. \nOur pointer mechanism is co-attentive in nature, learning pointers which are \nco-dependent on user-item relationships. Finally, we propose a multi-pointer \nlearning scheme that learns to combine multiple views of interactions between \nuser and item. Overall, we demonstrate the effectiveness of our proposed model \nvia extensive experiments on \\textbf{24} benchmark datasets from Amazon and \nYelp. Empirical results show that our approach significantly outperforms \nexisting state-of-the-art, with up to 19% and 71% relative improvement when \ncompared to TransNet and DeepCoNN respectively. We study the behavior of our \nmulti-pointer learning mechanism, shedding light on evidence aggregation \npatterns in review-based recommender systems. \n</p>"}, "author": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066050", "id": "tag:google.com,2005:reader/item/0000000365f48c60", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Game of Sketches: Deep Recurrent Models of Pictionary-style Word Guessing. (arXiv:1801.09356v1 [cs.CV])", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09356"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09356", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a746af64affd\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a746af64affd&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The ability of intelligent agents to play games in human-like fashion is \npopularly considered a benchmark of progress in Artificial Intelligence. \nSimilarly, performance on multi-disciplinary tasks such as Visual Question \nAnswering (VQA) is considered a marker for gauging progress in Computer Vision. \nIn our work, we bring games and VQA together. Specifically, we introduce the \nfirst computational model aimed at Pictionary, the popular word-guessing social \ngame. We first introduce Sketch-QA, an elementary version of Visual Question \nAnswering task. Styled after Pictionary, Sketch-QA uses incrementally \naccumulated sketch stroke sequences as visual data. Notably, Sketch-QA involves \nasking a fixed question (\"What object is being drawn?\") and gathering \nopen-ended guess-words from human guessers. We analyze the resulting dataset \nand present many interesting findings therein. To mimic Pictionary-style \nguessing, we subsequently propose a deep neural model which generates \nguess-words in response to temporally evolving human-drawn sketches. Our model \neven makes human-like mistakes while guessing, thus amplifying the human \nmimicry factor. We evaluate our model on the large-scale guess-word dataset \ngenerated via Sketch-QA task and compare with various baselines. We also \nconduct a Visual Turing Test to obtain human impressions of the guess-words \ngenerated by humans and our model. Experimental results demonstrate the promise \nof our approach for Pictionary and similarly themed games. \n</p>"}, "author": "Ravi Kiran Sarvadevabhatla, Shiv Surya, Trisha Mittal, Venkatesh Babu Radhakrishnan", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066049", "id": "tag:google.com,2005:reader/item/0000000365f48c63", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Improved Tabu Search Heuristics for Static Dial-A-Ride Problem: Faster and Better Convergence. (arXiv:1801.09547v1 [cs.AI])", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09547"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09547", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Multi-vehicle routing has become increasingly important with the rapid \ndevelopment of autonomous vehicle technology. Dial-a-ride problem (DARP), a \nvariant of vehicle routing problem (VRP), deals with the allocation of customer \nrequests to vehicles, scheduling the pick-up and drop-off times and the \nsequence of serving those requests by ensuring high customer satisfaction with \nminimized travel cost. In this paper, we propose an improved tabu search (ITS) \nheuristic for static DARP with the objective of obtaining high-quality \nsolutions in short time. Two new techniques, initialization heuristic, and time \nwindow adjustment are proposed to achieve faster convergence to the global \noptimum. Various numerical experiments are conducted for the proposed solution \nmethodology using DARP test instances from the literature and the convergence \nspeed up is validated. \n</p>"}, "author": "Songguang Ho, Sarat Chandra Nagavarapu, Ramesh Ramasamy Pandi, Justin Dauwels", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066048", "id": "tag:google.com,2005:reader/item/0000000365f48c67", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Deep Learning Approach for Very Similar Objects Recognition Application on Chihuahua and Muffin Problem. (arXiv:1801.09573v1 [cs.AI])", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09573"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09573", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We address the problem to tackle the very similar objects like Chihuahua or \nmuffin problem to recognize at least in human vision level. Our regular deep \nstructured machine learning still does not solve it. We saw many times for \nabout year in our community the problem. Today we proposed the state-of-the-art \nsolution for it. Our approach is quite tricky to get the very high accuracy. We \npropose the deep transfer learning method which could be tackled all this type \nof problems not limited to just Chihuahua or muffin problem. It is the best \nmethod to train with small data set not like require huge amount data. \n</p>"}, "author": "Enkhtogtokh Togootogtokh, Amarzaya Amartuvshin", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066047", "id": "tag:google.com,2005:reader/item/0000000365f48c6b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Reinforcement Learning using Capsules in Advanced Game Environments. (arXiv:1801.09597v1 [cs.AI])", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09597"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09597", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Reinforcement Learning (RL) is a research area that has blossomed \ntremendously in recent years and has shown remarkable potential for artificial \nintelligence based opponents in computer games. This success is primarily due \nto vast capabilities of Convolutional Neural Networks (ConvNet), enabling \nalgorithms to extract useful information from noisy environments. Capsule \nNetwork (CapsNet) is a recent introduction to the Deep Learning algorithm group \nand has only barely begun to be explored. The network is an architecture for \nimage classification, with superior performance for classification of the MNIST \ndataset. CapsNets have not been explored beyond image classification. \n</p> \n<p>This thesis introduces the use of CapsNet for Q-Learning based game \nalgorithms. To successfully apply CapsNet in advanced game play, three main \ncontributions follow. First, the introduction of four new game environments as \nframeworks for RL research with increasing complexity, namely Flash RL, Deep \nLine Wars, Deep RTS, and Deep Maze. These environments fill the gap between \nrelatively simple and more complex game environments available for RL research \nand are in the thesis used to test and explore the CapsNet behavior. \n</p> \n<p>Second, the thesis introduces a generative modeling approach to produce \nartificial training data for use in Deep Learning models including CapsNets. We \nempirically show that conditional generative modeling can successfully generate \ngame data of sufficient quality to train a Deep Q-Network well. \n</p> \n<p>Third, we show that CapsNet is a reliable architecture for Deep Q-Learning \nbased algorithms for game AI. A capsule is a group of neurons that determine \nthe presence of objects in the data and is in the literature shown to increase \nthe robustness of training and predictions while lowering the amount training \ndata needed. It should, therefore, be ideally suited for game plays. \n</p>"}, "author": "Per-Arne Andersen", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066046", "id": "tag:google.com,2005:reader/item/0000000365f48c6e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Human-Machine Inference Networks For Smart Decision Making: Opportunities and Challenges. (arXiv:1801.09626v1 [cs.HC])", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09626"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09626", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The emerging paradigm of Human-Machine Inference Networks (HuMaINs) combines \ncomplementary cognitive strengths of humans and machines in an intelligent \nmanner to tackle various inference tasks and achieves higher performance than \neither humans or machines by themselves. While inference performance \noptimization techniques for human-only or sensor-only networks are quite \nmature, HuMaINs require novel signal processing and machine learning solutions. \nIn this paper, we present an overview of the HuMaINs architecture with a focus \non three main issues that include architecture design, inference algorithms \nincluding security/privacy challenges, and application areas/use cases. \n</p>"}, "author": "Aditya Vempaty, Bhavya Kailkhura, Pramod K. Varshney", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066045", "id": "tag:google.com,2005:reader/item/0000000365f48c70", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Playing FPS Games with Deep Reinforcement Learning. (arXiv:1609.05521v2 [cs.AI] UPDATED)", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1609.05521"}], "alternate": [{"href": "http://arxiv.org/abs/1609.05521", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Advances in deep reinforcement learning have allowed autonomous agents to \nperform well on Atari games, often outperforming humans, using only raw pixels \nto make their decisions. However, most of these games take place in 2D \nenvironments that are fully observable to the agent. In this paper, we present \nthe first architecture to tackle 3D environments in first-person shooter games, \nthat involve partially observable states. Typically, deep reinforcement \nlearning methods only utilize visual input for training. We present a method to \naugment these models to exploit game feature information such as the presence \nof enemies or items, during the training phase. Our model is trained to \nsimultaneously learn these features along with minimizing a Q-learning \nobjective, which is shown to dramatically improve the training speed and \nperformance of our agent. Our architecture is also modularized to allow \ndifferent models to be independently trained for different phases of the game. \nWe show that the proposed architecture substantially outperforms built-in AI \nagents of the game as well as humans in deathmatch scenarios. \n</p>"}, "author": "Guillaume Lample, Devendra Singh Chaplot", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066044", "id": "tag:google.com,2005:reader/item/0000000365f48c73", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Controllable Invariance through Adversarial Feature Learning. (arXiv:1705.11122v3 [cs.LG] UPDATED)", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.11122"}], "alternate": [{"href": "http://arxiv.org/abs/1705.11122", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Learning meaningful representations that maintain the content necessary for a \nparticular task while filtering away detrimental variations is a problem of \ngreat interest in machine learning. In this paper, we tackle the problem of \nlearning representations invariant to a specific factor or trait of data. The \nrepresentation learning process is formulated as an adversarial minimax game. \nWe analyze the optimal equilibrium of such a game and find that it amounts to \nmaximizing the uncertainty of inferring the detrimental factor given the \nrepresentation while maximizing the certainty of making task-specific \npredictions. On three benchmark tasks, namely fair and bias-free \nclassification, language-independent generation, and lighting-independent image \nclassification, we show that the proposed framework induces an invariant \nrepresentation, and leads to better generalization evidenced by the improved \nperformance. \n</p>"}, "author": "Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, Graham Neubig", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066043", "id": "tag:google.com,2005:reader/item/0000000365f48c75", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Run, skeleton, run: skeletal model in a physics-based simulation. (arXiv:1711.06922v2 [cs.AI] UPDATED)", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1711.06922"}], "alternate": [{"href": "http://arxiv.org/abs/1711.06922", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present our approach to solve a physics-based reinforcement \nlearning challenge \"Learning to Run\" with objective to train \nphysiologically-based human model to navigate a complex obstacle course as \nquickly as possible. The environment is computationally expensive, has a \nhigh-dimensional continuous action space and is stochastic. We benchmark state \nof the art policy-gradient methods and test several improvements, such as layer \nnormalization, parameter noise, action and state reflecting, to stabilize \ntraining and improve its sample-efficiency. We found that the Deep \nDeterministic Policy Gradient method is the most efficient method for this \nenvironment and the improvements we have introduced help to stabilize training. \nLearned models are able to generalize to new physical scenarios, e.g. different \nobstacle courses. \n</p>"}, "author": "Mikhail Pavlov, Sergey Kolesnikov, Sergey M. Plis", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066042", "id": "tag:google.com,2005:reader/item/0000000365f48c78", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Fast Cosmic Web Simulations with Generative Adversarial Networks. (arXiv:1801.09070v1 [astro-ph.CO])", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09070"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09070", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Dark matter in the universe evolves through gravity to form a complex network \nof halos, filaments, sheets and voids, that is known as the cosmic web. \nComputational models of the underlying physical processes, such as classical \nN-body simulations, are extremely resource intensive, as they track the action \nof gravity in an expanding universe using billions of particles as tracers of \nthe cosmic matter distribution. Therefore, upcoming cosmology experiments will \nface a computational bottleneck that may limit the exploitation of their full \nscientific potential. To address this challenge, we demonstrate the application \nof a machine learning technique called Generative Adversarial Networks (GAN) to \nlearn models that can efficiently generate new, physically realistic \nrealizations of the cosmic web. Our training set is a small, representative \nsample of 2D image snapshots from N-body simulations of size 500 and 100 Mpc. \nWe show that the GAN-produced results are qualitatively and quantitatively very \nsimilar to the originals. Generation of a new cosmic web realization with a GAN \ntakes a fraction of a second, compared to the many hours needed by the N-body \ntechnique. We anticipate that GANs will therefore play an important role in \nproviding extremely fast and precise simulations of cosmic web in the era of \nlarge cosmological surveys, such as Euclid and LSST. \n</p>"}, "author": "Andres C Rodriguez, Tomasz Kacprzak, Aurelien Lucchi, Adam Amara, Raphael Sgier, Janis Fluri, Thomas Hofmann, Alexandre R&#xe9;fr&#xe9;gier", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066041", "id": "tag:google.com,2005:reader/item/0000000365f48c7b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Gradient descent revisited via an adaptive online learning rate. (arXiv:1801.09136v1 [stat.ML])", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09136"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09136", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Any gradient descent optimization requires to choose a learning rate. With \ndeeper and deeper models, tuning that learning rate can easily become tedious \nand does not necessarily lead to an ideal convergence. We propose a variation \nof the gradient descent algorithm in the which the learning rate is not fixed. \nInstead, we learn the learning rate itself, either by another gradient descent \n(first-order method), or by Newton's method (second-order). This way, gradient \ndescent for any machine learning algorithm can be optimized. \n</p>"}, "author": "Mathieu Ravaut, Satya Gorti", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517289278066", "timestampUsec": "1517289278066040", "id": "tag:google.com,2005:reader/item/0000000365f48c80", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Deep-Learning the Landscape. (arXiv:1706.02714v3 [hep-th] UPDATED)", "published": 1517289278, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.02714"}], "alternate": [{"href": "http://arxiv.org/abs/1706.02714", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a746af64b240\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a746af64b240&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>We propose a paradigm to deep-learn the ever-expanding databases which have \nemerged in mathematical physics and particle phenomenology, as diverse as the \nstatistics of string vacua or combinatorial and algebraic geometry. As concrete \nexamples, we establish multi-layer neural networks as both classifiers and \npredictors and train them with a host of available data ranging from Calabi-Yau \nmanifolds and vector bundles, to quiver representations for gauge theories. We \nfind that even a relatively simple neural network can learn many significant \nquantities to astounding accuracy in a matter of minutes and can also predict \nhithertofore unencountered results. This paradigm should prove a valuable tool \nin various investigations in landscapes in physics as well as pure mathematics. \n</p>"}, "author": "Yang-Hui He", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}]}