{"all_articles": [{"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893436", "id": "tag:google.com,2005:reader/item/0000000366c10d01", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "ReNN: Rule-embedded Neural Networks. (arXiv:1801.09856v1 [cs.LG])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09856"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09856", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a71d6b0c361c\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a71d6b0c361c&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>The artificial neural network shows powerful ability of inference, but it is \nstill criticized for lack of interpretability and prerequisite needs of big \ndataset. This paper proposes the Rule-embedded Neural Network (ReNN) to \novercome the shortages. ReNN first makes local-based inferences to detect local \npatterns, and then uses rules based on domain knowledge about the local \npatterns to generate rule-modulated map. After that, ReNN makes global-based \ninferences that synthesizes the local patterns and the rule-modulated map. To \nsolve the optimization problem caused by rules, we use a two-stage optimization \nstrategy to train the ReNN model. By introducing rules into ReNN, we can \nstrengthen traditional neural networks with long-term dependencies which are \ndifficult to learn with limited empirical dataset, thus improving inference \naccuracy. The complexity of neural networks can be reduced since long-term \ndependencies are not modeled with neural connections, and thus the amount of \ndata needed to optimize the neural networks can be reduced. Besides, inferences \nfrom ReNN can be analyzed with both local patterns and rules, and thus have \nbetter interpretability. In this paper, ReNN has been validated with a \ntime-series detection problem. \n</p>"}, "author": "Hu Wang", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893433", "id": "tag:google.com,2005:reader/item/0000000366c10d0c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Over-representation of Extreme Events in Decision-Making: A Rational Metacognitive Account. (arXiv:1801.09848v1 [q-bio.NC])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09848"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09848", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The Availability bias, manifested in the over-representation of extreme \neventualities in decision-making, is a well-known cognitive bias, and is \ngenerally taken as evidence of human irrationality. In this work, we present \nthe first rational, metacognitive account of the Availability bias, formally \narticulated at Marr's algorithmic level of analysis. Concretely, we present a \nnormative, metacognitive model of how a cognitive system should over-represent \nextreme eventualities, depending on the amount of time available at its \ndisposal for decision-making. Our model also accounts for two well-known \nframing effects in human decision-making under risk---the fourfold pattern of \nrisk preferences in outcome probability (Tversky &amp; Kahneman, 1992) and in \noutcome magnitude (Markovitz, 1952)---thereby providing the first \nmetacognitively-rational basis for those effects. Empirical evidence, \nfurthermore, confirms an important prediction of our model. Surprisingly, our \nmodel is unimaginably robust with respect to its focal parameter. We discuss \nthe implications of our work for studies on human decision-making, and conclude \nby presenting a counterintuitive prediction of our model, which, if confirmed, \nwould have intriguing implications for human decision-making under risk. To our \nknowledge, our model is the first metacognitive, resource-rational process \nmodel of cognitive biases in decision-making. \n</p>"}, "author": "Ardavan S. Nobandegani, Kevin da Silva Castanheira, A. Ross Otto, Thomas R. Shultz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893432", "id": "tag:google.com,2005:reader/item/0000000366c10d15", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Optimal Number of Choices in Rating Contexts. (arXiv:1605.06588v6 [cs.AI] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1605.06588"}], "alternate": [{"href": "http://arxiv.org/abs/1605.06588", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In many settings people must give numerical scores to entities from a small \ndiscrete set. For instance, rating physical attractiveness from 1--5 on dating \nsites, or papers from 1--10 for conference reviewing. We study the problem of \nunderstanding when using a different number of options is optimal. For \nconcreteness we assume the true underlying scores are integers from 1--100. We \nconsider the case when scores are uniform random and Gaussian. We study when \nusing 2, 3, 4, 5, and 10 options is optimal in these models. One may expect \nthat using more options would always improve performance in this model, but we \nshow that this is not necessarily the case, and that using fewer choices---even \njust two---can surprisingly be optimal in certain situations. While in theory \nfor this setting it would be optimal to use all 100 options, in practice this \nis prohibitive, and it is preferable to utilize a smaller number of options due \nto humans' limited computational resources. Our results suggest that using a \nsmaller number of options than is typical could be optimal in certain \nsituations. This would have many potential applications, as settings requiring \nentities to be ranked by humans are ubiquitous. \n</p>"}, "author": "Sam Ganzfried, Farzana Yusuf", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893431", "id": "tag:google.com,2005:reader/item/0000000366c10d1b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Towards a Quantum World Wide Web. (arXiv:1703.06642v2 [cs.AI] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1703.06642"}], "alternate": [{"href": "http://arxiv.org/abs/1703.06642", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We elaborate a quantum model for the meaning associated with corpora of \nwritten documents, like the pages forming the World Wide Web. To that end, we \nare guided by how physicists constructed quantum theory for microscopic \nentities, which unlike classical objects cannot be fully represented in our \nspatial theater. We suggest that a similar construction needs to be carried out \nby linguists and computational scientists, to capture the full meaning carried \nby collections of documental entities. More precisely, we show how to associate \na quantum-like 'entity of meaning' to a 'language entity formed by printed \ndocuments', considering the latter as the collection of traces that are left by \nthe former, in specific results of search actions that we describe as \nmeasurements. In other words, we offer a perspective where a collection of \ndocuments, like the Web, is described as the space of manifestation of a more \ncomplex entity - the QWeb - which is the object of our modeling, drawing its \ninspiration from previous studies on operational-realistic approaches to \nquantum physics and quantum modeling of human cognition and decision-making. We \nemphasize that a consistent QWeb model needs to account for the observed \ncorrelations between words appearing in printed documents, e.g., \nco-occurrences, as the latter would depend on the 'meaning connections' \nexisting between the concepts that are associated with these words. In that \nrespect, we show that both 'context and interference (quantum) effects' are \nrequired to explain the probabilities calculated by counting the relative \nnumber of documents containing certain words and co-ocurrrences of words. \n</p>"}, "author": "Diederik Aerts, Jonito Aerts Arguelles, Lester Beltran, Lyneth Beltran, Isaac Distrito, Massimiliano Sassoli de Bianchi, Sandro Sozzo, Tomas Veloz", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893430", "id": "tag:google.com,2005:reader/item/0000000366c10d22", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Towards Synthesizing Complex Programs from Input-Output Examples. (arXiv:1706.01284v2 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.01284"}], "alternate": [{"href": "http://arxiv.org/abs/1706.01284", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In recent years, deep learning techniques have been developed to improve the \nperformance of program synthesis from input-output examples. Albeit its \nsignificant progress, the programs that can be synthesized by state-of-the-art \napproaches are still simple in terms of their complexity. In this work, we move \na significant step forward along this direction by proposing a new class of \nchallenging tasks in the domain of program synthesis from input-output \nexamples: learning a context-free parser from pairs of input programs and their \nparse trees. We show that this class of tasks are much more challenging than \npreviously studied tasks, and the test accuracy of existing approaches is \nalmost 0%. \n</p> \n<p>We tackle the challenges by developing three novel techniques inspired by \nthree novel observations, which reveal the key ingredients of using deep \nlearning to synthesize a complex program. First, the use of a \nnon-differentiable machine is the key to effectively restrict the search space. \nThus our proposed approach learns a neural program operating a domain-specific \nnon-differentiable machine. Second, recursion is the key to achieve \ngeneralizability. Thus, we bake-in the notion of recursion in the design of our \nnon-differentiable machine. Third, reinforcement learning is the key to learn \nhow to operate the non-differentiable machine, but it is also hard to train the \nmodel effectively with existing reinforcement learning algorithms from a cold \nboot. We develop a novel two-phase reinforcement learning-based search \nalgorithm to overcome this issue. In our evaluation, we show that using our \nnovel approach, neural parsing programs can be learned to achieve 100% test \naccuracy on test inputs that are 500x longer than the training samples. \n</p>"}, "author": "Xinyun Chen, Chang Liu, Dawn Song", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893429", "id": "tag:google.com,2005:reader/item/0000000366c10d2c", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Bayesian Neural Networks. (arXiv:1801.07710v2 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.07710"}], "alternate": [{"href": "http://arxiv.org/abs/1801.07710", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>This paper describes and discusses Bayesian Neural Network (BNN). The paper \nshowcases a few different applications of them for classification and \nregression problems. BNNs are comprised of a Probabilistic Model and a Neural \nNetwork. The intent of such a design is to combine the strengths of Neural \nNetworks and Stochastic modeling. Neural Networks exhibit continuous function \napproximator capabilities. Stochastic models allow direct specification of a \nmodel with known interaction between parameters to generate data. During the \nprediction phase, stochastic models generate a complete posterior distribution \nand produce probabilistic guarantees on the predictions. Thus BNNs are a unique \ncombination of neural network and stochastic models with the stochastic model \nforming the core of this integration. BNNs can then produce probabilistic \nguarantees on it's predictions and also generate the distribution of parameters \nthat it has learnt from the observations. That means, in the parameter space, \none can deduce the nature and shape of the neural network's learnt parameters. \nThese two characteristics makes them highly attractive to theoreticians as well \nas practitioners. Recently there has been a lot of activity in this area, with \nthe advent of numerous probabilistic programming libraries such as: PyMC3, \nEdward, Stan etc. Further this area is rapidly gaining ground as a standard \nmachine learning approach for numerous problems \n</p>"}, "author": "Vikram Mullachery, Aniruddh Khera, Amir Husain", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893428", "id": "tag:google.com,2005:reader/item/0000000366c10d3d", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Cross-type Biomedical Named Entity Recognition with Deep Multi-Task Learning. (arXiv:1801.09851v1 [cs.IR])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09851"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09851", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Motivation: Biomedical named entity recognition (BioNER) is the most \nfundamental task in biomedical text mining. State-of-the-art BioNER systems \noften require handcrafted features specifically designed for each type of \nbiomedical entities. This feature generation process requires intensive labors \nfrom biomedical and linguistic experts, and makes it difficult to adapt these \nsystems to new biomedical entity types. Although recent studies explored using \nneural network models for BioNER to free experts from manual feature \ngeneration, these models still require substantial human efforts to annotate \nmassive training data. \n</p> \n<p>Results: We propose a multi-task learning framework for BioNER that is based \non neural network models to save human efforts. We build a global model by \ncollectively training multiple models that share parameters, each model \ncapturing the characteristics of a different biomedical entity type. In \nexperiments on five BioNER benchmark datasets covering four major biomedical \nentity types, our model outperforms state-of-the-art systems and other neural \nnetwork models by a large margin, even when only limited training data are \navailable. Further analysis shows that the large performance gains come from \nsharing character- and word-level information between different biomedical \nentities. The approach creates new opportunities for text-mining approaches to \nhelp biomedical scientists better exploit knowledge in biomedical literature. \n</p>"}, "author": "Xuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang, Marinka Zitnik, Jingbo Shang, Curtis Langlotz, Jiawei Han", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893427", "id": "tag:google.com,2005:reader/item/0000000366c10d4b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Cardiac Arrhythmia Detection from ECG Combining Convolutional and Long Short-Term Memory Networks. (arXiv:1801.10033v1 [eess.SP])", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10033"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10033", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Objectives: Atrial fibrillation (AF) is a common heart rhythm disorder \nassociated with deadly and debilitating consequences including heart failure, \nstroke, poor mental health, reduced quality of life and death. Having an \nautomatic system that diagnoses various types of cardiac arrhythmias would \nassist cardiologists to initiate appropriate preventive measures and to improve \nthe analysis of cardiac disease. To this end, this paper introduces a new \napproach to detect and classify automatically cardiac arrhythmias in \nelectrocardiograms (ECG) recordings. \n</p> \n<p>Methods: The proposed approach used a combination of Convolution Neural \nNetworks (CNNs) and a sequence of Long Short-Term Memory (LSTM) units, with \npooling, dropout and normalization techniques to improve their accuracy. The \nnetwork predicted a classification at every 18th input sample and we selected \nthe final prediction for classification. Results were cross-validated on the \nPhysionet Challenge 2017 training dataset, which contains 8,528 single lead ECG \nrecordings lasting from 9s to just over 60s. \n</p> \n<p>Results: Using the proposed structure and no explicit feature selection, \n10-fold stratified cross-validation gave an overall F-measure of 0.83.10-0.015 \non the held-out test data (mean-standard deviation over all folds) and 0.80 on \nthe hidden dataset of the Challenge entry server. \n</p>"}, "author": "Philip Warrick (1), Masun Nabhan Homsi (2) ((1) PeriGen. Inc., Montreal, Canada, (2) Simon Bolivar University, Caracas, Venezuela)", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893426", "id": "tag:google.com,2005:reader/item/0000000366c10d54", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bayesian Nonparametric Kernel-Learning. (arXiv:1506.08776v2 [stat.ML] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1506.08776"}], "alternate": [{"href": "http://arxiv.org/abs/1506.08776", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Kernel methods are ubiquitous tools in machine learning. However, there is \noften little reason for the common practice of selecting a kernel a priori. \nEven if a universal approximating kernel is selected, the quality of the finite \nsample estimator may be greatly affected by the choice of kernel. Furthermore, \nwhen directly applying kernel methods, one typically needs to compute a $N \n\\times N$ Gram matrix of pairwise kernel evaluations to work with a dataset of \n$N$ instances. The computation of this Gram matrix precludes the direct \napplication of kernel methods on large datasets, and makes kernel learning \nespecially difficult. In this paper we introduce Bayesian nonparmetric \nkernel-learning (BaNK), a generic, data-driven framework for scalable learning \nof kernels. BaNK places a nonparametric prior on the spectral distribution of \nrandom frequencies allowing it to both learn kernels and scale to large \ndatasets. We show that this framework can be used for large scale regression \nand classification tasks. Furthermore, we show that BaNK outperforms several \nother scalable approaches for kernel learning on a variety of real world \ndatasets. \n</p>"}, "author": "Junier Oliva, Avinava Dubey, Andrew G. Wilson, Barnabas Poczos, Jeff Schneider, Eric P. Xing", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893425", "id": "tag:google.com,2005:reader/item/0000000366c10d5b", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "Ensemble Adversarial Training: Attacks and Defenses. (arXiv:1705.07204v3 [stat.ML] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1705.07204"}], "alternate": [{"href": "http://arxiv.org/abs/1705.07204", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a71d6b0c3948\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a71d6b0c3948&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Adversarial examples are perturbed inputs designed to fool machine learning \nmodels. Adversarial training injects such examples into training data to \nincrease robustness. To scale this technique to large datasets, perturbations \nare crafted using fast single-step methods that maximize a linear approximation \nof the model's loss. We show that this form of adversarial training converges \nto a degenerate global minimum, wherein small curvature artifacts near the data \npoints obfuscate a linear approximation of the loss. The model thus learns to \ngenerate weak perturbations, rather than defend against strong ones. As a \nresult, we find that adversarial training remains vulnerable to black-box \nattacks, where we transfer perturbations computed on undefended models, as well \nas to a powerful novel single-step attack that escapes the non-smooth vicinity \nof the input data via a small random step. We further introduce Ensemble \nAdversarial Training, a technique that augments training data with \nperturbations transferred from other models. On ImageNet, Ensemble Adversarial \nTraining yields models with strong robustness to black-box attacks. In \nparticular, our most robust model won the first round of the NIPS 2017 \ncompetition on Defenses against Adversarial Attacks. \n</p>"}, "author": "Florian Tram&#xe8;r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893424", "id": "tag:google.com,2005:reader/item/0000000366c10d5e", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "SEARNN: Training RNNs with Global-Local Losses. (arXiv:1706.04499v2 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.04499"}], "alternate": [{"href": "http://arxiv.org/abs/1706.04499", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>We propose SEARNN, a novel training algorithm for recurrent neural networks \n(RNNs) inspired by the \"learning to search\" (L2S) approach to structured \nprediction. RNNs have been widely successful in structured prediction \napplications such as machine translation or parsing, and are commonly trained \nusing maximum likelihood estimation (MLE). Unfortunately, this training loss is \nnot always an appropriate surrogate for the test error: by only maximizing the \nground truth probability, it fails to exploit the wealth of information offered \nby structured losses. Further, it introduces discrepancies between training and \npredicting (such as exposure bias) that may hurt test performance. Instead, \nSEARNN leverages test-alike search space exploration to introduce global-local \nlosses that are closer to the test error. We first demonstrate improved \nperformance over MLE on two different tasks: OCR and spelling correction. Then, \nwe propose a subsampling strategy to enable SEARNN to scale to large vocabulary \nsizes. This allows us to validate the benefits of our approach on a machine \ntranslation task. \n</p>"}, "author": "R&#xe9;mi Leblond, Jean-Baptiste Alayrac, Anton Osokin, Simon Lacoste-Julien", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893423", "id": "tag:google.com,2005:reader/item/0000000366c10d62", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Towards Neural Phrase-based Machine Translation. (arXiv:1706.05565v5 [cs.CL] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1706.05565"}], "alternate": [{"href": "http://arxiv.org/abs/1706.05565", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our \nmethod explicitly models the phrase structures in output sequences using \nSleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence \nmodeling method. To mitigate the monotonic alignment requirement of SWAN, we \nintroduce a new layer to perform (soft) local reordering of input sequences. \nDifferent from existing neural machine translation (NMT) approaches, NPMT does \nnot use attention-based decoding mechanisms. Instead, it directly outputs \nphrases in a sequential order and can decode in linear time. Our experiments \nshow that NPMT achieves superior performances on IWSLT 2014 \nGerman-English/English-German and IWSLT 2015 English-Vietnamese machine \ntranslation tasks compared with strong NMT baselines. We also observe that our \nmethod produces meaningful phrases in output languages. \n</p>"}, "author": "Po-Sen Huang, Chong Wang, Sitao Huang, Dengyong Zhou, Li Deng", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893422", "id": "tag:google.com,2005:reader/item/0000000366c10d65", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Multiresolution Kernel Approximation for Gaussian Process Regression. (arXiv:1708.02183v3 [stat.ML] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.02183"}], "alternate": [{"href": "http://arxiv.org/abs/1708.02183", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Gaussian process regression generally does not scale to beyond a few \nthousands data points without applying some sort of kernel approximation \nmethod. Most approximations focus on the high eigenvalue part of the spectrum \nof the kernel matrix, $K$, which leads to bad performance when the length scale \nof the kernel is small. In this paper we introduce Multiresolution Kernel \nApproximation (MKA), the first true broad bandwidth kernel approximation \nalgorithm. Important points about MKA are that it is memory efficient, and it \nis a direct method, which means that it also makes it easy to approximate \n$K^{-1}$ and $\\mathop{\\textrm{det}}(K)$. \n</p>"}, "author": "Yi Ding, Risi Kondor, Jonathan Eskreis-Winkler", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517379621893", "timestampUsec": "1517379621893421", "id": "tag:google.com,2005:reader/item/0000000366c10d6a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields. (arXiv:1708.08819v3 [cs.LG] UPDATED)", "published": 1517379622, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1708.08819"}], "alternate": [{"href": "http://arxiv.org/abs/1708.08819", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Generative adversarial networks (GANs) evolved into one of the most \nsuccessful unsupervised techniques for generating realistic images. Even though \nit has recently been shown that GAN training converges, GAN models often end up \nin local Nash equilibria that are associated with mode collapse or otherwise \nfail to model the target distribution. We introduce Coulomb GANs, which pose \nthe GAN learning problem as a potential field of charged particles, where \ngenerated samples are attracted to training set samples but repel each other. \nThe discriminator learns a potential field while the generator decreases the \nenergy by moving its samples along the vector (force) field determined by the \ngradient of the potential field. Through decreasing the energy, the GAN model \nlearns to generate samples according to the whole target distribution and does \nnot only cover some of its modes. We prove that Coulomb GANs possess only one \nNash equilibrium which is optimal in the sense that the model distribution \nequals the target distribution. We show the efficacy of Coulomb GANs on a \nvariety of image datasets. On LSUN and celebA, Coulomb GANs set a new state of \nthe art and produce a previously unseen variety of different samples. \n</p>"}, "author": "Thomas Unterthiner, Bernhard Nessler, Calvin Seward, G&#xfc;nter Klambauer, Martin Heusel, Hubert Ramsauer, Sepp Hochreiter", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_good.xml", "title": "Good papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517375732311", "timestampUsec": "1517375732310848", "id": "tag:google.com,2005:reader/item/0000000366b945f6", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "The Benefits of Population Diversity in Evolutionary Algorithms: A Survey of Rigorous Runtime Analyses. (arXiv:1801.10087v1 [cs.NE])", "published": 1517375733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.10087"}], "alternate": [{"href": "http://arxiv.org/abs/1801.10087", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Population diversity is crucial in evolutionary algorithms to enable global \nexploration and to avoid poor performance due to premature convergence. This \nbook chapter reviews runtime analyses that have shown benefits of population \ndiversity, either through explicit diversity mechanisms or through naturally \nemerging diversity. These works show that the benefits of diversity are \nmanifold: diversity is important for global exploration and the ability to find \nseveral global optima. Diversity enhances crossover and enables crossover to be \nmore effective than mutation. Diversity can be crucial in dynamic optimization, \nwhen the problem landscape changes over time. And, finally, it facilitates \nsearch for the whole Pareto front in evolutionary multiobjective optimization. \nThe presented analyses rigorously quantify the performance of evolutionary \nalgorithms in the light of population diversity, laying the foundation for a \nrigorous understanding of how search dynamics are affected by the presence or \nabsence of population diversity and the introduction of diversity mechanisms. \n</p>"}, "author": "Dirk Sudholt", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml", "title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517375732311", "timestampUsec": "1517375732310847", "id": "tag:google.com,2005:reader/item/0000000366b945fc", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Deep Quaternion Networks. (arXiv:1712.04604v2 [cs.NE] UPDATED)", "published": 1517375733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1712.04604"}], "alternate": [{"href": "http://arxiv.org/abs/1712.04604", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>The field of deep learning has seen significant advancement in recent years. \nHowever, much of the existing work has been focused on real-valued numbers. \nRecent work has shown that a deep learning system using the complex numbers can \nbe deeper for a fixed parameter budget compared to its real-valued counterpart. \nIn this work, we explore the benefits of generalizing one step further into the \nhyper-complex numbers, quaternions specifically, and provide the architecture \ncomponents needed to build deep quaternion networks. We go over quaternion \nconvolutions, present a quaternion weight initialization scheme, and present \nalgorithms for quaternion batch-normalization. These pieces are tested in a \nclassification model by end-to-end training on the CIFAR-10 and CIFAR-100 data \nsets and a segmentation model by end-to-end training on the KITTI Road \nSegmentation data set. The quaternion networks show improved convergence \ncompared to real-valued and complex-valued networks, especially on the \nsegmentation task. \n</p>"}, "author": "Chase Gaudet, Anthony Maida", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml", "title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517375732311", "timestampUsec": "1517375732310845", "id": "tag:google.com,2005:reader/item/0000000366b94600", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Bounded Policy Synthesis for POMDPs with Safe-Reachability Objectives. (arXiv:1801.09780v1 [cs.RO])", "published": 1517375733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09780"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09780", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Planning robust executions under uncertainty is a fundamental challenge for \nbuilding autonomous robots. Partially Observable Markov Decision Processes \n(POMDPs) provide a standard framework for modeling uncertainty in many robot \napplications. A key algorithmic problem for POMDPs is policy synthesis. While \nthis problem has traditionally been posed w.r.t. optimality objectives, many \nrobot applications are better modeled by POMDPs where the objective is a \nboolean requirement. In this paper, we study the latter problem in a setting \nwhere the requirement is a safe-reachability property, which states that with a \nprobability above a certain threshold, it is possible to eventually reach a \ngoal state while satisfying a safety requirement. The central challenge in our \nproblem is that it requires reasoning over a vast space of probability \ndistributions. What's more, it has been shown that policy synthesis of POMDPs \nwith reachability objectives is undecidable in general. To address these \nchallenges, we introduce the notion of a goal-constrained belief space, which \nonly contains beliefs (probability distributions over states) reachable from \nthe initial belief under desired executions. This constrained space is \ngenerally much smaller than the original belief space. Our approach compactly \nrepresents this space over a bounded horizon using symbolic constraints, and \nemploys an incremental Satisfiability Modulo Theories (SMT) solver to \nefficiently search for a valid policy over it. We evaluate our method using a \ncase study involving a partially observable robotics domain with uncertain \nobstacles. Our results suggest that it is possible to synthesize policies over \nlarge belief spaces with a small number of SMT solver calls by focusing on \ngoal-constrained belief space, and our method o ers a stronger guarantee of \nboth safety and reachability than alternative unconstrained/constrained POMDP \nformulations. \n</p>"}, "author": "Yue Wang, Swarat Chaudhuri, Lydia E. Kavraki", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml", "title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517375732311", "timestampUsec": "1517375732310844", "id": "tag:google.com,2005:reader/item/0000000366b9460a", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Evaluating approaches for supervised semantic labeling. (arXiv:1801.09788v1 [cs.LG])", "published": 1517375733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09788"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09788", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>Relational data sources are still one of the most popular ways to store \nenterprise or Web data, however, the issue with relational schema is the lack \nof a well-defined semantic description. A common ontology provides a way to \nrepresent the meaning of a relational schema and can facilitate the integration \nof heterogeneous data sources within a domain. Semantic labeling is achieved by \nmapping attributes from the data sources to the classes and properties in the \nontology. We formulate this problem as a multi-class classification problem \nwhere previously labeled data sources are used to learn rules for labeling new \ndata sources. The majority of existing approaches for semantic labeling have \nfocused on data integration challenges such as naming conflicts and semantic \nheterogeneity. In addition, machine learning approaches typically have issues \naround class imbalance, lack of labeled instances and relative importance of \nattributes. To address these issues, we develop a new machine learning model \nwith engineered features as well as two deep learning models which do not \nrequire extensive feature engineering. We evaluate our new approaches with the \nstate-of-the-art. \n</p>"}, "author": "Natalia Ruemmele, Yuriy Tyshetskiy, Alex Collins", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml", "title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517375732311", "timestampUsec": "1517375732310843", "id": "tag:google.com,2005:reader/item/0000000366b94611", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/label/arXiv"], "title": "Predicting Rapid Fire Growth (Flashover) Using Conditional Generative Adversarial Networks. (arXiv:1801.09804v1 [cs.AI])", "published": 1517375733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09804"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09804", "type": "text/html"}], "summary": {"direction": "ltr", "content": "<p>A flashover occurs when a fire spreads very rapidly through crevices due to \nintense heat. Flashovers present one of the most frightening and challenging \nfire phenomena to those who regularly encounter them: firefighters. \nFirefighters' safety and lives often depend on their ability to predict \nflashovers before they occur. Typical pre-flashover fire characteristics \ninclude dark smoke, high heat, and rollover (\"angel fingers\") and can be \nquantified by color, size, and shape. Using a color video stream from a \nfirefighter's body camera, we applied generative adversarial neural networks \nfor image enhancement. The neural networks were trained to enhance very dark \nfire and smoke patterns in videos and monitor dynamic changes in smoke and fire \nareas. Preliminary tests with limited flashover training videos showed that we \npredicted a flashover as early as 55 seconds before it occurred. \n</p>"}, "author": "Kyongsik Yun, Jessi Bustos, Thomas Lu", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml", "title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/"}}, {"crawlTimeMsec": "1517375732311", "timestampUsec": "1517375732310842", "id": "tag:google.com,2005:reader/item/0000000366b94616", "categories": ["user/1005689817/state/com.google/reading-list", "user/1005689817/state/com.google/read", "user/1005689817/state/com.google/like", "user/1005689817/label/arXiv"], "title": "The Intriguing Properties of Model Explanations. (arXiv:1801.09808v1 [cs.LG])", "published": 1517375733, "updated": 0, "canonical": [{"href": "http://arxiv.org/abs/1801.09808"}], "alternate": [{"href": "http://arxiv.org/abs/1801.09808", "type": "text/html"}], "summary": {"direction": "ltr", "content": "\n                <center>\n                    <div style=\"text-align:center;color:#424242;font-size:0.8em;margin-bottom:30px;\">\n                        <div>\n                            Ads from Inoreader &bull; <a target=\"_blank\" style=\"text-decoration:underline;color:#424242;\" href=\"https://www.inoreader.com/about_ads\">Remove</a>\n                        </div>\n                        <div style=\"margin-top:10px;\">\n                            <a href=\"http://www.inoreader.com/adv/www/delivery/ck.php?n=a49f975d&cb=5a71d6b0c3c56\" target=\"_blank\"><img src=\"http://www.inoreader.com/adv/www/delivery/avw.php?zoneid=23&from_api=1&cb=5a71d6b0c3c56&n=a49f975d&sens=0&pd=2\" style=\"border:0;width:320px;height:50px;margin:0; auto 0 auto !important;float:none !important;\"></a>\n                        </div>\n                    </div>\n                    <div style=\"clear:both;\">\n                    </div>\n                </center><p>Linear approximations to the decision boundary of a complex model have become \none of the most popular tools for interpreting predictions. In this paper, we \nstudy such linear explanations produced either post-hoc by a few recent methods \nor generated along with predictions with contextual explanation networks \n(CENs). We focus on two questions: (i) whether linear explanations are always \nconsistent or can be misleading, and (ii) when integrated into the prediction \nprocess, whether and how explanations affect the performance of the model. Our \nanalysis sheds more light on certain properties of explanations produced by \ndifferent methods and suggests that learning models that explain and predict \njointly is often advantageous. \n</p>"}, "author": "Maruan Al-Shedivat, Avinava Dubey, Eric P. Xing", "likingUsers": [], "comments": [], "commentsNum": -1, "annotations": [], "origin": {"streamId": "feed/https://raw.githubusercontent.com/bednarrski/rss-classifier/master/papers_bad.xml", "title": "Boring papers from arXiv", "htmlUrl": "http://arxiv.org/"}}]}