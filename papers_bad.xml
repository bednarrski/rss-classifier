<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08085"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.00268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06006"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02502"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04932"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08083"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08202"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08242"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08247"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.03074"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08092"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09251"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.01933"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01907"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09676"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05484"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07172"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07944"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07956"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08010"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08049"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08117"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08141"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08151"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08195"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08235"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08295"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08301"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08317"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08324"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.08618"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.02111"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.04546"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.02883"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06373"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06695"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05112"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01811"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02199"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06784"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.08085">
<title>Inference of Quantized Neural Networks on Heterogeneous All-Programmable Devices. (arXiv:1806.08085v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.08085</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have established as a generic and powerful means to approach
challenging problems such as image classification, object detection or decision
making. Their successful employment foots on an enormous demand of compute. The
quantization of network parameters and the processed data has proven a valuable
measure to reduce the challenges of network inference so effectively that the
feasible scope of applications is expanded even into the embedded domain. This
paper describes the making of a real-time object detection in a live video
stream processed on an embedded all-programmable device. The presented case
illustrates how the required processing is tamed and parallelized across both
the CPU cores and the programmable logic and how the most suitable resources
and powerful extensions, such as NEON vectorization, are leveraged for the
individual processing steps. The crafted result is an extended Darknet
framework implementing a fully integrated, end-to-end solution from video
capture over object annotation to video output applying neural network
inference at different quantization levels running at 16~frames per second on
an embedded Zynq UltraScale+ (XCZU3EG) platform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preusser_T/0/1/0/all/0/1&quot;&gt;Thomas B. Preu&amp;#xdf;er&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gambardella_G/0/1/0/all/0/1&quot;&gt;Giulio Gambardella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fraser_N/0/1/0/all/0/1&quot;&gt;Nicholas Fraser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blott_M/0/1/0/all/0/1&quot;&gt;Michaela Blott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.00268">
<title>Algorithmically probable mutations reproduce aspects of evolution such as convergence rate, genetic memory, and modularity. (arXiv:1709.00268v8 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1709.00268</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural selection explains how life has evolved over millions of years from
more primitive forms. The speed at which this happens, however, has sometimes
defied formal explanations when based on random (uniformly distributed)
mutations. Here we investigate the application of a simplicity bias based on a
natural but algorithmic distribution of mutations (no recombination) in various
examples, particularly binary matrices in order to compare evolutionary
convergence rates. Results both on synthetic and on small biological examples
indicate an accelerated rate when mutations are not statistical uniform but
\textit{algorithmic uniform}. We show that algorithmic distributions can evolve
modularity and genetic memory by preservation of structures when they first
occur sometimes leading to an accelerated production of diversity but also
population extinctions, possibly explaining naturally occurring phenomena such
as diversity explosions (e.g. the Cambrian) and massive extinctions (e.g. the
End Triassic) whose causes are currently a cause for debate. The natural
approach introduced here appears to be a better approximation to biological
evolution than models based exclusively upon random uniform mutations, and it
also approaches a formal version of open-ended evolution based on previous
formal results. These results validate some suggestions in the direction that
computation may be an equally important driver of evolution. We also show that
inducing the method on problems of optimization, such as genetic algorithms,
has the potential to accelerate convergence of artificial evolutionary
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Orozco_S/0/1/0/all/0/1&quot;&gt;Santiago Hern&amp;#xe1;ndez-Orozco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiani_N/0/1/0/all/0/1&quot;&gt;Narsis A. Kiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenil_H/0/1/0/all/0/1&quot;&gt;Hector Zenil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06006">
<title>Hindsight policy gradients. (arXiv:1711.06006v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06006</link>
<description rdf:parseType="Literal">&lt;p&gt;A reinforcement learning agent that needs to pursue different goals across
episodes requires a goal-conditional policy. In addition to their potential to
generalize desirable behavior to unseen goals, such policies may also enable
higher-level planning based on subgoals. In sparse-reward environments, the
capacity to exploit information about the degree to which an arbitrary goal has
been achieved while another goal was intended appears crucial to enable sample
efficient learning. However, reinforcement learning agents have only recently
been endowed with such capacity for hindsight. In this paper, we demonstrate
how hindsight can be introduced to policy gradient methods, generalizing this
idea to a broad class of successful algorithms. Our experiments on a diverse
selection of sparse-reward environments show that hindsight leads to a
remarkable increase in sample efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rauber_P/0/1/0/all/0/1&quot;&gt;Paulo Rauber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ummadisingu_A/0/1/0/all/0/1&quot;&gt;Avinash Ummadisingu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mutz_F/0/1/0/all/0/1&quot;&gt;Filipe Mutz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1&quot;&gt;Juergen Schmidhuber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02502">
<title>GP-RVM: Genetic Programing-based Symbolic Regression Using Relevance Vector Machine. (arXiv:1806.02502v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02502</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a hybrid basis function construction method (GP-RVM) for
Symbolic Regression problem, which combines an extended version of Genetic
Programming called Kaizen Programming and Relevance Vector Machine to evolve an
optimal set of basis functions. Different from traditional evolutionary
algorithms where a single individual is a complete solution, our method
proposes a solution based on linear combination of basis functions built from
individuals during the evolving process. RVM which is a sparse Bayesian kernel
method selects suitable functions to constitute the basis. RVM determines the
posterior weight of a function by evaluating its quality and sparsity. The
solution produced by GP-RVM is a sparse Bayesian linear model of the
coefficients of many non-linear functions. Our hybrid approach is focused on
nonlinear white-box models selecting the right combination of functions to
build robust predictions without prior knowledge about data. Experimental
results show that GP-RVM outperforms conventional methods, which suggest that
it is an efficient and accurate technique for solving SR. The computational
complexity of GP-RVM scales in $O( M^{3})$, where $M$ is the number of
functions in the basis set and is typically much smaller than the number $N$ of
training patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rad_H/0/1/0/all/0/1&quot;&gt;Hossein Izadi Rad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Ji Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iba_H/0/1/0/all/0/1&quot;&gt;Hitoshi Iba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04932">
<title>Reservoir Computing Hardware with Cellular Automata. (arXiv:1806.04932v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04932</link>
<description rdf:parseType="Literal">&lt;p&gt;Elementary cellular automata (ECA) is a widely studied one-dimensional
processing methodology where the successive iteration of the automaton may lead
to the recreation of a rich pattern dynamic. Recently, cellular automata have
been proposed as a feasible way to implement Reservoir Computing (RC) systems
in which the automata rule is fixed and the training is performed using a
linear regression. In this work we perform an exhaustive study of the
performance of the different ECA rules when applied to pattern recognition of
time-independent input signals using a RC scheme. Once the different ECA rules
have been tested, the most accurate one (rule 90) is selected to implement a
digital circuit. Rule 90 is easily reproduced using a reduced set of XOR gates
and shift-registers, thus representing a high-performance alternative for RC
hardware implementation in terms of processing time, circuit area, power
dissipation and system accuracy. The model (both in software and its hardware
implementation) has been tested using a pattern recognition task of handwritten
numbers (the MNIST database) for which we obtained competitive results in terms
of accuracy, speed and power dissipation. The proposed model can be considered
to be a low-cost method to implement fast pattern recognition digital circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moran_A/0/1/0/all/0/1&quot;&gt;Alejandro Mor&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frasser_C/0/1/0/all/0/1&quot;&gt;Christiam F. Frasser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossello_J/0/1/0/all/0/1&quot;&gt;Josep L. Rossell&amp;#xf3;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08083">
<title>Expanding the Active Inference Landscape: More Intrinsic Motivations in the Perception-Action Loop. (arXiv:1806.08083v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.08083</link>
<description rdf:parseType="Literal">&lt;p&gt;Active inference is an ambitious theory that treats perception, inference and
action selection of autonomous agents under the heading of a single principle.
It suggests biologically plausible explanations for many cognitive phenomena,
including consciousness. In active inference, action selection is driven by an
objective function that evaluates possible future actions with respect to
current, inferred beliefs about the world. Active inference at its core is
independent from extrinsic rewards, resulting in a high level of robustness
across e.g.\ different environments or agent morphologies. In the literature,
paradigms that share this independence have been summarised under the notion of
intrinsic motivations. In general and in contrast to active inference, these
models of motivation come without a commitment to particular inference and
action selection mechanisms. In this article, we study if the inference and
action selection machinery of active inference can also be used by alternatives
to the originally included intrinsic motivation. The perception-action loop
explicitly relates inference and action selection to the environment and agent
memory, and is consequently used as foundation for our analysis. We reconstruct
the active inference approach, locate the original formulation within, and show
how alternative intrinsic motivations can be used while keeping many of the
original features intact. Furthermore, we illustrate the connection to
universal reinforcement learning by means of our formalism. Active inference
research may profit from comparisons of the dynamics induced by alternative
intrinsic motivations. Research on intrinsic motivations may profit from an
additional way to implement intrinsically motivated agents that also share the
biological plausibility of active inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biehl_M/0/1/0/all/0/1&quot;&gt;Martin Biehl&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guckelsberger_C/0/1/0/all/0/1&quot;&gt;Christian Guckelsberger&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salge_C/0/1/0/all/0/1&quot;&gt;Christoph Salge&lt;/a&gt; (3 and 4), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1&quot;&gt;Sim&amp;#xf3;n C. Smith&lt;/a&gt; (4 and 5), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polani_D/0/1/0/all/0/1&quot;&gt;Daniel Polani&lt;/a&gt; (4) ((1) Araya Inc., Tokyo, Japan, (2) Computational Creativity Group, Department of Computing, Goldsmiths, University of London, London, UK, (3) Game Innovation Lab, Department of Computer Science and Engineering, New York University, New York City, NY, USA, (4) Sepia Lab, Adaptive Systems Research Group, Department of Computer Science, University of Hertfordshire, Hatfield, UK, (5) Institute of Perception, Action and Behaviour, School of Informatics, The University of Edinburgh, UK)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08202">
<title>Metadata Enrichment of Multi-Disciplinary Digital Library: A Semantic-based Approach. (arXiv:1806.08202v1 [cs.DL])</title>
<link>http://arxiv.org/abs/1806.08202</link>
<description rdf:parseType="Literal">&lt;p&gt;In the scientific digital libraries, some papers from different research
communities can be described by community-dependent keywords even if they share
a semantically similar topic. Articles that are not tagged with enough keyword
variations are poorly indexed in any information retrieval system which limits
potentially fruitful exchanges between scientific disciplines. In this paper,
we introduce a novel experimentally designed pipeline for multi-label
semantic-based tagging developed for open-access metadata digital libraries.
The approach starts by learning from a standard scientific categorization and a
sample of topic tagged articles to find semantically relevant articles and
enrich its metadata accordingly. Our proposed pipeline aims to enable
researchers reaching articles from various disciplines that tend to use
different terminologies. It allows retrieving semantically relevant articles
given a limited known variation of search terms. In addition to achieving an
accuracy that is higher than an expanded query based method using a topic
synonym set extracted from a semantic network, our experiments also show a
higher computational scalability versus other comparable techniques. We created
a new benchmark extracted from the open-access metadata of a scientific digital
library and published it along with the experiment code to allow further
research in the topic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Natsheh_H/0/1/0/all/0/1&quot;&gt;Hussein T. Al-Natsheh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinet_L/0/1/0/all/0/1&quot;&gt;Lucie Martinet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muhlenbach_F/0/1/0/all/0/1&quot;&gt;Fabrice Muhlenbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rico_F/0/1/0/all/0/1&quot;&gt;Fabien Rico&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zighed_D/0/1/0/all/0/1&quot;&gt;Djamel A. Zighed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08242">
<title>Combining Support Vector Machine and Elephant Herding Optimization for Cardiac Arrhythmias. (arXiv:1806.08242v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1806.08242</link>
<description rdf:parseType="Literal">&lt;p&gt;Many people are currently suffering from heart diseases that can lead to
untimely death. The most common heart abnormality is arrhythmia, which is
simply irregular beating of the heart. A prediction system for the early
intervention and prevention of heart diseases, including cardiovascular
diseases (CDVs) and arrhythmia, is important. This paper introduces the
classification of electrocardiogram (ECG) heartbeats into normal or abnormal.
The approach is based on the combination of swarm optimization algorithms with
a modified PannTompkins algorithm (MPTA) and support vector machines (SVMs).
The MPTA was implemented to remove ECG noise, followed by the application of
the extended features extraction algorithm (EFEA) for ECG feature extraction.
Then, elephant herding optimization (EHO) was used to find a subset of ECG
features from a larger feature pool that provided better classification
performance than that achieved using the whole set. Finally, SVMs were used for
classification. The results show that the EHOSVM approach achieved good
classification results in terms of five statistical indices: accuracy, 93.31%;
sensitivity, 45.49%; precision, 46.45%; F-measure, 45.48%; and specificity,
45.48%. Furthermore, the results demonstrate a clear improvement in accuracy
compared to that of other methods when applied to the MITBIH arrhythmia
database.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hassanien_A/0/1/0/all/0/1&quot;&gt;Aboul Ella Hassanien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kilany_M/0/1/0/all/0/1&quot;&gt;Moataz Kilany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Houssein_E/0/1/0/all/0/1&quot;&gt;Essam H. Houssein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08247">
<title>Log Skeletons: A Classification Approach to Process Discovery. (arXiv:1806.08247v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.08247</link>
<description rdf:parseType="Literal">&lt;p&gt;To test the effectiveness of process discovery algorithms, a Process
Discovery Contest (PDC) has been set up. This PDC uses a classification
approach to measure this effectiveness: The better the discovered model can
classify whether or not a new trace conforms to the event log, the better the
discovery algorithm is supposed to be. Unfortunately, even the state-of-the-art
fully-automated discovery algorithms score poorly on this classification. Even
the best of these algorithms, the Inductive Miner, scored only 147 correct
classified traces out of 200 traces on the PDC of 2017. This paper introduces
the rule-based log skeleton model, which is closely related to the Declare
constraint model, together with a way to classify traces using this model. This
classification using log skeletons is shown to score better on the PDC of 2017
than state-of-the-art discovery algorithms: 194 out of 200. As a result, one
can argue that the fully-automated algorithm to construct (or: discover) a log
skeleton from an event log outperforms existing state-of-the-art
fully-automated discovery algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbeek_H/0/1/0/all/0/1&quot;&gt;H.M.W. Verbeek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_R/0/1/0/all/0/1&quot;&gt;R. Medeiros de Carvalho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.03074">
<title>Structural Learning of Probabilistic Graphical Models of Cumulative Phenomena. (arXiv:1703.03074v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.03074</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the critical issues when adopting Bayesian networks (BNs) to model
dependencies among random variables is to &quot;learn&quot; their structure. This is a
well-known NP-hard problem in its most general and classical formulation, which
is furthermore complicated by known pitfalls such as the issue of I-equivalence
among different structures. In this work we restrict the investigation to a
specific class of networks, i.e., those representing the dynamics of phenomena
characterized by the monotonic accumulation of events. Such phenomena allow to
set specific structural constraints based on Suppes&apos; theory of probabilistic
causation and, accordingly, to define constrained BNs, named Suppes-Bayes
Causal Networks (SBCNs). Within this framework, we study the structure learning
of SBCNs via extensive simulations with various state-of-the-art search
strategies, such as canonical local search techniques and Genetic Algorithms.
This investigation is intended to be an extension and an in-depth clarification
of our previous works on SBCN structure learning. Among the main results, we
show that Suppes&apos; constraints do simplify the learning task, by reducing the
solution search space and providing a temporal ordering on the variables, which
simplifies the complications derived by I-equivalent structures. Finally, we
report on tradeoffs among different optimization techniques that can be used to
learn SBCNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramazzotti_D/0/1/0/all/0/1&quot;&gt;Daniele Ramazzotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nobile_M/0/1/0/all/0/1&quot;&gt;Marco S. Nobile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antoniotti_M/0/1/0/all/0/1&quot;&gt;Marco Antoniotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graudenzi_A/0/1/0/all/0/1&quot;&gt;Alex Graudenzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08092">
<title>Generalizable Data-free Objective for Crafting Universal Adversarial Perturbations. (arXiv:1801.08092v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.08092</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models are susceptible to adversarial perturbations: small
changes to input that can cause large changes in output. It is also
demonstrated that there exist input-agnostic perturbations, called universal
adversarial perturbations, which can change the inference of target model on
most of the data samples. However, existing methods to craft universal
perturbations are (i) task specific, (ii) require samples from the training
data distribution, and (iii) perform complex optimizations. Additionally,
because of the data dependence, fooling ability of the crafted perturbations is
proportional to the available training data. In this paper, we present a novel,
generalizable and data-free approaches for crafting universal adversarial
perturbations. Independent of the underlying task, our objective achieves
fooling via corrupting the extracted features at multiple layers. Therefore,
the proposed objective is generalizable to craft image-agnostic perturbations
across multiple vision tasks such as object recognition, semantic segmentation,
and depth estimation. In the practical setting of black-box attack scenario
(when the attacker does not have access to the target model and it&apos;s training
data), we show that our objective outperforms the data dependent objectives to
fool the learned models. Further, via exploiting simple priors related to the
data distribution, our objective remarkably boosts the fooling ability of the
crafted perturbations. Significant fooling rates achieved by our objective
emphasize that the current deep learning models are now at an increased risk,
since our objective generalizes across multiple tasks without the requirement
of training data for crafting the perturbations. To encourage reproducible
research, we have released the codes for our proposed algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1&quot;&gt;Konda Reddy Mopuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganeshan_A/0/1/0/all/0/1&quot;&gt;Aditya Ganeshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1&quot;&gt;R. Venkatesh Babu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09251">
<title>Multi-Pointer Co-Attention Networks for Recommendation. (arXiv:1801.09251v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1801.09251</link>
<description rdf:parseType="Literal">&lt;p&gt;Many recent state-of-the-art recommender systems such as D-ATT, TransNet and
DeepCoNN exploit reviews for representation learning. This paper proposes a new
neural architecture for recommendation with reviews. Our model operates on a
multi-hierarchical paradigm and is based on the intuition that not all reviews
are created equal, i.e., only a select few are important. The importance,
however, should be dynamically inferred depending on the current target. To
this end, we propose a review-by-review pointer-based learning scheme that
extracts important reviews, subsequently matching them in a word-by-word
fashion. This enables not only the most informative reviews to be utilized for
prediction but also a deeper word-level interaction. Our pointer-based method
operates with a novel gumbel-softmax based pointer mechanism that enables the
incorporation of discrete vectors within differentiable neural architectures.
Our pointer mechanism is co-attentive in nature, learning pointers which are
co-dependent on user-item relationships. Finally, we propose a multi-pointer
learning scheme that learns to combine multiple views of interactions between
user and item. Overall, we demonstrate the effectiveness of our proposed model
via extensive experiments on \textbf{24} benchmark datasets from Amazon and
Yelp. Empirical results show that our approach significantly outperforms
existing state-of-the-art, with up to 19% and 71% relative improvement when
compared to TransNet and DeepCoNN respectively. We study the behavior of our
multi-pointer learning mechanism, shedding light on evidence aggregation
patterns in review-based recommender systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1&quot;&gt;Yi Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1&quot;&gt;Luu Anh Tuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1&quot;&gt;Siu Cheung Hui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.01933">
<title>A Survey Of Methods For Explaining Black Box Models. (arXiv:1802.01933v3 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1802.01933</link>
<description rdf:parseType="Literal">&lt;p&gt;In the last years many accurate decision support systems have been
constructed as black boxes, that is as systems that hide their internal logic
to the user. This lack of explanation constitutes both a practical and an
ethical issue. The literature reports many approaches aimed at overcoming this
crucial weakness sometimes at the cost of scarifying accuracy for
interpretability. The applications in which black box decision systems can be
used are various, and each approach is typically developed to provide a
solution for a specific problem and, as a consequence, delineating explicitly
or implicitly its own definition of interpretability and explanation. The aim
of this paper is to provide a classification of the main problems addressed in
the literature with respect to the notion of explanation and the type of black
box system. Given a problem definition, a black box type, and a desired
explanation this survey should help the researcher to find the proposals more
useful for his own work. The proposed classification of approaches to open
black box models should also be useful for putting the many research open
questions in perspective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guidotti_R/0/1/0/all/0/1&quot;&gt;Riccardo Guidotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monreale_A/0/1/0/all/0/1&quot;&gt;Anna Monreale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruggieri_S/0/1/0/all/0/1&quot;&gt;Salvatore Ruggieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turini_F/0/1/0/all/0/1&quot;&gt;Franco Turini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedreschi_D/0/1/0/all/0/1&quot;&gt;Dino Pedreschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannotti_F/0/1/0/all/0/1&quot;&gt;Fosca Giannotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01907">
<title>Exploration by Distributional Reinforcement Learning. (arXiv:1805.01907v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.01907</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a framework based on distributional reinforcement learning and
recent attempts to combine Bayesian parameter updates with deep reinforcement
learning. We show that our proposed framework conceptually unifies multiple
previous methods in exploration. We also derive a practical algorithm that
achieves efficient exploration on challenging control tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yunhao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1&quot;&gt;Shipra Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09676">
<title>Forming IDEAS Interactive Data Exploration &amp; Analysis System. (arXiv:1805.09676v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09676</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern cyber security operations collect an enormous amount of logging and
alerting data. While analysts have the ability to query and compute simple
statistics and plots from their data, current analytical tools are too simple
to admit deep understanding. To detect advanced and novel attacks, analysts
turn to manual investigations. While commonplace, current investigations are
time-consuming, intuition-based, and proving insufficient. Our hypothesis is
that arming the analyst with easy-to-use data science tools will increase their
work efficiency, provide them with the ability to resolve hypotheses with
scientific inquiry of their data, and support their decisions with evidence
over intuition. To this end, we present our work to build IDEAS (Interactive
Data Exploration and Analysis System). We present three real-world use-cases
that drive the system design from the algorithmic capabilities to the user
interface. Finally, a modular and scalable software architecture is discussed
along with plans for our pilot deployment with a security operation command.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bridges_R/0/1/0/all/0/1&quot;&gt;Robert A. Bridges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_M/0/1/0/all/0/1&quot;&gt;Maria A. Vincent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huffer_K/0/1/0/all/0/1&quot;&gt;Kelly M. T. Huffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodall_J/0/1/0/all/0/1&quot;&gt;John R. Goodall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamieson_J/0/1/0/all/0/1&quot;&gt;Jessie D. Jamieson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burch_Z/0/1/0/all/0/1&quot;&gt;Zachary Burch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05484">
<title>Nearly Zero-Shot Learning for Semantic Decoding in Spoken Dialogue Systems. (arXiv:1806.05484v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05484</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents two ways of dealing with scarce data in semantic decoding
using N-Best speech recognition hypotheses. First, we learn features by using a
deep learning architecture in which the weights for the unknown and known
categories are jointly optimised. Second, an unsupervised method is used for
further tuning the weights. Sharing weights injects prior knowledge to unknown
categories. The unsupervised tuning (i.e. the risk minimisation) improves the
F-Measure when recognising nearly zero-shot data on the DSTC3 corpus. This
unsupervised method can be applied subject to two assumptions: the rank of the
class marginal is assumed to be known and the class-conditional scores of the
classifier are assumed to follow a Gaussian distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojas_Barahona_L/0/1/0/all/0/1&quot;&gt;Lina M.Rojas-Barahona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1&quot;&gt;Stefan Ultes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budzianowski_P/0/1/0/all/0/1&quot;&gt;Pawel Budzianowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casanueva_I/0/1/0/all/0/1&quot;&gt;I&amp;#xf1;igo Casanueva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1&quot;&gt;Milica Gasic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1&quot;&gt;Bo-Hsiang Tseng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_S/0/1/0/all/0/1&quot;&gt;Steve Young&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07172">
<title>Surrogate Outcomes and Transportability. (arXiv:1806.07172v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.07172</link>
<description rdf:parseType="Literal">&lt;p&gt;Identification of causal effects is one of the most fundamental tasks of
causal inference. We consider a variant of the identifiability problem where a
causal effect of interest is not identifiable from observational data alone but
some experimental data is available for the identification task. This
corresponds to a real-world setting where experiments were conducted on a set
of variables, which we call surrogate outcomes, but the variables of interest
were not measured. This problem is a generalization of identifiability using
surrogate experiments and we label it as surrogate outcome identifiability and
show that the concept of transportability provides a sufficient criteria for
determining surrogate outcome identifiability for a large class of queries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tikka_S/0/1/0/all/0/1&quot;&gt;Santtu Tikka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karvanen_J/0/1/0/all/0/1&quot;&gt;Juha Karvanen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07944">
<title>Searching for a Single Community in a Graph. (arXiv:1806.07944v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1806.07944</link>
<description rdf:parseType="Literal">&lt;p&gt;In standard graph clustering/community detection, one is interested in
partitioning the graph into more densely connected subsets of nodes. In
contrast, the &quot;search&quot; problem of this paper aims to only find the nodes in a
&quot;single&quot; such community, the target, out of the many communities that may
exist. To do so , we are given suitable side information about the target; for
example, a very small number of nodes from the target are labeled as such.
&lt;/p&gt;
&lt;p&gt;We consider a general yet simple notion of side information: all nodes are
assumed to have random weights, with nodes in the target having higher weights
on average. Given these weights and the graph, we develop a variant of the
method of moments that identifies nodes in the target more reliably, and with
lower computation, than generic community detection methods that do not use
side information and partition the entire graph. Our empirical results show
significant gains in runtime, and also gains in accuracy over other graph
clustering algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Avik Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1&quot;&gt;Sujay Sanghavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1&quot;&gt;Sanjay Shakkottai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07956">
<title>Reconstructing networks with unknown and heterogeneous errors. (arXiv:1806.07956v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1806.07956</link>
<description rdf:parseType="Literal">&lt;p&gt;The vast majority of network datasets contains errors and omissions, although
this is rarely incorporated in traditional network analysis. Recently, an
increasing effort has been made to fill this methodological gap by developing
network reconstruction approaches based on Bayesian inference. These
approaches, however, rely on assumptions of uniform error rates and on direct
estimations of the existence of each edge via repeated measurements, something
that is currently unavailable for the majority of network data. Here we develop
a Bayesian reconstruction approach that lifts these limitations by not only
allowing for heterogeneous errors, but also for individual edge measurements
without direct error estimates. Our approach works by coupling the inference
approach with structured generative network models, which enable the
correlations between edges to be used as reliable error estimates. Although our
approach is general, we focus on the stochastic block model as the basic
generative process, from which efficient nonparametric inference can be
performed, and yields a principled method to infer hierarchical community
structure from noisy data. We demonstrate the efficacy of our approach with a
variety of empirical and artificial networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peixoto_T/0/1/0/all/0/1&quot;&gt;Tiago P. Peixoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08010">
<title>Fairness Without Demographics in Repeated Loss Minimization. (arXiv:1806.08010v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.08010</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models (e.g., speech recognizers) are usually trained to
minimize average loss, which results in representation disparity---minority
groups (e.g., non-native speakers) contribute less to the training objective
and thus tend to suffer higher loss. Worse, as model accuracy affects user
retention, a minority group can shrink over time. In this paper, we first show
that the status quo of empirical risk minimization (ERM) amplifies
representation disparity over time, which can even make initially fair models
unfair. To mitigate this, we develop an approach based on distributionally
robust optimization (DRO), which minimizes the worst case risk over all
distributions close to the empirical distribution. We prove that this approach
controls the risk of the minority group at each time step, in the spirit of
Rawlsian distributive justice, while remaining oblivious to the identity of the
groups. We demonstrate that DRO prevents disparity amplification on examples
where ERM fails, and show improvements in minority group user satisfaction in a
real-world text autocomplete task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori B. Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srivastava_M/0/1/0/all/0/1&quot;&gt;Megha Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Namkoong_H/0/1/0/all/0/1&quot;&gt;Hongseok Namkoong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08049">
<title>On the Robustness of Interpretability Methods. (arXiv:1806.08049v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.08049</link>
<description rdf:parseType="Literal">&lt;p&gt;We argue that robustness of explanations---i.e., that similar inputs should
give rise to similar explanations---is a key desideratum for interpretability.
We introduce metrics to quantify robustness and demonstrate that current
methods do not perform well according to these metrics. Finally, we propose
ways that robustness can be enforced on existing interpretability approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_Melis_D/0/1/0/all/0/1&quot;&gt;David Alvarez-Melis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1&quot;&gt;Tommi S. Jaakkola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08117">
<title>A data-driven model order reduction approach for Stokes flow through random porous media. (arXiv:1806.08117v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.08117</link>
<description rdf:parseType="Literal">&lt;p&gt;Direct numerical simulation of Stokes flow through an impermeable, rigid body
matrix by finite elements requires meshes fine enough to resolve the pore-size
scale and is thus a computationally expensive task. The cost is significantly
amplified when randomness in the pore microstructure is present and therefore
multiple simulations need to be carried out. It is well known that in the limit
of scale-separation, Stokes flow can be accurately approximated by Darcy&apos;s law
with an effective diffusivity field depending on viscosity and the pore-matrix
topology. We propose a fully probabilistic, Darcy-type, reduced-order model
which, based on only a few tens of full-order Stokes model runs, is capable of
learning a map from the fine-scale topology to the effective diffusivity and is
maximally predictive of the fine-scale response. The reduced-order model
learned can significantly accelerate uncertainty quantification tasks as well
as provide quantitative confidence metrics of the predictive estimates
produced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grigo_C/0/1/0/all/0/1&quot;&gt;Constantin Grigo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Koutsourelakis_P/0/1/0/all/0/1&quot;&gt;Phaedon-Stelios Koutsourelakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08141">
<title>Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions. (arXiv:1806.08141v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.08141</link>
<description rdf:parseType="Literal">&lt;p&gt;By building up on the recent theory that established the connection between
implicit generative modeling and optimal transport, in this study, we propose a
novel parameter-free algorithm for learning the underlying distributions of
complicated datasets and sampling from them. The proposed algorithm is based on
a functional optimization problem, which aims at finding a measure that is
close to the data distribution as much as possible and also expressive enough
for generative modeling purposes. We formulate the problem as a gradient flow
in the space of probability measures. The connections between gradient flows
and stochastic differential equations let us develop a computationally
efficient algorithm for solving the optimization problem, where the resulting
algorithm resembles the recent dynamics-based Markov Chain Monte Carlo
algorithms. We provide formal theoretical analysis where we prove finite-time
error guarantees for the proposed algorithm. Our experimental results support
our theory and shows that our algorithm is able to capture the structure of
challenging distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1&quot;&gt;Umut &amp;#x15e;im&amp;#x15f;ekli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liutkus_A/0/1/0/all/0/1&quot;&gt;Antoine Liutkus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Majewski_S/0/1/0/all/0/1&quot;&gt;Szymon Majewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1&quot;&gt;Alain Durmus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08151">
<title>Robust and Efficient Boosting Method using the Conditional Risk. (arXiv:1806.08151v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.08151</link>
<description rdf:parseType="Literal">&lt;p&gt;Well-known for its simplicity and effectiveness in classification, AdaBoost,
however, suffers from overfitting when class-conditional distributions have
significant overlap. Moreover, it is very sensitive to noise that appears in
the labels. This article tackles the above limitations simultaneously via
optimizing a modified loss function (i.e., the conditional risk). The proposed
approach has the following two advantages. (1) It is able to directly take into
account label uncertainty with an associated label confidence. (2) It
introduces a &quot;trustworthiness&quot; measure on training samples via the Bayesian
risk rule, and hence the resulting classifier tends to have finite sample
performance that is superior to that of the original AdaBoost when there is a
large overlap between class conditional distributions. Theoretical properties
of the proposed method are investigated. Extensive experimental results using
synthetic data and real-world data sets from UCI machine learning repository
are provided. The empirical study shows the high competitiveness of the
proposed method in predication accuracy and robustness when compared with the
original AdaBoost and several existing robust AdaBoost algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zhi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhe Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhong_B/0/1/0/all/0/1&quot;&gt;Bo Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dang_X/0/1/0/all/0/1&quot;&gt;Xin Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08195">
<title>Probabilistic PARAFAC2. (arXiv:1806.08195v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.08195</link>
<description rdf:parseType="Literal">&lt;p&gt;The PARAFAC2 is a multimodal factor analysis model suitable for analyzing
multi-way data when one of the modes has incomparable observation units, for
example because of differences in signal sampling or batch sizes. A fully
probabilistic treatment of the PARAFAC2 is desirable in order to improve
robustness to noise and provide a well founded principle for determining the
number of factors, but challenging because the factor loadings are constrained
to be orthogonal. We develop two probabilistic formulations of the PARAFAC2
along with variational procedures for inference: In the one approach, the mean
values of the factor loadings are orthogonal leading to closed form variational
updates, and in the other, the factor loadings themselves are orthogonal using
a matrix Von Mises-Fisher distribution. We contrast our probabilistic
formulation to the conventional direct fitting algorithm based on maximum
likelihood. On simulated data and real fluorescence spectroscopy and gas
chromatography-mass spectrometry data, we compare our approach to the
conventional PARAFAC2 model estimation and find that the probabilistic
formulation is more robust to noise and model order misspecification. The
probabilistic PARAFAC2 thus forms a promising framework for modeling multi-way
data accounting for uncertainty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jorgensen_P/0/1/0/all/0/1&quot;&gt;Philip J. H. J&amp;#xf8;rgensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nielsen_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf8;ren F. V. Nielsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hinrich_J/0/1/0/all/0/1&quot;&gt;Jesper L. Hinrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1&quot;&gt;Mikkel N. Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Madsen_K/0/1/0/all/0/1&quot;&gt;Kristoffer H. Madsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Morup_M/0/1/0/all/0/1&quot;&gt;Morten M&amp;#xf8;rup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08235">
<title>Semi-supervised Seizure Prediction with Generative Adversarial Networks. (arXiv:1806.08235v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.08235</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we propose an approach that can make use of not only labeled
EEG signals but also the unlabeled ones which is more accessible. We also
suggest the use of data fusion to further improve the seizure prediction
accuracy. Data fusion in our vision includes EEG signals, cardiogram signals,
body temperature and time. We use the short-time Fourier transform on 28-s EEG
windows as a pre-processing step. A generative adversarial network (GAN) is
trained in an unsupervised manner where information of seizure onset is
disregarded. The trained Discriminator of the GAN is then used as feature
extractor. Features generated by the feature extractor are classified by two
fully-connected layers (can be replaced by any classifier) for the labeled EEG
signals. This semi-supervised seizure prediction method achieves area under the
operating characteristic curve (AUC) of 77.68% and 75.47% for the CHBMIT scalp
EEG dataset and the Freiburg Hospital intracranial EEG dataset, respectively.
Unsupervised training without the need of labeling is important because not
only it can be performed in real-time during EEG signal recording, but also it
does not require feature engineering effort for each patient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_N/0/1/0/all/0/1&quot;&gt;Nhan Duy Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhlmann_L/0/1/0/all/0/1&quot;&gt;Levin Kuhlmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonyadi_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Bonyadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kavehei_O/0/1/0/all/0/1&quot;&gt;Omid Kavehei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08295">
<title>How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments. (arXiv:1806.08295v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.08295</link>
<description rdf:parseType="Literal">&lt;p&gt;Consistently checking the statistical significance of experimental results is
one of the mandatory methodological steps to address the so-called
&quot;reproducibility crisis&quot; in deep reinforcement learning. In this tutorial
paper, we explain how to determine the number of random seeds one should use to
provide a statistically significant comparison of the performance of two
algorithms. We also discuss the influence of deviations from the assumptions
usually made by statistical tests, we provide guidelines to counter their
negative effects and some code to perform the tests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Colas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1&quot;&gt;Olivier Sigaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08301">
<title>Online Saddle Point Problem with Applications to Constrained Online Convex Optimization. (arXiv:1806.08301v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.08301</link>
<description rdf:parseType="Literal">&lt;p&gt;We study an online saddle point problem where at each iteration a pair of
actions need to be chosen without knowledge of the future (convex-concave)
payoff functions. The objective is to minimize the gap between the cumulative
payoffs and the saddle point value of the aggregate payoff function, which we
measure using a metric called &quot;SP-regret&quot;. The problem generalizes the online
convex optimization framework and can be interpreted as finding the Nash
equilibrium for the aggregate of a sequence of two-player zero-sum games. We
propose an algorithm that achieves $\tilde{O}(\sqrt{T})$ SP-regret in the
general case, and $O(\log T)$ SP-regret for the strongly convex-concave case.
We then consider a constrained online convex optimization problem motivated by
a variety of applications in dynamic pricing, auctions, and crowdsourcing. We
relate this problem to an online saddle point problem and establish
$O(\sqrt{T})$ regret using a primal-dual algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rivera_A/0/1/0/all/0/1&quot;&gt;Adrian Rivera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;He Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08317">
<title>Fashion-Gen: The Generative Fashion Dataset and Challenge. (arXiv:1806.08317v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.08317</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new dataset of 293,008 high definition (1360 x 1360 pixels)
fashion images paired with item descriptions provided by professional stylists.
Each item is photographed from a variety of angles. We provide baseline results
on 1) high-resolution image generation, and 2) image generation conditioned on
the given text descriptions. We invite the community to improve upon these
baselines. In this paper, we also outline the details of a challenge that we
are launching based upon this dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rostamzadeh_N/0/1/0/all/0/1&quot;&gt;Negar Rostamzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hosseini_S/0/1/0/all/0/1&quot;&gt;Seyedarian Hosseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boquet_T/0/1/0/all/0/1&quot;&gt;Thomas Boquet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stokowiec_W/0/1/0/all/0/1&quot;&gt;Wojciech Stokowiec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jauvin_C/0/1/0/all/0/1&quot;&gt;Christian Jauvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pal_C/0/1/0/all/0/1&quot;&gt;Chris Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08324">
<title>Countdown Regression: Sharp and Calibrated Survival Predictions. (arXiv:1806.08324v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.08324</link>
<description rdf:parseType="Literal">&lt;p&gt;Personalized probabilistic forecasts of time to event (such as mortality) can
be crucial in decision making, especially in the clinical setting. Inspired by
ideas from the meteorology literature, we approach this problem through the
paradigm of maximizing sharpness of prediction distributions, subject to
calibration. In regression problems, it has been shown that optimizing the
continuous ranked probability score (CRPS) instead of maximum likelihood leads
to sharper prediction distributions while maintaining calibration. We introduce
the Survival-CRPS, a generalization of the CRPS to the time to event setting,
and present right-censored and interval-censored variants. To holistically
evaluate the quality of predicted distributions over time to event, we present
the Survival-AUPRC evaluation metric, an analog to area under the
precision-recall curve. We apply these ideas by building a recurrent neural
network for mortality prediction, using an Electronic Health Record dataset
covering millions of patients. We demonstrate significant benefits in models
trained by the Survival-CRPS objective instead of maximum likelihood.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avati_A/0/1/0/all/0/1&quot;&gt;Anand Avati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_T/0/1/0/all/0/1&quot;&gt;Tony Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1&quot;&gt;Kenneth Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nigam H. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1&quot;&gt;Andrew Ng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08342">
<title>Quantizing deep convolutional networks for efficient inference: A whitepaper. (arXiv:1806.08342v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.08342</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an overview of techniques for quantizing convolutional neural
networks for inference with integer weights and activations. Per-channel
quantization of weights and per-layer quantization of activations to 8-bits of
precision post-training produces classification accuracies within 2% of
floating point networks for a wide variety of CNN architectures. Model sizes
can be reduced by a factor of 4 by quantizing weights to 8-bits, even when
8-bit arithmetic is not supported. This can be achieved with simple, post
training quantization of weights.We benchmark latencies of quantized networks
on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations
compared to floating point on CPUs. Speedups of up to 10x are observed on
specialized processors with fixed point SIMD capabilities, like the Qualcomm
QDSPs with HVX.
&lt;/p&gt;
&lt;p&gt;Quantization-aware training can provide further improvements, reducing the
gap to floating point to 1% at 8-bit precision. Quantization-aware training
also allows for reducing the precision of weights to four bits with accuracy
losses ranging from 2% to 10%, with higher accuracy drop for smaller
networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing
convolutional networks and review best practices for quantization-aware
training to obtain high accuracy with quantized weights and activations. We
recommend that per-channel quantization of weights and per-layer quantization
of activations be the preferred quantization scheme for hardware acceleration
and kernel optimization. We also propose that future processors and hardware
accelerators for optimized inference support precisions of 4, 8 and 16 bits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1&quot;&gt;Raghuraman Krishnamoorthi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.08618">
<title>A Benchmark and Comparison of Active Learning for Logistic Regression. (arXiv:1611.08618v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.08618</link>
<description rdf:parseType="Literal">&lt;p&gt;Logistic regression is by far the most widely used classifier in real-world
applications. In this paper, we benchmark the state-of-the-art active learning
methods for logistic regression and discuss and illustrate their underlying
characteristics. Experiments are carried out on three synthetic datasets and 44
real-world datasets, providing insight into the behaviors of these active
learning methods with respect to the area of the learning curve (which plots
classification accuracy as a function of the number of queried examples) and
their computational costs. Surprisingly, one of the earliest and simplest
suggested active learning methods, i.e., uncertainty sampling, performs
exceptionally well overall. Another remarkable finding is that random sampling,
which is the rudimentary baseline to improve upon, is not overwhelmed by
individual active learning techniques in many cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yazhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loog_M/0/1/0/all/0/1&quot;&gt;Marco Loog&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.02111">
<title>Classification and clustering for observations of event time data using non-homogeneous Poisson process models. (arXiv:1703.02111v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.02111</link>
<description rdf:parseType="Literal">&lt;p&gt;Data of the form of event times arise in various applications. A simple model
for such data is a non-homogeneous Poisson process (NHPP) which is specified by
a rate function that depends on time. We consider the problem of having access
to multiple independent observations of event time data, observed on a common
interval, from which we wish to classify or cluster the observations according
to their rate functions. Each rate function is unknown but assumed to belong to
a finite number of rate functions each defining a distinct class. We model the
rate functions using a spline basis expansion, the coefficients of which need
to be estimated from data. The classification approach consists of using
training data for which the class membership is known, to calculate maximum
likelihood estimates of the coefficients for each group, then assigning test
observations to a group by a maximum likelihood criterion. For clustering, by
analogy to the Gaussian mixture model approach for Euclidean data, we consider
mixtures of NHPP and use the expectation-maximisation algorithm to estimate the
coefficients of the rate functions for the component models and group
membership probabilities for each observation. The classification and
clustering approaches perform well on both synthetic and real-world data sets.
Code associated with this paper is available at
https://github.com/duncan-barrack/NHPP .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrack_D/0/1/0/all/0/1&quot;&gt;Duncan Barrack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preston_S/0/1/0/all/0/1&quot;&gt;Simon Preston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.04546">
<title>Reinforcement Learning with Budget-Constrained Nonparametric Function Approximation for Opportunistic Spectrum Access. (arXiv:1706.04546v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1706.04546</link>
<description rdf:parseType="Literal">&lt;p&gt;Opportunistic spectrum access is one of the emerging techniques for
maximizing throughput in congested bands and is enabled by predicting idle
slots in spectrum. We propose a kernel-based reinforcement learning approach
coupled with a novel budget-constrained sparsification technique that
efficiently captures the environment to find the best channel access actions.
This approach allows learning and planning over the intrinsic state-action
space and extends well to large state spaces. We apply our methods to evaluate
coexistence of a reinforcement learning-based radio with a multi-channel
adversarial radio and a single-channel CSMA-CA radio. Numerical experiments
show the performance gains over carrier-sense systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1&quot;&gt;Theodoros Tsiligkaridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1&quot;&gt;David Romero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.02883">
<title>Maximum Volume Inscribed Ellipsoid: A New Simplex-Structured Matrix Factorization Framework via Facet Enumeration and Convex Optimization. (arXiv:1708.02883v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.02883</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider a structured matrix factorization model where one factor is
restricted to have its columns lying in the unit simplex. This
simplex-structured matrix factorization (SSMF) model and the associated
factorization techniques have spurred much interest in research topics over
different areas, such as hyperspectral unmixing in remote sensing, topic
discovery in machine learning, to name a few. In this paper we develop a new
theoretical SSMF framework whose idea is to study a maximum volume ellipsoid
inscribed in the convex hull of the data points. This maximum volume inscribed
ellipsoid (MVIE) idea has not been attempted in prior literature, and we show a
sufficient condition under which the MVIE framework guarantees exact recovery
of the factors. The sufficient recovery condition we show for MVIE is much more
relaxed than that of separable non-negative matrix factorization (or pure-pixel
search); coincidentally it is also identical to that of minimum volume
enclosing simplex, which is known to be a powerful SSMF framework for
non-separable problem instances. We also show that MVIE can be practically
implemented by performing facet enumeration and then by solving a convex
optimization problem. The potential of the MVIE framework is illustrated by
numerical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chia-Hsiang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruiyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wing-Kin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chi_C/0/1/0/all/0/1&quot;&gt;Chong-Yung Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06373">
<title>Thoracic Disease Identification and Localization with Limited Supervision. (arXiv:1711.06373v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06373</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate identification and localization of abnormalities from radiology
images play an integral part in clinical diagnosis and treatment planning.
Building a highly accurate prediction model for these tasks usually requires a
large number of images manually annotated with labels and finding sites of
abnormalities. In reality, however, such annotated data are expensive to
acquire, especially the ones with location annotations. We need methods that
can work well with only a small amount of location annotations. To address this
challenge, we present a unified approach that simultaneously performs disease
identification and localization through the same underlying model for all
images. We demonstrate that our approach can effectively leverage both class
information as well as limited location annotation, and significantly
outperforms the comparative reference baseline in both classification and
localization tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1&quot;&gt;Mei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yuan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li-Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06695">
<title>Accurate Inference for Adaptive Linear Models. (arXiv:1712.06695v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06695</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimators computed from adaptively collected data do not behave like their
non-adaptive brethren. Rather, the sequential dependence of the collection
policy can lead to severe distributional biases that persist even in the
infinite data limit. We develop a general method -- $\mathbf{W}$-decorrelation
-- for transforming the bias of adaptive linear regression estimators into
variance. The method uses only coarse-grained information about the data
collection policy and does not need access to propensity scores or exact
knowledge of the policy. We bound the finite-sample bias and variance of the
$\mathbf{W}$-estimator and develop asymptotically correct confidence intervals
based on a novel martingale central limit theorem. We then demonstrate the
empirical benefits of the generic $\mathbf{W}$-decorrelation procedure in two
different adaptive data settings: the multi-armed bandit and the autoregressive
time series.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deshpande_Y/0/1/0/all/0/1&quot;&gt;Yash Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Taddy_M/0/1/0/all/0/1&quot;&gt;Matt Taddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05112">
<title>Uplift Modeling from Separate Labels. (arXiv:1803.05112v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05112</link>
<description rdf:parseType="Literal">&lt;p&gt;Uplift modeling is aimed at estimating the incremental impact of an action on
an individual&apos;s behavior, which is useful in various application domains such
as targeted marketing (advertisement campaigns) and personalized medicine
(medical treatments). Conventional methods of uplift modeling require every
instance to be jointly equipped with two types of labels: the taken action and
its outcome. However, obtaining two labels for each instance at the same time
is difficult or expensive in many real-world problems. In this paper, we
propose a novel method of uplift modeling that is applicable to a more
practical setting where only one type of labels is available for each instance.
We show a generalization error bound for the proposed method and demonstrate
its effectiveness through experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yamane_I/0/1/0/all/0/1&quot;&gt;Ikko Yamane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yger_F/0/1/0/all/0/1&quot;&gt;Florian Yger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Atif_J/0/1/0/all/0/1&quot;&gt;Jamal Atif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01811">
<title>AdaGrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization. (arXiv:1806.01811v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01811</link>
<description rdf:parseType="Literal">&lt;p&gt;Adaptive gradient methods such as AdaGrad and its variants update the
stepsize in stochastic gradient descent on the fly according to the gradients
received along the way; such methods have gained widespread use in large-scale
optimization for their ability to converge robustly, without the need to fine
tune parameters such as the stepsize schedule. Yet, the theoretical guarantees
to date for AdaGrad are for online and convex optimization, which is quite
different from the offline and nonconvex setting where adaptive gradient
methods shine in practice. We bridge this gap by providing strong theoretical
guarantees in batch and stochastic setting, for the convergence of AdaGrad over
smooth, nonconvex landscapes, from any initialization of the stepsize, without
knowledge of Lipschitz constant of the gradient. We show in the stochastic
setting that AdaGrad converges to a stationary point at the optimal
$O(1/\sqrt{N})$ rate (up to a $\log(N)$ factor), and in the batch setting, at
the optimal $O(1/N)$ rate. Moreover, in both settings, the constant in the rate
matches the constant obtained as if the variance of the gradient noise and
Lipschitz constant of the gradient were known in advance and used to tune the
stepsize, up to a logarithmic factor of the mismatch between the optimal
stepsize and the stepsize used to initialize AdaGrad. In particular, our
results imply that AdaGrad is robust to both the unknown Lipschitz constant and
level of stochastic noise on the gradient, in a near-optimal sense. When there
is noise, AdaGrad converges at the rate of $O(1/\sqrt{N})$ with well-tuned
stepsize, and when there is not noise, the same algorithm converges at the rate
of $O(1/N)$ like well-tuned batch gradient descent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ward_R/0/1/0/all/0/1&quot;&gt;Rachel Ward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoxia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bottou_L/0/1/0/all/0/1&quot;&gt;Leon Bottou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02199">
<title>Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series. (arXiv:1806.02199v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02199</link>
<description rdf:parseType="Literal">&lt;p&gt;Human professionals are often required to make decisions based on complex
multivariate time series measurements in an online setting, e.g. in health
care. Since human cognition is not optimized to work well in high-dimensional
spaces, these decisions benefit from interpretable low-dimensional
representations. However, many representation learning algorithms for time
series data are difficult to interpret. This is due to non-intuitive mappings
from data features to salient properties of the representation and
non-smoothness over time. To address this problem, we propose to couple a
variational autoencoder to a discrete latent space and introduce a topological
structure through the use of self-organizing maps. This allows us to learn
discrete representations of time series, which give rise to smooth and
interpretable embeddings with superior clustering performance. Furthermore, to
allow for a probabilistic interpretation of our method, we integrate a Markov
model in the latent space. This model uncovers the temporal transition
structure, improves clustering performance even further and provides additional
explanatory insights as well as a natural representation of uncertainty. We
evaluate our model on static (Fashion-)MNIST data, a time series of linearly
interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two
macro states, as well as on a challenging real world medical time series
application. In the latter experiment, our representation uncovers meaningful
structure in the acute physiological state of a patient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1&quot;&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huser_M/0/1/0/all/0/1&quot;&gt;Matthias H&amp;#xfc;ser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strathmann_H/0/1/0/all/0/1&quot;&gt;Heiko Strathmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratsch_G/0/1/0/all/0/1&quot;&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06784">
<title>Flexible Collaborative Estimation of the Average Causal Effect of a Treatment using the Outcome-Highly-Adaptive Lasso. (arXiv:1806.06784v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1806.06784</link>
<description rdf:parseType="Literal">&lt;p&gt;Many estimators of the average causal effect of an intervention require
estimation of the propensity score, the outcome regression, or both. For these
estimators, we must carefully con- sider how to estimate the relevant
regressions. It is often beneficial to utilize flexible techniques such as
semiparametric regression or machine learning. However, optimal estimation of
the regression function does not necessarily lead to optimal estimation of the
average causal effect. Therefore, it is important to consider criteria for
evaluating regression estimators and selecting hyper-parameters. A recent
proposal addressed these issues via the outcome-adaptive lasso, a penalized
regression technique for estimating the propensity score. We build on this
proposal and offer a method that is simultaneously more flexible and more
efficient than the previous pro- posal. We propose the outcome-highly-adaptive
LASSO, a semi-parametric regression estimator designed to down-weight regions
of the confounder space that do not contribute variation to the outcome
regression. We show that tuning this method using collaborative targeted
learning leads to superior finite-sample performance relative to competing
estimators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ju_C/0/1/0/all/0/1&quot;&gt;Cheng Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Benkeser_D/0/1/0/all/0/1&quot;&gt;David Benkeser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laan_M/0/1/0/all/0/1&quot;&gt;Mark J. van der Laan&lt;/a&gt;</dc:creator>
</item></rdf:RDF>