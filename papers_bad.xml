<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-07T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02348"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02349"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02476"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02544"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02623"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02808"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.01733"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01275"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04127"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06382"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.08100"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02423"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02432"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02525"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02527"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02551"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02553"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02596"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02598"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02781"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.08306"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.00340"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.01166"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.04410"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.04045"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06273"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.08841"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02317"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06104"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04153"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08570"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1510.00967"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.02348">
<title>Smoothed Action Value Functions for Learning Gaussian Policies. (arXiv:1803.02348v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.02348</link>
<description rdf:parseType="Literal">&lt;p&gt;State-action value functions (i.e., Q-values) are ubiquitous in reinforcement
learning (RL), giving rise to popular algorithms such as SARSA and Q-learning.
We propose a new notion of action value defined by a Gaussian smoothed version
of the expected Q-value. We show that such smoothed Q-values still satisfy a
Bellman equation, making them learnable from experience sampled from an
environment. Moreover, the gradients of expected reward with respect to the
mean and covariance of a parameterized Gaussian policy can be recovered from
the gradient and Hessian of the smoothed Q-value function. Based on these
relationships, we develop new algorithms for training a Gaussian policy
directly from a learned smoothed Q-value approximator. The approach is
additionally amenable to proximal optimization by augmenting the objective with
a penalty on KL-divergence from a previous policy. We find that the ability to
learn both a mean and covariance during training leads to significantly
improved results on standard continuous control benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1&quot;&gt;Ofir Nachum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1&quot;&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucker_G/0/1/0/all/0/1&quot;&gt;George Tucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1&quot;&gt;Dale Schuurmans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02349">
<title>Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba. (arXiv:1803.02349v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1803.02349</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems (RSs) have been the most important technology for
increasing the business in Taobao, the largest online consumer-to-consumer
(C2C) platform in China. The billion-scale data in Taobao creates three major
challenges to Taobao&apos;s RS: scalability, sparsity and cold start. In this paper,
we present our technical solutions to address these three challenges. The
methods are based on the graph embedding framework. We first construct an item
graph from users&apos; behavior history. Each item is then represented as a vector
using graph embedding. The item embeddings are employed to compute pairwise
similarities between all items, which are then used in the recommendation
process. To alleviate the sparsity and cold start problems, side information is
incorporated into the embedding framework. We propose two aggregation methods
to integrate the embeddings of items and the corresponding side information.
Experimental results from offline experiments show that methods incorporating
side information are superior to those that do not. Further, we describe the
platform upon which the embedding methods are deployed and the workflow to
process the billion-scale data in Taobao. Using online A/B test, we show that
the online Click-Through-Rate (CTRs) are improved comparing to the previous
recommendation methods widely used in Taobao, further demonstrating the
effectiveness and feasibility of our proposed methods in Taobao&apos;s live
production environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jizhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Pipei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Huan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhibo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Binqiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dik Lun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02476">
<title>Decision-making processes in the Cognitive Theory of True Conditions. (arXiv:1803.02476v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.02476</link>
<description rdf:parseType="Literal">&lt;p&gt;The Cognitive Theory of True Conditions (CTTC) is a proposal to design the
implementation of cognitive abilities and to describe the model-theoretic
semantics of symbolic cognitive architectures. The CTTC is formulated
mathematically using the multi-optional many-sorted past present future(MMPPF)
structures. This article discussed how decision-making processes are described
in the CTTC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miguel_Tome_S/0/1/0/all/0/1&quot;&gt;Sergio Miguel-Tom&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02544">
<title>Visual Explanations From Deep 3D Convolutional Neural Networks for Alzheimer&apos;s Disease Classification. (arXiv:1803.02544v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.02544</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop three efficient approaches for generating visual explanations from
3D convolutional neural networks (3D-CNNs) for Alzheimer&apos;s disease
classification. One approach conducts sensitivity analysis on hierarchical 3D
image segmentation, and the other two visualize network activations on a
spatial map. Visual checks and a quantitative localization benchmark indicate
that all approaches identify important brain parts for Alzheimer&apos;s disease
diagnosis. Comparative analysis show that the sensitivity analysis based
approach has difficulty handling loosely distributed cerebral cortex, and
approaches based on visualization of activations are constrained by the
resolution of the convolutional layer. The complementarity of these methods
improves the understanding of 3D-CNNs in Alzheimer&apos;s disease classification
from different perspectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chengliang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1&quot;&gt;Anand Rangarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranka_S/0/1/0/all/0/1&quot;&gt;Sanjay Ranka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02623">
<title>TRLG: Fragile blind quad watermarking for image tamper detection and recovery by providing compact digests with quality optimized using LWT and GA. (arXiv:1803.02623v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1803.02623</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, an efficient fragile blind quad watermarking scheme for image
tamper detection and recovery based on lifting wavelet transform and genetic
algorithm is proposed. TRLG generates four compact digests with super quality
based on lifting wavelet transform and halftoning technique by distinguishing
the types of image blocks. In other words, for each 2*2 non-overlap blocks,
four chances for recovering destroyed blocks are considered. A special
parameter estimation technique based on genetic algorithm is performed to
improve and optimize the quality of digests and watermarked image. Furthermore,
CCS map is used to determine the mapping block for embedding information,
encrypting and confusing the embedded information. In order to improve the
recovery rate, Mirror-aside and Partner-block are proposed. The experiments
that have been conducted to evaluate the performance of TRLG proved the
superiority in terms of quality of the watermarked and recovered image, tamper
localization and security compared with state-of-the-art methods. The results
indicate that the PSNR and SSIM of the watermarked image are about 46 dB and
approximately one, respectively. Also, the mean of PSNR and SSIM of several
recovered images which has been destroyed about 90% is reached to 24 dB and
0.86, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haghighi_B/0/1/0/all/0/1&quot;&gt;Behrouz Bolourian Haghighi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taherinia_A/0/1/0/all/0/1&quot;&gt;Amir Hossein Taherinia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohajerzadeh_A/0/1/0/all/0/1&quot;&gt;Amir Hossein Mohajerzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02808">
<title>OntoWind: An Improved and Extended Wind Energy Ontology. (arXiv:1803.02808v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.02808</link>
<description rdf:parseType="Literal">&lt;p&gt;Ontologies are critical sources of semantic information for many application
domains. Hence, there are ontologies proposed and utilized for domains such as
medicine, chemical engineering, and electrical energy. In this paper, we
present an improved and extended version of a wind energy ontology previously
proposed. First, the ontology is restructured to increase its understandability
and coverage. Secondly, it is enriched with new concepts, crisp/fuzzy
attributes, and instances to increase its usability in semantic applications
regarding wind energy. The ultimate ontology is utilized within a Web-based
semantic portal application for wind energy, in order to showcase its
contribution in a genuine application. Hence, the current study is a
significant to wind and thereby renewable energy informatics, with the
presented publicly-available wind energy ontology and the implemented
proof-of-concept system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kucuk_D/0/1/0/all/0/1&quot;&gt;Dilek K&amp;#xfc;&amp;#xe7;&amp;#xfc;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kucuk_D/0/1/0/all/0/1&quot;&gt;Do&amp;#x11f;an K&amp;#xfc;&amp;#xe7;&amp;#xfc;k&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.01733">
<title>Boosting Variational Inference: an Optimization Perspective. (arXiv:1708.01733v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.01733</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational inference is a popular technique to approximate a possibly
intractable Bayesian posterior with a more tractable one. Recently, boosting
variational inference has been proposed as a new paradigm to approximate the
posterior by a mixture of densities by greedily adding components to the
mixture. However, as is the case with many other variational inference
algorithms, its theoretical properties have not been studied. In the present
work, we study the convergence properties of this approach from a modern
optimization viewpoint by establishing connections to the classic Frank-Wolfe
algorithm. Our analyses yields novel theoretical insights regarding the
sufficient conditions for convergence, explicit rates, and algorithmic
simplifications. Since a lot of focus in previous works for variational
inference has been on tractability, our work is especially important as a much
needed attempt to bridge the gap between probabilistic models and their
corresponding theoretical properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanna_R/0/1/0/all/0/1&quot;&gt;Rajiv Khanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_J/0/1/0/all/0/1&quot;&gt;Joydeep Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratsch_G/0/1/0/all/0/1&quot;&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01275">
<title>A Deeper Look at Experience Replay. (arXiv:1712.01275v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01275</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently experience replay is widely used in various deep reinforcement
learning (RL) algorithms, however in this paper we showcase that it is not as
good as people think. To be more specific, experience replay will significantly
hurt the learning process if the size of replay buffer is not well tuned.
Although experience replay is a necessary component in modern deep RL
algorithms to stabilize the network, we should be aware that the idea of
experience replay itself is not as good as people think. The size of the replay
buffer is an important hyper-parameter, which can significantly influence the
performance and has unfortunately been underestimated in the community for a
long time. In this paper we did a systematic empirical study of experience
replay under various function representations. We showcase that a large replay
buffer can significantly hurt the performance. Moreover, we propose a simple
O(1) method to remedy the negative influence of a large replay buffer. We
showcase its utility in both simple grid world and challenging domains like
Atari games. Moreover, we visualize how a large replay buffer hurts the
learning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shangtong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1&quot;&gt;Richard S. Sutton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04127">
<title>A New Algorithmic Decision for Categorical Syllogisms via Caroll&apos;s Diagrams. (arXiv:1802.04127v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04127</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we deal with a calculus system SLCD (Syllogistic Logic with
Carroll Diagrams), which gives a formal approach to logical reasoning with
diagrams, for representations of the fundamental Aristotelian categorical
propositions and show that they are closed under the syllogistic criterion of
inference which is the deletion of middle term. Therefore, it is implemented to
let the formalism comprise synchronically bilateral and trilateral
diagrammatical appearance and a naive algorithmic nature. And also, there is no
need specific knowledge or exclusive ability to understand as well as to use
it. Consequently, we give an effective algorithm used to determine whether a
syllogistic reasoning valid or not by using SLCD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gursoy_A/0/1/0/all/0/1&quot;&gt;Arif Gursoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senturk_I/0/1/0/all/0/1&quot;&gt;Ibrahim Senturk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oner_T/0/1/0/all/0/1&quot;&gt;Tahsin Oner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06382">
<title>Scalable Alignment Kernels via Space-Efficient Feature Maps. (arXiv:1802.06382v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06382</link>
<description rdf:parseType="Literal">&lt;p&gt;String kernels are attractive data analysis tools for analyzing string data.
Among them, alignment kernels are known for their high prediction accuracies in
string classifications when tested in combination with SVMs in various
applications. However, alignment kernels have a crucial drawback in that they
scale poorly due to their quadratic computation complexity in the number of
input strings, which limits large-scale applications in practice. We present
the first approximation named ESP+SFM for alignment kernels by leveraging a
metric embedding named edit-sensitive parsing (ESP) and space-efficient feature
maps (SFM) for random Fourier features (RFF) for large-scale string analyses.
Input strings are projected into vectors of RFF by leveraging ESP and SFM.
Then, SVMs are trained on the projected vectors, which enables to significantly
improve the scalability of alignment kernels while preserving their prediction
accuracies. We experimentally test ESP+ SFM on its ability to learn SVMs for
large-scale string classifications with various massive string data, and we
demonstrate the superior performance of ESP+SFM with respect to prediction
accuracy, scalability and computation efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabei_Y/0/1/0/all/0/1&quot;&gt;Yasuo Tabei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamanishi_Y/0/1/0/all/0/1&quot;&gt;Yoshihiro Yamanishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pagh_R/0/1/0/all/0/1&quot;&gt;Rasmus Pagh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.08100">
<title>Semi-supervised Embedding in Attributed Networks with Outliers. (arXiv:1703.08100v3 [cs.SI] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1703.08100</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel framework, called Semi-supervised Embedding
in Attributed Networks with Outliers (SEANO), to learn a low-dimensional vector
representation that systematically captures the topological proximity,
attribute affinity and label similarity of vertices in a partially labeled
attributed network (PLAN). Our method is designed to work in both transductive
and inductive settings while explicitly alleviating noise effects from
outliers. Experimental results on various datasets drawn from the web, text and
image domains demonstrate the advantages of SEANO over state-of-the-art methods
in semi-supervised classification under transductive as well as inductive
settings. We also show that a subset of parameters in SEANO is interpretable as
outlier score and can significantly outperform baseline methods when applied
for detecting network outliers. Finally, we present the use of SEANO in a
challenging real-world setting -- flood mapping of satellite images and show
that it is able to outperform modern remote sensing algorithms for this task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jiongqian Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_P/0/1/0/all/0/1&quot;&gt;Peter Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiankai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1&quot;&gt;Srinivasan Parthasarathy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02423">
<title>Matched Filters for Noisy Induced Subgraph Detection. (arXiv:1803.02423v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.02423</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of finding the vertex correspondence between two
graphs with different number of vertices where the smaller graph is still
potentially large. We propose a solution to this problem via a graph matching
matched filter: padding the smaller graph in different ways and then using
graph matching methods to align it to the larger network. Under a statistical
model for correlated pairs of graphs, which yields a noisy copy of the small
graph within the larger graph, the resulting optimization problem can be
guaranteed to recover the true vertex correspondence between the networks,
though there are currently no efficient algorithms for solving this problem. We
consider an approach that exploits a partially known correspondence and show
via varied simulations and applications to the Drosophila connectome that in
practice this approach can achieve good performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sussman_D/0/1/0/all/0/1&quot;&gt;Daniel L. Sussman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lyzinski_V/0/1/0/all/0/1&quot;&gt;Vince Lyzinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Youngser Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02432">
<title>On Nonlinear Dimensionality Reduction, Linear Smoothing and Autoencoding. (arXiv:1803.02432v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.02432</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop theory for nonlinear dimensionality reduction (NLDR). A number of
NLDR methods have been developed, but there is limited understanding of how
these methods work and the relationships between them. There is limited basis
for using existing NLDR theory for deriving new algorithms. We provide a novel
framework for analysis of NLDR via a connection to the statistical theory of
linear smoothers. This allows us to both understand existing methods and derive
new ones. We use this connection to smoothing to show that asymptotically,
existing NLDR methods correspond to discrete approximations of the solutions of
sets of differential equations given a boundary condition. In particular, we
can characterize many existing methods in terms of just three limiting
differential operators and boundary conditions. Our theory also provides a way
to assert that one method is preferable to another; indeed, we show Local
Tangent Space Alignment is superior within a class of methods that assume a
global coordinate chart defines an isometric embedding of the manifold.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ting_D/0/1/0/all/0/1&quot;&gt;Daniel Ting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02525">
<title>Fast Robust Methods for Singular State-Space Models. (arXiv:1803.02525v1 [math.OC])</title>
<link>http://arxiv.org/abs/1803.02525</link>
<description rdf:parseType="Literal">&lt;p&gt;State-space models are used in a wide range of time series analysis
formulations. Kalman filtering and smoothing are work-horse algorithms in these
settings. While classic algorithms assume Gaussian errors to simplify
estimation, recent advances use a broader range of optimization formulations to
allow outlier-robust estimation, as well as constraints to capture prior
information.
&lt;/p&gt;
&lt;p&gt;Here we develop methods on state-space models where either innovations or
error covariances may be singular. These models frequently arise in navigation
(e.g. for `colored noise&apos; models or deterministic integrals) and are ubiquitous
in auto-correlated time series models such as ARMA. We reformulate all
state-space models (singular as well as nonsinguar) as constrained convex
optimization problems, and develop an efficient algorithm for this
reformulation. The convergence rate is {\it locally linear}, with constants
that do not depend on the conditioning of the problem.
&lt;/p&gt;
&lt;p&gt;Numerical comparisons show that the new approach outperforms competing
approaches for {\it nonsingular} models, including state of the art interior
point (IP) methods. IP methods converge at superlinear rates; we expect them to
dominate. However, the steep rate of the proposed approach (independent of
problem conditioning) combined with cheap iterations wins against IP in a
run-time comparison. We therefore suggest that the proposed approach be the
{\it default choice} for estimating state space models outside of the Gaussian
context, regardless of whether the error covariances are singular or not.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jonker_J/0/1/0/all/0/1&quot;&gt;Jonathan Jonker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Aravkin_A/0/1/0/all/0/1&quot;&gt;Aleksandr Y. Aravkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Burke_J/0/1/0/all/0/1&quot;&gt;James V. Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pillonetto_G/0/1/0/all/0/1&quot;&gt;Gianluigi Pillonetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Webster_S/0/1/0/all/0/1&quot;&gt;Sarah Webster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02527">
<title>Differential Expression Analysis of Dynamical Sequencing Count Data with a Gamma Markov Chain. (arXiv:1803.02527v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1803.02527</link>
<description rdf:parseType="Literal">&lt;p&gt;Next-generation sequencing (NGS) to profile temporal changes in living
systems is gaining more attention for deriving better insights into the
underlying biological mechanisms compared to traditional static sequencing
experiments. Nonetheless, the majority of existing statistical tools for
analyzing NGS data lack the capability of exploiting the richer information
embedded in temporal data. Several recent tools have been developed to analyze
such data but they typically impose strict model assumptions, such as
smoothness on gene expression dynamic changes. To capture a broader range of
gene expression dynamic patterns, we develop the gamma Markov negative binomial
(GMNB) model that integrates a gamma Markov chain into a negative binomial
distribution model, allowing flexible temporal variation in NGS count data.
Using Bayes factors, GMNB enables more powerful temporal gene differential
expression analysis across different phenotypes or treatment conditions. In
addition, it naturally handles the heterogeneity of sequencing depth in
different samples, removing the need for ad-hoc normalization. Efficient Gibbs
sampling inference of the GMNB model parameters is achieved by exploiting novel
data augmentation techniques. Extensive experiments on both simulated and
real-world RNA-seq data show that GMNB outperforms existing methods in both
receiver operating characteristic (ROC) and precision-recall (PR) curves of
differential expression analysis results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hajiramezanali_E/0/1/0/all/0/1&quot;&gt;Ehsan Hajiramezanali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dadaneh_S/0/1/0/all/0/1&quot;&gt;Siamak Zamani Dadaneh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Figueiredo_P/0/1/0/all/0/1&quot;&gt;Paul de Figueiredo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sze_S/0/1/0/all/0/1&quot;&gt;Sing-Hoi Sze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xiaoning Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02551">
<title>Extracting Domain Invariant Features by Unsupervised Learning for Robust Automatic Speech Recognition. (arXiv:1803.02551v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.02551</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance of automatic speech recognition (ASR) systems can be
significantly compromised by previously unseen conditions, which is typically
due to a mismatch between training and testing distributions. In this paper, we
address robustness by studying domain invariant features, such that domain
information becomes transparent to ASR systems, resolving the mismatch problem.
Specifically, we investigate a recent model, called the Factorized Hierarchical
Variational Autoencoder (FHVAE). FHVAEs learn to factorize sequence-level and
segment-level attributes into different latent variables without supervision.
We argue that the set of latent variables that contain segment-level
information is our desired domain invariant feature for ASR. Experiments are
conducted on Aurora-4 and CHiME-4, which demonstrate 41% and 27% absolute word
error rate reductions respectively on mismatched domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1&quot;&gt;Wei-Ning Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1&quot;&gt;James Glass&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02553">
<title>Graph Learning from Filtered Signals: Graph System and Diffusion Kernel Identification. (arXiv:1803.02553v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.02553</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel graph signal processing framework for building
graph-based models from classes of filtered signals. In our framework,
graph-based modeling is formulated as a graph system identification problem,
where the goal is to learn a weighted graph (a graph Laplacian matrix) and a
graph-based filter (a function of graph Laplacian matrices). In order to solve
the proposed problem, an algorithm is developed to jointly identify a graph and
a graph-based filter (GBF) from multiple signal/data observations. Our
algorithm is valid under the assumption that GBFs are one-to-one functions. The
proposed approach can be applied to learn diffusion (heat) kernels, which are
popular in various fields for modeling diffusion processes. In addition, for
specific choices of graph-based filters, the proposed problem reduces to a
graph Laplacian estimation problem. Our experimental results demonstrate that
the proposed algorithm outperforms the current state-of-the-art methods. We
also implement our framework on a real climate dataset for modeling of
temperature signals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egilmez_H/0/1/0/all/0/1&quot;&gt;Hilmi E. Egilmez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavez_E/0/1/0/all/0/1&quot;&gt;Eduardo Pavez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_A/0/1/0/all/0/1&quot;&gt;Antonio Ortega&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02596">
<title>Revisiting differentially private linear regression: optimal and adaptive prediction &amp; estimation in unbounded domain. (arXiv:1803.02596v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.02596</link>
<description rdf:parseType="Literal">&lt;p&gt;We revisit the problem of linear regression under a differential privacy
constraint. By consolidating existing pieces in the literature, we clarify the
correct dependence of the feature, label and coefficient domain in the
optimization error and estimation error, hence revealing the delicate price of
differential privacy in statistical estimation and statistical learning.
Moreover, we propose simple modifications of two existing DP algorithms: (a)
posterior sampling, (b) sufficient statistics perturbation, and show that they
can be upgraded into **adaptive** algorithms that are able to exploit
data-dependent quantities and behave nearly optimally for every instance.
Extensive experiments are conducted on both simulated data and real data, which
conclude that both AdaOPS and AdaSSP outperform the existing techniques on
nearly all 36 data sets that we test on.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02598">
<title>The Ising distribution as a latent variable model. (arXiv:1803.02598v1 [cond-mat.dis-nn])</title>
<link>http://arxiv.org/abs/1803.02598</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that the Ising distribution can be treated as a latent variable
model, where a set of N real-valued, correlated random variables are drawn and
used to generate N binary spins independently. This allows to approximate the
Ising distribution by a simpler model where the latent variables follow a
multivariate normal distribution. The resulting approximation bears
similarities with the Thouless Anderson Palmer (TAP) solution from mean field
theory, but retains a broader range of applicability when the coupling weights
are not independently distributed. Moreover, unlike classic mean field
approaches, the approximation can be used to generate correlated spin patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Wohrer_A/0/1/0/all/0/1&quot;&gt;Adrien Wohrer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02781">
<title>Fast Dawid-Skene. (arXiv:1803.02781v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.02781</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real world problems can now be effectively solved using supervised
machine learning. A major roadblock is often the lack of an adequate quantity
of labeled data for training. A possible solution is to assign the task of
labeling data to a crowd, and then infer the true label using aggregation
methods. A well-known approach for aggregation is the Dawid-Skene (DS)
algorithm, which is based on the principle of Expectation-Maximization (EM). We
propose a new simple, yet effective, EM-based algorithm, which can be
interpreted as a &apos;hard&apos; version of DS, that allows much faster convergence
while maintaining similar accuracy in aggregation. We also show how the
proposed method can be extended to settings when there are multiple labels as
well as for online vote aggregation. Our experiments on standard vote
aggregation datasets show a significant speedup in time taken for convergence -
upto $\sim$8x over Dawid-Skene and $\sim$6x over other fast EM methods, at
competitive accuracy performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sinha_V/0/1/0/all/0/1&quot;&gt;Vaibhav Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rao_S/0/1/0/all/0/1&quot;&gt;Sukrut Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balasubramanian_V/0/1/0/all/0/1&quot;&gt;Vineeth N Balasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.08306">
<title>Machine Learning Approach for Improved Downlink Coordinated Multipoint in Heterogeneous Networks. (arXiv:1608.08306v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1608.08306</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method for practical downlink coordinated multipoint (DL CoMP)
implementation in 4G LTE/LTE-A systems using supervised machine learning.
Contributions of this paper are: 1) demonstrating that a support vector machine
classifier can learn the optimal conditions at which DL CoMP can be dynamically
triggered, 2) improving user throughput in DL CoMP as a result of learning the
optimal triggering conditions of DL CoMP, and 3) showing that the machine
learning approach is scalable to more than a single macro. The simulation
results show an improvement in the pico cell average and edge throughputs and a
reduction in the downlink block error rate due to the informed triggering of
the multiple radio streams as part of DL CoMP as learned from the support
vector machine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mismar_F/0/1/0/all/0/1&quot;&gt;Faris B. Mismar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Evans_B/0/1/0/all/0/1&quot;&gt;Brian L. Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.00340">
<title>Variational Bayes In Private Settings (VIPS). (arXiv:1611.00340v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.00340</link>
<description rdf:parseType="Literal">&lt;p&gt;Many applications of Bayesian data analysis involve sensitive information,
motivating methods which ensure that privacy is protected. We introduce a
general privacy-preserving framework for Variational Bayes (VB), a widely used
optimization-based Bayesian inference method. Our framework respects
differential privacy, the gold-standard privacy criterion, and encompasses a
large class of probabilistic models, called the Conjugate Exponential (CE)
family. We observe that we can straightforwardly privatise VB&apos;s approximate
posterior distributions for models in the CE family, by perturbing the expected
sufficient statistics of the complete-data likelihood. For a broadly-used class
of non-CE models, those with binomial likelihoods, we show how to bring such
models into the CE family, such that inferences in the modified model resemble
the private variational Bayes algorithm as closely as possible, using the
Polya-Gamma data augmentation scheme. The iterative nature of variational Bayes
presents a further challenge since iterations increase the amount of noise
needed. We overcome this by combining: (1) an improved composition method for
differential privacy, called the moments accountant, which provides a tight
bound on the privacy cost of multiple VB iterations and thus significantly
decreases the amount of additive noise; and (2) the privacy amplification
effect of subsampling mini-batches from large-scale data in stochastic
learning. We empirically demonstrate the effectiveness of our method in CE and
non-CE models including latent Dirichlet allocation, Bayesian logistic
regression, and sigmoid belief networks, evaluated on real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Park_M/0/1/0/all/0/1&quot;&gt;Mijung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Foulds_J/0/1/0/all/0/1&quot;&gt;James Foulds&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chaudhuri_K/0/1/0/all/0/1&quot;&gt;Kamalika Chaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.01166">
<title>Optimal Subsampling for Large Sample Logistic Regression. (arXiv:1702.01166v2 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1702.01166</link>
<description rdf:parseType="Literal">&lt;p&gt;For massive data, the family of subsampling algorithms is popular to downsize
the data volume and reduce computational burden. Existing studies focus on
approximating the ordinary least squares estimate in linear regression, where
statistical leverage scores are often used to define subsampling probabilities.
In this paper, we propose fast subsampling algorithms to efficiently
approximate the maximum likelihood estimate in logistic regression. We first
establish consistency and asymptotic normality of the estimator from a general
subsampling algorithm, and then derive optimal subsampling probabilities that
minimize the asymptotic mean squared error of the resultant estimator. An
alternative minimization criterion is also proposed to further reduce the
computational cost. The optimal subsampling probabilities depend on the full
data estimate, so we develop a two-step algorithm to approximate the optimal
subsampling procedure. This algorithm is computationally efficient and has a
significant reduction in computing time compared to the full data approach.
Consistency and asymptotic normality of the estimator from a two-step algorithm
are also established. Synthetic and real data sets are used to evaluate the
practical performance of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;HaiYing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Rong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_P/0/1/0/all/0/1&quot;&gt;Ping Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.04410">
<title>A strong converse bound for multiple hypothesis testing, with applications to high-dimensional estimation. (arXiv:1706.04410v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1706.04410</link>
<description rdf:parseType="Literal">&lt;p&gt;In statistical inference problems, we wish to obtain lower bounds on the
minimax risk, that is to bound the performance of any possible estimator. A
standard technique to obtain risk lower bounds involves the use of Fano&apos;s
inequality. In an information-theoretic setting, it is known that Fano&apos;s
inequality typically does not give a sharp converse result (error lower bound)
for channel coding problems. Moreover, recent work has shown that an argument
based on binary hypothesis testing gives tighter results. We adapt this
technique to the statistical setting, and argue that Fano&apos;s inequality can
always be replaced by this approach to obtain tighter lower bounds that can be
easily computed and are asymptotically sharp. We illustrate our technique in
three applications: density estimation, active learning of a binary classifier,
and compressed sensing, obtaining tighter risk lower bounds in each case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkataramanan_R/0/1/0/all/0/1&quot;&gt;Ramji Venkataramanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_O/0/1/0/all/0/1&quot;&gt;Oliver Johnson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.04045">
<title>Neural-Network Quantum States, String-Bond States, and Chiral Topological States. (arXiv:1710.04045v3 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1710.04045</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural-Network Quantum States have been recently introduced as an Ansatz for
describing the wave function of quantum many-body systems. We show that there
are strong connections between Neural-Network Quantum States in the form of
Restricted Boltzmann Machines and some classes of Tensor-Network states in
arbitrary dimensions. In particular we demonstrate that short-range Restricted
Boltzmann Machines are Entangled Plaquette States, while fully connected
Restricted Boltzmann Machines are String-Bond States with a nonlocal geometry
and low bond dimension. These results shed light on the underlying architecture
of Restricted Boltzmann Machines and their efficiency at representing many-body
quantum states. String-Bond States also provide a generic way of enhancing the
power of Neural-Network Quantum States and a natural generalization to systems
with larger local Hilbert space. We compare the advantages and drawbacks of
these different classes of states and present a method to combine them
together. This allows us to benefit from both the entanglement structure of
Tensor Networks and the efficiency of Neural-Network Quantum States into a
single Ansatz capable of targeting the wave function of strongly correlated
systems. While it remains a challenge to describe states with chiral
topological order using traditional Tensor Networks, we show that
Neural-Network Quantum States and their String-Bond States extension can
describe a lattice Fractional Quantum Hall state exactly. In addition, we
provide numerical evidence that Neural-Network Quantum States can approximate a
chiral spin liquid with better accuracy than Entangled Plaquette States and
local String-Bond States. Our results demonstrate the efficiency of neural
networks to describe complex quantum wave functions and pave the way towards
the use of String-Bond States as a tool in more traditional machine-learning
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Glasser_I/0/1/0/all/0/1&quot;&gt;Ivan Glasser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Pancotti_N/0/1/0/all/0/1&quot;&gt;Nicola Pancotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+August_M/0/1/0/all/0/1&quot;&gt;Moritz August&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Rodriguez_I/0/1/0/all/0/1&quot;&gt;Ivan D. Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cirac_J/0/1/0/all/0/1&quot;&gt;J. Ignacio Cirac&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06273">
<title>Combinatorial Penalties: Which structures are preserved by convex relaxations?. (arXiv:1710.06273v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06273</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the homogeneous and the non-homogeneous convex relaxations for
combinatorial penalty functions defined on support sets. Our study identifies
key differences in the tightness of the resulting relaxations through the
notion of the lower combinatorial envelope of a set-function along with new
necessary conditions for support identification. We then propose a general
adaptive estimator for convex monotone regularizers, and derive new sufficient
conditions for support recovery in the asymptotic setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halabi_M/0/1/0/all/0/1&quot;&gt;Marwa El Halabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis Bach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.08841">
<title>Algorithmic detectability threshold of the stochastic block model. (arXiv:1710.08841v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.08841</link>
<description rdf:parseType="Literal">&lt;p&gt;The assumption that the values of model parameters are known or correctly
learned, i.e., the Nishimori condition, is one of the requirements for the
detectability analysis of the stochastic block model in statistical inference.
In practice, however, there is no example demonstrating that we can know the
model parameters beforehand, and there is no guarantee that the model
parameters can be learned accurately. In this study, we consider the
expectation--maximization (EM) algorithm with belief propagation (BP) and
derive its algorithmic detectability threshold. Our analysis is not restricted
to the community structure, but includes general modular structures. Because
the algorithm cannot always learn the planted model parameters correctly, the
algorithmic detectability threshold is qualitatively different from the one
with the Nishimori condition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawamoto_T/0/1/0/all/0/1&quot;&gt;Tatsuro Kawamoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.02317">
<title>Multi-Player Bandits Models Revisited. (arXiv:1711.02317v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.02317</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-player Multi-Armed Bandits (MAB) have been extensively studied in the
literature, motivated by applications to Cognitive Radio systems. Driven by
such applications as well, we motivate the introduction of several levels of
feedback for multi-player MAB algorithms. Most existing work assume that
sensing information is available to the algorithm. Under this assumption, we
improve the state-of-the-art lower bound for the regret of any decentralized
algorithms and introduce two algorithms, RandTopM and MCTopM, that are shown to
empirically outperform existing algorithms. Moreover, we provide strong
theoretical guarantees for these algorithms, including a notion of asymptotic
optimality in terms of the number of selections of bad arms. We then introduce
a promising heuristic, called Selfish, that can operate without sensing
information, which is crucial for emerging applications to Internet of Things
networks. We investigate the empirical performance of this algorithm and
provide some first theoretical elements for the understanding of its behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Besson_L/0/1/0/all/0/1&quot;&gt;Lilian Besson&lt;/a&gt; (IETR, SEQUEL), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kaufmann_E/0/1/0/all/0/1&quot;&gt;Emilie Kaufmann&lt;/a&gt; (CRIStAL, SEQUEL)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06104">
<title>Towards better understanding of gradient-based attribution methods for Deep Neural Networks. (arXiv:1711.06104v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06104</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the flow of information in Deep Neural Networks (DNNs) is a
challenging problem that has gain increasing attention over the last few years.
While several methods have been proposed to explain network predictions, there
have been only a few attempts to compare them from a theoretical perspective.
What is more, no exhaustive empirical comparison has been performed in the
past. In this work, we analyze four gradient-based attribution methods and
formally prove conditions of equivalence and approximation between them. By
reformulating two of these methods, we construct a unified framework which
enables a direct comparison, as well as an easier implementation. Finally, we
propose a novel evaluation metric, called Sensitivity-n and test the
gradient-based attribution methods alongside with a simple perturbation-based
attribution method on several datasets in the domains of image and text
classification, using various network architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ancona_M/0/1/0/all/0/1&quot;&gt;Marco Ancona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceolini_E/0/1/0/all/0/1&quot;&gt;Enea Ceolini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oztireli_C/0/1/0/all/0/1&quot;&gt;Cengiz &amp;#xd6;ztireli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_M/0/1/0/all/0/1&quot;&gt;Markus Gross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04153">
<title>Bayesian Quadrature for Multiple Related Integrals. (arXiv:1801.04153v4 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04153</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian probabilistic numerical methods are a set of tools providing
posterior distributions on the output of numerical methods. The use of these
methods is usually motivated by the fact that they can represent our
uncertainty due to incomplete/finite information about the continuous
mathematical problem being approximated. In this paper, we demonstrate that
this paradigm can provide additional advantages, such as the possibility of
transferring information between several numerical methods. This allows users
to represent uncertainty in a more faithful manner and, as a by-product,
provide increased numerical efficiency. We propose the first such numerical
method by extending the well-known Bayesian quadrature algorithm to the case
where we are interested in computing the integral of several related functions.
We then prove convergence rates for the method in the well-specified and
misspecified cases, and demonstrate its efficiency in the context of
multi-fidelity models for complex engineering systems and a problem of global
illumination in computer graphics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xi_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois-Xavier Briol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1&quot;&gt;Mark Girolami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08570">
<title>Deep Learning in Pharmacogenomics: From Gene Regulation to Patient Stratification. (arXiv:1801.08570v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/1801.08570</link>
<description rdf:parseType="Literal">&lt;p&gt;This Perspective provides examples of current and future applications of deep
learning in pharmacogenomics, including: (1) identification of novel regulatory
variants located in noncoding domains and their function as applied to
pharmacoepigenomics; (2) patient stratification from medical records; and (3)
prediction of drugs, targets, and their interactions. Deep learning
encapsulates a family of machine learning algorithms that over the last decade
has transformed many important subfields of artificial intelligence (AI) and
has demonstrated breakthrough performance improvements on a wide range of tasks
in biomedicine. We anticipate that in the future deep learning will be widely
used to predict personalized drug response and optimize medication selection
and dosing, using knowledge extracted from large and complex molecular,
epidemiological, clinical, and demographic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kalinin_A/0/1/0/all/0/1&quot;&gt;Alexandr A. Kalinin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Higgins_G/0/1/0/all/0/1&quot;&gt;Gerald A. Higgins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Reamaroon_N/0/1/0/all/0/1&quot;&gt;Narathip Reamaroon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Soroushmehr_S/0/1/0/all/0/1&quot;&gt;S.M. Reza Soroushmehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Allyn_Feuer_A/0/1/0/all/0/1&quot;&gt;Ari Allyn-Feuer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dinov_I/0/1/0/all/0/1&quot;&gt;Ivo D. Dinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Najarian_K/0/1/0/all/0/1&quot;&gt;Kayvan Najarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Athey_B/0/1/0/all/0/1&quot;&gt;Brian D. Athey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1510.00967">
<title>Stable Robbins-Monro approximations through stochastic proximal updates. (arXiv:1510.00967v3 [math.ST] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1510.00967</link>
<description rdf:parseType="Literal">&lt;p&gt;The need for parameter estimation with massive data has reinvigorated
interest in iterative estimation procedures. Stochastic approximations, such as
stochastic gradient descent, are at the forefront of this recent development
because they yield simple, generic, and extremely fast iterative estimation
procedures. Such stochastic approximations, however, are often numerically
unstable. As a consequence, current practice has turned to proximal operators,
which can induce stable parameter updates within iterations. While the majority
of classical iterative estimation procedures are subsumed by the framework of
Robbins and Monro (1951), there is no such generalization for stochastic
approximations with proximal updates. In this paper, we conceptualize a general
stochastic approximation method with proximal updates. This method can be
applied even in situations where the analytical form of the objective is not
known, and so it generalizes many stochastic gradient procedures with proximal
operators currently in use. Our theoretical analysis indicates that the
proposed method has important stability benefits over the classical stochastic
approximation method. Exact instantiations of the proposed method are
challenging, but we show that approximate instantiations lead to procedures
that are easy to implement, and still dominate classical procedures by
achieving numerical stability without tradeoffs. This last advantage is akin to
that seen in deterministic proximal optimization, where the framework is
typically impossible to instantiate exactly, but where approximate
instantiations lead to new optimization procedures that dominate classical
ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Toulis_P/0/1/0/all/0/1&quot;&gt;Panos Toulis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Horel_T/0/1/0/all/0/1&quot;&gt;Thibaut Horel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Airoldi_E/0/1/0/all/0/1&quot;&gt;Edoardo M. Airoldi&lt;/a&gt;</dc:creator>
</item></rdf:RDF>