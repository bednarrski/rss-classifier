<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-30T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11778"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11818"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11987"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00970"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01507"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06541"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11704"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11706"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11714"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11730"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11815"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11820"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11984"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12085"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12090"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12115"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.02262"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.08146"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05509"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02025"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06891"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11122"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11640"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11643"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11659"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11686"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11712"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11754"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11769"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11783"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11792"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11793"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11811"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11845"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11897"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11908"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11954"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11956"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11959"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11973"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12002"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12021"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12044"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12062"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12111"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12120"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.04898"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.03522"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.08159"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07474"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04064"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05419"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01013"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03911"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09965"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10724"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.11778">
<title>Object Detection using Domain Randomization and Generative Adversarial Refinement of Synthetic Images. (arXiv:1805.11778v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.11778</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present an application of domain randomization and
generative adversarial networks (GAN) to train a near real-time object detector
for industrial electric parts, entirely in a simulated environment. Large scale
availability of labelled real world data is typically rare and difficult to
obtain in many industrial settings. As such here, only a few hundred of
unlabelled real images are used to train a Cyclic-GAN network, in combination
with various degree of domain randomization procedures. We demonstrate that
this enables robust translation of synthetic images to the real world domain.
We show that a combination of the original synthetic (simulation) and GAN
translated images, when used for training a Mask-RCNN object detection network
achieves greater than 0.95 mean average precision in detecting and classifying
a collection of industrial electric parts. We evaluate the performance across
different combinations of training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nogue_F/0/1/0/all/0/1&quot;&gt;Fernando Camaro Nogue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huie_A/0/1/0/all/0/1&quot;&gt;Andrew Huie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1&quot;&gt;Sakyasingha Dasgupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11818">
<title>Visual Referring Expression Recognition: What Do Systems Actually Learn?. (arXiv:1805.11818v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.11818</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an empirical analysis of the state-of-the-art systems for
referring expression recognition -- the task of identifying the object in an
image referred to by a natural language expression -- with the goal of gaining
insight into how these systems reason about language and vision. Surprisingly,
we find strong evidence that even sophisticated and linguistically-motivated
models for this task may ignore the linguistic structure, instead relying on
shallow correlations introduced by unintended biases in the data selection and
annotation process. For example, we show that a system trained and tested on
the input image $\textit{without the input referring expression}$ can achieve a
precision of 71.2% in top-2 predictions. Furthermore, a system that predicts
only the object category given the input can achieve a precision of 84.2% in
top-2 predictions. These surprisingly positive results for what should be
deficient prediction scenarios suggest that careful analysis of what our models
are learning -- and further, how our data is constructed -- is critical as we
seek to make substantive progress on grounded language tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cirik_V/0/1/0/all/0/1&quot;&gt;Volkan Cirik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1&quot;&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11987">
<title>l0-norm Based Centers Selection for Failure Tolerant RBF Networks. (arXiv:1805.11987v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.11987</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of this paper is to select the RBF neural network centers under
concurrent faults. It is well known that fault tolerance is a very attractive
property for neural network algorithms. And center selection is an important
procedure during the training process of RBF neural network. In this paper, we
will address these two issues simultaneously and devise two novel algorithms.
Both of them are based on the framework of ADMM and utilize the technique of
sparse approximation. For both two methods, we first define a fault tolerant
objective function. After that, the first method introduces the MCP function
(an approximate l0-norm function) and combine it with ADMM framework to select
the RBF centers. While the second method utilize ADMM and IHT to solve the
problem. The convergence of both two methods is proved. Simulation results show
that the proposed algorithms are superior to many existing center selection
algorithms under concurrent fault.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leung_C/0/1/0/all/0/1&quot;&gt;Chi-Sing Leung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+So_H/0/1/0/all/0/1&quot;&gt;Hing Cheung So&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Ruibin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zifa Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00970">
<title>A Classification-Based Study of Covariate Shift in GAN Distributions. (arXiv:1711.00970v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00970</link>
<description rdf:parseType="Literal">&lt;p&gt;A basic, and still largely unanswered, question in the context of Generative
Adversarial Networks (GANs) is whether they are truly able to capture all the
fundamental characteristics of the distributions they are trained on. In
particular, evaluating the diversity of GAN distributions is challenging and
existing methods provide only a partial understanding of this issue. In this
paper, we develop quantitative and scalable tools for assessing the diversity
of GAN distributions. Specifically, we take a classification-based perspective
and view loss of diversity as a form of covariate shift introduced by GANs. We
examine two specific forms of such shift: mode collapse and boundary
distortion. In contrast to prior work, our methods need only minimal human
supervision and can be readily applied to state-of-the-art GANs on large,
canonical datasets. Examining popular GANs using our tools indicates that these
GANs have significant problems in reproducing the more distributional
properties of their training dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santurkar_S/0/1/0/all/0/1&quot;&gt;Shibani Santurkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1&quot;&gt;Aleksander M&amp;#x105;dry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01507">
<title>Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks. (arXiv:1712.01507v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01507</link>
<description rdf:parseType="Literal">&lt;p&gt;Fully realizing the potential of acceleration for Deep Neural Networks (DNNs)
requires understanding and leveraging algorithmic properties. This paper builds
upon the algorithmic insight that bitwidth of operations in DNNs can be reduced
without compromising their classification accuracy. However, to prevent
accuracy loss, the bitwidth varies significantly across DNNs and it may even be
adjusted for each layer. Thus, a fixed-bitwidth accelerator would either offer
limited benefits to accommodate the worst-case bitwidth requirements, or lead
to a degradation in final accuracy. To alleviate these deficiencies, this work
introduces dynamic bit-level fusion/decomposition as a new dimension in the
design of DNN accelerators. We explore this dimension by designing Bit Fusion,
a bit-flexible accelerator, that constitutes an array of bit-level processing
elements that dynamically fuse to match the bitwidth of individual DNN layers.
This flexibility in the architecture enables minimizing the computation and the
communication at the finest granularity possible with no loss in accuracy. We
evaluate the benefits of BitFusion using eight real-world feed-forward and
recurrent DNNs. The proposed microarchitecture is implemented in Verilog and
synthesized in 45 nm technology. Using the synthesis results and cycle accurate
simulation, we compare the benefits of Bit Fusion to two state-of-the-art DNN
accelerators, Eyeriss and Stripes. In the same area, frequency, and process
technology, BitFusion offers 3.9x speedup and 5.1x energy savings over Eyeriss.
Compared to Stripes, BitFusion provides 2.6x speedup and 3.9x energy reduction
at 45 nm node when BitFusion area and frequency are set to those of Stripes.
Scaling to GPU technology node of 16 nm, BitFusion almost matches the
performance of a 250-Watt Titan Xp, which uses 8-bit vector instructions, while
BitFusion merely consumes 895 milliwatts of power.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1&quot;&gt;Hardik Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jongse Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suda_N/0/1/0/all/0/1&quot;&gt;Naveen Suda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1&quot;&gt;Liangzhen Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_B/0/1/0/all/0/1&quot;&gt;Benson Chau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Joon Kyung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1&quot;&gt;Vikas Chandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esmaeilzadeh_H/0/1/0/all/0/1&quot;&gt;Hadi Esmaeilzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06541">
<title>Size-Independent Sample Complexity of Neural Networks. (arXiv:1712.06541v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06541</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the sample complexity of learning neural networks, by providing new
bounds on their Rademacher complexity assuming norm constraints on the
parameter matrix of each layer. Compared to previous work, these complexity
bounds have improved dependence on the network depth, and under some additional
assumptions, are fully independent of the network size (both depth and width).
These results are derived using some novel techniques, which may be of
independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golowich_N/0/1/0/all/0/1&quot;&gt;Noah Golowich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakhlin_A/0/1/0/all/0/1&quot;&gt;Alexander Rakhlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1&quot;&gt;Ohad Shamir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11704">
<title>Deep Semantic Architecture with discriminative feature visualization for neuroimage analysis. (arXiv:1805.11704v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1805.11704</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuroimaging data analysis often involves \emph{a-priori} selection of data
features to study the underlying neural activity. Since this could lead to
sub-optimal feature selection and thereby prevent the detection of subtle
patterns in neural activity, data-driven methods have recently gained
popularity for optimizing neuroimaging data analysis pipelines and thereby,
improving our understanding of neural mechanisms. In this context, we developed
a deep convolutional architecture that can identify discriminating patterns in
neuroimaging data and applied it to electroencephalography (EEG) recordings
collected from 25 subjects performing a hand motor task before and after a rest
period or a bout of exercise. The deep network was trained to classify subjects
into exercise and control groups based on differences in their EEG signals.
Subsequently, we developed a novel method termed the cue-combination for Class
Activation Map (ccCAM), which enabled us to identify discriminating
spatio-temporal features within definite frequency bands (23--33 Hz) and assess
the effects of exercise on the brain. Additionally, the proposed architecture
allowed the visualization of the differences in the propagation of underlying
neural activity across the cortex between the two groups, for the first time in
our knowledge. Our results demonstrate the feasibility of using deep network
architectures for neuroimaging analysis in different contexts such as, for the
identification of robust brain biomarkers to better characterize and
potentially treat neurological disorders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Arna Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Maso_F/0/1/0/all/0/1&quot;&gt;Fabien dal Maso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Roig_M/0/1/0/all/0/1&quot;&gt;Marc Roig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mitsis_G/0/1/0/all/0/1&quot;&gt;Georgios D Mitsis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Boudrias_M/0/1/0/all/0/1&quot;&gt;Marie-H&amp;#xe9;l&amp;#xe8;ne Boudrias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11706">
<title>Supervised Policy Update. (arXiv:1805.11706v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.11706</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new sample-efficient methodology, called Supervised Policy
Update (SPU), for deep reinforcement learning. Starting with data generated by
the current policy, SPU optimizes over the proximal policy space to find a
non-parameterized policy. It then solves a supervised regression problem to
convert the non-parameterized policy to a parameterized policy, from which it
draws new samples. There is significant flexibility in setting the labels in
the supervised regression problem, with different settings corresponding to
different underlying optimization problems. We develop a methodology for
finding an optimal policy in the non-parameterized policy space, and show how
Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO)
can be addressed by this methodology. In terms of sample efficiency, our
experiments show SPU can outperform PPO for simulated robotic locomotion tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1&quot;&gt;Quan Ho Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_K/0/1/0/all/0/1&quot;&gt;Keith W. Ross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11714">
<title>Deep Video Portraits. (arXiv:1805.11714v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.11714</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach that enables photo-realistic re-animation of
portrait videos using only an input video. In contrast to existing approaches
that are restricted to manipulations of facial expressions only, we are the
first to transfer the full 3D head position, head rotation, face expression,
eye gaze, and eye blinking from a source actor to a portrait video of a target
actor. The core of our approach is a generative neural network with a novel
space-time architecture. The network takes as input synthetic renderings of a
parametric face model, based on which it predicts photo-realistic video frames
for a given target actor. The realism in this rendering-to-video transfer is
achieved by careful adversarial training, and as a result, we can create
modified target videos that mimic the behavior of the synthetically-created
input. In order to enable source-to-target video re-animation, we render a
synthetic target video with the reconstructed head animation parameters from a
source video, and feed it into the trained network -- thus taking full control
of the target. With the ability to freely recombine source and target
parameters, we are able to demonstrate a large variety of video rewrite
applications without explicitly modeling hair, body or background. For
instance, we can reenact the full head using interactive user-controlled
editing, and realize high-fidelity visual dubbing. To demonstrate the high
quality of our output, we conduct an extensive series of experiments and
evaluations, where for instance a user study shows that our video edits are
hard to detect.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyeongwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garrido_P/0/1/0/all/0/1&quot;&gt;Pablo Garrido&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1&quot;&gt;Ayush Tewari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weipeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1&quot;&gt;Justus Thies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1&quot;&gt;Patrick P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1&quot;&gt;Christian Richardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1&quot;&gt;Michael Zollh&amp;#xf6;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11730">
<title>Learn to Combine Modalities in Multimodal Deep Learning. (arXiv:1805.11730v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11730</link>
<description rdf:parseType="Literal">&lt;p&gt;Combining complementary information from multiple modalities is intuitively
appealing for improving the performance of learning-based approaches. However,
it is challenging to fully leverage different modalities due to practical
challenges such as varying levels of noise and conflicts between modalities.
Existing methods do not adopt a joint approach to capturing synergies between
the modalities while simultaneously filtering noise and resolving conflicts on
a per sample basis. In this work we propose a novel deep neural network based
technique that multiplicatively combines information from different source
modalities. Thus the model training process automatically focuses on
information from more reliable modalities while reducing emphasis on the less
reliable modalities. Furthermore, we propose an extension that multiplicatively
combines not only the single-source modalities, but a set of mixtured source
modalities to better capture cross-modal signal correlations. We demonstrate
the effectiveness of our proposed technique by presenting empirical results on
three multimodal classification tasks from different domains. The results show
consistent accuracy improvements on all three tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Ning Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Natarajan_P/0/1/0/all/0/1&quot;&gt;Prem Natarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11815">
<title>Enabling Pedestrian Safety using Computer Vision Techniques: A Case Study of the 2018 Uber Inc. Self-driving Car Crash. (arXiv:1805.11815v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.11815</link>
<description rdf:parseType="Literal">&lt;p&gt;Human lives are important. The decision to allow self-driving vehicles
operate on our roads carries great weight. This has been a hot topic of debate
between policy-makers, technologists and public safety institutions. The recent
Uber Inc. self-driving car crash, resulting in the death of a pedestrian, has
strengthened the argument that autonomous vehicle technology is still not ready
for deployment on public roads. In this work, we analyze the Uber car crash and
shed light on the question, &quot;Could the Uber Car Crash have been avoided?&quot;. We
apply state-of-the-art Computer Vision models to this highly practical
scenario. More generally, our experimental results are an evaluation of various
image enhancement and object recognition techniques for enabling pedestrian
safety in low-lighting conditions using the Uber crash as a case study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohli_P/0/1/0/all/0/1&quot;&gt;Puneet Kohli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Anjali Chadha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11820">
<title>Generic CP-Supported CMSA for Binary Integer Linear Programs. (arXiv:1805.11820v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.11820</link>
<description rdf:parseType="Literal">&lt;p&gt;Construct, Merge, Solve and Adapt (CMSA) is a general hybrid metaheuristic
for solving combinatorial optimization problems. At each iteration, CMSA (1)
constructs feasible solutions to the tackled problem instance in a
probabilistic way and (2) solves a reduced problem instance (if possible) to
optimality. The construction of feasible solutions is hereby problem-specific,
usually involving a fast greedy heuristic. The goal of this paper is to design
a problem-agnostic CMSA variant whose exclusive input is an integer linear
program (ILP). In order to reduce the complexity of this task, the current
study is restricted to binary ILPs. In addition to a basic problem-agnostic
CMSA variant, we also present an extended version that makes use of a
constraint propagation engine for constructing solutions. The results show that
our technique is able to match the upper bounds of the standalone application
of CPLEX in the context of rather easy-to-solve instances, while it generally
outperforms the standalone application of CPLEX in the context of hard
instances. Moreover, the results indicate that the support of the constraint
propagation engine is useful in the context of problems for which finding
feasible solutions is rather difficult.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blum_C/0/1/0/all/0/1&quot;&gt;Christian Blum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_H/0/1/0/all/0/1&quot;&gt;Haroldo Gambini Santos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11984">
<title>Automatic generation of object shapes with desired functionalities. (arXiv:1805.11984v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.11984</link>
<description rdf:parseType="Literal">&lt;p&gt;Functional design of objects is slow and still largely an artisanal activity,
with few Computer-Aided Design (CAD) tools existing to aid the exploration of
the design solution space. In order to accelerate object design, we introduce
an automatic method for generating object shapes with desired functionalities.
We employ the concept of shape arithmetic, where shapes are manipulated in an
arithmetic fashion inside a latent space representation learned by a neural
network. We propose a functionality arithmetic, with the aim of conceiving new
objects with desired functionalities from other objects with known
functionalities. This is done by first extracting the descriptions of forms
providing these functionalities, and then manipulating them using shape
arithmetic functions. Form-to-function mappings are extracted by identifying
the common features between objects belonging to the same class, inside which
all objects provide the same functionality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andries_M/0/1/0/all/0/1&quot;&gt;Mihai Andries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehban_A/0/1/0/all/0/1&quot;&gt;Atabak Dehban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_Victor_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Santos-Victor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12085">
<title>MPDCompress - Matrix Permutation Decomposition Algorithm for Deep Neural Network Compression. (arXiv:1805.12085v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12085</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have become the state-of-the-art technique for
machine learning tasks in various applications. However, due to their size and
the computational complexity, large DNNs are not readily deployable on edge
devices in real-time. To manage complexity and accelerate computation, network
compression techniques based on pruning and quantization have been proposed and
shown to be effective in reducing network size. However, such network
compression can result in irregular matrix structures that are mismatched with
modern hardware-accelerated platforms, such as graphics processing units (GPUs)
designed to perform the DNN matrix multiplications in a structured
(block-based) way. We propose MPDCompress, a DNN compression algorithm based on
matrix permutation decomposition via random mask generation. In-training
application of the masks molds the synaptic weight connection matrix to a
sub-graph separation format. Aided by the random permutations, a
hardware-desirable block matrix is generated, allowing for a more efficient
implementation and compression of the network. To show versatility, we
empirically verify MPDCompress on several network models, compression rates,
and image datasets. On the LeNet 300-100 model (MNIST dataset), Deep MNIST, and
CIFAR10, we achieve 10 X network compression with less than 1% accuracy loss
compared to non-compressed accuracy performance. On AlexNet for the full
ImageNet ILSVRC-2012 dataset, we achieve 8 X network compression with less than
1% accuracy loss, with top-5 and top-1 accuracies of 79.6% and 56.4%,
respectively. Finally, we observe that the algorithm can offer inference
speedups across various hardware platforms, with 4 X faster operation achieved
on several mobile GPUs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Supic_L/0/1/0/all/0/1&quot;&gt;Lazar Supic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naous_R/0/1/0/all/0/1&quot;&gt;Rawan Naous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sredojevic_R/0/1/0/all/0/1&quot;&gt;Ranko Sredojevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1&quot;&gt;Aleksandra Faust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stojanovic_V/0/1/0/all/0/1&quot;&gt;Vladimir Stojanovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12090">
<title>Model-Driven Artificial Intelligence for Online Network Optimization. (arXiv:1805.12090v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.12090</link>
<description rdf:parseType="Literal">&lt;p&gt;Future 5G wireless networks will rely on agile and automated network
management, where the usage of diverse resources must be jointly optimized with
surgical accuracy. A number of key wireless network functionalities (e.g.,
traffic steering, energy savings) give rise to hard optimization problems. What
is more, high spatio-temporal traffic variability coupled with the need to
satisfy strict per slice/service SLAs in modern networks, suggest that these
problems must be constantly (re-)solved, to maintain close-to-optimal
performance. To this end, in this paper we propose the framework of Online
Network Optimization (ONO), which seeks to maintain both agile and efficient
control over time, using an arsenal of data-driven, adaptive, and AI-based
techniques. Since the mathematical tools and the studied regimes vary widely
among these methodologies, a theoretical comparison is often out of reach.
Therefore, the important question &quot;what is the right ONO technique?&quot; remains
open to date. In this paper, we discuss the pros and cons of each technique and
further attempt a direct quantitative comparison for a specific use case, using
real data. Our results suggest that carefully combining the insights of problem
modeling with state-of-the-art AI techniques provides significant advantages at
reasonable complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vigneri_L/0/1/0/all/0/1&quot;&gt;Luigi Vigneri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liakopoulos_N/0/1/0/all/0/1&quot;&gt;Nikolaos Liakopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paschos_G/0/1/0/all/0/1&quot;&gt;Georgios S. Paschos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vassilaras_S/0/1/0/all/0/1&quot;&gt;Spyridon Vassilaras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Destounis_A/0/1/0/all/0/1&quot;&gt;Apostolos Destounis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spyropoulos_T/0/1/0/all/0/1&quot;&gt;Thrasyvoulos Spyropoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Debbah_M/0/1/0/all/0/1&quot;&gt;Merouane Debbah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12114">
<title>Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models. (arXiv:1805.12114v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12114</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-based reinforcement learning (RL) algorithms can attain excellent
sample efficiency, but often lag behind the best model-free algorithms in terms
of asymptotic performance, especially those with high-capacity parametric
function approximators, such as deep networks. In this paper, we study how to
bridge this gap, by employing uncertainty-aware dynamics models. We propose a
new algorithm called probabilistic ensembles with trajectory sampling (PETS)
that combines uncertainty-aware deep network dynamics models with
sampling-based uncertainty propagation. Our comparison to state-of-the-art
model-based and model-free deep RL algorithms shows that our approach matches
the asymptotic performance of model-free algorithms on several challenging
benchmark tasks, while requiring significantly fewer samples (e.g. 25 and 125
times fewer samples than Soft Actor Critic and Proximal Policy Optimization
respectively on the half-cheetah task).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_K/0/1/0/all/0/1&quot;&gt;Kurtland Chua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1&quot;&gt;Roberto Calandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAllister_R/0/1/0/all/0/1&quot;&gt;Rowan McAllister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12115">
<title>Amnestic Forgery: an Ontology of Conceptual Metaphors. (arXiv:1805.12115v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.12115</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Amnestic Forgery, an ontology for metaphor semantics,
based on MetaNet, which is inspired by the theory of Conceptual Metaphor.
Amnestic Forgery reuses and extends the Framester schema, as an ideal ontology
design framework to deal with both semiotic and referential aspects of frames,
roles, mappings, and eventually blending. The description of the resource is
supplied by a discussion of its applications, with examples taken from metaphor
generation, and the referential problems of metaphoric mappings. Both schema
and data are available from the Framester SPARQL endpoint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gangemi_A/0/1/0/all/0/1&quot;&gt;Aldo Gangemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Mehwish Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Presutti_V/0/1/0/all/0/1&quot;&gt;Valentina Presutti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.02262">
<title>InfoVAE: Information Maximizing Variational Autoencoders. (arXiv:1706.02262v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.02262</link>
<description rdf:parseType="Literal">&lt;p&gt;A key advance in learning generative models is the use of amortized inference
distributions that are jointly trained with the models. We find that existing
training objectives for variational autoencoders can lead to inaccurate
amortized inference distributions and, in some cases, improving the objective
provably degrades the inference quality. In addition, it has been observed that
variational autoencoders tend to ignore the latent variables when combined with
a decoding distribution that is too flexible. We again identify the cause in
existing training criteria and propose a new class of objectives (InfoVAE) that
mitigate these problems. We show that our model can significantly improve the
quality of the variational posterior and can make effective use of the latent
features regardless of the flexibility of the decoding distribution. Through
extensive qualitative and quantitative analyses, we demonstrate that our models
outperform competing approaches on multiple performance metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shengjia Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jiaming Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.08146">
<title>Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data. (arXiv:1706.08146v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.08146</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the question of accurately and efficiently computing low-rank
matrix or tensor factorizations given data compressed via random projections.
This problem arises naturally in the many settings in which data is acquired
via compressive sensing. We examine the approach of first performing
factorization in the compressed domain, and then reconstructing the original
high-dimensional factors from the recovered (compressed) factors. In both the
tensor and matrix settings, we establish conditions under which this natural
approach will provably recover the original factors. We support these
theoretical results with experiments on synthetic data and demonstrate the
practical applicability of our methods on real-world gene expression and EEG
time series data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharan_V/0/1/0/all/0/1&quot;&gt;Vatsal Sharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_K/0/1/0/all/0/1&quot;&gt;Kai Sheng Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailis_P/0/1/0/all/0/1&quot;&gt;Peter Bailis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1&quot;&gt;Gregory Valiant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05509">
<title>Note on Representing attribute reduction and concepts in concepts lattice using graphs. (arXiv:1711.05509v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05509</link>
<description rdf:parseType="Literal">&lt;p&gt;Mao H. (2017, Representing attribute reduction and concepts in concept
lattice using graphs. Soft Computing 21(24):7293--7311) claims to make
contributions to the study of reduction of attributes in concept lattices by
using graph theory. We show that her results are either trivial or already
well-known and all three algorithms proposed in the paper are incorrect.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konecny_J/0/1/0/all/0/1&quot;&gt;Jan Konecny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02025">
<title>Robot Localisation and 3D Position Estimation Using a Free-Moving Camera and Cascaded Convolutional Neural Networks. (arXiv:1801.02025v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02025</link>
<description rdf:parseType="Literal">&lt;p&gt;Many works in collaborative robotics and human-robot interaction focuses on
identifying and predicting human behaviour while considering the information
about the robot itself as given. This can be the case when sensors and the
robot are calibrated in relation to each other and often the reconfiguration of
the system is not possible, or extra manual work is required. We present a deep
learning based approach to remove the constraint of having the need for the
robot and the vision sensor to be fixed and calibrated in relation to each
other. The system learns the visual cues of the robot body and is able to
localise it, as well as estimate the position of robot joints in 3D space by
just using a 2D color image. The method uses a cascaded convolutional neural
network, and we present the structure of the network, describe our own
collected dataset, explain the network training and achieved results. A fully
trained system shows promising results in providing an accurate mask of where
the robot is located and a good estimate of its joints positions in 3D. The
accuracy is not good enough for visual servoing applications yet, however, it
can be sufficient for general safety and some collaborative tasks not requiring
very high precision. The main benefit of our method is the possibility of the
vision sensor to move freely. This allows it to be mounted on moving objects,
for example, a body of the person or a mobile robot working in the same
environment as the robots are operating in.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miseikis_J/0/1/0/all/0/1&quot;&gt;Justinas Miseikis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knobelreiter_P/0/1/0/all/0/1&quot;&gt;Patrick Knobelreiter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brijacak_I/0/1/0/all/0/1&quot;&gt;Inka Brijacak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahyanejad_S/0/1/0/all/0/1&quot;&gt;Saeed Yahyanejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glette_K/0/1/0/all/0/1&quot;&gt;Kyrre Glette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elle_O/0/1/0/all/0/1&quot;&gt;Ole Jakob Elle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torresen_J/0/1/0/all/0/1&quot;&gt;Jim Torresen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06891">
<title>Fourier Policy Gradients. (arXiv:1802.06891v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06891</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new way of deriving policy gradient updates for reinforcement
learning. Our technique, based on Fourier analysis, recasts integrals that
arise with expected policy gradients as convolutions and turns them into
multiplications. The obtained analytical solutions allow us to capture the low
variance benefits of EPG in a broad range of settings. For the critic, we treat
trigonometric and radial basis functions, two function families with the
universal approximation property. The choice of policy can be almost arbitrary,
including mixtures or hybrid continuous-discrete probability distributions.
Moreover, we derive a general family of sample-based estimators for stochastic
policy gradients, which unifies existing results on sample-based approximation.
We believe that this technique has the potential to shape the next generation
of policy gradient approaches, powered by analytical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fellows_M/0/1/0/all/0/1&quot;&gt;Matthew Fellows&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ciosek_K/0/1/0/all/0/1&quot;&gt;Kamil Ciosek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11122">
<title>Differentiable Particle Filters: End-to-End Learning with Algorithmic Priors. (arXiv:1805.11122v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11122</link>
<description rdf:parseType="Literal">&lt;p&gt;We present differentiable particle filters (DPFs): a differentiable
implementation of the particle filter algorithm with learnable motion and
measurement models. Since DPFs are end-to-end differentiable, we can
efficiently train their models by optimizing end-to-end state estimation
performance, rather than proxy objectives such as model accuracy. DPFs encode
the structure of recursive state estimation with prediction and measurement
update that operate on a probability distribution over states. This structure
represents an algorithmic prior that improves learning performance in state
estimation problems while enabling explainability of the learned model. Our
experiments on simulated and real data show substantial benefits from end-to-
end learning with algorithmic priors, e.g. reducing error rates by ~80%. Our
experiments also show that, unlike long short-term memory networks, DPFs learn
localization in a policy-agnostic way and thus greatly improve generalization.
Source code is available at
https://github.com/tu-rbo/differentiable-particle-filters .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jonschkowski_R/0/1/0/all/0/1&quot;&gt;Rico Jonschkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastogi_D/0/1/0/all/0/1&quot;&gt;Divyam Rastogi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brock_O/0/1/0/all/0/1&quot;&gt;Oliver Brock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11640">
<title>K-Beam Subgradient Descent for Minimax Optimization. (arXiv:1805.11640v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.11640</link>
<description rdf:parseType="Literal">&lt;p&gt;Minimax optimization plays a key role in adversarial training of machine
learning algorithms, such as learning generative models, domain adaptation,
privacy preservation, and robust learning. In this paper, we demonstrate the
failure of alternating gradient descent in minimax optimization problems due to
the discontinuity of solutions of the inner maximization. To address this, we
propose a new epsilon-subgradient descent algorithm that addresses this problem
by simultaneously tracking K candidate solutions. Practically, the algorithm
can find solutions that previous saddle-point algorithms cannot find, with only
a sublinear increase of complexity in K. We analyze the conditions under which
the algorithm converges to the true solution in detail. A significant
improvement in stability and convergence speed of the algorithm is observed in
simple representative problems, GAN training, and domain-adaptation problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamm_J/0/1/0/all/0/1&quot;&gt;Jihun Hamm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noh_Y/0/1/0/all/0/1&quot;&gt;Yung-Kyun Noh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11643">
<title>High Dimensional Robust Sparse Regression. (arXiv:1805.11643v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.11643</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide a novel -- and to the best of our knowledge, the first --
algorithm for high dimensional sparse regression with corruptions in
explanatory and/or response variables. Our algorithm recovers the true sparse
parameters in the presence of a constant fraction of arbitrary corruptions. Our
main contribution is a robust variant of Iterative Hard Thresholding. Using
this, we provide accurate estimators with sub-linear sample complexity. Our
algorithm consists of a novel randomized outlier removal technique for robust
sparse mean estimation that may be of interest in its own right: it is
orderwise more efficient computationally than existing algorithms, and succeeds
with high probability, thus making it suitable for general use in iterative
algorithms. We demonstrate the effectiveness on large-scale sparse regression
problems with arbitrary corruptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yanyao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caramanis_C/0/1/0/all/0/1&quot;&gt;Constantine Caramanis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11659">
<title>A Unified Particle-Optimization Framework for Scalable Bayesian Sampling. (arXiv:1805.11659v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11659</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been recent interest in developing scalable Bayesian sampling
methods for big-data analysis, such as stochastic gradient MCMC (SG-MCMC) and
Stein variational gradient descent (SVGD). A standard SG-MCMC algorithm
simulates samples from a discrete-time Markov chain to approximate a target
distribution, thus samples could be highly correlated, an undesired property
for SG-MCMC. In contrary, SVGD directly optimizes a set of particles to
approximate a target distribution, and thus is able to obtain good approximate
with relatively much fewer samples. In this paper, we propose a principle
particle-optimization framework based on Wasserstein gradient flows to unify
SG-MCMC and SVGD, and to allow new algorithms to be developed. Our framework
interprets SG-MCMC as particle optimization, revealing strong connections
between SG-MCMC and SVGD. The key component of our framework is several
particle-approximate techniques to efficiently solve the original partial
differential equations on the space of probability measures. Extensive
experiments on both synthetic data and deep neural networks demonstrate the
effectiveness and efficiency of our framework for scalable Bayesian sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changyou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenlin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liqun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11686">
<title>Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition. (arXiv:1805.11686v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.11686</link>
<description rdf:parseType="Literal">&lt;p&gt;The design of a reward function often poses a major practical challenge to
real-world applications of reinforcement learning. Approaches such as inverse
reinforcement learning attempt to overcome this challenge, but require expert
demonstrations, which can be difficult or expensive to obtain in practice. We
propose variational inverse control with events (VICE), which generalizes
inverse reinforcement learning methods to cases where full demonstrations are
not needed, such as when only samples of desired goal states are available. Our
method is grounded in an alternative perspective on control and reinforcement
learning, where an agent&apos;s goal is to maximize the probability that one or more
events will happen at some point in the future, rather than maximizing
cumulative rewards. We demonstrate the effectiveness of our methods on
continuous control tasks, with a focus on high-dimensional observations like
images where rewards are hard or even impossible to specify.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Justin Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Avi Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1&quot;&gt;Dibya Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Larry Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11712">
<title>A Novel Multi-clustering Method for Hierarchical Clusterings, Based on Boosting. (arXiv:1805.11712v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.11712</link>
<description rdf:parseType="Literal">&lt;p&gt;Bagging and boosting are proved to be the best methods of building multiple
classifiers in classification combination problems. In the area of &quot;flat
clustering&quot; problems, it is also recognized that multi-clustering methods based
on boosting provide clusterings of an improved quality. In this paper, we
introduce a novel multi-clustering method for &quot;hierarchical clusterings&quot; based
on boosting theory, which creates a more stable hierarchical clustering of a
dataset. The proposed algorithm includes a boosting iteration in which a
bootstrap of samples is created by weighted random sampling of elements from
the original dataset. A hierarchical clustering algorithm is then applied to
selected subsample to build a dendrogram which describes the hierarchy.
Finally, dissimilarity description matrices of multiple dendrogram results are
combined to a consensus one, using a hierarchical-clustering-combination
approach. Experiments on real popular datasets show that boosted method
provides superior quality solutions compared to standard hierarchical
clustering methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashedi_E/0/1/0/all/0/1&quot;&gt;Elaheh Rashedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirzaei_A/0/1/0/all/0/1&quot;&gt;Abdolreza Mirzaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11754">
<title>Optimal Testing in the Experiment-rich Regime. (arXiv:1805.11754v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11754</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the widespread adoption of large-scale A/B testing in industry,
we propose a new experimentation framework for the setting where potential
experiments are abundant (i.e., many hypotheses are available to test), and
observations are costly; we refer to this as the experiment-rich regime. Such
scenarios require the experimenter to internalize the opportunity cost of
assigning a sample to a particular experiment. We fully characterize the
optimal policy and give an algorithm to compute it. Furthermore, we develop a
simple heuristic that also provides intuition for the optimal policy. We use
simulations based on real data to compare both the optimal algorithm and the
heuristic to other natural alternative experimental design frameworks. In
particular, we discuss the paradox of power: high-powered classical tests can
lead to highly inefficient sampling in the experiment-rich regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmit_S/0/1/0/all/0/1&quot;&gt;Sven Schmit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shah_V/0/1/0/all/0/1&quot;&gt;Virag Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Johari_R/0/1/0/all/0/1&quot;&gt;Ramesh Johari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11769">
<title>Fast Incremental von Neumann Graph Entropy Computation: Theory, Algorithm, and Applications. (arXiv:1805.11769v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11769</link>
<description rdf:parseType="Literal">&lt;p&gt;The von Neumann graph entropy (VNGE) facilitates the measure of information
divergence and distance between graphs in a graph sequence and has successfully
been applied to various network learning tasks. Albeit its effectiveness, it is
computationally demanding by requiring the full eigenspectrum of the graph
Laplacian matrix. In this paper, we propose a Fast Incremental von Neumann
Graph EntRopy (FINGER) framework, which approaches VNGE with a performance
guarantee. FINGER reduces the cubic complexity of VNGE to linear complexity in
the number of nodes and edges, and thus enables online computation based on
incremental graph changes. We also show asymptotic consistency of FINGER to the
exact VNGE, and derive its approximation error bounds. Based on FINGER, we
propose ultra-efficient algorithms for computing Jensen-Shannon distance
between graphs. Our experimental results on different random graph models
demonstrate the computational efficiency and the asymptotic consistency of
FINGER. In addition, we also apply FINGER to two real-world applications and
one synthesized dataset, and corroborate its superior performance over seven
baseline graph similarity methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lingfei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rajapakse_I/0/1/0/all/0/1&quot;&gt;Indika Rajapakse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11783">
<title>To Trust Or Not To Trust A Classifier. (arXiv:1805.11783v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11783</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowing when a classifier&apos;s prediction can be trusted is useful in many
applications and critical for safely using AI. While the bulk of the effort in
machine learning research has been towards improving classifier performance,
understanding when a classifier&apos;s predictions should and should not be trusted
has received far less attention. The standard approach is to use the
classifier&apos;s discriminant or confidence score; however, we show there exists a
considerably more effective alternative. We propose a new score, called the
trust score, which measures the agreement between the classifier and a modified
nearest-neighbor classifier on the testing example. We show empirically that
high (low) trust scores produce surprisingly high precision at identifying
correctly (incorrectly) classified examples, consistently outperforming the
classifier&apos;s confidence score as well as many other baselines. Further, under
some mild distributional assumptions, we show that if the trust score for an
example is high (low), the classifier will likely agree (disagree) with the
Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of
statistical consistency under various nonparametric settings and build on
recent developments in topological data analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Heinrich Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Been Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Maya Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11792">
<title>Tight Regret Bounds for Bayesian Optimization in One Dimension. (arXiv:1805.11792v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11792</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of Bayesian optimization (BO) in one dimension, under
a Gaussian process prior and Gaussian sampling noise. We provide a theoretical
analysis showing that, under fairly mild technical assumptions on the kernel,
the best possible cumulative regret up to time $T$ behaves as
$\Omega(\sqrt{T})$ and $O(\sqrt{T\log T})$. This gives a tight characterization
up to a $\sqrt{\log T}$ factor, and includes the first non-trivial lower bound
for noisy BO. Our assumptions are satisfied, for example, by the squared
exponential and Mat\&apos;ern-$\nu$ kernels, with the latter requiring $\nu &amp;gt; 2$.
Our results certify the near-optimality of existing bounds (Srinivas {\em et
al.}, 2009) for the SE kernel, while proving them to be strictly suboptimal for
the Mat\&apos;ern kernel with $\nu &amp;gt; 2$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scarlett_J/0/1/0/all/0/1&quot;&gt;Jonathan Scarlett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11793">
<title>Infinite Arms Bandit: Optimality via Confidence Bounds. (arXiv:1805.11793v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11793</link>
<description rdf:parseType="Literal">&lt;p&gt;The infinite arms bandit problem was initiated by Berry et al. (1997). They
derived a regret lower bound of all solutions for Bernoulli rewards, and
proposed various bandit strategies based on success runs, but which do not
achieve this bound. We propose here a confidence bound target (CBT) algorithm
that achieves extensions of their regret lower bound for general reward
distributions and distribution priors. The algorithm does not require
information on the reward distributions, for each arm we require only the mean
and standard deviation of its rewards to compute a confidence bound. We play
the arm with the smallest confidence bound provided it is smaller than a target
mean. If the confidence bounds are all larger, then we play a new arm. We show
how the target mean can be computed from the prior so that the smallest
asymptotic regret, among all infinite arms bandit algorithms, is achieved. We
also show that in the absence of information on the prior, the target mean can
be determined empirically, and that the regret achieved is comparable to the
smallest regret. Numerical studies show that CBT is versatile and outperforms
its competitors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chan_H/0/1/0/all/0/1&quot;&gt;Hock Peng Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shouri Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11811">
<title>Stochastic Zeroth-order Optimization via Variance Reduction method. (arXiv:1805.11811v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11811</link>
<description rdf:parseType="Literal">&lt;p&gt;Derivative-free optimization has become an important technique used in
machine learning for optimizing black-box models. To conduct updates without
explicitly computing gradient, most current approaches iteratively sample a
random search direction from Gaussian distribution and compute the estimated
gradient along that direction. However, due to the variance in the search
direction, the convergence rates and query complexities of existing methods
suffer from a factor of $d$, where $d$ is the problem dimension. In this paper,
we introduce a novel Stochastic Zeroth-order method with Variance Reduction
under Gaussian smoothing (SZVR-G) and establish the complexity for optimizing
non-convex problems. With variance reduction on both sample space and search
space, the complexity of our algorithm is sublinear to $d$ and is strictly
better than current approaches, in both smooth and non-smooth cases. Moreover,
we extend the proposed method to the mini-batch version. Our experimental
results demonstrate the superior performance of the proposed method over
existing derivative-free optimization techniques. Furthermore, we successfully
apply our method to conduct a universal black-box attack to deep neural
networks and present some interesting results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Minhao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11845">
<title>An Information-Theoretic Analysis of Thompson Sampling for Large Action Spaces. (arXiv:1805.11845v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11845</link>
<description rdf:parseType="Literal">&lt;p&gt;Information-theoretic Bayesian regret bounds of Russo and Van Roy capture the
dependence of regret on prior uncertainty. However, this dependence is through
entropy, which can become arbitrarily large as the number of actions increases.
We establish new bounds that depend instead on a notion of rate-distortion.
Among other things, this allows us to recover through information-theoretic
arguments a near-optimal bound for the linear bandit. We also offer a bound for
the logistic bandit that dramatically improves on the best previously
available, though this bound depends on an information-theoretic statistic that
we have only been able to quantify via computation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Shi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roy_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11897">
<title>Differential Properties of Sinkhorn Approximation for Learning with Wasserstein Distance. (arXiv:1805.11897v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11897</link>
<description rdf:parseType="Literal">&lt;p&gt;Applications of optimal transport have recently gained remarkable attention
thanks to the computational advantages of entropic regularization. However, in
most situations the Sinkhorn approximation of the Wasserstein distance is
replaced by a regularized version that is less accurate but easy to
differentiate. In this work we characterize the differential properties of the
original Sinkhorn distance, proving that it enjoys the same smoothness as its
regularized version and we explicitly provide an efficient algorithm to compute
its gradient. We show that this result benefits both theory and applications:
on one hand, high order smoothness confers statistical guarantees to learning
with Wasserstein approximations. On the other hand, the gradient formula allows
us to efficiently solve learning and optimization problems in practice.
Promising preliminary experiments complement our analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luise_G/0/1/0/all/0/1&quot;&gt;Giulia Luise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1&quot;&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pontil_M/0/1/0/all/0/1&quot;&gt;Massimiliano Pontil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ciliberto_C/0/1/0/all/0/1&quot;&gt;Carlo Ciliberto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11908">
<title>Who Learns Better Bayesian Network Structures: Constraint-Based, Score-based or Hybrid Algorithms?. (arXiv:1805.11908v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1805.11908</link>
<description rdf:parseType="Literal">&lt;p&gt;The literature groups algorithms to learn the structure of Bayesian networks
from data in three separate classes: constraint-based algorithms, which use
conditional independence tests to learn the dependence structure of the data;
score-based algorithms, which use goodness-of-fit scores as objective functions
to maximise; and hybrid algorithms that combine both approaches. Famously,
Cowell (2001) showed that algorithms in the first two classes learn the same
structures when the topological ordering of the network is known and we use
entropy to assess conditional independence and goodness of fit.
&lt;/p&gt;
&lt;p&gt;In this paper we address the complementary question: how do these classes of
algorithms perform outside of the assumptions above? We approach this question
by recognising that structure learning is defined by the combination of a
statistical criterion and an algorithm that determines how the criterion is
applied to the data. Removing the confounding effect of different choices for
the statistical criterion, we find using both simulated and real-world data
that constraint-based algorithms do not appear to be more efficient or more
sensitive to errors than score-based algorithms; and that hybrid algorithms are
not faster or more accurate than constraint-based algorithms. This suggests
that commonly held beliefs on structure learning in the literature are strongly
influenced by the choice of particular statistical criteria rather than just
properties of the algorithms themselves.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scutari_M/0/1/0/all/0/1&quot;&gt;Marco Scutari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Graafland_C/0/1/0/all/0/1&quot;&gt;Catharina Elisabeth Graafland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gutierrez_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Manuel Guti&amp;#xe9;rrez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11954">
<title>Long Short-Term Memory Networks for CSI300 Volatility Prediction with Baidu Search Volume. (arXiv:1805.11954v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/1805.11954</link>
<description rdf:parseType="Literal">&lt;p&gt;Intense volatility in financial markets affect humans worldwide. Therefore,
relatively accurate prediction of volatility is critical. We suggest that
massive data sources resulting from human interaction with the Internet may
offer a new perspective on the behavior of market participants in periods of
large market movements. First we select 28 key words, which are related to
finance as indicators of the public mood and macroeconomic factors. Then those
28 words of the daily search volume based on Baidu index are collected
manually, from June 1, 2006 to October 29, 2017. We apply a Long Short-Term
Memory neural network to forecast CSI300 volatility using those search volume
data. Compared to the benchmark GARCH model, our forecast is more accurate,
which demonstrates the effectiveness of the LSTM neural network in volatility
forecasting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yu-Long Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Han_R/0/1/0/all/0/1&quot;&gt;Ren-Jie Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei-Ke Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11956">
<title>Short-term Load Forecasting with Deep Residual Networks. (arXiv:1805.11956v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11956</link>
<description rdf:parseType="Literal">&lt;p&gt;We present in this paper a model for forecasting short-term power loads based
on deep residual networks. The proposed model is able to integrate domain
knowledge and researchers&apos; understanding of the task by virtue of different
neural network building blocks. Specifically, a modified deep residual network
is formulated to improve the forecast results. Further, a two-stage ensemble
strategy is used to enhance the generalization capability of the proposed
model. We also apply the proposed model to probabilistic load forecasting using
Monte Carlo dropout. Three public datasets are used to prove the effectiveness
of the proposed model. Multiple test cases and comparison with existing models
show that the proposed model is able to provide accurate load forecasting
results and has high generalization capability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kunjin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kunlong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Ziyu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jinliang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11959">
<title>Algebraic Expression of Spatial and Temporal Pattern. (arXiv:1805.11959v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.11959</link>
<description rdf:parseType="Literal">&lt;p&gt;Universal learning machine is a theory trying to study machine learning from
mathematical point of view. The outside world is reflected inside an universal
learning machine according to pattern of incoming data. So, machine will have
its subjective view and such subjective view is adapting/learning. In
\cite{paper2, cpaper}, we discussed subjective spatial pattern, and established
a powerful tool -- X-form, which is an algebraic expression for subjective
spatial pattern. However, as the initial stage of study, we only discussed
spatial pattern there and leave temporal pattern for later study. Here, we will
discuss spatial and temporal patterns, and establish algebraic expression for
subjective patterns, spatial and temporal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Chuyu Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11973">
<title>MolGAN: An implicit generative model for small molecular graphs. (arXiv:1805.11973v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11973</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models for graph-structured data offer a new angle on the
problem of chemical synthesis: by optimizing differentiable models that
directly generate molecular graphs, it is possible to side-step expensive
search procedures in the discrete and vast space of chemical structures. We
introduce MolGAN, an implicit, likelihood-free generative model for small
molecular graphs that circumvents the need for expensive graph matching
procedures or node ordering heuristics of previous likelihood-based methods.
Our method adapts generative adversarial networks (GANs) to operate directly on
graph-structured data. We combine our approach with a reinforcement learning
objective to encourage the generation of molecules with specific desired
chemical properties. In experiments on the QM9 chemical database, we
demonstrate that our model is capable of generating close to 100% valid
compounds. MolGAN compares favorably both to recent proposals that use
string-based (SMILES) representations of molecules and to a likelihood-based
method that directly generates graphs, albeit being susceptible to mode
collapse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cao_N/0/1/0/all/0/1&quot;&gt;Nicola De Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kipf_T/0/1/0/all/0/1&quot;&gt;Thomas Kipf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12002">
<title>Why Is My Classifier Discriminatory?. (arXiv:1805.12002v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.12002</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent attempts to achieve fairness in predictive models focus on the balance
between fairness and accuracy. In sensitive applications such as healthcare or
criminal justice, this trade-off is often undesirable as any increase in
prediction error could have devastating consequences. In this work, we argue
that the fairness of predictions should be evaluated in context of the data,
and that unfairness induced by inadequate samples sizes or unmeasured
predictive variables should be addressed through data collection, rather than
by constraining the model. We decompose cost-based metrics of discrimination
into bias, variance, and noise, and propose actions aimed at estimating and
reducing each term. Finally, we perform case-studies on prediction of income,
mortality, and review ratings, confirming the value of this analysis. We find
that data collection is often a means to reduce discrimination without
sacrificing accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_I/0/1/0/all/0/1&quot;&gt;Irene Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Johansson_F/0/1/0/all/0/1&quot;&gt;Fredrik D. Johansson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sontag_D/0/1/0/all/0/1&quot;&gt;David Sontag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12021">
<title>Towards Adversarial Configurations for Software Product Lines. (arXiv:1805.12021v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12021</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring that all supposedly valid configurations of a software product line
(SPL) lead to well-formed and acceptable products is challenging since it is
most of the time impractical to enumerate and test all individual products of
an SPL. Machine learning classifiers have been recently used to predict the
acceptability of products associated with unseen configurations. For some
configurations, a tiny change in their feature values can make them pass from
acceptable to non-acceptable regarding users&apos; requirements and vice-versa. In
this paper, we introduce the idea of leveraging these specific configurations
and their positions in the feature space to improve the classifier and
therefore the engineering of an SPL. Starting from a variability model, we
propose to use Adversarial Machine Learning techniques to create new,
adversarial configurations out of already known configurations by modifying
their feature values. Using an industrial video generator we show how
adversarial configurations can improve not only the classifier, but also the
variability model, the variability implementation, and the testing oracle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Temple_P/0/1/0/all/0/1&quot;&gt;Paul Temple&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acher_M/0/1/0/all/0/1&quot;&gt;Mathieu Acher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1&quot;&gt;Battista Biggio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jezequel_J/0/1/0/all/0/1&quot;&gt;Jean-Marc J&amp;#xe9;z&amp;#xe9;quel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1&quot;&gt;Fabio Roli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12044">
<title>Predicting County Level Corn Yields Using Deep Long Short Term Memory Models. (arXiv:1805.12044v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.12044</link>
<description rdf:parseType="Literal">&lt;p&gt;Corn yield prediction is beneficial as it provides valuable information about
production and prices prior the harvest. Publicly available high-quality corn
yield prediction can help address emergent information asymmetry problems and
in doing so improve price efficiency in futures markets. This paper is the
first to employ Long Short-Term Memory (LSTM), a special form of Recurrent
Neural Network (RNN) method to predict corn yields. A cross sectional time
series of county-level corn yield and hourly weather data made the sample space
large enough to use deep learning technics. LSTM is efficient in time series
prediction with complex inner relations, which makes it suitable for this task.
The empirical results from county level data in Iowa show promising predictive
power relative to existing survey based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zehui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hendricks_N/0/1/0/all/0/1&quot;&gt;Nathan P. Hendricks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ganapathysubramanian_B/0/1/0/all/0/1&quot;&gt;Baskar Ganapathysubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hayes_D/0/1/0/all/0/1&quot;&gt;Dermot J. Hayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumik Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12062">
<title>Regularized Kernel and Neural Sobolev Descent: Dynamic MMD Transport. (arXiv:1805.12062v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12062</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Regularized Kernel and Neural Sobolev Descent for transporting a
source distribution to a target distribution along smooth paths of minimum
kinetic energy (defined by the Sobolev discrepancy), related to dynamic optimal
transport. In the kernel version, we give a simple algorithm to perform the
descent along gradients of the Sobolev critic, and show that it converges
asymptotically to the target distribution in the MMD sense. In the neural
version, we parametrize the Sobolev critic with a neural network with input
gradient norm constrained in expectation. We show in theory and experiments
that regularization has an important role in favoring smooth transitions
between distributions, avoiding large discrete jumps. Our analysis could
provide a new perspective on the impact of critic updates (early stopping) on
the paths to equilibrium in the GAN setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1&quot;&gt;Youssef Mroueh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sercu_T/0/1/0/all/0/1&quot;&gt;Tom Sercu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Anant Raj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12111">
<title>Dynamic Advisor-Based Ensemble (dynABE): Case Study in Stock Trend Prediction of a Major Critical Metal Producer. (arXiv:1805.12111v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/1805.12111</link>
<description rdf:parseType="Literal">&lt;p&gt;The demand of metals by modern technology has been shifting from common base
metals to a variety of minor metals, such as cobalt or indium. The industrial
importance and limited geological availability of some minor metals have led to
them being considered more &quot;critical,&quot; and there is a growing interest in such
critical metals and their producing companies. In this research, we create a
novel framework, Dynamic Advisor-Based Ensemble (dynABE), to predict the stock
trend of major critical metal producers. Specifically, dynABE first utilizes
domain knowledge to group the features into different &quot;advisors,&quot; each advisor
dealing with a particular economic sector. Then through ensembles of weak
classifiers, each advisor produces a prediction result, and all the advisors
are combined again in a biased online update fashion to dynamically make the
final prediction. Based on a misclassification error of 32% for Jinchuan
Group&apos;s stock (HKG: 2362), we further test a simple stock trading strategy,
which leads to a back-tested return of 296%, or an excess return of 130% within
one year. In addition, the feature set selected by dynABE also suggests
potentially influential factors to metal criticality, because stock prices of
major producers influence metal production. Therefore, not only does this
research propose a novel framework for specialized stock trend prediction, it
also provides domain insights into dynamic features that potentially influence
metal criticality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12120">
<title>On Consensus-Optimality Trade-offs in Collaborative Deep Learning. (arXiv:1805.12120v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.12120</link>
<description rdf:parseType="Literal">&lt;p&gt;In distributed machine learning, where agents collaboratively learn from
diverse private data sets, there is a fundamental tension between consensus and
optimality. In this paper, we build on recent algorithmic progresses in
distributed deep learning to explore various consensus-optimality trade-offs
over a fixed communication topology. First, we propose the incremental
consensus-based distributed SGD (i-CDSGD) algorithm, which involves multiple
consensus steps (where each agent communicates information with its neighbors)
within each SGD iteration. Second, we propose the generalized consensus-based
distributed SGD (g-CDSGD) algorithm that enables us to navigate the full
spectrum from complete consensus (all agents agree) to complete disagreement
(each agent converges to individual model parameters). We analytically
establish convergence of the proposed algorithms for strongly convex and
nonconvex objective functions; we also analyze the momentum variants of the
algorithms for the strongly convex case. We support our algorithms via
numerical experiments, and demonstrate significant improvements over existing
methods for collaborative deep learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhanhong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balu_A/0/1/0/all/0/1&quot;&gt;Aditya Balu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hegde_C/0/1/0/all/0/1&quot;&gt;Chinmay Hegde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumik Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.04898">
<title>Efficient Distributed Semi-Supervised Learning using Stochastic Regularization over Affinity Graphs. (arXiv:1612.04898v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1612.04898</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a computationally efficient, stochastic graph-regularization
technique that can be utilized for the semi-supervised training of deep neural
networks in a parallel or distributed setting. We utilize a technique, first
described in [13] for the construction of mini-batches for stochastic gradient
descent (SGD) based on synthesized partitions of an affinity graph that are
consistent with the graph structure, but also preserve enough stochasticity for
convergence of SGD to good local minima. We show how our technique allows a
graph-based semi-supervised loss function to be decomposed into a sum over
objectives, facilitating data parallelism for scalable training of machine
learning models. Empirical results indicate that our method significantly
improves classification accuracy compared to the fully-supervised case when the
fraction of labeled data is low, and in the parallel case, achieves significant
speed-up in terms of wall-clock time to convergence. We show the results for
both sequential and distributed-memory semi-supervised DNN training on a speech
corpus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thulasidasan_S/0/1/0/all/0/1&quot;&gt;Sunil Thulasidasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bilmes_J/0/1/0/all/0/1&quot;&gt;Jeffrey Bilmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kenyon_G/0/1/0/all/0/1&quot;&gt;Garrett Kenyon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.03522">
<title>On Consistency of Compressive Spectral Clustering. (arXiv:1702.03522v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1702.03522</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral clustering is one of the most popular methods for community
detection in graphs. A key step in spectral clustering algorithms is the eigen
decomposition of the $n{\times}n$ graph Laplacian matrix to extract its $k$
leading eigenvectors, where $k$ is the desired number of clusters among $n$
objects. This is prohibitively complex to implement for very large datasets.
However, it has recently been shown that it is possible to bypass the eigen
decomposition by computing an approximate spectral embedding through graph
filtering of random signals. In this paper, we analyze the working of spectral
clustering performed via graph filtering on the stochastic block model.
Specifically, we characterize the effects of sparsity, dimensionality and
filter approximation error on the consistency of the algorithm in recovering
planted clusters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pydi_M/0/1/0/all/0/1&quot;&gt;Muni Sreenivas Pydi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dukkipati_A/0/1/0/all/0/1&quot;&gt;Ambedkar Dukkipati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.08159">
<title>McKernel: A Library for Approximate Kernel Expansions in Log-linear Time. (arXiv:1702.08159v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1702.08159</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel Methods Next Generation (KMNG) introduces a framework to use kernel
approximates in the mini-batch setting with SGD Optimizer as an alternative to
Deep Learning. McKernel is a C++ library for KMNG ML Large-scale. It contains a
CPU optimized implementation of the Fastfood algorithm that allows the
computation of approximated kernel expansions in log-linear time. The algorithm
requires to compute the product of Walsh Hadamard Transform (WHT) matrices. A
cache friendly SIMD Fast Walsh Hadamard Transform (FWHT) that achieves
compelling speed and outperforms current state-of-the-art methods has been
developed. McKernel allows to obtain non-linear classification combining
Fastfood and a linear classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Curto_J/0/1/0/all/0/1&quot;&gt;Joachim D. Curt&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zarza_I/0/1/0/all/0/1&quot;&gt;Irene C. Zarza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Feng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1&quot;&gt;Alexander J. Smola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1&quot;&gt;Fernando De La Torre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1&quot;&gt;Chong-Wah Ngo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07474">
<title>Why are Big Data Matrices Approximately Low Rank?. (arXiv:1705.07474v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07474</link>
<description rdf:parseType="Literal">&lt;p&gt;Matrices of (approximate) low rank are pervasive in data science, appearing
in recommender systems, movie preferences, topic models, medical records, and
genomics. While there is a vast literature on how to exploit low rank structure
in these datasets, there is less attention on explaining why the low rank
structure appears in the first place. Here, we explain the effectiveness of low
rank models in data science by considering a simple generative model for these
matrices: we suppose that each row or column is associated to a (possibly high
dimensional) bounded latent variable, and entries of the matrix are generated
by applying a piecewise analytic function to these latent variables. These
matrices are in general full rank. However, we show that we can approximate
every entry of an $m \times n$ matrix drawn from this model to within a fixed
absolute error by a low rank matrix whose rank grows as $\mathcal O(\log(m +
n))$. Hence any sufficiently large matrix from such a latent variable model can
be approximated, up to a small entrywise error, by a low rank matrix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1&quot;&gt;Madeleine Udell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Townsend_A/0/1/0/all/0/1&quot;&gt;Alex Townsend&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04064">
<title>A Contextual Bandit Bake-off. (arXiv:1802.04064v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04064</link>
<description rdf:parseType="Literal">&lt;p&gt;Contextual bandit algorithms are essential for solving many real-world
interactive machine learning problems. Despite multiple recent successes on
statistically and computationally efficient methods, the practical behavior of
these algorithms is still poorly understood. We leverage the availability of
large numbers of supervised learning datasets to compare and empirically
optimize contextual bandit algorithms, focusing on practical methods that learn
by relying on optimization oracles from supervised learning. We find that a
recent method (Foster et al., 2018) using optimism under uncertainty works the
best overall. A surprisingly close second is a simple greedy baseline that only
explores implicitly through the diversity of contexts, followed by a variant of
Online Cover (Agarwal et al., 2014) which tends to be more conservative but
robust to problem specification by design. Along the way, we also evaluate and
improve several internal components of contextual bandit algorithm design.
Overall, this is a thorough study and review of contextual bandit methodology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1&quot;&gt;Alberto Bietti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Alekh Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Langford_J/0/1/0/all/0/1&quot;&gt;John Langford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05374">
<title>A Progressive Batching L-BFGS Method for Machine Learning. (arXiv:1802.05374v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05374</link>
<description rdf:parseType="Literal">&lt;p&gt;The standard L-BFGS method relies on gradient approximations that are not
dominated by noise, so that search directions are descent directions, the line
search is reliable, and quasi-Newton updating yields useful quadratic models of
the objective function. All of this appears to call for a full batch approach,
but since small batch sizes give rise to faster algorithms with better
generalization properties, L-BFGS is currently not considered an algorithm of
choice for large-scale machine learning applications. One need not, however,
choose between the two extremes represented by the full batch or highly
stochastic regimes, and may instead follow a progressive batching approach in
which the sample size increases during the course of the optimization. In this
paper, we present a new version of the L-BFGS algorithm that combines three
basic components - progressive batching, a stochastic line search, and stable
quasi-Newton updating - and that performs well on training logistic regression
and deep neural networks. We provide supporting convergence theory for the
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bollapragada_R/0/1/0/all/0/1&quot;&gt;Raghu Bollapragada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mudigere_D/0/1/0/all/0/1&quot;&gt;Dheevatsa Mudigere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nocedal_J/0/1/0/all/0/1&quot;&gt;Jorge Nocedal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hao-Jun Michael Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tang_P/0/1/0/all/0/1&quot;&gt;Ping Tak Peter Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05419">
<title>Generalised Structural CNNs (SCNNs) for time series data with arbitrary graph topology. (arXiv:1803.05419v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05419</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning methods, specifically convolutional neural networks (CNNs),
have seen a lot of success in the domain of image-based data, where the data
offers a clearly structured topology in the regular lattice of pixels. This
4-neighbourhood topological simplicity makes the application of convolutional
masks straightforward for time series data, such as video applications, but
many high-dimensional time series data are not organised in regular lattices,
and instead values may have adjacency relationships with non-trivial
topologies, such as small-world networks or trees. In our application case,
human kinematics, it is currently unclear how to generalise convolutional
kernels in a principled manner. Therefore we define and implement here a
framework for general graph-structured CNNs for time series analysis. Our
algorithm automatically builds convolutional layers using the specified
adjacency matrix of the data dimensions and convolutional masks that scale with
the hop distance. In the limit of a lattice-topology our method produces the
well-known image convolutional masks. We test our method first on synthetic
data of arbitrarily-connected graphs and human hand motion capture data, where
the hand is represented by a tree capturing the mechanical dependencies of the
joints. We are able to demonstrate, amongst other things, that inclusion of the
graph structure of the data dimensions improves model prediction significantly,
when compared against a benchmark CNN model with only time convolution layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Teh_T/0/1/0/all/0/1&quot;&gt;Thomas Teh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Auepanwiriyakul_C/0/1/0/all/0/1&quot;&gt;Chaiyawan Auepanwiriyakul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Harston_J/0/1/0/all/0/1&quot;&gt;John Alexander Harston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Faisal_A/0/1/0/all/0/1&quot;&gt;A. Aldo Faisal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01013">
<title>Resilient Non-Submodular Maximization over Matroid Constraints. (arXiv:1804.01013v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01013</link>
<description rdf:parseType="Literal">&lt;p&gt;The control and sensing of large-scale systems results in combinatorial
problems not only for sensor and actuator placement but also for scheduling or
observability/controllability. Such combinatorial constraints in system design
and implementation can be captured using a structure known as matroids. In
particular, the algebraic structure of matroids can be exploited to develop
scalable algorithms for sensor and actuator selection, along with quantifiable
approximation bounds. However, in large-scale systems, sensors and actuators
may fail or may be (cyber-)attacked. The objective of this paper is to focus on
resilient matroid-constrained problems arising in control and sensing but in
the presence of sensor and actuator failures. In general, resilient
matroid-constrained problems are computationally hard. Contrary to the
non-resilient case (with no failures), even though they often involve objective
functions that are monotone or submodular, no scalable approximation algorithms
are known for their solution. In this paper, we provide the first algorithm,
that also has the following properties: First, it achieves system-wide
resiliency, i.e., the algorithm is valid for any number of denial-of-service
attacks or failures. Second, it is scalable, as our algorithm terminates with
the same running time as state-of-the-art algorithms for (non-resilient)
matroid-constrained optimization. Third, it provides provable approximation
bounds on the system performance, since for monotone objective functions our
algorithm guarantees a solution close to the optimal. We quantify our
algorithm&apos;s approximation performance using a notion of curvature for monotone
(not necessarily submodular) set functions. Finally, we support our theoretical
analyses with numerical experiments, by considering a control-aware sensor
selection scenario, namely, sensing-constrained robot navigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tzoumas_V/0/1/0/all/0/1&quot;&gt;Vasileios Tzoumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jadbabaie_A/0/1/0/all/0/1&quot;&gt;Ali Jadbabaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pappas_G/0/1/0/all/0/1&quot;&gt;George J. Pappas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03911">
<title>Labelling as an unsupervised learning problem. (arXiv:1805.03911v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.03911</link>
<description rdf:parseType="Literal">&lt;p&gt;Unravelling hidden patterns in datasets is a classical problem with many
potential applications. In this paper, we present a challenge whose objective
is to discover nonlinear relationships in noisy cloud of points. If a set of
point satisfies a nonlinear relationship that is unlikely to be due to
randomness, we will label the set with this relationship. Since points can
satisfy one, many or no such nonlinear relationships, cloud of points will
typically have one, multiple or no labels at all. This introduces the labelling
problem that will be studied in this paper.
&lt;/p&gt;
&lt;p&gt;The objective of this paper is to develop a framework for the labelling
problem. We introduce a precise notion of a label, and we propose an algorithm
to discover such labels in a given dataset, which is then tested in synthetic
datasets. We also analyse, using tools from random matrix theory, the problem
of discovering false labels in the dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lyons_T/0/1/0/all/0/1&quot;&gt;Terry Lyons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arribas_I/0/1/0/all/0/1&quot;&gt;Imanol Perez Arribas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09965">
<title>LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning. (arXiv:1805.09965v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09965</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new class of gradient methods for distributed machine
learning that adaptively skip the gradient calculations to learn with reduced
communication and computation. Simple rules are designed to detect
slowly-varying gradients and, therefore, trigger the reuse of outdated
gradients. The resultant gradient-based algorithms are termed Lazily Aggregated
Gradient --- justifying our acronym LAG used henceforth. Theoretically, the
merits of this contribution are: i) the convergence rate is the same as batch
gradient descent in strongly-convex, convex, and nonconvex smooth cases; and,
ii) if the distributed datasets are heterogeneous (quantified by certain
measurable constants), the communication rounds needed to achieve a targeted
accuracy are reduced thanks to the adaptive reuse of lagged gradients.
Numerical experiments on both synthetic and real data corroborate a significant
communication reduction compared to alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wotao Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10724">
<title>RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records. (arXiv:1805.10724v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.10724</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past decade, we have seen many successful applications of recurrent
neural networks (RNNs) on electronic medical records (EMRs), which contain
histories of patients&apos; diagnoses, medications, and other various events, in
order to predict the current and future states of patients. Despite the strong
performance of RNNs, it is often very challenging for users to understand why
the model makes a particular prediction. Such black box nature of RNNs can
impede its wide adoption in clinical practice. Furthermore, we have no
established method to interactively leverage users&apos; domain expertise and prior
knowledge as inputs for steering the model. Therefore, our design study aims to
provide a visual analytics solution to increase interpretability and
interactivity of RNNs via a joint effort of medical experts, artificial
intelligence scientists, and visual analytics researchers. Following the
iterative design process between the experts, we design, implement, and
evaluate a visual analytics tool called RetainVis, which couples a recently
proposed, interpretable RNN-based model called RETAIN and visualizations for
users&apos; exploration of EMR data in the context of prediction tasks. Our study
shows the effective use of RetainVis for gaining insights into how RNN models
EMR data, using real medical records of patients with heart failure, cataract,
or dermatological symptoms. Our study also demonstrates how we made substantial
changes to the state-of-the-art RNN model called RETAIN in order to make use of
temporal information and increase interactivity. This study will provide a
useful guideline for researchers who aim to design more interpretable and
interactive visual analytics tool for RNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_B/0/1/0/all/0/1&quot;&gt;Bum Chul Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Min-Je Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Joanne Taery Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Edward Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Young Bin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1&quot;&gt;Soonwook Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1&quot;&gt;Jaegul Choo&lt;/a&gt;</dc:creator>
</item></rdf:RDF>