<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-09-10T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02627"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02651"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02721"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02942"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03055"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03142"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03330"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02753"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02804"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02838"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02855"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02869"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02909"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02926"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02927"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03001"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03044"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03057"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03073"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03075"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03084"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03119"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03149"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03152"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03200"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03202"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03359"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03363"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03371"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03428"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03470"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03478"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.08325"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07387"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04172"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00102"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04592"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00912"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07429"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07683"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09328"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09769"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10096"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09062"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00969"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01479"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02630"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02652"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02665"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02707"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02728"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02745"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02811"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02840"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02860"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02864"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02880"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02918"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02925"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02963"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03006"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03008"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03018"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03041"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03045"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03060"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03062"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03063"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03090"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03113"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03140"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03185"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03291"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03306"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03322"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03323"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03343"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03385"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03400"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03402"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03416"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03447"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03461"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1412.5250"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1507.04564"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.06003"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.10225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.06994"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.00095"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00961"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08363"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02781"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06118"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10846"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02396"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11454"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12547"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02543"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02970"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11470"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00020"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00510"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02188"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02292"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1809.02627">
<title>Unity: A General Platform for Intelligent Agents. (arXiv:1809.02627v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02627</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in Deep Reinforcement Learning and Robotics have been driven
by the presence of increasingly realistic and complex simulation environments.
Many of the existing platforms, however, provide either unrealistic visuals,
inaccurate physics, low task complexity, or a limited capacity for interaction
among artificial agents. Furthermore, many platforms lack the ability to
flexibly configure the simulation, hence turning the simulation environment
into a black-box from the perspective of the learning system. Here we describe
a new open source toolkit for creating and interacting with simulation
environments using the Unity platform: Unity ML-Agents Toolkit. By taking
advantage of Unity as a simulation platform, the toolkit enables the
development of learning environments which are rich in sensory and physical
complexity, provide compelling cognitive challenges, and support dynamic
multi-agent interaction. We detail the platform design, communication protocol,
set of example environments, and variety of training scenarios made possible
via the toolkit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juliani_A/0/1/0/all/0/1&quot;&gt;Arthur Juliani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berges_V/0/1/0/all/0/1&quot;&gt;Vincent-Pierre Berges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vckay_E/0/1/0/all/0/1&quot;&gt;Esh Vckay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henry_H/0/1/0/all/0/1&quot;&gt;Hunter Henry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattar_M/0/1/0/all/0/1&quot;&gt;Marwan Mattar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_D/0/1/0/all/0/1&quot;&gt;Danny Lange&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02651">
<title>Reservoir Computing based Neural Image Filters. (arXiv:1809.02651v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.02651</link>
<description rdf:parseType="Literal">&lt;p&gt;Clean images are an important requirement for machine vision systems to
recognize visual features correctly. However, the environment, optics,
electronics of the physical imaging systems can introduce extreme distortions
and noise in the acquired images. In this work, we explore the use of reservoir
computing, a dynamical neural network model inspired from biological systems,
in creating dynamic image filtering systems that extracts signal from noise
using inverse modeling. We discuss the possibility of implementing these
networks in hardware close to the sensors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganguly_S/0/1/0/all/0/1&quot;&gt;Samiran Ganguly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yunfei Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yunkun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stan_M/0/1/0/all/0/1&quot;&gt;Mircea R. Stan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Avik W. Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhar_N/0/1/0/all/0/1&quot;&gt;Nibir K. Dhar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02721">
<title>Learning to Solve NP-Complete Problems - A Graph Neural Network for the Decision TSP. (arXiv:1809.02721v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02721</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNN) are a promising technique for bridging
differential programming and combinatorial domains. GNNs employ trainable
modules which can be assembled in different configurations that reflect the
relational structure of each problem instance. In this paper, we show that GNNs
can learn to solve, with very little supervision, the decision variant of the
Traveling Salesperson Problem (TSP), a highly relevant $\mathcal{NP}$-Complete
problem. Our model is trained to function as an effective message-passing
algorithm in which edges (embedded with their weights) communicate with
vertices for a number of iterations after which the model is asked to decide
whether a route with cost $&amp;lt;C$ exists. We show that such a network can be
trained with sets of dual examples: given the optimal tour cost $C^{*}$, we
produce one decision instance with target cost $x\%$ smaller and one with
target cost $x\%$ larger than $C^{*}$. We were able to obtain $80\%$ accuracy
training with $-2\%,+2\%$ deviations, and the same trained model can generalize
for more relaxed deviations with increasing performance. We also show that the
model is capable of generalizing for larger problem sizes. Finally, we provide
a method for predicting the optimal route cost within $2\%$ deviation from the
ground truth. In summary, our work shows that Graph Neural Networks are
powerful enough to solve $\mathcal{NP}$-Complete problems which combine
symbolic and numeric data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prates_M/0/1/0/all/0/1&quot;&gt;Marcelo O. R. Prates&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avelar_P/0/1/0/all/0/1&quot;&gt;Pedro H. C. Avelar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemos_H/0/1/0/all/0/1&quot;&gt;Henrique Lemos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamb_L/0/1/0/all/0/1&quot;&gt;Luis Lamb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vardi_M/0/1/0/all/0/1&quot;&gt;Moshe Vardi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02942">
<title>Cellular automata as convolutional neural networks. (arXiv:1809.02942v1 [nlin.CG])</title>
<link>http://arxiv.org/abs/1809.02942</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning techniques have recently demonstrated broad success in
predicting complex dynamical systems ranging from turbulence to human speech,
motivating broader questions about how neural networks encode and represent
dynamical rules. We explore this problem in the context of cellular automata
(CA), simple dynamical systems that are intrinsically discrete and thus
difficult to analyze using standard tools from dynamical systems theory. We
show that any CA may readily be represented using a convolutional neural
network with a network-in-network architecture. This motivates our development
of a general convolutional multilayer perceptron architecture, which we find
can learn the dynamical rules for arbitrary CA when given videos of the CA as
training data. In the limit of large network widths, we find that training
dynamics are strongly stereotyped across replicates, and that common patterns
emerge in the structure of networks trained on different CA rulesets. We train
ensembles of networks on randomly-sampled CA, and we probe how the trained
networks internally represent the CA rules using an information-theoretic
technique based on distributions of layer activation patterns. We find that CA
with simpler rule tables produce trained networks with hierarchical structure
and layer specialization, while more complex CA tend to produce shallower
representations---illustrating how the underlying complexity of the CA&apos;s rules
influences the specificity of these internal representations. Our results
suggest how the entropy of a physical process can affect its representation
when learned by neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/nlin/1/au:+Gilpin_W/0/1/0/all/0/1&quot;&gt;William Gilpin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03055">
<title>LDW-SCSA: Logistic Dynamic Weight based Sine Cosine Search Algorithm for Numerical Functions Optimization. (arXiv:1809.03055v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1809.03055</link>
<description rdf:parseType="Literal">&lt;p&gt;Particle swarm optimization (PSO) and Sine Cosine algorithm (SCA) have been
widely used optimization methods but these methods have some disadvantages such
as trapped local optimum point. In order to solve this problem and obtain more
successful results than others, a novel logistic dynamic weight based sine
cosine search algorithm (LDW-SCSA) is presented in this paper. In the LDW-SCSA
method, logistic map is used as dynamic weight generator. Logistic map is one
of the famous and widely used chaotic map in the literature. Search process of
SCA is modified in the LDW-SCSA. To evaluate performance of the LDW-SCSA, the
widely used numerical benchmark functions were utilized as test suite and other
swarm optimization methods were used to obtain the comparison results. Superior
performances of the LDW-SCSA are proved success of this method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuncer_T/0/1/0/all/0/1&quot;&gt;Turker Tuncer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03142">
<title>Fast and Efficient Information Transmission with Burst Spikes in Deep Spiking Neural Networks. (arXiv:1809.03142v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1809.03142</link>
<description rdf:parseType="Literal">&lt;p&gt;The spiking neural networks (SNNs), the 3rd generation of neural networks,
are considered as one of the most promising artificial neural networks due to
their energy-efficient computing capability. Despite their potential, the SNNs
have a limited applicability owing to difficulties in training. Recently,
conversion of a trained deep neural network (DNN) model to an SNN model has
been extensively studied as an alternative approach. The result appears to be
comparable to that of the DNN in image classification tasks. However, rate
coding, one of the techniques used in modeling the SNNs, suffers from long
latency due to its inability to transmit sufficient information to a subsequent
neuron and this could have a catastrophic effect on a deeper SNN model. Another
type of neural coding, called phase coding, also determines the amount of
information being transmitted according to a global reference oscillator, and
therefore, is inefficient in hidden layers where dynamics of neurons can
change. In this paper, we propose a deep SNN model that can transmit
information faster, and more efficiently between neurons by adopting a notion
of burst spiking. Furthermore, we introduce a novel hybrid neural coding scheme
that uses different neural coding schemes for different types of layers. Our
experimental results for various image classification datasets, such as MNIST,
CIFAR-10 and CIFAR-100, showed that the proposed methods can improve inference
efficiency and shorten the latency while preserving high accuracy. Lastly, we
validated the proposed methods through firing pattern analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Seongsik Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seijoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1&quot;&gt;Hyeokjun Choe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sungroh Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03330">
<title>Neural Allocentric Intuitive Physics Prediction from Real Videos. (arXiv:1809.03330v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1809.03330</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans are able to make rich predictions about the future dynamics of
physical objects from a glance. On the other hand, most existing computer
vision approaches require strong assumptions about the underlying system,
ad-hoc modeling, or annotated datasets, to carry out even simple predictions.
To tackle this gap, we propose a new perspective on the problem of learning
intuitive physics that is inspired by the spatial memory representation of
objects and spaces in human brains, in particular the co-existence of
egocentric and allocentric spatial representations. We present a generic
framework that learns a layered representation of the physical world, using a
cascade of invertible modules. In this framework, real images are first
converted to a synthetic domain representation that reduces complexity arising
from lighting and texture. Then, an allocentric viewpoint transformer removes
viewpoint complexity by projecting images to a canonical view. Finally, a novel
Recurrent Latent Variation Network (RLVN) architecture learns the dynamics of
the objects interacting with the environment and predicts future motion,
leveraging the availability of unlimited synthetic simulations. Predicted
frames are then projected back to the original camera view and translated back
to the real world domain. Experimental results show the ability of the
framework to consistently and accurately predict several frames in the future
and the ability to adapt to real images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhihua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_S/0/1/0/all/0/1&quot;&gt;Stefano Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1&quot;&gt;Yishu Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1&quot;&gt;Zihang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Linhai Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1&quot;&gt;Andrew Markham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1&quot;&gt;Niki Trigoni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02753">
<title>Optimal and Low-Complexity Dynamic Spectrum Access for RF-Powered Ambient Backscatter System with Online Reinforcement Learning. (arXiv:1809.02753v1 [cs.NI])</title>
<link>http://arxiv.org/abs/1809.02753</link>
<description rdf:parseType="Literal">&lt;p&gt;Ambient backscatter has been introduced with a wide range of applications for
low power wireless communications. In this article, we propose an optimal and
low-complexity dynamic spectrum access framework for RF-powered ambient
backscatter system. In this system, the secondary transmitter not only harvests
energy from ambient signals (from incumbent users), but also backscatters these
signals to its receiver for data transmission. Under the dynamics of the
ambient signals, we first adopt the Markov decision process (MDP) framework to
obtain the optimal policy for the secondary transmitter, aiming to maximize the
system throughput. However, the MDP-based optimization requires complete
knowledge of environment parameters, e.g., the probability of a channel to be
idle and the probability of a successful packet transmission, that may not be
practical to obtain. To cope with such incomplete knowledge of the environment,
we develop a low-complexity online reinforcement learning algorithm that allows
the secondary transmitter to &quot;learn&quot; from its decisions and then attain the
optimal policy. Simulation results show that the proposed learning algorithm
not only efficiently deals with the dynamics of the environment, but also
improves the average throughput up to 50% and reduces the blocking probability
and delay up to 80% compared with conventional methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1&quot;&gt;Nguyen Van Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1&quot;&gt;Dinh Thai Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Diep N. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutkiewicz_E/0/1/0/all/0/1&quot;&gt;Eryk Dutkiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1&quot;&gt;Dusit Niyato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Ping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02804">
<title>Handling Concept Drift via Model Reuse. (arXiv:1809.02804v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02804</link>
<description rdf:parseType="Literal">&lt;p&gt;In many real-world applications, data are often collected in the form of
stream, and thus the distribution usually changes in nature, which is referred
as concept drift in literature. We propose a novel and effective approach to
handle concept drift via model reuse, leveraging previous knowledge by reusing
models. Each model is associated with a weight representing its reusability
towards current data, and the weight is adaptively adjusted according to the
model performance. We provide generalization and regret analysis. Experimental
results also validate the superiority of our approach on both synthetic and
real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1&quot;&gt;Le-Wen Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhi-Hua Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02838">
<title>Non-Parametric Variational Inference with Graph Convolutional Networks for Gaussian Processes. (arXiv:1809.02838v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02838</link>
<description rdf:parseType="Literal">&lt;p&gt;Inference for GP models with non-Gaussian noises is computationally expensive
when dealing with large datasets. Many recent inference methods approximate the
posterior distribution with a simpler distribution defined on a small number of
inducing points. The inference is accurate only when data points have strong
correlation with these inducing points. In this paper, we consider the
inference problem in a different direction: GP function values in the posterior
are mostly correlated in short distance. We construct a variational
distribution such that the inference for a data point considers only its
neighborhood. With this construction, the variational lower bound is highly
decomposible, hence we can run stochastic optimization with very small batches.
We then train Graph Convolutional Networks as a reusable model to identify
variational parameters for each data point. Model reuse greatly reduces the
number of parameters and the number of iterations needed in optimization. The
proposed method significantly speeds up the inference and often gets more
accurate results than previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Linfeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liping Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02855">
<title>iDriveSense: Dynamic Route Planning Involving Roads Quality Information. (arXiv:1809.02855v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1809.02855</link>
<description rdf:parseType="Literal">&lt;p&gt;Owing to the expeditious growth in the information and communication
technologies, smart cities have raised the expectations in terms of efficient
functioning and management. One key aspect of residents&apos; daily comfort is
assured through affording reliable traffic management and route planning.
Comprehensively, the majority of the present trip planning applications and
service providers are enabling their trip planning recommendations relying on
shortest paths and/or fastest routes. However, such suggestions may discount
drivers&apos; preferences with respect to safe and less disturbing trips. Road
anomalies such as cracks, potholes, and manholes induce risky driving scenarios
and can lead to vehicles damages and costly repairs. Accordingly, in this
paper, we propose a crowdsensing based dynamic route planning system.
Leveraging both the vehicle motion sensors and the inertial sensors within the
smart devices, road surface types and anomalies have been detected and
categorized. In addition, the monitored events are geo-referenced utilizing GPS
receivers on both vehicles and smart devices. Consequently, road segments
assessments are conducted using fuzzy system models based on aspects such as
the number of anomalies and their severity levels in each road segment.
Afterward, another fuzzy model is adopted to recommend the best trip routes
based on the road segments quality in each potential route. Extensive road
experiments are held to build and show the potential of the proposed system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Wakeel_A/0/1/0/all/0/1&quot;&gt;Amr S. El-Wakeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noureldin_A/0/1/0/all/0/1&quot;&gt;Aboelmagd Noureldin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanein_H/0/1/0/all/0/1&quot;&gt;Hossam S. Hassanein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zorba_N/0/1/0/all/0/1&quot;&gt;Nizar Zorba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02869">
<title>Modelling User&apos;s Theory of AI&apos;s Mind in Interactive Intelligent Systems. (arXiv:1809.02869v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02869</link>
<description rdf:parseType="Literal">&lt;p&gt;Many interactive intelligent systems, such as recommendation and information
retrieval systems, treat users as a passive data source. Yet, users form mental
models of systems and instead of passively providing feedback to the queries of
the system, they will strategically plan their actions within the constraints
of the mental model to steer the system and achieve their goals faster. We
propose to explicitly account for the user&apos;s theory of the AI&apos;s mind in the
user model: the intelligent system has a model of the user having a model of
the intelligent system. We study a case where the system is a contextual bandit
and the user model is a Markov decision process that plans based on a simpler
model of the bandit. Inference in the model can be reduced to probabilistic
inverse reinforcement learning, with the nested bandit model defining the
transition dynamics, and is implemented using probabilistic programming. Our
results show that improved performance is achieved if users can form accurate
mental models that the system can capture, implying predictability of the
interactive intelligent system is important not only for the user experience
but also for the design of the system&apos;s statistical models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peltola_T/0/1/0/all/0/1&quot;&gt;Tomi Peltola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celikok_M/0/1/0/all/0/1&quot;&gt;Mustafa Mert &amp;#xc7;elikok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daee_P/0/1/0/all/0/1&quot;&gt;Pedram Daee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1&quot;&gt;Samuel Kaski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02909">
<title>Elliptical Distributions-Based Weights-Determining Method for OWA Operators. (arXiv:1809.02909v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.02909</link>
<description rdf:parseType="Literal">&lt;p&gt;The ordered weighted averaging (OWA) operators play a crucial role in
aggregating multiple criteria evaluations into an overall assessment supporting
the decision makers&apos; choice. One key point steps is to determine the associated
weights. In this paper, we first briefly review some main methods for
determining the weights by using distribution functions. Then we propose a new
approach for determining OWA weights by using the RIM quantifier. Motivated by
the idea of normal distribution-based method to determine the OWA weights, we
develop a method based on elliptical distributions for determining the OWA
weights, and some of its desirable properties have been investigated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sha_X/0/1/0/all/0/1&quot;&gt;Xiuyan Sha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zeshui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Chuancun Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02926">
<title>Probabilistic Prediction of Interactive Driving Behavior via Hierarchical Inverse Reinforcement Learning. (arXiv:1809.02926v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02926</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles (AVs) are on the road. To safely and efficiently interact
with other road participants, AVs have to accurately predict the behavior of
surrounding vehicles and plan accordingly. Such prediction should be
probabilistic, to address the uncertainties in human behavior. Such prediction
should also be interactive, since the distribution over all possible
trajectories of the predicted vehicle depends not only on historical
information, but also on future plans of other vehicles that interact with it.
To achieve such interaction-aware predictions, we propose a probabilistic
prediction approach based on hierarchical inverse reinforcement learning (IRL).
First, we explicitly consider the hierarchical trajectory-generation process of
human drivers involving both discrete and continuous driving decisions. Based
on this, the distribution over all future trajectories of the predicted vehicle
is formulated as a mixture of distributions partitioned by the discrete
decisions. Then we apply IRL hierarchically to learn the distributions from
real human demonstrations. A case study for the ramp-merging driving scenario
is provided. The quantitative results show that the proposed approach can
accurately predict both the discrete driving decisions such as yield or pass as
well as the continuous trajectories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Liting Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wei Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02927">
<title>Generic Probabilistic Interactive Situation Recognition and Prediction: From Virtual to Real. (arXiv:1809.02927v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.02927</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and robust recognition and prediction of traffic situation plays an
important role in autonomous driving, which is a prerequisite for risk
assessment and effective decision making. Although there exist a lot of works
dealing with modeling driver behavior of a single object, it remains a
challenge to make predictions for multiple highly interactive agents that react
to each other simultaneously. In this work, we propose a generic probabilistic
hierarchical recognition and prediction framework which employs a two-layer
Hidden Markov Model (TLHMM) to obtain the distribution of potential situations
and a learning-based dynamic scene evolution model to sample a group of future
trajectories. Instead of predicting motions of a single entity, we propose to
get the joint distribution by modeling multiple interactive agents as a whole
system. Moreover, due to the decoupling property of the layered structure, our
model is suitable for knowledge transfer from simulation to real world
applications as well as among different traffic scenarios, which can reduce the
computational efforts of training and the demand for a large data amount. A
case study of highway ramp merging scenario is demonstrated to verify the
effectiveness and accuracy of the proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiachen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hengbo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wei Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03001">
<title>Evidence-based lean logic profiles for conceptual data modelling languages. (arXiv:1809.03001v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.03001</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiple logic-based reconstruction of conceptual data modelling languages
such as EER, UML Class Diagrams, and ORM exists. They mainly cover various
fragments of the languages and none are formalised such that the logic applies
simultaneously for all three modelling language families as unifying mechanism.
This hampers interchangeability, interoperability, and tooling support. In
addition, due to the lack of a systematic design process of the logic used for
the formalisation, hidden choices permeate the formalisations that have
rendered them incompatible. We aim to address these problems, first, by
structuring the logic design process in a methodological way. We generalise and
extend the DSL design process to apply to logic language design more generally
and, in particular, by incorporating an ontological analysis of language
features in the process. Second, availing of this extended process, of evidence
gathered of language feature usage, and of computational complexity insights
from Description Logics (DL), we specify logic profiles taking into account the
ontological commitments embedded in the languages. The profiles characterise
the minimum logic structure needed to handle the semantics of conceptual
models, enabling the development of interoperability tools. There is no known
DL language that matches exactly the features of those profiles and the common
core is small (in the tractable $\mathcal{ALNI}$). Although hardly any
inconsistencies can be derived with the profiles, it is promising for scalable
runtime use of conceptual data models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fillottrani_P/0/1/0/all/0/1&quot;&gt;Pablo Rub&amp;#xe9;n Fillottrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keet_C/0/1/0/all/0/1&quot;&gt;C. Maria Keet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03044">
<title>How clever is the FiLM model, and how clever can it be?. (arXiv:1809.03044v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.03044</link>
<description rdf:parseType="Literal">&lt;p&gt;The FiLM model achieves close-to-perfect performance on the diagnostic CLEVR
dataset and is distinguished from other such models by having a comparatively
simple and easily transferable architecture. In this paper, we investigate in
more detail the ability of FiLM to learn various linguistic constructions. Our
main results show that (a) FiLM is not able to learn relational statements
straight away except for very simple instances, (b) training on a broader set
of instances as well as pretraining on simpler instance types can help
alleviate these learning difficulties, (c) mixing is less robust than
pretraining and very sensitive to the compositional structure of the dataset.
Overall, our results suggest that the approach of big all-encompassing datasets
and the paradigm of &quot;the effectiveness of data&quot; may have fundamental
limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhnle_A/0/1/0/all/0/1&quot;&gt;Alexander Kuhnle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Huiyuan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Copestake_A/0/1/0/all/0/1&quot;&gt;Ann Copestake&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03057">
<title>Variance Reduction in Monte Carlo Counterfactual Regret Minimization (VR-MCCFR) for Extensive Form Games using Baselines. (arXiv:1809.03057v1 [cs.GT])</title>
<link>http://arxiv.org/abs/1809.03057</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning strategies for imperfect information games from samples of
interaction is a challenging problem. A common method for this setting, Monte
Carlo Counterfactual Regret Minimization (MCCFR), can have slow long-term
convergence rates due to high variance. In this paper, we introduce a variance
reduction technique (VR-MCCFR) that applies to any sampling variant of MCCFR.
Using this technique, per-iteration estimated values and updates are
reformulated as a function of sampled values and state-action baselines,
similar to their use in policy gradient reinforcement learning. The new
formulation allows estimates to be bootstrapped from other estimates within the
same episode, propagating the benefits of baselines along the sampled
trajectory; the estimates remain unbiased even when bootstrapping from other
estimates. Finally, we show that given a perfect baseline, the variance of the
value estimates can be reduced to zero. Experimental evaluation shows that
VR-MCCFR brings an order of magnitude speedup, while the empirical variance
decreases by three orders of magnitude. The decreased variance allows for the
first time CFR+ to be used with sampling, increasing the speedup to two orders
of magnitude.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_M/0/1/0/all/0/1&quot;&gt;Martin Schmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burch_N/0/1/0/all/0/1&quot;&gt;Neil Burch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanctot_M/0/1/0/all/0/1&quot;&gt;Marc Lanctot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moravcik_M/0/1/0/all/0/1&quot;&gt;Matej Moravcik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadlec_R/0/1/0/all/0/1&quot;&gt;Rudolf Kadlec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowling_M/0/1/0/all/0/1&quot;&gt;Michael Bowling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03073">
<title>Sample Complexity of Nonparametric Semi-Supervised Learning. (arXiv:1809.03073v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03073</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the sample complexity of semi-supervised learning (SSL) and
introduce new assumptions based on the mismatch between a mixture model learned
from unlabeled data and the true mixture model induced by the (unknown) class
conditional distributions. Under these assumptions, we establish an
$\Omega(K\log K)$ labeled sample complexity bound without imposing parametric
assumptions, where $K$ is the number of classes. Our results suggest that even
in nonparametric settings it is possible to learn a near-optimal classifier
using only a few labeled samples. Unlike previous theoretical work which
focuses on binary classification, we consider general multiclass classification
($K&amp;gt;2$), which requires solving a difficult permutation learning problem. This
permutation defines a classifier whose classification error is controlled by
the Wasserstein distance between mixing measures, and we provide finite-sample
results characterizing the behaviour of the excess risk of this classifier.
Finally, we describe three algorithms for computing these estimators based on a
connection to bipartite graph matching, and perform experiments to illustrate
the superiority of the MLE over the majority vote estimator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dan_C/0/1/0/all/0/1&quot;&gt;Chen Dan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leqi_L/0/1/0/all/0/1&quot;&gt;Liu Leqi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aragam_B/0/1/0/all/0/1&quot;&gt;Bryon Aragam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03075">
<title>Online Convex Optimization for Sequential Decision Processes and Extensive-Form Games. (arXiv:1809.03075v1 [cs.GT])</title>
<link>http://arxiv.org/abs/1809.03075</link>
<description rdf:parseType="Literal">&lt;p&gt;Regret minimization is a powerful tool for solving large-scale extensive-form
games. State-of-the-art methods rely on minimizing regret locally at each
decision point. In this work we derive a new framework for regret minimization
on sequential decision problems and extensive-form games with general compact
convex sets at each decision point and general convex losses, as opposed to
prior work which has been for simplex decision points and linear losses. We
call our framework laminar regret decomposition. It generalizes the CFR
algorithm to this more general setting. Furthermore, our framework enables a
new proof of CFR even in the known setting, which is derived from a perspective
of decomposing polytope regret, thereby leading to an arguably simpler
interpretation of the algorithm. Our generalization to convex compact sets and
convex losses allows us to develop new algorithms for several problems:
regularized sequential decision making, regularized Nash equilibria in
extensive-form games, and computing approximate extensive-form perfect
equilibria. Our generalization also leads to the first regret-minimization
algorithm for computing reduced-normal-form quantal response equilibria based
on minimizing local regrets. Experiments show that our framework leads to
algorithms that scale at a rate comparable to the fastest variants of
counterfactual regret minimization for computing Nash equilibrium, and
therefore our approach leads to the first algorithm for computing quantal
response equilibria in extremely large games. Finally we show that our
framework enables a new kind of scalable opponent exploitation approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farina_G/0/1/0/all/0/1&quot;&gt;Gabriele Farina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kroer_C/0/1/0/all/0/1&quot;&gt;Christian Kroer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandholm_T/0/1/0/all/0/1&quot;&gt;Tuomas Sandholm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03084">
<title>Efficient Counterfactual Learning from Bandit Feedback. (arXiv:1809.03084v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03084</link>
<description rdf:parseType="Literal">&lt;p&gt;What is the most statistically efficient way to do off-policy evaluation and
optimization with batch data from bandit feedback? For log data generated by
contextual bandit algorithms, we consider offline estimators for the expected
reward from a counterfactual policy. Our estimators are shown to have lowest
variance in a wide class of estimators, achieving variance reduction relative
to standard estimators. We also apply our estimators to improve online
advertisement design by a major advertisement company. Consistent with the
theoretical result, our estimators allow us to improve on the existing bandit
algorithm with more statistical confidence compared to a state-of-the-art
benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narita_Y/0/1/0/all/0/1&quot;&gt;Yusuke Narita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasui_S/0/1/0/all/0/1&quot;&gt;Shota Yasui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yata_K/0/1/0/all/0/1&quot;&gt;Kohei Yata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03119">
<title>Memristive LSTM network hardware architecture for time-series predictive modeling problem. (arXiv:1809.03119v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1809.03119</link>
<description rdf:parseType="Literal">&lt;p&gt;Analysis of time-series data allows to identify long-term trends and make
predictions that can help to improve our lives. With the rapid development of
artificial neural networks, long short-term memory (LSTM) recurrent neural
network (RNN) configuration is found to be capable in dealing with time-series
forecasting problems where data points are time-dependent and possess
seasonality trends. Gated structure of LSTM cell and flexibility in network
topology (one-to-many, many-to-one, etc.) allows to model systems with multiple
input variables and control several parameters such as the size of the
look-back window to make a prediction and number of time steps to be predicted.
These make LSTM attractive tool over conventional methods such as
autoregression models, the simple average, moving average, naive approach,
ARIMA, Holt&apos;s linear trend method, Holt&apos;s Winter seasonal method, and others.
In this paper, we propose a hardware implementation of LSTM network
architecture for time-series forecasting problem. All simulations were
performed using TSMC 0.18um CMOS technology and HP memristor model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_K/0/1/0/all/0/1&quot;&gt;Kazybek Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smagulova_K/0/1/0/all/0/1&quot;&gt;Kamilya Smagulova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_A/0/1/0/all/0/1&quot;&gt;Alex Pappachen James&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03149">
<title>Learning to Advertise with Adaptive Exposure via Constrained Two-Level Reinforcement Learning. (arXiv:1809.03149v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03149</link>
<description rdf:parseType="Literal">&lt;p&gt;For online advertising in e-commerce, the traditional problem is to assign
the right ad to the right user on fixed ad slots. In this paper, we investigate
the problem of advertising with adaptive exposure, in which the number of ad
slots and their locations can dynamically change over time based on their
relative scores with recommendation products. In order to maintain user
retention and long-term revenue, there are two types of constraints that need
to be met in exposure: query-level and day-level constraints. We model this
problem as constrained markov decision process with per-state constraint
(psCMDP) and propose a constrained two-level reinforcement learning to decouple
the original advertising exposure optimization problem into two relatively
independent sub-optimization problems. We also propose a constrained hindsight
experience replay mechanism to accelerate the policy training process.
Experimental results show that our method can improve the advertising revenue
while satisfying different levels of constraints under the real-world datasets.
Besides, the proposal of constrained hindsight experience replay mechanism can
significantly improve the training speed and the stability of policy
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weixun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Junqi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jianye Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chunjie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chuan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Kun Gai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03152">
<title>A Multi-Agent Reinforcement Learning Method for Impression Allocation in Online Display Advertising. (arXiv:1809.03152v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.03152</link>
<description rdf:parseType="Literal">&lt;p&gt;In online display advertising, guaranteed contracts and real-time bidding
(RTB) are two major ways to sell impressions for a publisher. Despite the
increasing popularity of RTB, there is still half of online display advertising
revenue generated from guaranteed contracts. Therefore, simultaneously selling
impressions through both guaranteed contracts and RTB is a straightforward
choice for a publisher to maximize its yield. However, deriving the optimal
strategy to allocate impressions is not a trivial task, especially when the
environment is unstable in real-world applications. In this paper, we formulate
the impression allocation problem as an auction problem where each contract can
submit virtual bids for individual impressions. With this formulation, we
derive the optimal impression allocation strategy by solving the optimal
bidding functions for contracts. Since the bids from contracts are decided by
the publisher, we propose a multi-agent reinforcement learning (MARL) approach
to derive cooperative policies for the publisher to maximize its yield in an
unstable environment. The proposed approach also resolves the common challenges
in MARL such as input dimension explosion, reward credit assignment, and
non-stationary environment. Experimental evaluations on large-scale real
datasets demonstrate the effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiujun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1&quot;&gt;Qing Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Kun Gai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03200">
<title>Decentralized Cooperative Planning for Automated Vehicles with Continuous Monte Carlo Tree Search. (arXiv:1809.03200v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.03200</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban traffic scenarios often require a high degree of cooperation between
traffic participants to ensure safety and efficiency. Observing the behavior of
others, humans infer whether or not others are cooperating. This work aims to
extend the capabilities of automated vehicles, enabling them to cooperate
implicitly in heterogeneous environments. Continuous actions allow for
arbitrary trajectories and hence are applicable to a much wider class of
problems than existing cooperative approaches with discrete action spaces.
Based on cooperative modeling of other agents, Monte Carlo Tree Search (MCTS)
in conjunction with Decoupled-UCT evaluates the action-values of each agent in
a cooperative and decentralized way, respecting the interdependence of actions
among traffic participants. The extension to continuous action spaces is
addressed by incorporating novel MCTS-specific enhancements for efficient
search space exploration. The proposed algorithm is evaluated under different
scenarios, showing that the algorithm is able to achieve effective cooperative
planning and generate solutions egocentric planning fails to identify.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurzer_K/0/1/0/all/0/1&quot;&gt;Karl Kurzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engelhorn_F/0/1/0/all/0/1&quot;&gt;Florian Engelhorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1&quot;&gt;J. Marius Z&amp;#xf6;llner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03202">
<title>Learning Sequence Encoders for Temporal Knowledge Graph Completion. (arXiv:1809.03202v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.03202</link>
<description rdf:parseType="Literal">&lt;p&gt;Research on link prediction in knowledge graphs has mainly focused on static
multi-relational data. In this work we consider temporal knowledge graphs where
relations between entities may only hold for a time interval or a specific
point in time. In line with previous work on static knowledge graphs, we
propose to address this problem by learning latent entity and relation type
representations. To incorporate temporal information, we utilize recurrent
neural networks to learn time-aware representations of relation types which can
be used in conjunction with existing latent factorization methods. The proposed
approach is shown to be robust to common challenges in real-world KGs: the
sparsity and heterogeneity of temporal expressions. Experiments show the
benefits of our approach on four temporal KGs. The data sets are available
under a permissive BSD-3 license 1.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Duran_A/0/1/0/all/0/1&quot;&gt;Alberto Garc&amp;#xed;a-Dur&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumancic_S/0/1/0/all/0/1&quot;&gt;Sebastijan Duman&amp;#x10d;i&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03359">
<title>Improving Optimization Bounds using Machine Learning: Decision Diagrams meet Deep Reinforcement Learning. (arXiv:1809.03359v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.03359</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding tight bounds on the optimal solution is a critical element of
practical solution methods for discrete optimization problems. In the last
decade, decision diagrams (DDs) have brought a new perspective on obtaining
upper and lower bounds that can be significantly better than classical bounding
mechanisms, such as linear relaxations. It is well known that the quality of
the bound achieved through this flexible bounding method is highly reliant on
the ordering of variables chosen for building the diagram, and finding an
ordering that optimizes standard metrics, or even improving one, is an NP-hard
problem. In this paper, we propose an innovative and generic approach based on
deep reinforcement learning for obtaining an ordering for tightening the bounds
obtained with relaxed and restricted DDs. We apply the approach to both the
Maximum Independent Set Problem and the Maximum Cut Problem. Experimental
results on synthetic instances show that the deep reinforcement learning
approach, by achieving tighter objective function bounds, generally outperforms
ordering methods commonly used in the literature when the distribution of
instances is known. To the best knowledge of the authors, this is the first
paper to apply machine learning to directly improve relaxation bounds obtained
by general-purpose bounding mechanisms for combinatorial optimization problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cappart_Q/0/1/0/all/0/1&quot;&gt;Quentin Cappart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goutierre_E/0/1/0/all/0/1&quot;&gt;Emmanuel Goutierre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergman_D/0/1/0/all/0/1&quot;&gt;David Bergman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rousseau_L/0/1/0/all/0/1&quot;&gt;Louis-Martin Rousseau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03363">
<title>Torchbearer: A Model Fitting Library for PyTorch. (arXiv:1809.03363v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03363</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce torchbearer, a model fitting library for pytorch aimed at
researchers working on deep learning or differentiable programming. The
torchbearer library provides a high level metric and callback API that can be
used for a wide range of applications. We also include a series of built in
callbacks that can be used for: model persistence, learning rate decay,
logging, data visualization and more. The extensive documentation includes an
example library for deep learning and dynamic programming problems and can be
found at &lt;a href=&quot;http://torchbearer.readthedocs.io.&quot;&gt;this http URL&lt;/a&gt; The code is licensed under the MIT
License and available at https://github.com/ecs-vlc/torchbearer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harris_E/0/1/0/all/0/1&quot;&gt;Ethan Harris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Painter_M/0/1/0/all/0/1&quot;&gt;Matthew Painter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1&quot;&gt;Jonathon Hare&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03371">
<title>Revisiting Inaccuracies of Time Series Averaging under Dynamic Time Warping. (arXiv:1809.03371v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.03371</link>
<description rdf:parseType="Literal">&lt;p&gt;This article revisits an analysis on inaccuracies of time series averaging
under dynamic time warping conducted by \cite{Niennattrakul2007}. The authors
presented a correctness-criterion and introduced drift-outs of averages from
clusters. They claimed that averages are inaccurate if they are incorrect or
drift-outs. Furthermore, they conjectured that such inaccuracies are caused by
the lack of triangle inequality. We show that a rectified version of the
correctness-criterion is unsatisfiable and that the concept of drift-out is
geometrically and operationally inconclusive. Satisfying the triangle
inequality is insufficient to achieve correctness and unnecessary to overcome
the drift-out phenomenon. We place the concept of drift-out on a principled
basis and show that sample means as global minimizers of a Fr\&apos;echet function
never drift out. The adjusted drift-out is a way to test to which extent an
approximation is coherent. Empirical results show that solutions obtained by
the state-of-the-art methods SSG and DBA are incoherent approximations of a
sample mean in over a third of all trials.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jain_B/0/1/0/all/0/1&quot;&gt;Brijnesh Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03428">
<title>Not Just Privacy: Improving Performance of Private Deep Learning in Mobile Cloud. (arXiv:1809.03428v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03428</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing demand for on-device deep learning services calls for a highly
efficient manner to deploy deep neural networks (DNNs) on mobile devices with
limited capacity. The cloud-based solution is a promising approach to enabling
deep learning applications on mobile devices where the large portions of a DNN
are offloaded to the cloud. However, revealing data to the cloud leads to
potential privacy risk. To benefit from the cloud data center without the
privacy risk, we design, evaluate, and implement a cloud-based framework ARDEN
which partitions the DNN across mobile devices and cloud data centers. A simple
data transformation is performed on the mobile device, while the
resource-hungry training and the complex inference rely on the cloud data
center. To protect the sensitive information, a lightweight privacy-preserving
mechanism consisting of arbitrary data nullification and random noise addition
is introduced, which provides strong privacy guarantee. A rigorous privacy
budget analysis is given. Nonetheless, the private perturbation to the original
data inevitably has a negative impact on the performance of further inference
on the cloud side. To mitigate this influence, we propose a noisy training
method to enhance the cloud-side network robustness to perturbed data. Through
the sophisticated design, ARDEN can not only preserve privacy but also improve
the inference performance. To validate the proposed ARDEN, a series of
experiments based on three image datasets and a real mobile application are
conducted. The experimental results demonstrate the effectiveness of ARDEN.
Finally, we implement ARDEN on a demo system to verify its practicality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Ji Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianguo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1&quot;&gt;Weidong Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaomin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1&quot;&gt;Bokai Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03470">
<title>ViZDoom Competitions: Playing Doom from Pixels. (arXiv:1809.03470v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.03470</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the first two editions of Visual Doom AI Competition,
held in 2016 and 2017. The challenge was to create bots that compete in a
multi-player deathmatch in a first-person shooter (FPS) game, Doom. The bots
had to make their decisions based solely on visual information, i.e., a raw
screen buffer. To play well, the bots needed to understand their surroundings,
navigate, explore, and handle the opponents at the same time. These aspects,
together with the competitive multi-agent aspect of the game, make the
competition a unique platform for evaluating the state of the art reinforcement
learning algorithms. The paper discusses the rules, solutions, results, and
statistics that give insight into the agents&apos; behaviors. Best-performing agents
are described in more detail. The results of the competition lead to the
conclusion that, although reinforcement learning can produce capable Doom bots,
they still are not yet able to successfully compete against humans in this
game. The paper also revisits the ViZDoom environment, which is a flexible,
easy to use, and efficient 3D platform for research for vision-based
reinforcement learning, based on a well-recognized first-person perspective
game Doom.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wydmuch_M/0/1/0/all/0/1&quot;&gt;Marek Wydmuch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kempka_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Kempka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaskowski_W/0/1/0/all/0/1&quot;&gt;Wojciech Ja&amp;#x15b;kowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03478">
<title>Towards a Fatality-Aware Benchmark of Probabilistic Reaction Prediction in Highly Interactive Driving Scenarios. (arXiv:1809.03478v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1809.03478</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles should be able to generate accurate probabilistic
predictions for uncertain behavior of other road users. Moreover, reactive
predictions are necessary in highly interactive driving scenarios to answer
&quot;what if I take this action in the future&quot; for autonomous vehicles. There is no
existing unified framework to homogenize the problem formulation,
representation simplification, and evaluation metric for various prediction
methods, such as probabilistic graphical models (PGM), neural networks (NN) and
inverse reinforcement learning (IRL). In this paper, we formulate a
probabilistic reaction prediction problem, and reveal the relationship between
reaction and situation prediction problems. We employ prototype trajectories
with designated motion patterns other than &quot;intention&quot; to homogenize the
representation so that probabilities corresponding to each trajectory generated
by different methods can be evaluated. We also discuss the reasons why
&quot;intention&quot; is not suitable to serve as a motion indicator in highly
interactive scenarios. We propose to use Brier score as the baseline metric for
evaluation. In order to reveal the fatality of the consequences when the
predictions are adopted by decision-making and planning, we propose a
fatality-aware metric, which is a weighted Brier score based on the criticality
of the trajectory pairs of the interacting entities. Conservatism and
non-defensiveness are defined from the weighted Brier score to indicate the
consequences caused by inaccurate predictions. Modified methods based on PGM,
NN and IRL are provided to generate probabilistic reaction predictions in an
exemplar scenario of nudging from a highway ramp. The results are evaluated by
the baseline and proposed metrics to construct a mini benchmark. Analysis on
the properties of each method is also provided by comparing the baseline and
proposed metric scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wei Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Liting Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yeping Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiachen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.08325">
<title>An adaptive prefix-assignment technique for symmetry reduction. (arXiv:1706.08325v2 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/1706.08325</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a technique for symmetry reduction that adaptively
assigns a prefix of variables in a system of constraints so that the generated
prefix-assignments are pairwise nonisomorphic under the action of the symmetry
group of the system. The technique is based on McKay&apos;s canonical extension
framework [J.~Algorithms 26 (1998), no.~2, 306--324]. Among key features of the
technique are (i) adaptability---the prefix sequence can be user-prescribed and
truncated for compatibility with the group of symmetries; (ii)
parallelizability---prefix-assignments can be processed in parallel
independently of each other; (iii) versatility---the method is applicable
whenever the group of symmetries can be concisely represented as the
automorphism group of a vertex-colored graph; and (iv) implementability---the
method can be implemented relying on a canonical labeling map for
vertex-colored graphs as the only nontrivial subroutine. To demonstrate the
practical applicability of our technique, we have prepared an experimental
open-source implementation of the technique and carry out a set of experiments
that demonstrate ability to reduce symmetry on hard instances. Furthermore, we
demonstrate that the implementation effectively parallelizes to compute
clusters with multiple nodes via a message-passing interface.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junttila_T/0/1/0/all/0/1&quot;&gt;Tommi Junttila&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karppa_M/0/1/0/all/0/1&quot;&gt;Matti Karppa&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaski_P/0/1/0/all/0/1&quot;&gt;Petteri Kaski&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohonen_J/0/1/0/all/0/1&quot;&gt;Jukka Kohonen&lt;/a&gt; (1) ((1) Aalto University, Department of Computer Science)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07387">
<title>How morphological development can guide evolution. (arXiv:1711.07387v5 [q-bio.PE] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07387</link>
<description rdf:parseType="Literal">&lt;p&gt;Organisms result from adaptive processes interacting across different time
scales. One such interaction is that between development and evolution. Models
have shown that development sweeps over several traits in a single agent,
sometimes exposing promising static traits. Subsequent evolution can then
canalize these rare traits. Thus, development can, under the right conditions,
increase evolvability. Here, we report on a previously unknown phenomenon when
embodied agents are allowed to develop and evolve: Evolution discovers body
plans robust to control changes, these body plans become genetically
assimilated, yet controllers for these agents are not assimilated. This allows
evolution to continue climbing fitness gradients by tinkering with the
developmental programs for controllers within these permissive body plans. This
exposes a previously unknown detail about the Baldwin effect: instead of all
useful traits becoming genetically assimilated, only traits that render the
agent robust to changes in other traits become assimilated. We refer to this as
differential canalization. This finding also has implications for the
evolutionary design of artificial and embodied agents such as robots: robots
robust to internal changes in their controllers may also be robust to external
changes in their environment, such as transferal from simulation to reality or
deployment in novel environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kriegman_S/0/1/0/all/0/1&quot;&gt;Sam Kriegman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cheney_N/0/1/0/all/0/1&quot;&gt;Nick Cheney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bongard_J/0/1/0/all/0/1&quot;&gt;Josh Bongard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04172">
<title>A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents. (arXiv:1712.04172v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04172</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a low-cost, easily realizable strategy to equip a
reinforcement learning (RL) agent the capability of behaving ethically. Our
model allows the designers of RL agents to solely focus on the task to achieve,
without having to worry about the implementation of multiple trivial ethical
patterns to follow. Based on the assumption that the majority of human
behavior, regardless which goals they are achieving, is ethical, our design
integrates human policy with the RL policy to achieve the target objective with
less chance of violating the ethical code that human beings normally obey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yueh-Hua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shou-De Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00102">
<title>Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference. (arXiv:1801.00102v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1801.00102</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new deep learning architecture for Natural Language
Inference (NLI). Firstly, we introduce a new architecture where alignment pairs
are compared, compressed and then propagated to upper layers for enhanced
representation learning. Secondly, we adopt factorization layers for efficient
and expressive compression of alignment vectors into scalar features, which are
then used to augment the base word representations. The design of our approach
is aimed to be conceptually simple, compact and yet powerful. We conduct
experiments on three popular benchmarks, SNLI, MultiNLI and SciTail, achieving
competitive performance on all. A lightweight parameterization of our model
also enjoys a $\approx 3$ times reduction in parameter size compared to the
existing state-of-the-art models, e.g., ESIM and DIIN, while maintaining
competitive performance. Additionally, visual analysis shows that our
propagated features are highly interpretable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1&quot;&gt;Yi Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1&quot;&gt;Luu Anh Tuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1&quot;&gt;Siu Cheung Hui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04592">
<title>Rebalancing Dockless Bike Sharing Systems. (arXiv:1802.04592v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04592</link>
<description rdf:parseType="Literal">&lt;p&gt;Bike sharing provides an environment-friendly way for traveling and is
booming all over the world. Yet, due to the high similarity of user travel
patterns, the bike imbalance problem constantly occurs, especially for dockless
bike sharing systems, causing significant impact on service quality and company
revenue. Thus, it has become a critical task for bike sharing systems to
resolve such imbalance efficiently. In this paper, we propose a novel deep
reinforcement learning framework for incentivizing users to rebalance such
systems. We model the problem as a Markov decision process and take both
spatial and temporal features into consideration. We develop a novel deep
reinforcement learning algorithm called Hierarchical Reinforcement Pricing
(HRP), which builds upon the Deep Deterministic Policy Gradient algorithm.
Different from existing methods that often ignore spatial information and rely
heavily on accurate prediction, HRP captures both spatial and temporal
dependencies using a divide-and-conquer structure with an embedded localized
module. We conduct extensive experiments to evaluate HRP, based on a dataset
from Mobike, a major Chinese dockless bike sharing company. Results show that
HRP performs close to the 24-timeslot look-ahead optimization, and outperforms
state-of-the-art methods in both service level and bike distribution. It also
transfers well when applied to unseen areas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Ling Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1&quot;&gt;Qingpeng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhixuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1&quot;&gt;Pingzhong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Longbo Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03048">
<title>Clustrophile 2: Guided Visual Clustering Analysis. (arXiv:1804.03048v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/1804.03048</link>
<description rdf:parseType="Literal">&lt;p&gt;Data clustering is a common unsupervised learning method frequently used in
exploratory data analysis. However, identifying relevant structures in
unlabeled, high-dimensional data is nontrivial, requiring iterative
experimentation with clustering parameters as well as data features and
instances. The number of possible clusterings for a typical dataset is vast,
and navigating in this vast space is also challenging. The absence of
ground-truth labels makes it impossible to define an optimal solution, thus
requiring user judgment to establish what can be considered a satisfiable
clustering result. Data scientists need adequate interactive tools to
effectively explore and navigate the large clustering space so as to improve
the effectiveness of exploratory clustering analysis. We introduce
\textit{Clustrophile~2}, a new interactive tool for guided clustering analysis.
\textit{Clustrophile~2} guides users in clustering-based exploratory analysis,
adapts user feedback to improve user guidance, facilitates the interpretation
of clusters, and helps quickly reason about differences between clusterings. To
this end, \textit{Clustrophile~2} contributes a novel feature, the Clustering
Tour, to help users choose clustering parameters and assess the quality of
different clustering results in relation to current analysis goals and user
expectations. We evaluate \textit{Clustrophile~2} through a user study with 12
data scientists, who used our tool to explore and interpret sub-cohorts in a
dataset of Parkinson&apos;s disease patients. Results suggest that
\textit{Clustrophile~2} improves the speed and effectiveness of exploratory
clustering analysis for both experts and non-experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavallo_M/0/1/0/all/0/1&quot;&gt;Marco Cavallo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1&quot;&gt;&amp;#xc7;a&amp;#x11f;atay Demiralp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00912">
<title>Fast Directional Self-Attention Mechanism. (arXiv:1805.00912v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00912</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a self-attention mechanism, dubbed &quot;fast
directional self-attention (Fast-DiSA)&quot;, which is a fast and light extension of
&quot;directional self-attention (DiSA)&quot;. The proposed Fast-DiSA performs as
expressively as the original DiSA but only uses much less computation time and
memory, in which 1) both token2token and source2token dependencies are modeled
by a joint compatibility function designed for a hybrid of both dot-product and
multi-dim ways; 2) both multi-head and multi-dim attention combined with
bi-directional temporal information captured by multiple positional masks are
in consideration without heavy time and memory consumption appearing in the
DiSA. The experiment results show that the proposed Fast-DiSA can achieve
state-of-the-art performance as fast and memory-friendly as CNNs. The code for
Fast-DiSA is released at
\url{https://github.com/taoshen58/DiSAN/tree/master/Fast-DiSA}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1&quot;&gt;Tao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1&quot;&gt;Guodong Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengqi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02114">
<title>An Accelerated Approach to Safely and Efficiently Test Pre-Production Autonomous Vehicles on Public Streets. (arXiv:1805.02114v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1805.02114</link>
<description rdf:parseType="Literal">&lt;p&gt;Various automobile and mobility companies, for instance Ford, Uber and Waymo,
are currently testing their pre-produced autonomous vehicle (AV) fleets on the
public roads. However, due to rareness of the safety-critical cases and,
effectively, unlimited number of possible traffic scenarios, these on-road
testing efforts have been acknowledged as tedious, costly, and risky. In this
study, we propose Accelerated De- ployment framework to safely and efficiently
estimate the AVs performance on public streets. We showed that by appropriately
addressing the gradual accuracy improvement and adaptively selecting meaningful
and safe environment under which the AV is deployed, the proposed framework
yield to highly accurate estimation with much faster evaluation time, and more
importantly, lower deployment risk. Our findings provide an answer to the
currently heated and active discussions on how to properly test AV performance
on public roads so as to achieve safe, efficient, and statistically-reliable
testing framework for AV technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arief_M/0/1/0/all/0/1&quot;&gt;Mansur Arief&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glynn_P/0/1/0/all/0/1&quot;&gt;Peter Glynn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07429">
<title>Designing communication systems via iterative improvement: error correction coding with Bayes decoder and codebook optimized for source symbol error. (arXiv:1805.07429v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07429</link>
<description rdf:parseType="Literal">&lt;p&gt;In error correction coding (ECC), the typical error metric is the bit error
rate (BER) which measures the number of bit errors. For this metric, the
positions of the bits are not relevant to the decoding, and in many noise
models, not relevant to the BER either. In many applications this is
unsatisfactory as typically all bits are not equal and have different
significance. We look at ECC from a Bayesian perspective and introduce Bayes
estimators with general loss functions to take into account the bit
significance. We propose ECC schemes that optimize this error metric. As the
problem is highly nonlinear, traditional ECC construction techniques are not
applicable. Using exhausive search is cost prohibitive, and thus we use
iterative improvement search techniques to find good codebooks. We provide
numerical experiments to show that they can be superior to classical linear
block codes such as Hamming codes and decoding methods such as minimum distance
decoding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chai Wah Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07431">
<title>Can machine learning identify interesting mathematics? An exploration using empirically observed laws. (arXiv:1805.07431v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07431</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the possibility of using machine learning to identify interesting
mathematical structures by using certain quantities that serve as fingerprints.
In particular, we extract features from integer sequences using two empirical
laws: Benford&apos;s law and Taylor&apos;s law and experiment with various classifiers to
identify whether a sequence is, for example, nice, important, multiplicative,
easy to compute or related to primes or palindromes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chai Wah Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07683">
<title>Learning Graph-Level Representations with Gated Recurrent Neural Networks. (arXiv:1805.07683v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07683</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently a variety of methods have been developed to encode graphs into
low-dimensional vectors that can be easily exploited by machine learning
algorithms. The majority of these methods start by embedding the graph nodes
into a low-dimensional vector space, followed by using some scheme to aggregate
the node embeddings. In this work, we develop a new approach to learn
graph-level representations, which includes a combination of unsupervised and
supervised learning components. We start by learning a set of node
representations in an unsupervised fashion. Graph nodes are mapped into node
sequences sampled from random walk approaches approximated by the
Gumbel-Softmax distribution. Recurrent neural network (RNN) units are modified
to accommodate both the node representations as well as their neighborhood
information. Experiments on standard graph classification benchmarks
demonstrate that our proposed approach achieves superior or comparable
performance relative to the state-of-the-art algorithms in terms of convergence
speed and classification accuracy. We further illustrate the effectiveness of
the different components used by our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yu Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+JaJa_J/0/1/0/all/0/1&quot;&gt;Joseph F. JaJa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09328">
<title>Diversified Late Acceptance Search. (arXiv:1806.09328v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.09328</link>
<description rdf:parseType="Literal">&lt;p&gt;The well-known Late Acceptance Hill Climbing (LAHC) search aims to overcome
the main downside of traditional Hill Climbing (HC) search, which is often
quickly trapped in a local optimum due to strictly accepting only non-worsening
moves within each iteration. In contrast, LAHC also accepts worsening moves, by
keeping a circular array of fitness values of previously visited solutions and
comparing the fitness values of candidate solutions against the least recent
element in the array. While this straightforward strategy has proven effective,
there are nevertheless situations where LAHC can unfortunately behave in a
similar manner to HC. For example, when a new local optimum is found, often the
same fitness value is stored many times in the array. To address this
shortcoming, we propose new acceptance and replacement strategies to take into
account worsening, improving, and sideways movement scenarios with the aim to
improve the diversity of values in the array. Compared to LAHC, the proposed
Diversified Late Acceptance Search approach is shown to lead to better quality
solutions that are obtained with a lower number of iterations on benchmark
Travelling Salesman Problems and Quadratic Assignment Problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namazi_M/0/1/0/all/0/1&quot;&gt;Majid Namazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanderson_C/0/1/0/all/0/1&quot;&gt;Conrad Sanderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newton_M/0/1/0/all/0/1&quot;&gt;M.A. Hakim Newton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polash_M/0/1/0/all/0/1&quot;&gt;M.M.A. Polash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sattar_A/0/1/0/all/0/1&quot;&gt;Abdul Sattar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09769">
<title>Plenoptic Monte Carlo Object Localization for Robot Grasping under Layered Translucency. (arXiv:1806.09769v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1806.09769</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to fully function in human environments, robot perception will need
to account for the uncertainty caused by translucent materials. Translucency
poses several open challenges in the form of transparent objects (e.g.,
drinking glasses), refractive media (e.g., water), and diffuse partial
occlusions (e.g., objects behind stained glass panels). This paper presents
Plenoptic Monte Carlo Localization (PMCL) as a method for localizing object
poses in the presence of translucency using plenoptic (light-field)
observations. We propose a new depth descriptor, the Depth Likelihood Volume
(DLV), and its use within a Monte Carlo object localization algorithm. We
present results of localizing and manipulating objects with translucent
materials and objects occluded by layers of translucency. Our PMCL
implementation uses observations from a Lytro first generation light field
camera to allow a Michigan Progress Fetch robot to perform grasping.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zheming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenkins_O/0/1/0/all/0/1&quot;&gt;Odest Chadwicke Jenkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10096">
<title>Towards a Deep Unified Framework for Nuclear Reactor Perturbation Analysis. (arXiv:1807.10096v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.10096</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we take the first steps towards a novel unified framework for
the analysis of perturbations in both the Time and Frequency domains. The
identification of type and source of such perturbations is fundamental for
monitoring reactor cores and guarantee safety while running at nominal
conditions. A 3D Convolutional Neural Network (3D-CNN) was employed to analyse
perturbations happening in the frequency domain, such as an absorber of
variable strength or propagating perturbation. Recurrent neural networks (RNN),
specifically Long Short-Term Memory (LSTM) networks were used to study signal
sequences related to perturbations induced in the time domain, including the
vibrations of fuel assemblies and the fluctuations of thermal-hydraulic
parameters at the inlet of the reactor coolant loops. 512 dimensional
representations were extracted from the 3D-CNN and LSTM architectures, and used
as input to a fused multi-sigmoid classification layer to recognise the
perturbation type. If the perturbation is in the frequency domain, a separate
fully-connected layer utilises said representations to regress the coordinates
of its source. The results showed that the perturbation type can be recognised
with high accuracy in all cases, and frequency domain scenario sources can be
localised with high precision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_F/0/1/0/all/0/1&quot;&gt;Fabio De Sousa Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caliva_F/0/1/0/all/0/1&quot;&gt;Francesco Caliva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chionis_D/0/1/0/all/0/1&quot;&gt;Dionysios Chionis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dokhane_A/0/1/0/all/0/1&quot;&gt;Abdelhamid Dokhane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mylonakis_A/0/1/0/all/0/1&quot;&gt;Antonios Mylonakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demaziere_C/0/1/0/all/0/1&quot;&gt;Christophe Demaziere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leontidis_G/0/1/0/all/0/1&quot;&gt;Georgios Leontidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollias_S/0/1/0/all/0/1&quot;&gt;Stefanos Kollias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09062">
<title>Cognitive Consistency Routing Algorithm of Capsule-network. (arXiv:1808.09062v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1808.09062</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Neural Networks (ANNs) are computational models inspired by the
central nervous system (especially the brain) of animals and are used to
estimate or generate unknown approximation functions relied on large amounts of
inputs. Capsule Neural Network (Sabour S, et al.[2017]) is a novel structure of
Convolutional Neural Networks which simulates the visual processing system of
human brain. In this paper, we introduce psychological theories which called
Cognitive Consistency to optimize the routing algorithm of Capsnet to make it
more close to the work pattern of human brain. It has been shown in the
experiment that a progress had been made compared with the baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huayu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00969">
<title>A Deeper Insight into the UnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation. (arXiv:1809.00969v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1809.00969</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an unsupervised deep learning framework called UnDEMoN
for estimating dense depth map and 6-DoF camera pose information directly from
monocular images. The proposed network is trained using unlabeled monocular
stereo image pairs and is shown to provide superior performance in depth and
ego-motion estimation compared to the existing state-of-the-art. These
improvements are achieved by introducing a new objective function that aims to
minimize spatial as well as temporal reconstruction losses simultaneously.
These losses are defined using bi-linear sampling kernel and penalized using
the Charbonnier penalty function. The objective function, thus created,
provides robustness to image gradient noises thereby improving the overall
estimation accuracy without resorting to any coarse to fine strategies which
are currently prevalent in the literature. Another novelty lies in the fact
that we combine a disparity-based depth estimation network with a pose
estimation network to obtain absolute scale-aware 6 DOF Camera pose and
superior depth map. The effectiveness of the proposed approach is demonstrated
through performance comparison with the existing supervised and unsupervised
methods on the KITTI driving dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+V_M/0/1/0/all/0/1&quot;&gt;Madhu Babu V&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumder_A/0/1/0/all/0/1&quot;&gt;Anima Majumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_K/0/1/0/all/0/1&quot;&gt;Kaushik Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Swagat Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01479">
<title>UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification. (arXiv:1809.01479v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1809.01479</link>
<description rdf:parseType="Literal">&lt;p&gt;The Fact Extraction and VERification (FEVER) shared task was launched to
support the development of systems able to verify claims by extracting
supporting or refuting facts from raw text. The shared task organizers provide
a large-scale dataset for the consecutive steps involved in claim verification,
in particular, document retrieval, fact extraction, and claim classification.
In this paper, we present our claim verification pipeline approach, which,
according to the preliminary results, scored third in the shared task, out of
23 competing systems. For the document retrieval, we implemented a new entity
linking approach. In order to be able to rank candidate facts and classify a
claim on the basis of several selected facts, we introduce two extensions to
the Enhanced LSTM (ESIM).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanselowski_A/0/1/0/all/0/1&quot;&gt;Andreas Hanselowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zile Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sorokin_D/0/1/0/all/0/1&quot;&gt;Daniil Sorokin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiller_B/0/1/0/all/0/1&quot;&gt;Benjamin Schiller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_C/0/1/0/all/0/1&quot;&gt;Claudia Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1&quot;&gt;Iryna Gurevych&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02630">
<title>Constrained Generation of Semantically Valid Graphs via Regularizing Variational Autoencoders. (arXiv:1809.02630v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02630</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models have achieved remarkable success in various data
domains, including images, time series, and natural languages. There remain,
however, substantial challenges for combinatorial structures, including graphs.
One of the key challenges lies in the difficulty of ensuring semantic validity
in context. For examples, in molecular graphs, the number of bonding-electron
pairs must not exceed the valence of an atom; whereas in protein interaction
networks, two proteins may be connected only when they belong to the same or
correlated gene ontology terms. These constraints are not easy to be
incorporated into a generative model. In this work, we propose a regularization
framework for variational autoencoders as a step toward semantic validity. We
focus on the matrix representation of graphs and formulate penalty terms that
regularize the output distribution of the decoder to encourage the satisfaction
of validity constraints. Experimental results confirm a much higher likelihood
of sampling valid graphs in our approach, compared with others reported in the
literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengfei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Cao Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02652">
<title>Are You Sure YouWant To Do That? Classification with Verification. (arXiv:1809.02652v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02652</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification systems typically act in isolation, meaning they are required
to implicitly memorize the characteristics of all candidate classes in order to
classify. The cost of this is increased memory usage and poor sample
efficiency. We propose a model which instead verifies using reference images
during the classification process, reducing the burden of memorization. The
model uses iterative nondifferentiable queries in order to classify an image.
We demonstrate that such a model is feasible to train and can match baseline
accuracy while being more parameter efficient. However, we show that finding
the correct balance between image recognition and verification is essential to
pushing the model towards desired behavior, suggesting that a pipeline of
recognition followed by verification is a more promising approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1&quot;&gt;Harris Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhury_A/0/1/0/all/0/1&quot;&gt;Atef Chaudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1&quot;&gt;Kevin Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02665">
<title>DreamNLP: Novel NLP System for Clinical Report Metadata Extraction using Count Sketch Data Streaming Algorithm: Preliminary Results. (arXiv:1809.02665v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02665</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting information from electronic health records (EHR) is a challenging
task since it requires prior knowledge of the reports and some natural language
processing algorithm (NLP). With the growing number of EHR implementations,
such knowledge is increasingly challenging to obtain in an efficient manner. We
address this challenge by proposing a novel methodology to analyze large sets
of EHRs using a modified Count Sketch data streaming algorithm termed DreamNLP.
By using DreamNLP, we generate a dictionary of frequently occurring terms or
heavy hitters in the EHRs using low computational memory compared to
conventional counting approach other NLP programs use. We demonstrate the
extraction of the most important breast diagnosis features from the EHRs in a
set of patients that underwent breast imaging. Based on the analysis,
extraction of these terms would be useful for defining important features for
downstream tasks such as machine learning for precision medicine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Sanghyun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivkin_N/0/1/0/all/0/1&quot;&gt;Nikita Ivkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1&quot;&gt;Vladimir Braverman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_M/0/1/0/all/0/1&quot;&gt;Michael A. Jacobs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02707">
<title>Thompson Sampling for Combinatorial Multi-armed Bandit with Probabilistically Triggered Arms. (arXiv:1809.02707v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02707</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze the regret of combinatorial Thompson sampling (CTS) for the
combinatorial multi-armed bandit with probabilistically triggered arms under
the semi-bandit feedback setting. We assume that the learner has access to an
exact optimization oracle but does not know the expected base arm outcomes
beforehand. When the expected reward function is Lipschitz continuous in the
expected base arm outcomes, we derive $O(\sum_{i =1}^m \log T / (p_i
\Delta_i))$ regret bound for CTS, where $m$ denotes the number of base arms,
$p_i$ denotes the minimum non-zero triggering probability of base arm $i$ and
$\Delta_i$ denotes the minimum suboptimality gap of base arm $i$. We also show
that CTS outperforms combinatorial upper confidence bound (CUCB) via numerical
experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huyuk_A/0/1/0/all/0/1&quot;&gt;Alihan H&amp;#xfc;y&amp;#xfc;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tekin_C/0/1/0/all/0/1&quot;&gt;Cem Tekin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02728">
<title>Coupled IGMM-GANs for deep multimodal anomaly detection in human mobility data. (arXiv:1809.02728v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02728</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting anomalous activity in human mobility data has a number of
applications including road hazard sensing, telematic based insurance, and
fraud detection in taxi services and ride sharing. In this paper we address two
challenges that arise in the study of anomalous human trajectories: 1) a lack
of ground truth data on what defines an anomaly and 2) the dependence of
existing methods on significant pre-processing and feature engineering. While
generative adversarial networks seem like a natural fit for addressing these
challenges, we find that existing GAN based anomaly detection algorithms
perform poorly due to their inability to handle multimodal patterns. For this
purpose we introduce an infinite Gaussian mixture model coupled with
(bi-directional) generative adversarial networks, IGMM-GAN, that is able to
generate synthetic, yet realistic, human mobility data and simultaneously
facilitates multimodal anomaly detection. Through estimation of a generative
probability density on the space of human trajectories, we are able to generate
realistic synthetic datasets that can be used to benchmark existing anomaly
detection methods. The estimated multimodal density also allows for a natural
definition of outlier that we use for detecting anomalous trajectories. We
illustrate our methodology and its improvement over existing GAN anomaly
detection on several human mobility datasets, along with MNIST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gray_K/0/1/0/all/0/1&quot;&gt;Kathryn Gray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smolyak_D/0/1/0/all/0/1&quot;&gt;Daniel Smolyak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badirli_S/0/1/0/all/0/1&quot;&gt;Sarkhan Badirli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohler_G/0/1/0/all/0/1&quot;&gt;George Mohler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02745">
<title>Molecular Hypergraph Grammar with its Application to Molecular Optimization. (arXiv:1809.02745v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02745</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper is concerned with a molecular optimization framework using
variational autoencoders (VAEs). In this paradigm, VAE allows us to convert a
molecular graph into/from its latent continuous vector, and therefore, the
molecular optimization problem can be solved by continuous optimization
techniques. One of the longstanding issues in this area is that it is difficult
to always generate valid molecules. The very recent work called the junction
tree variational autoencoder (JT-VAE) successfully solved this issue by
generating a molecule fragment-by-fragment. While it achieves the
state-of-the-art performance, it requires several neural networks to be
trained, which predict which atoms are used to connect fragments and
stereochemistry of each bond. In this paper, we present a molecular hypergraph
grammar variational autoencoder (MHG-VAE), which uses a single VAE to address
the issue. Our idea is to develop a novel graph grammar for molecular graphs
called molecular hypergraph grammar (MHG), which can specify the connections
between fragments and the stereochemistry on behalf of neural networks. This
capability allows us to address the issue using only a single VAE. We
empirically demonstrate the effectiveness of MHG-VAE over existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kajino_H/0/1/0/all/0/1&quot;&gt;Hiroshi Kajino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02811">
<title>Multi-label Classification of User Reactions in Online News. (arXiv:1809.02811v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02811</link>
<description rdf:parseType="Literal">&lt;p&gt;The increase in the number of Internet users and the strong interaction
brought by Web 2.0 made the Opinion Mining an important task in the area of
natural language processing. Although several methods are capable of performing
this task, few use multi-label classification, where there is a group of true
labels for each example. This type of classification is useful for situations
where the opinions are analyzed from the perspective of the reader. Recently,
Deep Learning has been registering the state of the art in several single-label
problems. This paper discuss the efficiency of the Long Short-Term Memory
compared to traditional multi-label classification approaches. To do that,
extensive tests were carried out on two news corpora written in Brazilian
Portuguese annotated with reactions. A new corpus called BFRC-PT is presented.
In the tests performed, the highest number of correct predictions was obtained
with the Classifier Chains method combined with the Random Forest algorithm.
When considering the class distribution, the best results were obtained with
the Binary Relevance method combined with the LSTM and Random Forest
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Curi_Z/0/1/0/all/0/1&quot;&gt;Zacarias Curi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Britto_A/0/1/0/all/0/1&quot;&gt;Alceu de Souza Britto Jr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paraiso_E/0/1/0/all/0/1&quot;&gt;Emerson Cabrera Paraiso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02840">
<title>Neural Guided Constraint Logic Programming for Program Synthesis. (arXiv:1809.02840v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02840</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthesizing programs using example input/outputs is a classic problem in
artificial intelligence. We present a method for solving Programming By Example
(PBE) problems by using a neural model to guide the search of a constraint
logic programming system called miniKanren. Crucially, the neural model uses
miniKanren&apos;s internal representation as input; miniKanren represents a PBE
problem as recursive constraints imposed by the provided examples. We explore
Recurrent Neural Network and Graph Neural Network models. We contribute a
modified miniKanren, drivable by an external agent, available at
https://github.com/xuexue/neuralkanren. We show that our neural-guided approach
using constraints can synthesize programs faster in many cases, and
importantly, can generalize to larger problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lisa Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenblatt_G/0/1/0/all/0/1&quot;&gt;Gregory Rosenblatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1&quot;&gt;Ethan Fetaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1&quot;&gt;Renjie Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byrd_W/0/1/0/all/0/1&quot;&gt;William E. Byrd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Might_M/0/1/0/all/0/1&quot;&gt;Mathew Might&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02860">
<title>Identifying The Most Informative Features Using A Structurally Interacting Elastic Net. (arXiv:1809.02860v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02860</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature selection can efficiently identify the most informative features with
respect to the target feature used in training. However, state-of-the-art
vector-based methods are unable to encapsulate the relationships between
feature samples into the feature selection process, thus leading to significant
information loss. To address this problem, we propose a new graph-based
structurally interacting elastic net method for feature selection.
Specifically, we commence by constructing feature graphs that can incorporate
pairwise relationship between samples. With the feature graphs to hand, we
propose a new information theoretic criterion to measure the joint relevance of
different pairwise feature combinations with respect to the target feature
graph representation. This measure is used to obtain a structural interaction
matrix where the elements represent the proposed information theoretic measure
between feature pairs. We then formulate a new optimization model through the
combination of the structural interaction matrix and an elastic net regression
model for the feature subset selection problem. This allows us to a) preserve
the information of the original vectorial space, b) remedy the information loss
of the original feature space caused by using graph representation, and c)
promote a sparse solution and also encourage correlated features to be
selected. Because the proposed optimization problem is non-convex, we develop
an efficient alternating direction multiplier method (ADMM) to locate the
optimal solutions. Extensive experiments on various datasets demonstrate the
effectiveness of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Lixin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lu Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhihong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hancock_E/0/1/0/all/0/1&quot;&gt;Edwin R. Hancock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02864">
<title>Online Adaptive Methods, Universality and Acceleration. (arXiv:1809.02864v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02864</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel method for convex unconstrained optimization that, without
any modifications, ensures: (i) accelerated convergence rate for smooth
objectives, (ii) standard convergence rate in the general (non-smooth) setting,
and (iii) standard convergence rate in the stochastic optimization setting. To
the best of our knowledge, this is the first method that simultaneously applies
to all of the above settings. At the heart of our method is an adaptive
learning rate rule that employs importance weights, in the spirit of adaptive
online learning algorithms (Duchi et al., 2011; Levy, 2017), combined with an
update that linearly couples two sequences, in the spirit of (Allen-Zhu and
Orecchia, 2017). An empirical examination of our method demonstrates its
applicability to the above mentioned scenarios and corroborates our theoretical
findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1&quot;&gt;Kfir Y. Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yurtsever_A/0/1/0/all/0/1&quot;&gt;Alp Yurtsever&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02880">
<title>PhaseLink: A Deep Learning Approach to Seismic Phase Association. (arXiv:1809.02880v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02880</link>
<description rdf:parseType="Literal">&lt;p&gt;Seismic phase association is a fundamental task in seismology that pertains
to linking together phase detections on different sensors that originate from a
common earthquake. It is widely employed to detect earthquakes on permanent and
temporary seismic networks, and underlies most seismicity catalogs produced
around the world. This task can be challenging because the number of sources is
unknown, events frequently overlap in time, or can occur simultaneously in
different parts of a network. We present PhaseLink, a framework based on recent
advances in deep learning for grid-free earthquake phase association. Our
approach learns to link phases together that share a common origin, and is
trained entirely on tens of millions of synthetic sequences of P- and S-wave
arrival times generated using a simple 1D velocity model. Our approach is
simple to implement for any tectonic regime, suitable for real-time processing,
and can naturally incorporate errors in arrival time picks. Rather than tuning
a set of ad hoc hyperparameters to improve performance, PhaseLink can be
improved by simply adding examples of problematic cases to the training
dataset. We demonstrate the state-of-the-art performance of PhaseLink on a
challenging recent sequence from southern California, and synthesized sequences
from Japan designed to test the point at which the method fails. These tests
show that PhaseLink can precisely associate P- and S-picks to events that are
separated by ~12 seconds in origin time. This approach is expected to improve
the resolution of seismicity catalogs, add stability to real-time seismic
monitoring, and streamline automated processing of large seismic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_Z/0/1/0/all/0/1&quot;&gt;Zachary E. Ross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yisong Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_M/0/1/0/all/0/1&quot;&gt;Men-Andrin Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauksson_E/0/1/0/all/0/1&quot;&gt;Egill Hauksson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heaton_T/0/1/0/all/0/1&quot;&gt;Thomas H. Heaton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02918">
<title>Towards Query Efficient Black-box Attacks: An Input-free Perspective. (arXiv:1809.02918v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.02918</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have highlighted that deep neural networks (DNNs) are
vulnerable to adversarial attacks, even in a black-box scenario. However, most
of the existing black-box attack algorithms need to make a huge amount of
queries to perform attacks, which is not practical in the real world. We note
one of the main reasons for the massive queries is that the adversarial example
is required to be visually similar to the original image, but in many cases,
how adversarial examples look like does not matter much. It inspires us to
introduce a new attack called \emph{input-free} attack, under which an
adversary can choose an arbitrary image to start with and is allowed to add
perceptible perturbations on it. Following this approach, we propose two
techniques to significantly reduce the query complexity. First, we initialize
an adversarial example with a gray color image on which every pixel has roughly
the same importance for the target model. Then we shrink the dimension of the
attack space by perturbing a small region and tiling it to cover the input
image. To make our algorithm more effective, we stabilize a projected gradient
ascent algorithm with momentum, and also propose a heuristic approach for
region size selection. Through extensive experiments, we show that with only
1,701 queries on average, we can perturb a gray image to any target class of
ImageNet with a 100\% success rate on InceptionV3. Besides, our algorithm has
successfully defeated two real-world systems, the Clarifai food detection API
and the Baidu Animal Identification API.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yali Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jun Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02925">
<title>Addressing Sample Inefficiency and Reward Bias in Inverse Reinforcement Learning. (arXiv:1809.02925v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02925</link>
<description rdf:parseType="Literal">&lt;p&gt;The Generative Adversarial Imitation Learning (GAIL) framework from Ho &amp;amp;
Ermon (2016) is known for being surprisingly sample efficient in terms of
demonstrations provided by an expert policy. However, the algorithm requires a
significantly larger number of policy interactions with the environment in
order to imitate the expert. In this work we address this problem by proposing
a sample efficient algorithm for inverse reinforcement learning that
incorporates both off-policy reinforcement learning and adversarial imitation
learning. We also show that GAIL has a number of biases associated with the
choice of reward function, which can unintentionally encode prior knowledge of
some tasks, and prevent learning in others. We address these shortcomings by
analyzing the issue and correcting invalid assumptions used when defining the
learned reward function. We demonstrate that our algorithm achieves
state-of-the-art performance for an inverse reinforcement learning framework on
a variety of standard benchmark tasks, and from demonstrations provided from
both learned agents and human experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kostrikov_I/0/1/0/all/0/1&quot;&gt;Ilya Kostrikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_K/0/1/0/all/0/1&quot;&gt;Kumar Krishna Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1&quot;&gt;Jonathan Tompson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02963">
<title>Variational Approximation Accuracy in Bayesian Non-negative Matrix Factorization. (arXiv:1809.02963v1 [math.ST])</title>
<link>http://arxiv.org/abs/1809.02963</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-negative matrix factorization (NMF) is a knowledge discovery method that
is used for many fields, besides, its variational inference and Gibbs sampling
method are also well-known. However, the variational approximation accuracy is
not yet clarified, since NMF is not statistically regular and the prior used in
the variational Bayesian NMF (VBNMF) has zero or divergence points. In this
paper, using algebraic geometrical methods, we theoretically analyze the
difference of the negative log evidence/marginal likelihood (free energy)
between VBNMF and Bayesian NMF, and give a lower bound of the approximation
accuracy, asymptotically. The results quantitatively show how well the VBNMF
algorithm can approximate Bayesian NMF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hayashi_N/0/1/0/all/0/1&quot;&gt;Naoki Hayashi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03006">
<title>Performance Metrics (Error Measures) in Machine Learning Regression, Forecasting and Prognostics: Properties and Typology. (arXiv:1809.03006v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1809.03006</link>
<description rdf:parseType="Literal">&lt;p&gt;Performance metrics (error measures) are vital components of the evaluation
frameworks in various fields. The intention of this study was to overview of a
variety of performance metrics and approaches to their classification. The main
goal of the study was to develop a typology that will help to improve our
knowledge and understanding of metrics and facilitate their selection in
machine learning regression, forecasting and prognostics. Based on the analysis
of the structure of numerous performance metrics, we propose a framework of
metrics which includes four (4) categories: primary metrics, extended metrics,
composite metrics, and hybrid sets of metrics. The paper identified three (3)
key components (dimensions) that determine the structure and properties of
primary metrics: method of determining point distance, method of normalization,
method of aggregation of point distances over a data set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Botchkarev_A/0/1/0/all/0/1&quot;&gt;Alexei Botchkarev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03008">
<title>Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability. (arXiv:1809.03008v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03008</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the concept of co-design in the context of neural network
verification. Specifically, we aim to train deep neural networks that not only
are robust to adversarial perturbations but also whose robustness can be
verified more easily. To this end, we identify two properties of network models
- weight sparsity and so-called ReLU stability - that turn out to significantly
impact the complexity of the corresponding verification task. We demonstrate
that improving weight sparsity alone already enables us to turn computationally
intractable verification problems into tractable ones. Then, improving ReLU
stability leads to an additional 4-13x speedup in verification times. An
important feature of our methodology is its &quot;universality,&quot; in the sense that
it can be used with a broad range of training procedures and verification
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1&quot;&gt;Kai Y. Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tjeng_V/0/1/0/all/0/1&quot;&gt;Vincent Tjeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafiullah_N/0/1/0/all/0/1&quot;&gt;Nur Muhammad Shafiullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1&quot;&gt;Aleksander Madry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03018">
<title>Leveraging Elastic Demand for Forecasting. (arXiv:1809.03018v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.03018</link>
<description rdf:parseType="Literal">&lt;p&gt;Demand variance can result in a mismatch between planned supply and actual
demand. Demand shaping strategies such as pricing can be used to shift elastic
demand to reduce the imbalance. In this work, we propose to consider elastic
demand in the forecasting phase. We present a method to reallocate the
historical elastic demand to reduce variance, thus making forecasting and
supply planning more effective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Houtao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krishnan_G/0/1/0/all/0/1&quot;&gt;Ganesh Krishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Ji Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Dong Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03041">
<title>An iterative method for classification of binary data. (arXiv:1809.03041v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.03041</link>
<description rdf:parseType="Literal">&lt;p&gt;In today&apos;s data driven world, storing, processing, and gleaning insights from
large-scale data are major challenges. Data compression is often required in
order to store large amounts of high-dimensional data, and thus, efficient
inference methods for analyzing compressed data are necessary. Building on a
recently designed simple framework for classification using binary data, we
demonstrate that one can improve classification accuracy of this approach
through iterative applications whose output serves as input to the next
application. As a side consequence, we show that the original framework can be
used as a data preprocessing step to improve the performance of other methods,
such as support vector machines. For several simple settings, we showcase the
ability to obtain theoretical guarantees for the accuracy of the iterative
classification method. The simplicity of the underlying classification
framework makes it amenable to theoretical analysis and studying this approach
will hopefully serve as a step toward developing theory for more sophisticated
deep learning technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Molitor_D/0/1/0/all/0/1&quot;&gt;Denali Molitor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Needell_D/0/1/0/all/0/1&quot;&gt;Deanna Needell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03045">
<title>Randomized Iterative Algorithms for Fisher Discriminant Analysis. (arXiv:1809.03045v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.03045</link>
<description rdf:parseType="Literal">&lt;p&gt;Fisher discriminant analysis (FDA) is a widely used method for classification
and dimensionality reduction. When the number of predictor variables greatly
exceeds the number of observations, one of the alternatives for conventional
FDA is regularized Fisher discriminant analysis (RFDA). In this paper, we
present a simple, iterative, sketching-based algorithm for RFDA that comes with
provable accuracy guarantees when compared to the conventional approach. Our
analysis builds upon two simple structural results that boil down to randomized
matrix multiplication, a fundamental and well-understood primitive of
randomized linear algebra. We analyze the behavior of RFDA when the ridge
leverage and the standard leverage scores are used to select predictor
variables and we prove that accurate approximations can be achieved by a sample
whose size depends on the effective degrees of freedom of the RFDA problem. Our
results yield significant improvements over existing approaches and our
empirical evaluations support our theoretical analyses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chowdhury_A/0/1/0/all/0/1&quot;&gt;Agniva Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiasen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Drineas_P/0/1/0/all/0/1&quot;&gt;Petros Drineas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03048">
<title>Clustering of graph vertex subset via Krylov subspace model reduction. (arXiv:1809.03048v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03048</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering via graph-Laplacian spectral imbedding is ubiquitous in data
science and machine learning. However, it becomes less efficient for large data
sets due to two factors. First, computing the partial eigendecomposition of the
graph-Laplacian typically requires a large Krylov subspace. Second, after the
spectral imbedding is complete, the clustering is typically performed with
various relaxations of k-means, which may become prone to getting stuck in
local minima and scale poorly in terms of computational cost for large data
sets. Here we propose two novel algorithms for spectral clustering of a subset
of the graph vertices (target subset) based on the theory of model order
reduction. They rely on realizations of a reduced order model (ROM) that
accurately approximates the diffusion transfer function of the original graph
for inputs and outputs restricted to the target subset. While our focus is
limited to this subset, our algorithms produce its clustering that is
consistent with the overall structure of the graph. Moreover, working with a
small target subset reduces greatly the required dimension of Krylov subspace
and allows to exploit the approximations of k-means in the regimes when they
are most robust and efficient, as verified by the numerical experiments. There
are several uses for our algorithms. First, they can be employed on their own
to clusterize a representative subset in cases when the full graph clustering
is either infeasible or not required. Second, they may be used for quality
control. Third, as they drastically reduce the problem size, they enable the
application of more powerful approximations of k-means like those based on
semi-definite programming (SDP) instead of the conventional Lloyd&apos;s algorithm.
Finally, they can be used as building blocks of a divide-and-conquer algorithm
for the full graph clustering. The latter will be reported in a separate
article.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Druskin_V/0/1/0/all/0/1&quot;&gt;Vladimir Druskin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mamonov_A/0/1/0/all/0/1&quot;&gt;Alexander V. Mamonov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaslavsky_M/0/1/0/all/0/1&quot;&gt;Mikhail Zaslavsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03060">
<title>Active Inverse Reward Design. (arXiv:1809.03060v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03060</link>
<description rdf:parseType="Literal">&lt;p&gt;Reward design, the problem of selecting an appropriate reward function for an
AI system, is both critically important, as it encodes the task the system
should perform, and challenging, as it requires reasoning about and
understanding the agent&apos;s environment in detail. AI practitioners often iterate
on the reward function for their systems in a trial-and-error process to get
their desired behavior. Inverse reward design (IRD) is a preference inference
method that infers a true reward function from an observed, possibly
misspecified, proxy reward function. This allows the system to determine when
it should trust its observed reward function and respond appropriately. This
has been shown to avoid problems in reward design such as negative side-effects
(omitting a seemingly irrelevant but important aspect of the task) and reward
hacking (learning to exploit unanticipated loopholes). In this paper, we
actively select the $\textit{set of proxy reward functions}$ available to the
designer. This improves the quality of inference and simplifies the associated
reward design problem. We present two types of queries: discrete queries, where
the system designer chooses from a discrete set of reward functions, and
feature queries, where the system queries the designer for weights on a small
set of features. We evaluate this approach with experiments in a personal
shopping assistant domain and a 2D navigation domain. We find that our approach
leads to reduced regret at test time compared with vanilla IRD. Our results
indicate that actively selecting the set of available reward functions is a
promising direction to improve the efficiency and effectiveness of reward
design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Mindermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rohin Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gleave_A/0/1/0/all/0/1&quot;&gt;Adam Gleave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1&quot;&gt;Dylan Hadfield-Menell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03062">
<title>Analysis of the generalization error: Empirical risk minimization over deep artificial neural networks overcomes the curse of dimensionality in the numerical approximation of Black-Scholes partial differential equations. (arXiv:1809.03062v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03062</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of new classification and regression algorithms based on
empirical risk minimization (ERM) over deep neural network hypothesis classes,
coined Deep Learning, revolutionized the area of artificial intelligence,
machine learning, and data analysis. More recently, these methods have been
applied to the numerical solution of high dimensional PDEs with great success.
In particular, recent simulations indicate that deep learning based algorithms
are capable of overcoming the curse of dimensionality for the numerical
solution of linear Kolmogorov PDEs. Kolmogorov PDEs have been widely used in
models from engineering, finance, and the natural sciences. Nearly all
approximation methods for Kolmogorov PDEs in the literature suffer under the
curse of dimensionality. By contrast, in recent work by some of the authors it
was shown that deep ReLU neural networks are capable of approximating solutions
of Kolmogorov PDEs without incurring the curse of dimensionality. The present
paper considerably strengthens these results by providing an analysis of the
generalization error. In particular we show that for Kolmogorov PDEs with
affine drift and diffusion coefficients and a given accuracy $\varepsilon&amp;gt;0$,
ERM over deep neural network hypothesis classes of size scaling polynomially in
the dimension $d$ and $\varepsilon^{-1}$ and with a number of training samples
scaling polynomially in the dimension $d$ and $\varepsilon^{-1}$ approximates
the solution of the Kolmogorov PDE to within accuracy $\varepsilon$ with high
probability. We conclude that ERM over deep neural network hypothesis classes
breaks the curse of dimensionality for the numerical solution of linear
Kolmogorov PDEs with affine drift and diffusion coefficients. To the best of
our knowledge this is the first rigorous mathematical result that proves the
efficiency of deep learning methods for high dimensional problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berner_J/0/1/0/all/0/1&quot;&gt;Julius Berner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grohs_P/0/1/0/all/0/1&quot;&gt;Philipp Grohs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jentzen_A/0/1/0/all/0/1&quot;&gt;Arnulf Jentzen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03063">
<title>The Curse of Concentration in Robust Learning: Evasion and Poisoning Attacks from Concentration of Measure. (arXiv:1809.03063v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03063</link>
<description rdf:parseType="Literal">&lt;p&gt;Many modern machine learning classifiers are shown to be vulnerable to
adversarial perturbations of the instances that can &quot;evade&quot; the classifier and
get misclassified. Despite a massive amount of work focusing on making
classifiers robust, the task seems quite challenging. In this work, through a
theoretical study, we investigate the adversarial risk and robustness of
classifiers and draw a connection to the well-known phenomenon of
&quot;concentration of measure&quot; in metric measure spaces. We show that if the metric
probability space of the test instance is concentrated, any classifier with
some initial constant error is inherently vulnerable to adversarial
perturbations.
&lt;/p&gt;
&lt;p&gt;One class of concentrated metric probability spaces are the so-called Levy
families that include many natural distributions. In this special case, our
attacks only need to perturb the test instance by at most $O(\sqrt n)$ to make
it misclassified, where $n$ is the data dimension. Using our general result
about Levy instance spaces, we first recover as special case some of the
previously proved results about the existence of adversarial examples. However,
many more Levy families are known for which we immediately obtain new attacks
finding adversarial examples (e.g., product distribution under the Hamming
distance).
&lt;/p&gt;
&lt;p&gt;Finally, we show that concentration of measure for product spaces implies the
existence of so called &quot;poisoning&quot; attacks in which the adversary tampers with
the training data with the goal of increasing the error of the classifier. We
show that for any deterministic learning algorithm that uses $m$ training
examples, there is an adversary who substitutes $O(\sqrt m)$ of the examples
with other (still correctly labeled) ones and can almost fully degrade the
confidence parameter of any PAC learning algorithm or alternatively increase
the risk to almost 1 if the adversary also knows the final test example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1&quot;&gt;Saeed Mahloujifar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diochnos_D/0/1/0/all/0/1&quot;&gt;Dimitrios I. Diochnos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmoody_M/0/1/0/all/0/1&quot;&gt;Mohammad Mahmoody&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03090">
<title>Approximation and Estimation for High-Dimensional Deep Learning Networks. (arXiv:1809.03090v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.03090</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been experimentally observed in recent years that multi-layer
artificial neural networks have a surprising ability to generalize, even when
trained with far more parameters than observations. Is there a theoretical
basis for this? The best available bounds on their metric entropy and
associated complexity measures are essentially linear in the number of
parameters, which is inadequate to explain this phenomenon. Here we examine the
statistical risk (mean squared predictive error) of multi-layer networks with
$\ell^1$-type controls on their parameters and with ramp activation functions
(also called lower-rectified linear units). In this setting, the risk is shown
to be upper bounded by $[(L^3 \log d)/n]^{1/2}$, where $d$ is the input
dimension to each layer, $L$ is the number of layers, and $n$ is the sample
size. In this way, the input dimension can be much larger than the sample size
and the estimator can still be accurate, provided the target function has such
$\ell^1$ controls and that the sample size is at least moderately large
compared to $L^3\log d$. The heart of the analysis is the development of a
sampling strategy that demonstrates the accuracy of a sparse covering of deep
ramp networks. Lower bounds show that the identified risk is close to being
optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barron_A/0/1/0/all/0/1&quot;&gt;Andrew R. Barron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klusowski_J/0/1/0/all/0/1&quot;&gt;Jason M. Klusowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03113">
<title>Second-Order Adversarial Attack and Certifiable Robustness. (arXiv:1809.03113v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03113</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a powerful second-order attack method that outperforms existing
attack methods on reducing the accuracy of state-of-the-art defense models
based on adversarial training. The effectiveness of our attack method motivates
an investigation of provable robustness of a defense model. To this end, we
introduce a framework that allows one to obtain a certifiable lower bound on
the prediction accuracy against adversarial examples. We conduct experiments to
show the effectiveness of our attack method. At the same time, our defense
models obtain higher accuracies compared to previous works under our proposed
attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changyou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenlin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03140">
<title>Deep MR Image Super-Resolution Using Structural Priors. (arXiv:1809.03140v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03140</link>
<description rdf:parseType="Literal">&lt;p&gt;High resolution magnetic resonance (MR) images are desired for accurate
diagnostics. In practice, image resolution is restricted by factors like
hardware, cost and processing constraints. Recently, deep learning methods have
been shown to produce compelling state of the art results for image
super-resolution. Paying particular attention to desired hi-resolution MR image
structure, we propose a new regularized network that exploits image priors,
namely a low-rank structure and a sharpness prior to enhance deep MR image
superresolution. Our contributions are then incorporating these priors in an
analytically tractable fashion in the learning of a convolutional neural
network (CNN) that accomplishes the super-resolution task. This is particularly
challenging for the low rank prior, since the rank is not a differentiable
function of the image matrix (and hence the network parameters), an issue we
address by pursuing differentiable approximations of the rank. Sharpness is
emphasized by the variance of the Laplacian which we show can be implemented by
a fixed {\em feedback} layer at the output of the network. Experiments
performed on two publicly available MR brain image databases exhibit promising
results particularly when training imagery is limited.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherukuri_V/0/1/0/all/0/1&quot;&gt;Venkateswararao Cherukuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tiantong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiff_S/0/1/0/all/0/1&quot;&gt;Steven J. Schiff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monga_V/0/1/0/all/0/1&quot;&gt;Vishal Monga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03185">
<title>Shallow vs deep learning architectures for white matter lesion segmentation in the early stages of multiple sclerosis. (arXiv:1809.03185v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03185</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a comparison of a shallow and a deep learning
architecture for the automated segmentation of white matter lesions in MR
images of multiple sclerosis patients. In particular, we train and test both
methods on early stage disease patients, to verify their performance in
challenging conditions, more similar to a clinical setting than what is
typically provided in multiple sclerosis segmentation challenges. Furthermore,
we evaluate a prototype naive combination of the two methods, which refines the
final segmentation. All methods were trained on 32 patients, and the evaluation
was performed on a pure test set of 73 cases. Results show low lesion-wise
false positives (30%) for the deep learning architecture, whereas the shallow
architecture yields the best Dice coefficient (63%) and volume difference
(19%). Combining both shallow and deep architectures further improves the
lesion-wise metrics (69% and 26% lesion-wise true and false positive rate,
respectively).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_F/0/1/0/all/0/1&quot;&gt;Francesco La Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fartaria_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rio Jo&amp;#xe3;o Fartaria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kober_T/0/1/0/all/0/1&quot;&gt;Tobias Kober&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richiardi_J/0/1/0/all/0/1&quot;&gt;Jonas Richiardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granziera_C/0/1/0/all/0/1&quot;&gt;Cristina Granziera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1&quot;&gt;Jean-Philippe Thiran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cuadra_M/0/1/0/all/0/1&quot;&gt;Meritxell Bach Cuadra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03291">
<title>Action-conditional Sequence Modeling for Recommendation. (arXiv:1809.03291v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1809.03291</link>
<description rdf:parseType="Literal">&lt;p&gt;In many online applications interactions between a user and a web-service are
organized in a sequential way, e.g., user browsing an e-commerce website. In
this setting, recommendation system acts throughout user navigation by showing
items. Previous works have addressed this recommendation setup through the task
of predicting the next item user will interact with. In particular, Recurrent
Neural Networks (RNNs) has been shown to achieve substantial improvements over
collaborative filtering baselines. In this paper, we consider interactions
triggered by the recommendations of deployed recommender system in addition to
browsing behavior. Indeed, it is reported that in online services interactions
with recommendations represent up to 30\% of total interactions. Moreover, in
practice, recommender system can greatly influence user behavior by promoting
specific items. In this paper, we extend the RNN modeling framework by taking
into account user interaction with recommended items. We propose and evaluate
RNN architectures that consist of the recommendation action module and the
state-action fusion module. Using real-world large-scale datasets we
demonstrate improved performance on the next item prediction task compared to
the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smirnova_E/0/1/0/all/0/1&quot;&gt;Elena Smirnova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03306">
<title>A Comparison of Handcrafted and Deep Neural Network Feature Extraction for Classifying Optical Coherence Tomography (OCT) Images. (arXiv:1809.03306v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.03306</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical Coherence Tomography allows ophthalmologist to obtain cross-section
imaging of eye retina. Assisted with digital image analysis methods, effective
disease detection could be performed. Various methods exist to extract feature
from OCT images. The proposed study aims to compare the effectiveness of
handcrafted and deep neural network features. The evaluated dataset consist of
32339 instances distributed in four classes, namely CNV, DME, DRUSEN, and
NORMAL. The methods are Histogram of Oriented Gradient (HOG), Local Binary
Pattern (LBP), DenseNet-169, and ResNet50. As a result, the deep neural network
based methods outperformed the handcrafted feature with 88% and 89% accuracy
for DenseNet and ResNet compared to 50 % and 42 % for HOG and LBP respectively.
The deep neural network based methods also demonstrated better result on the
under represented class.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1&quot;&gt;Kuntoro Adi Nugroho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03322">
<title>Guiding the Creation of Deep Learning-based Object Detectors. (arXiv:1809.03322v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.03322</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection is a computer vision field that has applications in several
contexts ranging from biomedicine and agriculture to security. In the last
years, several deep learning techniques have greatly improved object detection
models. Among those techniques, we can highlight the YOLO approach, that allows
the construction of accurate models that can be employed in real-time
applications. However, as most deep learning techniques, YOLO has a steep
learning curve and creating models using this approach might be challenging for
non-expert users. In this work, we tackle this problem by constructing a suite
of Jupyter notebooks that democratizes the construction of object detection
models using YOLO. The suitability of our approach has been proven with a
dataset of stomata images where we have achieved a mAP of 90.91%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casado_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;ngela Casado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heras_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf3;nathan Heras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03323">
<title>Deriving Enhanced Geographical Representations via Similarity-based Spectral Analysis: Predicting Colorectal Cancer Survival Curves in Iowa. (arXiv:1809.03323v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03323</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are capable of learning rich, nonlinear feature
representations shown to be beneficial in many predictive tasks. In this work,
we use such models to explore different geographical feature representations in
the context of predicting colorectal cancer survival curves for patients in the
state of Iowa, spanning the years 1989 to 2013. Specifically, we compare model
performance using &quot;area between the curves&quot; (ABC) to assess (a) whether
survival curves can be reasonably predicted for colorectal cancer patients in
the state of Iowa, (b) whether geographical features improve predictive
performance, (c) whether a simple binary representation, or a richer, spectral
analysis-elicited representation perform better, and (d) whether spectral
analysis-based representations can be improved upon by leveraging
geographically-descriptive features. In exploring (d), we devise a
similarity-based spectral analysis procedure, which allows for the combination
of geographically relational and geographically descriptive features. Our
findings suggest that survival curves can be reasonably estimated on average,
with predictive performance deviating at the five-year survival mark among all
models. We also find that geographical features improve predictive performance,
and that better performance is obtained using richer, spectral
analysis-elicited features. Furthermore, we find that similarity-based spectral
analysis-elicited representations improve upon the original spectral analysis
results by approximately 40%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lash_M/0/1/0/all/0/1&quot;&gt;Michael T. Lash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Street_W/0/1/0/all/0/1&quot;&gt;W. Nick Street&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lynch_C/0/1/0/all/0/1&quot;&gt;Charles F. Lynch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03343">
<title>Distributed dynamic modeling and monitoring for large-scale industrial processes under closed-loop control. (arXiv:1809.03343v1 [cs.SY])</title>
<link>http://arxiv.org/abs/1809.03343</link>
<description rdf:parseType="Literal">&lt;p&gt;For large-scale industrial processes under closed-loop control, process
dynamics directly resulting from control action are typical characteristics and
may show different behaviors between real faults and normal changes of
operating conditions. However, conventional distributed monitoring approaches
do not consider the closed-loop control mechanism and only explore static
characteristics, which thus are incapable of distinguishing between real
process faults and nominal changes of operating conditions, leading to
unnecessary alarms. In this regard, this paper proposes a distributed
monitoring method for closed-loop industrial processes by concurrently
exploring static and dynamic characteristics. First, the large-scale
closed-loop process is decomposed into several subsystems by developing a
sparse slow feature analysis (SSFA) algorithm which capture changes of both
static and dynamic information. Second, distributed models are developed to
separately capture static and dynamic characteristics from the local and global
aspects. Based on the distributed monitoring system, a two-level monitoring
strategy is proposed to check different influences on process characteristics
resulting from changes of the operating conditions and control action, and thus
the two changes can be well distinguished from each other. Case studies are
conducted based on both benchmark data and real industrial process data to
illustrate the effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenqing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chunhui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Biao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03385">
<title>SPASS: Scientific Prominence Active Search System with Deep Image Captioning Network. (arXiv:1809.03385v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03385</link>
<description rdf:parseType="Literal">&lt;p&gt;Planetary exploration missions with Mars rovers are complicated, which
generally require elaborated task planning by human experts, from the path to
take to the images to capture. NASA has been using this process to acquire over
22 million images from the planet Mars. In order to improve the degree of
automation and thus efficiency in this process, we propose a system for
planetary rovers to actively search for prominence of prespecified scientific
features in captured images. Scientists can prespecify such search tasks in
natural language and upload them to a rover, on which the deployed system
constantly captions captured images with a deep image captioning network and
compare the auto-generated captions to the prespecified search tasks by certain
metrics so as to prioritize those images for transmission. As a beneficial side
effect, the proposed system can also be deployed to ground-based planetary data
systems as a content-based search engine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_D/0/1/0/all/0/1&quot;&gt;Dicong Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03400">
<title>A Moral Framework for Understanding of Fair ML through Economic Models of Equality of Opportunity. (arXiv:1809.03400v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03400</link>
<description rdf:parseType="Literal">&lt;p&gt;Equality of opportunity (EOP) is an extensively studied conception of
fairness in political philosophy. In this work, we map recently proposed
notions of algorithmic fairness to economic models of EOP. We formally show
that through our proposed mapping, many existing definition of algorithmic
fairness, such as predictive value parity and equality of odds, can be
interpreted as special cases of EOP. In this respect, our work serves as a
unifying moral framework for understanding existing notions of algorithmic
fairness. Most importantly, this framework allows us to explicitly spell out
the moral assumptions underlying each notion of fairness, and also interpret
recent fairness impossibility results in a new light. Last but not least and
inspired by luck egalitarian models of EOP, we propose a new, more general
family of measures for algorithmic fairness. We empirically show that employing
a measure of algorithmic (un)fairness when its underlying moral assumptions are
not satisfied, can have devastating consequences on the subjects&apos; welfare.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1&quot;&gt;Hoda Heidari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loi_M/0/1/0/all/0/1&quot;&gt;Michele Loi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gummadi_K/0/1/0/all/0/1&quot;&gt;Krishna P. Gummadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03402">
<title>Does Your Phone Know Your Touch?. (arXiv:1809.03402v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03402</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores supervised techniques for continuous anomaly detection
from biometric touch screen data. A capacitive sensor array used to mimic a
touch screen as used to collect touch and swipe gestures from participants. The
gestures are recorded over fixed segments of time, with position and force
measured for each gesture. Support Vector Machine, Logistic Regression, and
Gaussian mixture models were tested to learn individual touch patterns. Test
results showed true negative and true positive scores of over 95% accuracy for
all gesture types, with logistic regression models far outperforming the other
methods. A more expansive and varied data collection over longer periods of
time is needed to determine pragmatic usage of these results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peruzzi_J/0/1/0/all/0/1&quot;&gt;John Peruzzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wingard_P/0/1/0/all/0/1&quot;&gt;Phillip Andrew Wingard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zucker_D/0/1/0/all/0/1&quot;&gt;David Zucker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03416">
<title>Identifying Relationships Among Sentences in Court Case Transcripts Using Discourse Relations. (arXiv:1809.03416v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.03416</link>
<description rdf:parseType="Literal">&lt;p&gt;Case Law has a significant impact on the proceedings of legal cases.
Therefore, the information that can be obtained from previous court cases is
valuable to lawyers and other legal officials when performing their duties.
This paper describes a methodology of applying discourse relations between
sentences when processing text documents related to the legal domain. In this
study, we developed a mechanism to classify the relationships that can be
observed among sentences in transcripts of United States court cases. First, we
defined relationship types that can be observed between sentences in court case
transcripts. Then we classified pairs of sentences according to the
relationship type by combining a machine learning model and a rule-based
approach. The results obtained through our system were evaluated using human
judges. To the best of our knowledge, this is the first study where discourse
relationships between sentences have been used to determine relationships among
sentences in legal court case transcripts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratnayaka_G/0/1/0/all/0/1&quot;&gt;Gathika Ratnayaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rupasinghe_T/0/1/0/all/0/1&quot;&gt;Thejan Rupasinghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1&quot;&gt;Nisansa de Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warushavithana_M/0/1/0/all/0/1&quot;&gt;Menuka Warushavithana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gamage_V/0/1/0/all/0/1&quot;&gt;Viraj Gamage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perera_A/0/1/0/all/0/1&quot;&gt;Amal Shehan Perera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03447">
<title>Expert-augmented actor-critic for ViZDoom and Montezumas Revenge. (arXiv:1809.03447v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03447</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an expert-augmented actor-critic algorithm, which we evaluate on
two environments with sparse rewards: Montezumas Revenge and a demanding maze
from the ViZDoom suite. In the case of Montezumas Revenge, an agent trained
with our method achieves very good results consistently scoring above 27,000
points (in many experiments beating the first world). With an appropriate
choice of hyperparameters, our algorithm surpasses the performance of the
expert data. In a number of experiments, we have observed an unreported bug in
Montezumas Revenge which allowed the agent to score more than 800,000 points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garmulewicz_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Garmulewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1&quot;&gt;Henryk Michalewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1&quot;&gt;Piotr Mi&amp;#x142;o&amp;#x15b;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03461">
<title>Physics-Informed Kriging: A Physics-Informed Gaussian Process Regression Method for Data-Model Convergence. (arXiv:1809.03461v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.03461</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a new Gaussian process regression (GPR) method:
physics-informed Kriging (PhIK). In the standard data-driven Kriging, the
unknown function of interest is usually treated as a Gaussian process with
assumed stationary covariance with hyperparameters estimated from data. In
PhIK, we compute the mean and covariance function from realizations of
available stochastic models, e.g., from realizations of governing stochastic
partial differential equations solutions. Such a constructed Gaussian process
generally is non-stationary, and does not assume a specific form of the
covariance function. Our approach avoids the costly optimization step in
data-driven GPR methods to identify the hyperparameters. More importantly, we
prove that the physical constraints in the form of a deterministic linear
operator are guaranteed in the resulting prediction. We also provide an error
estimate in preserving the physical constraints when errors are included in the
stochastic model realizations. To reduce the computational cost of obtaining
stochastic model realizations, we propose a multilevel Monte Carlo estimate of
the mean and covariance functions. Further, we present an active learning
algorithm that guides the selection of additional observation locations. The
efficiency and accuracy of PhIK are demonstrated for reconstructing a partially
known modified Branin function and learning a conservative tracer distribution
from sparse concentration measurements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tartakovsky_G/0/1/0/all/0/1&quot;&gt;Guzel Tartakovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tartakovsky_A/0/1/0/all/0/1&quot;&gt;Alexandre Tartakovsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1412.5250">
<title>High Dimensional Forecasting via Interpretable Vector Autoregression. (arXiv:1412.5250v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1412.5250</link>
<description rdf:parseType="Literal">&lt;p&gt;Vector autoregression (VAR) is a fundamental tool for modeling multivariate
time series. However, as the number of component series is increased, the VAR
model becomes overparameterized. Several authors have addressed this issue by
incorporating regularized approaches, such as the lasso in VAR estimation.
Traditional approaches address overparameterization by selecting a low lag
order, based on the assumption of short range dependence, assuming that a
universal lag order applies to all components. Such an approach constrains the
relationship between the components and impedes forecast performance. The
lasso-based approaches work much better in high-dimensional situations but do
not incorporate the notion of lag order selection.
&lt;/p&gt;
&lt;p&gt;We propose a new class of hierarchical lag structures (HLag) that embed the
notion of lag selection into a convex regularizer. The key modeling tool is a
group lasso with nested groups which guarantees that the sparsity pattern of
lag coefficients honors the VAR&apos;s ordered structure. The HLag framework offers
three structures, which allow for varying levels of flexibility. A simulation
study demonstrates improved performance in forecasting and lag order selection
over previous approaches, and a macroeconomic application further highlights
forecasting improvements as well as HLag&apos;s convenient, interpretable output.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nicholson_W/0/1/0/all/0/1&quot;&gt;William B. Nicholson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wilms_I/0/1/0/all/0/1&quot;&gt;Ines Wilms&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bien_J/0/1/0/all/0/1&quot;&gt;Jacob Bien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Matteson_D/0/1/0/all/0/1&quot;&gt;David S. Matteson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1507.04564">
<title>Selecting the best system and multi-armed bandits. (arXiv:1507.04564v3 [math.PR] UPDATED)</title>
<link>http://arxiv.org/abs/1507.04564</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider the problem of finding a population or a probability distribution
amongst many with the largest mean when these means are unknown but population
samples can be simulated or otherwise generated. Typically, by selecting
largest sample mean population, it can be shown that false selection
probability decays at an exponential rate. Lately, researchers have sought
algorithms that guarantee that this probability is restricted to a small
$\delta$ in order $\log(1/\delta)$ computational time by estimating the
associated large deviations rate function via simulation. We show that such
guarantees are misleading when populations have unbounded support even when
these may be light-tailed. Specifically, we show that any policy that
identifies the correct population with probability at least $1-\delta$ for each
problem instance requires infinite number of samples in expectation in making
such a determination in any problem instance. This suggests that some
restrictions are essential on populations to devise $O(\log(1/\delta))$
algorithms with $1 - \delta$ correctness guarantees. We note that under
restriction on population moments, such methods are easily designed, and that
sequential methods from stochastic multi-armed bandit literature can be adapted
to devise such algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Glynn_P/0/1/0/all/0/1&quot;&gt;Peter Glynn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Juneja_S/0/1/0/all/0/1&quot;&gt;Sandeep Juneja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.06003">
<title>Inexact Proximal Gradient Methods for Non-convex and Non-smooth Optimization. (arXiv:1612.06003v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1612.06003</link>
<description rdf:parseType="Literal">&lt;p&gt;In machine learning research, the proximal gradient methods are popular for
solving various optimization problems with non-smooth regularization. Inexact
proximal gradient methods are extremely important when exactly solving the
proximal operator is time-consuming, or the proximal operator does not have an
analytic solution. However, existing inexact proximal gradient methods only
consider convex problems. The knowledge of inexact proximal gradient methods in
the non-convex setting is very limited. % Moreover, for some machine learning
models, there is still no proposed solver for exactly solving the proximal
operator. To address this challenge, in this paper, we first propose three
inexact proximal gradient algorithms, including the basic version and
Nesterov&apos;s accelerated version. After that, we provide the theoretical analysis
to the basic and Nesterov&apos;s accelerated versions. The theoretical results show
that our inexact proximal gradient algorithms can have the same convergence
rates as the ones of exact proximal gradient algorithms in the non-convex
setting.
&lt;/p&gt;
&lt;p&gt;Finally, we show the applications of our inexact proximal gradient algorithms
on three representative non-convex learning problems. All experimental results
confirm the superiority of our new inexact proximal gradient algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1&quot;&gt;Bin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;De Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1&quot;&gt;Zhouyuan Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heng Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.10225">
<title>Bayesian stochastic blockmodeling. (arXiv:1705.10225v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.10225</link>
<description rdf:parseType="Literal">&lt;p&gt;This chapter provides a self-contained introduction to the use of Bayesian
inference to extract large-scale modular structures from network data, based on
the stochastic blockmodel (SBM), as well as its degree-corrected and
overlapping generalizations. We focus on nonparametric formulations that allow
their inference in a manner that prevents overfitting, and enables model
selection. We discuss aspects of the choice of priors, in particular how to
avoid underfitting via increased Bayesian hierarchies, and we contrast the task
of sampling network partitions from the posterior distribution with finding the
single point estimate that maximizes it, while describing efficient algorithms
to perform either one. We also show how inferring the SBM can be used to
predict missing and spurious links, and shed light on the fundamental
limitations of the detectability of modular structures in networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peixoto_T/0/1/0/all/0/1&quot;&gt;Tiago P. Peixoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.06994">
<title>Structured Probabilistic Pruning for Convolutional Neural Network Acceleration. (arXiv:1709.06994v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.06994</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel progressive parameter pruning method for
Convolutional Neural Network acceleration, named Structured Probabilistic
Pruning (SPP), which effectively prunes weights of convolutional layers in a
probabilistic manner. Unlike existing deterministic pruning approaches, where
unimportant weights are permanently eliminated, SPP introduces a pruning
probability for each weight, and pruning is guided by sampling from the pruning
probabilities. A mechanism is designed to increase and decrease pruning
probabilities based on importance criteria in the training process. Experiments
show that, with 4x speedup, SPP can accelerate AlexNet with only 0.3% loss of
top-5 accuracy and VGG-16 with 0.8% loss of top-5 accuracy in ImageNet
classification. Moreover, SPP can be directly applied to accelerate
multi-branch CNN networks, such as ResNet, without specific adaptations. Our 2x
speedup ResNet-50 only suffers 0.8% loss of top-5 accuracy on ImageNet. We
further show the effectiveness of SPP on transfer learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuehai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Haoji Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.00095">
<title>User-friendly guarantees for the Langevin Monte Carlo with inaccurate gradient. (arXiv:1710.00095v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1710.00095</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of sampling from a given probability
density function that is known to be smooth and strongly log-concave. We
analyze several methods of approximate sampling based on discretizations of the
(highly overdamped) Langevin diffusion and establish guarantees on its error
measured in the Wasserstein-2 distance. Our guarantees improve or extend the
state-of-the-art results in three directions. First, we provide an upper bound
on the error of the first-order Langevin Monte Carlo (LMC) algorithm with
optimized varying step-size. This result has the advantage of being horizon
free (we do not need to know in advance the target precision) and to improve by
a logarithmic factor the corresponding result for the constant step-size.
Second, we study the case where accurate evaluations of the gradient of the
log-density are unavailable, but one can have access to approximations of the
aforementioned gradient. In such a situation, we consider both deterministic
and stochastic approximations of the gradient and provide an upper bound on the
sampling error of the first-order LMC that quantifies the impact of the
gradient evaluation inaccuracies. Third, we establish upper bounds for two
versions of the second-order LMC, which leverage the Hessian of the
log-density. We nonasymptotic guarantees on the sampling error of these
second-order LMCs. These guarantees reveal that the second-order LMC algorithms
improve on the first-order LMC in ill-conditioned settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dalalyan_A/0/1/0/all/0/1&quot;&gt;Arnak S. Dalalyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Karagulyan_A/0/1/0/all/0/1&quot;&gt;Avetik G. Karagulyan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00961">
<title>Learning Independent Causal Mechanisms. (arXiv:1712.00961v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00961</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistical learning relies upon data sampled from a distribution, and we
usually do not care what actually generated it in the first place. From the
point of view of causal modeling, the structure of each distribution is induced
by physical mechanisms that give rise to dependences between observables.
Mechanisms, however, can be meaningful autonomous modules of generative models
that make sense beyond a particular entailed data distribution, lending
themselves to transfer between problems. We develop an algorithm to recover a
set of independent (inverse) mechanisms from a set of transformed data points.
The approach is unsupervised and based on a set of experts that compete for
data generated by the mechanisms, driving specialization. We analyze the
proposed method in a series of experiments on image data. Each expert learns to
map a subset of the transformed data back to a reference distribution. The
learned mechanisms generalize to novel domains. We discuss implications for
transfer learning and links to recent trends in generative modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parascandolo_G/0/1/0/all/0/1&quot;&gt;Giambattista Parascandolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1&quot;&gt;Niki Kilbertus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojas_Carulla_M/0/1/0/all/0/1&quot;&gt;Mateo Rojas-Carulla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08363">
<title>An efficient $k$-means-type algorithm for clustering datasets with incomplete records. (arXiv:1802.08363v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08363</link>
<description rdf:parseType="Literal">&lt;p&gt;The $k$-means algorithm is arguably the most popular nonparametric clustering
method but cannot generally be applied to datasets with incomplete records. The
usual practice then is to either impute missing values under an assumed
missing-completely-at-random mechanism or to ignore the incomplete records, and
apply the algorithm on the resulting dataset. We develop an efficient version
of the $k$-means algorithm that allows for clustering in the presence of
incomplete records. Our extension is called $k_m$-means and reduces to the
$k$-means algorithm when all records are complete. We also provide
initialization strategies for our algorithm and methods to estimate the number
of groups in the dataset. Illustrations and simulations demonstrate the
efficacy of our approach in a variety of settings and patterns of missing data.
Our methods are also applied to the analysis of activation images obtained from
a functional Magnetic Resonance Imaging experiment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lithio_A/0/1/0/all/0/1&quot;&gt;Andrew Lithio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1&quot;&gt;Ranjan Maitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02781">
<title>Fast Dawid-Skene: A Fast Vote Aggregation Scheme for Sentiment Classification. (arXiv:1803.02781v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02781</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real world problems can now be effectively solved using supervised
machine learning. A major roadblock is often the lack of an adequate quantity
of labeled data for training. A possible solution is to assign the task of
labeling data to a crowd, and then infer the true label using aggregation
methods. A well-known approach for aggregation is the Dawid-Skene (DS)
algorithm, which is based on the principle of Expectation-Maximization (EM). We
propose a new simple, yet effective, EM-based algorithm, which can be
interpreted as a `hard&apos; version of DS, that allows much faster convergence
while maintaining similar accuracy in aggregation. We show the use of this
algorithm as a quick and effective technique for online, real-time sentiment
annotation. We also prove that our algorithm converges to the estimated labels
at a linear rate. Our experiments on standard datasets show a significant
speedup in time taken for aggregation - upto $\sim$8x over Dawid-Skene and
$\sim$6x over other fast EM methods, at competitive accuracy performance. The
code for the implementation of the algorithms can be found at
https://github.com/GoodDeeds/Fast-Dawid-Skene
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sinha_V/0/1/0/all/0/1&quot;&gt;Vaibhav B Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rao_S/0/1/0/all/0/1&quot;&gt;Sukrut Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balasubramanian_V/0/1/0/all/0/1&quot;&gt;Vineeth N Balasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06118">
<title>Gaussian Processes indexed on the symmetric group: prediction and learning. (arXiv:1803.06118v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06118</link>
<description rdf:parseType="Literal">&lt;p&gt;In the framework of the supervised learning of a real function defined on a
space X , the so called Kriging method stands on a real Gaussian field defined
on X. The Euclidean case is well known and has been widely studied. In this
paper, we explore the less classical case where X is the non commutative finite
group of permutations. In this setting, we propose and study an harmonic
analysis of the covariance operators that enables to consider Gaussian
processes models and forecasting issues. Our theory is motivated by statistical
ranking problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bachoc_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Bachoc&lt;/a&gt; (GdR MASCOT-NUM), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Broto_B/0/1/0/all/0/1&quot;&gt;Baptiste Broto&lt;/a&gt; (CEA), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gamboa_F/0/1/0/all/0/1&quot;&gt;Fabrice Gamboa&lt;/a&gt; (IMT), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loubes_J/0/1/0/all/0/1&quot;&gt;Jean-Michel Loubes&lt;/a&gt; (IMT), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loubes_J/0/1/0/all/0/1&quot;&gt;J-M Loubes&lt;/a&gt; (IMT)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10846">
<title>Non-Convex Matrix Completion Against a Semi-Random Adversary. (arXiv:1803.10846v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10846</link>
<description rdf:parseType="Literal">&lt;p&gt;Matrix completion is a well-studied problem with many machine learning
applications. In practice, the problem is often solved by non-convex
optimization algorithms. However, the current theoretical analysis for
non-convex algorithms relies heavily on the assumption that every entry is
observed with exactly the same probability $p$, which is not realistic in
practice.
&lt;/p&gt;
&lt;p&gt;In this paper, we investigate a more realistic semi-random model, where the
probability of observing each entry is at least $p$. Even with this mild
semi-random perturbation, we can construct counter-examples where existing
non-convex algorithms get stuck in bad local optima.
&lt;/p&gt;
&lt;p&gt;In light of the negative results, we propose a pre-processing step that tries
to re-weight the semi-random input, so that it becomes &quot;similar&quot; to a random
input. We give a nearly-linear time algorithm for this problem, and show that
after our pre-processing, all the local minima of the non-convex objective can
be used to approximately recover the underlying ground-truth matrix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1&quot;&gt;Rong Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02396">
<title>Billion-scale Network Embedding with Iterative Random Projection. (arXiv:1805.02396v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.02396</link>
<description rdf:parseType="Literal">&lt;p&gt;Network embedding, which learns low-dimensional vector representation for
nodes in the network, has attracted considerable research attention recently.
However, the existing methods are incapable of handling billion-scale networks,
because they are computationally expensive and, at the same time, difficult to
be accelerated by distributed computing schemes. To address these problems, we
propose RandNE (Iterative Random Projection Network Embedding), a novel and
simple billion-scale network embedding method. Specifically, we propose a
Gaussian random projection approach to map the network into a low-dimensional
embedding space while preserving the high-order proximities between nodes. To
reduce the time complexity, we design an iterative projection procedure to
avoid the explicit calculation of the high-order proximities. Theoretical
analysis shows that our method is extremely efficient, and friendly to
distributed computing schemes without any communication cost in the
calculation. We also design a dynamic updating procedure which can efficiently
incorporate the dynamic changes of the networks without error aggregation.
Extensive experimental results demonstrate the efficiency and efficacy of
RandNE over state-of-the-art methods in several tasks including network
reconstruction, link prediction and node classification on multiple datasets
with different scales, ranging from thousands to billions of nodes and edges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1&quot;&gt;Peng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenwu Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11454">
<title>Distributed Stochastic Gradient Tracking Methods. (arXiv:1805.11454v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11454</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of distributed multi-agent optimization
over a network, where each agent possesses a local cost function that is smooth
and strongly convex. The global objective is to find a common solution that
minimizes the average of all cost functions. Assuming agents only have access
to unbiased estimates of the gradients of their local cost functions, we
consider a distributed stochastic gradient tracking method (DSGT) and a
gossip-like stochastic gradient tracking method (GSGT). We show that, in
expectation, the iterates generated by each agent are attracted to a
neighborhood of the optimal solution, where they accumulate exponentially fast
(under a constant stepsize choice). Under DSGT, the limiting (expected) error
bounds on the distance of the iterates from the optimal solution decrease with
the network size $n$, which is a comparable performance to a centralized
stochastic gradient algorithm. Moreover, we show that when the network is
well-connected, GSGT incurs lower communication cost than DSGT while
maintaining a similar computational cost. Numerical example further
demonstrates the effectiveness of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pu_S/0/1/0/all/0/1&quot;&gt;Shi Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nedic_A/0/1/0/all/0/1&quot;&gt;Angelia Nedi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12547">
<title>Long-time predictive modeling of nonlinear dynamical systems using neural networks. (arXiv:1805.12547v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.12547</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the use of feedforward neural networks (FNN) to develop models of
nonlinear dynamical systems from data. Emphasis is placed on predictions at
long times, with limited data availability. Inspired by global stability
analysis, and the observation of the strong correlation between the local error
and the maximum singular value of the Jacobian of the ANN, we introduce
Jacobian regularization in the loss function. This regularization suppresses
the sensitivity of the prediction to the local error and is shown to improve
accuracy and robustness. Comparison between the proposed approach and sparse
polynomial regression is presented in numerical examples ranging from simple
ODE systems to nonlinear PDE systems including vortex shedding behind a
cylinder, and instability-driven buoyant mixing flow. Furthermore, limitations
of feedforward neural networks are highlighted, especially when the training
data does not include a low dimensional attractor. Strategies of data
augmentation are presented as remedies to address these issues to a certain
extent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shaowu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duraisamy_K/0/1/0/all/0/1&quot;&gt;Karthik Duraisamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02543">
<title>Grouped Gaussian Processes for Solar Power Prediction. (arXiv:1806.02543v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02543</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider multi-task regression models where the observations are assumed
to be a linear combination of several latent node functions and weight
functions, which are both drawn from Gaussian process priors. Driven by the
problem of developing scalable methods for forecasting distributed solar and
other renewable power generation, we propose coupled priors over groups of
(node or weight) processes to exploit spatial dependence between functions. We
estimate forecast models for solar power at multiple distributed sites and
ground wind speed at multiple proximate weather stations. Our results show that
our approach maintains or improves point-prediction accuracy relative to
competing solar benchmarks and improves over wind forecast benchmark models on
all measures. At the same time our approach provides better quantification of
predictive uncertainties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dahl_A/0/1/0/all/0/1&quot;&gt;Astrid Dahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bonilla_E/0/1/0/all/0/1&quot;&gt;Edwin V. Bonilla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02970">
<title>PAC Ranking from Pairwise and Listwise Queries: Lower Bounds and Upper Bounds. (arXiv:1806.02970v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02970</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the adaptive (active) PAC (probably approximately
correct) top-$k$ ranking (i.e., top-$k$ item selection) and total ranking
problems from $l$-wise ($l\geq 2$) comparisons under the multinomial logit
(MNL) model. By adaptively choosing sets to query and observing the noisy
output of the most favored item of each query, we want to design ranking
algorithms that recover the top-$k$ or total ranking using as few queries as
possible. For the PAC top-$k$ ranking problem, we derive a lower bound on the
sample complexity (aka number of queries), and propose an algorithm that is
sample-complexity-optimal up to an $O(\log(k+l)/\log{k})$ factor. When $l=2$
(i.e., pairwise comparisons) or $l=O(poly(k))$, this algorithm matches the
lower bound. For the PAC total ranking problem, we derive a tight lower bound,
and propose an algorithm that matches the lower bound. When $l=2$, the MNL
model reduces to the popular Plackett-Luce (PL) model. In this setting, our
results still outperform the state-of-the-art both theoretically and
numerically. We also compare our algorithms with the state-of-the-art using
synthetic data as well as real-world data to verify the efficiency of our
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1&quot;&gt;Wenbo Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1&quot;&gt;Ness B. Shroff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11470">
<title>Deep Encoder-Decoder Models for Unsupervised Learning of Controllable Speech Synthesis. (arXiv:1807.11470v3 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/1807.11470</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating versatile and appropriate synthetic speech requires control over
the output expression separate from the spoken text. Important non-textual
speech variation is seldom annotated, in which case output control must be
learned in an unsupervised fashion. In this paper, we perform an in-depth study
of methods for unsupervised learning of control in statistical speech
synthesis. For example, we show that popular unsupervised training heuristics
can be interpreted as variational inference in certain autoencoder models. We
additionally connect these models to VQ-VAEs, another, recently-proposed class
of deep variational autoencoders, which we show can be derived from a very
similar mathematical argument. The implications of these new probabilistic
interpretations are discussed. We illustrate the utility of the various
approaches with an application to acoustic modelling for emotional speech
synthesis, where the unsupervised methods for learning expression control
(without access to emotional labels) are found to give results that in many
aspects match or surpass the previous best supervised approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Henter_G/0/1/0/all/0/1&quot;&gt;Gustav Eje Henter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1&quot;&gt;Jaime Lorenzo-Trueba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00020">
<title>Online Adaptative Curriculum Learning for GANs. (arXiv:1808.00020v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.00020</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) can successfully learn a probability
distribution and produce realistic samples. However, open questions such as
sufficient convergence conditions and mode collapse still persist. In this
paper, we build on existing work in the area by proposing a novel framework for
training the generator against an ensemble of discriminator networks, which can
be seen as a one-student/multiple-teachers setting. We formalize this problem
within the non-stationary Multi-Armed Bandit (MAB) framework, where we evaluate
the capability of a bandit algorithm to select discriminators for providing the
generator with feedback during learning. To this end, we propose a reward
function which reflects the amount of knowledge learned by the generator and
dynamically selects the optimal discriminator network. Finally, we connect our
algorithm to stochastic optimization methods and show that existing methods
using multiple discriminators in literature can be recovered from our
parametric model. Experimental results based on the Fr\&apos;echet Inception
Distance (FID) demonstrates faster convergence than existing baselines and show
that our method learns a curriculum.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1&quot;&gt;Thang Doan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monteiro_J/0/1/0/all/0/1&quot;&gt;Joao Monteiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albuquerque_I/0/1/0/all/0/1&quot;&gt;Isabela Albuquerque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazoure_B/0/1/0/all/0/1&quot;&gt;Bogdan Mazoure&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durand_A/0/1/0/all/0/1&quot;&gt;Audrey Durand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1&quot;&gt;Joelle Pineau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1&quot;&gt;R Devon Hjelm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00560">
<title>Spectral Mixture Kernels with Time and Phase Delay Dependencies. (arXiv:1808.00560v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.00560</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral Mixture (SM) kernels form a powerful class of kernels for Gaussian
processes, capable to discover patterns, extrapolate, and model negative
co-variances. In SM kernels, spectral mixture components are linearly combined
to construct a final flexible kernel. As a consequence SM kernels does not
explicitly model correlations between components and dependencies related to
time and phase delays between components, because only the auto-convolution of
base components are used. To address these drawbacks we introduce Generalized
Convolution Spectral Mixture (GCSM) kernels. We incorporate time and phase
delay into the base spectral mixture and use cross-convolution between a base
component and the complex conjugate of another base component to construct a
complex-valued and positive definite kernel representing correlations between
base components. In this way the total number of components in GCSM becomes
quadratic. We perform a thorough comparative experimental analysis of GCSM on
synthetic and real-life datasets. Results indicate the beneficial effect of the
extra features of GCSM. This is illustrated in the problem of forecasting the
long range trend of a river flow to monitor environment evolution, where GCSM
is capable of discovering correlated patterns that SM cannot and improving
patterns recognition ability of SM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groot_P/0/1/0/all/0/1&quot;&gt;Perry Groot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchiori_E/0/1/0/all/0/1&quot;&gt;Elena Marchiori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00510">
<title>Flatland: a Lightweight First-Person 2-D Environment for Reinforcement Learning. (arXiv:1809.00510v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.00510</link>
<description rdf:parseType="Literal">&lt;p&gt;Flatland is a simple, lightweight environment for fast prototyping and
testing of reinforcement learning agents. It is of lower complexity compared to
similar 3D platforms (e.g. DeepMind Lab or VizDoom), but emulates physical
properties of the real world, such as continuity, multi-modal
partially-observable states with first-person view and coherent physics. We
propose to use it as an intermediary benchmark for problems related to Lifelong
Learning. Flatland is highly customizable and offers a wide range of task
difficulty to extensively evaluate the properties of artificial agents. We
experiment with three reinforcement learning baseline agents and show that they
can rapidly solve a navigation task in Flatland. A video of an agent acting in
Flatland is available here: https://youtu.be/I5y6Y2ZypdA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caselles_Dupre_H/0/1/0/all/0/1&quot;&gt;Hugo Caselles-Dupr&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Annabi_L/0/1/0/all/0/1&quot;&gt;Louis Annabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagen_O/0/1/0/all/0/1&quot;&gt;Oksana Hagen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Ortiz_M/0/1/0/all/0/1&quot;&gt;Michael Garcia-Ortiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1&quot;&gt;David Filliat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02188">
<title>Differentially Private Bayesian Inference for Exponential Families. (arXiv:1809.02188v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.02188</link>
<description rdf:parseType="Literal">&lt;p&gt;The study of private inference has been sparked by growing concern regarding
the analysis of data when it stems from sensitive sources. We present the first
method for private Bayesian inference in exponential families that properly
accounts for noise introduced by the privacy mechanism. It is efficient because
it works only with sufficient statistics and not individual data. Unlike other
methods, it gives properly calibrated posterior beliefs in the non-asymptotic
data regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernstein_G/0/1/0/all/0/1&quot;&gt;Garrett Bernstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheldon_D/0/1/0/all/0/1&quot;&gt;Daniel Sheldon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02292">
<title>A Block Coordinate Ascent Algorithm for Mean-Variance Optimization. (arXiv:1809.02292v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.02292</link>
<description rdf:parseType="Literal">&lt;p&gt;Risk management in dynamic decision problems is a primary concern in many
fields, including financial investment, autonomous driving, and healthcare. The
mean-variance function is one of the most widely used objective functions in
risk management due to its simplicity and interpretability. Existing algorithms
for mean-variance optimization are based on multi-time-scale stochastic
approximation, whose learning rate schedules are often hard to tune, and have
only asymptotic convergence proof. In this paper, we develop a model-free
policy search framework for mean-variance optimization with finite-sample error
bound analysis (to local optima). Our starting point is a reformulation of the
original mean-variance function with its Fenchel dual, from which we propose a
stochastic block coordinate ascent policy search algorithm. Both the asymptotic
convergence guarantee of the last iteration&apos;s solution and the convergence rate
of the randomly picked solution are provided, and their applicability is
demonstrated on several benchmark domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tengyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yangyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1&quot;&gt;Mohammad Ghavamzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1&quot;&gt;Yinlam Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_D/0/1/0/all/0/1&quot;&gt;Daoming Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1&quot;&gt;Daesub Yoon&lt;/a&gt;</dc:creator>
</item></rdf:RDF>