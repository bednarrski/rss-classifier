<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-15T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04487"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04734"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.00837"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02827"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04271"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04340"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04345"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04346"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04378"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04380"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04520"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04541"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04589"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04590"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04600"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04622"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04701"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10314"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01704"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03612"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.07919"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04289"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04295"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04339"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04503"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04510"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04540"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04813"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04856"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1507.03734"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1510.02786"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.09391"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.03225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.02461"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09514"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06658"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02901"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.04487">
<title>Better Runtime Guarantees Via Stochastic Domination. (arXiv:1801.04487v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.04487</link>
<description rdf:parseType="Literal">&lt;p&gt;Apart from few exceptions, the mathematical runtime analysis of evolutionary
algorithms is mostly concerned with expected runtimes. In this work, we argue
that stochastic domination is a notion that should be used more frequently in
this area. Stochastic domination allows to formulate much more informative
performance guarantees than the expectation alone, it allows to decouple the
algorithm analysis into the true algorithmic part of detecting a domination
statement and probability theoretic part of deriving the desired probabilistic
guarantees from this statement, and it allows simpler and more natural proofs.
&lt;/p&gt;
&lt;p&gt;As particular results, we prove a fitness level theorem which shows that the
runtime is dominated by a sum of independent geometric random variables, we
prove tail bounds for several classic problems, and we give a short and natural
proof for Witt&apos;s result that the runtime of any $(\mu,p)$ mutation-based
algorithm on any function with unique optimum is subdominated by the runtime of
a variant of the (1+1) evolutionary algorithm on the OneMax function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04734">
<title>Full Wafer Redistribution and Wafer Embedding as Key Technologies for a Multi-Scale Neuromorphic Hardware Cluster. (arXiv:1801.04734v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1801.04734</link>
<description rdf:parseType="Literal">&lt;p&gt;Together with the Kirchhoff-Institute for Physics(KIP) the Fraunhofer IZM has
developed a full wafer redistribution and embedding technology as base for a
large-scale neuromorphic hardware system. The paper will give an overview of
the neuromorphic computing platform at the KIP and the associated hardware
requirements which drove the described technological developments. In the first
phase of the project standard redistribution technologies from wafer level
packaging were adapted to enable a high density reticle-to-reticle routing on
200mm CMOS wafers. Neighboring reticles were interconnected across the scribe
lines with an 8{\mu}m pitch routing based on semi-additive copper
metallization. Passivation by photo sensitive benzocyclobutene was used to
enable a second intra-reticle routing layer. Final IO pads with flash gold were
generated on top of each reticle. With that concept neuromorphic systems based
on full wafers could be assembled and tested. The fabricated high density
inter-reticle routing revealed a very high yield of larger than 99.9%. In order
to allow an upscaling of the system size to a large number of wafers with
feasible effort a full wafer embedding concept for printed circuit boards was
developed and proven in the second phase of the project. The wafers were
thinned to 250{\mu}m and laminated with additional prepreg layers and copper
foils into a core material. After lamination of the PCB panel the reticle IOs
of the embedded wafer were accessed by micro via drilling, copper
electroplating, lithography and subtractive etching of the PCB wiring
structure. The created wiring with 50um line width enabled an access of the
reticle IOs on the embedded wafer as well as a board level routing. The panels
with the embedded wafers were subsequently stressed with up to 1000 thermal
cycles between 0C and 100C and have shown no severe failure formation over the
cycle time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoschke_K/0/1/0/all/0/1&quot;&gt;Kai Zoschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guttler_M/0/1/0/all/0/1&quot;&gt;Maurice G&amp;#xfc;ttler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bottcher_L/0/1/0/all/0/1&quot;&gt;Lars B&amp;#xf6;ttcher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grubl_A/0/1/0/all/0/1&quot;&gt;Andreas Gr&amp;#xfc;bl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Husmann_D/0/1/0/all/0/1&quot;&gt;Dan Husmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schemmel_J/0/1/0/all/0/1&quot;&gt;Johannes Schemmel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_K/0/1/0/all/0/1&quot;&gt;Karlheinz Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehrmann_O/0/1/0/all/0/1&quot;&gt;Oswin Ehrmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.00837">
<title>Eye-Movement behavior identification for AD diagnosis. (arXiv:1702.00837v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1702.00837</link>
<description rdf:parseType="Literal">&lt;p&gt;In the present work, we develop a deep-learning approach for differentiating
the eye-movement behavior of people with neurodegenerative diseases over
healthy control subjects during reading well-defined sentences. We define an
information compaction of the eye-tracking data of subjects without and with
probable Alzheimer&apos;s disease when reading a set of well-defined, previously
validated, sentences including high-, low-predictable sentences, and proverbs.
Using this information we train a set of denoising sparse-autoencoders and
build a deep neural network with these and a softmax classifier. Our results
are very promising and show that these models may help to understand the
dynamics of eye movement behavior and its relationship with underlying
neuropsychological correlates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biondi_J/0/1/0/all/0/1&quot;&gt;Juan Biondi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_G/0/1/0/all/0/1&quot;&gt;Gerardo Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1&quot;&gt;Silvia Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agamennoni_O/0/1/0/all/0/1&quot;&gt;Osvaldo Agamennoni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02827">
<title>Novel Methods for Enhancing the Performance of Genetic Algorithms. (arXiv:1801.02827v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02827</link>
<description rdf:parseType="Literal">&lt;p&gt;In this thesis we propose new methods for crossover operator namely: cut on
worst gene (COWGC), cut on worst L+R gene (COWLRGC) and Collision Crossovers.
And also we propose several types of mutation operator such as: worst gene with
random gene mutation (WGWRGM) , worst LR gene with random gene mutation
(WLRGWRGM), worst gene with worst gene mutation (WGWWGM), worst gene with
nearest neighbour mutation (WGWNNM), worst gene with the worst around the
nearest neighbour mutation (WGWWNNM), worst gene inserted beside nearest
neighbour mutation (WGIBNNM), random gene inserted beside nearest neighbour
mutation (RGIBNNM), Swap worst gene locally mutation (SWGLM), Insert best
random gene before worst gene mutation (IBRGBWGM) and Insert best random gene
before random gene mutation (IBRGBRGM). In addition to proposing four selection
strategies, namely: select any crossover (SAC), select any mutation (SAM),
select best crossover (SBC) and select best mutation (SBM). The first two are
based on selection of the best crossover and mutation operator respectively,
and the other two strategies randomly select any operator. So we investigate
the use of more than one crossover/mutation operator (based on the proposed
strategies) to enhance the performance of genetic algorithms. Our experiments,
conducted on several Travelling Salesman Problems (TSP), show the superiority
of some of the proposed methods in crossover and mutation over some of the
well-known crossover and mutation operators described in the literature. In
addition, using any of the four strategies (SAC, SAM, SBC and SBM), found to be
better than using one crossover/mutation operator in general, because those
allow the GA to avoid local optima, or the so-called premature convergence.
Keywords: GAs, Collision crossover, Multi crossovers, Multi mutations, TSP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkafaween_E/0/1/0/all/0/1&quot;&gt;Esra&amp;#x27;a O Alkafaween&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04271">
<title>Comparative Study on Generative Adversarial Networks. (arXiv:1801.04271v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04271</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there have been tremendous advancements in the field of
machine learning. These advancements have been made through both academic as
well as industrial research. Lately, a fair amount of research has been
dedicated to the usage of generative models in the field of computer vision and
image classification. These generative models have been popularized through a
new framework called Generative Adversarial Networks. Moreover, many modified
versions of this framework have been proposed in the last two years. We study
the original model proposed by Goodfellow et al. as well as modifications over
the original model and provide a comparative analysis of these models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitawala_S/0/1/0/all/0/1&quot;&gt;Saifuddin Hitawala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04340">
<title>Predicting Future Lane Changes of Other Highway Vehicles using RNN-based Deep Models. (arXiv:1801.04340v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1801.04340</link>
<description rdf:parseType="Literal">&lt;p&gt;In the event of sensor failure, it is necessary for autonomous vehicles to
safely execute emergency maneuvers while avoiding other vehicles on the road.
In order to accomplish this, the sensor-failed vehicle must predict the future
semantic behaviors of other drivers, such as lane changes, as well as their
future trajectories given a small window of past sensor observations. We
address the first issue of semantic behavior prediction in this paper, by
introducing a prediction framework that leverages the power of recurrent neural
networks (RNNs) and graphical models. Our prediction goal is to predict the
future categorical driving intent, for lane changes, of neighboring vehicles up
to three seconds into the future given as little as a one-second window of past
LIDAR, GPS, inertial, and map data.
&lt;/p&gt;
&lt;p&gt;We collect real-world data containing over 500,000 samples of highway driving
using an autonomous Toyota vehicle. We propose a pair of models that leverage
RNNs: first, a monolithic RNN model that tries to directly map inputs to future
behavior through a long-short-term-memory network. Second, we propose a
composite RNN model by adopting the methodology of Structural Recurrent Neural
Networks (RNNs) to learn factor functions and take advantage of both the
high-level structure of graphical models and the sequence modeling power of
RNNs, which we expect to afford more transparent modeling and activity than the
monolithic RNN. To demonstrate our approach, we validate our models using
authentic interstate highway driving to predict the future lane change
maneuvers of other vehicles neighboring our autonomous vehicle. We find that
both RNN models outperform baselines, and they outperform each other in certain
conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Sajan Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffin_B/0/1/0/all/0/1&quot;&gt;Brent Griffin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusano_K/0/1/0/all/0/1&quot;&gt;Kristofer Kusano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1&quot;&gt;Jason J. Corso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04342">
<title>Combining Symbolic and Function Evaluation Expressions In Neural Programs. (arXiv:1801.04342v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04342</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural programming involves training neural networks to learn programs from
data. Previous works have failed to achieve good generalization performance,
especially on programs with high complexity or on large domains. This is
because they mostly rely either on black-box function evaluations that do not
capture the structure of the program, or on detailed execution traces that are
expensive to obtain, and hence the training data has poor coverage of the
domain under consideration. We present a novel framework that utilizes
black-box function evaluations, in conjunction with symbolic expressions that
integrate relationships between the given functions. We employ tree LSTMs to
incorporate the structure of the symbolic expression trees. We use tree
encoding for numbers present in function evaluation data, based on their
decimal representation. We present an evaluation benchmark for this task to
demonstrate our proposed model combines symbolic reasoning and function
evaluation in a fruitful manner, obtaining high accuracies in our experiments.
Our framework generalizes significantly better to expressions of higher depth
and is able to fill partial equations with valid completions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arabshahi_F/0/1/0/all/0/1&quot;&gt;Forough Arabshahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sameer Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Animashree Anandkumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04345">
<title>Engineering Cooperative Smart Things based on Embodied Cognition. (arXiv:1801.04345v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.04345</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of the Internet of Things (IoT) is to transform any thing around us,
such as a trash can or a street light, into a smart thing. A smart thing has
the ability of sensing, processing, communicating and/or actuating. In order to
achieve the goal of a smart IoT application, such as minimizing waste
transportation costs or reducing energy consumption, the smart things in the
application scenario must cooperate with each other without a centralized
control. Inspired by known approaches to design swarm of cooperative and
autonomous robots, we modeled our smart things based on the embodied cognition
concept. Each smart thing is a physical agent with a body composed of a
microcontroller, sensors and actuators, and a brain that is represented by an
artificial neural network. This type of agent is commonly called an embodied
agent. The behavior of these embodied agents is autonomously configured through
an evolutionary algorithm that is triggered according to the application
performance. To illustrate, we have designed three homogeneous prototypes for
smart street lights based on an evolved network. This application has shown
that the proposed approach results in a feasible way of modeling decentralized
smart things with self-developed and cooperative capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nascimento_N/0/1/0/all/0/1&quot;&gt;Nathalia Moraes do Nascimento&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucena_C/0/1/0/all/0/1&quot;&gt;Carlos Jose Pereira de Lucena&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04346">
<title>A Computational Model of Commonsense Moral Decision Making. (arXiv:1801.04346v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.04346</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new computational model of moral decision making, drawing on a
recent theory of commonsense moral learning via social dynamics. Our model
describes moral dilemmas as a utility function that computes trade-offs in
values over abstract moral dimensions, which provide interpretable parameter
values when implemented in machine-led ethical decision-making. Moreover,
characterizing the social structures of individuals and groups as a
hierarchical Bayesian model, we show that a useful description of an
individual&apos;s moral values - as well as a group&apos;s shared values - can be
inferred from a limited amount of observed data. Finally, we apply and evaluate
our approach to data from the Moral Machine, a web application that collects
human judgments on moral dilemmas involving autonomous vehicles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_R/0/1/0/all/0/1&quot;&gt;Richard Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleiman_Weiner_M/0/1/0/all/0/1&quot;&gt;Max Kleiman-Weiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abeliuk_A/0/1/0/all/0/1&quot;&gt;Andres Abeliuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awad_E/0/1/0/all/0/1&quot;&gt;Edmond Awad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dsouza_S/0/1/0/all/0/1&quot;&gt;Sohan Dsouza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Josh Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahwan_I/0/1/0/all/0/1&quot;&gt;Iyad Rahwan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04378">
<title>Fairness in Supervised Learning: An Information Theoretic Approach. (arXiv:1801.04378v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04378</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated decision making systems are increasingly being used in real-world
applications. In these systems for the most part, the decision rules are
derived by minimizing the training error on the available historical data.
Therefore, if there is a bias related to a sensitive attribute such as gender,
race, religion, etc. in the data, say, due to cultural/historical
discriminatory practices against a certain demographic, the system could
continue discrimination in decisions by including the said bias in its decision
rule. We present an information theoretic framework for designing fair
predictors from data, which aim to prevent discrimination against a specified
sensitive attribute in a supervised learning setting. We use equalized odds as
the criterion for discrimination, which demands that the prediction should be
independent of the protected attribute conditioned on the actual label. To
ensure fairness and generalization simultaneously, we compress the data to an
auxiliary variable, which is used for the prediction task. This auxiliary
variable is chosen such that it is decontaminated from the discriminatory
attribute in the sense of equalized odds. The final predictor is obtained by
applying a Bayesian decision rule to the auxiliary variable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghassami_A/0/1/0/all/0/1&quot;&gt;AmirEmad Ghassami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khodadadian_S/0/1/0/all/0/1&quot;&gt;Sajad Khodadadian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1&quot;&gt;Negar Kiyavash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04380">
<title>SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks. (arXiv:1801.04380v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1801.04380</link>
<description rdf:parseType="Literal">&lt;p&gt;Going deeper and wider in neural architectures improves the accuracy, while
the limited GPU DRAM places an undesired restriction on the network design
domain. Deep Learning (DL) practitioners either need change to less desired
network architectures, or nontrivially dissect a network across multiGPUs.
These distract DL practitioners from concentrating on their original machine
learning tasks. We present SuperNeurons: a dynamic GPU memory scheduling
runtime to enable the network training far beyond the GPU DRAM capacity.
SuperNeurons features 3 memory optimizations, \textit{Liveness Analysis},
\textit{Unified Tensor Pool}, and \textit{Cost-Aware Recomputation}, all
together they effectively reduce the network-wide peak memory usage down to the
maximal memory usage among layers. We also address the performance issues in
those memory saving techniques. Given the limited GPU DRAM, SuperNeurons not
only provisions the necessary memory for the training, but also dynamically
allocates the memory for convolution workspaces to achieve the high
performance. Evaluations against Caffe, Torch, MXNet and TensorFlow have
demonstrated that SuperNeurons trains at least 3.2432 deeper network than
current ones with the leading performance. Particularly, SuperNeurons can train
ResNet2500 that has $10^4$ basic network layers on a 12GB K40c.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Linnan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jinmian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yiyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Ang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuaiwen Leon Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zenglin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraska_T/0/1/0/all/0/1&quot;&gt;Tim Kraska&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04520">
<title>Non-Parametric Transformation Networks. (arXiv:1801.04520v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.04520</link>
<description rdf:parseType="Literal">&lt;p&gt;ConvNets have been very effective in many applications where it is required
to learn invariances to within-class nuisance transformations. However, through
their architecture, ConvNets only enforce invariance to translation. In this
paper, we introduce a new class of convolutional architectures called
Non-Parametric Transformation Networks (NPTNs) which can learn general
invariances and symmetries directly from data. NPTNs are a direct and natural
generalization of ConvNets and can be optimized directly using gradient
descent. They make no assumption regarding structure of the invariances present
in the data and in that aspect are very flexible and powerful. We also model
ConvNets and NPTNs under a unified framework called Transformation Networks
which establishes the natural connection between the two. We demonstrate the
efficacy of NPTNs on natural data such as MNIST and CIFAR 10 where it
outperforms ConvNet baselines with the same number of parameters. We show it is
effective in learning invariances unknown apriori directly from data from
scratch. Finally, we apply NPTNs to Capsule Networks and show that they enable
them to perform even better.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_D/0/1/0/all/0/1&quot;&gt;Dipan K. Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1&quot;&gt;Marios Savvides&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04541">
<title>Cooperative Multi-Agent Reinforcement Learning for Low-Level Wireless Communication. (arXiv:1801.04541v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1801.04541</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional radio systems are strictly co-designed on the lower levels of the
OSI stack for compatibility and efficiency. Although this has enabled the
success of radio communications, it has also introduced lengthy standardization
processes and imposed static allocation of the radio spectrum. Various
initiatives have been undertaken by the research community to tackle the
problem of artificial spectrum scarcity by both making frequency allocation
more dynamic and building flexible radios to replace the static ones. There is
reason to believe that just as computer vision and control have been overhauled
by the introduction of machine learning, wireless communication can also be
improved by utilizing similar techniques to increase the flexibility of
wireless networks. In this work, we pose the problem of discovering low-level
wireless communication schemes ex-nihilo between two agents in a fully
decentralized fashion as a reinforcement learning problem. Our proposed
approach uses policy gradients to learn an optimal bi-directional communication
scheme and shows surprisingly sophisticated and intelligent learning behavior.
We present the results of extensive experiments and an analysis of the fidelity
of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vrieze_C/0/1/0/all/0/1&quot;&gt;Colin de Vrieze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barratt_S/0/1/0/all/0/1&quot;&gt;Shane Barratt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tsai_D/0/1/0/all/0/1&quot;&gt;Daniel Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sahai_A/0/1/0/all/0/1&quot;&gt;Anant Sahai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04589">
<title>Deep Reinforcement Fuzzing. (arXiv:1801.04589v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.04589</link>
<description rdf:parseType="Literal">&lt;p&gt;Fuzzing is the process of finding security vulnerabilities in
input-processing code by repeatedly testing the code with modified inputs. In
this paper, we formalize fuzzing as a reinforcement learning problem using the
concept of Markov decision processes. This in turn allows us to apply
state-of-the-art deep Q-learning algorithms that optimize rewards, which we
define from runtime properties of the program under test. By observing the
rewards caused by mutating with a specific set of actions performed on an
initial program input, the fuzzing agent learns a policy that can next generate
new higher-reward inputs. We have implemented this new approach, and
preliminary empirical evidence shows that reinforcement fuzzing can outperform
baseline random fuzzing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bottinger_K/0/1/0/all/0/1&quot;&gt;Konstantin B&amp;#xf6;ttinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godefroid_P/0/1/0/all/0/1&quot;&gt;Patrice Godefroid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rishabh Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04590">
<title>Frame-Recurrent Video Super-Resolution. (arXiv:1801.04590v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.04590</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in video super-resolution have shown that convolutional
neural networks combined with motion compensation are able to merge information
from multiple low-resolution (LR) frames to generate high-quality images.
Current state-of-the-art methods process a batch of LR frames to generate a
single high-resolution (HR) frame and run this scheme in a sliding window
fashion over the entire video, effectively treating the problem as a large
number of separate multi-frame super-resolution tasks. This approach has two
main weaknesses: 1) Each input frame is processed and warped multiple times,
increasing the computational cost, and 2) each output frame is estimated
independently conditioned on the input frames, limiting the system&apos;s ability to
produce temporally consistent results.
&lt;/p&gt;
&lt;p&gt;In this work, we propose an end-to-end trainable frame-recurrent video
super-resolution framework that uses the previously inferred HR estimate to
super-resolve the subsequent frame. This naturally encourages temporally
consistent results and reduces the computational cost by warping only one image
in each step. Furthermore, due to its recurrent nature, the proposed method has
the ability to assimilate a large number of previous frames without increased
computational demands. Extensive evaluations and comparisons with previous
methods validate the strengths of our approach and demonstrate that the
proposed framework is able to significantly outperform the current state of the
art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajjadi_M/0/1/0/all/0/1&quot;&gt;Mehdi S. M. Sajjadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1&quot;&gt;Raviteja Vemulapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1&quot;&gt;Matthew Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04600">
<title>Deep Reinforcement Learning of Cell Movement in the Early Stage of C. elegans Embryogenesis. (arXiv:1801.04600v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.04600</link>
<description rdf:parseType="Literal">&lt;p&gt;Cell movement in the early phase of C. elegans development is regulated by a
highly complex process in which a set of rules and connections are formulated
at distinct scales. Previous efforts have demonstrated that agent-based,
multi-scale modeling systems can integrate physical and biological rules and
provide new avenues to study developmental systems. However, the application of
these systems to model cell movement is still challenging and requires a
comprehensive understanding of regulation networks at the right scales. Recent
developments in deep learning and reinforcement learning provide an
unprecedented opportunity to explore cell movement using 3D time-lapse
microscopy images. We presented a deep reinforcement learning approach within
an agent-based modeling system to characterize cell movement in the embryonic
development of C. elegans. We tested our model through two scenarios within
real developmental processes: the anterior movement of the Cpaaa cell via
intercalation and the restoration of the superficial left-right symmetry. Our
modeling system overcame the local optimization problems encountered by
traditional rule-based, agent-based modeling by using greedy algorithms. It
also overcame the computational challenges in the action selection which has
been plagued by the traditional tabular-based reinforcement learning approach.
Our system can automatically explore the cell movement path by using live
microscopy images and it can provide a unique capability to model cell movement
scenarios where regulatory mechanisms are not well studied. In addition, our
system can be used to explore potential paths of a cell under different
regulatory mechanisms or to facilitate new hypotheses for explaining certain
cell movement behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dali Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengcheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yichi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Husheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1&quot;&gt;Zhirong Bao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04622">
<title>Top k Memory Candidates in Memory Networks for Common Sense Reasoning. (arXiv:1801.04622v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.04622</link>
<description rdf:parseType="Literal">&lt;p&gt;Successful completion of reasoning task requires the agent to have relevant
prior knowledge or some given context of the world dynamics. Usually, the
information provided to the system for a reasoning task is just the query or
some supporting story, which is often not enough for common reasoning tasks.
The goal here is that, if the information provided along the question is not
sufficient to correctly answer the question, the model should choose k most
relevant documents that can aid its inference process. In this work, the model
dynamically selects top k most relevant memory candidates that can be used to
successfully solve reasoning tasks. Experiments were conducted on a subset of
Winograd Schema Challenge (WSC) problems to show that the proposed model has
the potential for commonsense reasoning. The WSC is a test of machine
intelligence, designed to be an improvement on the Turing test.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahajan_V/0/1/0/all/0/1&quot;&gt;Vatsal Mahajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04701">
<title>tau-FPL: Tolerance-Constrained Learning in Linear Time. (arXiv:1801.04701v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04701</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning a classifier with control on the false-positive rate plays a
critical role in many machine learning applications. Existing approaches either
introduce prior knowledge dependent label cost or tune parameters based on
traditional classifiers, which lack consistency in methodology because they do
not strictly adhere to the false-positive rate constraint. In this paper, we
propose a novel scoring-thresholding approach, tau-False Positive Learning
(tau-FPL) to address this problem. We show the scoring problem which takes the
false-positive rate tolerance into accounts can be efficiently solved in linear
time, also an out-of-bootstrap thresholding method can transform the learned
ranking function into a low false-positive classifier. Both theoretical
analysis and experimental results show superior performance of the proposed
tau-FPL over existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Ao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Nan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1&quot;&gt;Jian Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10314">
<title>Crossmodal Attentive Skill Learner. (arXiv:1711.10314v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10314</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the Crossmodal Attentive Skill Learner (CASL), integrated
with the recently-introduced Asynchronous Advantage Option-Critic (A2OC)
architecture [Harb et al., 2017] to enable hierarchical reinforcement learning
across multiple sensory inputs. We provide concrete examples where the approach
not only improves performance in a single task, but accelerates transfer to new
tasks. We demonstrate the attention mechanism anticipates and identifies useful
latent features, while filtering irrelevant sensor modalities during execution.
We modify the Arcade Learning Environment [Bellemare et al., 2013] to support
audio queries, and conduct evaluations of crossmodal learning in the Atari 2600
game Amidar. Finally, building on the recent work of Babaeizadeh et al. [2017],
we open-source a fast hybrid CPU-GPU implementation of CASL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omidshafiei_S/0/1/0/all/0/1&quot;&gt;Shayegan Omidshafiei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dong-Ki Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pazis_J/0/1/0/all/0/1&quot;&gt;Jason Pazis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1&quot;&gt;Jonathan P. How&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01704">
<title>Artificial Intelligence (AI) Methods in Optical Networks: A Comprehensive Survey. (arXiv:1801.01704v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01704</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) is an extensive scientific discipline which
enables computer systems to solve problems by emulating complex biological
processes such as learning, reasoning and self-correction. This paper presents
a comprehensive review of the application of AI techniques for improving
performance of optical communication systems and networks. The use of AI-based
techniques is first studied in applications related to optical transmission,
ranging from the characterization and operation of network components to
performance monitoring, mitigation of nonlinearities, and quality of
transmission estimation. Then, applications related to optical network control
and management are also reviewed, including topics like optical network
planning and operation in both transport and access networks. Finally, the
paper also presents a summary of opportunities and challenges in optical
networking where AI is expected to play a key role in the near future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mata_J/0/1/0/all/0/1&quot;&gt;Javier Mata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miguel_I/0/1/0/all/0/1&quot;&gt;Ignacio de Miguel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+n_R/0/1/0/all/0/1&quot;&gt;Ram&amp;#xf3; n J. Dur&amp;#xe1; n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merayo_N/0/1/0/all/0/1&quot;&gt;Noem&amp;#xed; Merayo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sandeep Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jukan_A/0/1/0/all/0/1&quot;&gt;Admela Jukan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chamania_M/0/1/0/all/0/1&quot;&gt;Mohit Chamania&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03612">
<title>Using probabilistic programs as proposals. (arXiv:1801.03612v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03612</link>
<description rdf:parseType="Literal">&lt;p&gt;Monte Carlo inference has asymptotic guarantees, but can be slow when using
generic proposals. Handcrafted proposals that rely on user knowledge about the
posterior distribution can be efficient, but are difficult to derive and
implement. This paper proposes to let users express their posterior knowledge
in the form of proposal programs, which are samplers written in probabilistic
programming languages. One strategy for writing good proposal programs is to
combine domain-specific heuristic algorithms with neural network models. The
heuristics identify high probability regions, and the neural networks model the
posterior uncertainty around the outputs of the algorithm. Proposal programs
can be used as proposal distributions in importance sampling and
Metropolis-Hastings samplers without sacrificing asymptotic consistency, and
can be optimized offline using inference compilation. Support for optimizing
and using proposal programs is easily implemented in a sampling-based
probabilistic programming runtime. The paper illustrates the proposed technique
with a proposal program that combines RANSAC and neural networks to accelerate
inference in a Bayesian linear regression with outliers model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cusumano_Towner_M/0/1/0/all/0/1&quot;&gt;Marco F. Cusumano-Towner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansinghka_V/0/1/0/all/0/1&quot;&gt;Vikash K. Mansinghka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.07919">
<title>EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis. (arXiv:1612.07919v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1612.07919</link>
<description rdf:parseType="Literal">&lt;p&gt;Single image super-resolution is the task of inferring a high-resolution
image from a single low-resolution input. Traditionally, the performance of
algorithms for this task is measured using pixel-wise reconstruction measures
such as peak signal-to-noise ratio (PSNR) which have been shown to correlate
poorly with the human perception of image quality. As a result, algorithms
minimizing these metrics tend to produce over-smoothed images that lack
high-frequency textures and do not look natural despite yielding high PSNR
values.
&lt;/p&gt;
&lt;p&gt;We propose a novel application of automated texture synthesis in combination
with a perceptual loss focusing on creating realistic textures rather than
optimizing for a pixel-accurate reproduction of ground truth images during
training. By using feed-forward fully convolutional neural networks in an
adversarial training setting, we achieve a significant boost in image quality
at high magnification ratios. Extensive experiments on a number of datasets
show the effectiveness of our approach, yielding state-of-the-art results in
both quantitative and qualitative benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajjadi_M/0/1/0/all/0/1&quot;&gt;Mehdi S. M. Sajjadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirsch_M/0/1/0/all/0/1&quot;&gt;Michael Hirsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04289">
<title>Asynchronous Stochastic Variational Inference. (arXiv:1801.04289v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.04289</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic variational inference (SVI) employs stochastic optimization to
scale up Bayesian computation to massive data. Since SVI is at its core a
stochastic gradient-based algorithm, horizontal parallelism can be harnessed to
allow larger scale inference. We propose a lock-free parallel implementation
for SVI which allows distributed computations over multiple slaves in an
asynchronous style. We show that our implementation leads to linear speed-up
while guaranteeing an asymptotic ergodic convergence rate $O(1/\sqrt(T)$ )
given that the number of slaves is bounded by $\sqrt(T)$ ($T$ is the total
number of iterations). The implementation is done in a high-performance
computing (HPC) environment using message passing interface (MPI) for python
(MPI4py). The extensive empirical evaluation shows that our parallel SVI is
lossless, performing comparably well to its counterpart serial SVI with linear
speed-up.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mohamad_S/0/1/0/all/0/1&quot;&gt;Saad Mohamad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bouchachia_A/0/1/0/all/0/1&quot;&gt;Abdelhamid Bouchachia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sayed_Mouchaweh_M/0/1/0/all/0/1&quot;&gt;Moamar Sayed-Mouchaweh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04295">
<title>Generalization Error Bounds for Noisy, Iterative Algorithms. (arXiv:1801.04295v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04295</link>
<description rdf:parseType="Literal">&lt;p&gt;In statistical learning theory, generalization error is used to quantify the
degree to which a supervised machine learning algorithm may overfit to training
data. Recent work [Xu and Raginsky (2017)] has established a bound on the
generalization error of empirical risk minimization based on the mutual
information $I(S;W)$ between the algorithm input $S$ and the algorithm output
$W$, when the loss function is sub-Gaussian. We leverage these results to
derive generalization error bounds for a broad class of iterative algorithms
that are characterized by bounded, noisy updates with Markovian structure. Our
bounds are very general and are applicable to numerous settings of interest,
including stochastic gradient Langevin dynamics (SGLD) and variants of the
stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm. Furthermore, our
error bounds hold for any output function computed over the path of iterates,
including the last iterate of the algorithm or the average of subsets of
iterates, and also allow for non-uniform sampling of data in successive updates
of the algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pensia_A/0/1/0/all/0/1&quot;&gt;Ankit Pensia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jog_V/0/1/0/all/0/1&quot;&gt;Varun Jog&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loh_P/0/1/0/all/0/1&quot;&gt;Po-Ling Loh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04339">
<title>Estimating the Number of Connected Components in a Graph via Subgraph Sampling. (arXiv:1801.04339v1 [math.ST])</title>
<link>http://arxiv.org/abs/1801.04339</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning properties of large graphs from samples has been an important
problem in statistical network analysis since the early work of Goodman
\cite{Goodman1949} and Frank \cite{Frank1978}. We revisit a problem formulated
by Frank \cite{Frank1978} of estimating the number of connected components in a
large graph based on the subgraph sampling model, in which we randomly sample a
subset of the vertices and observe the induced subgraph. The key question is
whether accurate estimation is achievable in the \emph{sublinear} regime where
only a vanishing fraction of the vertices are sampled. We show that it is
impossible if the parent graph is allowed to contain high-degree vertices or
long induced cycles. For the class of chordal graphs, where induced cycles of
length four or above are forbidden, we characterize the optimal sample
complexity within constant factors and construct linear-time estimators that
provably achieve these bounds. This significantly expands the scope of previous
results which have focused on unbiased estimators and special classes of graphs
such as forests or cliques.
&lt;/p&gt;
&lt;p&gt;Both the construction and the analysis of the proposed methodology rely on
combinatorial properties of chordal graphs and identities of induced subgraph
counts. They, in turn, also play a key role in proving minimax lower bounds
based on construction of random instances of graphs with matching structures of
small subgraphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Klusowski_J/0/1/0/all/0/1&quot;&gt;Jason M. Klusowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yihong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04503">
<title>Multivariate LSTM-FCNs for Time Series Classification. (arXiv:1801.04503v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04503</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past decade, multivariate time series classification has been
receiving a lot of attention. We propose augmenting the existing univariate
time series classification models, LSTM-FCN and ALSTM-FCN with a squeeze and
excitation block to further improve performance. Our proposed models outperform
most of the state of the art models while requiring minimum preprocessing. The
proposed models work efficiently on various complex multivariate time series
classification tasks such as activity recognition or action recognition.
Furthermore, the proposed models are highly efficient at test time and small
enough to deploy on memory constrained systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karim_F/0/1/0/all/0/1&quot;&gt;Fazle Karim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1&quot;&gt;Somshubra Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darabi_H/0/1/0/all/0/1&quot;&gt;Houshang Darabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harford_S/0/1/0/all/0/1&quot;&gt;Samuel Harford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04510">
<title>Brain EEG Time Series Selection: A Novel Graph-Based Approach for Classification. (arXiv:1801.04510v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04510</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain Electroencephalography (EEG) classification is widely applied to
analyze cerebral diseases in recent years. Unfortunately, invalid/noisy EEGs
degrade the diagnosis performance and most previously developed methods ignore
the necessity of EEG selection for classification. To this end, this paper
proposes a novel maximum weight clique-based EEG selection approach, named
mwcEEGs, to map EEG selection to searching maximum similarity-weighted cliques
from an improved Fr\&apos;{e}chet distance-weighted undirected EEG graph
simultaneously considering edge weights and vertex weights. Our mwcEEGs
improves the classification performance by selecting intra-clique pairwise
similar and inter-clique discriminative EEGs with similarity threshold
$\delta$. Experimental results demonstrate the algorithm effectiveness compared
with the state-of-the-art time series selection algorithms on real-world EEG
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1&quot;&gt;Chenglong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pi_D/0/1/0/all/0/1&quot;&gt;Dechang Pi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Lin Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04540">
<title>Fix your classifier: the marginal value of training the last weight layer. (arXiv:1801.04540v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04540</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are commonly used as models for classification for a wide
variety of tasks. Typically, a learned affine transformation is placed at the
end of such models, yielding a per-class value used for classification. This
classifier can have a vast number of parameters, which grows linearly with the
number of possible classes, thus requiring increasingly more resources.
&lt;/p&gt;
&lt;p&gt;In this work we argue that this classifier can be fixed, up to a global scale
constant, with little or no loss of accuracy for most tasks, allowing memory
and computational benefits. Moreover, we show that by initializing the
classifier with a Hadamard matrix we can speed up inference as well. We discuss
the implications for current understanding of neural network models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffer_E/0/1/0/all/0/1&quot;&gt;Elad Hoffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubara_I/0/1/0/all/0/1&quot;&gt;Itay Hubara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soudry_D/0/1/0/all/0/1&quot;&gt;Daniel Soudry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04813">
<title>Predicting Movie Genres Based on Plot Summaries. (arXiv:1801.04813v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1801.04813</link>
<description rdf:parseType="Literal">&lt;p&gt;This project explores several Machine Learning methods to predict movie
genres based on plot summaries. Naive Bayes, Word2Vec+XGBoost and Recurrent
Neural Networks are used for text classification, while K-binary
transformation, rank method and probabilistic classification with learned
probability threshold are employed for the multi-label problem involved in the
genre tagging task.Experiments with more than 250,000 movies show that
employing the Gated Recurrent Units (GRU) neural networks for the probabilistic
classification with learned probability threshold approach achieves the best
result on the test set. The model attains a Jaccard Index of 50.0%, a F-score
of 0.56, and a hit rate of 80.5%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_Q/0/1/0/all/0/1&quot;&gt;Quan Hoang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04856">
<title>Improving Orbit Prediction Accuracy through Supervised Machine Learning. (arXiv:1801.04856v1 [astro-ph.EP])</title>
<link>http://arxiv.org/abs/1801.04856</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the lack of information such as the space environment condition and
resident space objects&apos; (RSOs&apos;) body characteristics, current orbit predictions
that are solely grounded on physics-based models may fail to achieve required
accuracy for collision avoidance and have led to satellite collisions already.
This paper presents a methodology to predict RSOs&apos; trajectories with higher
accuracy than that of the current methods. Inspired by the machine learning
(ML) theory through which the models are learned based on large amounts of
observed data and the prediction is conducted without explicitly modeling space
objects and space environment, the proposed ML approach integrates
physics-based orbit prediction algorithms with a learning-based process that
focuses on reducing the prediction errors. Using a simulation-based space
catalog environment as the test bed, the paper demonstrates three types of
generalization capability for the proposed ML approach: 1) the ML model can be
used to improve the same RSO&apos;s orbit information that is not available during
the learning process but shares the same time interval as the training data; 2)
the ML model can be used to improve predictions of the same RSO at future
epochs; and 3) the ML model based on a RSO can be applied to other RSOs that
share some common features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiaoli Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1507.03734">
<title>Smooth Alternating Direction Methods for Nonsmooth Constrained Convex Optimization. (arXiv:1507.03734v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1507.03734</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose two new alternating direction methods to solve &quot;fully&quot; nonsmooth
constrained convex problems. Our algorithms have the best known worst-case
iteration-complexity guarantee under mild assumptions for both the objective
residual and feasibility gap. Through theoretical analysis, we show how to
update all the algorithmic parameters automatically with clear impact on the
convergence performance. We also provide a representative numerical example
showing the advantages of our methods over the classical alternating direction
methods using a well-known feasibility problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tran_Dinh_Q/0/1/0/all/0/1&quot;&gt;Quoc Tran-Dinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1510.02786">
<title>Recovering a Hidden Community Beyond the Kesten-Stigum Limit in $O(|E| \log^*|V|)$ Time. (arXiv:1510.02786v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1510.02786</link>
<description rdf:parseType="Literal">&lt;p&gt;Community detection is considered for a stochastic block model graph of n
vertices, with K vertices in the planted community, edge probability p for
pairs of vertices both in the community, and edge probability q for other pairs
of vertices.
&lt;/p&gt;
&lt;p&gt;The main focus of the paper is on weak recovery of the community based on the
graph G, with o(K) misclassified vertices on average, in the sublinear regime
$n^{1-o(1)} \leq K \leq o(n).$ A critical parameter is the effective
signal-to-noise ratio $\lambda=K^2(p-q)^2/((n-K)q)$, with $\lambda=1$
corresponding to the Kesten-Stigum threshold. We show that a belief propagation
algorithm achieves weak recovery if $\lambda&amp;gt;1/e$, beyond the Kesten-Stigum
threshold by a factor of $1/e.$ The belief propagation algorithm only needs to
run for $\log^\ast n+O(1) $ iterations, with the total time complexity $O(|E|
\log^*n)$, where $\log^*n$ is the iterated logarithm of $n.$ Conversely, if
$\lambda \leq 1/e$, no local algorithm can asymptotically outperform trivial
random guessing. Furthermore, a linear message-passing algorithm that
corresponds to applying power iteration to the non-backtracking matrix of the
graph is shown to attain weak recovery if and only if $\lambda&amp;gt;1$. In addition,
the belief propagation algorithm can be combined with a linear-time voting
procedure to achieve the information limit of exact recovery (correctly
classify all vertices with high probability) for all $K \ge \frac{n}{\log n}
\left( \rho_{\rm BP} +o(1) \right),$ where $\rho_{\rm BP}$ is a function of
$p/q$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hajek_B/0/1/0/all/0/1&quot;&gt;Bruce Hajek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yihong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaming Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.09391">
<title>Simultaneous Clustering and Estimation of Heterogeneous Graphical Models. (arXiv:1611.09391v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.09391</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider joint estimation of multiple graphical models arising from
heterogeneous and high-dimensional observations. Unlike most previous
approaches which assume that the cluster structure is given in advance, an
appealing feature of our method is to learn cluster structure while estimating
heterogeneous graphical models. This is achieved via a high dimensional version
of Expectation Conditional Maximization (ECM) algorithm (Meng and Rubin, 1993).
A joint graphical lasso penalty is imposed on the conditional maximization step
to extract both homogeneity and heterogeneity components across all clusters.
Our algorithm is computationally efficient due to fast sparse learning routines
and can be implemented without unsupervised learning knowledge. The superior
performance of our method is demonstrated by extensive experiments and its
application to a Glioblastoma cancer dataset reveals some new insights in
understanding the Glioblastoma cancer. In theory, a non-asymptotic error bound
is established for the output directly from our high dimensional ECM algorithm,
and it consists of two quantities: statistical error (statistical accuracy) and
optimization error (computational complexity). Such a result gives a
theoretical guideline in terminating our ECM iterations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hao_B/0/1/0/all/0/1&quot;&gt;Botao Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Will Wei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yufeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1&quot;&gt;Guang Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.03225">
<title>Optimal Generalized Decision Trees via Integer Programming. (arXiv:1612.03225v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1612.03225</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision trees have been a very popular class of predictive models for
decades due to their interpretability and good performance on categorical
features. However, they are not always robust and tend to overfit the data.
Additionally, if allowed to grow large, they lose interpretability. In this
paper, we present a novel mixed integer programming formulation to construct
optimal decision trees of specified size. We take special structure of
categorical features into account and allow combinatorial decisions (based on
subsets of values of such a feature) at each node. We show that very good
accuracy can be achieved with small trees using moderately-sized training sets.
The optimization problems we solve are easily tractable with modern solvers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunluk_O/0/1/0/all/0/1&quot;&gt;Oktay Gunluk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalagnanam_J/0/1/0/all/0/1&quot;&gt;Jayant Kalagnanam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menickelly_M/0/1/0/all/0/1&quot;&gt;Matt Menickelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheinberg_K/0/1/0/all/0/1&quot;&gt;Katya Scheinberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.02461">
<title>Subspace Clustering with Missing and Corrupted Data. (arXiv:1707.02461v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.02461</link>
<description rdf:parseType="Literal">&lt;p&gt;Given full or partial information about a collection of points that lie close
to a union of several subspaces, subspace clustering refers to the process of
clustering the points according to their subspace and identifying the
subspaces. One popular approach, sparse subspace clustering (SSC), represents
each sample as a weighted combination of the other samples, with weights of
minimal $\ell_1$ norm, and then uses those learned weights to cluster the
samples. SSC is stable in settings where each sample is contaminated by a
relatively small amount of noise. However, when there is a significant amount
of additive noise, or a considerable number of entries are missing, theoretical
guarantees are scarce. In this paper, we study a robust variant of SSC and
establish clustering guarantees in the presence of corrupted or missing data.
We give explicit bounds on amount of noise and missing data that the algorithm
can tolerate, both in deterministic settings and in a random generative model.
Notably, our approach provides guarantees for higher tolerance to noise and
missing data than existing analyses for this method. By design, the results
hold even when we do not know the locations of the missing data; e.g., as in
presence-only data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Charles_Z/0/1/0/all/0/1&quot;&gt;Zachary Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jalali_A/0/1/0/all/0/1&quot;&gt;Amin Jalali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Willett_R/0/1/0/all/0/1&quot;&gt;Rebecca Willett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09514">
<title>Asymptotic Analysis via Stochastic Differential Equations of Gradient Descent Algorithms in Statistical and Computational Paradigms. (arXiv:1711.09514v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09514</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates asymptotic behaviors of gradient descent algorithms
(particularly accelerated gradient descent and stochastic gradient descent) in
the context of stochastic optimization arose in statistics and machine learning
where objective functions are estimated from available data. We show that these
algorithms can be modeled by continuous-time ordinary or stochastic
differential equations, and their asymptotic dynamic evolutions and
distributions are governed by some linear ordinary or stochastic differential
equations, as the data size goes to infinity. We illustrate that our study can
provide a novel unified framework for a joint computational and statistical
asymptotic analysis on dynamic behaviors of these algorithms with the time (or
the number of iterations in the algorithms) and large sample behaviors of the
statistical decision rules (like estimators and classifiers) that the
algorithms are applied to compute, where the statistical decision rules are the
limits of the random sequences generated from these iterative algorithms as the
number of iterations goes to infinity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yazhen Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06658">
<title>MEBoost: Mixing Estimators with Boosting for Imbalanced Data Classification. (arXiv:1712.06658v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06658</link>
<description rdf:parseType="Literal">&lt;p&gt;Class imbalance problem has been a challenging research problem in the fields
of machine learning and data mining as most real life datasets are imbalanced.
Several existing machine learning algorithms try to maximize the accuracy
classification by correctly identifying majority class samples while ignoring
the minority class. However, the concept of the minority class instances
usually represents a higher interest than the majority class. Recently, several
cost sensitive methods, ensemble models and sampling techniques have been used
in literature in order to classify imbalance datasets. In this paper, we
propose MEBoost, a new boosting algorithm for imbalanced datasets. MEBoost
mixes two different weak learners with boosting to improve the performance on
imbalanced datasets. MEBoost is an alternative to the existing techniques such
as SMOTEBoost, RUSBoost, Adaboost, etc. The performance of MEBoost has been
evaluated on 12 benchmark imbalanced datasets with state of the art ensemble
methods like SMOTEBoost, RUSBoost, Easy Ensemble, EUSBoost, DataBoost.
Experimental results show significant improvement over the other methods and it
can be concluded that MEBoost is an effective and promising algorithm to deal
with imbalance datasets. The python version of the code is available here:
https://github.com/farshidrayhanuiu/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rayhan_F/0/1/0/all/0/1&quot;&gt;Farshid Rayhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Sajid Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahbub_A/0/1/0/all/0/1&quot;&gt;Asif Mahbub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jani_M/0/1/0/all/0/1&quot;&gt;Md. Rafsan Jani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shatabda_S/0/1/0/all/0/1&quot;&gt;Swakkhar Shatabda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farid_D/0/1/0/all/0/1&quot;&gt;Dewan Md. Farid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_C/0/1/0/all/0/1&quot;&gt;Chowdhury Mofizur Rahman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08655">
<title>Sparse travel time tomography with adaptive dictionaries. (arXiv:1712.08655v2 [physics.geo-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08655</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a 2D travel time tomography method which regularizes the inversion
by modeling groups of slowness pixels from discrete slowness maps, called
patches, as sparse linear combinations of atoms from a dictionary. We further
propose to learn optimal slowness dictionaries using dictionary learning, in
parallel with the inversion. This patch regularization, which we call the local
model, is integrated into the overall slowness map, called the global model.
Where the local model considers small-scale variations using a sparsity
constraint, the global model considers larger-scale features which are
constrained using $\ell_2$-norm regularization. This local-global modeling
strategy with dictionary learning has been successful for image restoration
tasks such as denoising and inpainting, where diverse image content is
recovered from noisy or incomplete measurements. We use this strategy in our
locally-sparse travel time tomography (LST) approach to model simultaneously
smooth and discontinuous slowness features. This is in contrast to conventional
tomography methods, which constrain models to be exclusively smooth or
discontinuous. We develop a $\textit{maximum a posteriori}$ formulation for LST
and exploit the sparsity of slowness patches using dictionary learning. We
demonstrate the LST approach on densely, but irregularly sampled synthetic
slowness maps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bianco_M/0/1/0/all/0/1&quot;&gt;Michael Bianco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gerstoft_P/0/1/0/all/0/1&quot;&gt;Peter Gerstoft&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02901">
<title>Convexification of Neural Graph. (arXiv:1801.02901v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02901</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditionally, most complex intelligence architectures are extremely
non-convex, which could not be well performed by convex optimization. However,
this paper decomposes complex structures into three types of nodes: operators,
algorithms and functions. Iteratively, propagating from node to node along
edge, we prove that &quot;regarding the tree-structured neural graph, it is nearly
convex in each variable, when the other variables are fixed.&quot; In fact, the
non-convex properties stem from circles and functions, which could be
transformed to be convex with our proposed \textit{\textbf{scale mechanism}}.
Experimentally, we justify our theoretical analysis by two practical
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Han Xiao&lt;/a&gt;</dc:creator>
</item></rdf:RDF>