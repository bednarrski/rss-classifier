<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-02T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00509"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06568"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11285"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00587"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00630"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00634"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00660"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00705"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00741"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00751"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00779"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00787"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00873"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00900"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00909"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00912"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.06452"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07387"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02869"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06306"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11192"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00521"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00541"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00571"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00616"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00692"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00801"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00811"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00861"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00862"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00868"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00869"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00878"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00915"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00917"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00928"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.05355"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.09283"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.02963"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.00509">
<title>Spiking Neural Algorithms for Markov Process Random Walk. (arXiv:1805.00509v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.00509</link>
<description rdf:parseType="Literal">&lt;p&gt;The random walk is a fundamental stochastic process that underlies many
numerical tasks in scientific computing applications. We consider here two
neural algorithms that can be used to efficiently implement random walks on
spiking neuromorphic hardware. The first method tracks the positions of
individual walkers independently by using a modular code inspired by the grid
cell spatial representation in the brain. The second method tracks the
densities of random walkers at each spatial location directly. We analyze the
scaling complexity of each of these methods and illustrate their ability to
model random walkers under different probabilistic conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Severa_W/0/1/0/all/0/1&quot;&gt;William Severa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehoucq_R/0/1/0/all/0/1&quot;&gt;Rich Lehoucq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parekh_O/0/1/0/all/0/1&quot;&gt;Ojas Parekh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aimone_J/0/1/0/all/0/1&quot;&gt;James B. Aimone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06568">
<title>ES Is More Than Just a Traditional Finite-Difference Approximator. (arXiv:1712.06568v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06568</link>
<description rdf:parseType="Literal">&lt;p&gt;An evolution strategy (ES) variant based on a simplification of a natural
evolution strategy recently attracted attention because it performs
surprisingly well in challenging deep reinforcement learning domains. It
searches for neural network parameters by generating perturbations to the
current set of parameters, checking their performance, and moving in the
aggregate direction of higher reward. Because it resembles a traditional
finite-difference approximation of the reward gradient, it can naturally be
confused with one. However, this ES optimizes for a different gradient than
just reward: It optimizes for the average reward of the entire population,
thereby seeking parameters that are robust to perturbation. This difference can
channel ES into distinct areas of the search space relative to gradient
descent, and also consequently to networks with distinct properties. This
unique robustness-seeking property, and its consequences for optimization, are
demonstrated in several domains. They include humanoid locomotion, where
networks from policy gradient-based reinforcement learning are significantly
less robust to parameter perturbation than ES-based policies solving the same
task. While the implications of such robustness and robustness-seeking remain
open to further study, this work&apos;s main contribution is to highlight such
differences and their potential importance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1&quot;&gt;Joel Lehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jay Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1&quot;&gt;Kenneth O. Stanley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11285">
<title>Adversarially Robust Generalization Requires More Data. (arXiv:1804.11285v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.11285</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models are often susceptible to adversarial perturbations of
their inputs. Even small perturbations can cause state-of-the-art classifiers
with high &quot;standard&quot; accuracy to produce an incorrect prediction with high
confidence. To better understand this phenomenon, we study adversarially robust
learning from the viewpoint of generalization. We show that already in a simple
natural data model, the sample complexity of robust learning can be
significantly larger than that of &quot;standard&quot; learning. This gap is information
theoretic and holds irrespective of the training algorithm or the model family.
We complement our theoretical results with experiments on popular image
classification datasets and show that a similar gap exists here as well. We
postulate that the difficulty of training robust classifiers stems, at least
partially, from this inherently larger sample complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santurkar_S/0/1/0/all/0/1&quot;&gt;Shibani Santurkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsipras_D/0/1/0/all/0/1&quot;&gt;Dimitris Tsipras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1&quot;&gt;Kunal Talwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1&quot;&gt;Aleksander M&amp;#x105;dry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00587">
<title>Structure-sensitive Multi-scale Deep Neural Network for Low-Dose CT Denoising. (arXiv:1805.00587v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.00587</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed tomography (CT) is a popular medical imaging modality in clinical
applications. At the same time, the x-ray radiation dose associated with CT
scans raises public concerns due to its potential risks to the patients. Over
the past years, major efforts have been dedicated to the development of
Low-Dose CT (LDCT) methods. However, the radiation dose reduction compromises
the signal-to-noise ratio (SNR), leading to strong noise and artifacts that
down-grade CT image quality. In this paper, we propose a novel 3D noise
reduction method, called Structure-sensitive Multi-scale Generative Adversarial
Net (SMGAN), to improve the LDCT image quality. Specifically, we incorporate
three-dimensional (3D) volumetric information to improve the image quality.
Also, different loss functions for training denoising models are investigated.
Experiments show that the proposed method can effectively preserve structural
and texture information from normal-dose CT (NDCT) images, and significantly
suppress noise and artifacts. Qualitative visual assessments by three
experienced radiologists demonstrate that the proposed method retrieves more
detailed information, and outperforms competing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenyu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qingsong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1&quot;&gt;Hongming Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gjesteby_L/0/1/0/all/0/1&quot;&gt;Lars Gjesteby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guang_L/0/1/0/all/0/1&quot;&gt;Li Guang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_S/0/1/0/all/0/1&quot;&gt;Shenghong Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuiyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1&quot;&gt;Wenxiang Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Ge Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00630">
<title>Residential Transformer Overloading Risk Assessment Using Clustering Analysis. (arXiv:1805.00630v1 [cs.CE])</title>
<link>http://arxiv.org/abs/1805.00630</link>
<description rdf:parseType="Literal">&lt;p&gt;Residential transformer population is a critical type of asset that many
electric utility companies have been attempting to manage proactively and
effectively to reduce unexpected failures and life losses that are often caused
by transformer overloading. Within the typical power asset portfolio, the
residential transformer asset is often large in population, having lowest
reliability design, lacking transformer loading data and susceptible to
customer loading behaviors such as adoption of distributed energy resources and
electric vehicles. On the bright side, the availability of more residential
operation data along with the advancement of data analytics techniques have
provided a new path to further our understanding of local residential
transformer overloading risks statistically. This research developed a new
data-driven method to combine clustering analysis and the simulation of
transformer temperature rise and insulation life loss to quantitatively and
statistically assess the overloading risk of residential transformer population
in one area and suggest proper risk management measures according to the
assessment results. Case studies from an actual Canadian utility company have
been presented and discussed in detail to demonstrate the applicability and
usefulness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;Ming Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Benzhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nassif_A/0/1/0/all/0/1&quot;&gt;Alex Nassif&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00634">
<title>A Probabilistic Extension of Action Language BC+. (arXiv:1805.00634v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.00634</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a probabilistic extension of action language BC+. Just like BC+ is
defined as a high-level notation of answer set programs for describing
transition systems, the proposed language, which we call pBC+, is defined as a
high-level notation of LPMLN programs---a probabilistic extension of answer set
programs. We show how probabilistic reasoning about transition systems, such as
prediction, postdiction, and planning problems, as well as probabilistic
diagnosis for dynamic domains, can be modeled in pBC+ and computed using an
implementation of LPMLN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joohyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00660">
<title>Functional ASP with Intensional Sets: Application to Gelfond-Zhang Aggregates. (arXiv:1805.00660v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.00660</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a variant of Answer Set Programming (ASP) with
evaluable functions that extends their application to sets of objects,
something that allows a fully logical treatment of aggregates. Formally, we
start from the syntax of First Order Logic with equality and the semantics of
Quantified Equilibrium Logic with evaluable functions (QELF). Then, we proceed
to incorporate a new kind of logical term, intensional set (a construct
commonly used to denote the set of objects characterised by a given formula),
and to extend QELF semantics for this new type of expression. In our extended
approach, intensional sets can be arbitrarily used as predicate or function
arguments or even nested inside other intensional sets, just as regular
first-order logical terms. As a result, aggregates can be naturally formed by
the application of some evaluable function (count, sum, maximum, etc) to a set
of objects expressed as an intensional set. This approach has several
advantages. First, while other semantics for aggregates depend on some
syntactic transformation (either via a reduct or a formula translation), the
QELF interpretation treats them as regular evaluable functions, providing a
compositional semantics and avoiding any kind of syntactic restriction. Second,
aggregates can be explicitly defined now within the logical language by the
simple addition of formulas that fix their meaning in terms of multiple
applications of some (commutative and associative) binary operation. For
instance, we can use recursive rules to define sum in terms of integer
addition. Last, but not least, we prove that the semantics we obtain for
aggregates coincides with the one defined by Gelfond and Zhang for the Alog
language, when we restrict to that syntactic fragment. (Under consideration for
acceptance in TPLP)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabalar_P/0/1/0/all/0/1&quot;&gt;Pedro Cabalar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fandinno_J/0/1/0/all/0/1&quot;&gt;Jorge Fandinno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerro_L/0/1/0/all/0/1&quot;&gt;Luis Fari&amp;#xf1;as del Cerro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pearce_D/0/1/0/all/0/1&quot;&gt;David Pearce&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00705">
<title>Investigating Audio, Visual, and Text Fusion Methods for End-to-End Automatic Personality Prediction. (arXiv:1805.00705v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.00705</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a tri-modal architecture to predict Big Five personality trait
scores from video clips with different channels for audio, text, and video
data. For each channel, stacked Convolutional Neural Networks are employed. The
channels are fused both on decision-level and by concatenating their respective
fully connected layers. It is shown that a multimodal fusion approach
outperforms each single modality channel, with an improvement of 9.4\% over the
best individual modality (video). Full backpropagation is also shown to be
better than a linear combination of modalities, meaning complex interactions
between modalities can be leveraged to build better models. Furthermore, we can
see the prediction relevance of each modality for each trait. The described
model can be used to increase the emotional intelligence of virtual agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1&quot;&gt;Elham J. Barezi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampman_O/0/1/0/all/0/1&quot;&gt;Onno Kampman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertero_D/0/1/0/all/0/1&quot;&gt;Dario Bertero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1&quot;&gt;Pascale Fung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00741">
<title>KNPTC: Knowledge and Neural Machine Translation Powered Chinese Pinyin Typo Correction. (arXiv:1805.00741v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.00741</link>
<description rdf:parseType="Literal">&lt;p&gt;Chinese pinyin input methods are very important for Chinese language
processing. Actually, users may make typos inevitably when they input pinyin.
Moreover, pinyin typo correction has become an increasingly important task with
the popularity of smartphones and the mobile Internet. How to exploit the
knowledge of users typing behaviors and support the typo correction for acronym
pinyin remains a challenging problem. To tackle these challenges, we propose
KNPTC, a novel approach based on neural machine translation (NMT). In contrast
to previous work, KNPTC is able to integrate explicit knowledge into NMT for
pinyin typo correction, and is able to learn to correct a variety of typos
without the guidance of manually selected constraints or languagespecific
features. In this approach, we first obtain the transition probabilities
between adjacent letters based on large-scale real-life datasets. Then, we
construct the &quot;ground-truth&quot; alignments of training sentence pairs by utilizing
these probabilities. Furthermore, these alignments are integrated into NMT to
capture sensible pinyin typo correction patterns. KNPTC is applied to correct
typos in real-life datasets, which achieves 32.77% increment on average in
accuracy rate of typo correction compared against the state-of-the-art system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Hengyi Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xingguang Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yonghao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yan Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansur_M/0/1/0/all/0/1&quot;&gt;Mairgup Mansur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiaofang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00751">
<title>From the Periphery to the Center: Information Brokerage in an Evolving Network. (arXiv:1805.00751v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.00751</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpersonal ties are pivotal to individual efficacy, status and performance
in an agent society. This paper explores three important and interrelated
themes in social network theory: the center/periphery partition of the network;
network dynamics; and social integration of newcomers. We tackle the question:
How would a newcomer harness information brokerage to integrate into a dynamic
network going from periphery to center? We model integration as the interplay
between the newcomer and the dynamics network and capture information brokerage
using a process of relationship building. We analyze theoretical guarantees for
the newcomer to reach the center through tactics; proving that a winning tactic
always exists for certain types of network dynamics. We then propose three
tactics and show their superior performance over alternative methods on four
real-world datasets and four network models. In general, our tactics place the
newcomer to the center by adding very few new edges on dynamic networks with
approximately 14000 nodes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bo Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yiping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiamou Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yijin Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hongyi Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hong Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00779">
<title>COBRAS-TS: A new approach to Semi-Supervised Clustering of Time Series. (arXiv:1805.00779v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.00779</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering is ubiquitous in data analysis, including analysis of time series.
It is inherently subjective: different users may prefer different clusterings
for a particular dataset. Semi-supervised clustering addresses this by allowing
the user to provide examples of instances that should (not) be in the same
cluster. This paper studies semi-supervised clustering in the context of time
series. We show that COBRAS, a state-of-the-art semi-supervised clustering
method, can be adapted to this setting. We refer to this approach as COBRAS-TS.
An extensive experimental evaluation supports the following claims: (1)
COBRAS-TS far outperforms the current state of the art in semi-supervised
clustering for time series, and thus presents a new baseline for the field; (2)
COBRAS-TS can identify clusters with separated components; (3) COBRAS-TS can
identify clusters that are characterized by small local patterns; (4) a small
amount of semi-supervision can greatly improve clustering quality for time
series; (5) the choice of the clustering algorithm matters (contrary to earlier
claims in the literature).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Craenendonck_T/0/1/0/all/0/1&quot;&gt;Toon Van Craenendonck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meert_W/0/1/0/all/0/1&quot;&gt;Wannes Meert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dumancic_S/0/1/0/all/0/1&quot;&gt;Sebastijan Dumancic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blockeel_H/0/1/0/all/0/1&quot;&gt;Hendrik Blockeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00787">
<title>Cognition in Dynamical Systems, Second Edition. (arXiv:1805.00787v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1805.00787</link>
<description rdf:parseType="Literal">&lt;p&gt;Cognition is the process of knowing. As carried out by a dynamical system, it
is the process by which the system absorbs information into its state. A
complex network of agents cognizes knowledge about its environment, internal
dynamics and initial state by forming emergent, macro-level patterns. Such
patterns require each agent to find its place while partially aware of the
whole pattern. Such partial awareness can be achieved by separating the system
dynamics into two parts by timescale: the propagation dynamics and the pattern
dynamics. The fast propagation dynamics describe the spread of signals across
the network. If they converge to a fixed point for any quasi-static state of
the slow pattern dynamics, that fixed point represents an aggregate of
macro-level information. On longer timescales, agents coordinate via positive
feedback to form patterns, which are defined using closed walks in the graph of
agents. Patterns can be coherent, in that every part of the pattern depends on
every other part for context. Coherent patterns are acausal, in that (a) they
cannot be predicted and (b) no part of the stored knowledge can be mapped to
any part of the pattern, or vice versa. A cognitive network&apos;s knowledge is
encoded or embodied by the selection of patterns which emerge. The theory of
cognition summarized here can model autocatalytic reaction-diffusion systems,
artificial neural networks, market economies and ant colony optimization, among
many other real and virtual systems. This theory suggests a new understanding
of complexity as a lattice of contexts rather than a single measure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1&quot;&gt;Jack Hall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00873">
<title>A Hybrid Q-Learning Sine-Cosine-based Strategy for Addressing the Combinatorial Test Suite Minimization Problem. (arXiv:1805.00873v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.00873</link>
<description rdf:parseType="Literal">&lt;p&gt;The sine-cosine algorithm (SCA) is a new population-based meta-heuristic
algorithm. In addition to exploiting sine and cosine functions to perform local
and global searches (hence the name sine-cosine), the SCA introduces several
random and adaptive parameters to facilitate the search process. Although it
shows promising results, the search process of the SCA is vulnerable to local
minima/maxima due to the adoption of a fixed switch probability and the bounded
magnitude of the sine and cosine functions (from -1 to 1). In this paper, we
propose a new hybrid Q-learning sine-cosine- based strategy, called the
Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the
switching probability. Instead, we rely on the Q-learning algorithm (based on
the penalty and reward mechanism) to dynamically identify the best operation
during runtime. Additionally, we integrate two new operations (L\&apos;evy flight
motion and crossover) into the QLSCA to facilitate jumping out of local
minima/maxima and enhance the solution diversity. To assess its performance, we
adopt the QLSCA for the combinatorial test suite minimization problem.
Experimental results reveal that the QLSCA is statistically superior with
regard to test suite size reduction compared to recent state-of-the-art
strategies, including the original SCA, the particle swarm test generator
(PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search
strategy (CS) at the 95% confidence level. However, concerning the comparison
with discrete particle swarm optimization (DPSO), there is no significant
difference in performance at the 95% confidence level. On a positive note, the
QLSCA statistically outperforms the DPSO in certain configurations at the 90%
confidence level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamli_K/0/1/0/all/0/1&quot;&gt;Kamal Z. Zamli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Din_F/0/1/0/all/0/1&quot;&gt;Fakhrud Din&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_B/0/1/0/all/0/1&quot;&gt;Bestoun S. Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bures_M/0/1/0/all/0/1&quot;&gt;Miroslav Bures&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00900">
<title>Images &amp; Recipes: Retrieval in the cooking context. (arXiv:1805.00900v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.00900</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in the machine learning community allowed different use cases
to emerge, as its association to domains like cooking which created the
computational cuisine. In this paper, we tackle the picture-recipe alignment
problem, having as target application the large-scale retrieval task (finding a
recipe given a picture, and vice versa). Our approach is validated on the
Recipe1M dataset, composed of one million image-recipe pairs and additional
class information, for which we achieve state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_M/0/1/0/all/0/1&quot;&gt;Micael Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cadene_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Cad&amp;#xe8;ne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1&quot;&gt;David Picard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1&quot;&gt;Laure Soulier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1&quot;&gt;Matthieu Cord&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00909">
<title>Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review. (arXiv:1805.00909v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.00909</link>
<description rdf:parseType="Literal">&lt;p&gt;The framework of reinforcement learning or optimal control provides a
mathematical formalization of intelligent decision making that is powerful and
broadly applicable. While the general form of the reinforcement learning
problem enables effective reasoning about uncertainty, the connection between
reinforcement learning and inference in probabilistic models is not immediately
obvious. However, such a connection has considerable value when it comes to
algorithm design: formalizing a problem as probabilistic inference in principle
allows us to bring to bear a wide array of approximate inference tools, extend
the model in flexible and powerful ways, and reason about compositionality and
partial observability. In this article, we will discuss how a generalization of
the reinforcement learning or optimal control problem, which is sometimes
termed maximum entropy reinforcement learning, is equivalent to exact
probabilistic inference in the case of deterministic dynamics, and variational
inference in the case of stochastic dynamics. We will present a detailed
derivation of this framework, overview prior work that has drawn on this and
related ideas to propose new reinforcement learning and control algorithms, and
describe perspectives on future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00912">
<title>Fast Directional Self-Attention Mechanism. (arXiv:1805.00912v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.00912</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a self-attention mechanism, dubbed &quot;fast
directional self-attention (Fast-DiSA)&quot;, which is a fast and light extension of
&quot;directional self-attention (DiSA)&quot;. The proposed Fast-DiSA performs as
expressively as the original DiSA but only uses much less computation time and
memory, in which 1) both token2token and source2token dependencies are modeled
by a joint compatibility function designed for a hybrid of both dot-product and
multi-dim ways; 2) both multi-head and multi-dim attention combined with
bi-directional temporal information captured by multiple positional masks are
in consideration without heavy time and memory consumption appearing in the
DiSA. The experiment results show that the proposed Fast-DiSA can achieve
state-of-the-art performance as fast and memory-friendly as CNNs. The code for
Fast-DiSA is released at
\url{https://github.com/taoshen58/DiSAN/tree/master/Fast-DiSA}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1&quot;&gt;Tao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1&quot;&gt;Guodong Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengqi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.06452">
<title>Algorithms for Semantic Segmentation of Multispectral Remote Sensing Imagery using Deep Learning. (arXiv:1703.06452v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1703.06452</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional neural networks (DCNNs) have been used to achieve
state-of-the-art performance on many computer vision tasks (e.g., object
recognition, object detection, semantic segmentation) thanks to a large
repository of annotated image data. Large labeled datasets for other sensor
modalities, e.g., multispectral imagery (MSI), are not available due to the
large cost and manpower required. In this paper, we adapt state-of-the-art DCNN
frameworks in computer vision for semantic segmentation for MSI imagery. To
overcome label scarcity for MSI data, we substitute real MSI for generated
synthetic MSI in order to initialize a DCNN framework. We evaluate our network
initialization scheme on the new RIT-18 dataset that we present in this paper.
This dataset contains very-high resolution MSI collected by an unmanned
aircraft system. The models initialized with synthetic imagery were less prone
to over-fitting and provide a state-of-the-art baseline for future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kemker_R/0/1/0/all/0/1&quot;&gt;Ronald Kemker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salvaggio_C/0/1/0/all/0/1&quot;&gt;Carl Salvaggio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1&quot;&gt;Christopher Kanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07387">
<title>How morphological development can guide evolution. (arXiv:1711.07387v3 [q-bio.PE] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07387</link>
<description rdf:parseType="Literal">&lt;p&gt;Organisms result from adaptive processes interacting across different time
scales. One such interaction is that between development and evolution. Models
have shown that development sweeps over several traits in a single agent,
sometimes exposing promising static traits. Subsequent evolution can then
canalize these rare traits. Thus, development can, under the right conditions,
increase evolvability. Here, we report on a previously unknown phenomenon when
embodied agents are allowed to develop and evolve: Evolution discovers body
plans robust to control changes, these body plans become genetically
assimilated, yet controllers for these agents are not assimilated. This allows
evolution to continue climbing fitness gradients by tinkering with the
developmental programs for controllers within these permissive body plans. This
exposes a previously unknown detail about the Baldwin effect: instead of all
useful traits becoming genetically assimilated, only traits that render the
agent robust to changes in other traits become assimilated. We refer to this as
differential canalization. This finding also has implications for the
evolutionary design of artificial and embodied agents such as robots: robots
robust to internal changes in their controllers may also be robust to external
changes in their environment, such as transferal from simulation to reality or
deployment in novel environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kriegman_S/0/1/0/all/0/1&quot;&gt;Sam Kriegman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cheney_N/0/1/0/all/0/1&quot;&gt;Nick Cheney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bongard_J/0/1/0/all/0/1&quot;&gt;Josh Bongard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02869">
<title>Perspectival Knowledge in PSOA RuleML: Representation, Model Theory, and Translation. (arXiv:1712.02869v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.02869</link>
<description rdf:parseType="Literal">&lt;p&gt;In Positional-Slotted Object-Applicative (PSOA) RuleML, a predicate
application (atom) can have an Object IDentifier (OID) and descriptors that may
be positional arguments (tuples) or attribute-value pairs (slots). PSOA RuleML
1.0 specifies for each descriptor whether it is to be interpreted under the
perspective of the predicate in whose scope it occurs. This perspectivity
dimension refines the space between oidless, positional atoms (relationships)
and oidful, slotted atoms (frames): While relationships use only a
predicate-scope-sensitive (predicate-dependent) tuple and frames use only
predicate-scope-insensitive (predicate-independent) slots, PSOA RuleML 1.0 uses
a systematics of orthogonal constructs also permitting atoms with
(predicate-)independent tuples and atoms with (predicate-)dependent slots. This
supports data and knowledge representation where a slot attribute can have
different values depending on the predicate. PSOA thus extends object-oriented
multi-membership and multiple inheritance. Based on objectification, PSOA laws
are given: Besides unscoping and centralization, the semantic restriction and
transformation of describution permits rescoping of one atom&apos;s independent
descriptors to another atom with the same OID but a different predicate. For
inheritance, default descriptors are realized by rules. On top of a metamodel
and a Grailog visualization, PSOA&apos;s atom systematics for facts, queries, and
rules is explained. The presentation and (XML-)serialization syntaxes of PSOA
RuleML 1.0 are introduced. Its model-theoretic semantics is formalized by
extending the interpretation functions for dependent descriptors. The open
PSOATransRun system since Version 1.3 realizes PSOA RuleML 1.0 by a translator
to runtime predicates, including for dependent tuples (prdtupterm) and slots
(prdsloterm). Our tests show efficiency advantages of dependent and tupled
modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boley_H/0/1/0/all/0/1&quot;&gt;Harold Boley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_G/0/1/0/all/0/1&quot;&gt;Gen Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06306">
<title>Optimizing Interactive Systems with Data-Driven Objectives. (arXiv:1802.06306v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06306</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective optimization is essential for interactive systems to provide a
satisfactory user experience. However, it is often challenging to find an
objective to optimize for. Generally, such objectives are manually crafted and
rarely capture complex user needs accurately. Conversely, we propose an
approach that infers the objective directly from observed user interactions.
These inferences can be made regardless of prior knowledge and across different
types of user behavior. Then we introduce: Interactive System Optimizer (ISO),
a novel algorithm that uses these inferred objectives for optimization. Our
main contribution is a new general principled approach to optimizing
interactive systems using data-driven objectives. We demonstrate the high
effectiveness of ISO over several GridWorld simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grotov_A/0/1/0/all/0/1&quot;&gt;Artem Grotov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1&quot;&gt;Julia Kiseleva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1&quot;&gt;Maarten de Rijke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oosterhuis_H/0/1/0/all/0/1&quot;&gt;Harrie Oosterhuis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11192">
<title>Explainable Recommendation: A Survey and New Perspectives. (arXiv:1804.11192v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1804.11192</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable Recommendation refers to the personalized recommendation
algorithms that address the problem of why -- they not only provide the user
with the recommendations, but also make the user aware why such items are
recommended by generating recommendation explanations, which help to improve
the effectiveness, efficiency, persuasiveness, and user satisfaction of
recommender systems. In recent years, a large number of explainable
recommendation approaches -- especially model-based explainable recommendation
algorithms -- have been proposed and adopted in real-world systems.
&lt;/p&gt;
&lt;p&gt;In this survey, we review the work on explainable recommendation that has
been published in or before the year of 2018. We first high-light the position
of explainable recommendation in recommender system research by categorizing
recommendation problems into the 5W, i.e., what, when, who, where, and why. We
then conduct a comprehensive survey of explainable recommendation itself in
terms of three aspects: 1) We provide a chronological research line of
explanations in recommender systems, including the user study approaches in the
early years, as well as the more recent model-based approaches. 2) We provide a
taxonomy for explainable recommendation algorithms, including user-based,
item-based, model-based, and post-model explanations. 3) We summarize the
application of explainable recommendation in different recommendation tasks,
including product recommendation, social recommendation, POI recommendation,
etc. We devote a chapter to discuss the explanation perspectives in the broader
IR and machine learning settings, as well as their relationship with
explainable recommendation research. We end the survey by discussing potential
future research directions to promote the explainable recommendation research
area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00521">
<title>Direct Runge-Kutta Discretization Achieves Acceleration. (arXiv:1805.00521v1 [math.OC])</title>
<link>http://arxiv.org/abs/1805.00521</link>
<description rdf:parseType="Literal">&lt;p&gt;We study gradient-based optimization methods obtained by directly
discretizing a second-order ordinary differential equation (ODE) related to the
continuous limit of Nesterov&apos;s accelerated gradient. When the function is
smooth enough, we show that acceleration can be achieved by a stable
discretization of the ODE using standard Runge-Kutta integrators. Specifically,
we prove that under Lipschitz-gradient, convexity, and order-$(s+2)$
differentiability assumptions, the sequence of iterates generated by
discretizing the proposed second-order ODE converges to the optimal solution at
a rate of $\mathcal{O}({N^{-2\frac{s}{s+1}}})$, where $s$ is the order of the
Runge-Kutta numerical integrator. By increasing $s$, the convergence rate of
our method approaches the optimal rate of $\mathcal{O}({N^{-2}})$. Furthermore,
we introduce a new local flatness condition on the objective, according to
which rates even faster than $\Oc(N^{-2})$ can be achieved with low-order
integrators and only gradient information. Notably, this flatness condition is
satisfied by several standard loss functions used in machine learning, and it
may be of broader independent interest. We provide numerical experiments that
verify the theoretical rates predicted by our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingzhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mokhtari_A/0/1/0/all/0/1&quot;&gt;Aryan Mokhtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sra_S/0/1/0/all/0/1&quot;&gt;Suvrit Sra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jadbabaie_A/0/1/0/all/0/1&quot;&gt;Ali Jadbabaie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00541">
<title>Scalable Importance Tempering and Bayesian Variable Selection. (arXiv:1805.00541v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1805.00541</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a Monte Carlo algorithm to sample from high-dimensional
probability distributions that combines Markov chain Monte Carlo (MCMC) and
importance sampling. We provide a careful theoretical analysis, including
guarantees on robustness to high-dimensionality, explicit comparison with
standard MCMC and illustrations of the potential improvements in efficiency.
Simple and concrete intuition is provided for when the novel scheme is expected
to outperform standard schemes. When applied to Bayesian Variable Selection
problems, the novel algorithm is orders of magnitude more efficient than
available alternative sampling schemes and allows to perform fast and reliable
fully Bayesian inferences with tens of thousands regressors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zanella_G/0/1/0/all/0/1&quot;&gt;Giacomo Zanella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_G/0/1/0/all/0/1&quot;&gt;Gareth Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00571">
<title>Solid Harmonic Wavelet Scattering for Predictions of Molecule Properties. (arXiv:1805.00571v1 [physics.chem-ph])</title>
<link>http://arxiv.org/abs/1805.00571</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a machine learning algorithm for the prediction of molecule
properties inspired by ideas from density functional theory. Using
Gaussian-type orbital functions, we create surrogate electronic densities of
the molecule from which we compute invariant &quot;solid harmonic scattering
coefficients&quot; that account for different types of interactions at different
scales. Multi-linear regressions of various physical properties of molecules
are computed from these invariant coefficients. Numerical experiments show that
these regressions have near state of the art performance, even with relatively
few training examples. Predictions over small sets of scattering coefficients
can reach a DFT precision while being interpretable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Eickenberg_M/0/1/0/all/0/1&quot;&gt;Michael Eickenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Exarchakis_G/0/1/0/all/0/1&quot;&gt;Georgios Exarchakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hirn_M/0/1/0/all/0/1&quot;&gt;Matthew Hirn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mallat_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Mallat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Thiry_L/0/1/0/all/0/1&quot;&gt;Louis Thiry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00616">
<title>$\ell_1$-regression with Heavy-tailed Distributions. (arXiv:1805.00616v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.00616</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the problem of linear regression with heavy-tailed
distributions. Different from previous studies that use the squared loss to
measure the performance, we choose the absolute loss, which is more robust in
the presence of large prediction errors. To address the challenge that both the
input and output could be heavy-tailed, we propose a truncated minimization
problem, and demonstrate that it enjoys an $\widetilde{O}(\sqrt{d/n})$ excess
risk, where $d$ is the dimensionality and $n$ is the number of samples.
Compared with traditional work on $\ell_1$-regression, the main advantage of
our result is that we achieve a high-probability risk bound without exponential
moment conditions on the input and output. Our theoretical guarantee is derived
from a novel combination of the PAC-Bayesian analysis and the covering number.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lijun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhi-Hua Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00692">
<title>Compressed Dictionary Learning. (arXiv:1805.00692v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.00692</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we show that the computational complexity of the Iterative
Thresholding and K-Residual-Means (ITKrM) algorithm for dictionary learning can
be significantly reduced by using dimensionality reduction techniques based on
the Johnson-Lindenstrauss Lemma. We introduce the Iterative
Compressed-Thresholding and K-Means (IcTKM) algorithm for fast dictionary
learning and study its convergence properties. We show that IcTKM can locally
recover a generating dictionary with low computational complexity up to a
target error $\tilde{\varepsilon}$ by compressing $d$-dimensional training data
into $m &amp;lt; d$ dimensions, where $m$ is proportional to $\log d$ and inversely
proportional to the distortion level $\delta$ incurred by compressing the data.
Increasing the distortion level $\delta$ reduces the computational complexity
of IcTKM at the cost of an increased recovery error and reduced admissible
sparsity level for the training data. For generating dictionaries comprised of
$K$ atoms, we show that IcTKM can stably recover the dictionary with distortion
levels up to the order $\delta \leq O(1/\sqrt{\log K})$. The compression
effectively shatters the data dimension bottleneck in the computational cost of
the ITKrM algorithm. For training data with sparsity levels $S \leq
O(K^{2/3})$, ITKrM can locally recover the dictionary with a computational cost
that scales as $O(d K \log(\tilde{\varepsilon}^{-1}))$ per training signal. We
show that for these same sparsity levels the computational cost can be brought
down to $O(\log^5 (d) K \log(\tilde{\varepsilon}^{-1}))$ with IcTKM, a
significant reduction when high-dimensional data is considered. Our theoretical
results are complemented with numerical simulations which demonstrate that
IcTKM is a powerful, low-cost algorithm for learning dictionaries from
high-dimensional data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Teixeira_F/0/1/0/all/0/1&quot;&gt;Flavio Teixeira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schnass_K/0/1/0/all/0/1&quot;&gt;Karin Schnass&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00794">
<title>ECG Heartbeat Classification: A Deep Transferable Representation. (arXiv:1805.00794v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1805.00794</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrocardiogram (ECG) can be reliably used as a measure to monitor the
functionality of the cardiovascular system. Recently, there has been a great
attention towards accurate categorization of heartbeats. While there are many
commonalities between different ECG conditions, the focus of most studies has
been classifying a set of conditions on a dataset annotated for that task
rather than learning and employing a transferable knowledge between different
tasks. In this paper, we propose a method based on deep convolutional neural
networks for the classification of heartbeats which is able to accurately
classify five different arrhythmias in accordance with the AAMI EC57 standard.
Furthermore, we suggest a method for transferring the knowledge acquired on
this task to the myocardial infarction (MI) classification task. We evaluated
the proposed method on PhysionNet&apos;s MIT-BIH and PTB Diagnostics datasets.
According to the results, the suggested method is able to make predictions with
the average accuracies of 93.4% and 95.9% on arrhythmia classification and MI
classification, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kachuee_M/0/1/0/all/0/1&quot;&gt;Mohammad Kachuee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazeli_S/0/1/0/all/0/1&quot;&gt;Shayan Fazeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarrafzadeh_M/0/1/0/all/0/1&quot;&gt;Majid Sarrafzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00801">
<title>Credit risk prediction in an imbalanced social lending environment. (arXiv:1805.00801v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.00801</link>
<description rdf:parseType="Literal">&lt;p&gt;Credit risk prediction is an effective way of evaluating whether a potential
borrower will repay a loan, particularly in peer-to-peer lending where class
imbalance problems are prevalent. However, few credit risk prediction models
for social lending consider imbalanced data and, further, the best resampling
technique to use with imbalanced data is still controversial. In an attempt to
address these problems, this paper presents an empirical comparison of various
combinations of classifiers and resampling techniques within a novel risk
assessment methodology that incorporates imbalanced data. The credit
predictions from each combination are evaluated with a G-mean measure to avoid
bias towards the majority class, which has not been considered in similar
studies. The results reveal that combining random forest and random
under-sampling may be an effective strategy for calculating the credit risk
associated with loan applicants in social lending markets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namvar_A/0/1/0/all/0/1&quot;&gt;Anahita Namvar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siami_M/0/1/0/all/0/1&quot;&gt;Mohammad Siami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabhi_F/0/1/0/all/0/1&quot;&gt;Fethi Rabhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naderpour_M/0/1/0/all/0/1&quot;&gt;Mohsen Naderpour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00811">
<title>An Evaluation of Classification and Outlier Detection Algorithms. (arXiv:1805.00811v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.00811</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper evaluates algorithms for classification and outlier detection
accuracies in temporal data. We focus on algorithms that train and classify
rapidly and can be used for systems that need to incorporate new data
regularly. Hence, we compare the accuracy of six fast algorithms using a range
of well-known time-series datasets. The analyses demonstrate that the choice of
algorithm is task and data specific but that we can derive heuristics for
choosing. Gradient Boosting Machines are generally best for classification but
there is no single winner for outlier detection though Gradient Boosting
Machines (again) and Random Forest are better. Hence, we recommend running
evaluations of a number of algorithms using our heuristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hodge_V/0/1/0/all/0/1&quot;&gt;Victoria J. Hodge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Austin_J/0/1/0/all/0/1&quot;&gt;Jim Austin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00861">
<title>Modelling cross-dependencies between Spain&apos;s regional tourism markets with an extension of the Gaussian process regression model. (arXiv:1805.00861v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.00861</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents an extension of the Gaussian process regression model for
multiple-input multiple-output forecasting. This approach allows modelling the
cross-dependencies between a given set of input variables and generating a
vectorial prediction. Making use of the existing correlations in international
tourism demand to all seventeen regions of Spain, the performance of the
proposed model is assessed in a multiple-step-ahead forecasting comparison. The
results of the experiment in a multivariate setting show that the Gaussian
process regression model significantly improves the forecasting accuracy of a
multi-layer perceptron neural network used as a benchmark. The results reveal
that incorporating the connections between different markets in the modelling
process may prove very useful to refine predictions at a regional level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Claveria_O/0/1/0/all/0/1&quot;&gt;Oscar Claveria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Monte_E/0/1/0/all/0/1&quot;&gt;Enric Monte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Torra_S/0/1/0/all/0/1&quot;&gt;Salvador Torra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00862">
<title>Spectral clustering algorithms for the detection of clusters in block-cyclic and block-acyclic graphs. (arXiv:1805.00862v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1805.00862</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose two spectral algorithms for partitioning nodes in directed graphs
respectively with a cyclic and an acyclic pattern of connection between groups
of nodes. Our methods are based on the computation of extremal eigenvalues of
the transition matrix associated to the directed graph. The two algorithms
outperform state-of-the art methods for directed graph clustering on synthetic
datasets, including methods based on blockmodels, bibliometric symmetrization
and random walks. Our algorithms have the same space complexity as classical
spectral clustering algorithms for undirected graphs and their time complexity
is also linear in the number of edges in the graph. One of our methods is
applied to a trophic network based on predator-prey relationships. It
successfully extracts common categories of preys and predators encountered in
food chains. The same method is also applied to highlight the hierarchical
structure of a worldwide network of Autonomous Systems depicting business
agreements between Internet Service Providers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lierde_H/0/1/0/all/0/1&quot;&gt;H. Van Lierde&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_T/0/1/0/all/0/1&quot;&gt;T. W. S. Chow&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delvenne_J/0/1/0/all/0/1&quot;&gt;J.-C. Delvenne&lt;/a&gt; (2) ((1) City University of Hong Kong, (2) Universite Catholique de Louvain)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00868">
<title>A Dynamic Model for Traffic Flow Prediction Using Improved DRN. (arXiv:1805.00868v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.00868</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time traffic flow prediction can not only provide travelers with
reliable traffic information and thus save time, but also assist traffic
management department to manage transportation system. It can greatly improve
the efficiency of transportation. Traditional traffic flow prediction methods
usually need a huge amount of data but still leaves a poor performance. With
the development of deep learning, researchers begin to pay attention to
artificial neural networks (ANNs) such as RNN and LSTM. However, these ANNs are
very time-consuming. In our article, we improve the Deep Residual Network and
build a dynamic model which previous researchers hardly use. Our result shows
that our model can not only be trained efficiently but also have a higher
accuracy. Additionally, our dynamic model is more suitable for practical
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zeren Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruimin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00869">
<title>Approximate Temporal Difference Learning is a Gradient Descent for Reversible Policies. (arXiv:1805.00869v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.00869</link>
<description rdf:parseType="Literal">&lt;p&gt;In reinforcement learning, temporal difference (TD) is the most direct
algorithm to learn the value function of a policy. For large or infinite state
spaces, exact representations of the value function are usually not available,
and it must be approximated by a function in some parametric family.
&lt;/p&gt;
&lt;p&gt;However, with \emph{nonlinear} parametric approximations (such as neural
networks), TD is not guaranteed to converge to a good approximation of the true
value function within the family, and is known to diverge even in relatively
simple cases. TD lacks an interpretation as a stochastic gradient descent of an
error between the true and approximate value functions, which would provide
such guarantees.
&lt;/p&gt;
&lt;p&gt;We prove that approximate TD is a gradient descent provided the current
policy is \emph{reversible}. This holds even with nonlinear approximations.
&lt;/p&gt;
&lt;p&gt;A policy with transition probabilities $P(s,s&apos;)$ between states is reversible
if there exists a function $\mu$ over states such that
$\frac{P(s,s&apos;)}{P(s&apos;,s)}=\frac{\mu(s&apos;)}{\mu(s)}$. In particular, every move can
be undone with some probability. This condition is restrictive; it is
satisfied, for instance, for a navigation problem in any unoriented graph.
&lt;/p&gt;
&lt;p&gt;In this case, approximate TD is exactly a gradient descent of the
\emph{Dirichlet norm}, the norm of the difference of \emph{gradients} between
the true and approximate value functions. The Dirichlet norm also controls the
bias of approximate policy gradient. These results hold even with no decay
factor ($\gamma=1$) and do not rely on contractivity of the Bellman operator,
thus proving stability of TD even with $\gamma=1$ for reversible policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ollivier_Y/0/1/0/all/0/1&quot;&gt;Yann Ollivier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00878">
<title>Modelling tourism demand to Spain with machine learning techniques. The impact of forecast horizon on model selection. (arXiv:1805.00878v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.00878</link>
<description rdf:parseType="Literal">&lt;p&gt;This study assesses the influence of the forecast horizon on the forecasting
performance of several machine learning techniques. We compare the fo recast
accuracy of Support Vector Regression (SVR) to Neural Network (NN) models,
using a linear model as a benchmark. We focus on international tourism demand
to all seventeen regions of Spain. The SVR with a Gaussian radial basis
function kernel outperforms the rest of the models for the longest forecast
horizons. We also find that machine learning methods improve their forecasting
accuracy with respect to linear models as forecast horizons increase. This
result shows the suitability of SVR for medium and long term forecasting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Claveria_O/0/1/0/all/0/1&quot;&gt;Oscar Claveria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Monte_E/0/1/0/all/0/1&quot;&gt;Enric Monte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Torra_S/0/1/0/all/0/1&quot;&gt;Salvador Torra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00915">
<title>Neural networks as Interacting Particle Systems: Asymptotic convexity of the Loss Landscape and Universal Scaling of the Approximation Error. (arXiv:1805.00915v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.00915</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks, a central tool in machine learning, have demonstrated
remarkable, high fidelity performance on image recognition and classification
tasks. These successes evince an ability to accurately represent high
dimensional functions, potentially of great use in computational and applied
mathematics. That said, there are few rigorous results about the representation
error and trainability of neural networks, as well as how they scale with the
network size. Here we characterize both the error and scaling by reinterpreting
the standard optimization algorithm used in machine learning applications,
stochastic gradient descent, as the evolution of a particle system with
interactions governed by a potential related to the objective or &quot;loss&quot;
function used to train the network. We show that, when the number $n$ of
parameters is large, the empirical distribution of the particles descends on a
convex landscape towards a minimizer at a rate independent of $n$. We establish
a Law of Large Numbers and a Central Limit Theorem for the empirical
distribution, which together show that the approximation error of the network
universally scales as $o(n^{-1})$. Remarkably, these properties do not depend
on the dimensionality of the domain of the function that we seek to represent.
Our analysis also quantifies the scale and nature of the noise introduced by
stochastic gradient descent and provides guidelines for the step size and batch
size to use when training a neural network. We illustrate our findings on
examples in which we train neural network to learn the energy function of the
continuous 3-spin model on the sphere. The approximation error scales as our
analysis predicts in as high a dimension as $d=25$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rotskoff_G/0/1/0/all/0/1&quot;&gt;Grant M. Rotskoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vanden_Eijnden_E/0/1/0/all/0/1&quot;&gt;Eric Vanden-Eijnden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00917">
<title>A Simple Discrete-Time Survival Model for Neural Networks. (arXiv:1805.00917v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.00917</link>
<description rdf:parseType="Literal">&lt;p&gt;There is currently great interest in applying neural networks to prediction
tasks in medicine. It is important for predictive models to be able to use
survival data, where each patient has a known follow-up time and
event/censoring indicator. This avoids information loss when training the model
and enables generation of predicted survival curves. In this paper, we describe
a discrete-time survival model that is designed to be used with neural
networks. The model is trained with the maximum likelihood method using
minibatch stochastic gradient descent (SGD). The use of SGD enables rapid
training speed. The model is flexible, so that the baseline hazard rate and the
effect of the input data can vary with follow-up time. It has been implemented
in the Keras deep learning framework, and source code for the model and several
examples is available online. We demonstrated the high performance of the model
by using it as part of a convolutional neural network to predict survival for
over 10,000 patients with metastatic cancer, using the full text of 1,137,317
provider notes. The model&apos;s C-index on the validation set was 0.71, which was
superior to a linear baseline model (C-index 0.69).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gensheimer_M/0/1/0/all/0/1&quot;&gt;Michael F. Gensheimer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Narasimhan_B/0/1/0/all/0/1&quot;&gt;Balasubramanian Narasimhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00928">
<title>Lidar Cloud Detection with Fully Convolutional Networks. (arXiv:1805.00928v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.00928</link>
<description rdf:parseType="Literal">&lt;p&gt;In this contribution, we present a novel approach for segmenting laser radar
(lidar) imagery into geometric time-height cloud locations with a fully
convolutional network (FCN). We describe a semi-supervised learning method to
train the FCN by: pre-training the classification layers of the FCN with
&apos;weakly labeled&apos; lidar data, using &apos;unsupervised&apos; pre-training with the cloud
locations of the Wang &amp;amp; Sassen (2001) cloud mask algorithm, and fully
supervised learning with hand-labeled cloud locations. We show the model
achieves higher levels of cloud identification compared to the cloud mask
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cromwell_E/0/1/0/all/0/1&quot;&gt;Erol Cromwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flynn_D/0/1/0/all/0/1&quot;&gt;Donna Flynn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.05355">
<title>Probabilistic Matrix Factorization for Automated Machine Learning. (arXiv:1705.05355v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.05355</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to achieve state-of-the-art performance, modern machine learning
techniques require careful data pre-processing and hyperparameter tuning.
Moreover, given the ever increasing number of machine learning models being
developed, model selection is becoming increasingly important. Automating the
selection and tuning of machine learning pipelines consisting of data
pre-processing methods and machine learning models, has long been one of the
goals of the machine learning community. In this paper, we tackle this
meta-learning task by combining ideas from collaborative filtering and Bayesian
optimization. Using probabilistic matrix factorization techniques and
acquisition functions from Bayesian optimization, we exploit experiments
performed in hundreds of different datasets to guide the exploration of the
space of possible pipelines. In our experiments, we show that our approach
quickly identifies high-performing pipelines across a wide range of datasets,
significantly outperforming the current state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fusi_N/0/1/0/all/0/1&quot;&gt;Nicolo Fusi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sheth_R/0/1/0/all/0/1&quot;&gt;Rishit Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Elibol_H/0/1/0/all/0/1&quot;&gt;Huseyn Melih Elibol&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.09283">
<title>GXNOR-Net: Training deep neural networks with ternary weights and activations without full-precision memory under a unified discretization framework. (arXiv:1705.09283v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.09283</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a pressing need to build an architecture that could subsume these
networks under a unified framework that achieves both higher performance and
less overhead. To this end, two fundamental issues are yet to be addressed. The
first one is how to implement the back propagation when neuronal activations
are discrete. The second one is how to remove the full-precision hidden weights
in the training phase to break the bottlenecks of memory/computation
consumption. To address the first issue, we present a multi-step neuronal
activation discretization method and a derivative approximation technique that
enable the implementing the back propagation algorithm on discrete DNNs. While
for the second issue, we propose a discrete state transition (DST) methodology
to constrain the weights in a discrete space without saving the hidden weights.
Through this way, we build a unified framework that subsumes the binary or
ternary networks as its special cases, and under which a heuristic algorithm is
provided at the website https://github.com/AcrossV/Gated-XNOR. More
particularly, we find that when both the weights and activations become ternary
values, the DNNs can be reduced to sparse binary networks, termed as gated XNOR
networks (GXNOR-Nets) since only the event of non-zero weight and non-zero
activation enables the control gate to start the XNOR logic operations in the
original binary networks. This promises the event-driven hardware design for
efficient mobile intelligence. We achieve advanced performance compared with
state-of-the-art algorithms. Furthermore, the computational sparsity and the
number of states in the discrete space can be flexibly modified to make it
suitable for various hardware platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1&quot;&gt;Lei Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_P/0/1/0/all/0/1&quot;&gt;Peng Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1&quot;&gt;Jing Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhenzhi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoqi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.02963">
<title>An Interactive Greedy Approach to Group Sparsity in High Dimension. (arXiv:1707.02963v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.02963</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparsity learning with known grouping structures has received considerable
attention due to wide modern applications in high-dimensional data analysis.
Although advantages of using group information have been well-studied by
shrinkage-based approaches, benefits of group sparsity have not been
well-documented for greedy-type methods, which much limits our understanding
and use of this important class of methods. In this paper, generalizing from a
popular forward-backward greedy approach, we propose a new interactive greedy
algorithm for group sparsity learning and prove that the proposed greedy-type
algorithm attains the desired benefits of group sparsity under high dimensional
settings. An estimation error bound refining other existing methods and a
guarantee for group support recovery are also established simultaneously. In
addition, we incorporate a general M-estimation framework and introduce an
interactive feature to allow extra algorithm flexibility without compromise in
theoretical properties. The promising use of our proposal is demonstrated
through numerical evaluations including a real industrial application in human
activity recognition at home. Supplementary materials for this article are
available online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qian_W/0/1/0/all/0/1&quot;&gt;Wei Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wending Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sogawa_Y/0/1/0/all/0/1&quot;&gt;Yasuhiro Sogawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fujimaki_R/0/1/0/all/0/1&quot;&gt;Ryohei Fujimaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xitong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>