<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-16T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05151"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05295"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05372"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05394"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1409.8498"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.06207"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.00084"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03825"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04590"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04987"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05007"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05039"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05055"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05141"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05398"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05413"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1510.02786"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.02089"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.08110"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.02346"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.09627"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.00139"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.05151">
<title>Constraint-free Natural Image Reconstruction from fMRI Signals Based on Convolutional Neural Network. (arXiv:1801.05151v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.05151</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, research on decoding brain activity based on functional
magnetic resonance imaging (fMRI) has made remarkable achievements. However,
constraint-free natural image reconstruction from brain activity is still a
challenge. The existing methods simplified the problem by using semantic prior
information or just reconstructing simple images such as letters and digitals.
Without semantic prior information, we present a novel method to reconstruct
nature images from fMRI signals of human visual cortex based on the computation
model of convolutional neural network (CNN). Firstly, we extracted the units
output of viewed natural images in each layer of a pre-trained CNN as CNN
features. Secondly, we transformed image reconstruction from fMRI signals into
the problem of CNN feature visualizations by training a sparse linear
regression to map from the fMRI patterns to CNN features. By iteratively
optimization to find the matched image, whose CNN unit features become most
similar to those predicted from the brain activity, we finally achieved the
promising results for the challenging constraint-free natural image
reconstruction. As there was no use of semantic prior information of the
stimuli when training decoding model, any category of images (not constraint by
the training set) could be reconstructed theoretically. We found that the
reconstructed images resembled the natural stimuli, especially in position and
shape. The experimental results suggest that hierarchical visual features can
effectively express the visual perception process of human brain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1&quot;&gt;Kai Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Linyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1&quot;&gt;Li Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Ying Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bin Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05295">
<title>Social Network based Short-Term Stock Trading System. (arXiv:1801.05295v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1801.05295</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel adaptive algorithm for the automated short-term
trading of financial instrument. The algorithm adopts a semantic sentiment
analysis technique to inspect the Twitter posts and to use them to predict the
behaviour of the stock market. Indeed, the algorithm is specifically developed
to take advantage of both the sentiment and the past values of a certain
financial instrument in order to choose the best investment decision. This
allows the algorithm to ensure the maximization of the obtainable profits by
trading on the stock market. We have conducted an investment simulation and
compared the performance of our proposed with a well-known benchmark (DJTATO
index) and the optimal results, in which an investor knows in advance the
future price of a product. The result shows that our approach outperforms the
benchmark and achieves the performance score close to the optimal result.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremonesi_P/0/1/0/all/0/1&quot;&gt;Paolo Cremonesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francalanci_C/0/1/0/all/0/1&quot;&gt;Chiara Francalanci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poli_A/0/1/0/all/0/1&quot;&gt;Alessandro Poli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pagano_R/0/1/0/all/0/1&quot;&gt;Roberto Pagano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazzoni_L/0/1/0/all/0/1&quot;&gt;Luca Mazzoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggioni_A/0/1/0/all/0/1&quot;&gt;Alberto Maggioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elahi_M/0/1/0/all/0/1&quot;&gt;Mehdi Elahi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05372">
<title>Learning Features For Relational Data. (arXiv:1801.05372v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.05372</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature engineering is one of the most important but tedious tasks in data
science projects. This work studies automation of feature learning for
relational data. We first theoretically proved that learning relevant features
from relational data for a given predictive analytics problem is NP-hard.
However, it is possible to empirically show that an efficient rule based
approach predefining transformations as a priori based on heuristics can
extract very useful features from relational data. Indeed, the proposed
approach outperformed the state of the art solutions with a significant margin.
We further introduce a deep neural network which automatically learns
appropriate transformations of relational data into a representation that
predicts the target variable well instead of being predefined as a priori by
users. In an extensive experiment with Kaggle competitions, the proposed
methods could win late medals. To the best of our knowledge, this is the first
time an automation system could win medals in Kaggle competitions with complex
relational data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_H/0/1/0/all/0/1&quot;&gt;Hoang Thanh Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minh_T/0/1/0/all/0/1&quot;&gt;Tran Ngoc Minh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinn_M/0/1/0/all/0/1&quot;&gt;Mathieu Sinn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buesser_B/0/1/0/all/0/1&quot;&gt;Beat Buesser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wistuba_M/0/1/0/all/0/1&quot;&gt;Martin Wistuba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05394">
<title>Time Series Segmentation through Automatic Feature Learning. (arXiv:1801.05394v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.05394</link>
<description rdf:parseType="Literal">&lt;p&gt;Internet of things (IoT) applications have become increasingly popular in
recent years, with applications ranging from building energy monitoring to
personal health tracking and activity recognition. In order to leverage these
data, automatic knowledge extraction - whereby we map from observations to
interpretable states and transitions - must be done at scale. As such, we have
seen many recent IoT data sets include annotations with a human expert
specifying states, recorded as a set of boundaries and associated labels in a
data sequence. These data can be used to build automatic labeling algorithms
that produce labels as an expert would. Here, we refer to human-specified
boundaries as breakpoints. Traditional changepoint detection methods only look
for statistically-detectable boundaries that are defined as abrupt variations
in the generative parameters of a data sequence. However, we observe that
breakpoints occur on more subtle boundaries that are non-trivial to detect with
these statistical methods. In this work, we propose a new unsupervised
approach, based on deep learning, that outperforms existing techniques and
learns the more subtle, breakpoint boundaries with a high accuracy. Through
extensive experiments on various real-world data sets - including
human-activity sensing data, speech signals, and electroencephalogram (EEG)
activity traces - we demonstrate the effectiveness of our algorithm for
practical applications. Furthermore, we show that our approach achieves
significantly better performance than previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wei-Han Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1&quot;&gt;Jorge Ortiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1&quot;&gt;Bongjun Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1&quot;&gt;Ruby Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1409.8498">
<title>Robust Learning for Repeated Stochastic Games via Meta-Gaming. (arXiv:1409.8498v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/1409.8498</link>
<description rdf:parseType="Literal">&lt;p&gt;In repeated stochastic games (RSGs), an agent must quickly adapt to the
behavior of previously unknown associates, who may themselves be learning. This
machine-learning problem is particularly challenging due, in part, to the
presence of multiple (even infinite) equilibria and inherently large strategy
spaces. In this paper, we introduce a method to reduce the strategy space of
two-player general-sum RSGs to a handful of expert strategies. This process,
called Mega, effectually reduces an RSG to a bandit problem. We show that the
resulting strategy space preserves several important properties of the original
RSG, thus enabling a learner to produce robust strategies within a reasonably
small number of interactions. To better establish strengths and weaknesses of
this approach, we empirically evaluate the resulting learning system against
other algorithms in three different RSGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crandall_J/0/1/0/all/0/1&quot;&gt;Jacob W. Crandall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.06207">
<title>Cooperating with Machines. (arXiv:1703.06207v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1703.06207</link>
<description rdf:parseType="Literal">&lt;p&gt;Since Alan Turing envisioned Artificial Intelligence (AI) [1], a major
driving force behind technical progress has been competition with human
cognition. Historical milestones have been frequently associated with computers
matching or outperforming humans in difficult cognitive tasks (e.g. face
recognition [2], personality classification [3], driving cars [4], or playing
video games [5]), or defeating humans in strategic zero-sum encounters (e.g.
Chess [6], Checkers [7], Jeopardy! [8], Poker [9], or Go [10]). In contrast,
less attention has been given to developing autonomous machines that establish
mutually cooperative relationships with people who may not share the machine&apos;s
preferences. A main challenge has been that human cooperation does not require
sheer computational power, but rather relies on intuition [11], cultural norms
[12], emotions and signals [13, 14, 15, 16], and pre-evolved dispositions
toward cooperation [17], common-sense mechanisms that are difficult to encode
in machines for arbitrary contexts. Here, we combine a state-of-the-art
machine-learning algorithm with novel mechanisms for generating and acting on
signals to produce a new learning algorithm that cooperates with people and
other machines at levels that rival human cooperation in a variety of
two-player repeated stochastic games. This is the first general-purpose
algorithm that is capable, given a description of a previously unseen game
environment, of learning to cooperate with people within short timescales in
scenarios previously unanticipated by algorithm designers. This is achieved
without complex opponent modeling or higher-order theories of mind, thus
showing that flexible, fast, and general human-machine cooperation is
computationally achievable using a non-trivial, but ultimately simple, set of
algorithmic mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crandall_J/0/1/0/all/0/1&quot;&gt;Jacob W. Crandall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudah_M/0/1/0/all/0/1&quot;&gt;Mayada Oudah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tennom/0/1/0/all/0/1&quot;&gt;Tennom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishowo_Oloko_F/0/1/0/all/0/1&quot;&gt;Fatimah Ishowo-Oloko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdallah_S/0/1/0/all/0/1&quot;&gt;Sherief Abdallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonnefon_J/0/1/0/all/0/1&quot;&gt;Jean-Fran&amp;#xe7;ois Bonnefon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cebrian_M/0/1/0/all/0/1&quot;&gt;Manuel Cebrian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shariff_A/0/1/0/all/0/1&quot;&gt;Azim Shariff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodrich_M/0/1/0/all/0/1&quot;&gt;Michael A. Goodrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahwan_I/0/1/0/all/0/1&quot;&gt;Iyad Rahwan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.00084">
<title>Behavior Trees in Robotics and AI: An Introduction. (arXiv:1709.00084v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1709.00084</link>
<description rdf:parseType="Literal">&lt;p&gt;A Behavior Tree (BT) is a way to structure the switching between different
tasks in an autonomous agent, such as a robot or a virtual entity in a computer
game. BTs are a very efficient way of creating complex systems that are both
modular and reactive. These properties are crucial in many applications, which
has led to the spread of BT from computer game programming to many branches of
AI and Robotics. In this book, we will first give an introduction to BTs, then
we describe how BTs relate to, and in many cases generalize, earlier switching
structures. These ideas are then used as a foundation for a set of efficient
and easy to use design principles. Properties such as safety, robustness, and
efficiency are important for an autonomous system, and we describe a set of
tools for formally analyzing these using a state space description of BTs. With
the new analysis tools, we can formalize the descriptions of how BTs generalize
earlier approaches. We also show the use of BTs in automated planning and
machine learning. Finally, we describe an extended set of tools to capture the
behavior of Stochastic BTs, where the outcomes of actions are described by
probabilities. These tools enable the computation of both success probabilities
and time to completion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colledanchise_M/0/1/0/all/0/1&quot;&gt;Michele Colledanchise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogren_P/0/1/0/all/0/1&quot;&gt;Petter &amp;#xd6;gren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03825">
<title>EARL: Joint Entity and Relation Linking for Question Answering over Knowledge Graphs. (arXiv:1801.03825v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03825</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to answer natural language questions over knowledge graphs, most
processing pipelines involve entity and relation linking. Traditionally, entity
linking and relation linking has been performed either as dependent sequential
tasks or independent parallel tasks. In this paper, we propose a framework
called &quot;EARL&quot;, which performs entity linking and relation linking as a joint
single task. EARL uses a graph connection based solution to the problem. We
model the linking task as an instance of the Generalised Travelling Salesman
Problem (GTSP) and use GTSP approximate algorithm solutions. We later develop
EARL which uses a pair-wise graph-distance based solution to the problem.The
system determines the best semantic connection between all keywords of the
question by referring to a knowledge graph. This is achieved by exploiting the
&quot;connection density&quot; between entity candidates and relation candidates. The
&quot;connection density&quot; based solution performs at par with the approximate GTSP
solution.We have empirically evaluated the framework on a dataset with 5000
questions. Our system surpasses state-of-the-art scores for entity linking task
by reporting an accuracy of 0.65 to 0.40 from the next best entity linker.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_M/0/1/0/all/0/1&quot;&gt;Mohnish Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1&quot;&gt;Debayan Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_D/0/1/0/all/0/1&quot;&gt;Debanjan Chaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1&quot;&gt;Jens Lehmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04590">
<title>Frame-Recurrent Video Super-Resolution. (arXiv:1801.04590v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04590</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in video super-resolution have shown that convolutional
neural networks combined with motion compensation are able to merge information
from multiple low-resolution (LR) frames to generate high-quality images.
Current state-of-the-art methods process a batch of LR frames to generate a
single high-resolution (HR) frame and run this scheme in a sliding window
fashion over the entire video, effectively treating the problem as a large
number of separate multi-frame super-resolution tasks. This approach has two
main weaknesses: 1) Each input frame is processed and warped multiple times,
increasing the computational cost, and 2) each output frame is estimated
independently conditioned on the input frames, limiting the system&apos;s ability to
produce temporally consistent results.
&lt;/p&gt;
&lt;p&gt;In this work, we propose an end-to-end trainable frame-recurrent video
super-resolution framework that uses the previously inferred HR estimate to
super-resolve the subsequent frame. This naturally encourages temporally
consistent results and reduces the computational cost by warping only one image
in each step. Furthermore, due to its recurrent nature, the proposed method has
the ability to assimilate a large number of previous frames without increased
computational demands. Extensive evaluations and comparisons with previous
methods validate the strengths of our approach and demonstrate that the
proposed framework is able to significantly outperform the current state of the
art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajjadi_M/0/1/0/all/0/1&quot;&gt;Mehdi S. M. Sajjadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1&quot;&gt;Raviteja Vemulapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1&quot;&gt;Matthew Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04987">
<title>On the Complexity of the Weighted Fussed Lasso. (arXiv:1801.04987v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.04987</link>
<description rdf:parseType="Literal">&lt;p&gt;The solution path of the 1D fused lasso for an $n$-dimensional input is
piecewise linear with $\mathcal{O}(n)$ segments (Hoefling et al. 2010 and
Tibshirani et al 2011). However, existing proofs of this bound do not hold for
the weighted fused lasso. At the same time, results for the generalized lasso,
of which the weighted fused lasso is a special case, allow $\Omega(3^n)$
segments (Mairal et al. 2012). In this paper, we prove that the number of
segments in the solution path of the weighted fused lasso is
$\mathcal{O}(n^3)$, and for some instances $\Omega(n^2)$. We also give a new,
very simple, proof of the $\mathcal{O}(n)$ bound for the fused lasso.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bento_J/0/1/0/all/0/1&quot;&gt;Jose Bento&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1&quot;&gt;Surjyendu Ray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05007">
<title>Divide and Recombine for Large and Complex Data: Model Likelihood Functions using MCMC. (arXiv:1801.05007v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1801.05007</link>
<description rdf:parseType="Literal">&lt;p&gt;In Divide &amp;amp; Recombine (D&amp;amp;R), big data are divided into subsets, each analytic
method is applied to subsets, and the outputs are recombined. This enables deep
analysis and practical computational performance. An innovate D\&amp;amp;R procedure is
proposed to compute likelihood functions of data-model (DM) parameters for big
data. The likelihood-model (LM) is a parametric probability density function of
the DM parameters. The density parameters are estimated by fitting the density
to MCMC draws from each subset DM likelihood function, and then the fitted
densities are recombined. The procedure is illustrated using normal and
skew-normal LMs for the logistic regression DM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhadra_A/0/1/0/all/0/1&quot;&gt;Anindya Bhadra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cleveland_W/0/1/0/all/0/1&quot;&gt;William S. Cleveland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05039">
<title>Global Convergence of Policy Gradient Methods for Linearized Control Problems. (arXiv:1801.05039v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.05039</link>
<description rdf:parseType="Literal">&lt;p&gt;Direct policy gradient methods for reinforcement learning and continuous
control problems are a popular approach for a variety of reasons: 1) they are
easy to implement without explicit knowledge of the underlying model 2) they
are an &quot;end-to-end&quot; approach, directly optimizing the performance metric of
interest 3) they inherently allow for richly parameterized policies. A notable
drawback is that even in the most basic continuous control problem (that of
linear quadratic regulators), these methods must solve a non-convex
optimization problem, where little is understood about their efficiency from
both computational and statistical perspectives. In contrast, system
identification and model based planning in optimal control theory have a much
more solid theoretical footing, where much is known with regards to their
computational and statistical properties. This work bridges this gap showing
that (model free) policy gradient methods globally converge to the optimal
solution and are efficient (polynomially so in relevant problem dependent
quantities) with regards to their sample and computational complexities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazel_M/0/1/0/all/0/1&quot;&gt;Maryam Fazel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1&quot;&gt;Rong Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1&quot;&gt;Sham M. Kakade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mesbahi_M/0/1/0/all/0/1&quot;&gt;Mehran Mesbahi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05055">
<title>Toward Metric Indexes for Incremental Insertion and Querying. (arXiv:1801.05055v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1801.05055</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we explore the use of metric index structures, which accelerate
nearest neighbor queries, in the scenario where we need to interleave
insertions and queries during deployment. This use-case is inspired by a
real-life need in malware analysis triage, and is surprisingly understudied.
Existing literature tends to either focus on only final query efficiency, often
does not support incremental insertion, or does not support arbitrary distance
metrics. We modify and improve three algorithms to support our scenario of
incremental insertion and querying with arbitrary metrics, and evaluate them on
multiple datasets and distance metrics while varying the value of $k$ for the
desired number of nearest neighbors. In doing so we determine that our improved
Vantage-Point tree of Minimum-Variance performs best for this scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1&quot;&gt;Edward Raff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicholas_C/0/1/0/all/0/1&quot;&gt;Charles Nicholas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05141">
<title>Image denoising and restoration with CNN-LSTM Encoder Decoder with Direct Attention. (arXiv:1801.05141v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.05141</link>
<description rdf:parseType="Literal">&lt;p&gt;Image denoising is always a challenging task in the field of computer vision
and image processing. In this paper, we have proposed an encoder-decoder model
with direct attention, which is capable of denoising and reconstruct highly
corrupted images. Our model consists of an encoder and a decoder, where the
encoder is a convolutional neural network and decoder is a multilayer Long
Short-Term memory network. In the proposed model, the encoder reads an image
and catches the abstraction of that image in a vector, where decoder takes that
vector as well as the corrupted image to reconstruct a clean image. We have
trained our model on MNIST handwritten digit database after making lower half
of every image as black as well as adding noise top of that. After a massive
destruction of the images where it is hard for a human to understand the
content of those images, our model can retrieve that image with minimal error.
Our proposed model has been compared with convolutional encoder-decoder, where
our model has performed better at generating missing part of the images than
convolutional autoencoder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haque_K/0/1/0/all/0/1&quot;&gt;Kazi Nazmul Haque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yousuf_M/0/1/0/all/0/1&quot;&gt;Mohammad Abu Yousuf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rana_R/0/1/0/all/0/1&quot;&gt;Rajib Rana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05398">
<title>On the Direction of Discrimination: An Information-Theoretic Analysis of Disparate Impact in Machine Learning. (arXiv:1801.05398v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1801.05398</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of machine learning, disparate impact refers to a form of
systematic discrimination whereby the output distribution of a model depends on
the value of a sensitive attribute (e.g., race or gender). In this paper, we
present an information-theoretic framework to analyze the disparate impact of a
binary classification model. We view the model as a fixed channel, and quantify
disparate impact as the divergence in output distributions over two groups. We
then aim to find a \textit{correction function} that can be used to perturb the
input distributions of each group in order to align their output distributions.
We present an optimization problem that can be solved to obtain a correction
function that will make the output distributions statistically
indistinguishable. We derive closed-form expression for the correction function
that can be used to compute it efficiently. We illustrate the use of the
correction function for a recidivism prediction application derived from the
ProPublica COMPAS dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ustun_B/0/1/0/all/0/1&quot;&gt;Berk Ustun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calmon_F/0/1/0/all/0/1&quot;&gt;Flavio P. Calmon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05413">
<title>Combinatorial Preconditioners for Proximal Algorithms on Graphs. (arXiv:1801.05413v1 [math.OC])</title>
<link>http://arxiv.org/abs/1801.05413</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel preconditioning technique for proximal optimization
methods that relies on graph algorithms to construct effective preconditioners.
Such combinatorial preconditioners arise from partitioning the graph into
forests. We prove that certain decompositions lead to a theoretically optimal
condition number. We also show how ideal decompositions can be realized using
matroid partitioning and propose efficient greedy variants thereof for
large-scale problems. Coupled with specialized solvers for the resulting scaled
proximal subproblems, the preconditioned algorithm achieves competitive
performance in machine learning and vision applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mollenhoff_T/0/1/0/all/0/1&quot;&gt;Thomas M&amp;#xf6;llenhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zhenzhang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1510.02786">
<title>Recovering a Hidden Community Beyond the Kesten-Stigum Threshold in $O(|E| \log^*|V|)$ Time. (arXiv:1510.02786v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1510.02786</link>
<description rdf:parseType="Literal">&lt;p&gt;Community detection is considered for a stochastic block model graph of n
vertices, with K vertices in the planted community, edge probability p for
pairs of vertices both in the community, and edge probability q for other pairs
of vertices.
&lt;/p&gt;
&lt;p&gt;The main focus of the paper is on weak recovery of the community based on the
graph G, with o(K) misclassified vertices on average, in the sublinear regime
$n^{1-o(1)} \leq K \leq o(n).$ A critical parameter is the effective
signal-to-noise ratio $\lambda=K^2(p-q)^2/((n-K)q)$, with $\lambda=1$
corresponding to the Kesten-Stigum threshold. We show that a belief propagation
algorithm achieves weak recovery if $\lambda&amp;gt;1/e$, beyond the Kesten-Stigum
threshold by a factor of $1/e.$ The belief propagation algorithm only needs to
run for $\log^\ast n+O(1) $ iterations, with the total time complexity $O(|E|
\log^*n)$, where $\log^*n$ is the iterated logarithm of $n.$ Conversely, if
$\lambda \leq 1/e$, no local algorithm can asymptotically outperform trivial
random guessing. Furthermore, a linear message-passing algorithm that
corresponds to applying power iteration to the non-backtracking matrix of the
graph is shown to attain weak recovery if and only if $\lambda&amp;gt;1$. In addition,
the belief propagation algorithm can be combined with a linear-time voting
procedure to achieve the information limit of exact recovery (correctly
classify all vertices with high probability) for all $K \ge \frac{n}{\log n}
\left( \rho_{\rm BP} +o(1) \right),$ where $\rho_{\rm BP}$ is a function of
$p/q$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hajek_B/0/1/0/all/0/1&quot;&gt;Bruce Hajek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yihong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaming Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.02089">
<title>The variational Laplace approach to approximate Bayesian inference. (arXiv:1703.02089v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1703.02089</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational approaches to approximate Bayesian inference provide very
efficient means of performing parameter estimation and model selection. Among
these, so-called variational-Laplace or VL schemes rely on Gaussian
approximations to posterior densities on model parameters. In this note, we
review the main variants of VL approaches, that follow from considering
nonlinear models of continuous and/or categorical data. En passant, we also
derive a few novel theoretical results that complete the portfolio of existing
analyses of variational Bayesian approaches, including investigations of their
asymptotic convergence. We also suggest practical ways of extending existing VL
approaches to hierarchical generative models that include (e.g., precision)
hyperparameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Daunizeau_J/0/1/0/all/0/1&quot;&gt;Jean Daunizeau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.08110">
<title>Training Gaussian Mixture Models at Scale via Coresets. (arXiv:1703.08110v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.08110</link>
<description rdf:parseType="Literal">&lt;p&gt;How can we train a statistical mixture model on a massive data set? In this
work we show how to construct coresets for mixtures of Gaussians. A coreset is
a weighted subset of the data, which guarantees that models fitting the coreset
also provide a good fit for the original data set. We show that, perhaps
surprisingly, Gaussian mixtures admit coresets of size polynomial in dimension
and the number of mixture components, while being independent of the data set
size. Hence, one can harness computationally intensive algorithms to compute a
good approximation on a significantly smaller data set. More importantly, such
coresets can be efficiently constructed both in distributed and streaming
settings and do not impose restrictions on the data generating process. Our
results rely on a novel reduction of statistical estimation to problems in
computational geometry and new combinatorial complexity results for mixtures of
Gaussians. Empirical evaluation on several real-world datasets suggests that
our coreset-based approach enables significant reduction in training-time with
negligible approximation error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lucic_M/0/1/0/all/0/1&quot;&gt;Mario Lucic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Faulkner_M/0/1/0/all/0/1&quot;&gt;Matthew Faulkner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feldman_D/0/1/0/all/0/1&quot;&gt;Dan Feldman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.02346">
<title>Joint Probabilistic Linear Discriminant Analysis. (arXiv:1704.02346v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1704.02346</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard probabilistic linear discriminant analysis (PLDA) for speaker
recognition assumes that the sample&apos;s features (usually, i-vectors) are given
by a sum of three terms: a term that depends on the speaker identity, a term
that models the within-speaker variability and is assumed independent across
samples, and a final term that models any remaining variability and is also
independent across samples. In this work, we propose a generalization of this
model where the within-speaker variability is not necessarily assumed
independent across samples but dependent on another discrete variable. This
variable, which we call the channel variable as in the standard PLDA approach,
could be, for example, a discrete category for the channel characteristics, the
language spoken by the speaker, the type of speech in the sample
(conversational, monologue, read), etc. The value of this variable is assumed
to be known during training but not during testing. Scoring is performed, as in
standard PLDA, by computing a likelihood ratio between the null hypothesis that
the two sides of a trial belong to the same speaker versus the alternative
hypothesis that the two sides belong to different speakers. The two likelihoods
are computed by marginalizing over two hypothesis about the channels in both
sides of a trial: that they are the same and that they are different. This way,
we expect that the new model will be better at coping with same-channel versus
different-channel trials than standard PLDA, since knowledge about the channel
(or language, or speech style) is used during training and implicitly
considered during scoring.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1&quot;&gt;Luciana Ferrer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.09627">
<title>Deep learning bank distress from news and numerical financial data. (arXiv:1706.09627v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.09627</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we focus our attention on the exploitation of the information
contained in financial news to enhance the performance of a classifier of bank
distress. Such information should be analyzed and inserted into the predictive
model in the most efficient way and this task deals with all the issues related
to text analysis and specifically analysis of news media. Among the different
models proposed for such purpose, we investigate one of the possible deep
learning approaches, based on a doc2vec representation of the textual data, a
kind of neural network able to map the sequential and symbolic text input onto
a reduced latent semantic space. Afterwards, a second supervised neural network
is trained combining news data with standard financial figures to classify
banks whether in distressed or tranquil states, based on a small set of known
distress events. Then the final aim is not only the improvement of the
predictive performance of the classifier but also to assess the importance of
news data in the classification process. Does news data really bring more
useful information not contained in standard financial variables? Our results
seem to confirm such hypothesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cerchiello_P/0/1/0/all/0/1&quot;&gt;Paola Cerchiello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nicola_G/0/1/0/all/0/1&quot;&gt;Giancarlo Nicola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ronnqvist_S/0/1/0/all/0/1&quot;&gt;Samuel Ronnqvist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sarlin_P/0/1/0/all/0/1&quot;&gt;Peter Sarlin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.00139">
<title>Fast Incremental SVDD Learning Algorithm with the Gaussian Kernel. (arXiv:1709.00139v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.00139</link>
<description rdf:parseType="Literal">&lt;p&gt;Support vector data description (SVDD) is a machine learning technique that
is used for single-class classification and outlier detection. The idea of SVDD
is to find a set of support vectors that defines a boundary around data. When
dealing with online or large data, existing batch SVDD methods have to be rerun
in each iteration. We propose an incremental learning algorithm for SVDD that
uses the Gaussian kernel. This algorithm builds on the observation that all
support vectors on the boundary have the same distance to the center of sphere
in a higher-dimensional feature space as mapped by the Gaussian kernel
function. Each iteration only involves the existing support vectors and the new
data point. The algorithm is based solely on matrix manipulations; the support
vectors and their corresponding Lagrange multiplier $\alpha_i$&apos;s are
automatically selected and determined in each iteration. It can be seen that
the complexity of our algorithm in each iteration is only $O(k^2)$, where $k$
is the number of support vectors. Our experimental results on some real data
sets show that our incremental algorithm achieves similar F-1 scores with much
less running time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hansi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenhao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kakde_D/0/1/0/all/0/1&quot;&gt;Deovrat Kakde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chaudhuri_A/0/1/0/all/0/1&quot;&gt;Arin Chaudhuri&lt;/a&gt;</dc:creator>
</item></rdf:RDF>