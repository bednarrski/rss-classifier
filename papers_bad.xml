<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-28T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06704"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04258"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08444"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08951"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08953"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08954"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09057"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09062"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09079"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09123"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03846"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10467"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11112"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11227"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00177"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08578"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08750"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08952"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09050"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09105"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09127"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09144"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09270"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09271"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09334"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09347"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09371"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.01216"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.02658"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02920"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04572"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03740"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10173"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08646"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1711.06704">
<title>Repeatability Is Not Enough: Learning Affine Regions via Discriminability. (arXiv:1711.06704v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06704</link>
<description rdf:parseType="Literal">&lt;p&gt;A method for learning local affine-covariant regions is presented. We show
that maximizing geometric repeatability does not lead to local regions, a.k.a
features,that are reliably matched and this necessitates descriptor-based
learning. We explore factors that influence such learning and registration: the
loss function, descriptor type, geometric parametrization and the trade-off
between matchability and geometric accuracy and propose a novel hard
negative-constant loss function for learning of affine regions. The affine
shape estimator -- AffNet -- trained with the hard negative-constant loss
outperforms the state-of-the-art in bag-of-words image retrieval and wide
baseline stereo. The proposed training process does not require precisely
geometrically aligned patches.The source codes and trained weights are
available at https://github.com/ducha-aiki/affnet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishkin_D/0/1/0/all/0/1&quot;&gt;Dmytro Mishkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1&quot;&gt;Filip Radenovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1&quot;&gt;Jiri Matas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04258">
<title>PALM: An Incremental Construction of Hyperplanes for Data Stream Regression. (arXiv:1805.04258v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1805.04258</link>
<description rdf:parseType="Literal">&lt;p&gt;Data stream has been the underlying challenge in the age of big data because
it calls for real-time data processing with the absence of a retraining process
and/or an iterative learning approach. In realm of fuzzy system community, data
stream is handled by algorithmic development of self-adaptive neurofuzzy
systems (SANFS) characterized by the single-pass learning mode and the open
structure property which enables effective handling of fast and rapidly
changing natures of data streams. The underlying bottleneck of SANFSs lies in
its design principle which involves a high number of free parameters (rule
premise and rule consequent) to be adapted in the training process. This figure
can even double in the case of type-2 fuzzy system. In this work, a novel
SANFS, namely parsimonious learning machine (PALM), is proposed. PALM features
utilization of a new type of fuzzy rule based on the concept of hyperplane
clustering which significantly reduces the number of network parameters because
it has no rule premise parameters. PALM is proposed in both type-1 and type-2
fuzzy systems where all of which characterize a fully dynamic rule-based
system. That is, it is capable of automatically generating, merging and tuning
the hyperplane-based fuzzy rule in the single pass manner. Moreover, an
extension of PALM, namely recurrent PALM (rPALM), is proposed and adopts the
concept of teacher-forcing mechanism in the deep learning literature. The
efficacy of PALM has been evaluated through numerical study with six real-world
and synthetic data streams from public database and our own real-world project
of autonomous vehicles. The proposed model showcases significant improvements
in terms of computational complexity and number of required parameters against
several renowned SANFSs, while attaining comparable and often better predictive
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferdaus_M/0/1/0/all/0/1&quot;&gt;Md Meftahul Ferdaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1&quot;&gt;Mahardhika Pratama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anavatti_S/0/1/0/all/0/1&quot;&gt;Sreenatha G. Anavatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garratt_M/0/1/0/all/0/1&quot;&gt;Matthew A. Garratt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08444">
<title>Guiding Deep Learning System Testing using Surprise Adequacy. (arXiv:1808.08444v1 [cs.SE] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1808.08444</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning (DL) systems are rapidly being adopted in safety and security
critical domains, urgently calling for ways to test their correctness and
robustness. Testing of DL systems has traditionally relied on manual collection
and labelling of data. Recently, a number of coverage criteria based on neuron
activation values have been proposed. These criteria essentially count the
number of neurons whose activation during the execution of a DL system
satisfied certain properties, such as being above predefined thresholds.
However, existing coverage criteria are not sufficiently fine grained to
capture subtle behaviours exhibited by DL systems. Moreover, evaluations have
focused on showing correlation between adversarial examples and proposed
criteria rather than evaluating and guiding their use for actual testing of DL
systems. We propose a novel test adequacy criterion for testing of DL systems,
called Surprise Adequacy for Deep Learning Systems (SADL), which is based on
the behaviour of DL systems with respect to their training data. We measure the
surprise of an input as the difference in DL system&apos;s behaviour between the
input and the training data (i.e., what was learnt during training), and
subsequently develop this as an adequacy criterion: a good test input should be
sufficiently but not overtly surprising compared to training data. Empirical
evaluation using a range of DL systems from simple image classifiers to
autonomous driving car platforms shows that systematic sampling of inputs based
on their surprise can improve classification accuracy of DL systems against
adversarial examples by up to 77.5% via retraining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinhan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldt_R/0/1/0/all/0/1&quot;&gt;Robert Feldt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1&quot;&gt;Shin Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08951">
<title>Water Disaggregation via Shape Features based Bayesian Discriminative Sparse Coding. (arXiv:1808.08951v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.08951</link>
<description rdf:parseType="Literal">&lt;p&gt;As the issue of freshwater shortage is increasing daily, it is critical to
take effective measures for water conservation. According to previous studies,
device level consumption could lead to significant freshwater conservation.
Existing water disaggregation methods focus on learning the signatures for
appliances; however, they are lack of the mechanism to accurately discriminate
parallel appliances&apos; consumption. In this paper, we propose a Bayesian
Discriminative Sparse Coding model using Laplace Prior (BDSC-LP) to extensively
enhance the disaggregation performance. To derive discriminative basis
functions, shape features are presented to describe the low-sampling-rate water
consumption patterns. A Gibbs sampling based inference method is designed to
extend the discriminative capability of the disaggregation dictionaries.
Extensive experiments were performed to validate the effectiveness of the
proposed model using both real-world and synthetic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bingsheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuchao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chang-Tien Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08953">
<title>Term Set Expansion based NLP Architect by Intel AI Lab. (arXiv:1808.08953v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.08953</link>
<description rdf:parseType="Literal">&lt;p&gt;We present SetExpander, a corpus-based system for expanding a seed set of
terms into amore complete set of terms that belong to the same semantic class.
SetExpander implements an iterative end-to-end workflow. It enables users to
easily select a seed set of terms, expand it, view the expanded set, validate
it, re-expand the validated set and store it, thus simplifying the extraction
of domain-specific fine-grained semantic classes.SetExpander has been used
successfully in real-life use cases including integration into an automated
recruitment system and an issues and defects resolution system. A video demo of
SetExpander is available at
https://drive.google.com/open?id=1e545bB87Autsch36DjnJHmq3HWfSd1Rv (some images
were blurred for privacy reasons)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mamou_J/0/1/0/all/0/1&quot;&gt;Jonathan Mamou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pereg_O/0/1/0/all/0/1&quot;&gt;Oren Pereg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasserblat_M/0/1/0/all/0/1&quot;&gt;Moshe Wasserblat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eirew_A/0/1/0/all/0/1&quot;&gt;Alon Eirew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Green_Y/0/1/0/all/0/1&quot;&gt;Yael Green&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guskin_S/0/1/0/all/0/1&quot;&gt;Shira Guskin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izsak_P/0/1/0/all/0/1&quot;&gt;Peter Izsak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korat_D/0/1/0/all/0/1&quot;&gt;Daniel Korat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08954">
<title>Behavior Trees as a Representation for Medical Procedures. (arXiv:1808.08954v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1808.08954</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: Effective collaboration between machines and clinicians requires
flexible data structures to represent medical processes and clinical practice
guidelines. Such a data structure could enable effective turn-taking between
human and automated components of a complex treatment, accurate on-line
monitoring of clinical treatments (for example to detect medical errors), or
automated treatment systems (such as future medical robots) whose overall
treatment plan is understandable and auditable by human experts.
&lt;/p&gt;
&lt;p&gt;Materials and Methods: Behavior trees (BTs) emerged from video game
development as a graphical language for modeling intelligent agent behavior.
BTs have several properties which are attractive for modeling medical
procedures including human-readability, authoring tools, and composability.
&lt;/p&gt;
&lt;p&gt;Results: This paper will illustrate construction of BTs for exemplary medical
procedures and clinical protocols.
&lt;/p&gt;
&lt;p&gt;Discussion and Conclusion: Behavior Trees thus form a useful, and human
authorable/readable bridge between clinical practice guidelines and AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hannaford_B/0/1/0/all/0/1&quot;&gt;Blake Hannaford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bly_R/0/1/0/all/0/1&quot;&gt;Randall Bly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Humphreys_I/0/1/0/all/0/1&quot;&gt;Ian Humphreys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whipple_M/0/1/0/all/0/1&quot;&gt;Mark Whipple&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09057">
<title>Choosing How to Choose Papers. (arXiv:1808.09057v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.09057</link>
<description rdf:parseType="Literal">&lt;p&gt;It is common to see a handful of reviewers reject a highly novel paper,
because they view, say, extensive experiments as far more important than
novelty, whereas the community as a whole would have embraced the paper. More
generally, the disparate mapping of criteria scores to final recommendations by
different reviewers is a major source of inconsistency in peer review. In this
paper we present a framework --- based on $L(p,q)$-norm empirical risk
minimization --- for learning the community&apos;s aggregate mapping. We draw on
computational social choice to identify desirable values of $p$ and $q$;
specifically, we characterize $p=q=1$ as the only choice that satisfies three
natural axiomatic properties. Finally, we implement and apply our approach to
reviews from IJCAI 2017.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noothigattu_R/0/1/0/all/0/1&quot;&gt;Ritesh Noothigattu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nihar B. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Procaccia_A/0/1/0/all/0/1&quot;&gt;Ariel D. Procaccia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09062">
<title>Cognitive Consistency Routing Algorithm of Capsule-network. (arXiv:1808.09062v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.09062</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Neural Networks (ANNs) are computational models inspired by the
central nervous system (especially the brain) of animals and are used to
estimate or generate unknown approximation functions relied on large amounts of
inputs. Capsule Neural Network (Sabour S, et al.[2017]) is a novel structure of
Convolutional Neural Networks which simulates the visual processing system of
human brain. In this paper, we introduce psychological theories which called
Cognitive Consistency to optimize the routing algorithm of Capsnet to make it
more close to the work pattern of human brain. It has been shown in the
experiment that a progress had been made compared with the baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huayu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09079">
<title>A Framework for Complementary Companion Character Behavior in Video Games. (arXiv:1808.09079v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.09079</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a game development framework capable of governing the behavior of
complementary companions in a video game. A &quot;complementary&quot; action is
contrasted with a mimicking action and is defined as any action by a friendly
non-player character that furthers the player&apos;s strategy. This is determined
through a combination of both player action and game state prediction processes
while allowing the AI companion to experiment. We determine the location of
interest for companion actions based on a dynamic set of regions customized to
the individual player. A user study shows promising results; a majority of
participants familiar with game design react positively to the companion
behavior, stating that they would consider using the frame-work in future games
themselves.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scott_G/0/1/0/all/0/1&quot;&gt;Gavin Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khosmood_F/0/1/0/all/0/1&quot;&gt;Foaad Khosmood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09123">
<title>Investigating Human + Machine Complementarity for Recidivism Predictions. (arXiv:1808.09123v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09123</link>
<description rdf:parseType="Literal">&lt;p&gt;When might human input help (or not) when assessing risk in fairness-related
domains? Dressel and Farid asked Mechanical Turk workers to evaluate a subset
of individuals in the ProPublica COMPAS data set for risk of recidivism, and
concluded that COMPAS predictions were no more accurate or fair than
predictions made by humans. We delve deeper into this claim in this paper. We
construct a Human Risk Score based on the predictions made by multiple
Mechanical Turk workers on the same individual, study the agreement and
disagreement between COMPAS and Human Scores on subgroups of individuals, and
construct hybrid Human+AI models to predict recidivism. Our key finding is that
on this data set, human and COMPAS decision making differed, but not in ways
that could be leveraged to significantly improve ground truth prediction. We
present the results of our analyses and suggestions for how machine and human
input may have complementary strengths to address challenges in the fairness
domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Sarah Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adebayo_J/0/1/0/all/0/1&quot;&gt;Julius Adebayo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inkpen_K/0/1/0/all/0/1&quot;&gt;Kori Inkpen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1&quot;&gt;Ece Kamar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03846">
<title>&quot;Dave...I can assure you...that it&apos;s going to be all right...&quot; -- A definition, case for, and survey of algorithmic assurances in human-autonomy trust relationships. (arXiv:1711.03846v4 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03846</link>
<description rdf:parseType="Literal">&lt;p&gt;People who design, use, and are affected by autonomous artificially
intelligent agents want to be able to \emph{trust} such agents -- that is, to
know that these agents will perform correctly, to understand the reasoning
behind their actions, and to know how to use them appropriately. Many
techniques have been devised to assess and influence human trust in
artificially intelligent agents. However, these approaches are typically ad
hoc, and have not been formally related to each other or to formal trust
models. This paper presents a survey of \emph{algorithmic assurances}, i.e.
programmed components of agent operation that are expressly designed to
calibrate user trust in artificially intelligent agents. Algorithmic assurances
are first formally defined and classified from the perspective of formally
modeled human-artificially intelligent agent trust relationships. Building on
these definitions, a synthesis of research across communities such as machine
learning, human-computer interaction, robotics, e-commerce, and others reveals
that assurance algorithms naturally fall along a spectrum in terms of their
impact on an agent&apos;s core functionality, with seven notable classes ranging
from integral assurances (which impact an agent&apos;s core functionality) to
supplemental assurances (which have no direct effect on agent performance).
Common approaches within each of these classes are identified and discussed;
benefits and drawbacks of different approaches are also investigated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Israelsen_B/0/1/0/all/0/1&quot;&gt;Brett W Israelsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nisar R Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10467">
<title>Interaction-Aware Probabilistic Behavior Prediction in Urban Environments. (arXiv:1804.10467v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10467</link>
<description rdf:parseType="Literal">&lt;p&gt;Planning for autonomous driving in complex, urban scenarios requires accurate
prediction of the trajectories of surrounding traffic participants. Their
future behavior depends on their route intentions, the road-geometry, traffic
rules and mutual interaction, resulting in interdependencies between their
trajectories. We present a probabilistic prediction framework based on a
dynamic Bayesian network, which represents the state of the complete scene
including all agents and respects the aforementioned dependencies. We propose
Markovian, context-dependent motion models to define the interaction-aware
behavior of drivers. At first, the state of the dynamic Bayesian network is
estimated over time by tracking the single agents via sequential Monte Carlo
inference. Secondly, we perform a probabilistic forward simulation of the
network&apos;s estimated belief state to generate the different combinatorial scene
developments. This provides the corresponding trajectories for the set of
possible, future scenes. Our framework can handle various road layouts and
number of traffic participants. We evaluate the approach in online simulations
and real-world scenarios. It is shown that our interaction-aware prediction
outperforms interaction-unaware physics- and map-based approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_J/0/1/0/all/0/1&quot;&gt;Jens Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubmann_C/0/1/0/all/0/1&quot;&gt;Constantin Hubmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lochner_J/0/1/0/all/0/1&quot;&gt;Julian L&amp;#xf6;chner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burschka_D/0/1/0/all/0/1&quot;&gt;Darius Burschka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11112">
<title>Is One Hyperparameter Optimizer Enough?. (arXiv:1807.11112v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1807.11112</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperparameter tuning is the black art of automatically finding a good
combination of control parameters for a data miner. While widely applied in
empirical Software Engineering, there has not been much discussion on which
hyperparameter tuner is best for software analytics. To address this gap in the
literature, this paper applied a range of hyperparameter optimizers (grid
search, random search, differential evolution, and Bayesian optimization) to
defect prediction problem. Surprisingly, no hyperparameter optimizer was
observed to be `best&apos; and, for one of the two evaluation measures studied here
(F-measure), hyperparameter optimization, in 50\% cases, was no better than
using default configurations.
&lt;/p&gt;
&lt;p&gt;We conclude that hyperparameter optimization is more nuanced than previously
believed. While such optimization can certainly lead to large improvements in
the performance of classifiers used in software analytics, it remains to be
seen which specific optimizers should be applied to a new dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_H/0/1/0/all/0/1&quot;&gt;Huy Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1&quot;&gt;Vivek Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1&quot;&gt;Tim Menzies&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11227">
<title>YouTube AV 50K: an Annotated Corpus for Comments in Autonomous Vehicles. (arXiv:1807.11227v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1807.11227</link>
<description rdf:parseType="Literal">&lt;p&gt;With one billion monthly viewers, and millions of users discussing and
sharing opinions, comments below YouTube videos are rich sources of data for
opinion mining and sentiment analysis. We introduce the YouTube AV 50K dataset,
a freely-available collections of more than 50,000 YouTube comments and
metadata below autonomous vehicle (AV)-related videos. We describe its creation
process, its content and data format, and discuss its possible usages.
Especially, we do a case study of the first self-driving car fatality to
evaluate the dataset, and show how we can use this dataset to better understand
public attitudes toward self-driving cars and public reactions to the accident.
Future developments of the dataset are also discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Minsoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kaiming Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1&quot;&gt;Siyuan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00177">
<title>Learning Dexterous In-Hand Manipulation. (arXiv:1808.00177v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.00177</link>
<description rdf:parseType="Literal">&lt;p&gt;We use reinforcement learning (RL) to learn dexterous in-hand manipulation
policies which can perform vision-based object reorientation on a physical
Shadow Dexterous Hand. The training is performed in a simulated environment in
which we randomize many of the physical properties of the system like friction
coefficients and an object&apos;s appearance. Our policies transfer to the physical
robot despite being trained entirely in simulation. Our method does not rely on
any human demonstrations, but many behaviors found in human manipulation emerge
naturally, including finger gaiting, multi-finger coordination, and the
controlled use of gravity. Our results were obtained using the same distributed
RL system that was used to train OpenAI Five. We also include a video of our
results: https://youtu.be/jwSbzNHGflM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OpenAI/0/1/0/all/0/1&quot;&gt;OpenAI&lt;/a&gt;: &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrychowicz_M/0/1/0/all/0/1&quot;&gt;Marcin Andrychowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baker_B/0/1/0/all/0/1&quot;&gt;Bowen Baker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chociej_M/0/1/0/all/0/1&quot;&gt;Maciek Chociej&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jozefowicz_R/0/1/0/all/0/1&quot;&gt;Rafal Jozefowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGrew_B/0/1/0/all/0/1&quot;&gt;Bob McGrew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pachocki_J/0/1/0/all/0/1&quot;&gt;Jakub Pachocki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petron_A/0/1/0/all/0/1&quot;&gt;Arthur Petron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plappert_M/0/1/0/all/0/1&quot;&gt;Matthias Plappert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Powell_G/0/1/0/all/0/1&quot;&gt;Glenn Powell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Alex Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jonas Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sidor_S/0/1/0/all/0/1&quot;&gt;Szymon Sidor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tobin_J/0/1/0/all/0/1&quot;&gt;Josh Tobin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welinder_P/0/1/0/all/0/1&quot;&gt;Peter Welinder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_L/0/1/0/all/0/1&quot;&gt;Lilian Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaremba_W/0/1/0/all/0/1&quot;&gt;Wojciech Zaremba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08578">
<title>Automatic 3D bi-ventricular segmentation of cardiac images by a shape-constrained multi-task deep learning approach. (arXiv:1808.08578v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1808.08578</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning approaches have achieved state-of-the-art performance in
cardiac magnetic resonance (CMR) image segmentation. However, most approaches
have focused on learning image intensity features for segmentation, whereas the
incorporation of anatomical shape priors has received less attention. In this
paper, we combine a multi-task deep learning approach with atlas propagation to
develop a shape-constrained bi-ventricular segmentation pipeline for short-axis
CMR volumetric images. The pipeline first employs a fully convolutional network
(FCN) that learns segmentation and landmark localisation tasks simultaneously.
The architecture of the proposed FCN uses a 2.5D representation, thus combining
the computational advantage of 2D FCNs networks and the capability of
addressing 3D spatial consistency without compromising segmentation accuracy.
Moreover, the refinement step is designed to explicitly enforce a shape
constraint and improve segmentation quality. This step is effective for
overcoming image artefacts (e.g. due to different breath-hold positions and
large slice thickness), which preclude the creation of anatomically meaningful
3D cardiac shapes. The proposed pipeline is fully automated, due to network&apos;s
ability to infer landmarks, which are then used downstream in the pipeline to
initialise atlas propagation. We validate the pipeline on 1831 healthy subjects
and 649 subjects with pulmonary hypertension. Extensive numerical experiments
on the two datasets demonstrate that our proposed method is robust and capable
of producing accurate, high-resolution and anatomically smooth bi-ventricular
3D models, despite the artefacts in input CMR volumes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinming Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bello_G/0/1/0/all/0/1&quot;&gt;Ghalib Bello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlemper_J/0/1/0/all/0/1&quot;&gt;Jo Schlemper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1&quot;&gt;Wenjia Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dawes_T/0/1/0/all/0/1&quot;&gt;Timothy J W Dawes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biffi_C/0/1/0/all/0/1&quot;&gt;Carlo Biffi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marvao_A/0/1/0/all/0/1&quot;&gt;Antonio de Marvao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doumou_G/0/1/0/all/0/1&quot;&gt;Georgia Doumou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ORegan_D/0/1/0/all/0/1&quot;&gt;Declan P O&amp;#x27;Regan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08750">
<title>Generalisation in humans and deep neural networks. (arXiv:1808.08750v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1808.08750</link>
<description rdf:parseType="Literal">&lt;p&gt;We compare the robustness of humans and current convolutional deep neural
networks (DNNs) on object recognition under twelve different types of image
degradations. First, using three well known DNNs (ResNet-152, VGG-19,
GoogLeNet) we find the human visual system to be more robust to nearly all of
the tested image manipulations, and we observe progressively diverging
classification error-patterns between humans and DNNs when the signal gets
weaker. Secondly, we show that DNNs trained directly on distorted images
consistently surpass human performance on the exact distortion types they were
trained on, yet they display extremely poor generalisation abilities when
tested on other distortion types. For example, training on salt-and-pepper
noise does not imply robustness on uniform white noise and vice versa. Thus,
changes in the noise distribution between training and testing constitutes a
crucial challenge to deep learning vision systems that can be systematically
addressed in a lifelong machine learning approach. Our new dataset consisting
of 83K carefully measured human psychophysical trials provide a useful
reference for lifelong robustness against image degradations set by the human
visual system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1&quot;&gt;Robert Geirhos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Temme_C/0/1/0/all/0/1&quot;&gt;Carlos R. Medina Temme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rauber_J/0/1/0/all/0/1&quot;&gt;Jonas Rauber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuett_H/0/1/0/all/0/1&quot;&gt;Heiko H. Schuett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1&quot;&gt;Matthias Bethge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wichmann_F/0/1/0/all/0/1&quot;&gt;Felix A. Wichmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08952">
<title>Deep Learning of Vortex Induced Vibrations. (arXiv:1808.08952v1 [physics.flu-dyn])</title>
<link>http://arxiv.org/abs/1808.08952</link>
<description rdf:parseType="Literal">&lt;p&gt;Vortex induced vibrations of bluff bodies occur when the vortex shedding
frequency is close to the natural frequency of the structure. Of interest is
the prediction of the lift and drag forces on the structure given some limited
and scattered information on the velocity field. This is an inverse problem
that is not straightforward to solve using standard computational fluid
dynamics (CFD) methods, especially since no information is provided for the
pressure. An even greater challenge is to infer the lift and drag forces given
some dye or smoke visualizations of the flow field. Here we employ deep neural
networks that are extended to encode the incompressible Navier-Stokes equations
coupled with the structure&apos;s dynamic motion equation. In the first case, given
scattered data in space-time on the velocity field and the structure&apos;s motion,
we use four coupled deep neural networks to infer very accurately the
structural parameters, the entire time-dependent pressure field (with no prior
training data), and reconstruct the velocity vector field and the structure&apos;s
dynamic motion. In the second case, given scattered data in space-time on a
concentration field only, we use five coupled deep neural networks to infer
very accurately the vector velocity field and all other quantities of interest
as before. This new paradigm of inference in fluid mechanics for coupled
multi-physics problems enables velocity and pressure quantification from flow
snapshots in small subdomains and can be exploited for flow control
applications and also for system identification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Raissi_M/0/1/0/all/0/1&quot;&gt;Maziar Raissi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Triantafyllou_M/0/1/0/all/0/1&quot;&gt;Michael S. Triantafyllou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Karniadakis_G/0/1/0/all/0/1&quot;&gt;George Em Karniadakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09034">
<title>Importance Weighting and Varational Inference. (arXiv:1808.09034v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09034</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work used importance sampling ideas for better variational bounds on
likelihoods. We clarify the applicability of these ideas to pure probabilistic
inference, by showing the resulting Importance Weighted Variational Inference
(IWVI) technique is an instance of augmented variational inference, thus
identifying the looseness in previous work. Experiments confirm IWVI&apos;s
practicality for probabilistic inference. As a second contribution, we
investigate inference with elliptical distributions, which improves accuracy in
low dimensions, and convergence in high dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Domke_J/0/1/0/all/0/1&quot;&gt;Justin Domke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheldon_D/0/1/0/all/0/1&quot;&gt;Daniel Sheldon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09050">
<title>Adversarial Feature Learning of Online Monitoring Data for Operation Reliability Assessment in Distribution Network. (arXiv:1808.09050v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.09050</link>
<description rdf:parseType="Literal">&lt;p&gt;With deployments of online monitoring systems in distribution networks,
massive amounts of data collected through them contain rich information on the
operating status of distribution networks. By leveraging the data, based on
bidirectional generative adversarial networks (BiGANs), we propose an
unsupervised approach for online distribution reliability assessment. It is
capable of discovering the latent structure and automatically learning the most
representative features of the spatio-temporal data in distribution networks in
an adversarial way and it does not rely on any assumptions of the input data.
Based on the extracted features, a statistical magnitude for them is calculated
to indicate the data behavior. Furthermore, distribution reliability states are
divided into different levels and we combine them with the calculated
confidence level $1-\alpha$, during which clear criteria is defined
empirically. Case studies on both synthetic data and real-world online
monitoring data show that our proposed approach is feasible for the assessment
of distribution operation reliability and outperforms other existed techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qiu_R/0/1/0/all/0/1&quot;&gt;Robert Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mi_T/0/1/0/all/0/1&quot;&gt;Tiebin Mi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09105">
<title>SOLAR: Deep Structured Latent Representations for Model-Based Reinforcement Learning. (arXiv:1808.09105v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09105</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-based reinforcement learning (RL) methods can be broadly categorized as
global model methods, which depend on learning models that provide sensible
predictions in a wide range of states, or local model methods, which
iteratively refit simple models that are used for policy improvement. While
predicting future states that will result from the current actions is
difficult, local model methods only attempt to understand system dynamics in
the neighborhood of the current policy, making it possible to produce local
improvements without ever learning to predict accurately far into the future.
The main idea in this paper is that we can learn representations that make it
easy to retrospectively infer simple dynamics given the data from the current
policy, thus enabling local models to be used for policy learning in complex
systems. To that end, we focus on learning representations with probabilistic
graphical model (PGM) structure, which allows us to devise an efficient local
model method that infers dynamics from real-world rollouts with the PGM as a
global prior. We compare our method to other model-based and model-free RL
methods on a suite of robotics tasks, including manipulation tasks on a real
Sawyer robotic arm directly from camera images. Videos of our results are
available at https://sites.google.com/view/solar-iclips
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Marvin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vikram_S/0/1/0/all/0/1&quot;&gt;Sharad Vikram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1&quot;&gt;Laura Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1&quot;&gt;Matthew J. Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09127">
<title>High-confidence error estimates for learned value functions. (arXiv:1808.09127v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.09127</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating the value function for a fixed policy is a fundamental problem in
reinforcement learning. Policy evaluation algorithms---to estimate value
functions---continue to be developed, to improve convergence rates, improve
stability and handle variability, particularly for off-policy learning. To
understand the properties of these algorithms, the experimenter needs
high-confidence estimates of the accuracy of the learned value functions. For
environments with small, finite state-spaces, like chains, the true value
function can be easily computed, to compute accuracy. For large, or continuous
state-spaces, however, this is no longer feasible. In this paper, we address
the largely open problem of how to obtain these high-confidence estimates, for
general state-spaces. We provide a high-confidence bound on an empirical
estimate of the value error to the true value error. We use this bound to
design an offline sampling algorithm, which stores the required quantities to
repeatedly compute value error estimates for any learned value function. We
provide experiments investigating the number of samples required by this
offline algorithm in simple benchmark reinforcement learning domains, and
highlight that there are still many open questions to be solved for this
important problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sajed_T/0/1/0/all/0/1&quot;&gt;Touqir Sajed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chung_W/0/1/0/all/0/1&quot;&gt;Wesley Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+White_M/0/1/0/all/0/1&quot;&gt;Martha White&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09144">
<title>Weighted total variation based convex clustering. (arXiv:1808.09144v1 [math.OC])</title>
<link>http://arxiv.org/abs/1808.09144</link>
<description rdf:parseType="Literal">&lt;p&gt;Data clustering is a fundamental problem with a wide range of applications.
Standard methods, eg the $k$-means method, usually require solving a non-convex
optimization problem. Recently, total variation based convex relaxation to the
$k$-means model has emerged as an attractive alternative for data clustering.
However, the existing results on its exact clustering property, ie, the
condition imposed on data so that the method can provably give correct
identification of all cluster memberships, is only applicable to very specific
data and is also much more restrictive than that of some other methods. This
paper aims at the revisit of total variation based convex clustering, by
proposing a weighted sum-of-$\ell_1$-norm relating convex model. Its exact
clustering property established in this paper, in both deterministic and
probabilistic context, is applicable to general data and is much sharper than
the existing results. These results provided good insights to advance the
research on convex clustering. Moreover, the experiments also demonstrated that
the proposed convex model has better empirical performance when be compared to
standard clustering methods, and thus it can see its potential in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guodong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yu Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Hui Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09270">
<title>Models for Predicting Community-Specific Interest in News Articles. (arXiv:1808.09270v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.09270</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we ask two questions: 1. Can we predict the type of community
interested in a news article using only features from the article content? and
2. How well do these models generalize over time? To answer these questions, we
compute well-studied content-based features on over 60K news articles from 4
communities on reddit.com. We train and test models over three different time
periods between 2015 and 2017 to demonstrate which features degrade in
performance the most due to concept drift. Our models can classify news
articles into communities with high accuracy, ranging from 0.81 ROC AUC to 1.0
ROC AUC. However, while we can predict the community-specific popularity of
news articles with high accuracy, practitioners should approach these models
carefully. Predictions are both community-pair dependent and feature group
dependent. Moreover, these feature groups generalize over time differently,
with some only degrading slightly over time, but others degrading greatly.
Therefore, we recommend that community-interest predictions are done in a
hierarchical structure, where multiple binary classifiers can be used to
separate community pairs, rather than a traditional multi-class model. Second,
these models should be retrained over time based on accuracy goals and the
availability of training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horne_B/0/1/0/all/0/1&quot;&gt;Benjamin D. Horne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dron_W/0/1/0/all/0/1&quot;&gt;William Dron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adali_S/0/1/0/all/0/1&quot;&gt;Sibel Adali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09271">
<title>Distance Based Source Domain Selection for Sentiment Classification. (arXiv:1808.09271v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.09271</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated sentiment classification (SC) on short text fragments has received
increasing attention in recent years. Performing SC on unseen domains with few
or no labeled samples can significantly affect the classification performance
due to different expression of sentiment in source and target domain. In this
study, we aim to mitigate this undesired impact by proposing a methodology
based on a predictive measure, which allows us to select an optimal source
domain from a set of candidates. The proposed measure is a linear combination
of well-known distance functions between probability distributions supported on
the source and target domains (e.g. Earth Mover&apos;s distance and Kullback-Leibler
divergence). The performance of the proposed methodology is validated through
an SC case study in which our numerical experiments suggest a significant
improvement in the cross domain classification error in comparison with a
random selected source domain for both a naive and adaptive learning setting.
In the case of more heterogeneous datasets, the predictability feature of the
proposed model can be utilized to further select a subset of candidate domains,
where the corresponding classifier outperforms the one trained on all available
source domains. This observation reinforces a hypothesis that our proposed
model may also be deployed as a means to filter out redundant information
during a training phase of SC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schultz_L/0/1/0/all/0/1&quot;&gt;Lex Razoux Schultz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1&quot;&gt;Marco Loog&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esfahani_P/0/1/0/all/0/1&quot;&gt;Peyman Mohajerin Esfahani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09334">
<title>A Discriminative Latent-Variable Model for Bilingual Lexicon Induction. (arXiv:1808.09334v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09334</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel discriminative latent-variable model for the task of
bilingual lexicon induction. Our model combines the bipartite matching
dictionary prior of Haghighi et al. (2008) with a state-of-the-art
embedding-based approach. To train the model, we derive an efficient Viterbi EM
algorithm. We provide empirical improvements on six language pairs under two
metrics and show that the prior theoretically and empirically helps to mitigate
the hubness problem. We also demonstrate how previous work may be viewed as a
similarly fashioned latent-variable model, albeit with a different prior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1&quot;&gt;Sebastian Ruder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kementchedjhieva_Y/0/1/0/all/0/1&quot;&gt;Yova Kementchedjhieva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1&quot;&gt;Anders S&amp;#xf8;gaard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09347">
<title>Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation. (arXiv:1808.09347v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09347</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, considerable effort has been devoted to deep domain adaptation in
computer vision and machine learning communities. However, most of existing
work only concentrates on learning shared feature representation by minimizing
the distribution discrepancy across different domains. Due to the fact that all
the domain alignment approaches can only reduce, but not remove the domain
shift. Target domain samples distributed near the edge of the clusters, or far
from their corresponding class centers are easily to be misclassified by the
hyperplane learned from the source domain. To alleviate this issue, we propose
to joint domain alignment and discriminative feature learning, which could
benefit both domain alignment and final classification. Specifically, an
instance-based discriminative feature learning method and a center-based
discriminative feature learning method are proposed, both of which guarantee
the domain invariant features with better intra-class compactness and
inter-class separability. Extensive experiments show that learning the
discriminative features in the shared feature space can significantly boost the
performance of deep domain adaptation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhihong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Boyuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xinyu Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09371">
<title>Matrix Factorization Equals Efficient Co-occurrence Representation. (arXiv:1808.09371v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09371</link>
<description rdf:parseType="Literal">&lt;p&gt;Matrix factorization is a simple and effective solution to the recommendation
problem. It has been extensively employed in the industry and has attracted
much attention from the academia. However, it is unclear what the
low-dimensional matrices represent. We show that matrix factorization can
actually be seen as simultaneously calculating the eigenvectors of the
user-user and item-item sample co-occurrence matrices. We then use insights
from random matrix theory (RMT) to show that picking the top eigenvectors
corresponds to removing sampling noise from user/item co-occurrence matrices.
Therefore, the low-dimension matrices represent a reduced noise user and item
co-occurrence space. We also analyze the structure of the top eigenvector and
show that it corresponds to global effects and removing it results in less
popular items being recommended. This increases the diversity of the items
recommended without affecting the accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khawar_F/0/1/0/all/0/1&quot;&gt;Farhan Khawar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Nevin L. Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.01216">
<title>Decentralized Frank-Wolfe Algorithm for Convex and Non-convex Problems. (arXiv:1612.01216v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1612.01216</link>
<description rdf:parseType="Literal">&lt;p&gt;Decentralized optimization algorithms have received much attention due to the
recent advances in network information processing. However, conventional
decentralized algorithms based on projected gradient descent are incapable of
handling high dimensional constrained problems, as the projection step becomes
computationally prohibitive to compute. To address this problem, this paper
adopts a projection-free optimization approach, a.k.a.~the Frank-Wolfe (FW) or
conditional gradient algorithm. We first develop a decentralized FW (DeFW)
algorithm from the classical FW algorithm. The convergence of the proposed
algorithm is studied by viewing the decentralized algorithm as an inexact FW
algorithm. Using a diminishing step size rule and letting $t$ be the iteration
number, we show that the DeFW algorithm&apos;s convergence rate is ${\cal O}(1/t)$
for convex objectives; is ${\cal O}(1/t^2)$ for strongly convex objectives with
the optimal solution in the interior of the constraint set; and is ${\cal
O}(1/\sqrt{t})$ towards a stationary point for smooth but non-convex
objectives. We then show that a consensus-based DeFW algorithm meets the above
guarantees with two communication rounds per iteration. Furthermore, we
demonstrate the advantages of the proposed DeFW algorithm on low-complexity
robust matrix completion and communication efficient sparse learning. Numerical
results on synthetic and real data are presented to support our findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wai_H/0/1/0/all/0/1&quot;&gt;Hoi-To Wai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lafond_J/0/1/0/all/0/1&quot;&gt;Jean Lafond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Scaglione_A/0/1/0/all/0/1&quot;&gt;Anna Scaglione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Moulines_E/0/1/0/all/0/1&quot;&gt;Eric Moulines&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.02658">
<title>Distributed Statistical Estimation and Rates of Convergence in Normal Approximation. (arXiv:1704.02658v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1704.02658</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a class of new algorithms for distributed statistical
estimation that exploit divide-and-conquer approach. We show that one of the
key benefits of the divide-and-conquer strategy is robustness, an important
characteristic for large distributed systems. We establish connections between
performance of these distributed algorithms and the rates of convergence in
normal approximation, and prove non-asymptotic deviations guarantees, as well
as limit theorems, for the resulting estimators. Our techniques are illustrated
through several examples: in particular, we obtain new results for the
median-of-means estimator, as well as provide performance guarantees for
distributed maximum likelihood estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Minsker_S/0/1/0/all/0/1&quot;&gt;Stanislav Minsker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Strawn_N/0/1/0/all/0/1&quot;&gt;Nate Strawn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02920">
<title>Spectral State Compression of Markov Processes. (arXiv:1802.02920v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02920</link>
<description rdf:parseType="Literal">&lt;p&gt;Model reduction of the Markov process is a basic problem in modeling
state-transition systems. Motivated by the state aggregation approach rooted in
control theory, we study the statistical state compression of a finite-state
Markov chain from empirical trajectories. Through the lens of spectral
decomposition, we study the rank and features of Markov processes, as well as
properties like representability, aggregatability, and lumpability. We develop
a class of spectral state compression methods for three tasks: (1) estimate the
transition matrix of a low-rank Markov model, (2) estimate the leading subspace
spanned by Markov features, and (3) recover latent structures of the state
space like state aggregation and lumpable partition. The proposed methods
provide an unsupervised learning framework for identifying Markov features and
clustering states. We provide upper bounds for the estimation errors and nearly
matching minimax lower bounds. Numerical studies are performed on synthetic
data and a dataset of New York City taxi trips.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04572">
<title>COPA: Constrained PARAFAC2 for Sparse &amp; Large Datasets. (arXiv:1803.04572v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04572</link>
<description rdf:parseType="Literal">&lt;p&gt;PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is modeling
treatments across a set of patients with the varying number of medical
encounters over time. Despite recent improvements on unconstrained PARAFAC2,
its model factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a {\it CO}nstrained {\it
PA}RAFAC2 (COPA) method, which carefully incorporates optimization constraints
such as temporal smoothness, sparsity, and non-negativity in the resulting
factors. To efficiently support all those constraints, COPA adopts a hybrid
optimization framework using alternating optimization and alternating direction
method of multiplier (AO-ADMM). As evaluated on large electronic health record
(EHR) datasets with hundreds of thousands of patients, COPA achieves
significant speedups (up to 36 times faster) over prior PARAFAC2 approaches
that only attempt to handle a subset of the constraints that COPA enables.
Overall, our method outperforms all the baselines attempting to handle a subset
of the constraints in terms of speed, while achieving the same level of
accuracy. Through a case study on temporal phenotyping of medically complex
children, we demonstrate how the constraints imposed by COPA reveal concise
phenotypes and meaningful temporal profiles of patients. The clinical
interpretation of both the phenotypes and the temporal profiles was confirmed
by a medical expert.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afshar_A/0/1/0/all/0/1&quot;&gt;Ardavan Afshar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perros_I/0/1/0/all/0/1&quot;&gt;Ioakeim Perros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1&quot;&gt;Evangelos E. Papalexakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Searles_E/0/1/0/all/0/1&quot;&gt;Elizabeth Searles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1&quot;&gt;Joyce Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03740">
<title>Multimodal Sparse Bayesian Dictionary Learning. (arXiv:1804.03740v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.03740</link>
<description rdf:parseType="Literal">&lt;p&gt;The purpose of this paper is to address the problem of learning dictionaries
for multimodal datasets, i.e. datasets collected from multiple data sources. We
present an algorithm called multimodal sparse Bayesian dictionary learning
(MSBDL). MSBDL leverages information from all available data modalities through
a joint sparsity constraint. The underlying framework offers a considerable
amount of flexibility to practitioners and addresses many of the shortcomings
of existing multimodal dictionary learning approaches. In particular, the
procedure includes the automatic tuning of hyperparameters and is unique in
that it allows the dictionaries for each data modality to have different
cardinality, a significant feature in cases when the dimensionality of data
differs across modalities. MSBDL is scalable and can be used in supervised
learning settings. Theoretical results relating to the convergence of MSBDL are
presented and the numerical results provide evidence of the superior
performance on synthetic and real datasets compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fedorov_I/0/1/0/all/0/1&quot;&gt;Igor Fedorov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rao_B/0/1/0/all/0/1&quot;&gt;Bhaskar D. Rao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10173">
<title>Differential Analysis of Directed Networks. (arXiv:1807.10173v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1807.10173</link>
<description rdf:parseType="Literal">&lt;p&gt;We developed a novel statistical method to identify structural differences
between networks characterized by structural equation models. We propose to
reparameterize the model to separate the differential structures from common
structures, and then design an algorithm with calibration and construction
stages to identify these differential structures. The calibration stage serves
to obtain consistent prediction by building the L2 regularized regression of
each endogenous variables against pre-screened exogenous variables, correcting
for potential endogeneity issue. The construction stage consistently selects
and estimates both common and differential effects by undertaking L1
regularized regression of each endogenous variable against the predicts of
other endogenous variables as well as its anchoring exogenous variables. Our
method allows easy parallel computation at each stage. Theoretical results are
obtained to establish nonasymptotic error bounds of predictions and estimates
at both stages, as well as the consistency of identified common and
differential effects. Our studies on synthetic data demonstrated that our
proposed method performed much better than independently constructing the
networks. A real data set is analyzed to illustrate the applicability of our
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ren_M/0/1/0/all/0/1&quot;&gt;Min Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dabao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08646">
<title>The Disparate Effects of Strategic Manipulation. (arXiv:1808.08646v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.08646</link>
<description rdf:parseType="Literal">&lt;p&gt;When consequential decisions are informed by algorithmic input, individuals
may feel compelled to alter their behavior in order to gain a system&apos;s
approval. Previous models of agent responsiveness, termed &quot;strategic
manipulation,&quot; have analyzed the interaction between a learner and agents in a
world where all agents are equally able to manipulate their features in an
attempt to &quot;trick&quot; a published classifier. In cases of real world
classification, however, an agent&apos;s ability to adapt to an algorithm, is not
simply a function of her personal interest in receiving a positive
classification, but is bound up in a complex web of social factors that affect
her ability to pursue certain action responses. In this paper, we adapt models
of strategic manipulation to better capture dynamics that may arise in a
setting of social inequality wherein candidate groups face different costs to
manipulation. We find that whenever one group&apos;s costs are higher than the
other&apos;s, the learner&apos;s equilibrium strategy exhibits an inequality-reinforcing
phenomenon wherein the learner erroneously admits some members of the
advantaged group, while erroneously excluding some members of the disadvantaged
group. We also consider the effects of potential interventions in which a
learner can subsidize members of the disadvantaged group, lowering their costs
in order to improve her own classification performance. Here we encounter a
paradoxical result: there exist cases in which providing a subsidy improves
only the learner&apos;s utility while actually making both candidate groups
worse-off--even the group receiving the subsidy. Our results reveal the
potentially adverse social ramifications of deploying tools that attempt to
evaluate an individual&apos;s &quot;quality&quot; when agents&apos; capacities to adaptively
respond differ.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Lily Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Immorlica_N/0/1/0/all/0/1&quot;&gt;Nicole Immorlica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaughan_J/0/1/0/all/0/1&quot;&gt;Jennifer Wortman Vaughan&lt;/a&gt;</dc:creator>
</item></rdf:RDF>