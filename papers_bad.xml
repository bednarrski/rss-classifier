<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-29T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05093"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09446"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09563"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09572"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09658"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09735"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09806"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09809"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09847"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03144"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09517"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09526"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09574"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09617"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09638"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09642"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09645"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09663"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09670"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09784"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09785"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09802"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09856"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09897"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09920"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08012"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04566"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07746"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12317"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05689"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07226"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07380"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08270"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.05093">
<title>Heterogeneous Multilayer Generalized Operational Perceptron. (arXiv:1804.05093v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05093</link>
<description rdf:parseType="Literal">&lt;p&gt;The traditional Multilayer Perceptron (MLP) using McCulloch-Pitts neuron
model is inherently limited to a set of neuronal activities, i.e., linear
weighted sum followed by nonlinear thresholding step. Previously, Generalized
Operational Perceptron (GOP) was proposed to extend conventional perceptron
model by defining a diverse set of neuronal activities to imitate a generalized
model of biological neurons. Together with GOP, Progressive Operational
Perceptron (POP) algorithm was proposed to optimize a pre-defined template of
multiple homogeneous layers in a layerwise manner. In this paper, we propose an
efficient algorithm to learn a compact, fully heterogeneous multilayer network
that allows each individual neuron, regardless of the layer, to have distinct
characteristics. Based on the complexity of the problem, the proposed algorithm
operates in a progressive manner on a neuronal level, searching for a compact
topology, not only in terms of depth but also width, i.e., the number of
neurons in each layer. The proposed algorithm is shown to outperform other
related learning methods in extensive experiments on several classification
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1&quot;&gt;Dat Thanh Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1&quot;&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1&quot;&gt;Moncef Gabbouj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1&quot;&gt;Alexandros Iosifidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09446">
<title>A Particle Filter based Multi-Objective Optimization Algorithm: PFOPS. (arXiv:1808.09446v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.09446</link>
<description rdf:parseType="Literal">&lt;p&gt;This letter is concerned with a recently developed paradigm of
population-based optimization, termed particle filter optimization (PFO). In
contrast with the commonly used meta-heuristics based methods, the PFO paradigm
is attractive in terms of coherence in theory and easiness in mathematical
analysis and interpretation. However, current PFO algorithms only work for
single-objective optimization cases, while many real-life problems involve
multiple objectives to be optimized simultaneously. To this end, we make an
effort to extend the scope of application of the PFO paradigm to
multi-objective optimization (MOO) cases. An idea called path sampling is
adopted within the PFO scheme to balance the different objectives to be
optimized. The resulting algorithm is thus termed PFO with Path Sampling
(PFOPS). Experimental results show that the proposed algorithm works
consistently well for three different types of MOO problems, which are
characterized by an associated convex, concave and discontinuous Pareto front,
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaochu Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09563">
<title>Autonomous drone cinematographer: Using artistic principles to create smooth, safe, occlusion-free trajectories for aerial filming. (arXiv:1808.09563v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1808.09563</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous aerial cinematography has the potential to enable automatic
capture of aesthetically pleasing videos without requiring human intervention,
empowering individuals with the capability of high-end film studios. Current
approaches either only handle off-line trajectory generation, or offer
strategies that reason over short time horizons and simplistic representations
for obstacles, which result in jerky movement and low real-life applicability.
In this work we develop a method for aerial filming that is able to trade off
shot smoothness, occlusion, and cinematography guidelines in a principled
manner, even under noisy actor predictions. We present a novel algorithm for
real-time covariant gradient descent that we use to efficiently find the
desired trajectories by optimizing a set of cost functions. Experimental
results show that our approach creates attractive shots, avoiding obstacles and
occlusion 65 times over 1.25 hours of flight time, re-planning at 5 Hz with a
10 s time horizon. We robustly film human actors, cars and bicycles performing
different motion among obstacles, using various shot types.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonatti_R/0/1/0/all/0/1&quot;&gt;Rogerio Bonatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanfu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1&quot;&gt;Sanjiban Choudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenshan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1&quot;&gt;Sebastian Scherer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09572">
<title>Cycle-of-Learning for Autonomous Systems from Human Interaction. (arXiv:1808.09572v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.09572</link>
<description rdf:parseType="Literal">&lt;p&gt;We discuss different types of human-robot interaction paradigms in the
context of training end-to-end reinforcement learning algorithms. We provide a
taxonomy to categorize the types of human interaction and present our
Cycle-of-Learning framework for autonomous systems that combines different
human-interaction modalities with reinforcement learning. Two key concepts
provided by our Cycle-of-Learning framework are how it handles the integration
of the different human-interaction modalities (demonstration, intervention, and
evaluation) and how to define the switching criteria between them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waytowich_N/0/1/0/all/0/1&quot;&gt;Nicholas R. Waytowich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goecks_V/0/1/0/all/0/1&quot;&gt;Vinicius G. Goecks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawhern_V/0/1/0/all/0/1&quot;&gt;Vernon J. Lawhern&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09658">
<title>APRIL: Interactively Learning to Summarise by Combining Active Preference Learning and Reinforcement Learning. (arXiv:1808.09658v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09658</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to perform automatic document summarisation without using
reference summaries. Instead, our method interactively learns from users&apos;
preferences. The merit of preference-based interactive summarisation is that
preferences are easier for users to provide than reference summaries. Existing
preference-based interactive learning methods suffer from high sample
complexity, i.e. they need to interact with the oracle for many rounds in order
to converge. In this work, we propose a new objective function, which enables
us to leverage active learning, preference learning and reinforcement learning
techniques in order to reduce the sample complexity. Both simulation and
real-user experiments suggest that our method significantly advances the state
of the art. Our source code is freely available at
https://github.com/UKPLab/emnlp2018-april.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1&quot;&gt;Christian M. Meyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1&quot;&gt;Iryna Gurevych&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09735">
<title>Bringing personalized learning into computer-aided question generation. (arXiv:1808.09735v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1808.09735</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel and statistical method of ability estimation
based on acquisition distribution for a personalized computer aided question
generation. This method captures the learning outcomes over time and provides a
flexible measurement based on the acquisition distributions instead of
precalibration. Compared to the previous studies, the proposed method is
robust, especially when an ability of a student is unknown. The results from
the empirical data show that the estimated abilities match the actual abilities
of learners, and the pretest and post-test of the experimental group show
significant improvement. These results suggest that this method can serves as
the ability estimation for a personalized computer-aided testing environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yi-Ting Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Meng Chang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yeali S. Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09806">
<title>MARL-FWC: Optimal Coordination of Freeway Traffic Control Measures. (arXiv:1808.09806v1 [cs.SY])</title>
<link>http://arxiv.org/abs/1808.09806</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective of this article is to optimize the overall traffic flow on
freeways using multiple ramp metering controls plus its complementary Dynamic
Speed Limits (DSLs). An optimal freeway operation can be reached when
minimizing the difference between the freeway density and the critical ratio
for maximum traffic flow. In this article, a Multi-Agent Reinforcement Learning
for Freeways Control (MARL-FWC) system for ramps metering and DSLs is proposed.
MARL-FWC introduces a new microscopic framework at the network level based on
collaborative Markov Decision Process modeling (Markov game) and an associated
cooperative Q-learning algorithm. The technique incorporates payoff propagation
(Max-Plus algorithm) under the coordination graphs framework, particularly
suited for optimal control purposes. MARL-FWC provides three control designs:
fully independent, fully distributed, and centralized; suited for different
network architectures. MARL-FWC was extensively tested in order to assess the
proposed model of the joint payoff, as well as the global payoff. Experiments
are conducted with heavy traffic flow under the renowned VISSIM traffic
simulator to evaluate MARL-FWC. The experimental results show a significant
decrease in the total travel time and an increase in the average speed (when
compared with the base case) while maintaining an optimal traffic flow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fares_A/0/1/0/all/0/1&quot;&gt;Ahmed Fares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomaa_W/0/1/0/all/0/1&quot;&gt;Walid Gomaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khamis_M/0/1/0/all/0/1&quot;&gt;Mohamed A. Khamis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09809">
<title>An Efficient Matheuristic for the Minimum-Weight Dominating Set Problem. (arXiv:1808.09809v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.09809</link>
<description rdf:parseType="Literal">&lt;p&gt;A minimum dominating set in a graph is a minimum set of vertices such that
every vertex of the graph either belongs to it, or is adjacent to one vertex of
this set. This mathematical object is of high relevance in a number of
applications related to social networks analysis, design of wireless networks,
coding theory, and data mining, among many others. When vertex weights are
given, minimizing the total weight of the dominating set gives rise to a
problem variant known as the minimum weight dominating set problem. To solve
this problem, we introduce a hybrid matheuristic combining a tabu search with
an integer programming solver. The latter is used to solve subproblems in which
only a fraction of the decision variables, selected relatively to the search
history, are left free while the others are fixed. Moreover, we introduce an
adaptive penalty to promote the exploration of intermediate infeasible
solutions during the search, enhance the algorithm with perturbations and node
elimination procedures, and exploit richer neighborhood classes. Extensive
experimental analyses on a variety of instance classes demonstrate the good
performance of the algorithm, and the contribution of each component in the
success of the search is analyzed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albuquerque_M/0/1/0/all/0/1&quot;&gt;Mayra Albuquerque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_T/0/1/0/all/0/1&quot;&gt;Thibaut Vidal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09819">
<title>Approximate Exploration through State Abstraction. (arXiv:1808.09819v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09819</link>
<description rdf:parseType="Literal">&lt;p&gt;Although exploration in reinforcement learning is well understood from a
theoretical point of view, provably correct methods remain impractical. In this
paper we study the interplay between exploration and approximation, what we
call \emph{approximate exploration}. We first provide results when the
approximation is explicit, quantifying the performance of an exploration
algorithm, MBIE-EB \citep{strehl2008analysis}, when combined with state
aggregation. In particular, we show that this allows the agent to trade off
between learning speed and quality of the policy learned. We then turn to a
successful exploration scheme in practical, pseudo-count based exploration
bonuses \citep{bellemare2016unifying}. We show that choosing a density model
implicitly defines an abstraction and that the pseudo-count bonus incentivizes
the agent to explore using this abstraction. We find, however, that implicit
exploration may result in a mismatch between the approximated value function
and exploration bonus, leading to either under- or over-exploration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taiga_A/0/1/0/all/0/1&quot;&gt;Adrien Ali Ta&amp;#xef;ga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellemare_M/0/1/0/all/0/1&quot;&gt;Marc G. Bellemare&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09847">
<title>Modelling Langford&apos;s Problem: A Viewpoint for Search. (arXiv:1808.09847v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.09847</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance of enumerating all solutions to an instance of Langford&apos;s
Problem is sensitive to the model and the search strategy. In this paper we
compare the performance of a large variety of models, all derived from two base
viewpoints. We empirically show that a channelled model with a static branching
order on one of the viewpoints offers the best performance out of all the
options we consider. Surprisingly, one of the base models proves very effective
for propagation, while the other provides an effective means of stating a
static search order.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akgun_O/0/1/0/all/0/1&quot;&gt;&amp;#xd6;zg&amp;#xfc;r Akg&amp;#xfc;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miguel_I/0/1/0/all/0/1&quot;&gt;Ian Miguel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03144">
<title>Neural Dynamic Programming for Musical Self Similarity. (arXiv:1802.03144v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03144</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a neural sequence model designed specifically for symbolic music.
The model is based on a learned edit distance mechanism which generalises a
classic recursion from computer sci- ence, leading to a neural dynamic program.
Re- peated motifs are detected by learning the transfor- mations between them.
We represent the arising computational dependencies using a novel data
structure, the edit tree; this perspective suggests natural approximations
which afford the scaling up of our otherwise cubic time algorithm. We
demonstrate our model on real and synthetic data; in all cases it out-performs
a strong stacked long short-term memory benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1&quot;&gt;Christian J. Walder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongwoo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08794">
<title>Theoretical Foundations of the A2RD Project: Part I. (arXiv:1808.08794v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1808.08794</link>
<description rdf:parseType="Literal">&lt;p&gt;This article identifies and discusses the theoretical foundations that were
considered in the design of the A2RD model. In addition to the points
considered, references are made to the studies available and considered in the
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braga_J/0/1/0/all/0/1&quot;&gt;Juliao Braga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1&quot;&gt;Joao Nuno Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Endo_P/0/1/0/all/0/1&quot;&gt;Patricia Takako Endo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omar_N/0/1/0/all/0/1&quot;&gt;Nizam Omar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09517">
<title>Extracting Epistatic Interactions in Type 2 Diabetes Genome-Wide Data Using Stacked Autoencoder. (arXiv:1808.09517v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09517</link>
<description rdf:parseType="Literal">&lt;p&gt;2 Diabetes is a leading worldwide public health concern, and its increasing
prevalence has significant health and economic importance in all nations. The
condition is a multifactorial disorder with a complex aetiology. The genetic
determinants remain largely elusive, with only a handful of identified
candidate genes. Genome wide association studies (GWAS) promised to
significantly enhance our understanding of genetic based determinants of common
complex diseases. To date, 83 single nucleotide polymorphisms (SNPs) for type 2
diabetes have been identified using GWAS. Standard statistical tests for single
and multi-locus analysis such as logistic regression, have demonstrated little
effect in understanding the genetic architecture of complex human diseases.
Logistic regression is modelled to capture linear interactions but neglects the
non-linear epistatic interactions present within genetic data. There is an
urgent need to detect epistatic interactions in complex diseases as this may
explain the remaining missing heritability in such diseases. In this paper, we
present a novel framework based on deep learning algorithms that deal with
non-linear epistatic interactions that exist in genome wide association data.
Logistic association analysis under an additive genetic model, adjusted for
genomic control inflation factor, is conducted to remove statistically
improbable SNPs to minimize computational overheads.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdulaimma_B/0/1/0/all/0/1&quot;&gt;Basma Abdulaimma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fergus_P/0/1/0/all/0/1&quot;&gt;Paul Fergus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chalmers_C/0/1/0/all/0/1&quot;&gt;Carl Chalmers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09526">
<title>Deep Lidar CNN to Understand the Dynamics of Moving Vehicles. (arXiv:1808.09526v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.09526</link>
<description rdf:parseType="Literal">&lt;p&gt;Perception technologies in Autonomous Driving are experiencing their golden
age due to the advances in Deep Learning. Yet, most of these systems rely on
the semantically rich information of RGB images. Deep Learning solutions
applied to the data of other sensors typically mounted on autonomous cars (e.g.
lidars or radars) are not explored much. In this paper we propose a novel
solution to understand the dynamics of moving vehicles of the scene from only
lidar information. The main challenge of this problem stems from the fact that
we need to disambiguate the proprio-motion of the &apos;observer&apos; vehicle from that
of the external &apos;observed&apos; vehicles. For this purpose, we devise a CNN
architecture which at testing time is fed with pairs of consecutive lidar
scans. However, in order to properly learn the parameters of this network,
during training we introduce a series of so-called pretext tasks which also
leverage on image data. These tasks include semantic information about
vehicleness and a novel lidar-flow feature which combines standard image-based
optical flow with lidar scans. We obtain very promising results and show that
including distilled image information only during training, allows improving
the inference results of the network at test time, even when image data is no
longer used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaquero_V/0/1/0/all/0/1&quot;&gt;Victor Vaquero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanfeliu_A/0/1/0/all/0/1&quot;&gt;Alberto Sanfeliu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1&quot;&gt;Francesc Moreno-Noguer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09574">
<title>Probabilistic Sparse Subspace Clustering Using Delayed Association. (arXiv:1808.09574v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.09574</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering and clustering subspaces in high-dimensional data is a
fundamental problem of machine learning with a wide range of applications in
data mining, computer vision, and pattern recognition. Earlier methods divided
the problem into two separate stages of finding the similarity matrix and
finding clusters. Similar to some recent works, we integrate these two steps
using a joint optimization approach. We make the following contributions: (i)
we estimate the reliability of the cluster assignment for each point before
assigning a point to a subspace. We group the data points into two groups of
&quot;certain&quot; and &quot;uncertain&quot;, with the assignment of latter group delayed until
their subspace association certainty improves. (ii) We demonstrate that delayed
association is better suited for clustering subspaces that have ambiguities,
i.e. when subspaces intersect or data are contaminated with outliers/noise.
(iii) We demonstrate experimentally that such delayed probabilistic association
leads to a more accurate self-representation and final clusters. The proposed
method has higher accuracy both for points that exclusively lie in one
subspace, and those that are on the intersection of subspaces. (iv) We show
that delayed association leads to huge reduction of computational cost, since
it allows for incremental spectral clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jaberi_M/0/1/0/all/0/1&quot;&gt;Maryam Jaberi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pensky_M/0/1/0/all/0/1&quot;&gt;Marianna Pensky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Foroosh_H/0/1/0/all/0/1&quot;&gt;Hassan Foroosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09617">
<title>Elastic bands across the path: A new framework and methods to lower bound DTW. (arXiv:1808.09617v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09617</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been renewed recent interest in developing effective lower bounds
for Dynamic Time Warping (DTW) distance between time series. These have many
applications in time series indexing, clustering, forecasting, regression and
classification. One of the key time series classification algorithms, the
nearest neighbor algorithm with DTW distance (NN-DTW) is very expensive to
compute, due to the quadratic complexity of DTW. Lower bound search can speed
up NN-DTW substantially. An effective and tight lower bound quickly prunes off
unpromising nearest neighbor candidates from the search space and minimises the
number of the costly DTW computations. The speed up provided by lower bound
search becomes increasingly critical as training set size increases. Different
lower bounds provide different trade-offs between computation time and
tightness. Most existing lower bounds interact with DTW warping window sizes.
They are very tight and effective at smaller warping window sizes, but become
looser as the warping window increases, thus reducing the pruning effectiveness
for NN-DTW. In this work, we present a new class of lower bounds that are
tighter than the popular Keogh lower bound, while requiring similar computation
time. Our new lower bounds take advantage of the DTW boundary condition,
monotonicity and continuity constraints to create a tighter lower bound. Of
particular significance, they remain relatively tight even for large windows. A
single parameter to these new lower bounds controls the speed-tightness
trade-off. We demonstrate that these new lower bounds provide an exceptional
balance between computation time and tightness for the NN-DTW time series
classification task, resulting in greatly improved efficiency for NN-DTW lower
bound search.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chang Wei Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petitjean_F/0/1/0/all/0/1&quot;&gt;Francois Petitjean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1&quot;&gt;Geoffrey I. Webb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09638">
<title>Replay attack spoofing detection system using replay noise by multi-task learning. (arXiv:1808.09638v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1808.09638</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a spoofing detection system for replay attack using
replay noise. In many previous studies across various domains, noise has been
reduced. However, in replay attack, we hypothesize that noise is the prominent
feature which is different with original signal and it can be one of the keys
to find whether a signal has been spoofed. We define the noise that is caused
by the replay attack as replay noise. Specifically, the noise of playback
devices, recording environments, and recording devices, is included in the
replay noise. We explore the effectiveness of training a deep neural network
simultaneously for replay attack spoofing detection and replay noise
classification. Multi-task learning was exploited to embed spoofing detection
and replay noise classification in the code layer. The experiment results on
the ASVspoof2017 datasets demonstrate that the performance of our proposed
system is relatively improved 30% on the evaluation set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shim_H/0/1/0/all/0/1&quot;&gt;Hyejin Shim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Jeeweon Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heo_H/0/1/0/all/0/1&quot;&gt;Heesoo Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sunghyun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hajin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09642">
<title>Online ICA: Understanding Global Dynamics of Nonconvex Optimization via Diffusion Processes. (arXiv:1808.09642v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.09642</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving statistical learning problems often involves nonconvex optimization.
Despite the empirical success of nonconvex statistical optimization methods,
their global dynamics, especially convergence to the desirable local minima,
remain less well understood in theory. In this paper, we propose a new analytic
paradigm based on diffusion processes to characterize the global dynamics of
nonconvex statistical optimization. As a concrete example, we study stochastic
gradient descent (SGD) for the tensor decomposition formulation of independent
component analysis. In particular, we cast different phases of SGD into
diffusion processes, i.e., solutions to stochastic differential equations.
Initialized from an unstable equilibrium, the global dynamics of SGD transit
over three consecutive phases: (i) an unstable Ornstein-Uhlenbeck process
slowly departing from the initialization, (ii) the solution to an ordinary
differential equation, which quickly evolves towards the desirable local
minimum, and (iii) a stable Ornstein-Uhlenbeck process oscillating around the
desirable local minimum. Our proof techniques are based upon Stroock and
Varadhan&apos;s weak convergence of Markov chains to diffusion processes, which are
of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chris Junchi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09645">
<title>Diffusion Approximations for Online Principal Component Estimation and Global Convergence. (arXiv:1808.09645v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.09645</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose to adopt the diffusion approximation tools to study
the dynamics of Oja&apos;s iteration which is an online stochastic gradient descent
method for the principal component analysis. Oja&apos;s iteration maintains a
running estimate of the true principal component from streaming data and enjoys
less temporal and spatial complexities. We show that the Oja&apos;s iteration for
the top eigenvector generates a continuous-state discrete-time Markov chain
over the unit sphere. We characterize the Oja&apos;s iteration in three phases using
diffusion approximation and weak convergence tools. Our three-phase analysis
further provides a finite-sample error bound for the running estimate, which
matches the minimax information lower bound for principal component analysis
under the additional assumption of bounded samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chris Junchi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09663">
<title>Wasserstein is all you need. (arXiv:1808.09663v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09663</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a unified framework for building unsupervised representations of
individual objects or entities (and their compositions), by associating with
each object both a distributional as well as a point estimate (vector
embedding). This is made possible by the use of optimal transport, which allows
us to build these associated estimates while harnessing the underlying geometry
of the ground space. Our method gives a novel perspective for building rich and
powerful feature representations that simultaneously capture uncertainty (via a
distributional estimate) and interpretability (with the optimal transport map).
As a guiding example, we formulate unsupervised representations for text, in
particular for sentence representation and entailment detection. Empirical
results show strong advantages gained through the proposed framework. This
approach can be used for any unsupervised or supervised problem (on text or
other modalities) with a co-occurrence structure, such as any sequence data.
The key tools underlying the framework are Wasserstein distances and
Wasserstein barycenters (and, hence the title!).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sidak Pal Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hug_A/0/1/0/all/0/1&quot;&gt;Andreas Hug&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dieuleveut_A/0/1/0/all/0/1&quot;&gt;Aymeric Dieuleveut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09670">
<title>Accelerated proximal boosting. (arXiv:1808.09670v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09670</link>
<description rdf:parseType="Literal">&lt;p&gt;Gradient boosting is a prediction method that iteratively combines weak
learners to produce a complex and accurate model. From an optimization point of
view, the learning procedure of gradient boosting mimics a gradient descent on
a functional variable. This paper proposes to build upon the proximal point
algorithm when the empirical risk to minimize is not differentiable. In
addition, the novel boosting approach, called accelerated proximal boosting,
benefits from Nesterov&apos;s acceleration in the same way as gradient boosting
[Biau et al., 2018]. Advantages of leveraging proximal methods for boosting are
illustrated by numerical experiments on simulated and real-world data. In
particular, we exhibit a favorable comparison over gradient boosting regarding
convergence rate and prediction accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fouillen_E/0/1/0/all/0/1&quot;&gt;Erwan Fouillen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyer_C/0/1/0/all/0/1&quot;&gt;Claire Boyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sangnier_M/0/1/0/all/0/1&quot;&gt;Maxime Sangnier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09744">
<title>Rule induction for global explanation of trained models. (arXiv:1808.09744v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09744</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the behavior of a trained network and finding explanations for
its outputs is important for improving the network&apos;s performance and
generalization ability, and for ensuring trust in automated systems. Several
approaches have previously been proposed to identify and visualize the most
important features by analyzing a trained network. However, the relations
between different features and classes are lost in most cases. We propose a
technique to induce sets of if-then-else rules that capture these relations to
globally explain the predictions of a network. We first calculate the
importance of the features in the trained network. We then weigh the original
inputs with these feature importance scores, simplify the transformed input
space, and finally fit a rule induction model to explain the model predictions.
We find that the output rule-sets can explain the predictions of a neural
network trained for 4-class text classification from the 20 newsgroups dataset
to a macro-averaged F-score of 0.80. We make the code available at
https://github.com/clips/interpret_with_rules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sushil_M/0/1/0/all/0/1&quot;&gt;Madhumita Sushil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suster_S/0/1/0/all/0/1&quot;&gt;Simon &amp;#x160;uster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daelemans_W/0/1/0/all/0/1&quot;&gt;Walter Daelemans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09784">
<title>Superhighway: Bypass Data Sparsity in Cross-Domain CF. (arXiv:1808.09784v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.09784</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-domain collaborative filtering (CF) aims to alleviate data sparsity in
single-domain CF by leveraging knowledge transferred from related domains. Many
traditional methods focus on enriching compared neighborhood relations in CF
directly to address the sparsity problem. In this paper, we propose
superhighway construction, an alternative explicit relation-enrichment
procedure, to improve recommendations by enhancing cross-domain connectivity.
Specifically, assuming partially overlapped items (users), superhighway
bypasses multi-hop inter-domain paths between cross-domain users (items,
respectively) with direct paths to enrich the cross-domain connectivity. The
experiments conducted on a real-world cross-region music dataset and a
cross-platform movie dataset show that the proposed superhighway construction
significantly improves recommendation performance in both target and source
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1&quot;&gt;Kwei-Herng Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Ting-Hsiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1&quot;&gt;Heng-Yu Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_M/0/1/0/all/0/1&quot;&gt;Ming-Feng Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuan-Ju Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09785">
<title>Using Taste Groups for Collaborative Filtering. (arXiv:1808.09785v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.09785</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit feedback is the simplest form of user feedback that can be used for
item recommendation. It is easy to collect and domain independent. However,
there is a lack of negative examples. Existing works circumvent this problem by
making various assumptions regarding the unconsumed items, which fail to hold
when the user did not consume an item because she was unaware of it. In this
paper, we propose as a novel method for addressing the lack of negative
examples in implicit feedback. The motivation is that if there is a large group
of users who share the same taste and none of them consumed an item, then it is
highly likely that the item is irrelevant to this taste. We use Hierarchical
Latent Tree Analysis(HLTA) to identify taste-based user groups and make
recommendations for a user based on her memberships in the groups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khawar_F/0/1/0/all/0/1&quot;&gt;Farhan Khawar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Nevin L. Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09794">
<title>Correlated Time Series Forecasting using Deep Neural Networks: A Summary of Results. (arXiv:1808.09794v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.09794</link>
<description rdf:parseType="Literal">&lt;p&gt;Cyber-physical systems often consist of entities that interact with each
other over time. Meanwhile, as part of the continued digitization of industrial
processes, various sensor technologies are deployed that enable us to record
time-varying attributes (a.k.a., time series) of such entities, thus producing
correlated time series. To enable accurate forecasting on such correlated time
series, this paper proposes two models that combine convolutional neural
networks (CNNs) and recurrent neural networks (RNNs). The first model employs a
CNN on each individual time series, combines the convoluted features, and then
applies an RNN on top of the convoluted features in the end to enable
forecasting. The second model adds additional auto-encoders into the individual
CNNs, making the second model a multi-task learning model, which provides
accurate and robust forecasting. Experiments on two real-world correlated time
series data set suggest that the proposed two models are effective and
outperform baselines in most settings.
&lt;/p&gt;
&lt;p&gt;This report extends the paper &quot;Correlated Time Series Forecasting using
Multi-Task Deep Neural Networks,&quot; to appear in ACM CIKM 2018, by providing
additional experimental results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cirstea_R/0/1/0/all/0/1&quot;&gt;Razvan-Gabriel Cirstea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micu_D/0/1/0/all/0/1&quot;&gt;Darius-Valer Micu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muresan_G/0/1/0/all/0/1&quot;&gt;Gabriel-Marcel Muresan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chenjuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09802">
<title>Modelling Irregular Spatial Patterns using Graph Convolutional Neural Networks. (arXiv:1808.09802v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.09802</link>
<description rdf:parseType="Literal">&lt;p&gt;The understanding of geographical reality is a process of data representation
and pattern discovery. Former studies mainly adopted continuous-field models to
represent spatial variables and to investigate the underlying spatial
continuity/heterogeneity in the regular spatial domain. In this article, we
introduce a more generalized model based on graph convolutional neural networks
(GCNs) that can capture the complex parameters of spatial patterns underlying
graph-structured spatial data, which generally contain both Euclidean spatial
information and non-Euclidean feature information. A trainable semi-supervised
prediction framework is proposed to model the spatial distribution patterns of
intra-urban points of interest(POI) check-ins. This work demonstrates the
feasibility of GCNs in complex geographic decision problems and provides a
promising tool to analyze irregular spatial data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Di Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09856">
<title>Application of Machine Learning in Rock Facies Classification with Physics-Motivated Feature Augmentation. (arXiv:1808.09856v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.09856</link>
<description rdf:parseType="Literal">&lt;p&gt;With recent progress in algorithms and the availability of massive amounts of
computation power, application of machine learning techniques is becoming a hot
topic in the oil and gas industry. One of the most promising aspects to apply
machine learning to the upstream field is the rock facies classification in
reservoir characterization, which is crucial in determining the net pay
thickness of reservoirs, thus a definitive factor in drilling decision making
process. For complex machine learning tasks like facies classification, feature
engineering is often critical. This paper shows the inclusion of
physics-motivated feature interaction in feature augmentation can further
improve the capability of machine learning in rock facies classification. We
demonstrate this approach with the SEG 2016 machine learning contest dataset
and the top winning algorithms. The improvement is roboust and can be $\sim5\%$
better than current existing best F-1 score, where F-1 is an evaluation metric
used to quantify average prediction accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yu Zeng&lt;/a&gt; (Corresponding author)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09897">
<title>Towards security defect prediction with AI. (arXiv:1808.09897v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1808.09897</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we investigate the limits of the current state of the art AI
system for detecting buffer overflows and compare it with current static
analysis tools. To do so, we developed a code generator, s-bAbI, capable of
producing an arbitrarily large number of code samples of controlled complexity.
We found that the static analysis engines we examined have good precision, but
poor recall on this dataset, except for a sound static analyzer that has good
precision and recall. We found that the state of the art AI system, a memory
network modeled after Choi et al. [1], can achieve similar performance to the
static analysis engines, but requires an exhaustive amount of training data in
order to do so. Our work points towards future approaches that may solve these
problems; namely, using representations of code that can capture appropriate
scope information and using deep learning methods that are able to perform
arithmetic operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sestili_C/0/1/0/all/0/1&quot;&gt;Carson D. Sestili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snavely_W/0/1/0/all/0/1&quot;&gt;William S. Snavely&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+VanHoudnos_N/0/1/0/all/0/1&quot;&gt;Nathan M. VanHoudnos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09920">
<title>Question Answering by Reasoning Across Documents with Graph Convolutional Networks. (arXiv:1808.09920v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.09920</link>
<description rdf:parseType="Literal">&lt;p&gt;Most research in reading comprehension has focused on answering questions
based on individual documents or even single paragraphs. We introduce a method
which integrates and reasons relying on information spread within documents and
across multiple documents. We frame it as an inference problem on a graph.
Mentions of entities are nodes of this graph where edges encode relations
between different mentions (e.g., within- and cross-document co-references).
Graph convolutional networks (GCNs) are applied to these graphs and trained to
perform multi-step reasoning. Our Entity-GCN method is scalable and compact,
and it achieves state-of-the-art results on the WikiHop dataset (Welbl et al.
2017).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1&quot;&gt;Nicola De Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1&quot;&gt;Wilker Aziz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1&quot;&gt;Ivan Titov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08012">
<title>Learning Topic Models by Neighborhood Aggregation. (arXiv:1802.08012v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08012</link>
<description rdf:parseType="Literal">&lt;p&gt;Topic models are frequently used in machine learning owing to their high
interpretability and modular structure. However, extending a topic model to
include a supervisory signal, to incorporate pre-trained word embedding vectors
and to include a nonlinear output function is not an easy task because one has
to resort to a highly intricate approximate inference procedure. The present
paper shows that topic modeling with pre-trained word embedding vectors can be
viewed as implementing a neighborhood aggregation algorithm where messages are
passed through a network defined over words. From the network view of topic
models, nodes correspond to words in a document and edges correspond to either
a relationship describing co-occurring words in a document or a relationship
describing the same word in the corpus. The network view allows us to extend
the model to include supervisory signals, incorporate pre-trained word
embedding vectors and include a nonlinear output function in a simple manner.
In experiments, we show that our approach outperforms the state-of-the-art
supervised Latent Dirichlet Allocation implementation in terms of both held-out
document classification tasks and topic coherence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hisano_R/0/1/0/all/0/1&quot;&gt;Ryohei Hisano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04566">
<title>Latent Geometry Inspired Graph Dissimilarities Enhance Affinity Propagation Community Detection in Complex Networks. (arXiv:1804.04566v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04566</link>
<description rdf:parseType="Literal">&lt;p&gt;Affinity propagation is one of the most effective unsupervised pattern
recognition algorithms for data clustering in high-dimensional feature space.
However, the numerous attempts to test its performance for community detection
in complex networks have been attaining results very far from the state of the
art methods such as Infomap and Louvain. Yet, all these studies agreed that the
crucial problem is to convert the unweighted network topology in a
&apos;smart-enough&apos; node dissimilarity matrix that is able to properly address the
message passing procedure behind affinity propagation clustering. Here we
introduce a conceptual innovation and we discuss how to leverage network latent
geometry notions in order to design dissimilarity matrices for affinity
propagation community detection. Our results demonstrate that the latent
geometry inspired dissimilarity measures we design bring affinity propagation
to equal or outperform current state of the art methods for community
detection. These findings are solidly proven considering both synthetic
&apos;realistic&apos; networks (with known ground-truth communities) and real networks
(with community metadata), even when the data structure is corrupted by noise
artificially induced by missing or spurious connectivity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cannistraci_C/0/1/0/all/0/1&quot;&gt;Carlo Vittorio Cannistraci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muscoloni_A/0/1/0/all/0/1&quot;&gt;Alessandro Muscoloni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07746">
<title>Network Reconstruction and Controlling Based on Structural Regularity Analysis. (arXiv:1805.07746v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07746</link>
<description rdf:parseType="Literal">&lt;p&gt;From the perspective of network analysis, the ubiquitous networks are
comprised of regular and irregular components, which makes uncovering the
complexity of network structures to be a fundamental challenge. Exploring the
regular information and identifying the roles of microscopic elements in
network data can help us recognize the principle of network organization and
contribute to network data utilization. However, the intrinsic structural
properties of networks remain so far inadequately explored and theorised. With
the realistic assumption that there are consistent features across the local
structures of networks, we propose a low-rank pursuit based self-representation
network model, in which the principle of network organization can be uncovered
by a representation matrix. According to this model, original true networks can
be reconstructed based on the observed unreliable network topology. In
particular, the proposed model enables us to estimate the extent to which the
networks are regulable, i.e., measuring the reconstructability of networks. In
addition, the model is capable of measuring the importance of microscopic
network elements, i.e., nodes and links, in terms of network regularity thereby
allowing us to regulate the reconstructability of networks based on them.
Extensive experiments on disparate real-world networks demonstrate the
effectiveness of the proposed network reconstruction and regulation algorithm.
Specifically, the network regularity metric can reflect the reconstructability
of networks, and the reconstruction accuracy can be improved by removing
irregular network links. Lastly, our approach provides an unique and novel
insight into the organization exploring of complex networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Shaojie Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_X/0/1/0/all/0/1&quot;&gt;Xingping Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xi-Zhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanbing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12317">
<title>Multiaccuracy: Black-Box Post-Processing for Fairness in Classification. (arXiv:1805.12317v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.12317</link>
<description rdf:parseType="Literal">&lt;p&gt;Prediction systems are successfully deployed in applications ranging from
disease diagnosis, to predicting credit worthiness, to image recognition. Even
when the overall accuracy is high, these systems may exhibit systematic biases
that harm specific subpopulations; such biases may arise inadvertently due to
underrepresentation in the data used to train a machine-learning model, or as
the result of intentional malicious discrimination. We develop a rigorous
framework of *multiaccuracy* auditing and post-processing to ensure accurate
predictions across *identifiable subgroups*.
&lt;/p&gt;
&lt;p&gt;Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have
black-box access to a predictor and a relatively small set of labeled data for
auditing; importantly, this black-box framework allows for improved fairness
and accountability of predictions, even when the predictor is minimally
transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show
that if the initial model is accurate on an identifiable subgroup, then the
post-processed model will be also. We experimentally demonstrate the
effectiveness of the approach to improve the accuracy among minority subgroups
in diverse applications (image classification, finance, population health).
Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for
&quot;black women&quot;) even when the sensitive features (e.g. &quot;race&quot;, &quot;gender&quot;) are not
given to the algorithm explicitly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Michael P. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorbani_A/0/1/0/all/0/1&quot;&gt;Amirata Ghorbani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05689">
<title>Graph Edit Distance Computation via Graph Neural Networks. (arXiv:1808.05689v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.05689</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph similarity search is among the most important graph-based applications,
e.g. finding the chemical compounds that are most similar to a query compound.
Graph similarity/distance computation, such as Graph Edit Distance (GED) and
Maximum Common Subgraph (MCS), is the core operation of graph similarity search
and many other applications, which is usually very costly to compute. Inspired
by the recent success of neural network approaches to several graph
applications, such as node classification and graph classification, we propose
a novel neural network-based approach to address this challenging while
classical graph problem, with the hope to alleviate the computational burden
while preserving a good performance. Our model generalizes to unseen graphs,
and in the worst case runs in linear time with respect to the number of nodes
in two graphs. Taking GED computation as an example, experimental results on
three real graph datasets demonstrate the effectiveness and efficiency of our
approach. Specifically, our model achieves smaller error and great time
reduction compared against several approximate algorithms on GED computation.
To the best of our knowledge, we are among the first to adopt neural networks
to model the similarity between two graphs, and provide a new direction for
future research on graph similarity computation and graph similarity search.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yunsheng Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Song Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Ting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yizhou Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07226">
<title>Mean-field approximation, convex hierarchies, and the optimality of correlation rounding: a unified perspective. (arXiv:1808.07226v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07226</link>
<description rdf:parseType="Literal">&lt;p&gt;The free energy is a key quantity of interest in Ising models, but
unfortunately, computing it in general is computationally intractable. Two
popular (variational) approximation schemes for estimating the free energy of
general Ising models (in particular, even in regimes where correlation decay
does not hold) are: (i) the mean-field approximation with roots in statistical
physics, which estimates the free energy from below, and (ii) hierarchies of
convex relaxations with roots in theoretical computer science, which estimate
the free energy from above. We show, surprisingly, that the tight regime for
both methods to compute the free energy to leading order is identical.
&lt;/p&gt;
&lt;p&gt;More precisely, we show that the mean-field approximation is within
$O((n\|J\|_{F})^{2/3})$ of the free energy, where $\|J\|_F$ denotes the
Frobenius norm of the interaction matrix of the Ising model. This
simultaneously subsumes both the breakthrough work of Basak and Mukherjee, who
showed the tight result that the mean-field approximation is within $o(n)$
whenever $\|J\|_{F} = o(\sqrt{n})$, as well as the work of Jain, Koehler, and
Mossel, who gave the previously best known non-asymptotic bound of
$O((n\|J\|_{F})^{2/3}\log^{1/3}(n\|J\|_{F}))$. We give a simple, algorithmic
proof of this result using a convex relaxation proposed by Risteski based on
the Sherali-Adams hierarchy, automatically giving sub-exponential time
approximation schemes for the free energy in this entire regime. Our
algorithmic result is tight under Gap-ETH.
&lt;/p&gt;
&lt;p&gt;We furthermore combine our techniques with spin glass theory to prove (in a
strong sense) the optimality of correlation rounding, refuting a recent
conjecture of Allen, O&apos;Donnell, and Zhou. Finally, we give the tight
generalization of all of these results to $k$-MRFs, capturing as a special case
previous work on approximating MAX-$k$-CSP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vishesh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1&quot;&gt;Frederic Koehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1&quot;&gt;Andrej Risteski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07380">
<title>On the Predictability of non-CGM Diabetes Data for Personalized Recommendation. (arXiv:1808.07380v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07380</link>
<description rdf:parseType="Literal">&lt;p&gt;With continuous glucose monitoring (CGM), data-driven models on blood glucose
prediction have been shown to be effective in related work. However, such (CGM)
systems are not always available, e.g., for a patient at home. In this work, we
conduct a study on 9 patients and examine the predictability of data-driven
(aka. machine learning) based models on patient-level blood glucose prediction;
with measurements are taken only periodically (i.e., after several hours). To
this end, we propose several post-prediction methods to account for the noise
nature of these data, that marginally improves the performance of the end
system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rokicki_M/0/1/0/all/0/1&quot;&gt;Markus Rokicki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08270">
<title>Building a Robust Text Classifier on a Test-Time Budget. (arXiv:1808.08270v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.08270</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a generic and interpretable learning framework for building robust
text classification model that achieves accuracy comparable to full models
under test-time budget constraints. Our approach learns a selector to identify
words that are relevant to the prediction tasks and passes them to the
classifier for processing. The selector is trained jointly with the classifier
and directly learns to incorporate with the classifier. We further propose a
data aggregation scheme to improve the robustness of the classifier. Our
learning framework is general and can be incorporated with any type of text
classification model. On real-world data, we show that the proposed approach
improves the performance of a given classifier and speeds up the model with a
mere loss in accuracy performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1&quot;&gt;Md Rizwan Parvez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1&quot;&gt;Tolga Bolukbasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_k/0/1/0/all/0/1&quot;&gt;kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1&quot;&gt;Venkatesh Saligrama&lt;/a&gt;</dc:creator>
</item></rdf:RDF>