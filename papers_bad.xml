<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-13T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04142"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04152"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04217"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04258"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03615"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04107"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04156"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04201"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04212"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04220"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04396"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04419"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.07685"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00455"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04837"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02896"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04246"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04437"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.02412"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.03020"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03553"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07495"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05398"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.01334"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00104"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03504"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.04142">
<title>Towards Budget-Driven Hardware Optimization for Deep Convolutional Neural Networks using Stochastic Computing. (arXiv:1805.04142v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.04142</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Deep Convolutional Neural Network (DCNN) has achieved tremendous
success in many machine learning applications. Nevertheless, the deep structure
has brought significant increases in computation complexity. Largescale deep
learning systems mainly operate in high-performance server clusters, thus
restricting the application extensions to personal or mobile devices. Previous
works on GPU and/or FPGA acceleration for DCNNs show increasing speedup, but
ignore other constraints, such as area, power, and energy. Stochastic Computing
(SC), as a unique data representation and processing technique, has the
potential to enable the design of fully parallel and scalable hardware
implementations of large-scale deep learning systems. This paper proposed an
automatic design allocation algorithm driven by budget requirement considering
overall accuracy performance. This systematic method enables the automatic
design of a DCNN where all design parameters are jointly optimized.
Experimental results demonstrate that proposed algorithm can achieve a joint
optimization of all design parameters given the comprehensive budget of a DCNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Ji Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_A/0/1/0/all/0/1&quot;&gt;Ao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Caiwen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Draper_J/0/1/0/all/0/1&quot;&gt;Jeffrey Draper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qinru Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1&quot;&gt;Bo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04152">
<title>Training Recurrent Neural Networks via Dynamical Trajectory-Based Optimization. (arXiv:1805.04152v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1805.04152</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new method to train recurrent neural networks using
dynamical trajectory-based optimization. The optimization method utilizes a
projected gradient system (PGS) and a quotient gradient system (QGS) to
determine the feasible regions of an optimization problem and search the
feasible regions for local minima. By exploring the feasible regions, local
minima are identified and the local minimum with the lowest cost is chosen as
the global minimum of the optimization problem. Lyapunov theory is used to
prove the stability of the local minima and their stability in the presence of
measurement errors. Numerical examples show that the new approach provides
better results than genetic algorithm and error backpropagation (EBP) trained
networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khodabandehlou_H/0/1/0/all/0/1&quot;&gt;Hamid Khodabandehlou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fadali_M/0/1/0/all/0/1&quot;&gt;M. Sami Fadali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04217">
<title>An Adaptive Population Size Differential Evolution with Novel Mutation Strategy for Constrained Optimization. (arXiv:1805.04217v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.04217</link>
<description rdf:parseType="Literal">&lt;p&gt;Differential evolution (DE) has competitive performance on constrained
optimization problems (COPs), which targets at searching for global optimal
solution without violating the constraints. Generally, researchers pay more
attention on avoiding violating the constraints than better objective function
value. To achieve the aim of searching the feasible solutions accurately, an
adaptive population size method and an adaptive mutation strategy are proposed
in the paper. The adaptive population method is similar to a state switch which
controls the exploring state and exploiting state according to the situation of
feasible solution search. The novel mutation strategy is designed to enhance
the effect of status switch based on adaptive population size, which is useful
to reduce the constraint violations. Moreover, a mechanism based on
multipopulation competition and a more precise method of constraint control are
adopted in the proposed algorithm. The proposed differential evolution
algorithm, APDE-NS, is evaluated on the benchmark problems from CEC2017
constrained real parameter optimization. The experimental results show the
effectiveness of the proposed method is competitive compared to other
state-of-the-art algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yuan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Meng-Zhu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04258">
<title>PALM: An Incremental Construction of Hyperplanes for Data Stream Regression. (arXiv:1805.04258v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.04258</link>
<description rdf:parseType="Literal">&lt;p&gt;Data stream has been the underlying challenge in the age of big data because
it calls for real-time data processing with the absence of a retraining process
and/or an iterative learning approach. In realm of fuzzy system community, data
stream is handled by algorithmic development of self-adaptive neurofuzzy
systems (SANFS) characterized by the single-pass learning mode and the open
structure property which enables effective handling of fast and rapidly
changing natures of data streams. The underlying bottleneck of SANFSs lies in
its design principle which involves a high number of free parameters (rule
premise and rule consequent) to be adapted in the training process. This figure
can even double in the case of type-2 fuzzy system. In this work, a novel
SANFS, namely parsimonious learning machine (PALM), is proposed. PALM features
utilization of a new type of fuzzy rule based on the concept of hyperplane
clustering which significantly reduces the number of network parameters because
it has no rule premise parameters. PALM is proposed in both type-1 and type-2
fuzzy systems where all of which characterize a fully dynamic rule-based
system. That is, it is capable of automatically generating, merging and tuning
the hyperplane based fuzzy rule in the single pass manner. The efficacy of PALM
has been evaluated through numerical study with six real-world and synthetic
data streams from public database and our own real-world project of autonomous
vehicles. The proposed model showcases significant improvements in terms of
computational complexity and number of required parameters against several
renowned SANFSs, while attaining comparable and often better predictive
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1&quot;&gt;Mahardhika Pratama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferdaus_M/0/1/0/all/0/1&quot;&gt;MD. Meftahul Ferdaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anavatti_S/0/1/0/all/0/1&quot;&gt;Sreenatha G. Anavatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garratt_M/0/1/0/all/0/1&quot;&gt;Matthew A. Garratt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03615">
<title>The Power of Genetic Algorithms: what remains of the pMSSM?. (arXiv:1805.03615v1 [hep-ph] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1805.03615</link>
<description rdf:parseType="Literal">&lt;p&gt;Genetic Algorithms (GAs) are explored as a tool for probing new physics with
high dimensionality. We study the 19-dimensional pMSSM, including experimental
constraints from all sources and assessing the consistency of potential signals
of new physics. We show that GAs excel at making a fast and accurate diagnosis
of the cross-compatibility of a set of experimental constraints in such high
dimensional models. In the case of the pMSSM, it is found that only ${\cal
O}(10^4)$ model evaluations are required to obtain a best fit point in
agreement with much more costly MCMC scans. This efficiency allows higher
dimensional models to be falsified, and patterns in the spectrum identified,
orders of magnitude more quickly. As examples of falsification, we consider the
muon anomalous magnetic moment, and the Galactic Centre gamma-ray excess
observed by Fermi-LAT, which could in principle be explained in terms of
neutralino dark matter. We show that both observables cannot be explained
within the pMSSM, and that they provide the leading contribution to the total
goodness of the fit, with $\chi^2_{\delta a_\mu^{\mathrm{SUSY}}}\approx12$ and
$\chi^2_{\rm GCE}\approx 155$, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Abel_S/0/1/0/all/0/1&quot;&gt;Steven Abel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Cerdeno_D/0/1/0/all/0/1&quot;&gt;David G. Cerdeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Robles_S/0/1/0/all/0/1&quot;&gt;Sandra Robles&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04107">
<title>Text-mining and ontologies: new approaches to knowledge discovery of microbial diversity. (arXiv:1805.04107v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1805.04107</link>
<description rdf:parseType="Literal">&lt;p&gt;Microbiology research has access to a very large amount of public information
on the habitats of microorganisms. Many areas of microbiology research uses
this information, primarily in biodiversity studies. However the habitat
information is expressed in unstructured natural language form, which hinders
its exploitation at large-scale. It is very common for similar habitats to be
described by different terms, which makes them hard to compare automatically,
e.g. intestine and gut. The use of a common reference to standardize these
habitat descriptions as claimed by (Ivana et al., 2010) is a necessity. We
propose the ontology called OntoBiotope that we have been developing since
2010. The OntoBiotope ontology is in a formal machine-readable representation
that enables indexing of information as well as conceptualization and
reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nedellec_C/0/1/0/all/0/1&quot;&gt;Claire N&amp;#xe9;dellec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bossy_R/0/1/0/all/0/1&quot;&gt;Robert Bossy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chaix_E/0/1/0/all/0/1&quot;&gt;Estelle Chaix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Deleger_L/0/1/0/all/0/1&quot;&gt;Louise Del&amp;#xe9;ger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04156">
<title>Computational Social Choice Meets Databases. (arXiv:1805.04156v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1805.04156</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a novel framework that aims to create bridges between the
computational social choice and the database management communities. This
framework enriches the tasks currently supported in computational social choice
with relational database context, thus making it possible to formulate
sophisticated queries about voting rules, candidates, voters, issues, and
positions. At the conceptual level, we give rigorous semantics to queries in
this framework by introducing the notions of necessary answers and possible
answers to queries. At the technical level, we embark on an investigation of
the computational complexity of the necessary answers. We establish a number of
results about the complexity of the necessary answers of conjunctive queries
involving positional scoring rules that contrast sharply with earlier results
about the complexity of the necessary winners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kimelfeld_B/0/1/0/all/0/1&quot;&gt;Benny Kimelfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolaitis_P/0/1/0/all/0/1&quot;&gt;Phokion G. Kolaitis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanovich_J/0/1/0/all/0/1&quot;&gt;Julia Stoyanovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04201">
<title>Learning to Grasp Without Seeing. (arXiv:1805.04201v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1805.04201</link>
<description rdf:parseType="Literal">&lt;p&gt;Can a robot grasp an unknown object without seeing it? In this paper, we
present a tactile-sensing based approach to this challenging problem of
grasping novel objects without prior knowledge of their location or physical
properties. Our key idea is to combine touch based object localization with
tactile based re-grasping. To train our learning models, we created a
large-scale grasping dataset, including more than 30 RGB frames and over 2.8
million tactile samples from 7800 grasp interactions of 52 objects. To learn a
representation of tactile signals, we propose an unsupervised auto-encoding
scheme, which shows a significant improvement of 4-9% over prior methods on a
variety of tactile perception tasks. Our system consists of two steps. First,
our touch localization model sequentially &apos;touch-scans&apos; the workspace and uses
a particle filter to aggregate beliefs from multiple hits of the target. It
outputs an estimate of the object&apos;s location, from which an initial grasp is
established. Next, our re-grasping model learns to progressively improve grasps
with tactile feedback based on the learned features. This network learns to
estimate grasp stability and predict adjustment for the next grasp. Re-grasping
thus is performed iteratively until our model identifies a stable grasp.
Finally, we demonstrate extensive experimental results on grasping a large set
of novel objects using tactile sensing alone. Furthermore, when applied on top
of a vision-based policy, our re-grasping model significantly boosts the
overall accuracy by 10.6%. We believe this is the first attempt at learning to
grasp with only tactile sensing and without any prior object knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murali_A/0/1/0/all/0/1&quot;&gt;Adithyavairavan Murali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandhi_D/0/1/0/all/0/1&quot;&gt;Dhiraj Gandhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhinav Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04212">
<title>Behavior Analysis of NLI Models: Uncovering the Influence of Three Factors on Robustness. (arXiv:1805.04212v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.04212</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural Language Inference is a challenging task that has received
substantial attention, and state-of-the-art models now achieve impressive test
set performance in the form of accuracy scores. Here, we go beyond this single
evaluation metric to examine robustness to semantically-valid alterations to
the input data. We identify three factors - insensitivity, polarity and unseen
pairs - and compare their impact on three SNLI models under a variety of
conditions. Our results demonstrate a number of strengths and weaknesses in the
models&apos; ability to generalise to new in-domain instances. In particular, while
strong performance is possible on unseen hypernyms, unseen antonyms are more
challenging for all the models. More generally, the models suffer from an
insensitivity to certain small but semantically significant alterations, and
are also often influenced by simple statistical correlations between words and
training labels. Overall, we show that evaluations of NLI models can benefit
from studying the influence of factors intrinsic to the models or found in the
dataset used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmona_V/0/1/0/all/0/1&quot;&gt;Vicente Ivan Sanchez Carmona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_J/0/1/0/all/0/1&quot;&gt;Jeff Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1&quot;&gt;Sebastian Riedel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04220">
<title>Human-Machine Collaborative Optimization via Apprenticeship Scheduling. (arXiv:1805.04220v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.04220</link>
<description rdf:parseType="Literal">&lt;p&gt;Coordinating agents to complete a set of tasks with intercoupled temporal and
resource constraints is computationally challenging, yet human domain experts
can solve these difficult scheduling problems using paradigms learned through
years of apprenticeship. A process for manually codifying this domain knowledge
within a computational framework is necessary to scale beyond the
``single-expert, single-trainee&quot; apprenticeship model. However, human domain
experts often have difficulty describing their decision-making processes,
causing the codification of this knowledge to become laborious. We propose a
new approach for capturing domain-expert heuristics through a pairwise ranking
formulation. Our approach is model-free and does not require enumerating or
iterating through a large state space. We empirically demonstrate that this
approach accurately learns multifaceted heuristics on a synthetic data set
incorporating job-shop scheduling and vehicle routing problems, as well as on
two real-world data sets consisting of demonstrations of experts solving a
weapon-to-target assignment problem and a hospital resource allocation problem.
We also demonstrate that policies learned from human scheduling demonstration
via apprenticeship learning can substantially improve the efficiency of a
branch-and-bound search for an optimal schedule. We employ this human-machine
collaborative optimization technique on a variant of the weapon-to-target
assignment problem. We demonstrate that this technique generates solutions
substantially superior to those produced by human domain experts at a rate up
to 9.5 times faster than an optimization approach and can be applied to
optimally solve problems twice as complex as those solved by a human
demonstrator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gombolay_M/0/1/0/all/0/1&quot;&gt;Matthew Gombolay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jensen_R/0/1/0/all/0/1&quot;&gt;Reed Jensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stigile_J/0/1/0/all/0/1&quot;&gt;Jessica Stigile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golen_T/0/1/0/all/0/1&quot;&gt;Toni Golen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Neel Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1&quot;&gt;Sung-Hyun Son&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1&quot;&gt;Julie Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04396">
<title>A Sensorimotor Perspective on Grounding the Semantic of Simple Visual Features. (arXiv:1805.04396v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1805.04396</link>
<description rdf:parseType="Literal">&lt;p&gt;In Machine Learning and Robotics, the semantic content of visual features is
usually provided to the system by a human who interprets its content. On the
contrary, strictly unsupervised approaches have difficulties relating the
statistics of sensory inputs to their semantic content without also relying on
prior knowledge introduced in the system. We proposed in this paper to tackle
this problem from a sensorimotor perspective. In line with the Sensorimotor
Contingencies Theory, we make the fundamental assumption that the semantic
content of sensory inputs at least partially stems from the way an agent can
actively transform it. We illustrate our approach by formalizing how simple
visual features can induce invariants in a naive agent&apos;s sensorimotor
experience, and evaluate it on a simple simulated visual system. Without any a
priori knowledge about the way its sensorimotor information is encoded, we show
how an agent can characterize the uniformity and edge-ness of the visual
features it interacts with.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laflaquiere_A/0/1/0/all/0/1&quot;&gt;Alban Laflaqui&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04419">
<title>Deep Hierarchical Reinforcement Learning Algorithm in Partially Observable Markov Decision Processes. (arXiv:1805.04419v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.04419</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, reinforcement learning has achieved many remarkable
successes due to the growing adoption of deep learning techniques and the rapid
growth in computing power. Nevertheless, it is well-known that flat
reinforcement learning algorithms are often not able to learn well and
data-efficient in tasks having hierarchical structures, e.g. consisting of
multiple subtasks. Hierarchical reinforcement learning is a principled approach
that is able to tackle these challenging tasks. On the other hand, many
real-world tasks usually have only partial observability in which state
measurements are often imperfect and partially observable. The problems of RL
in such settings can be formulated as a partially observable Markov decision
process (POMDP). In this paper, we study hierarchical RL in POMDP in which the
tasks have only partial observability and possess hierarchical properties. We
propose a hierarchical deep reinforcement learning approach for learning in
hierarchical POMDP. The deep hierarchical RL algorithm is proposed to apply to
both MDP and POMDP learning. We evaluate the proposed algorithm on various
challenging hierarchical POMDP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuyen_L/0/1/0/all/0/1&quot;&gt;Le Pham Tuyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1&quot;&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Layek_A/0/1/0/all/0/1&quot;&gt;Abu Layek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1&quot;&gt;TaeChoong Chung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.07685">
<title>KSR: A Semantic Representation of Knowledge Graph within a Novel Unsupervised Paradigm. (arXiv:1608.07685v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1608.07685</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge representation is a long-history topic in AI, which is very
important. A variety of models have been proposed for knowledge graph
embedding, which projects symbolic entities and relations into continuous
vector space. However, most related methods merely focus on the data-fitting of
knowledge graph, and ignore the interpretable semantic expression. Thus,
traditional embedding methods are not friendly for applications that require
semantic analysis, such as question answering and entity retrieval. To this
end, this paper proposes a semantic representation method for knowledge graph
\textbf{(KSR)}, which imposes a two-level hierarchical generative process that
globally extracts many aspects and then locally assigns a specific category in
each aspect for every triple. Since both aspects and categories are
semantics-relevant, the collection of categories in each aspect is treated as
the semantic representation of this triple. Extensive experiments show that our
model outperforms other state-of-the-art baselines substantially.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Han Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Minlie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00455">
<title>A Unified View of Piecewise Linear Neural Network Verification. (arXiv:1711.00455v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00455</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of Deep Learning and its potential use in many safety-critical
applications has motivated research on formal verification of Neural Network
(NN) models. Despite the reputation of learned NN models to behave as black
boxes and the theoretical hardness of proving their properties, researchers
have been successful in verifying some classes of models by exploiting their
piecewise linear structure. To facilitate progress on this crucial area, we
make two key contributions. First, we present a unified framework that
encompasses previous methods. This analysis results in the identification of
new methods that combine the strengths of multiple existing approaches. Second,
we propose a new data set of benchmarks which includes a collection of
previously released testcases. We use the benchmark to provide the first
experimental comparison of the algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunel_R/0/1/0/all/0/1&quot;&gt;Rudy Bunel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turkaslan_I/0/1/0/all/0/1&quot;&gt;Ilker Turkaslan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohli_P/0/1/0/all/0/1&quot;&gt;Pushmeet Kohli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1&quot;&gt;M. Pawan Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04837">
<title>Learning the Joint Representation of Heterogeneous Temporal Events for Clinical Endpoint Prediction. (arXiv:1803.04837v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04837</link>
<description rdf:parseType="Literal">&lt;p&gt;The availability of a large amount of electronic health records (EHR)
provides huge opportunities to improve health care service by mining these
data. One important application is clinical endpoint prediction, which aims to
predict whether a disease, a symptom or an abnormal lab test will happen in the
future according to patients&apos; history records. This paper develops deep
learning techniques for clinical endpoint prediction, which are effective in
many practical applications. However, the problem is very challenging since
patients&apos; history records contain multiple heterogeneous temporal events such
as lab tests, diagnosis, and drug administrations. The visiting patterns of
different types of events vary significantly, and there exist complex nonlinear
relationships between different events. In this paper, we propose a novel model
for learning the joint representation of heterogeneous temporal events. The
model adds a new gate to control the visiting rates of different events which
effectively models the irregular patterns of different events and their
nonlinear correlations. Experiment results with real-world clinical data on the
tasks of predicting death and abnormal lab tests prove the effectiveness of our
proposed approach over competitive baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Luchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jianhao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zichang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02896">
<title>Survey and cross-benchmark comparison of remaining time prediction methods in business process monitoring. (arXiv:1805.02896v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.02896</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive business process monitoring methods exploit historical process
execution logs to generate predictions about running instances (called cases)
of a business process, such as the prediction of the outcome, next activity or
remaining cycle time of a given process case. These insights could be used to
support operational managers in taking remedial actions as business processes
unfold, e.g. shifting resources from one case onto another to ensure this
latter is completed on time. A number of methods to tackle the remaining cycle
time prediction problem have been proposed in the literature. However, due to
differences in their experimental setup, choice of datasets, evaluation
measures and baselines, the relative merits of each method remain unclear. This
article presents a systematic literature review and taxonomy of methods for
remaining time prediction in the context of business processes, as well as a
cross-benchmark comparison of 16 such methods based on 16 real-life datasets
originating from different industry domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verenich_I/0/1/0/all/0/1&quot;&gt;Ilya Verenich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_M/0/1/0/all/0/1&quot;&gt;Marlon Dumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_M/0/1/0/all/0/1&quot;&gt;Marcello La Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Maggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teinemaa_I/0/1/0/all/0/1&quot;&gt;Irene Teinemaa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04193">
<title>An Unsupervised Clustering-Based Short-Term Solar Forecasting Methodology Using Multi-Model Machine Learning Blending. (arXiv:1805.04193v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04193</link>
<description rdf:parseType="Literal">&lt;p&gt;Solar forecasting accuracy is affected by weather conditions, and weather
awareness forecasting models are expected to improve the performance. However,
it may not be available and reliable to classify different forecasting tasks by
using only meteorological weather categorization. In this paper, an
unsupervised clustering-based (UC-based) solar forecasting methodology is
developed for short-term (1-hour-ahead) global horizontal irradiance (GHI)
forecasting. This methodology consists of three parts: GHI time series
unsupervised clustering, pattern recognition, and UC-based forecasting. The
daily GHI time series is first clustered by an Optimized Cross-validated
ClUsteRing (OCCUR) method, which determines the optimal number of clusters and
best clustering results. Then, support vector machine pattern recognition
(SVM-PR) is adopted to recognize the category of a certain day using the first
few hours&apos; data in the forecasting stage. GHI forecasts are generated by the
most suitable models in different clusters, which are built by a two-layer
Machine learning based Multi-Model (M3) forecasting framework. The developed
UC-based methodology is validated by using 1-year of data with six solar
features. Numerical results show that (i) UC-based models outperform non-UC
(all-in-one) models with the same M3 architecture by approximately 20%; (ii)
M3-based models also outperform the single-algorithm machine learning (SAML)
models by approximately 20%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Cong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1&quot;&gt;Mingjian Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hodge_B/0/1/0/all/0/1&quot;&gt;Bri-Mathias Hodge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Siyuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamann_H/0/1/0/all/0/1&quot;&gt;Hendrik F. Hamann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04246">
<title>Convex Programming Based Spectral Clustering. (arXiv:1805.04246v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04246</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering is a fundamental task in data analysis, and spectral clustering
has been recognized as a promising approach to it. Given a graph describing the
relationship between data, spectral clustering explores the underlying cluster
structure in two stages. The first stage embeds the nodes of the graph into
real space, and the second stage groups the embedded nodes into several
clusters. The use of the $k$-means method in the grouping stage is currently
standard practice. We present a spectral clustering algorithm that uses convex
programming in the grouping stage, and study how well it works. The concept
behind the algorithm design lies in the following observation. The nodes with
the largest degree in each cluster may be found by computing an enclosing
ellipsoid for embedded nodes in real space, and the clusters may be identified
by using those nodes. We show that the observations are valid, and the
algorithm returns clusters to provide the conductance of graph, if the gap
assumption, introduced by Peng el al. at COLT 2015, is satisfied. We also give
an experimental assessment of the algorithm&apos;s performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mizutani_T/0/1/0/all/0/1&quot;&gt;Tomohiko Mizutani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04437">
<title>Cross-lingual Document Retrieval using Regularized Wasserstein Distance. (arXiv:1805.04437v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.04437</link>
<description rdf:parseType="Literal">&lt;p&gt;Many information retrieval algorithms rely on the notion of a good distance
that allows to efficiently compare objects of different nature. Recently, a new
promising metric called Word Mover&apos;s Distance was proposed to measure the
divergence between text passages. In this paper, we demonstrate that this
metric can be extended to incorporate term-weighting schemes and provide more
accurate and computationally efficient matching between documents using
entropic regularization. We evaluate the benefits of both extensions in the
task of cross-lingual document retrieval (CLDR). Our experimental results on
eight CLDR problems suggest that the proposed methods achieve remarkable
improvements in terms of Mean Reciprocal Rank compared to several baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balikas_G/0/1/0/all/0/1&quot;&gt;Georgios Balikas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laclau_C/0/1/0/all/0/1&quot;&gt;Charlotte Laclau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redko_I/0/1/0/all/0/1&quot;&gt;Ievgen Redko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amini_M/0/1/0/all/0/1&quot;&gt;Massih-Reza Amini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.02412">
<title>A Robust Learning Algorithm for Regression Models Using Distributionally Robust Optimization under the Wasserstein Metric. (arXiv:1706.02412v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.02412</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a Distributionally Robust Optimization (DRO) approach to estimate
a robustified regression plane in a linear regression setting, when the
observed samples are potentially contaminated with adversarially corrupted
outliers. Our approach mitigates the impact of outliers through hedging against
a family of distributions on the observed data, some of which assign very low
probabilities to the outliers. The set of distributions under consideration are
close to the empirical distribution in the sense of the Wasserstein metric. We
show that this DRO formulation can be relaxed to a convex optimization problem
which encompasses a class of models. By selecting proper norm spaces for the
Wasserstein metric, we are able to recover several commonly used regularized
regression models. We provide new insights into the regularization term and
give guidance on the selection of the regularization coefficient from the
standpoint of a confidence region. We establish two types of performance
guarantees for the solution to our formulation under mild conditions. One is
related to its out-of-sample behavior (prediction bias), and the other concerns
the discrepancy between the estimated and true regression planes (estimation
bias). Extensive numerical results demonstrate the superiority of our approach
to a host of regression models, in terms of the prediction and estimation
accuracies. We also consider the application of our robust learning procedure
to outlier detection, and show that our approach achieves a much higher AUC
(Area Under the ROC Curve) than M-estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruidi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Paschalidis_I/0/1/0/all/0/1&quot;&gt;Ioannis Ch. Paschalidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.03020">
<title>Non-stationary Stochastic Optimization under $L_{p,q}$-Variation Measures. (arXiv:1708.03020v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.03020</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a non-stationary sequential stochastic optimization problem, in
which the underlying cost functions change over time under a variation budget
constraint. We propose an $L_{p,q}$-variation functional to quantify the
change, which yields less variation for dynamic function sequences whose
changes are constrained to short time periods or small subsets of input domain.
Under the $L_{p,q}$-variation constraint, we derive both upper and matching
lower regret bounds for smooth and strongly convex function sequences, which
generalize previous results in Besbes et al. (2015). Furthermore, we provide an
upper bound for general convex function sequences with noisy gradient feedback,
which matches the optimal rate as $p\to\infty$. Our results reveal some
surprising phenomena under this general variation functional, such as the curse
of dimensionality of the function domain. The key technical novelties in our
analysis include affinity lemmas that characterize the distance of the
minimizers of two convex functions with bounded Lp difference, and a cubic
spline based construction that attains matching lower bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yining Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03553">
<title>RNN-based counterfactual time-series prediction. (arXiv:1712.03553v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.03553</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes an alternative to the synthetic control method (SCM) for
estimating the effect of a policy intervention on an outcome over time.
Recurrent neural networks (RNNs) are used to predict counterfactual time-series
of treated unit outcomes using only the outcomes of control units as inputs.
Unlike SCM, the proposed method does not rely on pre-intervention covariates,
allows for nonconvex combinations of control units, can handle multiple treated
units, and can share model parameters across time-steps. RNNs outperform SCM in
terms of recovering experimental estimates from a field experiment extended to
a time-series observational setting. In placebo tests run on three different
benchmark datasets, RNNs are more accurate than SCM in predicting the
post-intervention time-series of control units, while yielding a comparable
proportion of false positives. The proposed method contributes to a new
literature that uses machine learning techniques for data-driven counterfactual
prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poulos_J/0/1/0/all/0/1&quot;&gt;Jason Poulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07495">
<title>A Distributed Frank-Wolfe Framework for Learning Low-Rank Matrices with the Trace Norm. (arXiv:1712.07495v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07495</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning a high-dimensional but low-rank matrix
from a large-scale dataset distributed over several machines, where
low-rankness is enforced by a convex trace norm constraint. We propose
DFW-Trace, a distributed Frank-Wolfe algorithm which leverages the low-rank
structure of its updates to achieve efficiency in time, memory and
communication usage. The step at the heart of DFW-Trace is solved approximately
using a distributed version of the power method. We provide a theoretical
analysis of the convergence of DFW-Trace, showing that we can ensure sublinear
convergence in expectation to an optimal solution with few power iterations per
epoch. We implement DFW-Trace in the Apache Spark distributed programming
framework and validate the usefulness of our approach on synthetic and real
data, including the ImageNet dataset with high-dimensional features extracted
from a deep neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wenjie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellet_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Bellet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1&quot;&gt;Patrick Gallinari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05398">
<title>On the Direction of Discrimination: An Information-Theoretic Analysis of Disparate Impact in Machine Learning. (arXiv:1801.05398v3 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1801.05398</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of machine learning, disparate impact refers to a form of
systematic discrimination whereby the output distribution of a model depends on
the value of a sensitive attribute (e.g., race or gender). In this paper, we
propose an information-theoretic framework to analyze the disparate impact of a
binary classification model. We view the model as a fixed channel, and quantify
disparate impact as the divergence in output distributions over two groups. Our
aim is to find a correction function that can perturb the input distributions
of each group to align their output distributions. We present an optimization
problem that can be solved to obtain a correction function that will make the
output distributions statistically indistinguishable. We derive closed-form
expressions to efficiently compute the correction function, and demonstrate the
benefits of our framework on a recidivism prediction problem based on the
ProPublica COMPAS dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ustun_B/0/1/0/all/0/1&quot;&gt;Berk Ustun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calmon_F/0/1/0/all/0/1&quot;&gt;Flavio P. Calmon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.01334">
<title>Information Assisted Dictionary Learning for fMRI data analysis. (arXiv:1802.01334v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.01334</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, the task-related fMRI problem is treated in its matrix
factorization formulation. The focus of the reported work is on the dictionary
learning (DL) matrix factorization approach. A major novelty of the paper lies
in the incorporation of well-established assumptions associated with the GLM
technique, which is currently in use by the neuroscientists. These assumptions
are embedded as constraints in the DL formulation. In this way, our approach
provides a framework of combining well-established and understood techniques
with a more ``modern&apos;&apos; and powerful tool. Furthermore, this paper offers a way
to relax a major drawback associated with DL techniques; that is, the proper
tuning of the DL regularization parameter. This parameter plays a critical role
in DL-based fMRI analysis since it essentially determines the shape and
structures of the estimated functional brain networks. However, in actual fMRI
data analysis, the lack of ground truth renders the a priori choice of the
regularization parameter a truly challenging task. Indeed, the values of the DL
regularization parameter, associated with the $\ell_1$ sparsity promoting norm,
do not convey any tangible physical meaning. So it is practically difficult to
guess its proper value. In this paper, the DL problem is reformulated around a
sparsity-promoting constraint that can directly be related to the minimum
amount of voxels that the spatial maps of the functional brain networks occupy.
Such information is documented and it is readily available to neuroscientists
and experts in the field.
&lt;/p&gt;
&lt;p&gt;The proposed method is tested against a number of other popular techniques
and the obtained performance gains are reported using a number of synthetic
fMRI data. Results with real data have also been obtained in the context of a
number of experiments and will be soon reported in a different publication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Morante_M/0/1/0/all/0/1&quot;&gt;Manuel Morante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kopsinis_Y/0/1/0/all/0/1&quot;&gt;Yannis Kopsinis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Theodoridis_S/0/1/0/all/0/1&quot;&gt;Sergios Theodoridis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00104">
<title>Learning Disentangled Joint Continuous and Discrete Representations. (arXiv:1804.00104v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00104</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a framework for learning disentangled and interpretable jointly
continuous and discrete representations in an unsupervised manner. By
augmenting the continuous latent distribution of variational autoencoders with
a relaxed discrete distribution and controlling the amount of information
encoded in each latent unit, we show how continuous and categorical factors of
variation can be discovered automatically from data. Experiments show that the
framework disentangles continuous and discrete generative factors on various
datasets and outperforms current disentangling methods when a discrete
generative factor is prominent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dupont_E/0/1/0/all/0/1&quot;&gt;Emilien Dupont&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03504">
<title>Diffusion Based Network Embedding. (arXiv:1805.03504v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.03504</link>
<description rdf:parseType="Literal">&lt;p&gt;In network embedding, random walks play a fundamental role in preserving
network structures. However, random walk based embedding methods have two
limitations. First, random walk methods are fragile when the sampling frequency
or the number of node sequences changes. Second, in disequilibrium networks
such as highly biases networks, random walk methods often perform poorly due to
the lack of global network information. In order to solve the limitations, we
propose in this paper a network diffusion based embedding method. To solve the
first limitation, our method employs a diffusion driven process to capture both
depth information and breadth information. The time dimension is also attached
to node sequences that can strengthen information preserving. To solve the
second limitation, our method uses the network inference technique based on
cascades to capture the global network information. To verify the performance,
we conduct experiments on node classification tasks using the learned
representations. Results show that compared with random walk based methods,
diffusion based models are more robust when samplings under each node is rare.
We also conduct experiments on a highly imbalanced network. Results shows that
the proposed model are more robust under the biased network structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_M/0/1/0/all/0/1&quot;&gt;Minglong Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1&quot;&gt;Lingfeng Niu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>