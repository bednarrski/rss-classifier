<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00519"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07686"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07752"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07770"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07822"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07887"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07893"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07901"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04380"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06096"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07682"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07704"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07788"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07800"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07811"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07897"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08041"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08058"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08101"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08107"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.06067"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.06899"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04495"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06424"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07374"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.00519">
<title>An Elementary Analysis of the Probability That a Binomial Random Variable Exceeds Its Expectation. (arXiv:1712.00519v3 [math.PR] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00519</link>
<description rdf:parseType="Literal">&lt;p&gt;We give an elementary proof of the fact that a binomial random variable $X$
with parameters $n$ and $0.29/n \le p &amp;lt; 1$ with probability at least $1/4$
strictly exceeds its expectation. We also show that for $1/n \le p &amp;lt; 1 - 1/n$,
$X$ exceeds its expectation by more than one with probability at least
$0.0370$. Both probabilities approach $1/2$ when $np$ and $n(1-p)$ tend to
infinity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07686">
<title>Pseudorehearsal in actor-critic agents with neural network function approximation. (arXiv:1712.07686v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.07686</link>
<description rdf:parseType="Literal">&lt;p&gt;Catastrophic forgetting has a significant negative impact in reinforcement
learning. The purpose of this study is to investigate how pseudorehearsal can
change performance of an actor-critic agent with neural-network function
approximation. We tested agent in a pole balancing task and compared different
pseudorehearsal approaches. We have found that pseudorehearsal can assist
learning and decrease forgetting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marochko_V/0/1/0/all/0/1&quot;&gt;Vladimir Marochko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johard_L/0/1/0/all/0/1&quot;&gt;Leonard Johard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazzara_M/0/1/0/all/0/1&quot;&gt;Manuel Mazzara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Longo_L/0/1/0/all/0/1&quot;&gt;Luca Longo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07752">
<title>Towards an unanimous international regulatory body for responsible use of Artificial Intelligence [UIRB-AI]. (arXiv:1712.07752v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.07752</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI), is once again in the phase of drastic
advancements. Unarguably, the technology itself can revolutionize the way we
live our everyday life. But the exponential growth of technology poses a
daunting task for policy researchers and law makers in making amendments to the
existing norms. In addition, not everyone in the society is studying the
potential socio-economic intricacies and cultural drifts that AI can bring
about. It is prudence to reflect from our historical past to propel the
development of technology in the right direction. To benefit the society of the
present and future, I scientifically explore the societal impact of AI. While
there are many public and private partnerships working on similar aspects, here
I describe the necessity for an Unanimous International Regulatory Body for all
applications of AI (UIRB-AI). I also discuss the benefits and drawbacks of such
an organization. To combat any drawbacks in the formation of an UIRB-AI, both
idealistic and pragmatic perspectives are discussed alternatively. The paper
further advances the discussion by proposing novel policies on how such
organization should be structured and how it can bring about a win-win
situation for everyone in the society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chidambaram_R/0/1/0/all/0/1&quot;&gt;Rajesh Chidambaram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07770">
<title>Bit-Vector Model Counting using Statistical Estimation. (arXiv:1712.07770v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1712.07770</link>
<description rdf:parseType="Literal">&lt;p&gt;Approximate model counting for bit-vector SMT formulas (generalizing \#SAT)
has many applications such as probabilistic inference and quantitative
information-flow security, but it is computationally difficult. Adding random
parity constraints (XOR streamlining) and then checking satisfiability is an
effective approximation technique, but it requires a prior hypothesis about the
model count to produce useful results. We propose an approach inspired by
statistical estimation to continually refine a probabilistic estimate of the
model count for a formula, so that each XOR-streamlined query yields as much
information as possible. We implement this approach, with an approximate
probability model, as a wrapper around an off-the-shelf SMT solver or SAT
solver. Experimental results show that the implementation is faster than the
most similar previous approaches which used simpler refinement strategies. The
technique also lets us model count formulas over floating-point constraints,
which we demonstrate with an application to a vulnerability in differential
privacy mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seonmo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCamant_S/0/1/0/all/0/1&quot;&gt;Stephen McCamant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07822">
<title>Geometrical Insights for Implicit Generative Modeling. (arXiv:1712.07822v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.07822</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning algorithms for implicit generative models can optimize a variety of
criteria that measure how the data distribution differs from the implicit model
distribution, including the Wasserstein distance, the Energy distance, and the
Maximum Mean Discrepancy criterion. A careful look at the geometries induced by
these distances on the space of probability measures reveals interesting
differences. In particular, we can establish surprising approximate global
convergence guarantees for the $1$-Wasserstein distance,even when the
parametric generator has a nonconvex parametrization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bottou_L/0/1/0/all/0/1&quot;&gt;Leon Bottou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arjovsky_M/0/1/0/all/0/1&quot;&gt;Martin Arjovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lopez_Paz_D/0/1/0/all/0/1&quot;&gt;David Lopez-Paz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oquab_M/0/1/0/all/0/1&quot;&gt;Maxime Oquab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07887">
<title>Multiagent-based Participatory Urban Simulation through Inverse Reinforcement Learning. (arXiv:1712.07887v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1712.07887</link>
<description rdf:parseType="Literal">&lt;p&gt;The multiagent-based participatory simulation features prominently in urban
planning as the acquired model is considered as the hybrid system of the domain
and the local knowledge. However, the key problem of generating realistic
agents for particular social phenomena invariably remains. The existing models
have attempted to dictate the factors involving human behavior, which appeared
to be intractable. In this paper, Inverse Reinforcement Learning (IRL) is
introduced to address this problem. IRL is developed for computational modeling
of human behavior and has achieved great successes in robotics, psychology and
machine learning. The possibilities presented by this new style of modeling are
drawn out as conclusions, and the relative challenges with this modeling are
highlighted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suzuki_S/0/1/0/all/0/1&quot;&gt;Soma Suzuki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07893">
<title>A Deep Policy Inference Q-Network for Multi-Agent Systems. (arXiv:1712.07893v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.07893</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DPIQN, a deep policy inference Q-network that targets multi-agent
systems composed of controllable agents, collaborators, and opponents that
interact with each other. We focus on one challenging issue in such
systems---modeling agents with varying strategies---and propose to employ
&quot;policy features&quot; learned from raw observations (e.g., raw images) of
collaborators and opponents by inferring their policies. DPIQN incorporates the
learned policy features as a hidden vector into its own deep Q-network (DQN),
such that it is able to predict better Q values for the controllable agents
than the state-of-the-art deep reinforcement learning models. We further
propose an enhanced version of DPIQN, called deep recurrent policy inference
Q-network (DRPIQN), for handling partial observability. Both DPIQN and DRPIQN
are trained by an adaptive training procedure, which adjusts the network&apos;s
attention to learn the policy features and its own Q-values at different phases
of the training process. We present a comprehensive analysis of DPIQN and
DRPIQN, and highlight their effectiveness and generalizability in various
multi-agent settings. Our models are evaluated in a classic soccer game
involving both competitive and collaborative scenarios. Experimental results
performed on 1 vs. 1 and 2 vs. 2 games show that DPIQN and DRPIQN demonstrate
superior performance to the baseline DQN and deep recurrent Q-network (DRQN)
models. We also explore scenarios in which collaborators or opponents
dynamically change their policies, and show that DPIQN and DRPIQN do lead to
better overall performance in terms of stability and mean scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1&quot;&gt;Zhang-Wei Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shih-Yang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shann_T/0/1/0/all/0/1&quot;&gt;Tzu-Yun Shann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yi-Hsiang Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chun-Yi Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07901">
<title>Improvements to Inference Compilation for Probabilistic Programming in Large-Scale Scientific Simulators. (arXiv:1712.07901v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.07901</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of Bayesian inference in the family of probabilistic
models implicitly defined by stochastic generative models of data. In
scientific fields ranging from population biology to cosmology, low-level
mechanistic components are composed to create complex generative models. These
models lead to intractable likelihoods and are typically non-differentiable,
which poses challenges for traditional approaches to inference. We extend
previous work in &quot;inference compilation&quot;, which combines universal
probabilistic programming and deep learning methods, to large-scale scientific
simulators, and introduce a C++ based probabilistic programming library called
CPProb. We successfully use CPProb to interface with SHERPA, a large code-base
used in particle physics. Here we describe the technical innovations realized
and planned for this library.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casado_M/0/1/0/all/0/1&quot;&gt;Mario Lezcano Casado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baydin_A/0/1/0/all/0/1&quot;&gt;Atilim Gunes Baydin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubio_D/0/1/0/all/0/1&quot;&gt;David Martinez Rubio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Tuan Anh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1&quot;&gt;Frank Wood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinrich_L/0/1/0/all/0/1&quot;&gt;Lukas Heinrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louppe_G/0/1/0/all/0/1&quot;&gt;Gilles Louppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cranmer_K/0/1/0/all/0/1&quot;&gt;Kyle Cranmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1&quot;&gt;Karen Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhimji_W/0/1/0/all/0/1&quot;&gt;Wahid Bhimji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhat/0/1/0/all/0/1&quot;&gt;Prabhat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04380">
<title>Neural Network Based Nonlinear Weighted Finite Automata. (arXiv:1709.04380v2 [cs.FL] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04380</link>
<description rdf:parseType="Literal">&lt;p&gt;Weighted finite automata (WFA) can expressively model functions defined over
strings but are inherently linear models. Given the recent successes of
nonlinear models in machine learning, it is natural to wonder whether
ex-tending WFA to the nonlinear setting would be beneficial. In this paper, we
propose a novel model of neural network based nonlinearWFA model (NL-WFA) along
with a learning algorithm. Our learning algorithm is inspired by the spectral
learning algorithm for WFAand relies on a nonlinear decomposition of the
so-called Hankel matrix, by means of an auto-encoder network. The expressive
power of NL-WFA and the proposed learning algorithm are assessed on both
synthetic and real-world data, showing that NL-WFA can lead to smaller model
sizes and infer complex grammatical structures from data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1&quot;&gt;Guillaume Rabusseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06096">
<title>Deep Learning in RF Sub-sampled B-mode Ultrasound Imaging. (arXiv:1712.06096v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06096</link>
<description rdf:parseType="Literal">&lt;p&gt;In portable, three dimensional, and ultra-fast ultrasound (US) imaging
systems, there is an increasing need to reconstruct high quality images from a
limited number of RF data from receiver (Rx) or scan-line (SC) sub-sampling.
However, due to the severe side lobe artifacts from RF sub-sampling, the
standard beam-former often produces blurry images with less contrast that are
not suitable for diagnostic purpose. To address this problem, some researchers
have studied compressed sensing (CS) to exploit the sparsity of the image or RF
data in some domains. However, the existing CS approaches require either
hardware changes or computationally expensive algorithms. To overcome these
limitations, here we propose a novel deep learning approach that directly
interpolates the missing RF data by utilizing redundancy in the Rx-SC plane. In
particular, the network design principle derives from a novel interpretation of
the deep neural network as a cascaded convolution framelets that learns the
data-driven bases for Hankel matrix decomposition. Our extensive experimental
results from sub-sampled RF data from a real US system confirmed that the
proposed method can effectively reduce the data rate without sacrificing the
image quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1&quot;&gt;Yeo Hun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Shujaat Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huh_J/0/1/0/all/0/1&quot;&gt;Jaeyoung Huh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07682">
<title>Deep metric learning for multi-labelled radiographs. (arXiv:1712.07682v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.07682</link>
<description rdf:parseType="Literal">&lt;p&gt;Many radiological studies can reveal the presence of several co-existing
abnormalities, each one represented by a distinct visual pattern. In this
article we address the problem of learning a distance metric for plain
radiographs that captures a notion of &quot;radiological similarity&quot;: two chest
radiographs are considered to be similar if they share similar abnormalities.
Deep convolutional neural networks (DCNs) are used to learn a low-dimensional
embedding for the radiographs that is equipped with the desired metric. Two
loss functions are proposed to deal with multi-labelled images and potentially
noisy labels. We report on a large-scale study involving over 745,000 chest
radiographs whose labels were automatically extracted from free-text
radiological reports through a natural language processing system. Using 4,500
validated exams, we demonstrate that the methodology performs satisfactorily on
clustering and image retrieval tasks. Remarkably, the learned metric separates
normal exams from those having radiological abnormalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Annarumma_M/0/1/0/all/0/1&quot;&gt;Mauro Annarumma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Montana_G/0/1/0/all/0/1&quot;&gt;Giovanni Montana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07704">
<title>Unsupervised learning of dynamical and molecular similarity using variance minimization. (arXiv:1712.07704v1 [physics.bio-ph])</title>
<link>http://arxiv.org/abs/1712.07704</link>
<description rdf:parseType="Literal">&lt;p&gt;In this report, we present an unsupervised machine learning method for
determining groups of molecular systems according to similarity in their
dynamics or structures using Ward&apos;s minimum variance objective function. We
first apply the minimum variance clustering to a set of simulated tripeptides
using the information theoretic Jensen-Shannon divergence between Markovian
transition matrices in order to gain insight into how point mutations affect
protein dynamics. Then, we extend the method to partition two chemoinformatic
datasets according to structural similarity to motivate a train/validation/test
split for supervised learning that avoids overfitting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Husic_B/0/1/0/all/0/1&quot;&gt;Brooke E. Husic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pande_V/0/1/0/all/0/1&quot;&gt;Vijay S. Pande&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07788">
<title>Deep Unsupervised Clustering Using Mixture of Autoencoders. (arXiv:1712.07788v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.07788</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised clustering is one of the most fundamental challenges in machine
learning. A popular hypothesis is that data are generated from a union of
low-dimensional nonlinear manifolds; thus an approach to clustering is
identifying and separating these manifolds. In this paper, we present a novel
approach to solve this problem by using a mixture of autoencoders. Our model
consists of two parts: 1) a collection of autoencoders where each autoencoder
learns the underlying manifold of a group of similar objects, and 2) a mixture
assignment neural network, which takes the concatenated latent vectors from the
autoencoders as input and infers the distribution over clusters. By jointly
optimizing the two parts, we simultaneously assign data to clusters and learn
the underlying manifolds of each cluster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dejiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eriksson_B/0/1/0/all/0/1&quot;&gt;Brian Eriksson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balzano_L/0/1/0/all/0/1&quot;&gt;Laura Balzano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07800">
<title>Model-Based Clustering of Nonparametric Weighted Networks. (arXiv:1712.07800v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1712.07800</link>
<description rdf:parseType="Literal">&lt;p&gt;Water pollution is a major global environmental problem, and it poses a great
environmental risk to public health and biological diversity. This work is
motivated by assessing the potential environmental threat of coal mining
through increased sulfate concentrations in river networks, which do not belong
to any simple parametric distribution. However, existing network models mainly
focus on binary or discrete networks and weighted networks with known
parametric weight distributions. We propose a principled nonparametric weighted
network model based on exponential-family random graph models and local
likelihood estimation and study its model-based clustering with application to
large-scale water pollution network analysis. We do not require any parametric
distribution assumption on network weights. The proposed method greatly extends
the methodology and applicability of statistical network models. Furthermore,
it is scalable to large and complex networks in large-scale environmental
studies and geoscientific research. The power of our proposed methods is
demonstrated in simulation studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Amal Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xue_L/0/1/0/all/0/1&quot;&gt;Lingzhou Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07811">
<title>Multi-dimensional Graph Fourier Transform. (arXiv:1712.07811v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1712.07811</link>
<description rdf:parseType="Literal">&lt;p&gt;Many signals on Cartesian product graphs appear in the real world, such as
digital images, sensor observation time series, and movie ratings on Netflix.
These signals are &quot;multi-dimensional&quot; and have directional characteristics
along each factor graph. However, the existing graph Fourier transform does not
distinguish these directions, and assigns 1-D spectra to signals on product
graphs. Further, these spectra are often multi-valued at some frequencies. Our
main result is a multi-dimensional graph Fourier transform that solves such
problems associated with the conventional GFT. Using algebraic properties of
Cartesian products, the proposed transform rearranges 1-D spectra obtained by
the conventional GFT into the multi-dimensional frequency domain, of which each
dimension represents a directional frequency along each factor graph. Thus, the
multi-dimensional graph Fourier transform enables directional frequency
analysis, in addition to frequency analysis with the conventional GFT.
Moreover, this rearrangement resolves the multi-valuedness of spectra in some
cases. The multi-dimensional graph Fourier transform is a foundation of novel
filterings and stationarities that utilize dimensional information of graph
signals, which are also discussed in this study. The proposed methods are
applicable to a wide variety of data that can be regarded as signals on
Cartesian product graphs. This study also notes that multivariate graph signals
can be regarded as 2-D univariate graph signals. This correspondence provides
natural definitions of the multivariate graph Fourier transform and the
multivariate stationarity based on their 2-D univariate versions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kurokawa_T/0/1/0/all/0/1&quot;&gt;Takashi Kurokawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oki_T/0/1/0/all/0/1&quot;&gt;Taihei Oki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nagao_H/0/1/0/all/0/1&quot;&gt;Hiromichi Nagao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07897">
<title>Non-convex Optimization for Machine Learning. (arXiv:1712.07897v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.07897</link>
<description rdf:parseType="Literal">&lt;p&gt;A vast majority of machine learning algorithms train their models and perform
inference by solving optimization problems. In order to capture the learning
and prediction problems accurately, structural constraints such as sparsity or
low rank are frequently imposed or else the objective itself is designed to be
a non-convex function. This is especially true of algorithms that operate in
high-dimensional spaces or that train non-linear models such as tensor models
and deep networks.
&lt;/p&gt;
&lt;p&gt;The freedom to express the learning problem as a non-convex optimization
problem gives immense modeling power to the algorithm designer, but often such
problems are NP-hard to solve. A popular workaround to this has been to relax
non-convex problems to convex ones and use traditional methods to solve the
(convex) relaxed optimization problems. However this approach may be lossy and
nevertheless presents significant challenges for large scale optimization.
&lt;/p&gt;
&lt;p&gt;On the other hand, direct approaches to non-convex optimization have met with
resounding success in several domains and remain the methods of choice for the
practitioner, as they frequently outperform relaxation-based techniques -
popular heuristics include projected gradient descent and alternating
minimization. However, these are often poorly understood in terms of their
convergence and other properties.
&lt;/p&gt;
&lt;p&gt;This monograph presents a selection of recent advances that bridge a
long-standing gap in our understanding of these heuristics. The monograph will
lead the reader through several widely used non-convex optimization techniques,
as well as applications thereof. The goal of this monograph is to both,
introduce the rich literature in this area, as well as equip the reader with
the tools and techniques needed to analyze these simple procedures for
non-convex problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jain_P/0/1/0/all/0/1&quot;&gt;Prateek Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kar_P/0/1/0/all/0/1&quot;&gt;Purushottam Kar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07924">
<title>A continuous framework for fairness. (arXiv:1712.07924v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1712.07924</link>
<description rdf:parseType="Literal">&lt;p&gt;Increasingly, discrimination by algorithms is perceived as a societal and
legal problem. As a response, a number of criteria for implementing algorithmic
fairness in machine learning have been developed in the literature. This paper
proposes the Continuous Fairness Algorithm (CFA$\theta$) which enables a
continuous interpolation between different fairness definitions. More
specifically, we make three main contributions to the existing literature.
First, our approach allows the decision maker to continuously vary between
concepts of individual and group fairness. As a consequence, the algorithm
enables the decision maker to adopt intermediate &quot;worldviews&quot; on the degree of
discrimination encoded in algorithmic processes, adding nuance to the extreme
cases of &quot;we&apos;re all equal&quot; (WAE) and &quot;what you see is what you get&quot; (WYSIWYG)
proposed so far in the literature. Second, we use optimal transport theory, and
specifically the concept of the barycenter, to maximize decision maker utility
under the chosen fairness constraints. Third, the algorithm is able to handle
cases of intersectionality, i.e., of multi-dimensional discrimination of
certain groups on grounds of several criteria. We discuss three main examples
(college admissions; credit application; insurance contracts) and map out the
policy implications of our approach. The explicit formalization of the
trade-off between individual and group fairness allows this post-processing
approach to be tailored to different situational contexts in which one or the
other fairness criterion may take precedence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hacker_P/0/1/0/all/0/1&quot;&gt;Philipp Hacker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiedemann_E/0/1/0/all/0/1&quot;&gt;Emil Wiedemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08041">
<title>Autism Classification Using Brain Functional Connectivity Dynamics and Machine Learning. (arXiv:1712.08041v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1712.08041</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of the present study is to identify autism using machine learning
techniques and resting-state brain imaging data, leveraging the temporal
variability of the functional connections (FC) as the only information. We
estimated and compared the FC variability across brain regions between typical,
healthy subjects and autistic population by analyzing brain imaging data from a
world-wide multi-site database known as ABIDE (Autism Brain Imaging Data
Exchange). Our analysis revealed that patients diagnosed with autism spectrum
disorder (ASD) show increased FC variability in several brain regions that are
associated with low FC variability in the typical brain. We then used the
enhanced FC variability of brain regions as features for training machine
learning models for ASD classification and achieved 65% accuracy in
identification of ASD versus control subjects within the dataset. We also used
node strength estimated from number of functional connections per node averaged
over the whole scan as features for ASD classification.The results reveal that
the dynamic FC measures outperform or are comparable with the static FC
measures in predicting ASD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tejwani_R/0/1/0/all/0/1&quot;&gt;Ravi Tejwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liska_A/0/1/0/all/0/1&quot;&gt;Adam Liska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+You_H/0/1/0/all/0/1&quot;&gt;Hongyuan You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Reinen_J/0/1/0/all/0/1&quot;&gt;Jenna Reinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Das_P/0/1/0/all/0/1&quot;&gt;Payel Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08048">
<title>Model selection for Gaussian processes utilizing sensitivity of posterior predictive distribution. (arXiv:1712.08048v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1712.08048</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose two novel methods for simplifying Gaussian process (GP) models by
examining the predictions of a full model in the vicinity of the training
points and thereby ordering the covariates based on their predictive relevance.
Our results on synthetic and real world data sets demonstrate improved variable
selection compared to automatic relevance determination (ARD) in terms of
consistency and predictive performance. We expect our proposed methods to be
useful in interpreting and understanding complex Gaussian process models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Paananen_T/0/1/0/all/0/1&quot;&gt;Topi Paananen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Piironen_J/0/1/0/all/0/1&quot;&gt;Juho Piironen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Andersen_M/0/1/0/all/0/1&quot;&gt;Michael Riis Andersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vehtari_A/0/1/0/all/0/1&quot;&gt;Aki Vehtari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08058">
<title>A brain signature highly predictive of future progression to Alzheimer&apos;s dementia. (arXiv:1712.08058v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1712.08058</link>
<description rdf:parseType="Literal">&lt;p&gt;Early prognosis of Alzheimer&apos;s dementia is hard. Mild cognitive impairment
(MCI) typically precedes Alzheimer&apos;s dementia, yet only a fraction (30%-50%) of
MCI individuals will progress to dementia. Even when a prognosis of dementia is
established using machine learning models and biomarkers, the fraction of MCI
progressors remain limited (50%-75%). Instead of aiming at precise diagnosis in
large clinical cohorts known for their heterogeneity, we propose here to
identify only a subset of individuals who share a common brain signature highly
predictive of oncoming dementia. This signature was discovered using a machine
learning model in a reference public sample (ADNI), where the model was trained
to identify patterns of brain atrophy and functional dysconnectivity commonly
seen in patients suffering from dementia (N = 24), and not seen in cognitively
normal individuals (N = 49). The model then recognized the same brain signature
in 10 MCI individuals, out of N = 56, 90% of which progressed to dementia
within three years. This result is a marked improvement on the state-of-the-art
in prognostic precision, while the brain signature still identified 47% of all
MCI progressors (N = 19). We thus discovered a sizable MCI subpopulation with
homogeneous brain abnormalities and highly predictable clinical trajectories,
which may represent an excellent recruitment target for clinical trials at the
prodromal stage of Alzheimer&apos;s disease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dansereau_C/0/1/0/all/0/1&quot;&gt;Christian Dansereau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tam_A/0/1/0/all/0/1&quot;&gt;Angela Tam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Badhwar_A/0/1/0/all/0/1&quot;&gt;AmanPreet Badhwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Urchs_S/0/1/0/all/0/1&quot;&gt;Sebastian Urchs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Orban_P/0/1/0/all/0/1&quot;&gt;Pierre Orban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rosa_Neto_P/0/1/0/all/0/1&quot;&gt;Pedro Rosa-Neto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bellec_P/0/1/0/all/0/1&quot;&gt;Pierre Bellec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08101">
<title>Profit Driven Decision Trees for Churn Prediction. (arXiv:1712.08101v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.08101</link>
<description rdf:parseType="Literal">&lt;p&gt;Customer retention campaigns increasingly rely on predictive models to detect
potential churners in a vast customer base. From the perspective of machine
learning, the task of predicting customer churn can be presented as a binary
classification problem. Using data on historic behavior, classification
algorithms are built with the purpose of accurately predicting the probability
of a customer defecting. The predictive churn models are then commonly selected
based on accuracy related performance measures such as the area under the ROC
curve (AUC). However, these models are often not well aligned with the core
business requirement of profit maximization, in the sense that, the models fail
to take into account not only misclassification costs, but also the benefits
originating from a correct classification. Therefore, the aim is to construct
churn prediction models that are profitable and preferably interpretable too.
The recently developed expected maximum profit measure for customer churn
(EMPC) has been proposed in order to select the most profitable churn model. We
present a new classifier that integrates the EMPC metric directly into the
model construction. Our technique, called ProfTree, uses an evolutionary
algorithm for learning profit driven decision trees. In a benchmark study with
real-life data sets from various telecommunication service providers, we show
that ProfTree achieves significant profit improvements compared to classic
accuracy driven tree-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hoppner_S/0/1/0/all/0/1&quot;&gt;Sebastiaan H&amp;#xf6;ppner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stripling_E/0/1/0/all/0/1&quot;&gt;Eugen Stripling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baesens_B/0/1/0/all/0/1&quot;&gt;Bart Baesens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Broucke_S/0/1/0/all/0/1&quot;&gt;Seppe vanden Broucke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Verdonck_T/0/1/0/all/0/1&quot;&gt;Tim Verdonck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08107">
<title>A Deep Learning Interpretable Classifier for Diabetic Retinopathy Disease Grading. (arXiv:1712.08107v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.08107</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural network models have been proven to be very successful in image
classification tasks, also for medical diagnosis, but their main concern is its
lack of interpretability. They use to work as intuition machines with high
statistical confidence but unable to give interpretable explanations about the
reported results. The vast amount of parameters of these models make difficult
to infer a rationale interpretation from them. In this paper we present a
diabetic retinopathy interpretable classifier able to classify retine images
into the different levels of disease severity and of explaining its results by
assigning a score for every point in the hidden and input space, evaluating its
contribution to the final classification in a linear way. The generated visual
maps can be interpreted by an expert in order to compare its own knowledge with
the interpretation given by the model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torre_J/0/1/0/all/0/1&quot;&gt;Jordi de la Torre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valls_A/0/1/0/all/0/1&quot;&gt;Aida Valls&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puig_D/0/1/0/all/0/1&quot;&gt;Domenec Puig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.06067">
<title>A Convex Program for Mixed Linear Regression with a Recovery Guarantee for Well-Separated Data. (arXiv:1612.06067v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1612.06067</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a convex approach for mixed linear regression over $d$ features.
This approach is a second-order cone program, based on L1 minimization, which
assigns an estimate regression coefficient in $\mathbb{R}^{d}$ for each data
point. These estimates can then be clustered using, for example, $k$-means. For
problems with two or more mixture classes, we prove that the convex program
exactly recovers all of the mixture components in the noiseless setting under
technical conditions that include a well-separation assumption on the data.
Under these assumptions, recovery is possible if each class has at least $d$
independent measurements. We also explore an iteratively reweighted least
squares implementation of this method on real and synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hand_P/0/1/0/all/0/1&quot;&gt;Paul Hand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Joshi_B/0/1/0/all/0/1&quot;&gt;Babhru Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.06899">
<title>Human experts vs. machines in taxa recognition. (arXiv:1708.06899v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.06899</link>
<description rdf:parseType="Literal">&lt;p&gt;Biomonitoring of waterbodies is vital as the number of anthropogenic
stressors on aquatic ecosystems keeps growing. However, the continuous decrease
in funding makes it impossible to meet monitoring goals or sustain traditional
manual sample processing. In this paper, we review what kind of statistical
tools can be used to enhance the cost efficiency of biomonitoring: We explore
automated identification of freshwater macroinvertebrates which are used as one
indicator group in biomonitoring of aquatic ecosystems. We present the first
classification results of a new imaging system producing multiple images per
specimen. Moreover, these results are compared with the results of human
experts. On a data set of 29 taxonomical groups, automated classification
produces a higher average accuracy than human experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arje_J/0/1/0/all/0/1&quot;&gt;Johanna &amp;#xc4;rje&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tirronen_V/0/1/0/all/0/1&quot;&gt;Ville Tirronen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karkkainen_S/0/1/0/all/0/1&quot;&gt;Salme K&amp;#xe4;rkk&amp;#xe4;inen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meissner_K/0/1/0/all/0/1&quot;&gt;Kristian Meissner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raitoharju_J/0/1/0/all/0/1&quot;&gt;Jenni Raitoharju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Iosifidis_A/0/1/0/all/0/1&quot;&gt;Alexandros Iosifidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gabbouj_M/0/1/0/all/0/1&quot;&gt;Moncef Gabbouj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kiranyaz_S/0/1/0/all/0/1&quot;&gt;Serkan Kiranyaz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04495">
<title>Inverse Ising problem in continuous time: A latent variable approach. (arXiv:1709.04495v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04495</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the inverse Ising problem, i.e. the inference of network
couplings from observed spin trajectories for a model with continuous time
Glauber dynamics. By introducing two sets of auxiliary latent random variables
we render the likelihood into a form, which allows for simple iterative
inference algorithms with analytical updates. The variables are: (1) Poisson
variables to linearise an exponential term which is typical for point process
likelihoods and (2) P\&apos;olya-Gamma variables, which make the likelihood
quadratic in the coupling parameters. Using the augmented likelihood, we derive
an expectation-maximization (EM) algorithm to obtain the maximum likelihood
estimate of network parameters. Using a third set of latent variables we extend
the EM algorithm to sparse couplings via L1 regularization. Finally, we develop
an efficient approximate Bayesian inference algorithm using a variational
approach. We demonstrate the performance of our algorithms on data simulated
from an Ising model. For data which are simulated from a more biologically
plausible network with spiking neurons, we show that the Ising model captures
well the low order statistics of the data and how the Ising couplings are
related to the underlying synaptic structure of the simulated network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Donner_C/0/1/0/all/0/1&quot;&gt;Christian Donner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Opper_M/0/1/0/all/0/1&quot;&gt;Manfred Opper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00342">
<title>Orthogonal Machine Learning: Power and Limitations. (arXiv:1711.00342v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00342</link>
<description rdf:parseType="Literal">&lt;p&gt;Double machine learning provides $\sqrt{n}$-consistent estimates of
parameters of interest even when high-dimensional or nonparametric nuisance
parameters are estimated at an $n^{-1/4}$ rate. The key is to employ
Neyman-orthogonal moment equations which are first-order insensitive to
perturbations in the nuisance parameters. We show that the $n^{-1/4}$
requirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order
notion of orthogonality that grants robustness to more complex or
higher-dimensional nuisance parameters. In the partially linear regression
setting popular in causal inference, we show that we can construct second-order
orthogonal moments if and only if the treatment residual is not normally
distributed. Our proof relies on Stein&apos;s lemma and may be of independent
interest. We conclude by demonstrating the robustness benefits of an explicit
doubly-orthogonal estimation procedure for treatment effect.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zadik_I/0/1/0/all/0/1&quot;&gt;Ilias Zadik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06424">
<title>Learning to Write Stylized Chinese Characters by Reading a Handful of Examples. (arXiv:1712.06424v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06424</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically writing stylized Chinese characters is an attractive yet
challenging task due to its wide applicabilities. In this paper, we propose a
novel framework named Style-Aware Variational Auto-Encoder (SA-VAE) to flexibly
generate Chinese characters. Specifically, we propose to capture the different
characteristics of a Chinese character by disentangling the latent features
into content-related and style-related components. Considering of the complex
shapes and structures, we incorporate the structure information as prior
knowledge into our framework to guide the generation. Our framework shows a
powerful one-shot/low-shot generalization ability by inferring the style
component given a character with unseen style. To the best of our knowledge,
this is the first attempt to learn to write new-style Chinese characters by
observing only one or a few examples. Extensive experiments demonstrate its
effectiveness in generating different stylized Chinese characters by fusing the
feature vectors corresponding to different contents and styles, which is of
significant importance in real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Danyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tongzheng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongxun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07374">
<title>Adversarial Structured Prediction for Multivariate Measures. (arXiv:1712.07374v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07374</link>
<description rdf:parseType="Literal">&lt;p&gt;Many predicted structured objects (e.g., sequences, matchings, trees) are
evaluated using the F-score, alignment error rate (AER), or other multivariate
performance measures. Since inductively optimizing these measures using
training data is typically computationally difficult, empirical risk
minimization of surrogate losses is employed, using, e.g., the hinge loss for
(structured) support vector machines. These approximations often introduce a
mismatch between the learner&apos;s objective and the desired application
performance, leading to inconsistency. We take a different approach:
adversarially approximate training data while optimizing the exact F-score or
AER. Structured predictions under this formulation result from solving zero-sum
games between a predictor seeking the best performance and an adversary seeking
the worst while required to (approximately) match certain structured properties
of the training data. We explore this approach for word alignment (AER
evaluation) and named entity recognition (F-score evaluation) with linear-chain
constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rezaei_A/0/1/0/all/0/1&quot;&gt;Ashkan Rezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ziebart_B/0/1/0/all/0/1&quot;&gt;Brian D. Ziebart&lt;/a&gt;</dc:creator>
</item></rdf:RDF>