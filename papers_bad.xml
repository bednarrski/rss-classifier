<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01831"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01627"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01702"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01825"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06889"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00587"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.09012"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01577"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01608"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01618"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01648"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01667"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01685"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01711"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01743"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01745"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01852"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01872"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.00909"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00181"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01113"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03769"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.01831">
<title>Ultra Low Power Deep-Learning-powered Autonomous Nano Drones. (arXiv:1805.01831v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1805.01831</link>
<description rdf:parseType="Literal">&lt;p&gt;Flying in dynamic, urban, highly-populated environments represents an open
problem in robotics. State-of-the-art (SoA) autonomous Unmanned Aerial Vehicles
(UAVs) employ advanced computer vision techniques based on computationally
expensive algorithms, such as Simultaneous Localization and Mapping (SLAM) or
Convolutional Neural Networks (CNNs) to navigate in such environments. In the
Internet-of-Things (IoT) era, nano-size UAVs capable of autonomous navigation
would be extremely desirable as self-aware mobile IoT nodes. However,
autonomous flight is considered unaffordable in the context of nano-scale UAVs,
where the ultra-constrained power envelopes of tiny rotor-crafts limit the
on-board computational capabilities to low-power microcontrollers. In this
work, we present the first vertically integrated system for fully autonomous
deep neural network-based navigation on nano-size UAVs. Our system is based on
GAP8, a novel parallel ultra-low-power computing platform, and deployed on a 27
g commercial, open-source CrazyFlie 2.0 nano-quadrotor. We discuss a
methodology and software mapping tools that enable the SoA CNN presented in [1]
to be fully executed on-board within a strict 12 fps real-time constraint with
no compromise in terms of flight results, while all processing is done with
only 94 mW on average - 1% of the power envelope of the deployed nano-aircraft.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palossi_D/0/1/0/all/0/1&quot;&gt;Daniele Palossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loquercio_A/0/1/0/all/0/1&quot;&gt;Antonio Loquercio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conti_F/0/1/0/all/0/1&quot;&gt;Francesco Conti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flamand_E/0/1/0/all/0/1&quot;&gt;Eric Flamand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1&quot;&gt;Davide Scaramuzza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01048">
<title>RF-PUF: IoT Security Enhancement through Authentication of Wireless Nodes using In-situ Machine Learning. (arXiv:1805.01048v1 [cs.CR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1805.01048</link>
<description rdf:parseType="Literal">&lt;p&gt;Physical unclonable functions (PUF) in silicon exploit die-to-die
manufacturing variations during fabrication for uniquely identifying each die.
Since it is practically a hard problem to recreate exact silicon features
across dies, a PUFbased authentication system is robust, secure and
cost-effective, as long as bias removal and error correction are taken into
account. In this work, we utilize the effects of inherent process variation on
analog and radio-frequency (RF) properties of multiple wireless transmitters
(Tx) in a sensor network, and detect the features at the receiver (Rx) using a
deep neural network based framework. The proposed mechanism/framework, called
RF-PUF, harnesses already existing RF communication hardware and does not
require any additional PUF-generation circuitry in the Tx for practical
implementation. Simulation results indicate that the RF-PUF framework can
distinguish up to 10000 transmitters (with standard foundry defined variations
for a 65 nm process, leading to non-idealities such as LO offset and I-Q
imbalance) under varying channel conditions, with a probability of false
detection &amp;lt; 10e-3
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_B/0/1/0/all/0/1&quot;&gt;Baibhab Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1&quot;&gt;Debayan Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1&quot;&gt;Shreyas Sen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01374">
<title>RF-PUF: Enhancing IoT Security through Authentication of Wireless Nodes using In-situ Machine Learning. (arXiv:1805.01374v1 [cs.CR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1805.01374</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional authentication in radio-frequency (RF) systems enable secure data
transmission within a network through techniques such as digital signatures and
hash-based message authentication codes (HMAC). However, these techniques may
not prevent a malicious attacker from stealing the secret encryption keys using
invasive, modeling or side channel attacks. Physically unclonable functions
(PUF), on the other hand, can exploit manufacturing process variations to
uniquely identify silicon chips which makes a PUF-based system extremely robust
and secure at low cost, as it is practically impossible to replicate the same
silicon characteristics across dies. In this paper, we present RF- PUF: a deep
neural network based framework that allows real-time authentication of wireless
nodes, using the effects of inherent process variation on RF properties of the
wireless transmitters (Tx), detected through in-situ machine learning at the
receiver (Rx) end. The proposed method utilizes the already-existing asymmetric
RF communication framework and does not require any additional circuitry for
PUF generation or feature extraction. Simulation results involving the process
variations in a standard 65 nm technology node, and features such as LO offset
and I-Q imbalance detected with a neural network having 50 neurons in the
hidden layer indicate that the framework can distinguish up to 4800
transmitters with an accuracy of 99.9% (~ 99% for 10,000 transmitters) under
varying channel conditions, and without the need for traditional preambles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_B/0/1/0/all/0/1&quot;&gt;Baibhab Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1&quot;&gt;Debayan Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maity_S/0/1/0/all/0/1&quot;&gt;Shovan Maity&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1&quot;&gt;Shreyas Sen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01627">
<title>BelMan: Bayesian Bandits on the Belief--Reward Manifold. (arXiv:1805.01627v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01627</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a generic, Bayesian, information geometric approach to the
exploration--exploitation trade-off in multi-armed bandit problems. Our
approach, BelMan, uniformly supports pure exploration,
exploration--exploitation, and two-phase bandit problems. The knowledge on
bandit arms and their reward distributions is summarised by the barycentre of
the joint distributions of beliefs and rewards of the arms, the
\emph{pseudobelief-reward}, within the beliefs-rewards manifold. BelMan
alternates \emph{information projection} and \emph{reverse information
projection}, i.e., projection of the pseudobelief-reward onto beliefs-rewards
to choose the arm to play, and projection of the resulting beliefs-rewards onto
the pseudobelief-reward. It introduces a mechanism that infuses an exploitative
bias by means of a \emph{focal distribution}, i.e., a reward distribution that
gradually concentrates on higher rewards. Comparative performance evaluation
with state-of-the-art algorithms shows that BelMan is not only competitive but
can also outperform other approaches in specific setups, for instance involving
many arms and continuous rewards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_D/0/1/0/all/0/1&quot;&gt;Debabrota Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senellart_P/0/1/0/all/0/1&quot;&gt;Pierre Senellart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bressan_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Bressan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01702">
<title>Beyond the Click-Through Rate: Web Link Selection with Multi-level Feedback. (arXiv:1805.01702v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01702</link>
<description rdf:parseType="Literal">&lt;p&gt;The web link selection problem is to select a small subset of web links from
a large web link pool, and to place the selected links on a web page that can
only accommodate a limited number of links, e.g., advertisements,
recommendations, or news feeds. Despite the long concerned click-through rate
which reflects the attractiveness of the link itself, the revenue can only be
obtained from user actions after clicks, e.g., purchasing after being directed
to the product pages by recommendation links. Thus, the web links have an
intrinsic \emph{multi-level feedback structure}. With this observation, we
consider the context-free web link selection problem, where the objective is to
maximize revenue while ensuring that the attractiveness is no less than a
preset threshold. The key challenge of the problem is that each link&apos;s
multi-level feedbacks are stochastic, and unobservable unless the link is
selected. We model this problem with a constrained stochastic multi-armed
bandit formulation, and design an efficient link selection algorithm, called
Constrained Upper Confidence Bound algorithm (\textbf{Con-UCB}), and prove
$O(\sqrt{T\ln T})$ bounds on both the regret and the violation of the
attractiveness constraint. We conduct extensive experiments on three real-world
datasets, and show that \textbf{Con-UCB} outperforms state-of-the-art
context-free bandit algorithms concerning the multi-level feedback structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_K/0/1/0/all/0/1&quot;&gt;Kechao Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Longbo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lui_J/0/1/0/all/0/1&quot;&gt;John C.S. Lui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01825">
<title>Simplified SPARQL REST API - CRUD on JSON Object Graphs via URI Paths. (arXiv:1805.01825v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1805.01825</link>
<description rdf:parseType="Literal">&lt;p&gt;Within the Semantic Web community, SPARQL is one of the predominant languages
to query and update RDF knowledge. However, the complexity of SPARQL, the
underlying graph structure and various encodings are common sources of
confusion for Semantic Web novices.
&lt;/p&gt;
&lt;p&gt;In this paper we present a general purpose approach to convert any given
SPARQL endpoint into a simple to use REST API. To lower the initial hurdle, we
represent the underlying graph as an interlinked view of nested JSON objects
that can be traversed by the API path.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroder_M/0/1/0/all/0/1&quot;&gt;Markus Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rn Hees&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernardi_A/0/1/0/all/0/1&quot;&gt;Ansgar Bernardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ewert_D/0/1/0/all/0/1&quot;&gt;Daniel Ewert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klotz_P/0/1/0/all/0/1&quot;&gt;Peter Klotz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stadtmuller_S/0/1/0/all/0/1&quot;&gt;Steffen Stadtm&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06889">
<title>Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers. (arXiv:1801.06889v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06889</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has recently seen rapid development and significant attention
due to its state-of-the-art performance on previously-thought hard problems.
However, because of the innate complexity and nonlinear structure of deep
neural networks, the underlying decision making processes for why these models
are achieving such high performance are challenging and sometimes mystifying to
interpret. As deep learning spreads across domains, it is of paramount
importance that we equip users of deep learning with tools for understanding
when a model works correctly, when it fails, and ultimately how to improve its
performance. Standardized toolkits for building neural networks have helped
democratize deep learning; visual analytics systems have now been developed to
support model explanation, interpretation, debugging, and improvement. We
present a survey of the role of visual analytics in deep learning research,
noting its short yet impactful history and summarize the state-of-the-art using
a human-centered interrogative framework, focusing on the Five W&apos;s and How
(Why, Who, What, How, When, and Where), to thoroughly summarize deep learning
visual analytics research. We conclude by highlighting research directions and
open research problems. This survey helps new researchers and practitioners in
both visual analytics and deep learning to quickly learn key aspects of this
young and rapidly growing body of research, whose impact spans a diverse range
of domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohman_F/0/1/0/all/0/1&quot;&gt;Fred Hohman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahng_M/0/1/0/all/0/1&quot;&gt;Minsuk Kahng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pienta_R/0/1/0/all/0/1&quot;&gt;Robert Pienta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00587">
<title>Structure-sensitive Multi-scale Deep Neural Network for Low-Dose CT Denoising. (arXiv:1805.00587v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00587</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed tomography (CT) is a popular medical imaging modality in clinical
applications. At the same time, the x-ray radiation dose associated with CT
scans raises public concerns due to its potential risks to the patients. Over
the past years, major efforts have been dedicated to the development of
Low-Dose CT (LDCT) methods. However, the radiation dose reduction compromises
the signal-to-noise ratio (SNR), leading to strong noise and artifacts that
down-grade CT image quality. In this paper, we propose a novel 3D noise
reduction method, called Structure-sensitive Multi-scale Generative Adversarial
Net (SMGAN), to improve the LDCT image quality. Specifically, we incorporate
three-dimensional (3D) volumetric information to improve the image quality.
Also, different loss functions for training denoising models are investigated.
Experiments show that the proposed method can effectively preserve structural
and texture information from normal-dose CT (NDCT) images, and significantly
suppress noise and artifacts. Qualitative visual assessments by three
experienced radiologists demonstrate that the proposed method retrieves more
detailed information, and outperforms competing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenyu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qingsong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1&quot;&gt;Hongming Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gjesteby_L/0/1/0/all/0/1&quot;&gt;Lars Gjesteby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_S/0/1/0/all/0/1&quot;&gt;Shenghong Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuiyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1&quot;&gt;Wenxiang Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Ge Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.09012">
<title>An Energy-Efficient Mixed-Signal Neuron for Inherently Error-Resilient Neuromorphic Systems. (arXiv:1710.09012v1 [cs.ET] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1710.09012</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents the design and analysis of a mixed-signal neuron (MS-N)
for convolutional neural networks (CNN) and compares its performance with a
digital neuron (Dig-N) in terms of operating frequency, power and noise. The
circuit-level implementation of the MS-N in 65 nm CMOS technology exhibits 2-3
orders of magnitude better energy-efficiency over Dig-N for neuromorphic
computing applications - especially at low frequencies due to the high leakage
currents from many transistors in Dig-N. The inherent error-resiliency of CNN
is exploited to handle the thermal and flicker noise of MS-N. A system-level
analysis using a cohesive circuit-algorithmic framework on MNIST and CIFAR-10
datasets demonstrate an increase of 3% in worst-case classification error for
MNIST when the integrated noise power in the bandwidth is ~ 1 {\mu}V2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_B/0/1/0/all/0/1&quot;&gt;Baibhab Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1&quot;&gt;Priyadarshini Panda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maity_S/0/1/0/all/0/1&quot;&gt;Shovan Maity&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushik Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1&quot;&gt;Shreyas Sen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01577">
<title>Local angles and dimension estimation from data on manifolds. (arXiv:1805.01577v1 [math.ST])</title>
<link>http://arxiv.org/abs/1805.01577</link>
<description rdf:parseType="Literal">&lt;p&gt;For data living in a manifold $M\subseteq \mathbb{R}^m$ and a point $p\in M$
we consider a statistic $U_{k,n}$ which estimates the variance of the angle
between pairs of vectors $X_i-p$ and $X_j-p$, for data points $X_i$, $X_j$,
near $p$, and evaluate this statistic as a tool for estimation of the intrinsic
dimension of $M$ at $p$. Consistency of the local dimension estimator is
established and the asymptotic distribution of $U_{k,n}$ is found under minimal
regularity assumptions. Performance of the proposed methodology is compared
against state-of-the-art methods on simulated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Diaz_M/0/1/0/all/0/1&quot;&gt;Mateo D&amp;#xed;az&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Quiroz_A/0/1/0/all/0/1&quot;&gt;Adolfo J. Quiroz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Velasco_M/0/1/0/all/0/1&quot;&gt;Mauricio Velasco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01608">
<title>Causal Queries from Observational Data in Biological Systems via Bayesian Networks: An Empirical Study in Small Networks. (arXiv:1805.01608v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1805.01608</link>
<description rdf:parseType="Literal">&lt;p&gt;Biological networks are a very convenient modelling and visualisation tool to
discover knowledge from modern high-throughput genomics and postgenomics data
sets. Indeed, biological entities are not isolated, but are components of
complex multi-level systems. We go one step further and advocate for the
consideration of causal representations of the interactions in living
systems.We present the causal formalism and bring it out in the context of
biological networks, when the data is observational. We also discuss its
ability to decipher the causal information flow as observed in gene expression.
We also illustrate our exploration by experiments on small simulated networks
as well as on a real biological data set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+White_A/0/1/0/all/0/1&quot;&gt;Alex White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Vignes_M/0/1/0/all/0/1&quot;&gt;Matthieu Vignes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01618">
<title>Distribution Assertive Regression. (arXiv:1805.01618v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.01618</link>
<description rdf:parseType="Literal">&lt;p&gt;In regression modelling approach, the main step is to fit the regression line
as close as possible to the target variable. In this process most algorithms
try to fit all of the data in a single line and hence fitting all parts of
target variable in one go. It was observed that the error between predicted and
target variable usually have a varying behavior across the various quantiles of
the dependent variable and hence single point diagnostic like MAPE has its
limitation to signify the level of fitness across the distribution of
Y(dependent variable). To address this problem, a novel approach is proposed in
the paper to deal with regression fitting over various quantiles of target
variable. Using this approach we have significantly improved the eccentric
behavior of the distance (error) between predicted and actual value of
regression. Our proposed solution is based on understanding the segmented
behavior of the data with respect to the internal segments within the data and
approach for retrospectively fitting the data based on each quantile behavior.
We believe exploring and using this approach would help in achieving better and
more explainable results in most settings of real world data modelling
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pathak_K/0/1/0/all/0/1&quot;&gt;Kumarjit Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kapila_J/0/1/0/all/0/1&quot;&gt;Jitin Kapila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barvey_A/0/1/0/all/0/1&quot;&gt;Aasheesh Barvey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gawande_N/0/1/0/all/0/1&quot;&gt;Nikit Gawande&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01648">
<title>Sharp Convergence Rates for Langevin Dynamics in the Nonconvex Setting. (arXiv:1805.01648v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.01648</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of sampling from a distribution where the negative
logarithm of the target density is $L$-smooth everywhere and $m$-strongly
convex outside a ball of radius $R$, but potentially non-convex inside this
ball. We study both overdamped and underdamped Langevin MCMC and prove upper
bounds on the time required to obtain a sample from a distribution that is
within $\epsilon$ of the target distribution in $1$-Wasserstein distance. For
the first-order method (overdamped Langevin MCMC), the time complexity is
$\tilde{\mathcal{O}}\left(e^{cLR^2}\frac{d}{\epsilon^2}\right)$, where $d$ is
the dimension of the underlying space. For the second-order method (underdamped
Langevin MCMC), the time complexity is
$\tilde{\mathcal{O}}\left(e^{cLR^2}\frac{\sqrt{d}}{\epsilon}\right)$ for some
explicit positive constant $c$. Surprisingly, the convergence rate is only
polynomial in the dimension $d$ and the target accuracy $\epsilon$. It is
however exponential in the problem parameter $LR^2$, which is a measure of
non-logconcavity of the target distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1&quot;&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abbasi_Yadkori_Y/0/1/0/all/0/1&quot;&gt;Yasin Abbasi-Yadkori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1&quot;&gt;Peter L. Bartlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01667">
<title>Intracranial Error Detection via Deep Learning. (arXiv:1805.01667v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01667</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning techniques have revolutionized the field of machine learning
and were recently successfully applied to various classification problems in
noninvasive electroencephalography (EEG). However, these methods were so far
only rarely evaluated for use in intracranial EEG. We employed convolutional
neural networks (CNNs) to classify and characterize the error-related brain
response as measured in 24 intracranial EEG recordings. Decoding accuracies of
CNNs were significantly higher than those of a regularized linear discriminant
analysis. Using time-resolved deep decoding, it was possible to classify errors
in various regions in the human brain, and further to decode errors over 200 ms
before the actual erroneous button press, e.g., in the precentral gyrus.
Moreover, deeper networks performed better than shallower networks in
distinguishing correct from error trials in all-channel decoding. In single
recordings, up to 100 % decoding accuracy was achieved. Visualization of the
networks&apos; learned features indicated that multivariate decoding on an ensemble
of channels yields related, albeit non-redundant information compared to
single-channel decoding. In summary, here we show the usefulness of deep
learning for both intracranial error decoding and mapping of the
spatio-temporal structure of the human error processing network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volker_M/0/1/0/all/0/1&quot;&gt;Martin V&amp;#xf6;lker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammer_J/0/1/0/all/0/1&quot;&gt;Ji&amp;#x159;&amp;#xed; Hammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1&quot;&gt;Robin T. Schirrmeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behncke_J/0/1/0/all/0/1&quot;&gt;Joos Behncke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiederer_L/0/1/0/all/0/1&quot;&gt;Lukas D.J. Fiederer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulze_Bonhage_A/0/1/0/all/0/1&quot;&gt;Andreas Schulze-Bonhage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marusic_P/0/1/0/all/0/1&quot;&gt;Petr Marusi&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1&quot;&gt;Wolfram Burgard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1&quot;&gt;Tonio Ball&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01685">
<title>Combinatorial Pure Exploration with Continuous and Separable Reward Functions and Its Applications (Extended Version). (arXiv:1805.01685v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01685</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the Combinatorial Pure Exploration problem with Continuous and
Separable reward functions (CPE-CS) in the stochastic multi-armed bandit
setting. In a CPE-CS instance, we are given several stochastic arms with
unknown distributions, as well as a collection of possible decisions. Each
decision has a reward according to the distributions of arms. The goal is to
identify the decision with the maximum reward, using as few arm samples as
possible. The problem generalizes the combinatorial pure exploration problem
with linear rewards, which has attracted significant attention in recent years.
In this paper, we propose an adaptive learning algorithm for the CPE-CS
problem, and analyze its sample complexity. In particular, we introduce a new
hardness measure called the consistent optimality hardness, and give both the
upper and lower bounds of sample complexity. Moreover, we give examples to
demonstrate that our solution has the capacity to deal with non-linear reward
functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiran Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ok_J/0/1/0/all/0/1&quot;&gt;Jungseul Ok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01711">
<title>Using Quantum Mechanics to Cluster Time Series. (arXiv:1805.01711v1 [physics.data-an])</title>
<link>http://arxiv.org/abs/1805.01711</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article we present a method by which we can reduce a time series into
a single point in $\mathbb{R}^{13}$. We have chosen 13 dimensions so as to
prevent too many points from being labeled as &quot;noise.&quot; When using a Euclidean
(or Mahalanobis) metric, a simple clustering algorithm will with near certainty
label the majority of points as &quot;noise.&quot; On pure physical considerations, this
is not possible. Included in our 13 dimensions are four parameters which
describe the coefficients of a cubic polynomial attached to a Gaussian picking
up a general trend, four parameters picking up periodicity in a time series,
two each for amplitude of a wave and period of a wave, and the final five
report the &quot;leftover&quot; noise of the detrended and aperiodic time series. Of the
final five parameters, four are the centralized probabilistic moments, and the
final for the relative size of the series. The first main contribution of this
work is to apply a theorem of quantum mechanics about the completeness of the
solutions to the quantum harmonic oscillator on $L^2(\mathbb{R})$ to estimating
trends in time series. The second main contribution is the method of fitting
parameters. After many numerical trials, we realized that methods such a
Newton-Rhaphson and Levenberg-Marquardt converge extremely fast if the initial
guess is good. Thus we guessed many initial points in our parameter space and
computed only a few iterations, a technique common in Keogh&apos;s work on time
series clustering. Finally, we have produced a model which gives incredibly
accurate results quickly. We ackowledge that there are faster methods as well
of more accurate methods, but this work shows that we can still increase
computation speed with little, if any, cost to accuracy in the sense of data
clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Alexander_C/0/1/0/all/0/1&quot;&gt;Clark Alexander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Luke Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Akhmametyeva_S/0/1/0/all/0/1&quot;&gt;Sofya Akhmametyeva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01743">
<title>Classification of Epileptic EEG Signals by Wavelet based CFC. (arXiv:1805.01743v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1805.01743</link>
<description rdf:parseType="Literal">&lt;p&gt;Electroencephalogram, an influential equipment for analyzing humans
activities and recognition of seizure attacks can play a crucial role in
designing accurate systems which can distinguish ictal seizures from regular
brain alertness, since it is the first step towards accomplishing a high
accuracy computer aided diagnosis system (CAD). In this article a novel
approach for classification of ictal signals with wavelet based cross frequency
coupling (CFC) is suggested. After extracting features by wavelet based CFC,
optimal features have been selected by t-test and quadratic discriminant
analysis (QDA) have completed the Classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahmadi_A/0/1/0/all/0/1&quot;&gt;Amirmasoud Ahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Behroozi_M/0/1/0/all/0/1&quot;&gt;Mahsa Behroozi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shalchyan_V/0/1/0/all/0/1&quot;&gt;Vahid Shalchyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Daliri_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Daliri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01745">
<title>A brief introduction to the Grey Machine Learning. (arXiv:1805.01745v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.01745</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a brief introduction to the key points of the Grey
Machine Learning (GML) based on the kernels. The general formulation of the
grey system models have been firstly summarized, and then the nonlinear
extension of the grey models have been developed also with general
formulations. The kernel implicit mapping is used to estimate the nonlinear
function of the GML model, by extending the nonparametric formulation of the
LSSVM, the estimation of the nonlinear function of the GML model can also be
expressed by the kernels. A short discussion on the priority of this new
framework to the existing grey models and LSSVM have also been discussed in
this paper. And the perspectives and future orientations of this framework have
also been presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01852">
<title>Valid Inference for $L_2$-Boosting. (arXiv:1805.01852v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.01852</link>
<description rdf:parseType="Literal">&lt;p&gt;We review several recently proposed post-selection inference frameworks and
assess their transferability to the component-wise functional gradient descent
algorithm (CFGD) under normality assumption for model errors, also known as
$L_2$-Boosting. The CFGD is one of the most versatile toolboxes to analyze
data, as it scales well to high-dimensional data sets, allows for a very
flexible definition of additive regression models and incorporates inbuilt
variable selection. %After addressing several issues associated with Due to the
iterative nature, which can repeatedly select the same component to update, an
inference framework for component-wise boosting algorithms requires adaptations
of existing approaches; we propose tests and confidence intervals for linear,
grouped and penalized additive model components estimated using the
$L_2$-boosting selection process. We apply our framework to the prostate cancer
data set and investigate the properties of our concepts in simulation studies.
%The most general and promising selective inference framework for
$L_2$-Boosting as well as for more general gradient-descent boosting algorithms
is an sampling approach which constitutes an adoption of the recently proposed
method by Yang et al. (2016).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rugamer_D/0/1/0/all/0/1&quot;&gt;David R&amp;#xfc;gamer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Greven_S/0/1/0/all/0/1&quot;&gt;Sonja Greven&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01872">
<title>Automatic Estimation of Modulation Transfer Functions. (arXiv:1805.01872v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.01872</link>
<description rdf:parseType="Literal">&lt;p&gt;The modulation transfer function (MTF) is widely used to characterise the
performance of optical systems. Measuring it is costly and it is thus rarely
available for a given lens specimen. Instead, MTFs based on simulations or, at
best, MTFs measured on other specimens of the same lens are used. Fortunately,
images recorded through an optical system contain ample information about its
MTF, only that it is confounded with the statistics of the images. This work
presents a method to estimate the MTF of camera lens systems directly from
photographs, without the need for expensive equipment. We use a custom grid
display to accurately measure the point response of lenses to acquire ground
truth training data. We then use the same lenses to record natural images and
employ a data-driven supervised learning approach using a convolutional neural
network to estimate the MTF on small image patches, aggregating the information
into MTF charts over the entire field of view. It generalises to unseen lenses
and can be applied for single photographs, with the performance improving if
multiple photographs are available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1&quot;&gt;Matthias Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volchkov_V/0/1/0/all/0/1&quot;&gt;Valentin Volchkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirsch_M/0/1/0/all/0/1&quot;&gt;Michael Hirsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.00909">
<title>Machine learning for neural decoding. (arXiv:1708.00909v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/1708.00909</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite rapid advances in machine learning tools, the majority of neural
decoding approaches still use traditional methods. Improving the performance of
neural decoding algorithms allows us to better understand the information
contained in a neural population, and can help advance engineering applications
such as brain machine interfaces. Here, we apply modern machine learning
techniques, including neural networks and gradient boosting, to decode from
spiking activity in 1) motor cortex, 2) somatosensory cortex, and 3)
hippocampus. We compare the predictive ability of these modern methods with
traditional decoding methods such as Wiener and Kalman filters. Modern methods,
in particular neural networks and ensembles, significantly outperformed the
traditional approaches. For instance, for all of the three brain areas, an LSTM
decoder explained over 40% of the unexplained variance from a Wiener filter.
These results suggest that modern machine learning techniques should become the
standard methodology for neural decoding. We provide a tutorial and code to
facilitate wider implementation of these methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Glaser_J/0/1/0/all/0/1&quot;&gt;Joshua I. Glaser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chowdhury_R/0/1/0/all/0/1&quot;&gt;Raeed H. Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Perich_M/0/1/0/all/0/1&quot;&gt;Matthew G. Perich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Miller_L/0/1/0/all/0/1&quot;&gt;Lee E. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kording_K/0/1/0/all/0/1&quot;&gt;Konrad P. Kording&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00181">
<title>Personalized Gaussian Processes for Future Prediction of Alzheimer&apos;s Disease Progression. (arXiv:1712.00181v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00181</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce the use of a personalized Gaussian Process model
(pGP) to predict the key metrics of Alzheimer&apos;s Disease progression (MMSE,
ADAS-Cog13, CDRSB and CS) based on each patient&apos;s previous visits. We start by
learning a population-level model using multi-modal data from previously seen
patients using the base Gaussian Process (GP) regression. Then, this model is
adapted sequentially over time to a new patient using domain adaptive GPs to
form the patient&apos;s pGP. We show that this new approach, together with an
auto-regressive formulation, leads to significant improvements in forecasting
future clinical status and cognitive scores for target patients when compared
to modeling the population with traditional GPs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peterson_K/0/1/0/all/0/1&quot;&gt;Kelly Peterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudovic_O/0/1/0/all/0/1&quot;&gt;Ognjen Rudovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerrero_R/0/1/0/all/0/1&quot;&gt;Ricardo Guerrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picard_R/0/1/0/all/0/1&quot;&gt;Rosalind W. Picard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01113">
<title>Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD. (arXiv:1803.01113v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01113</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed Stochastic Gradient Descent (SGD) when run in a synchronous
manner, suffers from delays in waiting for the slowest learners (stragglers).
Asynchronous methods can alleviate stragglers, but cause gradient staleness
that can adversely affect convergence. In this work we present a novel
theoretical characterization of the speed-up offered by asynchronous methods by
analyzing the trade-off between the error in the trained model and the actual
training runtime (wall- clock time). The novelty in our work is that our
runtime analysis considers random straggler delays, which helps us design and
compare distributed SGD algorithms that strike a balance between stragglers and
staleness. We also present a new convergence analysis of asynchronous SGD
variants without bounded or exponential delay assumptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Sanghamitra Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Joshi_G/0/1/0/all/0/1&quot;&gt;Gauri Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Soumyadip Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dube_P/0/1/0/all/0/1&quot;&gt;Parijat Dube&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nagpurkar_P/0/1/0/all/0/1&quot;&gt;Priya Nagpurkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03769">
<title>A Minimax Surrogate Loss Approach to Conditional Difference Estimation. (arXiv:1803.03769v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03769</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new machine learning approach to estimate personalized treatment
effects in the classical potential outcomes framework with binary outcomes. To
overcome the problem that both treatment and control outcomes for the same unit
are required for supervised learning, we propose surrogate loss functions that
incorporate both treatment and control data. The new surrogates yield tighter
bounds than the sum of losses for treatment and control groups. A specific
choice of loss function, namely a type of hinge loss, yields a minimax support
vector machine formulation. The resulting optimization problem requires the
solution to only a single convex optimization problem, incorporating both
treatment and control units, and it enables the kernel trick to be used to
handle nonlinear (also non-parametric) estimation. Statistical learning bounds
are also presented for the framework, and experimental results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goh_S/0/1/0/all/0/1&quot;&gt;Siong Thye Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;</dc:creator>
</item></rdf:RDF>