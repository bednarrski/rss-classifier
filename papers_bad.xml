<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-22T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07390"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07074"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07168"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07191"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07220"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07251"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07261"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07275"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07302"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07429"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07069"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07105"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07169"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07172"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07216"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07217"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07226"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07249"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07258"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07260"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07379"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07380"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07382"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07412"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07440"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.02703"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.02419"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05236"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04577"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11470"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05527"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06314"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06576"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06638"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.07390">
<title>An Explicit Neural Network Construction for Piecewise Constant Function Approximation. (arXiv:1808.07390v1 [math.NA])</title>
<link>http://arxiv.org/abs/1808.07390</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an explicit construction for feedforward neural network (FNN),
which provides a piecewise constant approximation for multivariate functions.
The proposed FNN has two hidden layers, where the weights and thresholds are
explicitly defined and do not require numerical optimization for training.
Unlike most of the existing work on explicit FNN construction, the proposed FNN
does not rely on tensor structure in multiple dimensions. Instead, it
automatically creates Voronoi tessellation of the domain, based on the given
data of the target function, and piecewise constant approximation of the
function. This makes the construction more practical for applications. We
present both theoretical analysis and numerical examples to demonstrate its
properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kailiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xiu_D/0/1/0/all/0/1&quot;&gt;Dongbin Xiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07074">
<title>The What, the Why, and the How of Artificial Explanations in Automated Decision-Making. (arXiv:1808.07074v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.07074</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing incorporation of Artificial Intelligence in the form of
automated systems into decision-making procedures highlights not only the
importance of decision theory for automated systems but also the need for these
decision procedures to be explainable to the people involved in them.
Traditional realist accounts of explanation, wherein explanation is a relation
that holds (or does not hold) eternally between an explanans and an
explanandum, are not adequate to account for the notion of explanation required
for artificial decision procedures. We offer an alternative account of
explanation as used in the context of automated decision-making that makes
explanation an epistemic phenomenon, and one that is dependent on context. This
account of explanation better accounts for the way that we talk about, and use,
explanations and derived concepts, such as `explanatory power&apos;, and also allows
us to differentiate between reasons or causes on the one hand, which do not
need to have an epistemic aspect, and explanations on the other, which do have
such an aspect. Against this theoretical backdrop we then review existing
approaches to explanation in Artificial Intelligence and Machine Learning, and
suggest desiderata which truly explainable decision systems should fulfill.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Besold_T/0/1/0/all/0/1&quot;&gt;Tarek R. Besold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uckelman_S/0/1/0/all/0/1&quot;&gt;Sara L. Uckelman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07168">
<title>On Deep Neural Networks for Detecting Heart Disease. (arXiv:1808.07168v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07168</link>
<description rdf:parseType="Literal">&lt;p&gt;Heart disease is the leading cause of death, and experts estimate that
approximately half of all heart attacks and strokes occur in people who have
not been flagged as &quot;at risk.&quot; Thus, there is an urgent need to improve the
accuracy of heart disease diagnosis. To this end, we investigate the potential
of using data analysis, and in particular the design and use of deep neural
networks (DNNs) for detecting heart disease based on routine clinical data. Our
main contribution is the design, evaluation, and optimization of DNN
architectures of increasing depth for heart disease diagnosis. This work led to
the discovery of a novel five layer DNN architecture - named Heart Evaluation
for Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields
best prediction accuracy. HEARO-5&apos;s design employs regularization optimization
and automatically deals with missing data and/or data outliers. To evaluate and
tune the architectures we use k-way cross-validation as well as Matthews
correlation coefficient (MCC) to measure the quality of our classifications.
The study is performed on the publicly available Cleveland dataset of medical
information, and we are making our developments open source, to further
facilitate openness and research on the use of DNNs in medicine. The HEARO-5
architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms
currently published research in the area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomov_N/0/1/0/all/0/1&quot;&gt;Nathalie-Sofia Tomov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomov_S/0/1/0/all/0/1&quot;&gt;Stanimire Tomov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07191">
<title>Identifying High-Quality Chinese News Comments Based on Multi-Target Text Matching Model. (arXiv:1808.07191v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.07191</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of information technology, there is an explosive growth
in the number of online comment concerning news, blogs and so on. The massive
comments are overloaded, and often contain some misleading and unwelcome
information. Therefore, it is necessary to identify high-quality comments and
filter out low-quality comments. In this work, we introduce a novel task:
high-quality comment identification (HQCI), which aims to automatically assess
the quality of online comments. First, we construct a news comment corpus,
which consists of news, comments, and the corresponding quality label. Second,
we analyze the dataset, and find the quality of comments can be measured in
three aspects: informativeness, consistency, and novelty. Finally, we propose a
novel multi-target text matching model, which can measure three aspects by
referring to the news and surrounding comments. Experimental results show that
our method can outperform various baselines by a large margin on the news
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Deli Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shuming Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1&quot;&gt;Pengcheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xu Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07220">
<title>Approximating Poker Probabilities with Deep Learning. (arXiv:1808.07220v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.07220</link>
<description rdf:parseType="Literal">&lt;p&gt;Many poker systems, whether created with heuristics or machine learning, rely
on the probability of winning as a key input. However calculating the precise
probability using combinatorics is an intractable problem, so instead we
approximate it. Monte Carlo simulation is an effective technique that can be
used to approximate the probability that a player will win and/or tie a hand.
However, without the use of a memory-intensive lookup table or a supercomputer,
it becomes infeasible to run millions of times when training an agent with
self-play. To combat the space-time tradeoff, we use deep learning to
approximate the probabilities obtained from the Monte Carlo simulation with
high accuracy. The learned model proves to be a lightweight alternative to
Monte Carlo simulation, which ultimately allows us to use the probabilities as
inputs during self-play efficiently. The source code and stored neural network
can be found at https://github.com/brandinho/Poker-Probability-Approximation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_B/0/1/0/all/0/1&quot;&gt;Brandon Da Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07251">
<title>Genie: An Open Box Counterfactual Policy Estimator for Optimizing Sponsored Search Marketplace. (arXiv:1808.07251v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.07251</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an offline counterfactual policy estimation
framework called Genie to optimize Sponsored Search Marketplace. Genie employs
an open box simulation engine with click calibration model to compute the KPI
impact of any modification to the system. From the experimental results on Bing
traffic, we showed that Genie performs better than existing observational
approaches that employs randomized experiments for traffic slices that have
frequent policy updates. We also show that Genie can be used to tune completely
new policies efficiently without creating risky randomized experiments due to
cold start problem. As time of today, Genie hosts more than 10000 optimization
jobs yearly which runs more than 30 Million processing node hours of big data
jobs for Bing Ads. For the last 3 years, Genie has been proven to be the one of
the major platforms to optimize Bing Ads Marketplace due to its reliability
under frequent policy changes and its efficiency to minimize risks in real
experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bayir_M/0/1/0/all/0/1&quot;&gt;Murat Ali Bayir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mingsen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yaojia Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yifan Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07261">
<title>Increasing Trust in AI Services through Supplier&apos;s Declarations of Conformity. (arXiv:1808.07261v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1808.07261</link>
<description rdf:parseType="Literal">&lt;p&gt;The accuracy and reliability of machine learning algorithms are an important
concern for suppliers of artificial intelligence (AI) services, but
considerations beyond accuracy, such as safety, security, and provenance, are
also critical elements to engender consumers&apos; trust in a service. In this
paper, we propose a supplier&apos;s declaration of conformity (SDoC) for AI services
to help increase trust in AI services. An SDoC is a transparent, standardized,
but often not legally required, document used in many industries and sectors to
describe the lineage of a product along with the safety and performance testing
it has undergone. We envision an SDoC for AI services to contain purpose,
performance, safety, security, and provenance information to be completed and
voluntarily released by AI service providers for examination by consumers.
Importantly, it conveys product-level rather than component-level functional
testing. We suggest a set of declaration items tailored to AI and provide
examples for two fictitious AI services.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hind_M/0/1/0/all/0/1&quot;&gt;Michael Hind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1&quot;&gt;Sameep Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mojsilovic_A/0/1/0/all/0/1&quot;&gt;Aleksandra Mojsilovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_R/0/1/0/all/0/1&quot;&gt;Ravi Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olteanu_A/0/1/0/all/0/1&quot;&gt;Alexandra Olteanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1&quot;&gt;Kush R. Varshney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07275">
<title>CentralNet: a Multilayer Approach for Multimodal Fusion. (arXiv:1808.07275v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.07275</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel multimodal fusion approach, aiming to produce
best possible decisions by integrating information coming from multiple media.
While most of the past multimodal approaches either work by projecting the
features of different modalities into the same space, or by coordinating the
representations of each modality through the use of constraints, our approach
borrows from both visions. More specifically, assuming each modality can be
processed by a separated deep convolutional network, allowing to take decisions
independently from each modality, we introduce a central network linking the
modality specific networks. This central network not only provides a common
feature embedding but also regularizes the modality specific networks through
the use of multi-task learning. The proposed approach is validated on 4
different computer vision tasks on which it consistently improves the accuracy
of existing multimodal fusion approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vielzeuf_V/0/1/0/all/0/1&quot;&gt;Valentin Vielzeuf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lechervy_A/0/1/0/all/0/1&quot;&gt;Alexis Lechervy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pateux_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Pateux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurie_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;ric Jurie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07302">
<title>Hybrid ASP-based Approach to Pattern Mining. (arXiv:1808.07302v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.07302</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting small sets of relevant patterns from a given dataset is a central
challenge in data mining. The relevance of a pattern is based on user-provided
criteria; typically, all patterns that satisfy certain criteria are considered
relevant. Rule-based languages like Answer Set Programming (ASP) seem
well-suited for specifying such criteria in a form of constraints. Although
progress has been made, on the one hand, on solving individual mining problems
and, on the other hand, developing generic mining systems, the existing methods
either focus on scalability or on generality. In this paper we make steps
towards combining local (frequency, size, cost) and global (various condensed
representations like maximal, closed, skyline) constraints in a generic and
efficient way. We present a hybrid approach for itemset, sequence and graph
mining which exploits dedicated highly optimized mining systems to detect
frequent patterns and then filters the results using declarative ASP. To
further demonstrate the generic nature of our hybrid framework we apply it to a
problem of approximately tiling a database. Experiments on real-world datasets
show the effectiveness of the proposed method and computational gains for
itemset, sequence and graph mining, as well as approximate tiling.
&lt;/p&gt;
&lt;p&gt;Under consideration in Theory and Practice of Logic Programming (TPLP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paramonov_S/0/1/0/all/0/1&quot;&gt;Sergey Paramonov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stepanova_D/0/1/0/all/0/1&quot;&gt;Daria Stepanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miettinen_P/0/1/0/all/0/1&quot;&gt;Pauli Miettinen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07431">
<title>Deep Boosted Regression for MR to CT Synthesis. (arXiv:1808.07431v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/1808.07431</link>
<description rdf:parseType="Literal">&lt;p&gt;Attenuation correction is an essential requirement of positron emission
tomography (PET) image reconstruction to allow for accurate quantification.
However, attenuation correction is particularly challenging for PET-MRI as
neither PET nor magnetic resonance imaging (MRI) can directly image tissue
attenuation properties. MRI-based computed tomography (CT) synthesis has been
proposed as an alternative to physics based and segmentation-based approaches
that assign a population-based tissue density value in order to generate an
attenuation map. We propose a novel deep fully convolutional neural network
that generates synthetic CTs in a recursive manner by gradually reducing the
residuals of the previous network, increasing the overall accuracy and
generalisability, while keeping the number of trainable parameters within
reasonable limits. The model is trained on a database of 20 pre-acquired MRI/CT
pairs and a four-fold random bootstrapped validation with a 80:20 split is
performed. Quantitative results show that the proposed framework outperforms a
state-of-the-art atlas-based approach decreasing the Mean Absolute Error (MAE)
from 131HU to 68HU for the synthetic CTs and reducing the PET reconstruction
error from 14.3% to 7.2%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Klaser_K/0/1/0/all/0/1&quot;&gt;Kerstin Kl&amp;#xe4;ser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Markiewicz_P/0/1/0/all/0/1&quot;&gt;Pawel Markiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ranzini_M/0/1/0/all/0/1&quot;&gt;Marta Ranzini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Modat_M/0/1/0/all/0/1&quot;&gt;Marc Modat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hutton_B/0/1/0/all/0/1&quot;&gt;Brian F Hutton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Atkinson_D/0/1/0/all/0/1&quot;&gt;David Atkinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Thielemans_K/0/1/0/all/0/1&quot;&gt;Kris Thielemans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cardoso_M/0/1/0/all/0/1&quot;&gt;M Jorge Cardoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;Sebastien Ourselin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07429">
<title>Sketched Answer Set Programming. (arXiv:1705.07429v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07429</link>
<description rdf:parseType="Literal">&lt;p&gt;Answer Set Programming (ASP) is a powerful modeling formalism for
combinatorial problems. However, writing ASP models is not trivial. We propose
a novel method, called Sketched Answer Set Programming (SkASP), aiming at
supporting the user in resolving this issue. The user writes an ASP program
while marking uncertain parts open with question marks. In addition, the user
provides a number of positive and negative examples of the desired program
behaviour. The sketched model is rewritten into another ASP program, which is
solved by traditional methods. As a result, the user obtains a functional and
reusable ASP program modelling her problem. We evaluate our approach on 21 well
known puzzles and combinatorial problems inspired by Karp&apos;s 21 NP-complete
problems and demonstrate a use-case for a database application based on ASP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paramonov_S/0/1/0/all/0/1&quot;&gt;Sergey Paramonov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bessiere_C/0/1/0/all/0/1&quot;&gt;Christian Bessiere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dries_A/0/1/0/all/0/1&quot;&gt;Anton Dries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1&quot;&gt;Luc De Raedt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07069">
<title>Machine learning non-local correlations. (arXiv:1808.07069v1 [quant-ph])</title>
<link>http://arxiv.org/abs/1808.07069</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to witness non-local correlations lies at the core of
foundational aspects of quantum mechanics and its application in the processing
of information. Commonly, this is achieved via the violation of Bell
inequalities. Unfortunately, however, their systematic derivation quickly
becomes unfeasible as the scenario of interest grows in complexity. To cope
with that, we propose here a machine learning approach for the detection and
quantification of non-locality. It consists of an ensemble of multilayer
perceptrons blended with genetic algorithms achieving a high performance in a
number of relevant Bell scenarios. Our results offer a novel method and a
proof-of-principle for the relevance of machine learning for understanding
non-locality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Canabarro_A/0/1/0/all/0/1&quot;&gt;Askery Canabarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Brito_S/0/1/0/all/0/1&quot;&gt;Samura&amp;#xed; Brito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chaves_R/0/1/0/all/0/1&quot;&gt;Rafael Chaves&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07105">
<title>Non-asymptotic bounds for sampling algorithms without log-concavity. (arXiv:1808.07105v1 [math.PR])</title>
<link>http://arxiv.org/abs/1808.07105</link>
<description rdf:parseType="Literal">&lt;p&gt;Discrete time analogues of ergodic stochastic differential equations (SDEs)
are one of the most popular and flexible tools for sampling high-dimensional
probability measures. Non-asymptotic analysis in the $L^2$ Wasserstein distance
of sampling algorithms based on Euler discretisations of SDEs has been recently
developed by several authors for log-concave probability distributions. In this
work we replace the log-concavity assumption with a log-concavity at infinity
condition. We provide novel $L^2$ convergence rates for Euler schemes,
expressed explicitly in terms of problem parameters. From there we derive
non-asymptotic bounds on the distance between the laws induced by Euler schemes
and the invariant laws of SDEs, both for schemes with standard and with
randomised (inaccurate) drifts. We also obtain bounds for the hierarchy of
discretisation, which enables us to deploy a multi-level Monte Carlo estimator.
Our proof relies on a novel construction of a coupling for the Markov chains
that can be used to control both the $L^1$ and $L^2$ Wasserstein distances
simultaneously. Finally, we provide a weak convergence analysis that covers
both the standard and the randomised (inaccurate) drift case. In particular, we
reveal that the variance of the randomised drift does not influence the rate of
weak convergence of the Euler scheme to the SDE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Majka_M/0/1/0/all/0/1&quot;&gt;Mateusz B. Majka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mijatovic_A/0/1/0/all/0/1&quot;&gt;Aleksandar Mijatovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Szpruch_L/0/1/0/all/0/1&quot;&gt;Lukasz Szpruch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07169">
<title>Statistical Neurodynamics of Deep Networks: Geometry of Signal Spaces. (arXiv:1808.07169v1 [cond-mat.dis-nn])</title>
<link>http://arxiv.org/abs/1808.07169</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistical neurodynamics studies macroscopic behaviors of randomly connected
neural networks. We consider a deep layered feedforward network where input
signals are processed layer by layer. The manifold of input signals is embedded
in a higher dimensional manifold of the next layer as a curved submanifold,
provided the number of neurons is larger than that of inputs. We show
geometrical features of the embedded manifold, proving that the manifold
enlarges or shrinks locally isotropically so that it is always embedded
conformally. We study the curvature of the embedded manifold. The scalar
curvature converges to a constant or diverges to infinity slowly. The distance
between two signals also changes, converging eventually to a stable fixed
value, provided both the number of neurons in a layer and the number of layers
tend to infinity. This causes a problem, since when we consider a curve in the
input space, it is mapped as a continuous curve of fractal nature, but our
theory contradictorily suggests that the curve eventually converges to a
discrete set of equally spaced points. In reality, the numbers of neurons and
layers are finite and thus, it is expected that the finite size effect causes
the discrepancies between our theory and reality. We need to further study the
discrepancies to understand their implications on information processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Amari_S/0/1/0/all/0/1&quot;&gt;Shun-ichi Amari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Karakida_R/0/1/0/all/0/1&quot;&gt;Ryo Karakida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Oizumi_M/0/1/0/all/0/1&quot;&gt;Masafumi Oizumi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07172">
<title>Fisher Information and Natural Gradient Learning of Random Deep Networks. (arXiv:1808.07172v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07172</link>
<description rdf:parseType="Literal">&lt;p&gt;A deep neural network is a hierarchical nonlinear model transforming input
signals to output signals. Its input-output relation is considered to be
stochastic, being described for a given input by a parameterized conditional
probability distribution of outputs. The space of parameters consisting of
weights and biases is a Riemannian manifold, where the metric is defined by the
Fisher information matrix. The natural gradient method uses the steepest
descent direction in a Riemannian manifold, so it is effective in learning,
avoiding plateaus. It requires inversion of the Fisher information matrix,
however, which is practically impossible when the matrix has a huge number of
dimensions. Many methods for approximating the natural gradient have therefore
been introduced. The present paper uses statistical neurodynamical method to
reveal the properties of the Fisher information matrix in a net of random
connections under the mean field approximation. We prove that the Fisher
information matrix is unit-wise block diagonal supplemented by small order
terms of off-block-diagonal elements, which provides a justification for the
quasi-diagonal natural gradient method by Y. Ollivier. A unitwise
block-diagonal Fisher metrix reduces to the tensor product of the Fisher
information matrices of single units. We further prove that the Fisher
information matrix of a single unit has a simple reduced form, a sum of a
diagonal matrix and a rank 2 matrix of weight-bias correlations. We obtain the
inverse of Fisher information explicitly. We then have an explicit form of the
natural gradient, without relying on the numerical matrix inversion, which
drastically speeds up stochastic gradient learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amari_S/0/1/0/all/0/1&quot;&gt;Shun-ichi Amari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karakida_R/0/1/0/all/0/1&quot;&gt;Ryo Karakida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oizumi_M/0/1/0/all/0/1&quot;&gt;Masafumi Oizumi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07216">
<title>Model Interpretation: A Unified Derivative-based Framework for Nonparametric Regression and Supervised Machine Learning. (arXiv:1808.07216v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.07216</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpreting a nonparametric regression model with many predictors is known
to be a challenging problem. There has been renewed interest in this topic due
to the extensive use of machine learning algorithms and the difficulty in
understanding and explaining their input-output relationships. This paper
develops a unified framework using a derivative-based approach for existing
tools in the literature, including the partial-dependence plots, marginal plots
and accumulated effects plots. It proposes a new interpretation technique
called the accumulated total derivative effects plot and demonstrates how its
components can be used to develop extensive insights in complex regression
models with correlated predictors. The techniques are illustrated through
simulation results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nair_V/0/1/0/all/0/1&quot;&gt;Vijayan Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sudjianto_A/0/1/0/all/0/1&quot;&gt;Agus Sudjianto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07217">
<title>Don&apos;t Use Large Mini-Batches, Use Local SGD. (arXiv:1808.07217v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07217</link>
<description rdf:parseType="Literal">&lt;p&gt;Mini-batch stochastic gradient methods are the current state of the art for
large-scale distributed training of neural networks and other machine learning
models. However, they fail to adapt to a changing communication vs computation
trade-off in a system, such as when scaling to a large number of workers or
devices. More so, the fixed requirement of communication bandwidth for gradient
exchange severely limits the scalability to multi-node training e.g. in
datacenters, and even more so for training on decentralized networks such as
mobile devices. We argue that variants of local SGD, which perform several
update steps on a local model before communicating to other nodes, offer
significantly improved overall performance and communication efficiency, as
well as adaptivity to the underlying system resources. Furthermore, we present
a new hierarchical extension of local SGD, and demonstrate that it can
efficiently adapt to several levels of computation costs in a heterogeneous
distributed system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1&quot;&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07226">
<title>Mean-field approximation, convex hierarchies, and the optimality of correlation rounding: a unified perspective. (arXiv:1808.07226v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07226</link>
<description rdf:parseType="Literal">&lt;p&gt;The free energy is a key quantity of interest in Ising models, but
unfortunately, computing it in general is computationally intractable. Two
popular (variational) approximation schemes for estimating the free energy of
general Ising models (in particular, even in regimes where correlation decay
does not hold) are: (i) the mean-field approximation with roots in statistical
physics, which estimates the free energy from below, and (ii) hierarchies of
convex relaxations with roots in theoretical computer science, which estimate
the free energy from above. We show, surprisingly, that the tight regime for
both methods to compute the free energy to leading order is identical.
&lt;/p&gt;
&lt;p&gt;More precisely, we show that the mean-field approximation is within
$O((n\|J\|_{F})^{2/3})$ of the free energy, where $\|J\|_F$ denotes the
Frobenius norm of the interaction matrix of the Ising model. This
simultaneously subsumes both the breakthrough work of Basak and Mukherjee, who
showed the tight result that the mean-field approximation is within $o(n)$
whenever $\|J\|_{F} = o(\sqrt{n})$, as well as the work of Jain, Koehler, and
Mossel, who gave the previously best known non-asymptotic bound of
$O((n\|J\|_{F})^{2/3}\log^{1/3}(n\|J\|_{F}))$. We give a simple, algorithmic
proof of this result using a convex relaxation proposed by Risteski based on
the Sherali-Adams hierarchy, automatically giving sub-exponential time
approximation schemes for the free energy in this entire regime. Our
algorithmic result is tight under Gap-ETH.
&lt;/p&gt;
&lt;p&gt;We furthermore combine our techniques with spin glass theory to prove (in a
strong sense) the optimality of correlation rounding, refuting a recent
conjecture of Allen, O&apos;Donnell, and Zhou. Finally, we give the tight
generalization of all of these results to $k$-MRFs, capturing as a special case
previous work on approximating MAX-$k$-CSP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vishesh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1&quot;&gt;Frederic Koehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1&quot;&gt;Andrej Risteski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07249">
<title>Analysis of Network Lasso For Semi-Supervised Regression. (arXiv:1808.07249v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.07249</link>
<description rdf:parseType="Literal">&lt;p&gt;We characterize the statistical properties of network Lasso for
semi-supervised regression problems involving network- structured data. This
characterization is based on the con- nectivity properties of the empirical
graph which encodes the similarities between individual data points. Loosely
speaking, network Lasso is accurate if the available label informa- tion is
well connected with the boundaries between clusters of the network-structure
datasets. We make this property precise using the notion of network flows. In
particular, the existence of a sufficiently large network flow over the
empirical graph implies a network compatibility condition which, in turn, en-
sures accuracy of network Lasso.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jung_A/0/1/0/all/0/1&quot;&gt;Alexander Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07258">
<title>Escaping from Collapsing Modes in a Constrained Space. (arXiv:1808.07258v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07258</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks (GANs) often suffer from unpredictable
mode-collapsing during training. We study the issue of mode collapse of
Boundary Equilibrium Generative Adversarial Network (BEGAN), which is one of
the state-of-the-art generative models. Despite its potential of generating
high-quality images, we find that BEGAN tends to collapse at some modes after a
period of training. We propose a new model, called \emph{BEGAN with a
Constrained Space} (BEGAN-CS), which includes a latent-space constraint in the
loss function. We show that BEGAN-CS can significantly improve training
stability and suppress mode collapse without either increasing the model
complexity or degrading the image quality. Further, we visualize the
distribution of latent vectors to elucidate the effect of latent-space
constraint. The experimental results show that our method has additional
advantages of being able to train on small datasets and to generate images
similar to a given real image yet with variations of designated attributes
on-the-fly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Chia-Che Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chieh Hubert Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Che-Rung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1&quot;&gt;Da-Cheng Juan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hwann-Tzong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07260">
<title>On an improvement of LASSO by scaling. (arXiv:1808.07260v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07260</link>
<description rdf:parseType="Literal">&lt;p&gt;A sparse modeling is a major topic in machine learning and statistics. LASSO
(Least Absolute Shrinkage and Selection Operator) is a popular sparse modeling
method while it has been known to yield unexpected large bias especially at a
sparse representation. There have been several studies for improving this
problem such as the introduction of non-convex regularization terms. The
important point is that this bias problem directly affects model selection in
applications since a sparse representation cannot be selected by a prediction
error based model selection even if it is a good representation. In this
article, we considered to improve this problem by introducing a scaling that
expands LASSO estimator to compensate excessive shrinkage, thus a large bias in
LASSO estimator. We here gave an empirical value for the amount of scaling.
There are two advantages of this scaling method as follows. Since the proposed
scaling value is calculated by using LASSO estimator, we only need LASSO
estimator that is obtained by a fast and stable optimization procedure such as
LARS (Least Angle Regression) under LASSO modification or coordinate descent.
And, the simplicity of our scaling method enables us to derive SURE (Stein&apos;s
Unbiased Risk Estimate) under the modified LASSO estimator with scaling. Our
scaling method together with model selection based on SURE is fully empirical
and do not need additional hyper-parameters. In a simple numerical example, we
verified that our scaling method actually improves LASSO and the SURE based
model selection criterion can stably choose an appropriate sparse model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagiwara_K/0/1/0/all/0/1&quot;&gt;Katsuyuki Hagiwara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07379">
<title>Privacy Mining from IoT-based Smart Homes. (arXiv:1808.07379v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1808.07379</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a wide range of smart devices are deployed in a variety of
environments to improve the quality of human life. One of the important
IoT-based applications is smart homes for healthcare, especially for elders.
IoT-based smart homes enable elders&apos; health to be properly monitored and taken
care of. However, elders&apos; privacy might be disclosed from smart homes due to
non-fully protected network communication or other reasons. To demonstrate how
serious this issue is, we introduce in this paper a Privacy Mining Approach
(PMA) to mine privacy from smart homes by conducting a series of deductions and
analyses on sensor datasets generated by smart homes. The experimental results
demonstrate that PMA is able to deduce a global sensor topology for a smart
home and disclose elders&apos; privacy in terms of their house layouts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Ming-Chang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jia-Chun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owe_O/0/1/0/all/0/1&quot;&gt;Olaf Owe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07380">
<title>On the Predictability of non-CGM Diabetes Data for Personalized Recommendation. (arXiv:1808.07380v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1808.07380</link>
<description rdf:parseType="Literal">&lt;p&gt;With continuous glucose monitoring (CGM), data-driven models on blood glucose
prediction have been shown to be effective in related work. However, such (CGM)
systems are not always available, e.g., for a patient at home. In this work, we
conduct a study on 9 patients and examine the predictability of data-driven
(aka. machine learning) based models on patient-level blood glucose prediction;
with measurements are taken only periodically (i.e., after several hours). To
this end, we propose several post-prediction methods to account for the noise
nature of these data, that marginally improves the performance of the end
system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rokicki_M/0/1/0/all/0/1&quot;&gt;Markus Rokicki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07382">
<title>Convergence of Cubic Regularization for Nonconvex Optimization under KL Property. (arXiv:1808.07382v1 [math.OC])</title>
<link>http://arxiv.org/abs/1808.07382</link>
<description rdf:parseType="Literal">&lt;p&gt;Cubic-regularized Newton&apos;s method (CR) is a popular algorithm that guarantees
to produce a second-order stationary solution for solving nonconvex
optimization problems. However, existing understandings of the convergence rate
of CR are conditioned on special types of geometrical properties of the
objective function. In this paper, we explore the asymptotic convergence rate
of CR by exploiting the ubiquitous Kurdyka-Lojasiewicz (KL) property of
nonconvex objective functions. In specific, we characterize the asymptotic
convergence rate of various types of optimality measures for CR including
function value gap, variable distance gap, gradient norm and least eigenvalue
of the Hessian matrix. Our results fully characterize the diverse convergence
behaviors of these optimality measures in the full parameter regime of the KL
property. Moreover, we show that the obtained asymptotic convergence rates of
CR are order-wise faster than those of first-order gradient descent algorithms
under the KL property.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingbin Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07412">
<title>Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks. (arXiv:1808.07412v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1808.07412</link>
<description rdf:parseType="Literal">&lt;p&gt;Statically estimating the number of processor clock cycles it takes to
execute a basic block of assembly instructions in steady state (throughput) is
important for compiler backend optimizations such as register allocation,
instruction selection and instruction scheduling. This is complicated specially
in modern x86-64 Complex Instruction Set Computer (CISC) machines with
sophisticated processor microarchitectures. Traditionally, compiler writers
invest time experimenting and referring to processor manuals to analytically
model modern processors with incomplete specifications. This is tedious, error
prone and should be done for each processor generation. We present Ithemal, the
first automatically learnt estimator to statically predict throughput of a set
of basic block instructions using machine learning. Ithemal uses a novel
Directed Acyclic Graph-Recurrent Neural Network (DAG-RNN) based data-driven
approach for throughput estimation. We show that Ithemal is accurate than
state-of-the-art hand written tools used in compiler backends and static
machine code analyzers. In particular, our model has a worst case average error
of 10.53% on actual throughput values when compared to best case average errors
of 19.57% for the LLVM scheduler (llvm-mca) and 22.51% for IACA, Intel&apos;s
machine code analyzer when compared on three different microarchitectures,
while predicting throughput values at a faster rate than aforementioned tools.
We also show that Ithemal is portable, learning throughput estimation for Intel
Nehalem, Haswell and Skylake microarchitectures without requiring changes to
its structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendis_C/0/1/0/all/0/1&quot;&gt;Charith Mendis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amarasinghe_S/0/1/0/all/0/1&quot;&gt;Saman Amarasinghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbin_M/0/1/0/all/0/1&quot;&gt;Michael Carbin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07440">
<title>3D Topology Optimization using Convolutional Neural Networks. (arXiv:1808.07440v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.07440</link>
<description rdf:parseType="Literal">&lt;p&gt;Topology optimization is computationally demanding that requires the assembly
and solution to a finite element problem for each material distribution
hypothesis. As a complementary alternative to the traditional physics-based
topology optimization, we explore a data-driven approach that can quickly
generate accurate solutions. To this end, we propose a deep learning approach
based on a 3D encoder-decoder Convolutional Neural Network architecture for
accelerating 3D topology optimization and to determine the optimal
computational strategy for its deployment. Analysis of iteration-wise progress
of the Solid Isotropic Material with Penalization process is used as a
guideline to study how the earlier steps of the conventional topology
optimization can be used as input for our approach to predict the final
optimized output structure directly from this input. We conduct a comparative
study between multiple strategies for training the neural network and assess
the effect of using various input combinations for the CNN to finalize the
strategy with the highest accuracy in predictions for practical deployment. For
the best performing network, we achieved about 40% reduction in overall
computation time while also attaining structural accuracies in the order of
96%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banga_S/0/1/0/all/0/1&quot;&gt;Saurabh Banga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehani_H/0/1/0/all/0/1&quot;&gt;Harsh Gehani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhilare_S/0/1/0/all/0/1&quot;&gt;Sanket Bhilare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Sagar Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kara_L/0/1/0/all/0/1&quot;&gt;Levent Kara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.02703">
<title>Nonparametric Bayesian inference of the microcanonical stochastic block model. (arXiv:1610.02703v4 [physics.data-an] UPDATED)</title>
<link>http://arxiv.org/abs/1610.02703</link>
<description rdf:parseType="Literal">&lt;p&gt;A principled approach to characterize the hidden structure of networks is to
formulate generative models, and then infer their parameters from data. When
the desired structure is composed of modules or &quot;communities&quot;, a suitable
choice for this task is the stochastic block model (SBM), where nodes are
divided into groups, and the placement of edges is conditioned on the group
memberships. Here, we present a nonparametric Bayesian method to infer the
modular structure of empirical networks, including the number of modules and
their hierarchical organization. We focus on a microcanonical variant of the
SBM, where the structure is imposed via hard constraints, i.e. the generated
networks are not allowed to violate the patterns imposed by the model. We show
how this simple model variation allows simultaneously for two important
improvements over more traditional inference approaches: 1. Deeper Bayesian
hierarchies, with noninformative priors replaced by sequences of priors and
hyperpriors, that not only remove limitations that seriously degrade the
inference on large networks, but also reveal structures at multiple scales; 2.
A very efficient inference algorithm that scales well not only for networks
with a large number of nodes and edges, but also with an unlimited number of
modules. We show also how this approach can be used to sample modular
hierarchies from the posterior distribution, as well as to perform model
selection. We discuss and analyze the differences between sampling from the
posterior and simply finding the single parameter estimate that maximizes it.
Furthermore, we expose a direct equivalence between our microcanonical approach
and alternative derivations based on the canonical SBM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Peixoto_T/0/1/0/all/0/1&quot;&gt;Tiago P. Peixoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.02419">
<title>Estimating Mixture Entropy with Pairwise Distances. (arXiv:1706.02419v4 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1706.02419</link>
<description rdf:parseType="Literal">&lt;p&gt;Mixture distributions arise in many parametric and non-parametric settings --
for example, in Gaussian mixture models and in non-parametric estimation. It is
often necessary to compute the entropy of a mixture, but, in most cases, this
quantity has no closed-form expression, making some form of approximation
necessary. We propose a family of estimators based on a pairwise distance
function between mixture components, and show that this estimator class has
many attractive properties. For many distributions of interest, the proposed
estimators are efficient to compute, differentiable in the mixture parameters,
and become exact when the mixture components are clustered. We prove this
family includes lower and upper bounds on the mixture entropy. The Chernoff
$\alpha$-divergence gives a lower bound when chosen as the distance function,
with the Bhattacharyya distance providing the tightest lower bound for
components that are symmetric and members of a location family. The
Kullback-Leibler divergence gives an upper bound when used as the distance
function. We provide closed-form expressions of these bounds for mixtures of
Gaussians, and discuss their applications to the estimation of mutual
information. We then demonstrate that our bounds are significantly tighter than
well-known existing bounds using numeric simulations. This estimator class is
very useful in optimization problems involving maximization/minimization of
entropy and mutual information, such as MaxEnt and rate distortion problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolchinsky_A/0/1/0/all/0/1&quot;&gt;Artemy Kolchinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tracey_B/0/1/0/all/0/1&quot;&gt;Brendan D. Tracey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05236">
<title>MORF: A Framework for Predictive Modeling and Replication At Scale With Privacy-Restricted MOOC Data. (arXiv:1801.05236v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/1801.05236</link>
<description rdf:parseType="Literal">&lt;p&gt;Big data repositories from online learning platforms such as Massive Open
Online Courses (MOOCs) represent an unprecedented opportunity to advance
research on education at scale and impact a global population of learners. To
date, such research has been hindered by poor reproducibility and a lack of
replication, largely due to three types of barriers: experimental, inferential,
and data. We present a novel system for large-scale computational research, the
MOOC Replication Framework (MORF), to jointly address these barriers. We
discuss MORF&apos;s architecture, an open-source platform-as-a-service (PaaS) which
includes a simple, flexible software API providing for multiple modes of
research (predictive modeling or production rule analysis) integrated with a
high-performance computing environment. All experiments conducted on MORF use
executable Docker containers which ensure complete reproducibility while
allowing for the use of any software or language which can be installed in the
linux-based Docker container. Each experimental artifact is assigned a DOI and
made publicly available. MORF has the potential to accelerate and democratize
research on its massive data repository, which currently includes over 200
MOOCs, as demonstrated by initial research conducted on the platform. We also
highlight ways in which MORF represents a solution template to a more general
class of problems faced by computational researchers in other domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Josh Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brooks_C/0/1/0/all/0/1&quot;&gt;Christopher Brooks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andres_J/0/1/0/all/0/1&quot;&gt;Juan Miguel L. Andres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baker_R/0/1/0/all/0/1&quot;&gt;Ryan Baker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04577">
<title>Feature-Based Aggregation and Deep Reinforcement Learning: A Survey and Some New Implementations. (arXiv:1804.04577v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04577</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we discuss policy iteration methods for approximate solution of
a finite-state discounted Markov decision problem, with a focus on
feature-based aggregation methods and their connection with deep reinforcement
learning schemes. We introduce features of the states of the original problem,
and we formulate a smaller &quot;aggregate&quot; Markov decision problem, whose states
relate to the features. We discuss properties and possible implementations of
this type of aggregation, including a new approach to approximate policy
iteration. In this approach the policy improvement operation combines
feature-based aggregation with feature construction using deep neural networks
or other calculations. We argue that the cost function of a policy may be
approximated much more accurately by the nonlinear function of the features
provided by aggregation, than by the linear function of the features provided
by neural network-based reinforcement learning, thereby potentially leading to
more effective policy improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertsekas_D/0/1/0/all/0/1&quot;&gt;Dimitri P. Bertsekas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11470">
<title>Deep Encoder-Decoder Models for Unsupervised Learning of Controllable Speech Synthesis. (arXiv:1807.11470v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/1807.11470</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating versatile and appropriate synthetic speech requires control over
the output expression separate from the spoken text. Important non-textual
speech variation is seldom annotated, in which case output control must be
learned in an unsupervised fashion. In this paper, we perform an in-depth study
of methods for unsupervised learning of control in statistical speech
synthesis. For example, we show that popular unsupervised training heuristics
can be interpreted as variational inference in certain autoencoder models. We
additionally connect these models to VQ-VAEs, another, recently-proposed class
of deep variational autoencoders, which we show can be derived from a very
similar mathematical argument. The implications of these new probabilistic
interpretations are discussed. We illustrate the utility of the various
approaches with an application to acoustic modelling for emotional speech
synthesis, where the unsupervised methods for learning expression control
(without access to emotional labels) are found to give results that in many
aspects match or surpass the previous best supervised approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Henter_G/0/1/0/all/0/1&quot;&gt;Gustav Eje Henter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1&quot;&gt;Jaime Lorenzo-Trueba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05527">
<title>Deep Learning for Energy Markets. (arXiv:1808.05527v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1808.05527</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning (DL) provides a methodology to predict extreme loads observed
in energy grids. Forecasting energy loads and prices is challenging due to
sharp peaks and troughs that arise from intraday system constraints due to
supply and demand fluctuations. We propose deep spatio-temporal models and
extreme value theory (DL-EVT) to capture the tail behavior of load spikes. Deep
architectures, such as ReLU and LSTM can model generation trends and temporal
dependencies while EVT captures highly volatile load spikes. To illustrate our
methodology, we use hourly price and demand data from the PJM interconnection
for 4719 nodes and we develop a deep predictor. DL-EVT outperforms traditional
Fourier and time series methods, both in-and out-of-sample, by capturing the
nonlinearities in prices. Finally, we conclude with directions for future
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Polson_M/0/1/0/all/0/1&quot;&gt;Michael Polson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sokolov_V/0/1/0/all/0/1&quot;&gt;Vadim Sokolov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06314">
<title>A General Framework of Multi-Armed Bandit Processes by Arm Switch Restrictions. (arXiv:1808.06314v2 [math.PR] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06314</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a general framework of multi-armed bandit (MAB) processes
by introducing a type of restrictions on the switches among arms evolving in
continuous time.
&lt;/p&gt;
&lt;p&gt;The Gittins index process is constructed for any single arm subject to the
restrictions on switches and then the optimality of the corresponding Gittins
index rule is established. The Gittins indices defined in this paper are
consistent with the ones for MAB processes in continuous time, integer time,
semi-Markovian setting as well as general discrete time setting, so that the
new theory covers the classical models as special cases and also applies to
many other situations that have not yet been touched in the literature. While
the proof of the optimality of Gittins index policies benefits from ideas in
the existing theory of MAB processes in continuous time, new techniques are
introduced which drastically simplify the proof.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bao_W/0/1/0/all/0/1&quot;&gt;Wenqing Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xiaoqiang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xianyi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06576">
<title>Peptide-Spectra Matching from Weak Supervision. (arXiv:1808.06576v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06576</link>
<description rdf:parseType="Literal">&lt;p&gt;As in many other scientific domains, we face a fundamental problem when using
machine learning to identify proteins from mass spectrometry data: large ground
truth datasets mapping inputs to correct outputs are extremely difficult to
obtain. Instead, we have access to imperfect hand-coded models crafted by
domain experts. In this paper, we apply deep neural networks to an important
step of the protein identification problem, the pairing of mass spectra with
short sequences of amino acids called peptides. We train our model to
differentiate between top scoring results from a state-of-the art classical
system and hard-negative second and third place results. Our resulting model is
much better at identifying peptides with spectra than the model used to
generate its training data. In particular, we achieve a 43% improvement over
standard matching methods and a 10% improvement over a combination of the
matching method and an industry standard cross-spectra reranking tool.
Importantly, in a more difficult experimental regime that reflects current
challenges facing biologists, our advantage over the previous state-of-the-art
grows to 15% even after reranking. We believe this approach will generalize to
other challenging scientific problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Schoenholz_S/0/1/0/all/0/1&quot;&gt;Samuel S. Schoenholz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hackett_S/0/1/0/all/0/1&quot;&gt;Sean Hackett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Deming_L/0/1/0/all/0/1&quot;&gt;Laura Deming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Melamud_E/0/1/0/all/0/1&quot;&gt;Eugene Melamud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jaitly_N/0/1/0/all/0/1&quot;&gt;Navdeep Jaitly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+McAllister_F/0/1/0/all/0/1&quot;&gt;Fiona McAllister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+OBrien_J/0/1/0/all/0/1&quot;&gt;Jonathon O&amp;#x27;Brien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dahl_G/0/1/0/all/0/1&quot;&gt;George Dahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bennett_B/0/1/0/all/0/1&quot;&gt;Bryson Bennett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew M. Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Koller_D/0/1/0/all/0/1&quot;&gt;Daphne Koller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06638">
<title>Supervised Kernel PCA For Longitudinal Data. (arXiv:1808.06638v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06638</link>
<description rdf:parseType="Literal">&lt;p&gt;In statistical learning, high covariate dimensionality poses challenges for
robust prediction and inference. To address this challenge, supervised
dimension reduction is often performed, where dependence on the outcome is
maximized for a selected covariate subspace with smaller dimensionality.
Prevalent dimension reduction techniques assume data are $i.i.d.$, which is not
appropriate for longitudinal data comprising multiple subjects with repeated
measurements over time. In this paper, we derive a decomposition of the
Hilbert-Schmidt Independence Criterion as a supervised loss function for
longitudinal data, enabling dimension reduction between and within clusters
separately, and propose a dimensionality-reduction technique, $sklPCA$, that
performs this decomposed dimension reduction. We also show that this technique
yields superior model accuracy compared to the model it extends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Staples_P/0/1/0/all/0/1&quot;&gt;Patrick Staples&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ouyang_M/0/1/0/all/0/1&quot;&gt;Min Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dougherty_R/0/1/0/all/0/1&quot;&gt;Robert F. Dougherty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ryslik_G/0/1/0/all/0/1&quot;&gt;Gregory A. Ryslik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dagum_P/0/1/0/all/0/1&quot;&gt;Paul Dagum&lt;/a&gt;</dc:creator>
</item></rdf:RDF>