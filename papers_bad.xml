<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-31T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10062"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07878"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02781"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10043"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10054"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10070"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10248"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10280"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.09198"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04555"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01244"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11135"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07752"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1604.05225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10024"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10087"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10110"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10277"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1502.01368"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.02010"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.02556"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.10062">
<title>Multi-timescale memory dynamics in a reinforcement learning network with attention-gated memory. (arXiv:1712.10062v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1712.10062</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning and memory are intertwined in our brain and their relationship is at
the core of several recent neural network models. In particular, the
Attention-Gated MEmory Tagging model (AuGMEnT) is a reinforcement learning
network with an emphasis on biological plausibility of memory dynamics and
learning. We find that the AuGMEnT network does not solve some hierarchical
tasks, where higher-level stimuli have to be maintained over a long time, while
lower-level stimuli need to be remembered and forgotten over a shorter
timescale. To overcome this limitation, we introduce hybrid AuGMEnT, with leaky
or short-timescale and non-leaky or long-timescale units in memory, that allow
to exchange lower-level information while maintaining higher-level one, thus
solving both hierarchical and distractor tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Martinolli_M/0/1/0/all/0/1&quot;&gt;Marco Martinolli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gerstner_W/0/1/0/all/0/1&quot;&gt;Wulfram Gerstner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gilra_A/0/1/0/all/0/1&quot;&gt;Aditya Gilra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07878">
<title>TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning. (arXiv:1705.07878v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07878</link>
<description rdf:parseType="Literal">&lt;p&gt;High network communication cost for synchronizing gradients and parameters is
the well-known bottleneck of distributed training. In this work, we propose
TernGrad that uses ternary gradients to accelerate distributed deep learning in
data parallelism. Our approach requires only three numerical levels {-1,0,1},
which can aggressively reduce the communication time. We mathematically prove
the convergence of TernGrad under the assumption of a bound on gradients.
Guided by the bound, we propose layer-wise ternarizing and gradient clipping to
improve its convergence. Our experiments show that applying TernGrad on AlexNet
does not incur any accuracy loss and can even improve accuracy. The accuracy
loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a
performance model is proposed to study the scalability of TernGrad. Experiments
show significant speed gains for various deep neural networks. Our source code
is available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wei Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Cong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_F/0/1/0/all/0/1&quot;&gt;Feng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chunpeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yandan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiran Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hai Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02781">
<title>On Usage of Autoencoders and Siamese Networks for Online Handwritten Signature Verification. (arXiv:1712.02781v2 [cs.NE] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.02781</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel writer-independent global feature
extraction framework for the task of automatic signature verification which
aims to make robust systems for automatically distinguishing negative and
positive samples. Our method consists of an autoencoder for modeling the sample
space into a fixed length latent space and a Siamese Network for classifying
the fixed-length samples obtained from the autoencoder based on the reference
samples of a subject as being &quot;Genuine&quot; or &quot;Forged.&quot; During our experiments,
usage of Attention Mechanism and applying Downsampling significantly improved
the accuracy of the proposed framework. We evaluated our proposed framework
using SigWiComp2013 Japanese and GPDSsyntheticOnLineOffLineSignature datasets.
On the SigWiComp2013 Japanese dataset, we achieved 8.65% EER that means 1.2%
relative improvement compared to the best-reported result. Furthermore, on the
GPDSsyntheticOnLineOffLineSignature dataset, we achieved average EERs of 0.13%,
0.12%, 0.21% and 0.25% respectively for 150, 300, 1000 and 2000 test subjects
which indicates improvement of relative EER on the best-reported result by
95.67%, 95.26%, 92.9% and 91.52% respectively. Apart from the accuracy gain,
because of the nature of our proposed framework which is based on neural
networks and consequently is as simple as some consecutive matrix
multiplications, it has less computational cost than conventional methods such
as DTW and could be used concurrently on devices such as GPU, TPU, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahrabian_K/0/1/0/all/0/1&quot;&gt;Kian Ahrabian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babaali_B/0/1/0/all/0/1&quot;&gt;Bagher Babaali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10043">
<title>Robust Covariate Shift Prediction with General Losses and Feature Views. (arXiv:1712.10043v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.10043</link>
<description rdf:parseType="Literal">&lt;p&gt;Covariate shift relaxes the widely-employed independent and identically
distributed (IID) assumption by allowing different training and testing input
distributions. Unfortunately, common methods for addressing covariate shift by
trying to remove the bias between training and testing distributions using
importance weighting often provide poor performance guarantees in theory and
unreliable predictions with high variance in practice. Recently developed
methods that construct a predictor that is inherently robust to the
difficulties of learning under covariate shift are restricted to minimizing
logloss and can be too conservative when faced with high-dimensional learning
tasks. We address these limitations in two ways: by robustly minimizing various
loss functions, including non-convex ones, under the testing distribution; and
by separately shaping the influence of covariate shift according to different
feature-based views of the relationship between input variables and example
labels. These generalizations make robust covariate shift prediction applicable
to more task scenarios. We demonstrate the benefits on classification under
covariate shift tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziebart_B/0/1/0/all/0/1&quot;&gt;Brian D. Ziebart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10054">
<title>Corpus specificity in LSA and Word2vec: the role of out-of-domain documents. (arXiv:1712.10054v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1712.10054</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent Semantic Analysis (LSA) and Word2vec are some of the most widely used
word embeddings. Despite the popularity of these techniques, the precise
mechanisms by which they acquire new semantic relations between words remain
unclear. In the present article we investigate whether LSA and Word2vec
capacity to identify relevant semantic dimensions increases with size of
corpus. One intuitive hypothesis is that the capacity to identify relevant
dimensions should increase as the amount of data increases. However, if corpus
size grow in topics which are not specific to the domain of interest, signal to
noise ratio may weaken. Here we set to examine and distinguish these
alternative hypothesis. To investigate the effect of corpus specificity and
size in word-embeddings we study two ways for progressive elimination of
documents: the elimination of random documents vs. the elimination of documents
unrelated to a specific task. We show that Word2vec can take advantage of all
the documents, obtaining its best performance when it is trained with the whole
corpus. On the contrary, the specialization (removal of out-of-domain
documents) of the training corpus, accompanied by a decrease of dimensionality,
can increase LSA word-representation quality while speeding up the processing
time. Furthermore, we show that the specialization without the decrease in LSA
dimensionality can produce a strong performance reduction in specific tasks.
From a cognitive-modeling point of view, we point out that LSA&apos;s word-knowledge
acquisitions may not be efficiently exploiting higher-order co-occurrences and
global relations, whereas Word2vec does.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altszyler_E/0/1/0/all/0/1&quot;&gt;Edgar Altszyler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigman_M/0/1/0/all/0/1&quot;&gt;Mariano Sigman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slezak_D/0/1/0/all/0/1&quot;&gt;Diego Fernandez Slezak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10070">
<title>Reinforcement Learning with Analogical Similarity to Guide Schema Induction and Attention. (arXiv:1712.10070v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.10070</link>
<description rdf:parseType="Literal">&lt;p&gt;Research in analogical reasoning suggests that higher-order cognitive
functions such as abstract reasoning, far transfer, and creativity are founded
on recognizing structural similarities among relational systems. Here we
integrate theories of analogy with the computational framework of reinforcement
learning (RL). We propose a psychology theory that is a computational synergy
between analogy and RL, in which analogical comparison provides the RL learning
algorithm with a measure of relational similarity, and RL provides feedback
signals that can drive analogical learning. Simulation results support the
power of this approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1&quot;&gt;James M. Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1&quot;&gt;Matt Jones&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10248">
<title>Deep Learning Interior Tomography for Region-of-Interest Reconstruction. (arXiv:1712.10248v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.10248</link>
<description rdf:parseType="Literal">&lt;p&gt;Interior tomography for the region-of-interest (ROI) imaging has advantages
of using a small detector and reducing X-ray radiation dose. However, standard
analytic reconstruction suffers from severe cupping artifacts due to existence
of null space in the truncated Radon transform. Existing penalized
reconstruction methods may address this problem but they require extensive
computations due to the iterative reconstruction. Inspired by the recent deep
learning approaches to low-dose and sparse view CT, here we propose a deep
learning architecture that removes null space signals from the FBP
reconstruction. Experimental results have shown that the proposed method
provides near-perfect reconstruction with about 7-10 dB improvement in PSNR
over existing methods in spite of significantly reduced run-time complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yoseob Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jawook Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10280">
<title>First Draft on the xInf Model for Universal Physical Computation and Reverse Engineering of Natural Intelligence. (arXiv:1712.10280v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1712.10280</link>
<description rdf:parseType="Literal">&lt;p&gt;Turing Machines are universal computing machines in theory. It has been a
long debate whether Turing Machines can simulate the consciousness mind
behaviors in the materialistic universe. Three different hypotheses come out of
such debate, in short:(A) Can; (B) Cannot; (C) Super-Turing machines can.
Because Turing Machines or other kinds of theoretical computing models are
abstract objects while behaviors are real observables, this debate involves at
least three distinct fields of science and technology: physics, computer
engineering, and experimental neuroscience. However, the languages used in
these different fields are highly heterogeneous and not easily interpretable
for each other, making it very difficult to reach partial agreements regarding
this debate, Therefore, the main goal of this manuscript is to establish a
proper language that can translate among those different fields. First, I
propose a theoretical model for analyzing how theoretical computing machines
would physically run in physical time. This model, termed as the xInf, is at
first place Turing-complete in theory, and depending on the properties of
physical time, it can be either Turing-equivalent or Super-Turing in the
physical universe. The xInf Model is demonstrated to be a suitable universal
language to translate among physics, computer engineering, and neuroscience.
Finally, I propose a conjecture that there exists a Minimal Complete Set of
rules in the xInf Model that enables the construction of a physical machine
using inorganic materials that can pass the Turing Test in physical time. I
cannot demonstrate whether such a conjecture to be testified or falsified on
paper using finite-order logic, my only solution is physical time itself, i.e.
an evolutionary competition will eventually tell the conclusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jia_H/0/1/0/all/0/1&quot;&gt;Hongbo Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.09198">
<title>Data-Driven Stochastic Robust Optimization: A General Computational Framework and Algorithm for Optimization under Uncertainty in the Big Data Era. (arXiv:1707.09198v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1707.09198</link>
<description rdf:parseType="Literal">&lt;p&gt;A novel data-driven stochastic robust optimization (DDSRO) framework is
proposed for optimization under uncertainty leveraging labeled multi-class
uncertainty data. Uncertainty data in large datasets are often collected from
various conditions, which are encoded by class labels. Machine learning methods
including Dirichlet process mixture model and maximum likelihood estimation are
employed for uncertainty modeling. A DDSRO framework is further proposed based
on the data-driven uncertainty model through a bi-level optimization structure.
The outer optimization problem follows a two-stage stochastic programming
approach to optimize the expected objective across different data classes;
adaptive robust optimization is nested as the inner problem to ensure the
robustness of the solution while maintaining computational tractability. A
decomposition-based algorithm is further developed to solve the resulting
multi-level optimization problem efficiently. Case studies on process network
design and planning are presented to demonstrate the applicability of the
proposed framework and algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_C/0/1/0/all/0/1&quot;&gt;Chao Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_F/0/1/0/all/0/1&quot;&gt;Fengqi You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04555">
<title>Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network. (arXiv:1709.04555v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04555</link>
<description rdf:parseType="Literal">&lt;p&gt;The prediction of organic reaction outcomes is a fundamental problem in
computational chemistry. Since a reaction may involve hundreds of atoms, fully
exploring the space of possible transformations is intractable. The current
solution utilizes reaction templates to limit the space, but it suffers from
coverage and efficiency issues. In this paper, we propose a template-free
approach to efficiently explore the space of product molecules by first
pinpointing the reaction center -- the set of nodes and edges where graph edits
occur. Since only a small number of atoms contribute to reaction center, we can
directly enumerate candidate products. The generated candidates are scored by a
Weisfeiler-Lehman Difference Network that models high-order interactions
between changes occurring at nodes across the molecule. Our framework
outperforms the top-performing template-based approach with a 10\% margin,
while running orders of magnitude faster. Finally, we demonstrate that the
model accuracy rivals the performance of domain experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Wengong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coley_C/0/1/0/all/0/1&quot;&gt;Connor W. Coley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1&quot;&gt;Regina Barzilay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1&quot;&gt;Tommi Jaakkola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01244">
<title>Lifelong Learning by Adjusting Priors. (arXiv:1711.01244v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01244</link>
<description rdf:parseType="Literal">&lt;p&gt;In representational lifelong learning an agent aims to learn to solve novel
tasks while updating its representation in light of previous tasks. Under the
assumption that future tasks are `related&apos; to previous tasks, representations
should be learned in such a way that they capture the common structure across
learned tasks, while allowing the learner sufficient flexibility to adapt to
novel aspects of a new task. We develop a framework for lifelong learning in
deep neural networks that is based on generalization bounds, developed within
the PAC-Bayes framework. Learning takes place through the construction of a
distribution over networks based on the tasks seen so far, and its utilization
for learning a new task. Thus, prior knowledge is incorporated through setting
a history-dependent prior for novel tasks. We develop a gradient-based
algorithm implementing these ideas, based on minimizing an objective function
motivated by generalization bounds, and demonstrate its effectiveness through
numerical examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Amit_R/0/1/0/all/0/1&quot;&gt;Ron Amit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meir_R/0/1/0/all/0/1&quot;&gt;Ron Meir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11135">
<title>Video Captioning via Hierarchical Reinforcement Learning. (arXiv:1711.11135v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11135</link>
<description rdf:parseType="Literal">&lt;p&gt;Video captioning is the task of automatically generating a textual
description of the actions in a video. Although previous work (e.g.
sequence-to-sequence model) has shown promising results in abstracting a coarse
description of a short video, it is still very challenging to caption a video
containing multiple fine-grained actions with a detailed description. This
paper aims to address the challenge by proposing a novel hierarchical
reinforcement learning framework for video captioning, where a high-level
Manager module learns to design sub-goals and a low-level Worker module
recognizes the primitive actions to fulfill the sub-goal. With this
compositional framework to reinforce video captioning at different levels, our
approach significantly outperforms all the baseline methods on a newly
introduced large-scale dataset for fine-grained video captioning. Furthermore,
our non-ensemble model has already achieved the state-of-the-art results on the
widely-used MSR-VTT dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiawei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuan-Fang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07752">
<title>Towards an unanimous international regulatory body for responsible use of Artificial Intelligence [UIRB-AI]. (arXiv:1712.07752v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07752</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI), is once again in the phase of drastic
advancements. Unarguably, the technology itself can revolutionize the way we
live our everyday life. But the exponential growth of technology poses a
daunting task for policy researchers and law makers in making amendments to the
existing norms. In addition, not everyone in the society is studying the
potential socio-economic intricacies and cultural drifts that AI can bring
about. It is prudence to reflect from our historical past to propel the
development of technology in the right direction. To benefit the society of the
present and future, I scientifically explore the societal impact of AI. While
there are many public and private partnerships working on similar aspects, here
I describe the necessity for an Unanimous International Regulatory Body for all
applications of AI (UIRB-AI). I also discuss the benefits and drawbacks of such
an organization. To combat any drawbacks in the formation of an UIRB-AI, both
idealistic and pragmatic perspectives are discussed alternatively. The paper
further advances the discussion by proposing novel policies on how such
organization should be structured and how it can bring about a win-win
situation for everyone in the society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chidambaram_R/0/1/0/all/0/1&quot;&gt;Rajesh Chidambaram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1604.05225">
<title>Annotation Order Matters: Recurrent Image Annotator for Arbitrary Length Image Tagging. (arXiv:1604.05225v3 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1604.05225</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic image annotation has been an important research topic in
facilitating large scale image management and retrieval. Existing methods focus
on learning image-tag correlation or correlation between tags to improve
annotation accuracy. However, most of these methods evaluate their performance
using top-k retrieval performance, where k is fixed. Although such setting
gives convenience for comparing different methods, it is not the natural way
that humans annotate images. The number of annotated tags should depend on
image contents. Inspired by the recent progress in machine translation and
image captioning, we propose a novel Recurrent Image Annotator (RIA) model that
forms image annotation task as a sequence generation problem so that RIA can
natively predict the proper length of tags according to image contents. We
evaluate the proposed model on various image annotation datasets. In addition
to comparing our model with existing methods using the conventional top-k
evaluation measures, we also provide our model as a high quality baseline for
the arbitrary length image tagging task. Moreover, the results of our
experiments show that the order of tags in training phase has a great impact on
the final annotation performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jiren Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakayama_H/0/1/0/all/0/1&quot;&gt;Hideki Nakayama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10024">
<title>Machine Learning for Partial Identification: Example of Bracketed Data. (arXiv:1712.10024v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.10024</link>
<description rdf:parseType="Literal">&lt;p&gt;Partially identified models occur commonly in economic applications. A common
problem in this literature is a regression problem with bracketed
(interval-censored) outcome variable Y, which creates a set-identified
parameter of interest. The recent studies have only considered
finite-dimensional linear regression in such context. To incorporate more
complex controls into the problem, we consider a partially linear projection of
Y on the set functions that are linear in treatment/policy variables and
nonlinear in the controls. We characterize the identified set for the linear
component of this projection and propose an estimator of its support function.
Our estimator converges at parametric rate and has asymptotic normality
properties. It may be useful for labor economics applications that involve
bracketed salaries and rich, high-dimensional demographic data about the
subjects of the study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Semenova_V/0/1/0/all/0/1&quot;&gt;Vira Semenova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10087">
<title>Finite-sample risk bounds for maximum likelihood estimation with arbitrary penalties. (arXiv:1712.10087v1 [math.ST])</title>
<link>http://arxiv.org/abs/1712.10087</link>
<description rdf:parseType="Literal">&lt;p&gt;The MDL two-part coding $ \textit{index of resolvability} $ provides a
finite-sample upper bound on the statistical risk of penalized likelihood
estimators over countable models. However, the bound does not apply to
unpenalized maximum likelihood estimation or procedures with exceedingly small
penalties. In this paper, we point out a more general inequality that holds for
arbitrary penalties. In addition, this approach makes it possible to derive
exact risk bounds of order $1/n$ for iid parametric models, which improves on
the order $(\log n)/n$ resolvability bounds. We conclude by discussing
implications for adaptive estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Brinda_W/0/1/0/all/0/1&quot;&gt;W. D. Brinda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Klusowski_J/0/1/0/all/0/1&quot;&gt;Jason M. Klusowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10110">
<title>Beyond Keywords and Relevance: A Personalized Ad Retrieval Framework in E-Commerce Sponsored Search. (arXiv:1712.10110v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1712.10110</link>
<description rdf:parseType="Literal">&lt;p&gt;On most sponsored search platforms, advertisers bid on some keywords for
their advertisements (ads). Given a search request, ad retrieval module
rewrites the query into bidding keywords, and uses these keywords as keys to
select Top N ads through inverted indexes. In this way, an ad will not be
retrieved even if queries are related when the advertiser does not bid on
corresponding keywords. Moreover, most ad retrieval approaches regard rewriting
and ad-selecting as two separated tasks, and focus on boosting relevance
between search queries and ads. Recently, in e-commerce sponsored search more
and more personalized information has been introduced, such as user profiles,
long-time and real-time clicks. Personalized information makes ad retrieval
able to employ more elements (e.g. real-time clicks) as search signals and
retrieval keys, however it makes ad retrieval more difficult to measure ads
retrieved through different signals. To address these problems, we propose a
novel ad retrieval framework beyond keywords and relevance in e-commerce
sponsored search. Firstly, we employ historical ad click data to initialize a
hierarchical network representing signals, keys and ads, in which personalized
information is introduced. Then we train a model on top of the hierarchical
network by learning the weights of edges. Finally we select the best edges
according to the model, boosting RPM/CTR. Experimental results on our
e-commerce platform demonstrate that our ad retrieval framework achieves good
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Su Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianshu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1&quot;&gt;Daorui Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kaipeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bo Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10277">
<title>A Stochastic Trust Region Algorithm. (arXiv:1712.10277v1 [math.OC])</title>
<link>http://arxiv.org/abs/1712.10277</link>
<description rdf:parseType="Literal">&lt;p&gt;An algorithm is proposed for solving stochastic and finite sum minimization
problems. Based on a trust region methodology, the algorithm employs normalized
steps, at least as long as the norms of the stochastic gradient estimates are
within a user-defined interval. The complete algorithm---which dynamically
chooses whether or not to employ normalized steps---is proved to have
convergence guarantees that are similar to those possessed by a traditional
stochastic gradient approach under various sets of conditions related to the
accuracy of the stochastic gradient estimates and choice of stepsize sequence.
The results of numerical experiments are presented when the method is employed
to minimize convex and nonconvex machine learning test problems, illustrating
that the method can outperform a traditional stochastic gradient approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Curtis_F/0/1/0/all/0/1&quot;&gt;Frank E. Curtis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Scheinberg_K/0/1/0/all/0/1&quot;&gt;Katya Scheinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shi_R/0/1/0/all/0/1&quot;&gt;Rui Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1502.01368">
<title>Sparse Representation Classification Beyond L1 Minimization and the Subspace Assumption. (arXiv:1502.01368v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1502.01368</link>
<description rdf:parseType="Literal">&lt;p&gt;The sparse representation classifier (SRC) has been utilized in various
classification problems, which makes use of L1 minimization and is shown to
work well for image recognition problems that satisfy a subspace assumption. In
this paper we propose a new implementation of SRC via screening, establish its
equivalence to the original SRC under regularity conditions, and prove its
classification consistency under a latent subspace model. The results are
demonstrated via simulations and real data experiments, where the new algorithm
achieves comparable numerical performance but significantly faster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Cencheng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Li Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yuexiao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.02010">
<title>Convergence Analysis of Distributed Inference with Vector-Valued Gaussian Belief Propagation. (arXiv:1611.02010v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.02010</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers inference over distributed linear Gaussian models using
factor graphs and Gaussian belief propagation (BP). The distributed inference
algorithm involves only local computation of the information matrix and of the
mean vector, and message passing between neighbors. Under broad conditions, it
is shown that the message information matrix converges to a unique positive
definite limit matrix for arbitrary positive semidefinite initialization, and
it approaches an arbitrarily small neighborhood of this limit matrix at a
doubly exponential rate. A necessary and sufficient convergence condition for
the belief mean vector to converge to the optimal centralized estimator is
provided under the assumption that the message information matrix is
initialized as a positive semidefinite matrix. Further, it is shown that
Gaussian BP always converges when the underlying factor graph is given by the
union of a forest and a single loop. The proposed convergence condition in the
setup of distributed linear Gaussian models is shown to be strictly weaker than
other existing convergence conditions and requirements, including the Gaussian
Markov random field based walk-summability condition, and applicable to a large
class of scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jian Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shaodan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yik-Chung Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kar_S/0/1/0/all/0/1&quot;&gt;Soummya Kar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moura_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; M. F. Moura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.02556">
<title>Classification and Representation via Separable Subspaces: Performance Limits and Algorithms. (arXiv:1705.02556v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1705.02556</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the classification performance of Kronecker-structured models in two
asymptotic regimes and developed an algorithm for separable, fast and compact
K-S dictionary learning for better classification and representation of
multidimensional signals by exploiting the structure in the signal. First, we
study the classification performance in terms of diversity order and pairwise
geometry of the subspaces. We derive an exact expression for the diversity
order as a function of the signal and subspace dimensions of a K-S model. Next,
we study the classification capacity, the maximum rate at which the number of
classes can grow as the signal dimension goes to infinity. Then we describe a
fast algorithm for Kronecker-Structured Learning of Discriminative Dictionaries
(K-SLD2). Finally, we evaluate the empirical classification performance of K-S
models for the synthetic data, showing that they agree with the diversity order
analysis. We also evaluate the performance of K-SLD2 on synthetic and
real-world datasets showing that the K-SLD2 balances compact signal
representation and good classification performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jindal_I/0/1/0/all/0/1&quot;&gt;Ishan Jindal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nokleby_M/0/1/0/all/0/1&quot;&gt;Matthew Nokleby&lt;/a&gt;</dc:creator>
</item></rdf:RDF>