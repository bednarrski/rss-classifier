<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07741"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07850"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07439"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07461"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07470"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07495"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07498"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07552"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07635"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07685"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07709"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07717"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07723"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07757"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07851"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07857"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.03907"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.06766"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09904"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08706"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02772"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03582"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05108"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07409"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07504"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07506"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07533"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07555"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07562"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07568"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07569"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07644"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07690"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07703"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07755"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07772"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07788"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07811"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07846"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07870"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1511.00158"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.00112"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.01701"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07774"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07274"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06058"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02097"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07580"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05052"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07223"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.07741">
<title>A framework for large-scale evaluation of deep learning for EEG. (arXiv:1806.07741v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1806.07741</link>
<description rdf:parseType="Literal">&lt;p&gt;EEG is the most common signal source for noninvasive BCI applications. For
such applications, the EEG signal needs to be decoded and translated into
appropriate actions. A recently emerging EEG decoding approach is deep learning
with Convolutional or Recurrent Neural Networks (CNNs, RNNs) with many
different architectures already published. Here we present a novel framework
for the large-scale evaluation of different deep-learning architectures on
different EEG datasets. This framework comprises (i) a collection of EEG
datasets currently comprising 100 examples (recording sessions) from six
different classification problems, (ii) a collection of different EEG decoding
algorithms, and (iii) a wrapper linking the decoders to the data as well as
handling structured documentation of all settings and (hyper-) parameters and
statistics, designed to ensure transparency and reproducibility. As an
applications example we used our framework by comparing three publicly
available CNN architectures: the Braindecode Deep4 ConvNet, Braindecode Shallow
ConvNet, and EEGNet. We also show how our framework can be used to study
similarities and differences in the performance of different decoding methods
across tasks. We argue that the deep learning EEG framework as described here
could help to tap the full potential of deep learning for BCI applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heilmeyer_F/0/1/0/all/0/1&quot;&gt;Felix A. Heilmeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schirrmeister_R/0/1/0/all/0/1&quot;&gt;Robin T. Schirrmeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fiederer_L/0/1/0/all/0/1&quot;&gt;Lukas D. J. Fiederer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Volker_M/0/1/0/all/0/1&quot;&gt;Martin V&amp;#xf6;lker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Behncke_J/0/1/0/all/0/1&quot;&gt;Joos Behncke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ball_T/0/1/0/all/0/1&quot;&gt;Tonio Ball&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07850">
<title>Log-sum-exp neural networks and posynomial models for convex and log-log-convex data. (arXiv:1806.07850v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.07850</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that a one-layer feedforward neural network with exponential
activation functions in the inner layer and logarithmic activation in the
output neuron is a universal approximator of convex functions. Such a network
represents a family of scaled log-sum exponential functions, here named LSET.
The proof uses a dequantization argument from tropical geometry. Under a
suitable exponential transformation LSE maps to a family of generalized
posynomial functions GPOST, which we also show to be universal approximators
for log-log-convex functions. The key feature of interest in the proposed
approach is that, once a LSET network is trained on data, the resulting model
is convex in the variables, which makes it readily amenable to efficient design
based on convex optimization. Similarly, once a GPOST model is trained on data,
it yields a posynomial model that can be efficiently optimized with respect to
its variables by using Geometric Programming (GP). Many relevant phenomena in
physics and engineering can indeed be modeled, either exactly or approximately,
via convex or log-log-convex models. The proposed methodology is illustrated by
two numerical examples in which LSET and GPOST models are used to first
approximate data gathered from the simulations of two physical processes (the
vibration from a vehicle suspension system, and the peak power generated by the
combustion of propane), and to later optimize these models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calafiore_G/0/1/0/all/0/1&quot;&gt;Giuseppe C. Calafiore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaubert_S/0/1/0/all/0/1&quot;&gt;Stephane Gaubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Possieri_C/0/1/0/all/0/1&quot;&gt;Corrado Possieri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07439">
<title>HybridNet: Integrating Model-based and Data-driven Learning to Predict Evolution of Dynamical Systems. (arXiv:1806.07439v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.07439</link>
<description rdf:parseType="Literal">&lt;p&gt;The robotic systems continuously interact with complex dynamical systems in
the physical world. Reliable predictions of spatiotemporal evolution of these
dynamical systems, with limited knowledge of system dynamics, are crucial for
autonomous operation. In this paper, we present HybridNet, a framework that
integrates data-driven deep learning and model-driven computation to reliably
predict spatiotemporal evolution of a dynamical systems even with in-exact
knowledge of their parameters. A data-driven deep neural network (DNN) with
Convolutional LSTM (ConvLSTM) as the backbone is employed to predict the
time-varying evolution of the external forces/perturbations. On the other hand,
the model-driven computation is performed using Cellular Neural Network (CeNN),
a neuro-inspired algorithm to model dynamical systems defined by coupled
partial differential equations (PDEs). CeNN converts the intricate numerical
computation into a series of convolution operations, enabling a trainable PDE
solver. With a feedback control loop, HybridNet can learn the physical
parameters governing the system&apos;s dynamics in real-time, and accordingly adapt
the computation models to enhance prediction accuracy for time-evolving
dynamical systems. The experimental results on two dynamical systems, namely,
heat convection-diffusion system, and fluid dynamical system, demonstrate that
the HybridNet produces higher accuracy than the state-of-the-art deep learning
based approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1&quot;&gt;Yun Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+She_X/0/1/0/all/0/1&quot;&gt;Xueyuan She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhopadhyay_S/0/1/0/all/0/1&quot;&gt;Saibal Mukhopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07461">
<title>Learning Treatment Regimens from Electronic Medical Records. (arXiv:1806.07461v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1806.07461</link>
<description rdf:parseType="Literal">&lt;p&gt;Appropriate treatment regimens play a vital role in improving patient health
status. Although some achievements have been made, few of the recent studies of
learning treatment regimens have exploited different kinds of patient
information due to the difficulty in adopting heterogeneous data to many data
mining methods. Moreover, current studies seem too rigid with fixed intervals
of treatment periods corresponding to the varying lengths of hospital stay. To
this end, this work proposes a generic data-driven framework which can derive
group-treatment regimens from electronic medical records by utilizing a
mixed-variate restricted Boltzmann machine and incorporating medical domain
knowledge. We conducted experiments on coronary artery disease as a case study.
The obtained results show that the framework is promising and capable of
assisting physicians in making clinical decisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_K/0/1/0/all/0/1&quot;&gt;Khanh-Hung Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Tu-Bao Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07470">
<title>Contrastive Explanations with Local Foil Trees. (arXiv:1806.07470v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.07470</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in interpretable Machine Learning (iML) and eXplainable AI
(XAI) construct explanations based on the importance of features in
classification tasks. However, in a high-dimensional feature space this
approach may become unfeasible without restraining the set of important
features. We propose to utilize the human tendency to ask questions like &quot;Why
this output (the fact) instead of that output (the foil)?&quot; to reduce the number
of features to those that play a main role in the asked contrast. Our proposed
method utilizes locally trained one-versus-all decision trees to identify the
disjoint set of rules that causes the tree to classify data points as the foil
and not as the fact. In this study we illustrate this approach on three
benchmark classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Waa_J/0/1/0/all/0/1&quot;&gt;Jasper van der Waa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Robeer_M/0/1/0/all/0/1&quot;&gt;Marcel Robeer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Diggelen_J/0/1/0/all/0/1&quot;&gt;Jurriaan van Diggelen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brinkhuis_M/0/1/0/all/0/1&quot;&gt;Matthieu Brinkhuis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neerincx_M/0/1/0/all/0/1&quot;&gt;Mark Neerincx&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07495">
<title>Joint Neural Entity Disambiguation with Output Space Search. (arXiv:1806.07495v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1806.07495</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel model for entity disambiguation that
combines both local contextual information and global evidences through Limited
Discrepancy Search (LDS). Given an input document, we start from a complete
solution constructed by a local model and conduct a search in the space of
possible corrections to improve the local solution from a global view point.
Our search utilizes a heuristic function to focus more on the least confident
local decisions and a pruning function to score the global solutions based on
their local fitness and the global coherences among the predicted entities.
Experimental results on CoNLL 2003 and TAC 2010 benchmarks verify the
effectiveness of our model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahbazi_H/0/1/0/all/0/1&quot;&gt;Hamed Shahbazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fern_X/0/1/0/all/0/1&quot;&gt;Xiaoli Z. Fern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghaeini_R/0/1/0/all/0/1&quot;&gt;Reza Ghaeini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obeidat_R/0/1/0/all/0/1&quot;&gt;Rasha Obeidat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tadepalli_P/0/1/0/all/0/1&quot;&gt;Prasad Tadepalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07498">
<title>Defining Locality for Surrogates in Post-hoc Interpretablity. (arXiv:1806.07498v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07498</link>
<description rdf:parseType="Literal">&lt;p&gt;Local surrogate models, to approximate the local decision boundary of a
black-box classifier, constitute one approach to generate explanations for the
rationale behind an individual prediction made by the back-box. This paper
highlights the importance of defining the right locality, the neighborhood on
which a local surrogate is trained, in order to approximate accurately the
local black-box decision boundary. Unfortunately, as shown in this paper, this
issue is not only a parameter or sampling distribution challenge and has a
major impact on the relevance and quality of the approximation of the local
black-box decision boundary and thus on the meaning and accuracy of the
generated explanation. To overcome the identified problems, quantified with an
adapted measure and procedure, we propose to generate surrogate-based
explanations for individual predictions based on a sampling centered on
particular place of the decision boundary, relevant for the prediction to be
explained, rather than on the prediction itself as it is classically done. We
evaluate the novel approach compared to state-of-the-art methods and a
straightforward improvement thereof on four UCI datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laugel_T/0/1/0/all/0/1&quot;&gt;Thibault Laugel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renard_X/0/1/0/all/0/1&quot;&gt;Xavier Renard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lesot_M/0/1/0/all/0/1&quot;&gt;Marie-Jeanne Lesot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marsala_C/0/1/0/all/0/1&quot;&gt;Christophe Marsala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Detyniecki_M/0/1/0/all/0/1&quot;&gt;Marcin Detyniecki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07552">
<title>Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems. (arXiv:1806.07552v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.07552</link>
<description rdf:parseType="Literal">&lt;p&gt;Several researchers have argued that a machine learning system&apos;s
interpretability should be defined in relation to a specific agent or task: we
should not ask if the system is interpretable, but to whom is it interpretable.
We describe a model intended to help answer this question, by identifying
different roles that agents can fulfill in relation to the machine learning
system. We illustrate the use of our model in a variety of scenarios, exploring
how an agent&apos;s role influences its goals, and the implications for defining
interpretability. Finally, we make suggestions for how our model could be
useful to interpretability researchers, system developers, and regulatory
bodies auditing machine learning systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomsett_R/0/1/0/all/0/1&quot;&gt;Richard Tomsett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braines_D/0/1/0/all/0/1&quot;&gt;Dave Braines&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harborne_D/0/1/0/all/0/1&quot;&gt;Dan Harborne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1&quot;&gt;Alun Preece&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Supriyo Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07635">
<title>Dynamic Risk Assessment for Vehicles of Higher Automation Levels by Deep Learning. (arXiv:1806.07635v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.07635</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicles of higher automation levels require the creation of situation
awareness. One important aspect of this situation awareness is an understanding
of the current risk of a driving situation. In this work, we present a novel
approach for the dynamic risk assessment of driving situations based on images
of a front stereo camera using deep learning. To this end, we trained a deep
neural network with recorded monocular images, disparity maps and a risk metric
for diverse traffic scenes. Our approach can be used to create the
aforementioned situation awareness of vehicles of higher automation levels and
can serve as a heterogeneous channel to systems based on radar or lidar sensors
that are used traditionally for the calculation of risk metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feth_P/0/1/0/all/0/1&quot;&gt;Patrik Feth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akram_M/0/1/0/all/0/1&quot;&gt;Mohammed Naveed Akram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuster_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9; Schuster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1&quot;&gt;Oliver Wasenm&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07685">
<title>Approximation by filter functions. (arXiv:1806.07685v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.07685</link>
<description rdf:parseType="Literal">&lt;p&gt;In this exploratory article, we draw attention to the common formal ground
among various estimators such as the belief functions of evidence theory and
their relatives, approximation quality of rough set theory, and contextual
probability. The unifying concept will be a general filter function composed of
a basic probability and a weighting which varies according to the problem at
hand. To compare the various filter functions we conclude with a simulation
study with an example from the area of item response theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duntsch_I/0/1/0/all/0/1&quot;&gt;Ivo D&amp;#xfc;ntsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gediga_G/0/1/0/all/0/1&quot;&gt;G&amp;#xfc;nther Gediga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07709">
<title>Notes on Abstract Argumentation Theory. (arXiv:1806.07709v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.07709</link>
<description rdf:parseType="Literal">&lt;p&gt;This note reviews Section 2 of Dung&apos;s seminal 1995 paper on abstract
argumentation theory. In particular, we clarify and make explicit all of the
proofs mentioned therein, and provide many more examples to the definitions, in
a way that should be helpful to readers approaching abstract argumentation
theory for the first time. However, we provide minimal commentary and will
refer the reader to Dung&apos;s paper for the intuitions behind various concepts.
The appropriate mathematical prerequisites are provided in the appendices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_A/0/1/0/all/0/1&quot;&gt;Anthony Peter Young&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07717">
<title>Weighted Abstract Dialectical Frameworks: Extended and Revised Report. (arXiv:1806.07717v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.07717</link>
<description rdf:parseType="Literal">&lt;p&gt;Abstract Dialectical Frameworks (ADFs) generalize Dung&apos;s argumentation
frameworks allowing various relationships among arguments to be expressed in a
systematic way. We further generalize ADFs so as to accommodate arbitrary
acceptance degrees for the arguments. This makes ADFs applicable in domains
where both the initial status of arguments and their relationship are only
insufficiently specified by Boolean functions. We define all standard ADF
semantics for the weighted case, including grounded, preferred and stable
semantics. We illustrate our approach using acceptance degrees from the unit
interval and show how other valuation structures can be integrated. In each
case it is sufficient to specify how the generalized acceptance conditions are
represented by formulas, and to specify the information ordering underlying the
characteristic ADF operator. We also present complexity results for problems
related to weighted ADFs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brewka_G/0/1/0/all/0/1&quot;&gt;Gerhard Brewka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puhrer_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg P&amp;#xfc;hrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strass_H/0/1/0/all/0/1&quot;&gt;Hannes Strass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallner_J/0/1/0/all/0/1&quot;&gt;Johannes P. Wallner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woltran_S/0/1/0/all/0/1&quot;&gt;Stefan Woltran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07723">
<title>Combinatorial Testing for Deep Learning Systems. (arXiv:1806.07723v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1806.07723</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) has achieved remarkable progress over the past decade and
been widely applied to many safety-critical applications. However, the
robustness of DL systems recently receives great concerns, such as adversarial
examples against computer vision systems, which could potentially result in
severe consequences. Adopting testing techniques could help to evaluate the
robustness of a DL system and therefore detect vulnerabilities at an early
stage. The main challenge of testing such systems is that its runtime state
space is too large: if we view each neuron as a runtime state for DL, then a DL
system often contains massive states, rendering testing each state almost
impossible. For traditional software, combinatorial testing (CT) is an
effective testing technique to reduce the testing space while obtaining
relatively high defect detection abilities. In this paper, we perform an
exploratory study of CT on DL systems. We adapt the concept in CT and propose a
set of coverage criteria for DL systems, as well as a CT coverage guided test
generation technique. Our evaluation demonstrates that CT provides a promising
avenue for testing DL systems. We further pose several open questions and
interesting directions for combinatorial testing of DL systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1&quot;&gt;Minhui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jianjun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yadong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07757">
<title>How to Maximize the Spread of Social Influence: A Survey. (arXiv:1806.07757v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1806.07757</link>
<description rdf:parseType="Literal">&lt;p&gt;This survey presents the main results achieved for the influence maximization
problem in social networks. This problem is well studied in the literature and,
thanks to its recent applications, some of which currently deployed on the
field, it is receiving more and more attention in the scientific community. The
problem can be formulated as follows: given a graph, with each node having a
certain probability of influencing its neighbors, select a subset of vertices
so that the number of nodes in the network that are influenced is maximized.
Starting from this model, we introduce the main theoretical developments and
computational results that have been achieved, taking into account different
diffusion models describing how the information spreads throughout the network,
various ways in which the sources of information could be placed, and how to
tackle the problem in the presence of uncertainties affecting the network.
Finally, we present one of the main application that has been developed and
deployed exploiting tools and techniques previously discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nittis_G/0/1/0/all/0/1&quot;&gt;Giuseppe De Nittis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatti_N/0/1/0/all/0/1&quot;&gt;Nicola Gatti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07851">
<title>Sim-to-Real Reinforcement Learning for Deformable Object Manipulation. (arXiv:1806.07851v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1806.07851</link>
<description rdf:parseType="Literal">&lt;p&gt;We have seen much recent progress in rigid object manipulation, but
interaction with deformable objects has notably lagged behind. Due to the large
configuration space of deformable objects, solutions using traditional
modelling approaches require significant engineering work. Perhaps then,
bypassing the need for explicit modelling and instead learning the control in
an end-to-end manner serves as a better approach? Despite the growing interest
in the use of end-to-end robot learning approaches, only a small amount of work
has focused on their applicability to deformable object manipulation. Moreover,
due to the large amount of data needed to learn these end-to-end solutions, an
emerging trend is to learn to control policies in simulation and then transfer
them over to the real world. To-date, no work has explored whether it is
possible to learn and transfer deformable object policies. We believe that if
sim-to-real methods are the way forward, then it should be possible to learn to
interact with a wide variety of objects, and not just rigid objects. In this
work, we use a combination of state-of-the-art deep reinforcement learning
algorithms to solve the problem of manipulating deformable objects
(specifically cloth). We evaluate our approach on three tasks --- folding a
towel up to a mark, folding a face towel diagonally, and draping a piece of
cloth over a hanger. Our agents are fully trained in simulation with domain
randomisation, and then successfully deployed in the real world without having
seen any real deformable objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1&quot;&gt;Jan Matas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1&quot;&gt;Stephen James&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1&quot;&gt;Andrew J. Davison&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07857">
<title>RUDDER: Return Decomposition for Delayed Rewards. (arXiv:1806.07857v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07857</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel reinforcement learning approach for finite Markov decision
processes (MDPs) with delayed rewards. In this work, biases of temporal
difference (TD) estimates are proved to be corrected only exponentially slowly
in the number of delay steps. Furthermore, variances of Monte Carlo (MC)
estimates are proved to increase the variance of other estimates, the number of
which can exponentially grow in the number of delay steps. We introduce RUDDER,
a return decomposition method, which creates a new MDP with same optimal
policies as the original MDP but with redistributed rewards that have largely
reduced delays. If the return decomposition is optimal, then the new MDP does
not have delayed rewards and TD estimates are unbiased. In this case, the
rewards track Q-values so that the future expected reward is always zero. We
experimentally confirm our theoretical results on bias and variance of TD and
MC estimates. On artificial tasks with different lengths of reward delays, we
show that RUDDER is exponentially faster than TD, MC, and MC Tree Search
(MCTS). RUDDER outperforms rainbow, A3C, DDQN, Distributional DQN, Dueling
DDQN, Noisy DQN, and Prioritized DDQN on the delayed reward Atari game Venture
in only a fraction of the learning time. RUDDER considerably improves the
state-of-the-art on the delayed reward Atari game Bowling in much less learning
time. Source code is available at https://github.com/ml-jku/baselines-rudder,
with demonstration videos at https://goo.gl/EQerZV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arjona_Medina_J/0/1/0/all/0/1&quot;&gt;Jose A. Arjona-Medina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gillhofer_M/0/1/0/all/0/1&quot;&gt;Michael Gillhofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widrich_M/0/1/0/all/0/1&quot;&gt;Michael Widrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1&quot;&gt;Thomas Unterthiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1&quot;&gt;Sepp Hochreiter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.03907">
<title>Reinforcement Learning in Rich-Observation MDPs using Spectral Methods. (arXiv:1611.03907v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1611.03907</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) in Markov decision processes (MDPs) with large
state spaces is a challenging problem. The performance of standard RL
algorithms degrades drastically with the dimensionality of state space.
However, in practice, these large MDPs typically incorporate a latent or hidden
low-dimensional structure. In this paper, we study the setting of
rich-observation Markov decision processes (ROMDP), where there are a small
number of hidden states which possess an injective mapping to the observation
states. In other words, every observation state is generated through a single
hidden state, and this mapping is unknown a priori. We introduce a spectral
decomposition method that consistently learns this mapping, and more
importantly, achieves it with low regret. The estimated mapping is integrated
into an optimistic RL algorithm (UCRL), which operates on the estimated hidden
space. We derive finite-time regret bounds for our algorithm with a weak
dependence on the dimensionality of the observed space. In fact, our algorithm
asymptotically achieves the same average regret as the oracle UCRL algorithm,
which has the knowledge of the mapping from hidden to observed spaces. Thus, we
derive an efficient spectral RL algorithm for ROMDPs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1&quot;&gt;Kamyar Azizzadenesheli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1&quot;&gt;Alessandro Lazaric&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Animashree Anandkumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.06766">
<title>Outcome-Oriented Predictive Process Monitoring: Review and Benchmark. (arXiv:1707.06766v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1707.06766</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive business process monitoring refers to the act of making
predictions about the future state of ongoing cases of a business process,
based on their incomplete execution traces and logs of historical (completed)
traces. Motivated by the increasingly pervasive availability of fine-grained
event data about business process executions, the problem of predictive process
monitoring has received substantial attention in the past years. In particular,
a considerable number of methods have been put forward to address the problem
of outcome-oriented predictive process monitoring, which refers to classifying
each ongoing case of a process according to a given set of possible categorical
outcomes - e.g., Will the customer complain or not? Will an order be delivered,
canceled or withdrawn? Unfortunately, different authors have used different
datasets, experimental settings, evaluation measures and baselines to assess
their proposals, resulting in poor comparability and an unclear picture of the
relative merits and applicability of different methods. To address this gap,
this article presents a systematic review and taxonomy of outcome-oriented
predictive process monitoring methods, and a comparative experimental
evaluation of eleven representative methods using a benchmark covering 24
predictive process monitoring tasks based on nine real-life event logs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teinemaa_I/0/1/0/all/0/1&quot;&gt;Irene Teinemaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_M/0/1/0/all/0/1&quot;&gt;Marlon Dumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_M/0/1/0/all/0/1&quot;&gt;Marcello La Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Maria Maggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09904">
<title>Ab initio Algorithmic Causal Deconvolution of Intertwined Programs and Networks by Generative Mechanism. (arXiv:1802.09904v7 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09904</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex data is usually produced by interacting sources with different
mechanisms. Here we introduce a parameter-free model-based approach, based upon
the seminal concept of Algorithmic Probability, that decomposes an observation
and signal into its most likely algorithmic generative sources. Our methods use
a causal calculus to infer model representations. We demonstrate the method
ability to distinguish interacting mechanisms and deconvolve them, regardless
of whether the objects produce strings, space-time evolution diagrams, images
or networks. We numerically test and evaluate our causal separation methods and
find that it can disentangle examples of observations from discrete dynamical
systems, and complex networks. We think that these causal separating techniques
can contribute to tackle the challenge of causation for estimations of better
rooted probability distributions thereby complementing more limited
statistical-oriented techniques that otherwise would lack model inference
capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenil_H/0/1/0/all/0/1&quot;&gt;Hector Zenil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiani_N/0/1/0/all/0/1&quot;&gt;Narsis A. Kiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zea_A/0/1/0/all/0/1&quot;&gt;Allan A. Zea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1&quot;&gt;Jesper Tegn&amp;#xe9;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08706">
<title>Alarm-Based Prescriptive Process Monitoring. (arXiv:1803.08706v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08706</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive process monitoring is concerned with the analysis of events
produced during the execution of a process in order to predict the future state
of ongoing cases thereof. Existing techniques in this field are able to
predict, at each step of a case, the likelihood that the case will end up in an
undesired outcome. These techniques, however, do not take into account what
process workers may do with the generated predictions in order to decrease the
likelihood of undesired outcomes. This paper proposes a framework for
prescriptive process monitoring, which extends predictive process monitoring
approaches with the concepts of alarms, interventions, compensations, and
mitigation effects. The framework incorporates a parameterized cost model to
assess the cost-benefit tradeoffs of applying prescriptive process monitoring
in a given setting. The paper also outlines an approach to optimize the
generation of alarms given a dataset and a set of cost model parameters. The
proposed approach is empirically evaluated using a range of real-life event
logs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teinemaa_I/0/1/0/all/0/1&quot;&gt;Irene Teinemaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tax_N/0/1/0/all/0/1&quot;&gt;Niek Tax&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leoni_M/0/1/0/all/0/1&quot;&gt;Massimiliano de Leoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_M/0/1/0/all/0/1&quot;&gt;Marlon Dumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Maria Maggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02772">
<title>Active Mini-Batch Sampling using Repulsive Point Processes. (arXiv:1804.02772v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02772</link>
<description rdf:parseType="Literal">&lt;p&gt;The convergence speed of stochastic gradient descent (SGD) can be improved by
actively selecting mini-batches. We explore sampling schemes where similar data
points are less likely to be selected in the same mini-batch. In particular, we
prove that such repulsive sampling schemes lowers the variance of the gradient
estimator. This generalizes recent work on using Determinantal Point Processes
(DPPs) for mini-batch diversification (Zhang et al., 2017) to the broader class
of repulsive point processes. We first show that the phenomenon of variance
reduction by diversified sampling generalizes in particular to non-stationary
point processes. We then show that other point processes may be computationally
much more efficient than DPPs. In particular, we propose and investigate
Poisson Disk sampling---frequently encountered in the computer graphics
community---for this task. We show empirically that our approach improves over
standard SGD both in terms of convergence speed as well as final model
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oztireli_C/0/1/0/all/0/1&quot;&gt;Cengiz &amp;#xd6;ztireli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salvi_G/0/1/0/all/0/1&quot;&gt;Giampiero Salvi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03582">
<title>A Scalable Framework for Trajectory Prediction. (arXiv:1806.03582v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.03582</link>
<description rdf:parseType="Literal">&lt;p&gt;Trajectory prediction (TP) is of great importance for a wide range of
location-based applications in intelligent transport systems such as
location-based advertising, route planning, traffic management, and early
warning systems. In the last few years, the widespread use of GPS navigation
systems and wireless communication technology enabled vehicles has resulted in
huge volumes of trajectory data. The task of utilizing this data employing
spatio-temporal techniques for trajectory prediction in an efficient and
accurate manner is an ongoing research problem. Existing TP approaches are
limited to short-term predictions. Moreover, they cannot handle a large volume
of trajectory data for long-term prediction. To address these limitations, we
propose a scalable clustering and Markov chain based hybrid framework, called
Traj-clusiVAT-based TP, for both short-term and long-term trajectory
prediction, which can handle a large number of overlapping trajectories in a
dense road network. In addition, Traj-clusiVAT can also determine the number of
clusters, which represent different movement behaviours in input trajectory
data. In our experiments, we compare our proposed approach with a mixed Markov
model (MMM)-based scheme, and a trajectory clustering, NETSCAN-based TP method
for both short- and long-term trajectory predictions. We performed our
experiments on two real, vehicle trajectory datasets, including a large-scale
trajectory dataset consisting of 3.28 million trajectories obtained from 15,061
taxis in Singapore over a period of one month. Experimental results on two real
trajectory datasets show that our proposed approach outperforms the existing
approaches in terms of both short- and long-term prediction performances, based
on prediction accuracy and distance error (in km).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathore_P/0/1/0/all/0/1&quot;&gt;Punit Rathore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1&quot;&gt;Dheeraj Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajasegarar_S/0/1/0/all/0/1&quot;&gt;Sutharshan Rajasegarar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palaniswami_M/0/1/0/all/0/1&quot;&gt;Marimuthu Palaniswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bezdek_J/0/1/0/all/0/1&quot;&gt;James C. Bezdek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05108">
<title>Holographic Automata for Ambient Immersive A. I. via Reservoir Computing. (arXiv:1806.05108v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05108</link>
<description rdf:parseType="Literal">&lt;p&gt;We prove the existence of a semilinear representation of Cellular Automata
(CA) with the introduction of multiple convolution kernels. Examples of the
technique are presented for rules akin to the &quot;edge-of-chaos&quot; including the
Turing universal rule 110 for further utilization in the area of reservoir
computing. We also examine the significance of their dual representation on a
frequency or wavelength domain as a superposition of plane waves for
distributed computing applications including a new proposal for a &quot;Hologrid&quot;
that could be realized with present Wi-Fi,Li-Fi technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raptis_T/0/1/0/all/0/1&quot;&gt;Theophanes E. Raptis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07409">
<title>Built-in Vulnerabilities to Imperceptible Adversarial Perturbations. (arXiv:1806.07409v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.07409</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing models that are robust to small adversarial perturbations of their
inputs has proven remarkably difficult. In this work we show that the reverse
problem---making models more vulnerable---is surprisingly easy. After
presenting some proofs of concept on MNIST, we introduce a generic tilting
attack that injects vulnerabilities into the linear layers of pre-trained
networks without affecting their performance on natural data. We illustrate
this attack on a multilayer perceptron trained on SVHN and use it to design a
stand-alone adversarial module which we call a steganogram decoder. Finally, we
show on CIFAR-10 that a state-of-the-art network can be trained to misclassify
images in the presence of imperceptible backdoor signals. These different
results suggest that adversarial perturbations are not always informative of
the true features used by a model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanay_T/0/1/0/all/0/1&quot;&gt;Thomas Tanay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrews_J/0/1/0/all/0/1&quot;&gt;Jerone T. A. Andrews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffin_L/0/1/0/all/0/1&quot;&gt;Lewis D. Griffin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07504">
<title>A Latent Variable Approach to Gaussian Process Modeling with Qualitative and Quantitative Factors. (arXiv:1806.07504v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.07504</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer simulations often involve both qualitative and numerical inputs.
Existing Gaussian process (GP) methods for handling this mainly assume a
different response surface for each combination of levels of the qualitative
factors and relate them via a multiresponse cross-covariance matrix. We
introduce a substantially different approach that maps each qualitative factor
to an underlying numerical latent variable (LV), with the mapped value for each
level estimated similarly to the covariance lengthscale parameters. This
provides a parsimonious GP parameterization that treats qualitative factors the
same as numerical variables and views them as effecting the response via
similar physical mechanisms. This has strong physical justification, as the
effects of a qualitative factor in any physics-based simulation model must
always be due to some underlying numerical variables. Even when the underlying
variables are many, sufficient dimension reduction arguments imply that their
effects can be represented by a low-dimensional LV. This conjecture is
supported by the superior predictive performance observed across a variety of
examples. Moreover, the mapped LVs provide substantial insight into the nature
and effects of the qualitative factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tao_S/0/1/0/all/0/1&quot;&gt;Siyu Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Apley_D/0/1/0/all/0/1&quot;&gt;Daniel W. Apley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07506">
<title>A Simple Fusion of Deep and Shallow Learning for Acoustic Scene Classification. (arXiv:1806.07506v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1806.07506</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past, Acoustic Scene Classification systems have been based on hand
crafting audio features that are input to a classifier. Nowadays, the common
trend is to adopt data driven techniques, e.g., deep learning, where audio
representations are learned from data. In this paper, we propose a system that
consists of a simple fusion of two methods of the aforementioned types: a deep
learning approach where log-scaled mel-spectrograms are input to a
convolutional neural network, and a feature engineering approach, where a
collection of hand-crafted features is input to a gradient boosting machine. We
first show that both methods provide complementary information to some extent.
Then, we use a simple late fusion strategy to combine both methods. We report
classification accuracy of each method individually and the combined system on
the TUT Acoustic Scenes 2017 dataset. The proposed fused system outperforms
each of the individual methods and attains a classification accuracy of 72.8%
on the evaluation set, improving the baseline system by 11.8%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1&quot;&gt;Eduardo Fonseca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1&quot;&gt;Rong Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serra_X/0/1/0/all/0/1&quot;&gt;Xavier Serra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07533">
<title>An Asynchronous Distributed Expectation Maximization Algorithm For Massive Data: The DEM Algorithm. (arXiv:1806.07533v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1806.07533</link>
<description rdf:parseType="Literal">&lt;p&gt;The family of Expectation-Maximization (EM) algorithms provides a general
approach to fitting flexible models for large and complex data. The expectation
(E) step of EM-type algorithms is time-consuming in massive data applications
because it requires multiple passes through the full data. We address this
problem by proposing an asynchronous and distributed generalization of the EM
called the Distributed EM (DEM). Using DEM, existing EM-type algorithms are
easily extended to massive data settings by exploiting the divide-and-conquer
technique and widely available computing power, such as grid computing. The DEM
algorithm reserves two groups of computing processes called \emph{workers} and
\emph{managers} for performing the E step and the maximization step (M step),
respectively. The samples are randomly partitioned into a large number of
disjoint subsets and are stored on the worker processes. The E step of DEM
algorithm is performed in parallel on all the workers, and every worker
communicates its results to the managers at the end of local E step. The
managers perform the M step after they have received results from a
$\gamma$-fraction of the workers, where $\gamma$ is a fixed constant in $(0,
1]$. The sequence of parameter estimates generated by the DEM algorithm retains
the attractive properties of EM: convergence of the sequence of parameter
estimates to a local mode and linear global rate of convergence. Across diverse
simulations focused on linear mixed-effects models, the DEM algorithm is
significantly faster than competing EM-type algorithms while having a similar
accuracy. The DEM algorithm maintains its superior empirical performance on a
movie ratings database consisting of 10 million ratings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srivastava_S/0/1/0/all/0/1&quot;&gt;Sanvesh Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+DePalma_G/0/1/0/all/0/1&quot;&gt;Glen DePalma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chuanhai Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07555">
<title>Stagewise Safe Bayesian Optimization with Gaussian Processes. (arXiv:1806.07555v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07555</link>
<description rdf:parseType="Literal">&lt;p&gt;Enforcing safety is a key aspect of many problems pertaining to sequential
decision making under uncertainty, which require the decisions made at every
step to be both informative of the optimal decision and also safe. For example,
we value both efficacy and comfort in medical therapy, and efficiency and
safety in robotic control. We consider this problem of optimizing an unknown
utility function with absolute feedback or preference feedback subject to
unknown safety constraints. We develop an efficient safe Bayesian optimization
algorithm, StageOpt, that separates safe region expansion and utility function
maximization into two distinct stages. Compared to existing approaches which
interleave between expansion and optimization, we show that StageOpt is more
efficient and naturally applicable to a broader class of problems. We provide
theoretical guarantees for both the satisfaction of safety constraints as well
as convergence to the optimal utility value. We evaluate StageOpt on both a
variety of synthetic experiments, as well as in clinical practice. We
demonstrate that StageOpt is more effective than existing safe optimization
approaches, and is able to safely and effectively optimize spinal cord
stimulation therapy in our clinical experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1&quot;&gt;Yanan Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_V/0/1/0/all/0/1&quot;&gt;Vincent Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burdick_J/0/1/0/all/0/1&quot;&gt;Joel W. Burdick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yisong Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07562">
<title>Efficient inference in stochastic block models with vertex labels. (arXiv:1806.07562v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.07562</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the stochastic block model with two communities where vertices
contain side information in the form of a vertex label. These vertex labels may
have arbitrary label distributions, depending on the community memberships. We
analyze a linearized version of the popular belief propagation algorithm. We
show that this algorithm achieves the highest accuracy possible whenever a
certain function of the network parameters has a unique fixed point. Whenever
this function has multiple fixed points, the belief propagation algorithm may
not perform optimally. We show that increasing the information in the vertex
labels may reduce the number of fixed points and hence lead to optimality of
belief propagation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stegehuis_C/0/1/0/all/0/1&quot;&gt;Clara Stegehuis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1&quot;&gt;Laurent Massouli&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07568">
<title>Doubly Nested Network for Resource-Efficient Inference. (arXiv:1806.07568v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07568</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose doubly nested network(DNNet) where all neurons represent their own
sub-models that solve the same task. Every sub-model is nested both layer-wise
and channel-wise. While nesting sub-models layer-wise is straight-forward with
deep-supervision as proposed in \cite{xie2015holistically}, channel-wise
nesting has not been explored in the literature to our best knowledge.
Channel-wise nesting is non-trivial as neurons between consecutive layers are
all connected to each other. In this work, we introduce a technique to solve
this problem by sorting channels topologically and connecting neurons
accordingly. For the purpose, channel-causal convolutions are used. Slicing
doubly nested network gives a working sub-network. The most notable application
of our proposed network structure with slicing operation is resource-efficient
inference. At test time, computing resources such as time and memory available
for running the prediction algorithm can significantly vary across devices and
applications. Given a budget constraint, we can slice the network accordingly
and use a sub-model for inference within budget, requiring no additional
computation such as training or fine-tuning after deployment. We demonstrate
the effectiveness of our approach in several practical scenarios of utilizing
available resource efficiently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaehong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Sungeun Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yongseok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiwon Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07569">
<title>A Distributed Second-Order Algorithm You Can Trust. (arXiv:1806.07569v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07569</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the rapid growth of data and computational resources, distributed
optimization has become an active research area in recent years. While
first-order methods seem to dominate the field, second-order methods are
nevertheless attractive as they potentially require fewer communication rounds
to converge. However, there are significant drawbacks that impede their wide
adoption, such as the computation and the communication of a large Hessian
matrix. In this paper we present a new algorithm for distributed training of
generalized linear models that only requires the computation of diagonal blocks
of the Hessian matrix on the individual workers. To deal with this approximate
information we propose an adaptive approach that - akin to trust-region methods
- dynamically adapts the auxiliary model to compensate for modeling errors. We
provide theoretical rates of convergence for a wide class of problems including
L1-regularized objectives. We also demonstrate that our approach achieves
state-of-the-art results on multiple large benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunner_C/0/1/0/all/0/1&quot;&gt;Celestine D&amp;#xfc;nner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1&quot;&gt;Aurelien Lucchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gargiani_M/0/1/0/all/0/1&quot;&gt;Matilde Gargiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_A/0/1/0/all/0/1&quot;&gt;An Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1&quot;&gt;Thomas Hofmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07644">
<title>Cross-Domain Deep Face Matching for Real Banking Security Systems. (arXiv:1806.07644v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.07644</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring the security of transactions is currently one of the major
challenges facing banking systems. The usage of face for biometric
authentication of users is becoming adopted worldwide due its convenience and
acceptability by people, and also given that, nowadays, almost all computers
and mobile devices have built-in cameras. Such user authentication approach is
attracting large investments from banking and financial institutions,
especially in cross-domain scenarios, in which facial images from ID documents
are compared with digital self-portraits (selfies) taken with the cameras of
mobile devices, for the automated opening of new checking accounts or financial
transactions authorization. In this work, besides of collecting a large
cross-domain face database, with 27,002 real facial images of selfies and ID
documents (13,501 subjects) captured from the systems of the major public
Brazilian bank, we propose a novel approach for such cross-domain face matching
based on deep features extracted by two well-referenced Convolutional Neural
Networks (CNN). Results obtained on the large dataset collected, which we
called FaceBank, with accuracy rates higher than 93%, demonstrate the
robustness of the proposed approach to the cross-domain problem (comparing
faces in IDs and selfies) and its feasible application in real banking security
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_J/0/1/0/all/0/1&quot;&gt;Johnatan S. Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souza_G/0/1/0/all/0/1&quot;&gt;Gustavo B. Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1&quot;&gt;Anderson R. Rocha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deus_F/0/1/0/all/0/1&quot;&gt;Fl&amp;#xe1;vio E. Deus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marana_A/0/1/0/all/0/1&quot;&gt;Aparecido N. Marana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07690">
<title>Non-Parametric Calibration of Probabilistic Regression. (arXiv:1806.07690v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.07690</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of calibration is to retrospectively adjust the outputs from a
machine learning model to provide better probability estimates on the target
variable. While calibration has been investigated thoroughly in classification,
it has not yet been well-established for regression tasks. This paper considers
the problem of calibrating a probabilistic regression model to improve the
estimated probability densities over the real-valued targets. We propose to
calibrate a regression model through the cumulative probability density, which
can be derived from calibrating a multi-class classifier. We provide three
non-parametric approaches to solve the problem, two of which provide empirical
estimates and the third providing smooth density estimates. The proposed
approaches are experimentally evaluated to show their ability to improve the
performance of regression models on the predictive likelihood.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kull_M/0/1/0/all/0/1&quot;&gt;Meelis Kull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Flach_P/0/1/0/all/0/1&quot;&gt;Peter Flach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07703">
<title>Multi-View Multi-Graph Embedding for Brain Network Clustering Analysis. (arXiv:1806.07703v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.07703</link>
<description rdf:parseType="Literal">&lt;p&gt;Network analysis of human brain connectivity is critically important for
understanding brain function and disease states. Embedding a brain network as a
whole graph instance into a meaningful low-dimensional representation can be
used to investigate disease mechanisms and inform therapeutic interventions.
Moreover, by exploiting information from multiple neuroimaging modalities or
views, we are able to obtain an embedding that is more useful than the
embedding learned from an individual view. Therefore, multi-view multi-graph
embedding becomes a crucial task. Currently, only a few studies have been
devoted to this topic, and most of them focus on the vector-based strategy
which will cause structural information contained in the original graphs lost.
As a novel attempt to tackle this problem, we propose Multi-view Multi-graph
Embedding (M2E) by stacking multi-graphs into multiple partially-symmetric
tensors and using tensor techniques to simultaneously leverage the dependencies
and correlations among multi-view and multi-graph brain networks. Extensive
experiments on real HIV and bipolar disorder brain network datasets demonstrate
the superior performance of M2E on clustering brain networks by leveraging the
multi-view multi-graph interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lifang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cao_B/0/1/0/all/0/1&quot;&gt;Bokai Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ragin_A/0/1/0/all/0/1&quot;&gt;Ann B. Ragin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Leow_A/0/1/0/all/0/1&quot;&gt;Alex D. Leow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07755">
<title>An empirical study on evaluation metrics of generative adversarial networks. (arXiv:1806.07755v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07755</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating generative adversarial networks (GANs) is inherently challenging.
In this paper, we revisit several representative sample-based evaluation
metrics for GANs, and address the problem of how to evaluate the evaluation
metrics. We start with a few necessary conditions for metrics to produce
meaningful scores, such as distinguishing real from generated samples,
identifying mode dropping and mode collapsing, and detecting overfitting. With
a series of carefully designed experiments, we comprehensively investigate
existing sample-based metrics and identify their strengths and limitations in
practical settings. Based on these results, we observe that kernel Maximum Mean
Discrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test seem to
satisfy most of the desirable properties, provided that the distances between
samples are computed in a suitable feature space. Our experiments also unveil
interesting properties about the behavior of several popular GAN models, such
as whether they are memorizing training samples, and how far they are from
learning the target distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiantong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Felix Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1&quot;&gt;Kilian Weinberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07772">
<title>Accurate and Diverse Sampling of Sequences based on a &quot;Best of Many&quot; Sample Objective. (arXiv:1806.07772v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07772</link>
<description rdf:parseType="Literal">&lt;p&gt;For autonomous agents to successfully operate in the real world, anticipation
of future events and states of their environment is a key competence. This
problem has been formalized as a sequence extrapolation problem, where a number
of observations are used to predict the sequence into the future. Real-world
scenarios demand a model of uncertainty of such predictions, as predictions
become increasingly uncertain -- in particular on long time horizons. While
impressive results have been shown on point estimates, scenarios that induce
multi-modal distributions over future sequences remain challenging. Our work
addresses these challenges in a Gaussian Latent Variable model for sequence
prediction. Our core contribution is a &quot;Best of Many&quot; sample objective that
leads to more accurate and more diverse predictions that better capture the
true variations in real-world sequence data. Beyond our analysis of improved
model fit, our models also empirically outperform prior work on three diverse
tasks ranging from traffic scenes to weather data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1&quot;&gt;Apratim Bhattacharyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1&quot;&gt;Bernt Schiele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1&quot;&gt;Mario Fritz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07788">
<title>Random Feature Stein Discrepancies. (arXiv:1806.07788v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.07788</link>
<description rdf:parseType="Literal">&lt;p&gt;Computable Stein discrepancies have been deployed for a variety of
applications, including sampler selection in posterior inference, approximate
Bayesian inference, and goodness-of-fit testing. Existing
convergence-determining Stein discrepancies admit strong theoretical guarantees
but suffer from a computational cost that grows quadratically in the sample
size. While linear-time Stein discrepancies have been proposed for
goodness-of-fit testing, they exhibit avoidable degradations in testing
power---even when power is explicitly optimized. To address these shortcomings,
we introduce feature Stein discrepancies ($\Phi$SDs), a new family of quality
measures that can be cheaply approximated using importance sampling. We show
how to construct $\Phi$SDs that provably determine the convergence of a sample
to its target and develop high-accuracy approximations---random $\Phi$SDs
(R$\Phi$SDs)---which are computable in near-linear time. In our experiments
with sampler selection for approximate posterior inference and goodness-of-fit
testing, R$\Phi$SDs typically perform as well or better than quadratic-time
KSDs while being orders of magnitude faster to compute.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huggins_J/0/1/0/all/0/1&quot;&gt;Jonathan H Huggins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07811">
<title>Stochastic Nested Variance Reduction for Nonconvex Optimization. (arXiv:1806.07811v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07811</link>
<description rdf:parseType="Literal">&lt;p&gt;We study finite-sum nonconvex optimization problems, where the objective
function is an average of $n$ nonconvex functions. We propose a new stochastic
gradient descent algorithm based on nested variance reduction. Compared with
conventional stochastic variance reduced gradient (SVRG) algorithm that uses
two reference points to construct a semi-stochastic gradient with diminishing
variance in each iteration, our algorithm uses $K+1$ nested reference points to
build a semi-stochastic gradient to further reduce its variance in each
iteration. For smooth nonconvex functions, the proposed algorithm converges to
an $\epsilon$-approximate first-order stationary point (i.e., $\|\nabla
F(\mathbf{x})\|_2\leq \epsilon$) within $\tilde{O}(n\land
\epsilon^{-2}+\epsilon^{-3}\land n^{1/2}\epsilon^{-2})$ number of stochastic
gradient evaluations. This improves the best known gradient complexity of SVRG
$O(n+n^{2/3}\epsilon^{-2})$ and that of SCSG $O(n\land
\epsilon^{-2}+\epsilon^{-10/3}\land n^{2/3}\epsilon^{-2})$. For gradient
dominated functions, our algorithm also achieves a better gradient complexity
than the state-of-the-art algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dongruo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Pan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1&quot;&gt;Quanquan Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07819">
<title>Disentangling Multiple Conditional Inputs in GANs. (arXiv:1806.07819v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.07819</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a method that disentangles the effects of multiple
input conditions in Generative Adversarial Networks (GANs). In particular, we
demonstrate our method in controlling color, texture, and shape of a generated
garment image for computer-aided fashion design. To disentangle the effect of
input attributes, we customize conditional GANs with consistency loss
functions. In our experiments, we tune one input at a time and show that we can
guide our network to generate novel and realistic images of clothing articles.
In addition, we present a fashion design process that estimates the input
attributes of an existing garment and modifies them using our generator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yildirim_G/0/1/0/all/0/1&quot;&gt;G&amp;#xf6;khan Yildirim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seward_C/0/1/0/all/0/1&quot;&gt;Calvin Seward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergmann_U/0/1/0/all/0/1&quot;&gt;Urs Bergmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07846">
<title>Rethinking Machine Learning Development and Deployment for Edge Devices. (arXiv:1806.07846v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.07846</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML), especially deep learning is made possible by the
availability of big data, enormous compute power and, often overlooked,
development tools or frameworks. As the algorithms become mature and efficient,
more and more ML inference is moving out of datacenters/cloud and deployed on
edge devices. This model deployment process can be challenging as the
deployment environment and requirements can be substantially different from
those during model development. In this paper, we propose a new ML development
and deployment approach that is specially designed and optimized for
inference-only deployment on edge devices. We build a prototype and demonstrate
that this approach can address all the deployment challenges and result in more
efficient and high-quality solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1&quot;&gt;Liangzhen Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suda_N/0/1/0/all/0/1&quot;&gt;Naveen Suda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07870">
<title>Sequential change-point detection in high-dimensional Gaussian graphical models. (arXiv:1806.07870v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.07870</link>
<description rdf:parseType="Literal">&lt;p&gt;High dimensional piecewise stationary graphical models represent a versatile
class for modelling time varying networks arising in diverse application areas,
including biology, economics, and social sciences. There has been recent work
in offline detection and estimation of regime changes in the topology of sparse
graphical models. However, the online setting remains largely unexplored,
despite its high relevance to applications in sensor networks and other
engineering monitoring systems, as well as financial markets. To that end, this
work introduces a novel scalable online algorithm for detecting an unknown
number of abrupt changes in the inverse covariance matrix of sparse Gaussian
graphical models with small delay. The proposed algorithm is based upon
monitoring the conditional log-likelihood of all nodes in the network and can
be extended to a large class of continuous and discrete graphical models. We
also investigate asymptotic properties of our procedure under certain mild
regularity conditions on the graph size, sparsity level, number of samples, and
pre- and post-changes in the topology of the network. Numerical works on both
synthetic and real data illustrate the good performance of the proposed
methodology both in terms of computational and statistical efficiency across
numerous experimental settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Keshavarz_H/0/1/0/all/0/1&quot;&gt;Hossein Keshavarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Michailidis_G/0/1/0/all/0/1&quot;&gt;George Michailidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Atchade_Y/0/1/0/all/0/1&quot;&gt;Yves Atchade&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1511.00158">
<title>Prediction of Dynamical time Series Using Kernel Based Regression and Smooth Splines. (arXiv:1511.00158v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1511.00158</link>
<description rdf:parseType="Literal">&lt;p&gt;Prediction of dynamical time series with additive noise using support vector
machines or kernel based regression has been proved to be consistent for
certain classes of discrete dynamical systems. Consistency implies that these
methods are effective at computing the expected value of a point at a future
time given the present coordinates. However, the present coordinates themselves
are noisy, and therefore, these methods are not necessarily effective at
removing noise. In this article, we consider denoising and prediction as
separate problems for flows, as opposed to discrete time dynamical systems, and
show that the use of smooth splines is more effective at removing noise.
Combination of smooth splines and kernel based regression yields predictors
that are more accurate on benchmarks typically by a factor of 2 or more. We
prove that kernel based regression in combination with smooth splines converges
to the exact predictor for time series extracted from any compact invariant set
of any sufficiently smooth flow. As a consequence of convergence, one can find
examples where the combination of kernel based regression with smooth splines
is superior by even a factor of $100$. The predictors that we compute operate
on delay coordinate data and not the full state vector, which is typically not
observable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Navarrete_R/0/1/0/all/0/1&quot;&gt;Raymundo Navarrete&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Viswanath_D/0/1/0/all/0/1&quot;&gt;Divakar Viswanath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.00112">
<title>Configurable 3D Scene Synthesis and 2D Image Rendering with Per-Pixel Ground Truth using Stochastic Grammars. (arXiv:1704.00112v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1704.00112</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a systematic learning-based approach to the generation of massive
quantities of synthetic 3D scenes and arbitrary numbers of photorealistic 2D
images thereof, with associated ground truth information, for the purposes of
training, benchmarking, and diagnosing learning-based computer vision and
robotics algorithms. In particular, we devise a learning-based pipeline of
algorithms capable of automatically generating and rendering a potentially
infinite variety of indoor scenes by using a stochastic grammar, represented as
an attributed Spatial And-Or Graph, in conjunction with state-of-the-art
physics-based rendering. Our pipeline is capable of synthesizing scene layouts
with high diversity, and it is configurable inasmuch as it enables the precise
customization and control of important attributes of the generated scenes. It
renders photorealistic RGB images of the generated scenes while automatically
synthesizing detailed, per-pixel ground truth data, including visible surface
depth and normal, object identity, and material information (detailed to object
parts), as well as environments (e.g., illuminations and camera viewpoints). We
demonstrate the value of our synthesized dataset, by improving performance in
certain machine-learning-based scene understanding tasks--depth and surface
normal prediction, semantic segmentation, reconstruction, etc.--and by
providing benchmarks for and diagnostics of trained models by modifying object
attributes and scene properties in a controllable manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chenfanfu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1&quot;&gt;Siyuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yixin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jenny Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lap-Fai Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terzopoulos_D/0/1/0/all/0/1&quot;&gt;Demetri Terzopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.01701">
<title>Learning Certifiably Optimal Rule Lists for Categorical Data. (arXiv:1704.01701v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.01701</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the design and implementation of a custom discrete optimization
technique for building rule lists over a categorical feature space. Our
algorithm produces rule lists with optimal training performance, according to
the regularized empirical risk, with a certificate of optimality. By leveraging
algorithmic bounds, efficient data structures, and computational reuse, we
achieve several orders of magnitude speedup in time and a massive reduction of
memory consumption. We demonstrate that our approach produces optimal rule
lists on practical problems in seconds. Our results indicate that it is
possible to construct optimal sparse rule lists that are approximately as
accurate as the COMPAS proprietary risk prediction tool on data from Broward
County, Florida, but that are completely interpretable. This framework is a
novel alternative to CART and other decision tree methods for interpretable
modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Angelino_E/0/1/0/all/0/1&quot;&gt;Elaine Angelino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Larus_Stone_N/0/1/0/all/0/1&quot;&gt;Nicholas Larus-Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alabi_D/0/1/0/all/0/1&quot;&gt;Daniel Alabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Seltzer_M/0/1/0/all/0/1&quot;&gt;Margo Seltzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07774">
<title>Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients. (arXiv:1705.07774v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07774</link>
<description rdf:parseType="Literal">&lt;p&gt;The ADAM optimizer is exceedingly popular in the deep learning community.
Often it works very well, sometimes it doesn&apos;t. Why? We interpret ADAM as a
combination of two aspects: for each weight, the update direction is determined
by the sign of stochastic gradients, whereas the update magnitude is determined
by an estimate of their relative variance. We disentangle these two aspects and
analyze them in isolation, gaining insight into the mechanisms underlying ADAM.
This analysis also extends recent results on adverse effects of ADAM on
generalization, isolating the sign aspect as the problematic one. Transferring
the variance adaptation to SGD gives rise to a novel method, completing the
practitioner&apos;s toolbox for problems where ADAM fails.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balles_L/0/1/0/all/0/1&quot;&gt;Lukas Balles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1&quot;&gt;Philipp Hennig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07274">
<title>Speech recognition for medical conversations. (arXiv:1711.07274v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07274</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we explored building automatic speech recognition models for
transcribing doctor patient conversation. We collected a large scale dataset of
clinical conversations ($14,000$ hr), designed the task to represent the real
word scenario, and explored several alignment approaches to iteratively improve
data quality. We explored both CTC and LAS systems for building speech
recognition models. The LAS was more resilient to noisy data and CTC required
more data clean up. A detailed analysis is provided for understanding the
performance for clinical tasks. Our analysis showed the speech recognition
models performed well on important medical utterances, while errors occurred in
causal conversations. Overall we believe the resulting models can provide
reasonable quality in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1&quot;&gt;Chung-Cheng Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tripathi_A/0/1/0/all/0/1&quot;&gt;Anshuman Tripathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_K/0/1/0/all/0/1&quot;&gt;Katherine Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Co_C/0/1/0/all/0/1&quot;&gt;Chris Co&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaitly_N/0/1/0/all/0/1&quot;&gt;Navdeep Jaitly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaunzeikare_D/0/1/0/all/0/1&quot;&gt;Diana Jaunzeikare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1&quot;&gt;Anjuli Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Patrick Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sak_H/0/1/0/all/0/1&quot;&gt;Hasim Sak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankar_A/0/1/0/all/0/1&quot;&gt;Ananth Sankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tansuwan_J/0/1/0/all/0/1&quot;&gt;Justin Tansuwan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_N/0/1/0/all/0/1&quot;&gt;Nathan Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yonghui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuedong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06058">
<title>Constant-Time Predictive Distributions for Gaussian Processes. (arXiv:1803.06058v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06058</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most compelling features of Gaussian process (GP) regression is
its ability to provide well-calibrated posterior distributions. Recent advances
in inducing point methods have sped up GP marginal likelihood and posterior
mean computations, leaving posterior covariance estimation and sampling as the
remaining computational bottlenecks. In this paper we address these
shortcomings by using the Lanczos algorithm to rapidly approximate the
predictive covariance matrix. Our approach, which we refer to as LOVE (LanczOs
Variance Estimates), substantially improves time and space complexity. In our
experiments, LOVE computes covariances up to 2,000 times faster and draws
samples 18,000 times faster than existing methods, all without sacrificing
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1&quot;&gt;Geoff Pleiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Jacob R. Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1&quot;&gt;Kilian Q. Weinberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02097">
<title>Multi-view Banded Spectral Clustering with Application to ICD9 Clustering. (arXiv:1804.02097v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02097</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent development in methodology, community detection remains a
challenging problem. Existing literature largely focuses on the standard
setting where a network is learned using an observed adjacency matrix from a
single data source. Constructing a shared network from multiple data sources is
more challenging due to the heterogeneity across populations. Additionally, no
existing method leverages the prior distance knowledge available in many
domains to help the discovery of the network structure. To bridge this gap, in
this paper we propose a novel spectral clustering method that optimally
combines multiple data sources while leveraging the prior distance knowledge.
The proposed method combines a banding step guided by the distance knowledge
with a subsequent weighting step to maximize consensus across multiple sources.
Its statistical performance is thoroughly studied under a multi-view stochastic
block model. We also provide a simple yet optimal rule of choosing weights in
practice. The efficacy and robustness of the method is fully demonstrated
through extensive simulations. Finally, we apply the method to cluster the
International classification of diseases, ninth revision (ICD9), codes and
yield a very insightful clustering structure by integrating information from a
large claim database and two healthcare systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Luwan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liao_K/0/1/0/all/0/1&quot;&gt;Katherine Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohane_I/0/1/0/all/0/1&quot;&gt;Issac Kohane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1&quot;&gt;Tianxi Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07580">
<title>Robust And Scalable Learning Of Complex Dataset Topologies Via Elpigraph. (arXiv:1804.07580v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07580</link>
<description rdf:parseType="Literal">&lt;p&gt;Large datasets represented by multidimensional data point clouds often
possess non-trivial distributions with branching trajectories and excluded
regions, with the recent single-cell transcriptomic studies of developing
embryo being notable examples. Reducing the complexity and producing compact
and interpretable representations of such data remains a challenging task. Most
of the existing computational methods are based on exploring the local data
point neighbourhood relations, a step that can perform poorly in the case of
multidimensional and noisy data. Here we present ElPiGraph, a scalable and
robust method for approximation of datasets with complex structures which does
not require computing the complete data distance matrix or the data point
neighbourhood graph. This method is able to withstand high levels of noise and
is capable of approximating complex topologies via principal graph ensembles
that can be combined into a consensus principal graph. ElPiGraph deals
efficiently with large and complex datasets in various fields from biology,
where it can be used to infer gene dynamics from single-cell RNA-Seq, to
astronomy, where it can be used to explore complex structures in the
distribution of galaxies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albergante_L/0/1/0/all/0/1&quot;&gt;Luca Albergante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirkes_E/0/1/0/all/0/1&quot;&gt;Evgeny M. Mirkes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huidong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1&quot;&gt;Alexis Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faure_L/0/1/0/all/0/1&quot;&gt;Louis Faure&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barillot_E/0/1/0/all/0/1&quot;&gt;Emmanuel Barillot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinello_L/0/1/0/all/0/1&quot;&gt;Luca Pinello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1&quot;&gt;Alexander N. Gorban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zinovyev_A/0/1/0/all/0/1&quot;&gt;Andrei Zinovyev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05052">
<title>A Gentle Introduction to Supervised Machine Learning. (arXiv:1805.05052v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05052</link>
<description rdf:parseType="Literal">&lt;p&gt;This tutorial is based on the lecture notes for the courses &quot;Machine
Learning: Basic Principles&quot; and &quot;Artificial Intelligence&quot;, which I have taught
during fall 2017 and spring 2018 at Aalto university. The aim is to provide an
accessible introduction to some of the main concepts and methods within
supervised machine learning. Most of the current systems which are con- sidered
as (artificially) intelligent are based on some form of supervised machine
learning. After discussing the main building blocks of a formal machine
learning problem, some of the most popular algorithmic design patterns for
machine learning methods are presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_A/0/1/0/all/0/1&quot;&gt;Alexander Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07223">
<title>ASIC Implementation of Time-Domain Digital Backpropagation with Deep-Learned Chromatic Dispersion Filters. (arXiv:1806.07223v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1806.07223</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider time-domain digital backpropagation with chromatic dispersion
filters jointly optimized and quantized using machine-learning techniques.
Compared to the baseline implementations, we show improved BER performance and
&amp;gt;40% power dissipation reductions in 28-nm CMOS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fougstedt_C/0/1/0/all/0/1&quot;&gt;Christoffer Fougstedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hager_C/0/1/0/all/0/1&quot;&gt;Christian H&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1&quot;&gt;Lars Svensson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1&quot;&gt;Henry D. Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larsson_Edefors_P/0/1/0/all/0/1&quot;&gt;Per Larsson-Edefors&lt;/a&gt;</dc:creator>
</item></rdf:RDF>