<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-09-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02440"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02493"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02572"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02195"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05300"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01947"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00076"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02206"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02251"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02260"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02317"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02343"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02378"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02382"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02387"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02499"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08365"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07683"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07717"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01266"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02070"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02153"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02157"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02162"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02188"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02196"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02213"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02230"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02235"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02244"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02262"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02288"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02292"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02314"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02322"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02337"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02341"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02385"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02397"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02403"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02408"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02482"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02505"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02512"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02519"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02589"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02596"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02599"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.00168"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06664"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01648"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07474"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04731"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04256"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07380"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07801"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.08619"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01818"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01921"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1809.02440">
<title>Optimizing deep video representation to match brain activity. (arXiv:1809.02440v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1809.02440</link>
<description rdf:parseType="Literal">&lt;p&gt;The comparison of observed brain activity with the statistics generated by
artificial intelligence systems is useful to probe brain functional
organization under ecological conditions. Here we study fMRI activity in ten
subjects watching color natural movies and compute deep representations of
these movies with an architecture that relies on optical flow and image
content. The association of activity in visual areas with the different layers
of the deep architecture displays complexity-related contrasts across visual
areas and reveals a striking foveal/peripheral dichotomy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richard_H/0/1/0/all/0/1&quot;&gt;Hugo Richard&lt;/a&gt; (PARIETAL), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinho_A/0/1/0/all/0/1&quot;&gt;Ana Pinho&lt;/a&gt; (NEUROSPIN), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thirion_B/0/1/0/all/0/1&quot;&gt;Bertrand Thirion&lt;/a&gt; (PARIETAL), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charpiat_G/0/1/0/all/0/1&quot;&gt;Guillaume Charpiat&lt;/a&gt; (TAU)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02493">
<title>Hierarchical Selective Recruitment in Linear-Threshold Brain Networks - Part II: Inter-Layer Dynamics and Top-Down Recruitment. (arXiv:1809.02493v1 [cs.SY])</title>
<link>http://arxiv.org/abs/1809.02493</link>
<description rdf:parseType="Literal">&lt;p&gt;Goal-driven selective attention (GDSA) is a remarkable function that allows
the complex dynamical networks of the brain to support coherent perception and
cognition. Part I of this two-part paper proposes a new control-theoretic
framework, termed hierarchical selective recruitment (HSR), to rigorously
explain the emergence of GDSA from the brain&apos;s network structure and dynamics.
This part completes the development of HSR by deriving conditions on the joint
structure of the hierarchical subnetworks that guarantee top-down recruitment
of the task-relevant part of each subnetwork by the subnetwork at the layer
immediately above, while inhibiting the activity of task-irrelevant subnetworks
at all the hierarchical layers. To further verify the merit and applicability
of this framework, we carry out a comprehensive case study of selective
listening in rodents and show that a small network with HSR-based structure and
minimal size can explain the data with remarkable accuracy while satisfying the
theoretical requirements of HSR. Our technical approach relies on the theory of
switched systems and provides a novel converse Lyapunov theorem for
state-dependent switched affine systems that is of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nozari_E/0/1/0/all/0/1&quot;&gt;Erfan Nozari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cortes_J/0/1/0/all/0/1&quot;&gt;Jorge Cort&amp;#xe9;s&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02572">
<title>The largest cognitive systems will be optoelectronic. (arXiv:1809.02572v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1809.02572</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrons and photons offer complementary strengths for information
processing. Photons are excellent for communication, while electrons are
superior for computation and memory. Cognition requires distributed computation
to be communicated across the system for information integration. We present
reasoning from neuroscience, network theory, and device physics supporting the
conjecture that large-scale cognitive systems will benefit from electronic
devices performing synaptic, dendritic, and neuronal information processing
operating in conjunction with photonic communication. On the chip scale,
integrated dielectric waveguides enable fan-out to thousands of connections. On
the system scale, fiber and free-space optics can be employed. The largest
cognitive systems will be limited by the distance light can travel during the
period of a network oscillation. We calculate that optoelectronic networks the
area of a large data center ($10^5$\,m$^2$) will be capable of system-wide
information integration at $1$\,MHz. At frequencies of cortex-wide integration
in the human brain ($4$\,Hz, theta band), optoelectronic systems could
integrate information across the surface of the earth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shainline_J/0/1/0/all/0/1&quot;&gt;Jeffrey M. Shainline&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02195">
<title>Granger-causal Attentive Mixtures of Experts: Learning Important Features with Neural Networks. (arXiv:1802.02195v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02195</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge of the importance of input features towards decisions made by
machine-learning models is essential to increase our understanding of both the
models and the underlying data. Here, we present a new approach to estimating
feature importance with neural networks based on the idea of distributing the
features of interest among experts in an attentive mixture of experts (AME).
AMEs use attentive gating networks trained with a Granger-causal objective to
learn to jointly produce accurate predictions as well as estimates of feature
importance in a single model. Our experiments on an established benchmark and
two real-world datasets show (i) that the feature importance estimates provided
by AMEs compare favourably to those provided by state-of-the-art methods, (ii)
that AMEs are significantly faster than existing methods, and (iii) that the
associations discovered by AMEs are consistent with those reported by domain
experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwab_P/0/1/0/all/0/1&quot;&gt;Patrick Schwab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miladinovic_D/0/1/0/all/0/1&quot;&gt;Djordje Miladinovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlen_W/0/1/0/all/0/1&quot;&gt;Walter Karlen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05300">
<title>Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise. (arXiv:1802.05300v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05300</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing importance of massive datasets with the advent of deep learning
makes robustness to label noise a critical property for classifiers to have.
Sources of label noise include automatic labeling for large datasets,
non-expert labeling, and label corruption by data poisoning adversaries. In the
latter case, corruptions may be arbitrarily bad, even so bad that a classifier
predicts the wrong labels with high confidence. To protect against such sources
of noise, we leverage the fact that a small set of clean labels is often easy
to procure. We demonstrate that robustness to label noise up to severe
strengths can be achieved by using a set of trusted data with clean labels, and
propose a loss correction that utilizes trusted examples in a data-efficient
manner to mitigate the effects of label noise on deep neural network
classifiers. Across vision and natural language processing tasks, we experiment
with various label noises at several strengths, and show that our method
significantly outperforms existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1&quot;&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1&quot;&gt;Mantas Mazeika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1&quot;&gt;Duncan Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1&quot;&gt;Kevin Gimpel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01947">
<title>Circuit designs for superconducting optoelectronic loop neurons. (arXiv:1805.01947v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1805.01947</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical communication achieves high fanout and short delay advantageous for
information integration in neural systems. Superconducting detectors enable
signaling with single photons for maximal energy efficiency. We present designs
of superconducting optoelectronic neurons based on superconducting
single-photon detectors, Josephson junctions, semiconductor light sources, and
multi-planar dielectric waveguides. These circuits achieve complex synaptic and
neuronal functions with high energy efficiency, leveraging the strengths of
light for communication and superconducting electronics for computation. The
neurons send few-photon signals to synaptic connections. These signals
communicate neuronal firing events as well as update synaptic weights.
Spike-timing-dependent plasticity is implemented with a single photon
triggering each step of the process. Microscale light-emitting diodes and
waveguide networks enable connectivity from a neuron to thousands of synaptic
connections, and the use of light for communication enables synchronization of
neurons across an area limited only by the distance light can travel within the
period of a network oscillation. Experimentally, each of the requisite circuit
elements has been demonstrated, yet a hardware platform combining them all has
not been attempted. Compared to digital logic or quantum computing, device
tolerances are relaxed. For this neural application, optical sources providing
incoherent pulses with 10,000 photons produced with efficiency of 10$^{-3}$
operating at 20\,MHz at 4.2\,K are sufficient to enable a massively scalable
neural computing platform with connectivity comparable to the brain and thirty
thousand times higher speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shainline_J/0/1/0/all/0/1&quot;&gt;Jeffrey M. Shainline&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_S/0/1/0/all/0/1&quot;&gt;Sonia M. Buckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCaughan_A/0/1/0/all/0/1&quot;&gt;Adam N. McCaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiles_J/0/1/0/all/0/1&quot;&gt;Jeff Chiles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirin_R/0/1/0/all/0/1&quot;&gt;Richard P. Mirin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1&quot;&gt;Sae Woo Nam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00076">
<title>News Session-Based Recommendations using Deep Neural Networks. (arXiv:1808.00076v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1808.00076</link>
<description rdf:parseType="Literal">&lt;p&gt;News recommender systems are aimed to personalize users experiences and help
them to discover relevant articles from a large and dynamic search space.
Therefore, news domain is a challenging scenario for recommendations, due to
its sparse user profiling, fast growing number of items, accelerated item&apos;s
value decay, and users preferences dynamic shift. Some promising results have
been recently achieved by the usage of Deep Learning techniques on Recommender
Systems, specially for item&apos;s feature extraction and for session-based
recommendations with Recurrent Neural Networks. In this paper, it is proposed
an instantiation of the CHAMELEON -- a Deep Learning Meta-Architecture for News
Recommender Systems. This architecture is composed of two modules, the first
responsible to learn news articles representations, based on their text and
metadata, and the second module aimed to provide session-based recommendations
using Recurrent Neural Networks. The recommendation task addressed in this work
is next-item prediction for users sessions: &quot;what is the next most likely
article a user might read in a session?&quot; Users sessions context is leveraged by
the architecture to provide additional information in such extreme cold-start
scenario of news recommendation. Users&apos; behavior and item features are both
merged in an hybrid recommendation approach. A temporal offline evaluation
method is also proposed as a complementary contribution, for a more realistic
evaluation of such task, considering dynamic factors that affect global
readership interests like popularity, recency, and seasonality. Experiments
with an extensive number of session-based recommendation methods were performed
and the proposed instantiation of CHAMELEON meta-architecture obtained a
significant relative improvement in top-n accuracy and ranking metrics (10\% on
Hit Rate and 13\% on MRR) over the best benchmark methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1&quot;&gt;Gabriel de Souza P. Moreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1&quot;&gt;Felipe Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunha_A/0/1/0/all/0/1&quot;&gt;Adilson Marques da Cunha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02193">
<title>Logical Rule Induction and Theory Learning Using Neural Theorem Proving. (arXiv:1809.02193v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.02193</link>
<description rdf:parseType="Literal">&lt;p&gt;A hallmark of human cognition is the ability to continually acquire and
distill observations of the world into meaningful, predictive theories. In this
paper we present a new mechanism for logical theory acquisition which takes a
set of observed facts and learns to extract from them a set of logical rules
and a small set of core facts which together entail the observations. Our
approach is neuro-symbolic in the sense that the rule pred- icates and core
facts are given dense vector representations. The rules are applied to the core
facts using a soft unification procedure to infer additional facts. After k
steps of forward inference, the consequences are compared to the initial
observations and the rules and core facts are then encouraged towards
representations that more faithfully generate the observations through
inference. Our approach is based on a novel neural forward-chaining
differentiable rule induction network. The rules are interpretable and learned
compositionally from their predicates, which may be invented. We demonstrate
the efficacy of our approach on a variety of ILP rule induction and domain
theory learning datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campero_A/0/1/0/all/0/1&quot;&gt;Andres Campero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pareja_A/0/1/0/all/0/1&quot;&gt;Aldo Pareja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klinger_T/0/1/0/all/0/1&quot;&gt;Tim Klinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Josh Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1&quot;&gt;Sebastian Riedel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02206">
<title>Challenges of Context and Time in Reinforcement Learning: Introducing Space Fortress as a Benchmark. (arXiv:1809.02206v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02206</link>
<description rdf:parseType="Literal">&lt;p&gt;Research in deep reinforcement learning (RL) has coalesced around improving
performance on benchmarks like the Arcade Learning Environment. However, these
benchmarks conspicuously miss important characteristics like abrupt
context-dependent shifts in strategy and temporal sensitivity that are often
present in real-world domains. As a result, RL research has not focused on
these challenges, resulting in algorithms which do not understand critical
changes in context, and have little notion of real world time. To tackle this
issue, this paper introduces the game of Space Fortress as a RL benchmark which
incorporates these characteristics. We show that existing state-of-the-art RL
algorithms are unable to learn to play the Space Fortress game. We then confirm
that this poor performance is due to the RL algorithms&apos; context insensitivity
and reward sparsity. We also identify independent axes along which to vary
context and temporal sensitivity, allowing Space Fortress to be used as a
testbed for understanding both characteristics in combination and also in
isolation. We release Space Fortress as an open-source Gym environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Akshat Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hope_R/0/1/0/all/0/1&quot;&gt;Ryan Hope&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1&quot;&gt;Katia Sycara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02251">
<title>Adversarial Feature-Mapping for Speech Enhancement. (arXiv:1809.02251v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1809.02251</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature-mapping with deep neural networks is commonly used for single-channel
speech enhancement, in which a feature-mapping network directly transforms the
noisy features to the corresponding enhanced ones and is trained to minimize
the mean square errors between the enhanced and clean features. In this paper,
we propose an adversarial feature-mapping (AFM) method for speech enhancement
which advances the feature-mapping approach with adversarial learning. An
additional discriminator network is introduced to distinguish the enhanced
features from the real clean ones. The two networks are jointly optimized to
minimize the feature-mapping loss and simultaneously mini-maximize the
discrimination loss. The distribution of the enhanced features is further
pushed towards that of the clean features through this adversarial multi-task
training. To achieve better performance on ASR task, senone-aware (SA) AFM is
further proposed in which an acoustic model network is jointly trained with the
feature-mapping and discriminator networks to optimize the senone
classification loss in addition to the AFM losses. Evaluated on the CHiME-3
dataset, the proposed AFM achieves 16.95% and 5.27% relative word error rate
(WER) improvements over the real noisy data and the feature-mapping baseline
respectively and the SA-AFM achieves 9.85% relative WER improvement over the
multi-conditional acoustic model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Zhong Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yifan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Biing-Hwang/0/1/0/all/0/1&quot;&gt;Biing-Hwang&lt;/a&gt; (Fred) &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Juang/0/1/0/all/0/1&quot;&gt;Juang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02260">
<title>The Force of Proof by Which Any Argument Prevails. (arXiv:1809.02260v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.02260</link>
<description rdf:parseType="Literal">&lt;p&gt;Jakob Bernoulli, working in the late 17th century, identified a gap in
contemporary probability theory. He cautioned that it was inadequate to specify
force of proof (probability of provability) for some kinds of uncertain
arguments. After 300 years, this gap remains in present-day probability theory.
We present axioms analogous to Kolmogorov&apos;s axioms for probability, specifying
uncertainty that lies in an argument&apos;s inference/implication itself rather than
in its premise and conclusion. The axioms focus on arguments spanning two
Boolean algebras, but generalize the obligatory: &quot;force of proof of A implies B
is the probability of B or not A&quot; in the case that the Boolean algebras are
identical. We propose a categorical framework that relies on generalized
probabilities (objects) to express uncertainty in premises, to mix with
arguments (morphisms) to express uncertainty embedded directly in
inference/implication. There is a direct application to Shafer&apos;s evidence
theory (Dempster-Shafer theory), greatly expanding its scope for applications.
Therefore, we can offer this framework not only as an optimal solution to a
difficult historical puzzle, but also to advance the frontiers of contemporary
artificial intelligence.
&lt;/p&gt;
&lt;p&gt;Keywords: force of proof, probability of provability, Ars Conjectandi, non
additive probabilities, evidence theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shay_B/0/1/0/all/0/1&quot;&gt;Brian Shay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brazil_P/0/1/0/all/0/1&quot;&gt;Patrick Brazil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02317">
<title>QoS aware Automatic Web Service Composition with Multiple objectives. (arXiv:1809.02317v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.02317</link>
<description rdf:parseType="Literal">&lt;p&gt;With an increasing number of web services, providing an end-to-end Quality of
Service (QoS) guarantee in responding to user queries is becoming an important
concern. Multiple QoS parameters (e.g., response time, latency, throughput,
reliability, availability, success rate) are associated with a service,
thereby, service composition with a large number of candidate services is a
challenging multi-objective optimization problem. In this paper, we study the
multi-constrained multi-objective QoS aware web service composition problem and
propose three different approaches to solve the same, one optimal, based on
Pareto front construction and two other based on heuristically traversing the
solution space. We compare the performance of the heuristics against the
optimal, and show the effectiveness of our proposals over other classical
approaches for the same problem setting, with experiments on WSC-2009 and
ICEBE-2005 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1&quot;&gt;Soumi Chattopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Ansuman Banerjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02343">
<title>Exploiting local and global performance of candidate systems for aggregation of summarization techniques. (arXiv:1809.02343v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1809.02343</link>
<description rdf:parseType="Literal">&lt;p&gt;With an ever growing number of extractive summarization techniques being
proposed, there is less clarity then ever about how good each system is
compared to the rest. Several studies highlight the variance in performance of
these systems with change in datasets or even across documents within the same
corpus. An effective way to counter this variance and to make the systems more
robust could be to use inputs from multiple systems when generating a summary.
In the present work, we define a novel way of creating such ensemble by
exploiting similarity between the content of candidate summaries to estimate
their reliability. We define GlobalRank which captures the performance of a
candidate system on an overall corpus and LocalRank which estimates its
performance on a given document cluster. We then use these two scores to assign
a weight to each individual systems, which is then used to generate the new
aggregate ranking. Experiments on DUC2003 and DUC 2004 datasets show a
significant improvement in terms of ROUGE score, over existing sate-of-art
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_P/0/1/0/all/0/1&quot;&gt;Parth Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumder_P/0/1/0/all/0/1&quot;&gt;Prasenjit Majumder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02378">
<title>Monte Carlo Tree Search with Scalable Simulation Periods for Continuously Running Tasks. (arXiv:1809.02378v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.02378</link>
<description rdf:parseType="Literal">&lt;p&gt;Monte Carlo Tree Search (MCTS) is particularly adapted to domains where the
potential actions can be represented as a tree of sequential decisions. For an
effective action selection, MCTS performs many simulations to build a reliable
tree representation of the decision space. As such, a bottleneck to MCTS
appears when enough simulations cannot be performed between action selections.
This is particularly highlighted in continuously running tasks, for which the
time available to perform simulations between actions tends to be limited due
to the environment&apos;s state constantly changing. In this paper, we present an
approach that takes advantage of the anytime characteristic of MCTS to increase
the simulation time when allowed. Our approach is to effectively balance the
prospect of selecting an action with the time that can be spared to perform
MCTS simulations before the next action selection. For that, we considered the
simulation time as a decision variable to be selected alongside an action. We
extended the Hierarchical Optimistic Optimization applied to Tree (HOOT) method
to adapt our approach to environments with a continuous decision space. We
evaluated our approach for environments with a continuous decision space
through OpenAI gym&apos;s Pendulum and Continuous Mountain Car environments and for
environments with discrete action space through the arcade learning environment
(ALE) platform. The evaluation results show that, with variable simulation
times, the proposed approach outperforms the conventional MCTS in the evaluated
continuous decision space tasks and improves the performance of MCTS in most of
the ALE tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_S/0/1/0/all/0/1&quot;&gt;Seydou Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hiraoka_T/0/1/0/all/0/1&quot;&gt;Takuya Hiraoka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onishi_T/0/1/0/all/0/1&quot;&gt;Takashi Onishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakata_T/0/1/0/all/0/1&quot;&gt;Toru Nakata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1&quot;&gt;Yoshimasa Tsuruoka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02382">
<title>On2Vec: Embedding-based Relation Prediction for Ontology Population. (arXiv:1809.02382v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.02382</link>
<description rdf:parseType="Literal">&lt;p&gt;Populating ontology graphs represents a long-standing problem for the
Semantic Web community. Recent advances in translation-based graph embedding
methods for populating instance-level knowledge graphs lead to promising new
approaching for the ontology population problem. However, unlike instance-level
graphs, the majority of relation facts in ontology graphs come with
comprehensive semantic relations, which often include the properties of
transitivity and symmetry, as well as hierarchical relations. These
comprehensive relations are often too complex for existing graph embedding
methods, and direct application of such methods is not feasible. Hence, we
propose On2Vec, a novel translation-based graph embedding method for ontology
population. On2Vec integrates two model components that effectively
characterize comprehensive relation facts in ontology graphs. The first is the
Component-specific Model that encodes concepts and relations into
low-dimensional embedding spaces without a loss of relational properties; the
second is the Hierarchy Model that performs focused learning of hierarchical
relation facts. Experiments on several well-known ontology graphs demonstrate
the promising capabilities of On2Vec in predicting and verifying new relation
facts. These promising results also make possible significant improvements in
related methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Muhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yingtao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xuelu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1&quot;&gt;Zijun Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaniolo_C/0/1/0/all/0/1&quot;&gt;Carlo Zaniolo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02387">
<title>Improving On-policy Learning with Statistical Reward Accumulation. (arXiv:1809.02387v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02387</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning has obtained significant breakthroughs in recent
years. Most methods in deep-RL achieve good results via the maximization of the
reward signal provided by the environment, typically in the form of discounted
cumulative returns. Such reward signals represent the immediate feedback of a
particular action performed by an agent. However, tasks with sparse reward
signals are still challenging to on-policy methods. In this paper, we introduce
an effective characterization of past reward statistics (which can be seen as
long-term feedback signals) to supplement this immediate reward feedback. In
particular, value functions are learned with multi-critics supervision,
enabling complex value functions to be more easily approximated in on-policy
learning, even when the reward signals are sparse. We also introduce a novel
exploration mechanism called &quot;hot-wiring&quot; that can give a boost to seemingly
trapped agents. We demonstrate the effectiveness of our advantage actor
multi-critic (A2MC) method across the discrete domains in Atari games as well
as continuous domains in the MuJoCo environments. A video demo is provided at
https://youtu.be/zBmpf3Yz8tc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yubin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Ke Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiaoou Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02499">
<title>MixUp as Locally Linear Out-Of-Manifold Regularization. (arXiv:1809.02499v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02499</link>
<description rdf:parseType="Literal">&lt;p&gt;MixUp, a data augmentation approach through mixing random samples, has been
shown to be able to significantly improve the predictive accuracy of the
current art of deep neural networks. The power of MixUp, however, is mostly
established empirically and its working and effectiveness have not been
explained in any depth. In this paper, we develop a theoretical understanding
for MixUp as a form of out-of-manifold regularization, which constrains the
model on the input space beyond the data manifold. This analytical study also
enables us to identify MixUp&apos;s limitation caused by manifold intrusion, where
synthetic samples collide with real examples of the manifold. Such intrusion
gives rise to over regularization and thereby under-fitting. To address this
issue, we further propose a novel regularizer, where mixing policies are
adaptively learned from the data and a manifold intrusion loss is embraced as
to avoid collision with the data manifold. We empirically show, using several
benchmark datasets, our regularizer&apos;s effectiveness in terms of over
regularization avoiding and accuracy improvement upon current art of deep
classification models and MixUp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hongyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yongyi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Richong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08365">
<title>Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising. (arXiv:1802.08365v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08365</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time bidding (RTB) is an important mechanism in online display
advertising, where a proper bid for each page view plays an essential role for
good marketing results. Budget constrained bidding is a typical scenario in RTB
where the advertisers hope to maximize the total value of the winning
impressions under a pre-set budget constraint. However, the optimal bidding
strategy is hard to be derived due to the complexity and volatility of the
auction environment. To address these challenges, in this paper, we formulate
budget constrained bidding as a Markov Decision Process and propose a
model-free reinforcement learning framework to resolve the optimization
problem. Our analysis shows that the immediate reward from environment is
misleading under a critical resource constraint. Therefore, we innovate a
reward function design methodology for the reinforcement learning problems with
constraints. Based on the new reward design, we employ a deep neural network to
learn the appropriate reward so that the optimal policy can be learned
effectively. Different from the prior model-based work, which suffers from the
scalability problem, our framework is easy to be deployed in large-scale
industrial applications. The experimental evaluations demonstrate the
effectiveness of our framework on large-scale real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiujun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1&quot;&gt;Qing Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoxun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Kun Gai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07683">
<title>Learning Graph-Level Representations with Gated Recurrent Neural Networks. (arXiv:1805.07683v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07683</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently a variety of methods have been developed to encode graphs into
low-dimensional vectors that can be easily exploited by machine learning
algorithms. The majority of these methods start by embedding the graph nodes
into a low-dimensional vector space, followed by using some scheme to aggregate
the node embeddings. In this work, we develop a new approach to learn
graph-level representations, which includes a combination of unsupervised and
supervised learning components. We start by learning a set of global node
representations in an unsupervised fashion, followed by a strategy to map the
graph nodes into sequences of node-neighbor pairs. Gated recurrent neural
network (RNN) units are modified to accommodate both the node representations
as well as their neighborhood information. Experiments on standard graph
classification benchmarks demonstrate that our proposed approach achieves
superior or comparable performance relative to the state-of-the-art algorithms
in terms of convergence speed and classification accuracy. We further
illustrate the effectiveness of the different components used by our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yu Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+JaJa_J/0/1/0/all/0/1&quot;&gt;Joseph F. JaJa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07717">
<title>Weighted Abstract Dialectical Frameworks: Extended and Revised Report. (arXiv:1806.07717v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.07717</link>
<description rdf:parseType="Literal">&lt;p&gt;Abstract Dialectical Frameworks (ADFs) generalize Dung&apos;s argumentation
frameworks allowing various relationships among arguments to be expressed in a
systematic way. We further generalize ADFs so as to accommodate arbitrary
acceptance degrees for the arguments. This makes ADFs applicable in domains
where both the initial status of arguments and their relationship are only
insufficiently specified by Boolean functions. We define all standard ADF
semantics for the weighted case, including grounded, preferred and stable
semantics. We illustrate our approach using acceptance degrees from the unit
interval and show how other valuation structures can be integrated. In each
case it is sufficient to specify how the generalized acceptance conditions are
represented by formulas, and to specify the information ordering underlying the
characteristic ADF operator. We also present complexity results for problems
related to weighted ADFs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brewka_G/0/1/0/all/0/1&quot;&gt;Gerhard Brewka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puhrer_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg P&amp;#xfc;hrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strass_H/0/1/0/all/0/1&quot;&gt;Hannes Strass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallner_J/0/1/0/all/0/1&quot;&gt;Johannes P. Wallner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woltran_S/0/1/0/all/0/1&quot;&gt;Stefan Woltran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01266">
<title>Coverage-Guided Fuzzing for Deep Neural Networks. (arXiv:1809.01266v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/1809.01266</link>
<description rdf:parseType="Literal">&lt;p&gt;In company with the data explosion over the past decade, deep neural network
(DNN) based software has experienced unprecedented leap and is becoming the key
driving force of many novel industrial applications, including many
safety-critical scenarios such as autonomous driving. Despite great success
achieved in various human intelligence tasks, similar to traditional software,
DNNs could also exhibit incorrect behaviors caused by hidden defects causing
severe accidents and losses. In this paper, we propose an automated fuzz
testing framework for hunting potential defects of general-purpose DNNs. It
performs metamorphic mutation to generate new semantically preserved tests, and
leverages multiple plugable coverage criteria as feedback to guide the test
generation from different perspectives. To be scalable towards practical-sized
DNNs, our framework maintains tests in batch, and prioritizes the tests
selection based on active feedback. The effectiveness of our framework is
extensively investigated on 3 popular datasets (MNIST, CIFAR-10, ImageNet) and
7 DNNs with diverse complexities, under large set of 6 coverage criteria as
feedback. The large-scale experiments demonstrate that our fuzzing framework
can (1) significantly boost the coverage with guidance; (2) generate useful
tests to detect erroneous behaviors and facilitate the DNN model quality
evaluation; (3) accurately capture potential defects during DNN quantization
for platform migration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaofei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1&quot;&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongxu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1&quot;&gt;Minhui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jianjun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianxiong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1&quot;&gt;Simon See&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02070">
<title>ARCHER: Aggressive Rewards to Counter bias in Hindsight Experience Replay. (arXiv:1809.02070v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.02070</link>
<description rdf:parseType="Literal">&lt;p&gt;Experience replay is an important technique for addressing
sample-inefficiency in deep reinforcement learning (RL), but faces difficulty
in learning from binary and sparse rewards due to disproportionately few
successful experiences in the replay buffer. Hindsight experience replay (HER)
was recently proposed to tackle this difficulty by manipulating unsuccessful
transitions, but in doing so, HER introduces a significant bias in the replay
buffer experiences and therefore achieves a suboptimal improvement in
sample-efficiency. In this paper, we present an analysis on the source of bias
in HER, and propose a simple and effective method to counter the bias, to most
effectively harness the sample-efficiency provided by HER. Our method,
motivated by counter-factual reasoning and called ARCHER, extends HER with a
trade-off to make rewards calculated for hindsight experiences numerically
greater than real rewards. We validate our algorithm on two continuous control
environments from DeepMind Control Suite - Reacher and Finger, which simulate
manipulation tasks with a robotic arm - in combination with various reward
functions, task complexities and goal sampling strategies. Our experiments
consistently demonstrate that countering bias using more aggressive hindsight
rewards increases sample efficiency, thus establishing the greater benefit of
ARCHER in RL applications with limited computing budget.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanka_S/0/1/0/all/0/1&quot;&gt;Sameera Lanka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianfu Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02153">
<title>Variational Bayesian Inference for Robust Streaming Tensor Factorization and Completion. (arXiv:1809.02153v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.02153</link>
<description rdf:parseType="Literal">&lt;p&gt;Streaming tensor factorization is a powerful tool for processing high-volume
and multi-way temporal data in Internet networks, recommender systems and
image/video data analysis. Existing streaming tensor factorization algorithms
rely on least-squares data fitting and they do not possess a mechanism for
tensor rank determination. This leaves them susceptible to outliers and
vulnerable to over-fitting. This paper presents a Bayesian robust streaming
tensor factorization model to identify sparse outliers, automatically determine
the underlying tensor rank and accurately fit low-rank structure. We implement
our model in Matlab and compare it with existing algorithms on tensor datasets
generated from dynamic MRI and Internet traffic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hawkins_C/0/1/0/all/0/1&quot;&gt;Cole Hawkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02157">
<title>Large Scale Learning with Kre\u{\i}n Kernels. (arXiv:1809.02157v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.02157</link>
<description rdf:parseType="Literal">&lt;p&gt;We extend the Nystr\&quot;om method for low-rank approximation of positive
definite Mercer kernels to approximation of indefinite kernel matrices. Our
result is the first derivation of the approach that does not require the
positive definiteness of the kernel function. Building on this result, we then
devise highly scalable methods for learning in reproducing kernel Kre\u{\i}n
spaces. The main motivation for our work comes from problems with structured
representations (e.g., graphs, strings, time-series), where it is relatively
easy to devise a pairwise (dis)similarity function based on intuition/knowledge
of a domain expert. Such pairwise functions are typically not positive definite
and it is often well beyond the expertise of practitioners to verify this
condition. The proposed large scale approaches for learning in reproducing
kernel Kre\u{\i}n spaces provide principled and theoretically well-founded
means to tackle this class of problems. The effectiveness of the approaches is
evaluated empirically using kernels defined on structured and vectorial data
representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oglic_D/0/1/0/all/0/1&quot;&gt;Dino Oglic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gartner_T/0/1/0/all/0/1&quot;&gt;Thomas G&amp;#xe4;rtner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02162">
<title>Escaping Saddle Points in Constrained Optimization. (arXiv:1809.02162v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02162</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on escaping from saddle points in smooth nonconvex
optimization problems subject to a convex set $\mathcal{C}$. We propose a
generic framework that yields convergence to a second-order stationary point of
the problem, if the convex set $\mathcal{C}$ is simple for a quadratic
objective function. To be more precise, our results hold if one can find a
$\rho$-approximate solution of a quadratic program subject to $\mathcal{C}$ in
polynomial time, where $\rho&amp;lt;1$ is a positive constant that depends on the
structure of the set $\mathcal{C}$. Under this condition, we show that the
sequence of iterates generated by the proposed framework reaches an
$(\epsilon,\gamma)$-second order stationary point (SOSP) in at most
$\mathcal{O}(\max\{\epsilon^{-2},\rho^{-3}\gamma^{-3}\})$ iterations. We
further characterize the overall arithmetic operations to reach an SOSP when
the convex set $\mathcal{C}$ can be written as a set of quadratic constraints.
Finally, we extend our results to the stochastic setting and characterize the
number of stochastic gradient and Hessian evaluations to reach an
$(\epsilon,\gamma)$-SOSP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mokhtari_A/0/1/0/all/0/1&quot;&gt;Aryan Mokhtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozdaglar_A/0/1/0/all/0/1&quot;&gt;Asuman Ozdaglar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jadbabaie_A/0/1/0/all/0/1&quot;&gt;Ali Jadbabaie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02188">
<title>Differentially Private Bayesian Inference for Exponential Families. (arXiv:1809.02188v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02188</link>
<description rdf:parseType="Literal">&lt;p&gt;The study of private inference has been sparked by growing concern regarding
the analysis of data when it stems from sensitive sources. We present the first
method for private Bayesian inference in exponential families that properly
accounts for noise introduced by the privacy mechanism. It is efficient because
it works only with sufficient statistics and not individual data. Unlike other
methods, it gives properly calibrated posterior beliefs in the non-asymptotic
data regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernstein_G/0/1/0/all/0/1&quot;&gt;Garrett Bernstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheldon_D/0/1/0/all/0/1&quot;&gt;Daniel Sheldon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02196">
<title>Bayesian Nonparametric Spectral Estimation. (arXiv:1809.02196v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.02196</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral estimation (SE) aims to identify how the energy of a signal (e.g., a
time series) is distributed across different frequencies. This can become
particularly challenging when only partial and noisy observations are
available, where current methods fail to handle uncertainty appropriately. In
this context, we propose a joint probabilistic model for signals, observations
and spectra, where SE is addressed as an inference problem. Assuming a Gaussian
process prior over the signal, we apply Bayes&apos; rule to find the analytic
posterior distribution of the spectrum given a set of observations. Besides its
expressiveness and natural account of spectral uncertainty, the proposed model
also provides a functional-form representation of the power spectral density,
which can be optimised efficiently. Comparison with previous approaches is
addressed theoretically, showing that the proposed method is an
infinite-dimensional variant of the Lomb-Scargle approach, and also empirically
through three experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tobar_F/0/1/0/all/0/1&quot;&gt;Felipe Tobar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02213">
<title>Dynamic Hierarchical Empirical Bayes: A Predictive Model Applied to Online Advertising. (arXiv:1809.02213v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.02213</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting keywords performance, such as number of impressions, click-through
rate (CTR), conversion rate (CVR), revenue per click (RPC), and cost per click
(CPC), is critical for sponsored search in the online advertising industry. An
interesting phenomenon is that, despite the size of the overall data, the data
are very sparse at the individual unit level. To overcome the sparsity and
leverage hierarchical information across the data structure, we propose a
Dynamic Hierarchical Empirical Bayesian (DHEB) model that dynamically
determines the hierarchy through a data-driven process and provides
shrinkage-based estimations. Our method is also equipped with an efficient
empirical approach to derive inferences through the hierarchy. We evaluate the
proposed method in both simulated and real-world datasets and compare to
several competitive models. The results favor the proposed method among all
comparisons in terms of both accuracy and efficiency. In the end, we design a
two-phase system to serve prediction in real time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiaojing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yiwen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pani_A/0/1/0/all/0/1&quot;&gt;Abhishek Pani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02230">
<title>Deep Neural Net with Attention for Multi-channel Multi-touch Attribution. (arXiv:1809.02230v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02230</link>
<description rdf:parseType="Literal">&lt;p&gt;Customers are usually exposed to online digital advertisement channels, such
as email marketing, display advertising, paid search engine marketing, along
their way to purchase or subscribe products( aka. conversion). The marketers
track all the customer journey data and try to measure the effectiveness of
each advertising channel. The inference about the influence of each channel
plays an important role in budget allocation and inventory pricing decisions.
Several simplistic rule-based strategies and data-driven algorithmic strategies
have been widely used in marketing field, but they do not address the issues,
such as channel interaction, time dependency, user characteristics. In this
paper, we propose a novel attribution algorithm based on deep learning to
assess the impact of each advertising channel. We present Deep Neural Net With
Attention multi-touch attribution model (DNAMTA) model in a supervised learning
fashion of predicting if a series of events leads to conversion, and it leads
us to have a deep understanding of the dynamic interaction effects between
media channels. DNAMTA also incorporates user-context information, such as user
demographics and behavior, as control variables to reduce the estimation biases
of media effects. We used computational experiment of large real world
marketing dataset to demonstrate that our proposed model is superior to
existing methods in both conversion prediction and media channel influence
evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+li_N/0/1/0/all/0/1&quot;&gt;Ning li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arava_S/0/1/0/all/0/1&quot;&gt;Sai Kumar Arava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pani_A/0/1/0/all/0/1&quot;&gt;Abhishek Pani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02235">
<title>A Bandit Approach to Multiple Testing with False Discovery Control. (arXiv:1809.02235v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.02235</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an adaptive sampling approach for multiple testing which aims to
maximize statistical power while ensuring anytime false discovery control. We
consider $n$ distributions whose means are partitioned by whether they are
below or equal to a baseline (nulls), versus above the baseline (actual
positives). In addition, each distribution can be sequentially and repeatedly
sampled. Inspired by the multi-armed bandit literature, we provide an algorithm
that takes as few samples as possible to exceed a target true positive
proportion (i.e. proportion of actual positives discovered) while giving
anytime control of the false discovery proportion (nulls predicted as actual
positives). Our sample complexity results match known information theoretic
lower bounds and through simulations we show a substantial performance
improvement over uniform sampling and an adaptive elimination style algorithm.
Given the simplicity of the approach, and its sample efficiency, the method has
promise for wide adoption in the biological sciences, clinical testing for drug
discovery, and online A/B/n testing problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jamieson_K/0/1/0/all/0/1&quot;&gt;Kevin Jamieson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jain_L/0/1/0/all/0/1&quot;&gt;Lalit Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02244">
<title>Learning Optimal Fair Policies. (arXiv:1809.02244v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02244</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning optimal policies from observational data
in a way that satisfies certain fairness criteria. The issue of fairness arises
where some covariates used in decision making are sensitive features, or are
correlated with sensitive features. (Nabi and Shpitser 2018) formalized
fairness in the context of regression problems as constraining the causal
effects of sensitive features along certain disallowed causal pathways. The
existence of these causal effects may be called retrospective unfairness in the
sense of already being present in the data before analysis begins, and may be
due to discriminatory practices or the biased way in which variables are
defined or recorded. In the context of learning policies, what we call
prospective bias, i.e., the inappropriate dependence of learned policies on
sensitive features, is also possible. In this paper, we use methods from causal
and semiparametric inference to learn optimal policies in a way that addresses
both retrospective bias in the data, and prospective bias due to the policy. In
addition, our methods appropriately address statistical bias due to model
misspecification and confounding bias, which are important in the estimation of
path-specific causal effects from observational data. We apply our methods to
both synthetic data and real criminal justice data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nabi_R/0/1/0/all/0/1&quot;&gt;Razieh Nabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malinsky_D/0/1/0/all/0/1&quot;&gt;Daniel Malinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shpitser_I/0/1/0/all/0/1&quot;&gt;Ilya Shpitser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02262">
<title>Logistic Regression Augmented Community Detection for Network Data with Application in Identifying Autism-Related Gene Pathways. (arXiv:1809.02262v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.02262</link>
<description rdf:parseType="Literal">&lt;p&gt;When searching for gene pathways leading to specific disease outcomes,
additional information on gene characteristics is often available that may
facilitate to differentiate genes related to the disease from irrelevant
background when connections involving both types of genes are observed and
their relationships to the disease are unknown. We propose method to single out
irrelevant background genes with the help of auxiliary information through a
logistic regression, and cluster relevant genes into cohesive groups using the
adjacency matrix. Expectation-maximization algorithm is modified to maximize a
joint pseudo-likelihood assuming latent indicators for relevance to the disease
and latent group memberships as well as Poisson or multinomial distributed link
numbers within and between groups. A robust version allowing arbitrary linkage
patterns within the background is further derived. Asymptotic consistency of
label assignments under the stochastic blockmodel is proven. Superior
performance and robustness in finite samples are observed in simulation
studies. The proposed robust method identifies previously missed gene sets
underlying autism related neurological diseases using diverse data sources
including de novo mutations, gene expressions and protein-protein interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pan_Q/0/1/0/all/0/1&quot;&gt;Qing Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chengan Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02288">
<title>Tensor Ring Decomposition with Rank Minimization on Latent Space: An Efficient Approach for Tensor Completion. (arXiv:1809.02288v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02288</link>
<description rdf:parseType="Literal">&lt;p&gt;In tensor completion tasks, the traditional low-rank tensor decomposition
models suffer from laborious model selection problem due to high model
sensitivity. Especially for tensor ring (TR) decomposition, the number of model
possibility grows exponentially with the tensor order, which makes it rather
challenging to find the optimal TR decomposition. In this paper, by exploiting
the low-rank structure on TR latent space, we propose a novel tensor completion
method, which is robust to model selection. In contrast to imposing low-rank
constraint on the data space, we introduce nuclear norm regularization on the
latent TR factors, resulting in that the optimization step using singular value
decomposition (SVD) can be performed at a much smaller scale. By leveraging the
alternating direction method of multipliers (ADMM) scheme, the latent TR
factors with optimal rank and the recovered tensor can be obtained
simultaneously. Our proposed algorithm effectively alleviates the burden of
TR-rank selection, therefore the computational cost is greatly reduced. The
extensive experimental results on synthetic data and real-world data
demonstrate the superior high performance and efficiency of the proposed
approach against the state-of-the-art algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Longhao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandic_D/0/1/0/all/0/1&quot;&gt;Danilo Mandic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jianting Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qibin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02292">
<title>A Block Coordinate Ascent Algorithm for Mean-Variance Optimization. (arXiv:1809.02292v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02292</link>
<description rdf:parseType="Literal">&lt;p&gt;Risk management in dynamic decision problems is a primary concern in many
fields, including financial investment, autonomous driving, and healthcare. The
mean-variance function is one of the most widely used objective functions in
risk management due to its simplicity and interpretability. Existing algorithms
for mean-variance optimization are based on multi-time-scale stochastic
approximation, whose learning rate schedules are often hard to tune, and have
only asymptotic convergence proof. In this paper, we develop a model-free
policy search framework for mean-variance optimization with finite-sample error
bound analysis (to local optima). Our starting point is a reformulation of the
original mean-variance function with its Fenchel dual, from which we propose a
stochastic block coordinate ascent policy search algorithm. Both the asymptotic
convergence guarantee of the last iteration&apos;s solution and the convergence rate
of the randomly picked solution are provided, and their applicability is
demonstrated on several benchmark domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tengyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yangyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1&quot;&gt;Mohammad Ghavamzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1&quot;&gt;Yinlam Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_D/0/1/0/all/0/1&quot;&gt;Daoming Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1&quot;&gt;Daesub Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02314">
<title>Fast greedy algorithms for dictionary selection with generalized sparsity constraints. (arXiv:1809.02314v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02314</link>
<description rdf:parseType="Literal">&lt;p&gt;In dictionary selection, several atoms are selected from finite candidates
that successfully approximate given data points in the sparse representation.
We propose a novel efficient greedy algorithm for dictionary selection. Not
only does our algorithm work much faster than the known methods, but it can
also handle more complex sparsity constraints, such as average sparsity. Using
numerical experiments, we show that our algorithm outperforms the known methods
for dictionary selection, achieving competitive performances with dictionary
learning algorithms in a smaller running time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1&quot;&gt;Kaito Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soma_T/0/1/0/all/0/1&quot;&gt;Tasuku Soma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02322">
<title>ADM for grid CRF loss in CNN segmentation. (arXiv:1809.02322v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02322</link>
<description rdf:parseType="Literal">&lt;p&gt;Variants of gradient descent (GD) dominate CNN loss minimization in computer
vision. But, as we show, some powerful loss functions are practically useless
only due to their poor optimization by GD. In the context of weakly-supervised
CNN segmentation, we present a general ADM approach to regularized losses,
which are inspired by well-known MRF/CRF models in &quot;shallow&quot; segmentation.
While GD fails on the popular nearest-neighbor Potts loss, ADM splitting with
$\alpha$-expansion solver significantly improves optimization of such grid CRF
losses yielding state-of-the-art training quality. Denser CRF losses become
amenable to basic GD, but they produce lower quality object boundaries in
agreement with known noisy performance of dense CRF inference in shallow
segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marin_D/0/1/0/all/0/1&quot;&gt;Dmitrii Marin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1&quot;&gt;Meng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1&quot;&gt;Ismail Ben Ayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boykov_Y/0/1/0/all/0/1&quot;&gt;Yuri Boykov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02337">
<title>Information-Theoretic Active Learning for Content-Based Image Retrieval. (arXiv:1809.02337v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.02337</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Information-Theoretic Active Learning (ITAL), a novel batch-mode
active learning method for binary classification, and apply it for acquiring
meaningful user feedback in the context of content-based image retrieval.
Instead of combining different heuristics such as uncertainty, diversity, or
density, our method is based on maximizing the mutual information between the
predicted relevance of the images and the expected user feedback regarding the
selected batch. We propose suitable approximations to this computationally
demanding problem and also integrate an explicit model of user behavior that
accounts for possible incorrect labels and unnameable instances. Furthermore,
our approach does not only take the structure of the data but also the expected
model output change caused by the user feedback into account. In contrast to
other methods, ITAL turns out to be highly flexible and provides
state-of-the-art performance across various datasets, such as MIRFLICKR and
ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barz_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Barz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kading_C/0/1/0/all/0/1&quot;&gt;Christoph K&amp;#xe4;ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1&quot;&gt;Joachim Denzler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02341">
<title>An Anderson-Chebyshev Mixing Method for Nonlinear Optimization. (arXiv:1809.02341v1 [math.OC])</title>
<link>http://arxiv.org/abs/1809.02341</link>
<description rdf:parseType="Literal">&lt;p&gt;Anderson mixing (or Anderson acceleration) is an efficient acceleration
method for fixed point iterations (i.e., $x_{t+1}=G(x_t)$), e.g., gradient
descent can be viewed as iteratively applying the operation $G(x) =
x-\alpha\nabla f(x)$. It is known that Anderson mixing is quite efficient in
practice and can be viewed as an extension of Krylov subspace methods for
nonlinear problems. First, we show that Anderson mixing with Chebyshev
polynomial parameters can achieve the optimal convergence rate
$O(\sqrt{\kappa}\ln\frac{1}{\epsilon})$, which improves the previous result
$O(\kappa\ln\frac{1}{\epsilon})$ provided by [Toth and Kelley, 2015] for
quadratic functions. Then, we provide a convergence analysis for minimizing
general nonlinear problems. Besides, if the hyperparameters (e.g., the
Lipschitz smooth parameter $L$) are not available, we propose a Guessing
Algorithm for guessing them dynamically and also prove a similar convergence
rate. Finally, the experimental results demonstrate that the proposed
Anderson-Chebyshev mixing method converges significantly faster than other
algorithms, e.g., vanilla gradient descent (GD), Nesterov&apos;s Accelerated GD.
Also, these algorithms combined with the proposed guessing algorithm (guessing
the hyperparameters dynamically) achieve much better performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhize Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02385">
<title>Mixtures of Skewed Matrix Variate Bilinear Factor Analyzers. (arXiv:1809.02385v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1809.02385</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering is the process of finding and analyzing underlying group
structures in data. In recent years, data as become increasingly higher
dimensional and therefore an increased need for dimension reduction techniques
for use in clustering. Although such techniques are firmly established in the
literature for multivariate data, there is a relative paucity in the area of
matrix variate or three way data. Furthermore, these few methods all assume
matrix variate normality which is not always sensible if skewness is present.
We propose a mixture of bilinear factor analyzers model using four skewed
matrix variate distributions, namely the matrix variate skew-t, generalized
hyperbolic, variance gamma and normal inverse Gaussian distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gallaugher_M/0/1/0/all/0/1&quot;&gt;Michael P.B. Gallaugher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McNicholas_P/0/1/0/all/0/1&quot;&gt;Paul D. McNicholas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02397">
<title>Detecting Potential Local Adversarial Examples for Human-Interpretable Defense. (arXiv:1809.02397v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.02397</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models are increasingly used in the industry to make
decisions such as credit insurance approval. Some people may be tempted to
manipulate specific variables, such as the age or the salary, in order to get
better chances of approval. In this ongoing work, we propose to discuss, with a
first proposition, the issue of detecting a potential local adversarial example
on classical tabular data by providing to a human expert the locally critical
features for the classifier&apos;s decision, in order to control the provided
information and avoid a fraud.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Renard_X/0/1/0/all/0/1&quot;&gt;Xavier Renard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laugel_T/0/1/0/all/0/1&quot;&gt;Thibault Laugel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lesot_M/0/1/0/all/0/1&quot;&gt;Marie-Jeanne Lesot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marsala_C/0/1/0/all/0/1&quot;&gt;Christophe Marsala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Detyniecki_M/0/1/0/all/0/1&quot;&gt;Marcin Detyniecki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02403">
<title>Deep Recurrent Survival Analysis. (arXiv:1809.02403v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02403</link>
<description rdf:parseType="Literal">&lt;p&gt;Survival analysis is a hotspot in statistical research for modeling
time-to-event information with data censorship handling, which has been widely
used in many applications such as clinical research, information system and
other fields with survivorship bias. Many works have been proposed for survival
analysis ranging from traditional statistic methods to machine learning models.
However, the existing methodologies either utilize counting-based statistics on
the segmented data, or have a pre-assumption on the event probability
distribution w.r.t. time. Moreover, few works consider sequential patterns
within the feature space. In this paper, we propose a Deep Recurrent Survival
Analysis model which combines deep learning for conditional probability
prediction at fine-grained level of the data, and survival analysis for
tackling the censorship. By capturing the time dependency through modeling the
conditional probability of the event for each sample, our method predicts the
likelihood of the true event occurrence and estimates the survival rate over
time, i.e., the probability of the non-occurrence of the event, for the
censored data. Meanwhile, without assuming any specific form of the event
probability distribution, our model shows great advantages over the previous
works on fitting various sophisticated data distributions. In the experiments
on the three real-world tasks from different fields, our model significantly
outperforms the state-of-the-art solutions under various metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jiarui Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02408">
<title>A Primer on Causality in Data Science. (arXiv:1809.02408v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1809.02408</link>
<description rdf:parseType="Literal">&lt;p&gt;Many questions in Data Science are fundamentally causal in that our objective
is to learn the effect of some exposure (randomized or not) on an outcome
interest. Even studies that are seemingly non-causal (e.g. prediction or
prevalence estimation) have causal elements, such as differential censoring or
measurement. As a result, we, as Data Scientists, need to consider the
underlying causal mechanisms that gave rise to the data, rather than simply the
pattern or association observed in the data. In this work, we review the
&quot;Causal Roadmap&quot;, a formal framework to augment our traditional statistical
analyses in an effort to answer the causal questions driving our research.
Specific steps of the Roadmap include clearly stating the scientific question,
defining of the causal model, translating the scientific question into a causal
parameter, assessing the assumptions needed to translate the causal parameter
into a statistical estimand, implementation of statistical estimators including
parametric and semi-parametric methods, and interpretation of our findings.
Throughout we focus on the effect of an exposure occurring at a single time
point and provide extensions to more advanced settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saddiki_H/0/1/0/all/0/1&quot;&gt;Hachem Saddiki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balzer_L/0/1/0/all/0/1&quot;&gt;Laura B. Balzer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02482">
<title>BiasedWalk: Biased Sampling for Representation Learning on Graphs. (arXiv:1809.02482v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02482</link>
<description rdf:parseType="Literal">&lt;p&gt;Network embedding algorithms are able to learn latent feature representations
of nodes, transforming networks into lower dimensional vector representations.
Typical key applications, which have effectively been addressed using network
embeddings, include link prediction, multilabel classification and community
detection. In this paper, we propose BiasedWalk, a scalable, unsupervised
feature learning algorithm that is based on biased random walks to sample
context information about each node in the network. Our random-walk based
sampling can behave as Breath-First-Search (BFS) and Depth-First-Search (DFS)
samplings with the goal to capture homophily and role equivalence between the
nodes in the network. We have performed a detailed experimental evaluation
comparing the performance of the proposed algorithm against various baseline
methods, on several datasets and learning tasks. The experiment results show
that the proposed method outperforms the baseline ones in most of the tasks and
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duong Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malliaros_F/0/1/0/all/0/1&quot;&gt;Fragkiskos D. Malliaros&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02497">
<title>Sparse Kernel PCA for Outlier Detection. (arXiv:1809.02497v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02497</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a new method to perform Sparse Kernel Principal
Component Analysis (SKPCA) and also mathematically analyze the validity of
SKPCA. We formulate SKPCA as a constrained optimization problem with elastic
net regularization (Hastie et al.) in kernel feature space and solve it. We
consider outlier detection (where KPCA is employed) as an application for
SKPCA, using the RBF kernel. We test it on 5 real-world datasets and show that
by using just 4% (or even less) of the principal components (PCs), where each
PC has on average less than 12% non-zero elements in the worst case among all 5
datasets, we are able to nearly match and in 3 datasets even outperform KPCA.
We also compare the performance of our method with a recently proposed method
for SKPCA by Wang et al. and show that our method performs better in terms of
both accuracy and sparsity. We also provide a novel probabilistic proof to
justify the existence of sparse solutions for KPCA using the RBF kernel. To the
best of our knowledge, this is the first attempt at theoretically analyzing the
validity of SKPCA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1&quot;&gt;Rudrajit Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1&quot;&gt;Aditya Golatkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awate_S/0/1/0/all/0/1&quot;&gt;Suyash P. Awate&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02505">
<title>Stochastically Controlled Stochastic Gradient for the Convex and Non-convex Composition problem. (arXiv:1809.02505v1 [math.OC])</title>
<link>http://arxiv.org/abs/1809.02505</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the convex and non-convex composition problem with
the structure $\frac{1}{n}\sum\nolimits_{i = 1}^n {{F_i}( {G( x )} )}$, where
$G( x )=\frac{1}{n}\sum\nolimits_{j = 1}^n {{G_j}( x )} $ is the inner
function, and $F_i(\cdot)$ is the outer function. We explore the variance
reduction based method to solve the composition optimization. Due to the fact
that when the number of inner function and outer function are large, it is not
reasonable to estimate them directly, thus we apply the stochastically
controlled stochastic gradient (SCSG) method to estimate the gradient of the
composition function and the value of the inner function. The query complexity
of our proposed method for the convex and non-convex problem is equal to or
better than the current method for the composition problem. Furthermore, we
also present the mini-batch version of the proposed method, which has the
improved the query complexity with related to the size of the mini-batch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02512">
<title>Multi-level hypothesis testing for populations of heterogeneous networks. (arXiv:1809.02512v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.02512</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we consider hypothesis testing and anomaly detection on
datasets where each observation is a weighted network. Examples of such data
include brain connectivity networks from fMRI flow data, or word co-occurrence
counts for populations of individuals. Current approaches to hypothesis testing
for weighted networks typically requires thresholding the edge-weights, to
transform the data to binary networks. This results in a loss of information,
and outcomes are sensitivity to choice of threshold levels. Our work avoids
this, and we consider weighted-graph observations in two situations, 1) where
each graph belongs to one of two populations, and 2) where entities belong to
one of two populations, with each entity possessing multiple graphs (indexed
e.g. by time). Specifically, we propose a hierarchical Bayesian hypothesis
testing framework that models each population with a mixture of latent space
models for weighted networks, and then tests populations of networks for
differences in distribution over components. Our framework is capable of
population-level, entity-specific, as well as edge-specific hypothesis testing.
We apply it to synthetic data and three real-world datasets: two social media
datasets involving word co-occurrences from discussions on Twitter of the
political unrest in Brazil, and on Instagram concerning Attention Deficit
Hyperactivity Disorder (ADHD) and depression drugs, and one medical dataset
involving fMRI brain-scans of human subjects. The results show that our
proposed method has lower Type I error and higher statistical power compared to
alternatives that need to threshold the edge weights. Moreover, they show our
proposed method is better suited to deal with highly heterogeneous datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gomes_G/0/1/0/all/0/1&quot;&gt;Guilherme Gomes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rao_V/0/1/0/all/0/1&quot;&gt;Vinayak Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neville_J/0/1/0/all/0/1&quot;&gt;Jennifer Neville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02519">
<title>Fairness Through Causal Awareness: Learning Latent-Variable Models for Biased Data. (arXiv:1809.02519v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02519</link>
<description rdf:parseType="Literal">&lt;p&gt;How do we learn from biased data? Historical datasets often reflect
historical prejudices; sensitive or protected attributes may affect the
observed treatments and outcomes. Classification algorithms tasked with
predicting outcomes accurately from these datasets tend to replicate these
biases. We advocate a causal modeling approach to learning from biased data and
reframe fair classification as an intervention problem. We propose a causal
model in which the sensitive attribute confounds both the treatment and the
outcome. Building on prior work in deep learning and generative modeling, we
describe how to learn the parameters of this causal model from observational
data alone, even in the presence of unobserved confounders. We show
experimentally that fairness-aware causal modeling provides better estimates of
the causal effects between the sensitive attribute, the treatment, and the
outcome. We further present evidence that estimating these causal effects can
help us to learn policies which are both more accurate and fair, when presented
with a historically biased dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madras_D/0/1/0/all/0/1&quot;&gt;David Madras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1&quot;&gt;Elliot Creager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pitassi_T/0/1/0/all/0/1&quot;&gt;Toniann Pitassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02589">
<title>HyperGCN: Hypergraph Convolutional Networks for Semi-Supervised Classification. (arXiv:1809.02589v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02589</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph-based semi-supervised learning (SSL) is an important learning problem
where the goal is to assign labels to initially unlabeled nodes in a graph.
Graph Convolutional Networks (GCNs) have recently been shown to be effective
for graph-based SSL problems. GCNs inherently assume existence of pairwise
relationships in the graph-structured data. However, in many real-world
problems, relationships go beyond pairwise connections and hence are more
complex. Hypergraphs provide a natural modeling tool to capture such complex
relationships. In this work, we explore the use of GCNs for hypergraph-based
SSL. In particular, we propose HyperGCN, an SSL method which uses a layer-wise
propagation rule for convolutional neural networks operating directly on
hypergraphs. To the best of our knowledge, this is the first principled
adaptation of GCNs to hypergraphs. HyperGCN is able to encode both the
hypergraph structure and hypernode features in an effective manner. Through
detailed experimentation, we demonstrate HyperGCN&apos;s effectiveness at
hypergraph-based SSL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadati_N/0/1/0/all/0/1&quot;&gt;Naganand Yadati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nimishakavi_M/0/1/0/all/0/1&quot;&gt;Madhav Nimishakavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1&quot;&gt;Prateek Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louis_A/0/1/0/all/0/1&quot;&gt;Anand Louis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1&quot;&gt;Partha Talukdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02596">
<title>VOS: a Method for Variational Oversampling of Imbalanced Data. (arXiv:1809.02596v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.02596</link>
<description rdf:parseType="Literal">&lt;p&gt;Class imbalanced datasets are common in real-world applications that range
from credit card fraud detection to rare disease diagnostics. Several popular
classification algorithms assume that classes are approximately balanced, and
hence build the accompanying objective function to maximize an overall accuracy
rate. In these situations, optimizing the overall accuracy will lead to highly
skewed predictions towards the majority class. Moreover, the negative business
impact resulting from false positives (positive samples incorrectly classified
as negative) can be detrimental. Many methods have been proposed to address the
class imbalance problem, including methods such as over-sampling,
under-sampling and cost-sensitive methods. In this paper, we consider the
over-sampling method, where the aim is to augment the original dataset with
synthetically created observations of the minority classes. In particular,
inspired by the recent advances in generative modelling techniques (e.g.,
Variational Inference and Generative Adversarial Networks), we introduce a new
oversampling technique based on variational autoencoders. Our experiments show
that the new method is superior in augmenting datasets for downstream
classification tasks when compared to traditional oversampling methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fajardo_V/0/1/0/all/0/1&quot;&gt;Val Andrei Fajardo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Findlay_D/0/1/0/all/0/1&quot;&gt;David Findlay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Houmanfar_R/0/1/0/all/0/1&quot;&gt;Roshanak Houmanfar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jaiswal_C/0/1/0/all/0/1&quot;&gt;Charu Jaiswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jiaxi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Honglei Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02599">
<title>Simple coarse graining and sampling strategies for image recognition. (arXiv:1809.02599v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.02599</link>
<description rdf:parseType="Literal">&lt;p&gt;A conceptually simple way to recognize images is to directly compare test-set
data and training-set data. The accuracy of this approach is limited by the
method of comparison used, and by the extent to which the training-set data
covers the required configuration space. Here we show that this coverage can be
substantially increased using simple strategies of coarse graining (replacing
groups of images by their centroids) and sampling (using distinct sets of
centroids in combination). We use the MNIST data set to show that coarse
graining can be used to convert a subset of training images into about an order
of magnitude fewer image centroids, with no loss of accuracy of classification
of test-set images by direct (nearest-neighbor) classification. Distinct
batches of centroids can be used in combination as a means of sampling
configuration space, and can classify test-set data more accurately than can
the unaltered training set. The approach works most naturally with multiple
processors in parallel.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitelam_S/0/1/0/all/0/1&quot;&gt;Stephen Whitelam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.00168">
<title>Learning Optimized Risk Scores. (arXiv:1610.00168v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.00168</link>
<description rdf:parseType="Literal">&lt;p&gt;Risk scores are simple classification models that let users make quick risk
predictions by adding and subtracting a few small numbers. These models are
widely used in medicine and criminal justice, but are difficult to learn from
data because they need to be calibrated, sparse, use small integer coefficients
and obey application-specific constraints. In this paper, we present a new
machine learning approach to learn risk scores. We formulate the risk score
problem as a mixed integer nonlinear program, and present a new cutting plane
algorithm for non-convex settings to efficiently recover its optimal solution.
We improve our algorithm with specialized techniques to generate feasible
solutions, narrow the optimality gap, and reduce data-related computation. Our
approach can fit risk scores in a way that scales linearly in the number of
samples, provides a certificate of optimality, and obeys real-world constraints
without parameter tuning or post-processing. We illustrate the performance
benefits of this approach through an extensive set of numerical experiments,
where we compare risk scores built using our approach to those built using
heuristic approaches. We also discuss the practical benefits of our approach
through an application where we build a customized risk score for ICU seizure
prediction in collaboration with the Massachusetts General Hospital.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ustun_B/0/1/0/all/0/1&quot;&gt;Berk Ustun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06664">
<title>Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer. (arXiv:1711.06664v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06664</link>
<description rdf:parseType="Literal">&lt;p&gt;In many machine learning applications, there are multiple decision-makers
involved, both automated and human. The interaction between these agents often
goes unaddressed in algorithmic development. In this work, we explore a simple
version of this interaction with a two-stage framework containing an automated
model and an external decision-maker. The model can choose to say &quot;Pass&quot;, and
pass the decision downstream, as explored in rejection learning. We extend this
concept by proposing &quot;learning to defer&quot;, which generalizes rejection learning
by considering the effect of other agents in the decision-making process. We
propose a learning algorithm which accounts for potential biases held by
external decision-makers in a system. Experiments demonstrate that learning to
defer can make systems not only more accurate but also less biased. Even when
working with inconsistent or biased users, we show that deferring models still
greatly improve the accuracy and/or fairness of the entire system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Madras_D/0/1/0/all/0/1&quot;&gt;David Madras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pitassi_T/0/1/0/all/0/1&quot;&gt;Toniann Pitassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01648">
<title>Sharp Convergence Rates for Langevin Dynamics in the Nonconvex Setting. (arXiv:1805.01648v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.01648</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of sampling from a distribution where the negative
logarithm of the target density is $L$-smooth everywhere and $m$-strongly
convex outside a ball of radius $R$, but potentially non-convex inside this
ball. We study both overdamped and underdamped Langevin MCMC and prove upper
bounds on the time required to obtain a sample from a distribution that is
within $\epsilon$ of the target distribution in $1$-Wasserstein distance. For
the first-order method (overdamped Langevin MCMC), the time complexity is
$\tilde{\mathcal{O}}\left(e^{cLR^2}\frac{d}{\epsilon^2}\right)$, where $d$ is
the dimension of the underlying space. For the second-order method (underdamped
Langevin MCMC), the time complexity is
$\tilde{\mathcal{O}}\left(e^{cLR^2}\frac{\sqrt{d}}{\epsilon}\right)$ for some
explicit positive constant $c$. Surprisingly, the convergence rate is only
polynomial in the dimension $d$ and the target accuracy $\epsilon$. It is
however exponential in the problem parameter $LR^2$, which is a measure of
non-logconcavity of the target distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1&quot;&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abbasi_Yadkori_Y/0/1/0/all/0/1&quot;&gt;Yasin Abbasi-Yadkori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1&quot;&gt;Peter L. Bartlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07474">
<title>Projection-Free Bandit Convex Optimization. (arXiv:1805.07474v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07474</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose the first computationally efficient projection-free
algorithm for bandit convex optimization (BCO). We show that our algorithm
achieves a sublinear regret of $O(nT^{4/5})$ (where $T$ is the horizon and $n$
is the dimension) for any bounded convex functions with uniformly bounded
gradients. We also evaluate the performance of our algorithm against baselines
on both synthetic and real data sets for quadratic programming, portfolio
selection and matrix completion problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karbasi_A/0/1/0/all/0/1&quot;&gt;Amin Karbasi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04731">
<title>Deep learning to represent sub-grid processes in climate models. (arXiv:1806.04731v3 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04731</link>
<description rdf:parseType="Literal">&lt;p&gt;The representation of nonlinear sub-grid processes, especially clouds, has
been a major source of uncertainty in climate models for decades.
Cloud-resolving models better represent many of these processes and can now be
run globally but only for short-term simulations of at most a few years because
of computational limitations. Here we demonstrate that deep learning can be
used to capture many advantages of cloud-resolving modeling at a fraction of
the computational cost. We train a deep neural network to represent all
atmospheric sub-grid processes in a climate model by learning from a
multi-scale model in which convection is treated explicitly. The trained neural
network then replaces the traditional sub-grid parameterizations in a global
general circulation model in which it freely interacts with the resolved
dynamics and the surface-flux scheme. The prognostic multi-year simulations are
stable and closely reproduce not only the mean climate of the cloud-resolving
simulation but also key aspects of variability, including precipitation
extremes and the equatorial wave spectrum. Furthermore, the neural network
approximately conserves energy despite not being explicitly instructed to.
Finally, we show that the neural network parameterization generalizes to new
surface forcing patterns but struggles to cope with temperatures far outside
its training manifold. Our results show the feasibility of using deep learning
for climate model parameterization. In a broader context, we anticipate that
data-driven Earth System Model development could play a key role in reducing
climate prediction uncertainty in the coming decade.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rasp_S/0/1/0/all/0/1&quot;&gt;Stephan Rasp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pritchard_M/0/1/0/all/0/1&quot;&gt;Michael S. Pritchard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gentine_P/0/1/0/all/0/1&quot;&gt;Pierre Gentine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04256">
<title>CT Super-resolution GAN Constrained by the Identical, Residual, and Cycle Learning Ensemble(GAN-CIRCLE). (arXiv:1808.04256v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/1808.04256</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed tomography (CT) is widely used in screening, diagnosis, and
image-guided therapy for both clinical and research purposes. Since CT involves
ionizing radiation, an overarching thrust of related technical research is
development of novel methods enabling ultrahigh quality imaging with fine
structural details while reducing the X-ray radiation. In this paper, we
present a semi-supervised deep learning approach to accurately recover
high-resolution (HR) CT images from low-resolution (LR) counterparts.
Specifically, with the generative adversarial network (GAN) as the building
block, we enforce the cycle-consistency in terms of the Wasserstein distance to
establish a nonlinear end-to-end mapping from noisy LR input images to denoised
and deblurred HR outputs. We also include the joint constraints in the loss
function to facilitate structural preservation. In this deep imaging process,
we incorporate deep convolutional neural network (CNN), residual learning, and
network in network techniques for feature extraction and restoration. In
contrast to the current trend of increasing network depth and complexity to
boost the CT imaging performance, which limit its real-world applications by
imposing considerable computational and memory overheads, we apply a parallel
$1\times1$ CNN to compress the output of the hidden layer and optimize the
number of layers and the number of filters for each convolutional layer.
Quantitative and qualitative evaluations demonstrate that our proposed model is
accurate, efficient and robust for super-resolution (SR) image restoration from
noisy LR input images. In particular, we validate our composite SR networks on
three large-scale CT datasets, and obtain promising results as compared to the
other state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenyu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoliu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shan_H/0/1/0/all/0/1&quot;&gt;Hongming Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ju_S/0/1/0/all/0/1&quot;&gt;Shenghong Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuiyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cong_W/0/1/0/all/0/1&quot;&gt;Wenxiang Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vannier_M/0/1/0/all/0/1&quot;&gt;Michael W. Vannier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saha_P/0/1/0/all/0/1&quot;&gt;Punam K. Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Ge Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07380">
<title>On the Predictability of non-CGM Diabetes Data for Personalized Recommendation. (arXiv:1808.07380v4 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07380</link>
<description rdf:parseType="Literal">&lt;p&gt;With continuous glucose monitoring (CGM), data-driven models on blood glucose
prediction have been shown to be effective in related work. However, such (CGM)
systems are not always available, e.g., for a patient at home. In this work, we
conduct a study on 9 patients and examine the predictability of data-driven
(aka. machine learning) based models on patient-level blood glucose prediction;
with measurements are taken only periodically (i.e., after several hours). To
this end, we propose several post-prediction methods to account for the noise
nature of these data, that marginally improves the performance of the end
system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tu Ngoc Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rokicki_M/0/1/0/all/0/1&quot;&gt;Markus Rokicki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07801">
<title>On a &apos;Two Truths&apos; Phenomenon in Spectral Graph Clustering. (arXiv:1808.07801v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07801</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering is concerned with coherently grouping observations without any
explicit concept of true groupings. Spectral graph clustering - clustering the
vertices of a graph based on their spectral embedding - is commonly approached
via K-means (or, more generally, Gaussian mixture model) clustering composed
with either Laplacian or Adjacency spectral embedding (LSE or ASE). Recent
theoretical results provide new understanding of the problem and solutions, and
lead us to a &apos;Two Truths&apos; LSE vs. ASE spectral graph clustering phenomenon
convincingly illustrated here via a diffusion MRI connectome data set: the
different embedding methods yield different clustering results, with LSE
capturing left hemisphere/right hemisphere affinity structure and ASE capturing
gray matter/white matter core-periphery structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Youngser Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vogelstein_J/0/1/0/all/0/1&quot;&gt;Joshua T. Vogelstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Conroy_J/0/1/0/all/0/1&quot;&gt;John M. Conroy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lyzinski_V/0/1/0/all/0/1&quot;&gt;Vince Lyzinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tang_M/0/1/0/all/0/1&quot;&gt;Minh Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Athreya_A/0/1/0/all/0/1&quot;&gt;Avanti Athreya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cape_J/0/1/0/all/0/1&quot;&gt;Joshua Cape&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bridgeford_E/0/1/0/all/0/1&quot;&gt;Eric Bridgeford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.08619">
<title>Discriminative but Not Discriminatory: A Comparison of Fairness Definitions under Different Worldviews. (arXiv:1808.08619v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.08619</link>
<description rdf:parseType="Literal">&lt;p&gt;We mathematically compare three competing definitions of group-level
nondiscrimination: demographic parity, equalized odds, and calibration. Using
the theoretical framework of Friedler et al., we study the properties of each
definition under various worldviews, which are assumptions about how, if at
all, the observed data is biased. We prove that different worldviews call for
different definitions of fairness, and we specify when it is appropriate to use
demographic parity and equalized odds. In addition, we argue that calibration
is unsuitable for the purpose of ensuring nondiscrimination. Finally, we define
a worldview that is more realistic than the previously considered ones, and we
introduce a new notion of fairness that is suitable for this worldview.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeom_S/0/1/0/all/0/1&quot;&gt;Samuel Yeom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschantz_M/0/1/0/all/0/1&quot;&gt;Michael Carl Tschantz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01225">
<title>Compositional Stochastic Average Gradient for Machine Learning and Related Applications. (arXiv:1809.01225v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.01225</link>
<description rdf:parseType="Literal">&lt;p&gt;Many machine learning, statistical inference, and portfolio optimization
problems require minimization of a composition of expected value functions
(CEVF). Of particular interest is the finite-sum versions of such compositional
optimization problems (FS-CEVF). Compositional stochastic variance reduced
gradient (C-SVRG) methods that combine stochastic compositional gradient
descent (SCGD) and stochastic variance reduced gradient descent (SVRG) methods
are the state-of-the-art methods for FS-CEVF problems. We introduce
compositional stochastic average gradient descent (C-SAG) a novel extension of
the stochastic average gradient method (SAG) to minimize composition of
finite-sum functions. C-SAG, like SAG, estimates gradient by incorporating
memory of previous gradient information. We present theoretical analyses of
C-SAG which show that C-SAG, like SAG, and C-SVRG, achieves a linear
convergence rate when the objective function is strongly convex; However, C-CAG
achieves lower oracle query complexity per iteration than C-SVRG. Finally, we
present results of experiments showing that C-SAG converges substantially
faster than full gradient (FG), as well as C-SVRG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_T/0/1/0/all/0/1&quot;&gt;Tsung-Yu Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+EL_Manzalawy_Y/0/1/0/all/0/1&quot;&gt;Yasser EL-Manzalawy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yiwei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honavar_V/0/1/0/all/0/1&quot;&gt;Vasant Honavar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01818">
<title>Improving Explorability in Variational Inference with Annealed Variational Objectives. (arXiv:1809.01818v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.01818</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the advances in the representational capacity of approximate
distributions for variational inference, the optimization process can still
limit the density that is ultimately learned. We demonstrate the drawbacks of
biasing the true posterior to be unimodal, and introduce Annealed Variational
Objectives (AVO) into the training of hierarchical variational methods.
Inspired by Annealed Importance Sampling, the proposed method facilitates
learning by incorporating energy tempering into the optimization objective. In
our experiments, we demonstrate our method&apos;s robustness to deterministic warm
up, and the benefits of encouraging exploration in the latent space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chin-Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Shawn Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacoste_A/0/1/0/all/0/1&quot;&gt;Alexandre Lacoste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01921">
<title>RDPD: Rich Data Helps Poor Data via Imitation. (arXiv:1809.01921v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.01921</link>
<description rdf:parseType="Literal">&lt;p&gt;In many situations, we have both rich- and poor- data environments: in a
rich-data environment (e.g., intensive care units), we have high-quality
multi-modality data. On the other hand, in a poor-data environment (e.g., at
home), we often only have access to a single data modality with low quality.
How can we learn an accurate and efficient model for the poor-data environment
by leveraging multi-modality data from the rich-data environment? In this work,
we propose a knowledge distillation model RDPD to enhance a small model trained
on poor data with a complex model trained on rich data. In an end-to-end
fashion, RDPD trains a student model built on a single modality data (poor
data) to imitate the behavior and performance of a teacher model from
multimodal data (rich data) via jointly optimizing the combined loss of
attention imitation and target imitation. We evaluated RDPD on three real-world
datasets. RDPD consistently outperformed all baselines across all three
datasets, especially achieving the greatest performance improvement over a
standard neural network model trained on the common features (Direct model) by
24.56% on PR-AUC and 12.21% on ROC-AUC, and over the standard knowledge
distillation model by 5.91% on PR-AUC and 4.44% on ROC-AUC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Shenda Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Cao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengfei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;</dc:creator>
</item></rdf:RDF>