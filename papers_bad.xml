<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-25T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08804"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08984"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09351"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09432"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.05970"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06487"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04749"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08925"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08941"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09029"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09328"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09444"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09453"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09487"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09506"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09514"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09597"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.05632"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.00724"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04434"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08773"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02336"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08829"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08835"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08836"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08838"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08887"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08911"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08946"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09039"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09060"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09077"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09178"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09228"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09277"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09300"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09390"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09429"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09460"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09544"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09548"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09550"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09571"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09588"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09602"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.04312"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.00049"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.01179"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04212"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08160"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08392"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04912"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04537"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00183"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01302"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01440"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05554"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00636"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07687"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10833"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12244"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12472"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04090"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.08804">
<title>Hierarchical Graph Representation Learning withDifferentiable Pooling. (arXiv:1806.08804v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.08804</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, graph neural networks (GNNs) have revolutionized the field of graph
representation learning through effectively learned node embeddings, and
achieved state-of-the-art results in tasks such as node classification and link
prediction. However, current GNN methods are inherently flat and do not learn
hierarchical representations of graphs---a limitation that is especially
problematic for the task of graph classification, where the goal is to predict
the label associated with an entire graph. Here we propose DiffPool, a
differentiable graph pooling module that can generate hierarchical
representations of graphs and can be combined with various graph neural network
architectures in an end-to-end fashion. DiffPool learns a differentiable soft
cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a
set of clusters, which then form the coarsened input for the next GNN layer.
Our experimental results show that combining existing GNN methods with DiffPool
yields an average improvement of 5-10% accuracy on graph classification
benchmarks, compared to all existing pooling approaches, achieving a new
state-of-the-art on four out of five benchmark data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1&quot;&gt;Rex Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1&quot;&gt;Jiaxuan You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morris_C/0/1/0/all/0/1&quot;&gt;Christopher Morris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1&quot;&gt;William L. Hamilton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08984">
<title>An Improved Generic Bet-and-Run Strategy for Speeding Up Stochastic Local Search. (arXiv:1806.08984v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.08984</link>
<description rdf:parseType="Literal">&lt;p&gt;A commonly used strategy for improving optimization algorithms is to restart
the algorithm when it is believed to be trapped in an inferior part of the
search space. Building on the recent success of Bet-and-Run approaches for
restarted local search solvers, we introduce an improved generic Bet-and-Run
strategy. The goal is to obtain the best possible results within a given time
budget t using a given black-box optimization algorithm. If no prior knowledge
about problem features and algorithm behavior is available, the question about
how to use the time budget most efficiently arises. We propose to first start
k&amp;gt;=1 independent runs of the algorithm during an initialization budget t1&amp;lt;t,
pausing these runs, then apply a decision maker D to choose 1&amp;lt;=m&amp;lt;=k runs from
them (consuming t2&amp;gt;=0 time units in doing so), and then continuing these runs
for the remaining t3=t-t1-t2 time units. In previous Bet-and-Run strategies,
the decision maker D=currentBest would simply select the run with the best-
so-far results at negligible time. We propose using more advanced methods to
discriminate between &quot;good&quot; and &quot;bad&quot; sample runs, with the goal of increasing
the correlation of the chosen run with the a-posteriori best one. We test
several different approaches, including neural networks trained or polynomials
fitted on the current trace of the algorithm to predict which run may yield the
best results if granted the remaining budget. We show with extensive
experiments that this approach can yield better results than the previous
methods, but also find that the currentBest method is a very reliable and
robust baseline approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weise_T/0/1/0/all/0/1&quot;&gt;Thomas Weise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zijun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1&quot;&gt;Markus Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09351">
<title>Multi-objective Model-based Policy Search for Data-efficient Learning with Sparse Rewards. (arXiv:1806.09351v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09351</link>
<description rdf:parseType="Literal">&lt;p&gt;The most data-efficient algorithms for reinforcement learning in robotics are
model-based policy search algorithms, which alternate between learning a
dynamical model of the robot and optimizing a policy to maximize the expected
return given the model and its uncertainties. However, the current algorithms
lack an effective exploration strategy to deal with sparse or misleading reward
scenarios: if they do not experience any state with a positive reward during
the initial random exploration, it is very unlikely to solve the problem. Here,
we propose a novel model-based policy search algorithm, Multi-DEX, that
leverages a learned dynamical model to efficiently explore the task space and
solve tasks with sparse rewards in a few episodes. To achieve this, we frame
the policy search problem as a multi-objective, model-based policy optimization
problem with three objectives: (1) generate maximally novel state trajectories,
(2) maximize the expected return and (3) keep the system in state-space regions
for which the model is as accurate as possible. We then optimize these
objectives using a Pareto-based multi-objective optimization algorithm. The
experiments show that Multi-DEX is able to solve sparse reward scenarios (with
a simulated robotic arm) in much lower interaction time than VIME, TRPO,
GEP-PG, CMA-ES and Black-DROPS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaushik_R/0/1/0/all/0/1&quot;&gt;Rituraj Kaushik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatzilygeroudis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Chatzilygeroudis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Mouret&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09432">
<title>Predicting Effective Control Parameters for Differential Evolution using Cluster Analysis of Objective Function Features. (arXiv:1806.09432v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.09432</link>
<description rdf:parseType="Literal">&lt;p&gt;A methodology is introduced which uses three simple objective function
features to predict effective control parameters for differential evolution.
This is achieved using cluster analysis techniques to classify objective
functions using these features. Information on prior performance of various
control parameters for each classification is then used to determine which
control parameters to use in future optimisations. Our approach is compared to
an state-of-the-art adaptive technique along with non-adaptive techniques. Two
accepted bench mark suites are used to compare performance, in all cases we
show that the improvement resulting from our approach is statistically
significant. The majority of the computational effort of this methodology is
performed off-line, however even taking into account the additional on-line
cost our approach outperforms other adaptive techniques. We also study the key
tuning parameters of our methodology, which further support the finding that
the simple features selected are predictors of effective control parameters.
The findings presented in this paper are significant because they show that
simple to calculate features of objective functions can help select control
parameters for optimisation algorithms. This can have an immediate positive
impact the application of these optimisation algorithms on real world problems
where it is often difficult to select effective control parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1&quot;&gt;Sean P. Walton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1&quot;&gt;M. Rowan Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.05970">
<title>Generic Black-Box End-to-End Attack Against State of the Art API Call Based Malware Classifiers. (arXiv:1707.05970v5 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1707.05970</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a black-box attack against API call based machine
learning malware classifiers, focusing on generating adversarial sequences
combining API calls and static features (e.g., printable strings) that will be
misclassified by the classifier without affecting the malware functionality. We
show that this attack is effective against many classifiers due to the
transferability principle between RNN variants, feed forward DNNs, and
traditional machine learning classifiers such as SVM. We also implement GADGET,
a software framework to convert any malware binary to a binary undetected by
malware classifiers, using the proposed attack, without access to the malware
source code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenberg_I/0/1/0/all/0/1&quot;&gt;Ishai Rosenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1&quot;&gt;Asaf Shabtai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rokach_L/0/1/0/all/0/1&quot;&gt;Lior Rokach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1&quot;&gt;Yuval Elovici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06487">
<title>Classification and Geometry of General Perceptual Manifolds. (arXiv:1710.06487v3 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06487</link>
<description rdf:parseType="Literal">&lt;p&gt;Perceptual manifolds arise when a neural population responds to an ensemble
of sensory signals associated with different physical features (e.g.,
orientation, pose, scale, location, and intensity) of the same perceptual
object. Object recognition and discrimination requires classifying the
manifolds in a manner that is insensitive to variability within a manifold. How
neuronal systems give rise to invariant object classification and recognition
is a fundamental problem in brain theory as well as in machine learning. Here
we study the ability of a readout network to classify objects from their
perceptual manifold representations. We develop a statistical mechanical theory
for the linear classification of manifolds with arbitrary geometry revealing a
remarkable relation to the mathematics of conic decomposition. Novel
geometrical measures of manifold radius and manifold dimension are introduced
which can explain the classification capacity for manifolds of various
geometries. The general theory is demonstrated on a number of representative
manifolds, including L2 ellipsoids prototypical of strictly convex manifolds,
L1 balls representing polytopes consisting of finite sample points, and
orientation manifolds which arise from neurons tuned to respond to a continuous
angle variable, such as object orientation. The effects of label sparsity on
the classification capacity of manifolds are elucidated, revealing a scaling
relation between label sparsity and manifold radius. Theoretical predictions
are corroborated by numerical simulations using recently developed algorithms
to compute maximum margin solutions for manifold dichotomies. Our theory and
its extensions provide a powerful and rich framework for applying statistical
mechanics of linear classification to data arising from neuronal responses to
object stimuli, as well as to artificial deep networks trained for object
recognition tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Chung_S/0/1/0/all/0/1&quot;&gt;SueYeon Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Daniel D. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Sompolinsky_H/0/1/0/all/0/1&quot;&gt;Haim Sompolinsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04749">
<title>Per-Corpus Configuration of Topic Modelling for GitHub and Stack Overflow Collections. (arXiv:1804.04749v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04749</link>
<description rdf:parseType="Literal">&lt;p&gt;To make sense of large amounts of textual data, topic modelling is frequently
used as a text-mining tool for the discovery of hidden semantic structures in
text bodies. Latent Dirichlet allocation (LDA) is a commonly used topic model
that aims to explain the structure of a corpus by grouping texts. LDA requires
multiple parameters to work well, and there are only rough and sometimes
conflicting guidelines available on how these parameters should be set. In this
paper, we contribute (i) a broad study of parameters to arrive at good local
optima, (ii) an a-posteriori characterisation of text corpora related to eight
programming languages from GitHub and Stack Overflow, and (iii) an analysis of
corpus feature importance via per-corpus LDA configuration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Treude_C/0/1/0/all/0/1&quot;&gt;Christoph Treude&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1&quot;&gt;Markus Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08925">
<title>An Inductive Formalization of Self Reproduction in Dynamical Hierarchies. (arXiv:1806.08925v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.08925</link>
<description rdf:parseType="Literal">&lt;p&gt;Formalizing self reproduction in dynamical hierarchies is one of the
important problems in Artificial Life (AL) studies. We study, in this paper, an
inductively defined algebraic framework for self reproduction on macroscopic
organizational levels under dynamical system setting for simulated AL models
and explore some existential results. Starting with defining self reproduction
for atomic entities we define self reproduction with possible mutations on
higher organizational levels in terms of hierarchical sets and the
corresponding inductively defined `meta&apos; - reactions. We introduce constraints
to distinguish a collection of entities from genuine cases of emergent
organizational structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_J/0/1/0/all/0/1&quot;&gt;Janardan Misra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08941">
<title>A Recursive PLS (Partial Least Squares) based Approach for Enterprise Threat Management. (arXiv:1806.08941v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1806.08941</link>
<description rdf:parseType="Literal">&lt;p&gt;Most of the existing solutions to enterprise threat management are preventive
approaches prescribing means to prevent policy violations with varying degrees
of success. In this paper we consider the complementary scenario where a number
of security violations have already occurred, or security threats, or
vulnerabilities have been reported and a security administrator needs to
generate optimal response to these security events. We present a principled
approach to study and model the human expertise in responding to the emergent
threats owing to these security events. A recursive Partial Least Squares based
adaptive learning model is defined using a factorial analysis of the security
events together with a method for estimating the effect of global context
dependent semantic information used by the security administrators. Presented
model is theoretically optimal and operationally recursive in nature to deal
with the set of security events being generated continuously. We discuss the
underlying challenges and ways in which the model could be operationalized in
centralized versus decentralized, and real-time versus batch processing modes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_J/0/1/0/all/0/1&quot;&gt;Janardan Misra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09029">
<title>Improving Text-to-SQL Evaluation Methodology. (arXiv:1806.09029v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1806.09029</link>
<description rdf:parseType="Literal">&lt;p&gt;To be informative, an evaluation must measure how well systems generalize to
realistic unseen data. We identify limitations of and propose improvements to
current evaluations of text-to-SQL systems. First, we compare human-generated
and automatically generated questions, characterizing properties of queries
necessary for real-world applications. To facilitate evaluation on multiple
datasets, we release standardized and improved versions of seven existing
datasets and one new text-to-SQL dataset. Second, we show that the current
division of data into training and test sets measures robustness to variations
in the way questions are asked, but only partially tests how well systems
generalize to new queries; therefore, we propose a complementary dataset split
for evaluation of future work. Finally, we demonstrate how the common practice
of anonymizing variables during evaluation removes an important challenge of
the task. Our observations highlight key difficulties, and our methodology
enables effective measurement of future development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finegan_Dollak_C/0/1/0/all/0/1&quot;&gt;Catherine Finegan-Dollak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1&quot;&gt;Jonathan K. Kummerfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanathan_K/0/1/0/all/0/1&quot;&gt;Karthik Ramanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadasivam_S/0/1/0/all/0/1&quot;&gt;Sesh Sadasivam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1&quot;&gt;Dragomir Radev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09328">
<title>Diversified Late Acceptance Search. (arXiv:1806.09328v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.09328</link>
<description rdf:parseType="Literal">&lt;p&gt;The well-known Late Acceptance Hill Climbing (LAHC) search aims to overcome
the main downside of traditional Hill Climbing (HC) search, which is often
quickly trapped in a local optimum due to strictly accepting only non-worsening
moves within each iteration. In contrast, LAHC also accepts worsening moves, by
keeping a circular array of fitness values of previously visited solutions and
comparing the fitness values of candidate solutions against the least recent
element in the array. While the straightforward strategy followed by LAHC has
proven effective, there are nevertheless situations where LAHC can
unfortunately behave in a similar manner to HC, even when using a large fitness
array. For example, when the same fitness value is stored many times in the
array, particularly when a new local optimum is found. To address this
shortcoming, we propose to improve both the diversity of the accepted solutions
and the diversity of values in the array through new acceptance and replacement
strategies. The proposed Diversified Late Acceptance Search approach is shown
to outperform the current state-of-the-art LAHC method on benchmark sets of
Travelling Salesman Problem and Quadratic Assignment Problem instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namazi_M/0/1/0/all/0/1&quot;&gt;Majid Namazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanderson_C/0/1/0/all/0/1&quot;&gt;Conrad Sanderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newton_M/0/1/0/all/0/1&quot;&gt;M.A. Hakim Newton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polash_M/0/1/0/all/0/1&quot;&gt;M.M.A. Polash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sattar_A/0/1/0/all/0/1&quot;&gt;Abdul Sattar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09444">
<title>A Transferable Pedestrian Motion Prediction Model for Intersections with Different Geometries. (arXiv:1806.09444v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09444</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel framework for accurate pedestrian intent
prediction at intersections. Given some prior knowledge of the curbside
geometry, the presented framework can accurately predict pedestrian
trajectories, even in new intersections that it has not been trained on. This
is achieved by making use of the contravariant components of trajectories in
the curbside coordinate system, which ensures that the transformation of
trajectories across intersections is affine, regardless of the curbside
geometry. Our method is based on the Augmented Semi Nonnegative Sparse Coding
(ASNSC) formulation and we use that as a baseline to show improvement in
prediction performance on real pedestrian datasets collected at two
intersections in Cambridge, with distinctly different curbside and crosswalk
geometries. We demonstrate a 7.2% improvement in prediction accuracy in the
case of same train and test intersections. Furthermore, we show a comparable
prediction performance of TASNSC when trained and tested in different
intersections with the baseline, trained and tested on the same intersection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaipuria_N/0/1/0/all/0/1&quot;&gt;Nikita Jaipuria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habibi_G/0/1/0/all/0/1&quot;&gt;Golnaz Habibi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1&quot;&gt;Jonathan P. How&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09453">
<title>Context-Aware Pedestrian Motion Prediction In Urban Intersections. (arXiv:1806.09453v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09453</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel context-based approach for pedestrian motion
prediction in crowded, urban intersections, with the additional flexibility of
prediction in similar, but new, environments. Previously, Chen et. al. combined
Markovian-based and clustering-based approaches to learn motion primitives in a
grid-based world and subsequently predict pedestrian trajectories by modeling
the transition between learned primitives as a Gaussian Process (GP). This work
extends that prior approach by incorporating semantic features from the
environment (relative distance to curbside and status of pedestrian traffic
lights) in the GP formulation for more accurate predictions of pedestrian
trajectories over the same timescale. We evaluate the new approach on
real-world data collected using one of the vehicles in the MIT Mobility On
Demand fleet. The results show 12.5% improvement in prediction accuracy and a
2.65 times reduction in Area Under the Curve (AUC), which is used as a metric
to quantify the span of predicted set of trajectories, such that a lower AUC
corresponds to a higher level of confidence in the future direction of
pedestrian motion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habibi_G/0/1/0/all/0/1&quot;&gt;Golnaz Habibi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaipuria_N/0/1/0/all/0/1&quot;&gt;Nikita Jaipuria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1&quot;&gt;Jonathan P. How&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09487">
<title>Finding Optimal Solutions to Token Swapping by Conflict-based Search and Reduction to SAT. (arXiv:1806.09487v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.09487</link>
<description rdf:parseType="Literal">&lt;p&gt;We study practical approaches to solving the token swapping (TSWAP) problem
optimally in this short paper. In TSWAP, we are given an undirected graph with
colored vertices. A colored token is placed in each vertex. A pair of tokens
can be swapped between adjacent vertices. The goal is to perform a sequence of
swaps so that token and vertex colors agree across the graph. The minimum
number of swaps is required in the optimization variant of the problem. We
observed similarities between the TSWAP problem and multi-agent path finding
(MAPF) where instead of tokens we have multiple agents that need to be moved
from their current vertices to given unique target vertices. The difference
between both problems consists in local conditions that state transitions
(swaps/moves) must satisfy. We developed two algorithms for solving TSWAP
optimally by adapting two different approaches to MAPF - CBS and MDD- SAT. This
constitutes the first attempt to design optimal solving algorithms for TSWAP.
Experimental evaluation on various types of graphs shows that the reduction to
SAT scales better than CBS in optimal TSWAP solving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surynek_P/0/1/0/all/0/1&quot;&gt;Pavel Surynek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09506">
<title>Optimal Seeding and Self-Reproduction from a Mathematical Point of View. (arXiv:1806.09506v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.09506</link>
<description rdf:parseType="Literal">&lt;p&gt;P. Kabamba developed generation theory as a tool for studying
self-reproducing systems. We provide an alternative definition of a generation
system and give a complete solution to the problem of finding optimal seeds for
a finite self-replicating system. We also exhibit examples illustrating a
connection between self-replication and fixed-point theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gitik_R/0/1/0/all/0/1&quot;&gt;Rita Gitik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09514">
<title>The Emotional Voices Database: Towards Controlling the Emotion Dimension in Voice Generation Systems. (arXiv:1806.09514v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1806.09514</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a database of emotional speech intended to be
open-sourced and used for synthesis and generation purpose. It contains data
for male and female actors in English and a male actor in French. The database
covers 5 emotion classes so it could be suitable to build synthesis and voice
transformation systems with the potential to control the emotional dimension in
a continuous way. We show the data&apos;s efficiency by building a simple MLP system
converting neutral to angry speech style and evaluate it via a CMOS perception
test. Even though the system is a very simple one, the test show the efficiency
of the data which is promising for future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adigwe_A/0/1/0/all/0/1&quot;&gt;Adaeze Adigwe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tits_N/0/1/0/all/0/1&quot;&gt;No&amp;#xe9; Tits&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haddad_K/0/1/0/all/0/1&quot;&gt;Kevin El Haddad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1&quot;&gt;Sarah Ostadabbas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutoit_T/0/1/0/all/0/1&quot;&gt;Thierry Dutoit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09597">
<title>Stochastic natural gradient descent draws posterior samples in function space. (arXiv:1806.09597v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09597</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural gradient descent (NGD) minimises the cost function on a Riemannian
manifold whose metric is defined by the Fisher information. In this work, we
prove that if the model predictions on the training set approach the true
conditional distribution of labels given inputs, then the noise inherent in
minibatch gradients causes the stationary distribution of NGD to approach a
Bayesian posterior, whose temperature $T \approx \epsilon N/(2B)$ is controlled
by the learning rate $\epsilon$, training set size $N$ and batch size $B$. The
parameter-dependence of the Fisher metric introduces an implicit prior over the
parameters, which we identify as the well-known Jeffreys prior. To support our
claims, we show that the distribution of samples from NGD is close to the
Laplace approximation to the posterior when $T = 1$. Furthermore, the test loss
of ensembles drawn using NGD falls rapidly as we increase the batch size until
$B \approx \epsilon N/2$, while above this point the test loss is constant or
rises slowly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1&quot;&gt;Samuel L. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duckworth_D/0/1/0/all/0/1&quot;&gt;Daniel Duckworth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc V. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1&quot;&gt;Jascha Sohl-Dickstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.05632">
<title>On the adoption of abductive reasoning for time series interpretation. (arXiv:1609.05632v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1609.05632</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series interpretation aims to provide an explanation of what is observed
in terms of its underlying processes. The present work is based on the
assumption that the common classification-based approaches to time series
interpretation suffer from a set of inherent weaknesses, whose ultimate cause
lies in the monotonic nature of the deductive reasoning paradigm. In this
document we propose a new approach to this problem, based on the initial
hypothesis that abductive reasoning properly accounts for the human ability to
identify and characterize the patterns appearing in a time series. The result
of this interpretation is a set of conjectures in the form of observations,
organized into an abstraction hierarchy and explaining what has been observed.
A knowledge-based framework and a set of algorithms for the interpretation task
are provided, implementing a hypothesize-and-test cycle guided by an
attentional mechanism. As a representative application domain, interpretation
of the electrocardiogram allows us to highlight the strengths of the proposed
approach in comparison with traditional classification-based approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teijeiro_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Teijeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felix_P/0/1/0/all/0/1&quot;&gt;Paulo F&amp;#xe9;lix&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.00724">
<title>Efficient Probabilistic Performance Bounds for Inverse Reinforcement Learning. (arXiv:1707.00724v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1707.00724</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of reinforcement learning there has been recent progress towards
safety and high-confidence bounds on policy performance. However, to our
knowledge, no practical methods exist for determining high-confidence policy
performance bounds in the inverse reinforcement learning setting---where the
true reward function is unknown and only samples of expert behavior are given.
We propose a sampling method based on Bayesian inverse reinforcement learning
that uses demonstrations to determine practical high-confidence upper bounds on
the $\alpha$-worst-case difference in expected return between any evaluation
policy and the optimal policy under the expert&apos;s unknown reward function. We
evaluate our proposed bound on both a standard grid navigation task and a
simulated driving task and achieve tighter and more accurate bounds than a
feature count-based baseline. We also give examples of how our proposed bound
can be utilized to perform risk-aware policy selection and risk-aware policy
improvement. Because our proposed bound requires several orders of magnitude
fewer demonstrations than existing high-confidence bounds, it is the first
practical method that allows agents that learn from demonstration to express
confidence in the quality of their learned policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1&quot;&gt;Daniel S. Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1&quot;&gt;Scott Niekum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04434">
<title>signSGD: Compressed Optimisation for Non-Convex Problems. (arXiv:1802.04434v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04434</link>
<description rdf:parseType="Literal">&lt;p&gt;Training large neural networks requires distributing learning across multiple
workers, where the cost of communicating gradients can be a significant
bottleneck. signSGD alleviates this problem by transmitting just the sign of
each minibatch stochastic gradient. We prove that it can get the best of both
worlds: compressed gradients and SGD-level convergence rate. The relative
$\ell_1/\ell_2$ geometry of gradients, noise and curvature informs whether
signSGD or SGD is theoretically better suited to a particular problem. On the
practical side we find that the momentum counterpart of signSGD is able to
match the accuracy and convergence speed of Adam on deep Imagenet models. We
extend our theory to the distributed setting, where the parameter server uses
majority vote to aggregate gradient signs from each worker enabling 1-bit
compression of worker-server communication in both directions. Using a theorem
by Gauss we prove that majority vote can achieve the same reduction in variance
as full precision distributed SGD. Thus, there is great promise for sign-based
optimisation schemes to achieve fast communication and fast convergence. Code
to reproduce experiments is to be found at https://github.com/jxbz/signSGD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernstein_J/0/1/0/all/0/1&quot;&gt;Jeremy Bernstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1&quot;&gt;Kamyar Azizzadenesheli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08773">
<title>GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models. (arXiv:1802.08773v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08773</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling and generating graphs is fundamental for studying networks in
biology, engineering, and social sciences. However, modeling complex
distributions over graphs and then efficiently sampling from these
distributions is challenging due to the non-unique, high-dimensional nature of
graphs and the complex, non-local dependencies that exist between edges in a
given graph. Here we propose GraphRNN, a deep autoregressive model that
addresses the above challenges and approximates any distribution of graphs with
minimal assumptions about their structure. GraphRNN learns to generate graphs
by training on a representative set of graphs and decomposes the graph
generation process into a sequence of node and edge formations, conditioned on
the graph structure generated so far.
&lt;/p&gt;
&lt;p&gt;In order to quantitatively evaluate the performance of GraphRNN, we introduce
a benchmark suite of datasets, baselines and novel evaluation metrics based on
Maximum Mean Discrepancy, which measure distances between sets of graphs. Our
experiments show that GraphRNN significantly outperforms all baselines,
learning to generate diverse graphs that match the structural characteristics
of a target set, while also scaling to graphs 50 times larger than previous
deep models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1&quot;&gt;Jiaxuan You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1&quot;&gt;Rex Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1&quot;&gt;William L. Hamilton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02336">
<title>Spatial Frequency Loss for Learning Convolutional Autoencoders. (arXiv:1806.02336v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1806.02336</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a learning method for convolutional autoencoders (CAEs)
for extracting features from images. CAEs can be obtained by utilizing
convolutional neural networks to learn an approximation to the identity
function in an unsupervised manner. The loss function based on the pixel loss
(PL) that is the mean squared error between the pixel values of original and
reconstructed images is the common choice for learning. However, using the loss
function leads to blurred reconstructed images. A method for learning CAEs
using a loss function computed from features reflecting spatial frequencies is
proposed to mitigate the problem. The blurs in reconstructed images show lack
of high spatial frequency components mainly constituting edges and detailed
textures that are important features for tasks such as object detection and
spatial matching. In order to evaluate the lack of components, a convolutional
layer with a Laplacian filter bank as weights is added to CAEs and the mean
squared error of features in a subband, called the spatial frequency loss
(SFL), is computed from the outputs of each filter. The learning is performed
using a loss function based on the SFL. Empirical evaluation demonstrates that
using the SFL reduces the blurs in reconstructed images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_N/0/1/0/all/0/1&quot;&gt;Naoyuki Ichimura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08819">
<title>Forecasting Internally Displaced Population Migration Patterns in Syria and Yemen. (arXiv:1806.08819v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1806.08819</link>
<description rdf:parseType="Literal">&lt;p&gt;Armed conflict has led to an unprecedented number of internally displaced
persons (IDPs) - individuals who are forced out of their homes but remain
within their country. IDPs often urgently require shelter, food, and
healthcare, yet prediction of when large fluxes of IDPs will cross into an area
remains a major challenge for aid delivery organizations. Accurate forecasting
of IDP migration would empower humanitarian aid groups to more effectively
allocate resources during conflicts. We show that monthly flow of IDPs from
province to province in both Syria and Yemen can be accurately forecasted one
month in advance, using publicly available data. We model monthly IDP flow
using data on food price, fuel price, wage, geospatial, and news data. We find
that machine learning approaches can more accurately forecast migration trends
than baseline persistence models. Our findings thus potentially enable
proactive aid allocation for IDPs in anticipation of forecasted arrivals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huynh_B/0/1/0/all/0/1&quot;&gt;Benjamin Q. Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Basu_S/0/1/0/all/0/1&quot;&gt;Sanjay Basu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08829">
<title>Diffusion Scattering Transforms on Graphs. (arXiv:1806.08829v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.08829</link>
<description rdf:parseType="Literal">&lt;p&gt;Stability is a key aspect of data analysis. In many applications, the natural
notion of stability is geometric, as illustrated for example in computer
vision. Scattering transforms construct deep convolutional representations
which are certified stable to input deformations. This stability to
deformations can be interpreted as stability with respect to changes in the
metric structure of the domain. In this work, we show that scattering
transforms can be generalized to non-Euclidean domains using diffusion
wavelets, while preserving a notion of stability with respect to metric changes
in the domain, measured with diffusion maps. The resulting representation is
stable to metric perturbations of the domain while being able to capture
&quot;high-frequency&quot; information, akin to the Euclidean Scattering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1&quot;&gt;Fernando Gama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Alejandro Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1&quot;&gt;Joan Bruna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08834">
<title>Smart Inverter Grid Probing for Learning Loads: Part I - Identifiability Analysis. (arXiv:1806.08834v1 [math.OC])</title>
<link>http://arxiv.org/abs/1806.08834</link>
<description rdf:parseType="Literal">&lt;p&gt;Distribution grids currently lack comprehensive real-time metering.
Nevertheless, grid operators require precise knowledge of loads and renewable
generation to accomplish any feeder optimization task. At the same time, new
grid technologies, such as solar photovoltaics and energy storage units are
interfaced via inverters with advanced sensing and actuation capabilities. In
this context, this two-part work puts forth the idea of engaging power
electronics to probe an electric grid and record its voltage response at
actuated and metered buses, to infer non-metered loads. Probing can be
accomplished by commanding inverters to momentarily perturb their power
injections. Multiple probing actions can be induced within a few tens of
seconds. In Part I, load inference via grid probing is formulated as an
implicit nonlinear system identification task, which is shown to be
topologically observable under certain conditions. The conditions can be
readily checked upon solving a max-flow problem on a bipartite graph derived
from the feeder topology and the placement of actuated and non-metered buses.
The analysis holds for single- and multi-phase grids, radial or meshed, and
applies to phasor or magnitude-only voltage data. The topological observability
of distribution systems using smart meter or phasor data is cast and analyzed a
special case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bhela_S/0/1/0/all/0/1&quot;&gt;Siddharth Bhela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kekatos_V/0/1/0/all/0/1&quot;&gt;Vassilis Kekatos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Veeramachaneni_S/0/1/0/all/0/1&quot;&gt;Sriharsha Veeramachaneni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08835">
<title>Domain Adaptation for Infection Prediction from Symptoms Based on Data from Different Study Designs and Contexts. (arXiv:1806.08835v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.08835</link>
<description rdf:parseType="Literal">&lt;p&gt;Acute respiratory infections have epidemic and pandemic potential and thus
are being studied worldwide, albeit in many different contexts and study
formats. Predicting infection from symptom data is critical, though using
symptom data from varied studies in aggregate is challenging because the data
is collected in different ways. Accordingly, different symptom profiles could
be more predictive in certain studies, or even symptoms of the same name could
have different meanings in different contexts. We assess state-of-the-art
transfer learning methods for improving prediction of infection from symptom
data in multiple types of health care data ranging from clinical, to home-visit
as well as crowdsourced studies. We show interesting characteristics regarding
six different study types and their feature domains. Further, we demonstrate
that it is possible to use data collected from one study to predict infection
in another, at close to or better than using a single dataset for prediction on
itself. We also investigate in which conditions specific transfer learning and
domain adaptation methods may perform better on symptom data. This work has the
potential for broad applicability as we show how it is possible to transfer
learning from one public health study design to another, and data collected
from one study may be used for prediction of labels for another, even collected
through different study designs, populations and contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rehman_N/0/1/0/all/0/1&quot;&gt;Nabeel Abdur Rehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aliapoulios_M/0/1/0/all/0/1&quot;&gt;Maxwell Matthaios Aliapoulios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Umarwani_D/0/1/0/all/0/1&quot;&gt;Disha Umarwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chunara_R/0/1/0/all/0/1&quot;&gt;Rumi Chunara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08836">
<title>Smart Inverter Grid Probing for Learning Loads: Part II - Probing Injection Design. (arXiv:1806.08836v1 [math.OC])</title>
<link>http://arxiv.org/abs/1806.08836</link>
<description rdf:parseType="Literal">&lt;p&gt;This two-part work puts forth the idea of engaging power electronics to probe
an electric grid to infer non-metered loads. Probing can be accomplished by
commanding inverters to perturb their power injections and record the induced
voltage response. Once a probing setup is deemed topologically observable by
the tests of Part I, Part II provides a methodology for designing probing
injections abiding by inverter and network constraints to improve load
estimates. The task is challenging since system estimates depend on both
probing injections and unknown loads in an implicit nonlinear fashion. The
methodology first constructs a library of candidate probing vectors by sampling
over the feasible set of inverter injections. Leveraging a linearized grid
model and a robust approach, the candidate probing vectors violating voltage
constraints for any anticipated load value are subsequently rejected. Among the
qualified candidates, the design finally identifies the probing vectors
yielding the most diverse system states. The probing task under noisy phasor
and non-phasor data is tackled using a semidefinite-program (SDP) relaxation.
Numerical tests using synthetic and real-world data on a benchmark feeder
validate the conditions of Part I; the SDP-based solver; the importance of
probing design; and the effects of probing duration and noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bhela_S/0/1/0/all/0/1&quot;&gt;Siddharth Bhela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kekatos_V/0/1/0/all/0/1&quot;&gt;Vassilis Kekatos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Veeramachaneni_S/0/1/0/all/0/1&quot;&gt;Sriharsha Veeramachaneni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08838">
<title>Bayesian Optimization of Combinatorial Structures. (arXiv:1806.08838v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.08838</link>
<description rdf:parseType="Literal">&lt;p&gt;The optimization of expensive-to-evaluate black-box functions over
combinatorial structures is an ubiquitous task in machine learning, engineering
and the natural sciences. The combinatorial explosion of the search space and
costly evaluations pose challenges for current techniques in discrete
optimization and machine learning, and critically require new algorithmic ideas
(NIPS BayesOpt 2017). This article proposes, to the best of our knowledge, the
first algorithm to overcome these challenges, based on an adaptive, scalable
model that identifies useful combinatorial structure even when data is scarce.
Our acquisition function pioneers the use of semidefinite programming to
achieve efficiency and scalability. Experimental evaluations demonstrate that
this algorithm consistently outperforms other methods from combinatorial and
Bayesian optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baptista_R/0/1/0/all/0/1&quot;&gt;Ricardo Baptista&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poloczek_M/0/1/0/all/0/1&quot;&gt;Matthias Poloczek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08887">
<title>The Sparse Manifold Transform. (arXiv:1806.08887v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.08887</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a signal representation framework called the {\em sparse manifold
transform} that combines key ideas from sparse coding, manifold learning and
slow feature analysis. It turns non-linear transformations in the primary
sensory signal space into linear interpolations in a representational embedding
space while maintaining approximate invertibility. The sparse manifold
transform is an unsupervised and generative framework that explicitly and
simultaneously models the sparse discreteness and low-dimensional manifold
structure found in natural scenes. When stacked, it also models hierarchical
composition. We provide a theoretical description of the transform and
demonstrate properties of the learned representation on both synthetic data and
natural videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yubei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Paiton_D/0/1/0/all/0/1&quot;&gt;Dylan M. Paiton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Olshausen_B/0/1/0/all/0/1&quot;&gt;Bruno A. Olshausen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08911">
<title>Overlapping Sliced Inverse Regression for Dimension Reduction. (arXiv:1806.08911v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.08911</link>
<description rdf:parseType="Literal">&lt;p&gt;Sliced inverse regression (SIR) is a pioneer tool for supervised dimension
reduction. It identifies the effective dimension reduction space, the subspace
of significant factors with intrinsic lower dimensionality. In this paper, we
propose to refine the SIR algorithm through an overlapping slicing scheme. The
new algorithm, called overlapping sliced inverse regression (OSIR), is able to
estimate the effective dimension reduction space and determine the number of
effective factors more accurately. We show that such overlapping procedure has
the potential to identify the information contained in the derivatives of the
inverse regression curve, which helps to explain the superiority of OSIR. We
also prove that OSIR algorithm is $\sqrt n $-consistent and verify its
effectiveness by simulations and real applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhou Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiang Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08946">
<title>Multilevel Wavelet Decomposition Network for Interpretable Time Series Analysis. (arXiv:1806.08946v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.08946</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed the unprecedented rising of time series from
almost all kindes of academic and industrial fields. Various types of deep
neural network models have been introduced to time series analysis, but the
important frequency information is yet lack of effective modeling. In light of
this, in this paper we propose a wavelet-based neural network structure called
multilevel Wavelet Decomposition Network (mWDN) for building frequency-aware
deep learning models for time series analysis. mWDN preserves the advantage of
multilevel discrete wavelet decomposition in frequency learning while enables
the fine-tuning of all parameters under a deep neural network framework. Based
on mWDN, we further propose two deep learning models called Residual
Classification Flow (RCF) and multi-frequecy Long Short-Term Memory (mLSTM) for
time series classification and forecasting, respectively. The two models take
all or partial mWDN decomposed sub-series in different frequencies as input,
and resort to the back propagation algorithm to learn all the parameters
globally, which enables seamless embedding of wavelet-based frequency analysis
into deep learning frameworks. Extensive experiments on 40 UCR datasets and a
real-world user volume dataset demonstrate the excellent performance of our
time series models based on mWDN. In particular, we propose an importance
analysis method to mWDN based models, which successfully identifies those
time-series elements and mWDN layers that are crucially important to time
series analysis. This indeed indicates the interpretability advantage of mWDN,
and can be viewed as an indepth exploration to interpretable deep learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianfeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junjie Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09039">
<title>Parallel Transport Unfolding: A Connection-based Manifold Learning Approach. (arXiv:1806.09039v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09039</link>
<description rdf:parseType="Literal">&lt;p&gt;Manifold learning offers nonlinear dimensionality reduction of
high-dimensional datasets. In this paper, we bring geometry processing to bear
on manifold learning by introducing a new approach based on metric connection
for generating a quasi-isometric, low-dimensional mapping from a sparse and
irregular sampling of an arbitrary manifold embedded in a high-dimensional
space. Geodesic distances of discrete paths on the input pointset are evaluated
through &quot;parallel transport unfolding&quot; (PTU) to offer robustness to poor
sampling and arbitrary topology. Our new geometric procedure exhibits the same
strong resilience to noise as one of the staples of manifold learning, the
Isomap algorithm, as it also exploits all pairwise geodesic distances to
compute a low-dimensional embedding. While Isomap is limited to
geodesically-convex sampled domains, parallel transport unfolding does not
suffer from this crippling limitation, resulting in an improved robustness to
irregularity and voids in the sampling. Moreover, it involves only simple
linear algebra, significantly improves the accuracy of all pairwise geodesic
distance approximations, and has the same computational complexity as Isomap.
Finally, we show that our connection-based distance estimation can be used for
faster variants of Isomap such as L-Isomap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budninskiy_M/0/1/0/all/0/1&quot;&gt;Max Budninskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1&quot;&gt;Glorian Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Leman Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1&quot;&gt;Yiying Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desbrun_M/0/1/0/all/0/1&quot;&gt;Mathieu Desbrun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09048">
<title>A classification point-of-view about conditional Kendall&apos;s tau. (arXiv:1806.09048v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1806.09048</link>
<description rdf:parseType="Literal">&lt;p&gt;We show how the problem of estimating conditional Kendall&apos;s tau can be
rewritten as a classification task. Conditional Kendall&apos;s tau is a conditional
dependence parameter that is a characteristic of a given pair of random
variables. The goal is to predict whether the pair is concordant (value of $1$)
or discordant (value of $-1$) conditionally on some covariates. We prove the
consistency and the asymptotic normality of a family of penalized approximate
maximum likelihood estimators, including the equivalent of the logit and probit
regressions in our framework. Then, we detail specific algorithms adapting
usual machine learning techniques, including nearest neighbors, decision trees,
random forests and neural networks, to the setting of the estimation of
conditional Kendall&apos;s tau. A small simulation study compares their finite
sample properties. Finally, we apply all these estimators to a dataset of
European stock indices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Derumigny_A/0/1/0/all/0/1&quot;&gt;Alexis Derumigny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fermanian_J/0/1/0/all/0/1&quot;&gt;Jean-David Fermanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09060">
<title>Disentangled VAE Representations for Multi-Aspect and Missing Data. (arXiv:1806.09060v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09060</link>
<description rdf:parseType="Literal">&lt;p&gt;Many problems in machine learning and related application areas are
fundamentally variants of conditional modeling and sampling across multi-aspect
data, either multi-view, multi-modal, or simply multi-group. For example,
sampling from the distribution of English sentences conditioned on a given
French sentence or sampling audio waveforms conditioned on a given piece of
text. Central to many of these problems is the issue of missing data: we can
observe many English, French, or German sentences individually but only
occasionally do we have data for a sentence pair. Motivated by these
applications and inspired by recent progress in variational autoencoders for
grouped data, we develop factVAE, a deep generative model capable of handling
multi-aspect data, robust to missing observations, and with a prior that
encourages disentanglement between the groups and the latent dimensions. The
effectiveness of factVAE is demonstrated on a variety of rich real-world
datasets, including motion capture poses and pictures of faces captured from
varying poses and perspectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ainsworth_S/0/1/0/all/0/1&quot;&gt;Samuel K. Ainsworth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foti_N/0/1/0/all/0/1&quot;&gt;Nicholas J. Foti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1&quot;&gt;Emily B. Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09077">
<title>Beyond Backprop: Alternating Minimization with co-Activation Memory. (arXiv:1806.09077v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.09077</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel online algorithm for training deep feedforward neural
networks that employs alternating minimization (block-coordinate descent)
between the weights and activation variables. It extends off-line alternating
minimization approaches to online, continual learning, and improves over
stochastic gradient descent (SGD) with backpropagation in several ways: it
avoids the vanishing gradient issue, it allows for non-differentiable
nonlinearities, and it permits parallel weight updates across the layers.
Unlike SGD, our approach employs co-activation memory inspired by the online
sparse coding algorithm of [Mairal et al, 2009]. Furthermore, local iterative
optimization with explicit activation updates is a potentially more
biologically plausible learning mechanism than backpropagation. We provide
theoretical convergence analysis and promising empirical results on several
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Choromanska_A/0/1/0/all/0/1&quot;&gt;Anna Choromanska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kumaravel_S/0/1/0/all/0/1&quot;&gt;Sadhana Kumaravel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luss_R/0/1/0/all/0/1&quot;&gt;Ronny Luss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rish_I/0/1/0/all/0/1&quot;&gt;Irina Rish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kingsbury_B/0/1/0/all/0/1&quot;&gt;Brian Kingsbury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tejwani_R/0/1/0/all/0/1&quot;&gt;Ravi Tejwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bouneffouf_D/0/1/0/all/0/1&quot;&gt;Djallel Bouneffouf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09178">
<title>A Unified Analysis of Random Fourier Features. (arXiv:1806.09178v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.09178</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide the first unified theoretical analysis of supervised learning with
random Fourier features, covering different types of loss functions
characteristic to kernel methods developed for this setting. More specifically,
we investigate learning with squared error and Lipschitz continuous loss
functions and give the sharpest expected risk convergence rates for problems in
which random Fourier features are sampled either using the spectral measure
corresponding to a shift-invariant kernel or the ridge leverage score function
proposed in~\cite{avron2017random}. The trade-off between the number of
features and the expected risk convergence rate is expressed in terms of the
regularization parameter and the effective dimension of the problem. While the
former can effectively capture the complexity of the target hypothesis, the
latter is known for expressing the fine structure of the kernel with respect to
the marginal distribution of a data generating
process~\cite{caponnetto2007optimal}. In addition to our theoretical results,
we propose an approximate leverage score sampler for large scale problems and
show that it can be significantly more effective than the spectral measure
sampler.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ton_J/0/1/0/all/0/1&quot;&gt;Jean-Francois Ton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oglic_D/0/1/0/all/0/1&quot;&gt;Dino Oglic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sejdinovic_D/0/1/0/all/0/1&quot;&gt;Dino Sejdinovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09228">
<title>Deep $k$-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions. (arXiv:1806.09228v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09228</link>
<description rdf:parseType="Literal">&lt;p&gt;The current trend of pushing CNNs deeper with convolutions has created a
pressing demand to achieve higher compression gains on CNNs where convolutions
dominate the computation and parameter amount (e.g., GoogLeNet, ResNet and Wide
ResNet). Further, the high energy consumption of convolutions limits its
deployment on mobile devices. To this end, we proposed a simple yet effective
scheme for compressing convolutions though applying k-means clustering on the
weights, compression is achieved through weight-sharing, by only recording $K$
cluster centers and weight assignment indexes. We then introduced a novel
spectrally relaxed $k$-means regularization, which tends to make hard
assignments of convolutional layer weights to $K$ learned cluster centers
during re-training. We additionally propose an improved set of metrics to
estimate energy consumption of CNN hardware implementations, whose estimation
results are verified to be consistent with previously proposed energy
estimation tool extrapolated from actual hardware measurements. We finally
evaluated Deep $k$-Means across several CNN models in terms of both compression
ratio and energy consumption reduction, observing promising results without
incurring accuracy loss. The code is available at
https://github.com/Sandbox3aster/Deep-K-Means
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junru Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeraraghavan_A/0/1/0/all/0/1&quot;&gt;Ashok Veeraraghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yingyan Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09277">
<title>Towards Optimal Transport with Global Invariances. (arXiv:1806.09277v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.09277</link>
<description rdf:parseType="Literal">&lt;p&gt;Many problems in machine learning involve calculating correspondences between
sets of objects, such as point clouds or images. Discrete optimal transport
(OT) provides a natural and successful approach to such tasks whenever the two
sets of objects can be represented in the same space or when we can evaluate
distances between the objects. Unfortunately neither requirement is likely to
hold when object representations are learned from data. Indeed, automatically
derived representations such as word embeddings are typically fixed only up to
some global transformations, for example, reflection or rotation. As a result,
pairwise distances across the two types of objects are ill-defined without
specifying their relative transformation. In this work, we propose a general
framework for optimal transport in the presence of latent global
transformations. We discuss algorithms for the specific case of orthonormal
transformations, and show promising results in unsupervised word alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alvarez_Melis_D/0/1/0/all/0/1&quot;&gt;David Alvarez-Melis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jaakkola_T/0/1/0/all/0/1&quot;&gt;Tommi S. Jaakkola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09300">
<title>Improving Chemical Autoencoder Latent Space and Molecular De novo Generation Diversity with Heteroencoders. (arXiv:1806.09300v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09300</link>
<description rdf:parseType="Literal">&lt;p&gt;Chemical autoencoders are attractive models as they combine chemical space
navigation with possibilities for de novo molecule generation in areas of
interest. This enables them to produce focused chemical libraries around a
single lead compound so they can be employed early in a drug discovery project.
Here it is shown that the choice of chemical representation, such as SMILES
strings, has a large influence on the properties of the latent space. It is
further explored to what extent translating between different chemical
representations influences the latent space similarity to the SMILES strings or
circular fingerprints. By employing SMILES enumeration for both the encoder and
decoder, it is found that the decoder has the largest influence on the
properties of the latent space. Training a sequence to sequence heteroencoder
based on recurrent neural networks(RNNs) with long short-term memory cells
(LSTM) to predicts different enumerated SMILES strings from the same canonical
SMILES string gives the largest similarity between latent space distance and
molecular similarity measured as circular fingerprints similarity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bjerrum_E/0/1/0/all/0/1&quot;&gt;Esben Jannik Bjerrum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09390">
<title>Accelerating likelihood optimization for ICA on real signals. (arXiv:1806.09390v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.09390</link>
<description rdf:parseType="Literal">&lt;p&gt;We study optimization methods for solving the maximum likelihood formulation
of independent component analysis (ICA). We consider both the the problem
constrained to white signals and the unconstrained problem. The Hessian of the
objective function is costly to compute, which renders Newton&apos;s method
impractical for large data sets. Many algorithms proposed in the literature can
be rewritten as quasi-Newton methods, for which the Hessian approximation is
cheap to compute. These algorithms are very fast on simulated data where the
linear mixture assumption really holds. However, on real signals, we observe
that their rate of convergence can be severely impaired. In this paper, we
investigate the origins of this behavior, and show that the recently proposed
Preconditioned ICA for Real Data (Picard) algorithm overcomes this issue on
both constrained and unconstrained problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1&quot;&gt;Pierre Ablin&lt;/a&gt; (PARIETAL), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cardoso_J/0/1/0/all/0/1&quot;&gt;Jean-Fran&amp;#xe7;ois Cardoso&lt;/a&gt; (IAP, CNRS), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gramfort_A/0/1/0/all/0/1&quot;&gt;Alexandre Gramfort&lt;/a&gt; (PARIETAL)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09429">
<title>A Distributed Flexible Delay-tolerant Proximal Gradient Algorithm. (arXiv:1806.09429v1 [math.OC])</title>
<link>http://arxiv.org/abs/1806.09429</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop and analyze an asynchronous algorithm for distributed convex
optimization when the objective writes a sum of smooth functions, local to each
worker, and a non-smooth function. Unlike many existing methods, our
distributed algorithm is adjustable to various levels of communication cost,
delays, machines computational power, and functions smoothness. A unique
feature is that the stepsizes do not depend on communication delays nor number
of machines, which is highly desirable for scalability. We prove that the
algorithm converges linearly in the strongly convex case, and provide
guarantees of convergence for the non-strongly convex case. The obtained rates
are the same as the vanilla proximal gradient algorithm over some introduced
epoch sequence that subsumes the delays of the system. We provide numerical
results on large-scale machine learning problems to demonstrate the merits of
the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mishchenko_K/0/1/0/all/0/1&quot;&gt;Konstantin Mishchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Iutzeler_F/0/1/0/all/0/1&quot;&gt;Franck Iutzeler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Malick_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xf4;me Malick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09431">
<title>Propagating Uncertainty through the tanh Function with Application to Reservoir Computing. (arXiv:1806.09431v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.09431</link>
<description rdf:parseType="Literal">&lt;p&gt;Many neural networks use the tanh activation function, however when given a
probability distribution as input, the problem of computing the output
distribution in neural networks with tanh activation has not yet been
addressed. One important example is the initialization of the echo state
network in reservoir computing, where random initialization of the reservoir
requires time to wash out the initial conditions, thereby wasting precious data
and computational resources. Motivated by this problem, we propose a novel
solution utilizing a moment based approach to propagate uncertainty through an
Echo State Network to reduce the washout time. In this work, we contribute two
new methods to propagate uncertainty through the tanh activation function and
propose the Probabilistic Echo State Network (PESN), a method that is shown to
have better average performance than deterministic Echo State Networks given
the random initialization of reservoir states. Additionally we test single and
multi-step uncertainty propagation of our method on two regression tasks and
show that we are able to recover similar means and variances as computed by
Monte-Carlo simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gandhi_M/0/1/0/all/0/1&quot;&gt;Manan Gandhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Keuntaek Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Theodorou_E/0/1/0/all/0/1&quot;&gt;Evangelos Theodorou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09460">
<title>A Tour of Reinforcement Learning: The View from Continuous Control. (arXiv:1806.09460v1 [math.OC])</title>
<link>http://arxiv.org/abs/1806.09460</link>
<description rdf:parseType="Literal">&lt;p&gt;This manuscript surveys reinforcement learning from the perspective of
optimization and control with a focus on continuous control applications. It
surveys the general formulation, terminology, and typical experimental
implementations of reinforcement learning and reviews competing solution
paradigms. In order to compare the relative merits of various techniques, this
survey presents a case study of the Linear Quadratic Regulator (LQR) with
unknown dynamics, perhaps the simplest and best studied problem in optimal
control. The manuscript describes how merging techniques from learning theory
and control can provide non-asymptotic characterizations of LQR performance and
shows that these characterizations tend to match experimental behavior. In
turn, when revisiting more complex applications, many of the observed phenomena
in LQR persist. In particular, theory and experiment demonstrate the role and
importance of models and the cost of generality in reinforcement learning
algorithms. This survey concludes with a discussion of some of the challenges
in designing learning systems that safely and reliably interact with complex
and uncertain environments and how tools from reinforcement learning and
controls might be combined to approach these challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Recht_B/0/1/0/all/0/1&quot;&gt;Benjamin Recht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09544">
<title>Towards Optimal Estimation of Bivariate Isotonic Matrices with Unknown Permutations. (arXiv:1806.09544v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.09544</link>
<description rdf:parseType="Literal">&lt;p&gt;Many applications, including rank aggregation, crowd-labeling, and graphon
estimation, can be modeled in terms of a bivariate isotonic matrix with unknown
permutations acting on its rows and columns. We consider the problem of
estimating such a matrix based on noisy observations of a subset of its
entries, and design and analyze polynomial-time algorithms that improve upon
the state of the art. In particular, our results imply that any such $n \times
n$ matrix can be estimated efficiently in the normalized, squared Frobenius
norm at rate $\widetilde{\mathcal O}(n^{-3/4})$, thus narrowing the gap between
$\widetilde{\mathcal O}(n^{-1})$ and $\widetilde{\mathcal O}(n^{-1/2})$,
hitherto the rates of the most statistically and computationally efficient
methods, respectively. Additionally, our algorithms are minimax optimal in
another natural metric that measures how well an estimate captures each row of
the matrix. Along the way, we prove matching upper and lower bounds on the
minimax radii of certain cone testing problems, which may be of independent
interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mao_C/0/1/0/all/0/1&quot;&gt;Cheng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pananjady_A/0/1/0/all/0/1&quot;&gt;Ashwin Pananjady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wainwright_M/0/1/0/all/0/1&quot;&gt;Martin J. Wainwright&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09548">
<title>Learning dynamical systems with particle stochastic approximation EM. (arXiv:1806.09548v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1806.09548</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the particle stochastic approximation EM (PSAEM) algorithm for
learning of dynamical systems. The method builds on the EM algorithm, an
iterative procedure for maximum likelihood inference in latent variable models.
By combining stochastic approximation EM and particle Gibbs with ancestor
sampling (PGAS), PSAEM obtains superior computational performance and
convergence properties compared to plain particle-smoothing-based
approximations of the EM algorithm. PSAEM can be used for plain maximum
likelihood inference as well as for empirical Bayes learning of
hyperparameters. Specifically, the latter point means that existing PGAS
implementations easily can be extended with PSAEM to estimate hyperparameters
at almost no extra computational cost. We discuss the convergence properties of
the algorithm, and demonstrate it on several machine learning applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Svensson_A/0/1/0/all/0/1&quot;&gt;Andreas Svensson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lindsten_F/0/1/0/all/0/1&quot;&gt;Fredrik Lindsten&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09550">
<title>Inference Trees: Adaptive Inference with Exploration. (arXiv:1806.09550v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1806.09550</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce inference trees (ITs), a new class of inference methods that
build on ideas from Monte Carlo tree search to perform adaptive sampling in a
manner that balances exploration with exploitation, ensures consistency, and
alleviates pathologies in existing adaptive methods. ITs adaptively sample from
hierarchical partitions of the parameter space, while simultaneously learning
these partitions in an online manner. This enables ITs to not only identify
regions of high posterior mass, but also maintain uncertainty estimates to
track regions where significant posterior mass may have been missed. ITs can be
based on any inference method that provides a consistent estimate of the
marginal likelihood. They are particularly effective when combined with
sequential Monte Carlo, where they capture long-range dependencies and yield
improvements beyond proposal adaptation alone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1&quot;&gt;Tom Rainforth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1&quot;&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wood_F/0/1/0/all/0/1&quot;&gt;Frank Wood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongseok Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meent_J/0/1/0/all/0/1&quot;&gt;Jan-Willem van de Meent&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09571">
<title>Asymptotic Properties of Recursive Maximum Likelihood Estimation in Non-Linear State-Space Models. (arXiv:1806.09571v1 [math.ST])</title>
<link>http://arxiv.org/abs/1806.09571</link>
<description rdf:parseType="Literal">&lt;p&gt;Using stochastic gradient search and the optimal filter derivative, it is
possible to perform recursive (i.e., online) maximum likelihood estimation in a
non-linear state-space model. As the optimal filter and its derivative are
analytically intractable for such a model, they need to be approximated
numerically. In [Poyiadjis, Doucet and Singh, Biometrika 2018], a recursive
maximum likelihood algorithm based on a particle approximation to the optimal
filter derivative has been proposed and studied through numerical simulations.
Here, this algorithm and its asymptotic behavior are analyzed theoretically. We
show that the algorithm accurately estimates maxima to the underlying (average)
log-likelihood when the number of particles is sufficiently large. We also
derive (relatively) tight bounds on the estimation error. The obtained results
hold under (relatively) mild conditions and cover several classes of non-linear
state-space models met in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tadic_V/0/1/0/all/0/1&quot;&gt;Vladislav Z.B. Tadic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Doucet_A/0/1/0/all/0/1&quot;&gt;Arnaud Doucet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09588">
<title>Fundamental limits of detection in the spiked Wigner model. (arXiv:1806.09588v1 [math.PR])</title>
<link>http://arxiv.org/abs/1806.09588</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the fundamental limits of detecting the presence of an additive
rank-one perturbation, or spike, to a Wigner matrix. When the spike comes from
a prior that is i.i.d. across coordinates, we prove that the log-likelihood
ratio of the spiked model against the non-spiked one is asymptotically normal
below a certain reconstruction threshold which is not necessarily of a
&quot;spectral&quot; nature, and that it is degenerate above. This establishes the
maximal region of contiguity between the planted and null models. It is known
that this threshold also marks a phase transition for estimating the spike: the
latter task is possible above the threshold and impossible below. Therefore,
both estimation and detection undergo the same transition in this random matrix
model. We also provide further information about the performance of the optimal
test. Our proofs are based on Gaussian interpolation methods and a rigorous
incarnation of the cavity method, as devised by Guerra and Talagrand in their
study of the Sherrington--Kirkpatrick spin-glass model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Alaoui_A/0/1/0/all/0/1&quot;&gt;Ahmed El Alaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Krzakala_F/0/1/0/all/0/1&quot;&gt;Florent Krzakala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09602">
<title>A Machine-learning framework for automatic reference-free quality assessment in MRI. (arXiv:1806.09602v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.09602</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic resonance (MR) imaging offers a wide variety of imaging techniques.
A large amount of data is created per examination which needs to be checked for
sufficient quality in order to derive a meaningful diagnosis. This is a manual
process and therefore time- and cost-intensive. Any imaging artifacts
originating from scanner hardware, signal processing or induced by the patient
may reduce the image quality and complicate the diagnosis or any image
post-processing. Therefore, the assessment or the ensurance of sufficient image
quality in an automated manner is of high interest. Usually no reference image
is available or difficult to define. Therefore, classical reference-based
approaches are not applicable. Model observers mimicking the human observers
(HO) can assist in this task. Thus, we propose a new machine-learning-based
reference-free MR image quality assessment framework which is trained on
HO-derived labels to assess MR image quality immediately after each
acquisition. We include the concept of active learning and present an efficient
blinded reading platform to reduce the effort in the HO labeling procedure.
Derived image features and the applied classifiers (support-vector-machine,
deep neural network) are investigated for a cohort of 250 patients. The MR
image quality assessment framework can achieve a high test accuracy of 93.7$\%$
for estimating quality classes on a 5-point Likert-scale. The proposed MR image
quality assessment framework is able to provide an accurate and efficient
quality estimation which can be used as a prospective quality assurance
including automatic acquisition adaptation or guided MR scanner operation,
and/or as a retrospective quality assessment including support of diagnostic
decisions or quality control in cohort studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuestner_T/0/1/0/all/0/1&quot;&gt;Thomas Kuestner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1&quot;&gt;Sergios Gatidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liebgott_A/0/1/0/all/0/1&quot;&gt;Annika Liebgott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_M/0/1/0/all/0/1&quot;&gt;Martin Schwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mauch_L/0/1/0/all/0/1&quot;&gt;Lukas Mauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martirosian_P/0/1/0/all/0/1&quot;&gt;Petros Martirosian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_H/0/1/0/all/0/1&quot;&gt;Holger Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwenzer_N/0/1/0/all/0/1&quot;&gt;Nina F. Schwenzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaou_K/0/1/0/all/0/1&quot;&gt;Konstantin Nikolaou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bamberg_F/0/1/0/all/0/1&quot;&gt;Fabian Bamberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schick_F/0/1/0/all/0/1&quot;&gt;Fritz Schick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.04312">
<title>FDR-Corrected Sparse Canonical Correlation Analysis with Applications to Imaging Genomics. (arXiv:1705.04312v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1705.04312</link>
<description rdf:parseType="Literal">&lt;p&gt;Reducing the number of false discoveries is presently one of the most
pressing issues in the life sciences. It is of especially great importance for
many applications in neuroimaging and genomics, where datasets are typically
high-dimensional, which means that the number of explanatory variables exceeds
the sample size. The false discovery rate (FDR) is a criterion that can be
employed to address that issue. Thus it has gained great popularity as a tool
for testing multiple hypotheses. Canonical correlation analysis (CCA) is a
statistical technique that is used to make sense of the cross-correlation of
two sets of measurements collected on the same set of samples (e.g., brain
imaging and genomic data for the same mental illness patients), and sparse CCA
extends the classical method to high-dimensional settings. Here we propose a
way of applying the FDR concept to sparse CCA, and a method to control the FDR.
The proposed FDR correction directly influences the sparsity of the solution,
adapting it to the unknown true sparsity level. Theoretical derivation as well
as simulation studies show that our procedure indeed keeps the FDR of the
canonical vectors below a user-specified target level. We apply the proposed
method to an imaging genomics dataset from the Philadelphia Neurodevelopmental
Cohort. Our results link the brain connectivity profiles derived from brain
activity during an emotion identification task, as measured by functional
magnetic resonance imaging (fMRI), to the corresponding subjects&apos; genomic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gossmann_A/0/1/0/all/0/1&quot;&gt;Alexej Gossmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zille_P/0/1/0/all/0/1&quot;&gt;Pascal Zille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Calhoun_V/0/1/0/all/0/1&quot;&gt;Vince Calhoun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Ping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.00049">
<title>Interpretable Active Learning. (arXiv:1708.00049v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.00049</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning has long been a topic of study in machine learning. However,
as increasingly complex and opaque models have become standard practice, the
process of active learning, too, has become more opaque. There has been little
investigation into interpreting what specific trends and patterns an active
learning strategy may be exploring. This work expands on the Local
Interpretable Model-agnostic Explanations framework (LIME) to provide
explanations for active learning recommendations. We demonstrate how LIME can
be used to generate locally faithful explanations for an active learning
strategy, and how these explanations can be used to understand how different
models and datasets explore a problem space over time. In order to quantify the
per-subgroup differences in how an active learning strategy queries spatial
regions, we introduce a notion of uncertainty bias (based on disparate impact)
to measure the discrepancy in the confidence for a model&apos;s predictions between
one subgroup and another. Using the uncertainty bias measure, we show that our
query explanations accurately reflect the subgroup focus of the active learning
queries, allowing for an interpretable explanation of what is being learned as
points with similar sources of uncertainty have their uncertainty bias
resolved. We demonstrate that this technique can be applied to track
uncertainty bias over user-defined clusters or automatically generated clusters
based on the source of uncertainty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Phillips_R/0/1/0/all/0/1&quot;&gt;Richard L. Phillips&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kyu Hyun Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Friedler_S/0/1/0/all/0/1&quot;&gt;Sorelle A. Friedler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.01179">
<title>Continuous-Time Flows for Efficient Inference and Density Estimation. (arXiv:1709.01179v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.01179</link>
<description rdf:parseType="Literal">&lt;p&gt;Two fundamental problems in unsupervised learning are efficient inference for
latent-variable models and robust density estimation based on large amounts of
unlabeled data. Algorithms for the two tasks, such as normalizing flows and
generative adversarial networks (GANs), are often developed independently. In
this paper, we propose the concept of {\em continuous-time flows} (CTFs), a
family of diffusion-based methods that are able to asymptotically approach a
target distribution. Distinct from normalizing flows and GANs, CTFs can be
adopted to achieve the above two goals in one framework, with theoretical
guarantees. Our framework includes distilling knowledge from a CTF for
efficient inference, and learning an explicit energy-based distribution with
CTFs for density estimation. Both tasks rely on a new technique for
distribution matching within amortized learning. Experiments on various tasks
demonstrate promising performance of the proposed CTF framework, compared to
related techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changyou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liqun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenlin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pu_Y/0/1/0/all/0/1&quot;&gt;Yunchen Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04212">
<title>Asymptotic Bayesian Generalization Error in a General Stochastic Matrix Factorization. (arXiv:1709.04212v5 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04212</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic matrix factorization (SMF) can be regarded as a restriction of
non-negative matrix factorization (NMF). SMF is useful for inference of topic
models, NMF for binary matrices data, Markov chains, and Bayesian networks.
However, SMF needs strong assumptions to reach a unique factorization and its
theoretical prediction accuracy has not yet been clarified. In this paper, we
study the maximum the pole of zeta function (real log canonical threshold) of a
general SMF and derive an upper bound of the generalization error in Bayesian
inference. The results give a foundation for a widely applicable and rigorous
factorization method of SMF and mean that the generalization error in SMF
becomes smaller than regular statistical models by Bayesian inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hayashi_N/0/1/0/all/0/1&quot;&gt;Naoki Hayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Watanabe_S/0/1/0/all/0/1&quot;&gt;Sumio Watanabe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08160">
<title>An Interpretable and Sparse Neural Network Model for Nonlinear Granger Causality Discovery. (arXiv:1711.08160v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08160</link>
<description rdf:parseType="Literal">&lt;p&gt;While most classical approaches to Granger causality detection repose upon
linear time series assumptions, many interactions in neuroscience and economics
applications are nonlinear. We develop an approach to nonlinear Granger
causality detection using multilayer perceptrons where the input to the network
is the past time lags of all series and the output is the future value of a
single series. A sufficient condition for Granger non-causality in this setting
is that all of the outgoing weights of the input data, the past lags of a
series, to the first hidden layer are zero. For estimation, we utilize a group
lasso penalty to shrink groups of input weights to zero. We also propose a
hierarchical penalty for simultaneous Granger causality and lag estimation. We
validate our approach on simulated data from both a sparse linear
autoregressive model and the sparse and nonlinear Lorenz-96 model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tank_A/0/1/0/all/0/1&quot;&gt;Alex Tank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cover_I/0/1/0/all/0/1&quot;&gt;Ian Cover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Foti_N/0/1/0/all/0/1&quot;&gt;Nicholas J. Foti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shojaie_A/0/1/0/all/0/1&quot;&gt;Ali Shojaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fox_E/0/1/0/all/0/1&quot;&gt;Emily B. Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08392">
<title>An Efficient ADMM Algorithm for Structural Break Detection in Multivariate Time Series. (arXiv:1711.08392v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08392</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an efficient alternating direction method of multipliers (ADMM)
algorithm for segmenting a multivariate non-stationary time series with
structural breaks into stationary regions. We draw from recent work where the
series is assumed to follow a vector autoregressive model within segments and a
convex estimation procedure may be formulated using group fused lasso
penalties. Our ADMM approach first splits the convex problem into a global
quadratic program and a simple group lasso proximal update. We show that the
global problem may be parallelized over rows of the time dependent transition
matrices and furthermore that each subproblem may be rewritten in a form
identical to the log-likelihood of a Gaussian state space model. Consequently,
we develop a Kalman smoothing algorithm to solve the global update in time
linear in the length of the series.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tank_A/0/1/0/all/0/1&quot;&gt;Alex Tank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fox_E/0/1/0/all/0/1&quot;&gt;Emily B. Fox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shojaie_A/0/1/0/all/0/1&quot;&gt;Ali Shojaie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04912">
<title>Quasi-Oracle Estimation of Heterogeneous Treatment Effects. (arXiv:1712.04912v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04912</link>
<description rdf:parseType="Literal">&lt;p&gt;Flexible estimation of heterogeneous treatment effects lies at the heart of
many statistical challenges, such as personalized medicine and optimal resource
allocation. In this paper, we develop a general class of two-step algorithms
for heterogeneous treatment effect estimation in observational studies. We
first estimate marginal effects and treatment propensities in order to form an
objective function that isolates the causal component of the signal. Then, we
optimize this data-adaptive objective function. Our approach has several
advantages over existing methods. From a practical perspective, our method is
flexible and easy to use: For both steps, we can use any loss-minimization
method, e.g., penalized regression, deep neutral networks, or boosting;
moreover, these methods can be fine-tuned by cross validation. Meanwhile, in
the case of penalized kernel regression, we show that our method has a
quasi-oracle property: Even if the pilot estimates for marginal effects and
treatment propensities are not particularly accurate, we achieve the same
regret bounds as an oracle who has a priori knowledge of these two nuisance
components. We implement variants of our approach based on both penalized
regression and boosting in a variety of simulation setups, and find promising
performance relative to existing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nie_X/0/1/0/all/0/1&quot;&gt;Xinkun Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wager_S/0/1/0/all/0/1&quot;&gt;Stefan Wager&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04537">
<title>Tighter Variational Bounds are Not Necessarily Better. (arXiv:1802.04537v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04537</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide theoretical and empirical evidence that using tighter evidence
lower bounds (ELBOs) can be detrimental to the process of learning an inference
network by reducing the signal-to-noise ratio of the gradient estimator. Our
results call into question common implicit assumptions that tighter ELBOs are
better variational objectives for simultaneous model learning and inference
amortization schemes. Based on our insights, we introduce three new algorithms:
the partially importance weighted auto-encoder (PIWAE), the multiply importance
weighted auto-encoder (MIWAE), and the combination importance weighted
auto-encoder (CIWAE), each of which includes the standard importance weighted
auto-encoder (IWAE) as a special case. We show that each can deliver
improvements over IWAE, even when performance is measured by the IWAE target
itself. Furthermore, our results suggest that PIWAE may be able to deliver
simultaneous improvements in the training of both the inference and generative
networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1&quot;&gt;Tom Rainforth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kosiorek_A/0/1/0/all/0/1&quot;&gt;Adam R. Kosiorek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Tuan Anh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maddison_C/0/1/0/all/0/1&quot;&gt;Chris J. Maddison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Igl_M/0/1/0/all/0/1&quot;&gt;Maximilian Igl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wood_F/0/1/0/all/0/1&quot;&gt;Frank Wood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1&quot;&gt;Yee Whye Teh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00183">
<title>Learning with Correntropy-induced Losses for Regression with Mixture of Symmetric Stable Noise. (arXiv:1803.00183v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00183</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, correntropy and its applications in machine learning have
been drawing continuous attention owing to its merits in dealing with
non-Gaussian noise and outliers. However, theoretical understanding of
correntropy, especially in the statistical learning context, is still limited.
In this study, within the statistical learning framework, we investigate
correntropy based regression in the presence of non-Gaussian noise or outliers.
Motivated by the practical way of generating non-Gaussian noise or outliers, we
introduce mixture of symmetric stable noise, which include Gaussian noise,
Cauchy noise, and their mixture as special cases, to model non-Gaussian noise
or outliers. We demonstrate that under the mixture of symmetric stable noise
assumption, correntropy based regression can learn the conditional mean
function or the conditional median function well without resorting to the
finite-variance or even the finite first-order moment condition on the noise.
In particular, for the above two cases, we establish asymptotic optimal
learning rates for correntropy based regression estimators that are
asymptotically of type $\mathcal{O}(n^{-1})$. These results justify the
effectiveness of the correntropy based regression estimators in dealing with
outliers as well as non-Gaussian noise. We believe that the present study
completes our understanding towards correntropy based regression from a
statistical learning viewpoint, and may also shed some light on robust
statistical learning for regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yunlong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Y/0/1/0/all/0/1&quot;&gt;Yiming Ying&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01302">
<title>Distributed Nonparametric Regression under Communication Constraints. (arXiv:1803.01302v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01302</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the problem of nonparametric estimation of a smooth
function with data distributed across multiple machines. We assume an
independent sample from a white noise model is collected at each machine, and
an estimator of the underlying true function needs to be constructed at a
central machine. We place limits on the number of bits that each machine can
use to transmit information to the central machine. Our results give both
asymptotic lower bounds and matching upper bounds on the statistical risk under
various settings. We identify three regimes, depending on the relationship
among the number of machines, the size of the data available at each machine,
and the communication budget. When the communication budget is small, the
statistical risk depends solely on this communication bottleneck, regardless of
the sample size. In the regime where the communication budget is large, the
classic minimax risk in the non-distributed estimation setting is recovered. In
an intermediate regime, the statistical risk depends on both the sample size
and the communication budget.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuancheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lafferty_J/0/1/0/all/0/1&quot;&gt;John Lafferty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01440">
<title>Hierarchical Modeling and Shrinkage for User Session Length Prediction in Media Streaming. (arXiv:1803.01440v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01440</link>
<description rdf:parseType="Literal">&lt;p&gt;An important metric of users&apos; satisfaction and engagement within on-line
streaming services is the user session length, i.e. the amount of time they
spend on a service continuously without interruption. Being able to predict
this value directly benefits the recommendation and ad pacing contexts in music
and video streaming services. Recent research has shown that predicting the
exact amount of time spent is highly nontrivial due to many external factors
for which a user can end a session, and the lack of predictive covariates. Most
of the other related literature on duration based user engagement has focused
on dwell time for websites, for search and display ads, mainly for post-click
satisfaction prediction or ad ranking.
&lt;/p&gt;
&lt;p&gt;In this work we present a novel framework inspired by hierarchical Bayesian
modeling to predict, at the moment of login, the amount of time a user will
spend in the streaming service. The time spent by a user on a platform depends
upon user-specific latent variables which are learned via hierarchical
shrinkage. Our framework enjoys theoretical guarantees and naturally
incorporates flexible parametric/nonparametric models on the covariates,
including models robust to outliers. Our proposal is found to outperform
state-of- the-art estimators in terms of efficiency and predictive performance
on real world public and private datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dedieu_A/0/1/0/all/0/1&quot;&gt;Antoine Dedieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mazumder_R/0/1/0/all/0/1&quot;&gt;Rahul Mazumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vahabi_H/0/1/0/all/0/1&quot;&gt;Hossein Vahabi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05554">
<title>Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models. (arXiv:1803.05554v3 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05554</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning a Bayesian network (BN) from data can be useful for decision-making
or discovering causal relationships. However, traditional methods often fail in
modern applications, which exhibit a larger number of observed variables than
data points. The resulting uncertainty about the underlying network as well as
the desire to incorporate prior information recommend a Bayesian approach to
learning the BN, but the highly combinatorial structure of BNs poses a striking
challenge for inference. The current state-of-the-art methods such as order
MCMC are faster than previous methods but prevent the use of many natural
structural priors and still have running time exponential in the maximum
indegree of the true directed acyclic graph (DAG) of the BN. We here propose an
alternative posterior approximation based on the observation that, if we
incorporate empirical conditional independence tests, we can focus on a
high-probability DAG associated with each order of the vertices. We show that
our method allows the desired flexibility in prior specification, removes
timing dependence on the maximum indegree and yields provably good posterior
approximations; in addition, we show that it achieves superior accuracy,
scalability, and sampler mixing on several datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Agrawal_R/0/1/0/all/0/1&quot;&gt;Raj Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Broderick_T/0/1/0/all/0/1&quot;&gt;Tamara Broderick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Uhler_C/0/1/0/all/0/1&quot;&gt;Caroline Uhler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00636">
<title>Recursive Optimization of Convex Risk Measures: Mean-Semideviation Models. (arXiv:1804.00636v4 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00636</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop and analyze recursive, data-driven, stochastic subgradient methods
for optimizing a new and versatile class of convex risk measures, termed here
as mean-semideviations. Their construction relies on on the concept of a risk
regularizer, a one-dimensional nonlinear map with certain properties,
essentially generalizing the positive part weighting function in the
mean-upper-semideviation risk measure. After we formally introduce
mean-semideviations, we study their basic properties, and we present a
fundamental constructive characterization result, demonstrating their
generality.
&lt;/p&gt;
&lt;p&gt;We then introduce and rigorously analyze the MESSAGEp algorithm, an efficient
stochastic subgradient procedure for iteratively solving convex
mean-semideviation risk-averse problems to optimality. The MESSAGEp algorithm
may be derived as an application of the T-SCGD algorithm of (Yang et al.,
2018). However, the generic theoretical framework of (Yang et al., 2018) is
narrow and structurally restrictive, as far as optimization of
mean-semideviations is concerned, including the classical
mean-upper-semideviation risk measure. By exploiting problem structure, we
propose a substantially weaker set of assumptions, under which we establish
pathwise convergence of the MESSAGEp algorithm, under the same strong sense as
in (Yang et al., 2018). The new framework reveals a fundamental trade-off
between the expansiveness of the random position function and the smoothness of
the particular mean-semideviation risk measure under consideration. Further, we
explicitly show that the class of mean-semideviation problems supported under
our framework is strictly larger than the respective class of problems
supported in (Yang et al., 2018). Thus, applicability of compositional
stochastic optimization is established for a strictly wider spectrum of
mean-semideviation problems, justifying the purpose of this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kalogerias_D/0/1/0/all/0/1&quot;&gt;Dionysios S. Kalogerias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Powell_W/0/1/0/all/0/1&quot;&gt;Warren B. Powell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07687">
<title>Machine Teaching for Inverse Reinforcement Learning: Algorithms and Applications. (arXiv:1805.07687v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07687</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse reinforcement learning (IRL) infers a reward function from
demonstrations, allowing for policy improvement and generalization. However,
despite much recent interest in IRL, little work has been done to understand of
the minimum set of demonstrations needed to teach a specific sequential
decision-making task. We formalize the problem of finding optimal
demonstrations for IRL as a machine teaching problem where the goal is to find
the minimum number of demonstrations needed to specify the reward equivalence
class of the demonstrator. We extend previous work on algorithmic teaching for
sequential decision-making tasks by showing an equivalence to the set cover
problem, and use this equivalence to develop an efficient algorithm for
determining the set of maximally-informative demonstrations. We apply our
proposed machine teaching algorithm to two novel applications: benchmarking
active learning IRL algorithms and developing an IRL algorithm that, rather
than assuming demonstrations are i.i.d., uses counterfactual reasoning over
informative demonstrations to learn more efficiently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1&quot;&gt;Daniel S. Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1&quot;&gt;Scott Niekum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08268">
<title>Effective Dimension of Exp-concave Optimization. (arXiv:1805.08268v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08268</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the role of the effective (a.k.a. statistical) dimension in
determining both the statistical and the computational costs associated with
exp-concave stochastic minimization. We derive sample complexity bounds that
scale with $\frac{d_{\lambda}}{\epsilon}$, where $d_{\lambda}$ is the effective
dimension associated with the regularization parameter $\lambda$. These are the
first fast rates in this setting that do not exhibit any explicit dependence
either on the intrinsic dimension or the $\ell_{2}$-norm of the optimal
classifier.
&lt;/p&gt;
&lt;p&gt;We also propose fast preconditioned method that solves the ERM problem in
time $\tilde{O} \left(\min \left \{\frac{\lambda&apos;}{\lambda} \left(
nnz(A)+d_{\lambda&apos;}^{2}d\right) :~\lambda&apos; \ge \lambda \right \} \right)$,
where $nnz(A)$ is the number of nonzero entries in the data.
&lt;/p&gt;
&lt;p&gt;Our analysis emphasizes interesting connections between leverage scores,
algorithmic stability and regularization. In particular, our algorithm involves
a novel technique for optimizing a tradeoff between oracle complexity and
effective dimension. All of our results extend to the kernel setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1&quot;&gt;Naman Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonen_A/0/1/0/all/0/1&quot;&gt;Alon Gonen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10833">
<title>Bayesian Learning with Wasserstein Barycenters. (arXiv:1805.10833v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.10833</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we introduce a novel paradigm for Bayesian learning based on
optimal transport theory. Namely, we propose to use the Wasserstein barycenter
of the posterior law on models, as an alternative to the maximum a posteriori
estimator and Bayesian model average. We exhibit conditions granting the
existence and consistency of this estimator, discuss some of its basic and
specific properties, and provide insight for practical implementations relying
on standard sampling in finite-dimensional parameter spaces. We thus contribute
to the recent blooming of applications of optimal transport theory in machine
learning, beyond the discrete setting so far considered. The advantages of the
proposed estimator are presented in theoretical terms and through analytical
and numeral examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rios_G/0/1/0/all/0/1&quot;&gt;Gonzalo Rios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Backhoff_Veraguas_J/0/1/0/all/0/1&quot;&gt;Julio Backhoff-Veraguas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fontbona_J/0/1/0/all/0/1&quot;&gt;Joaquin Fontbona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tobar_F/0/1/0/all/0/1&quot;&gt;Felipe Tobar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12244">
<title>Mining gold from implicit models to improve likelihood-free inference. (arXiv:1805.12244v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.12244</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulators often provide the best description of real-world phenomena;
however, they also lead to challenging inverse problems because the density
they implicitly define is often intractable. We present a new suite of
simulation-based inference techniques that go beyond the traditional
Approximate Bayesian Computation approach, which struggles in a
high-dimensional setting, and extend methods that use surrogate models based on
neural networks. We show that additional information, such as the joint
likelihood ratio and the joint score, can often be extracted from simulators
and used to augment the training data for these surrogate models. Finally, we
demonstrate that these new techniques are more sample efficient and provide
higher-fidelity inference than traditional methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brehmer_J/0/1/0/all/0/1&quot;&gt;Johann Brehmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Louppe_G/0/1/0/all/0/1&quot;&gt;Gilles Louppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pavez_J/0/1/0/all/0/1&quot;&gt;Juan Pavez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cranmer_K/0/1/0/all/0/1&quot;&gt;Kyle Cranmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12472">
<title>Distributed Estimation of Gaussian Correlations. (arXiv:1805.12472v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1805.12472</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a distributed estimation problem in which two remotely located
parties, Alice and Bob, observe an unlimited number of i.i.d. samples
corresponding to two different parts of a random vector. Alice can send $k$
bits on average to Bob, who in turn wants to estimate the cross-correlation
matrix between the two parts of the vector. In the case where the parties
observe jointly Gaussian scalar random variables with an unknown correlation
$\rho$, we obtain two constructive and simple unbiased estimators attaining a
variance of $(1-\rho^2)/(2k\ln 2)$, which coincides with a known but
non-constructive random coding result of Zhang and Berger. We extend our
approach to the vector Gaussian case, which has not been treated before, and
construct an estimator that is uniformly better than the scalar estimator
applied separately to each of the correlations. We then show that the Gaussian
performance can essentially be attained even when the distribution is
completely unknown. This in particular implies that in the general problem of
distributed correlation estimation, the variance can decay at least as $O(1/k)$
with the number of transmitted bits. This behavior, however, is not tight: we
give an example of a rich family of distributions for which local samples
reveal essentially nothing about the correlations, and where a slightly
modified estimator attains a variance of $2^{-\Omega(k)}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hadar_U/0/1/0/all/0/1&quot;&gt;Uri Hadar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shayevitz_O/0/1/0/all/0/1&quot;&gt;Ofer Shayevitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04090">
<title>ATOMO: Communication-efficient Learning via Atomic Sparsification. (arXiv:1806.04090v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04090</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed model training suffers from communication overheads due to
frequent gradient updates transmitted between compute nodes. To mitigate these
overheads, several studies propose the use of sparsified stochastic gradients.
We argue that these are facets of a general sparsification method that can
operate on any possible atomic decomposition. Notable examples include
element-wise, singular value, and Fourier decompositions. We present ATOMO, a
general framework for atomic sparsification of stochastic gradients. Given a
gradient, an atomic decomposition, and a sparsity budget, ATOMO gives a random
unbiased sparsification of the atoms minimizing variance. We show that methods
such as QSGD and TernGrad are special cases of ATOMO and show that sparsifiying
gradients in their singular value decomposition (SVD), rather than the
coordinate-wise one, can lead to significantly faster distributed training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sievert_S/0/1/0/all/0/1&quot;&gt;Scott Sievert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shengchao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Charles_Z/0/1/0/all/0/1&quot;&gt;Zachary Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papailiopoulos_D/0/1/0/all/0/1&quot;&gt;Dimitris Papailiopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wright_S/0/1/0/all/0/1&quot;&gt;Stephen Wright&lt;/a&gt;</dc:creator>
</item></rdf:RDF>