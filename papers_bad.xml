<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00987"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01221"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00983"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01060"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01157"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01214"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01385"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01388"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10105"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00909"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00979"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00982"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01044"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01045"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01049"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01078"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01136"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01156"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01174"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01209"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01222"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01237"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01278"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01288"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01360"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01367"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01207"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.05929"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11586"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08397"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05753"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06218"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08233"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00521"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00868"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00917"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.00987">
<title>Automatic Inference of Cross-modal Connection Topologies for X-CNNs. (arXiv:1805.00987v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.00987</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a way to learn cross-modal convolutional neural network
(X-CNN) architectures from a base convolutional network (CNN) and the training
data to reduce the design cost and enable applying cross-modal networks in
sparse data environments. Two approaches for building X-CNNs are presented. The
base approach learns the topology in a data-driven manner, by using
measurements performed on the base CNN and supplied data. The iterative
approach performs further optimisation of the topology through a combined
learning procedure, simultaneously learning the topology and training the
network. The approaches were evaluated agains examples of hand-designed X-CNNs
and their base variants, showing superior performance and, in some cases,
gaining an additional 9% of accuracy. From further considerations, we conclude
that the presented methodology takes less time than any manual approach would,
whilst also significantly reducing the design complexity. The application of
the methods is fully automated and implemented in Xsertion library.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karazija_L/0/1/0/all/0/1&quot;&gt;Laurynas Karazija&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1&quot;&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Li&amp;#xf2;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01221">
<title>Design and Analysis of Diversity-Based Parent Selection Schemes for Speeding Up Evolutionary Multi-objective Optimisation. (arXiv:1805.01221v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.01221</link>
<description rdf:parseType="Literal">&lt;p&gt;Parent selection in evolutionary algorithms for multi-objective optimisation
is usually performed by dominance mechanisms or indicator functions that prefer
non-dominated points. We propose to refine the parent selection on evolutionary
multi-objective optimisation with diversity-based metrics. The aim is to focus
on individuals with a high diversity contribution located in poorly explored
areas of the search space, so the chances of creating new non-dominated
individuals are better than in highly populated areas. We show by means of
rigorous runtime analysis that the use of diversity-based parent selection
mechanisms in the Simple Evolutionary Multi-objective Optimiser (SEMO) and
Global SEMO for the well known bi-objective functions ${\rm O{\small
NE}M{\small IN}M{\small AX}}$ and ${\rm LOTZ}$ can significantly improve their
performance. Our theoretical results are accompanied by experimental studies
that show a correspondence between theory and empirical results and motivate
further theoretical investigations in terms of stagnation. We show that
stagnation might occur when favouring individuals with a high diversity
contribution in the parent selection step and provide a discussion on which
scheme to use for more complex problems based on our theoretical and
experimental results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osuna_E/0/1/0/all/0/1&quot;&gt;Edgar Covantes Osuna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wanru Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_F/0/1/0/all/0/1&quot;&gt;Frank Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sudholt_D/0/1/0/all/0/1&quot;&gt;Dirk Sudholt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00983">
<title>Robust Deep Reinforcement Learning for Security and Safety in Autonomous Vehicle Systems. (arXiv:1805.00983v1 [cs.SY])</title>
<link>http://arxiv.org/abs/1805.00983</link>
<description rdf:parseType="Literal">&lt;p&gt;To operate effectively in tomorrow&apos;s smart cities, autonomous vehicles (AVs)
must rely on intra-vehicle sensors such as camera and radar as well as
inter-vehicle communication. Such dependence on sensors and communication links
exposes AVs to cyber-physical (CP) attacks by adversaries that seek to take
control of the AVs by manipulating their data. Thus, to ensure safe and optimal
AV dynamics control, the data processing functions at AVs must be robust to
such CP attacks. To this end, in this paper, the state estimation process for
monitoring AV dynamics, in presence of CP attacks, is analyzed and a novel
adversarial deep reinforcement learning (RL) algorithm is proposed to maximize
the robustness of AV dynamics control to CP attacks. The attacker&apos;s action and
the AV&apos;s reaction to CP attacks are studied in a game-theoretic framework. In
the formulated game, the attacker seeks to inject faulty data to AV sensor
readings so as to manipulate the inter-vehicle optimal safe spacing and
potentially increase the risk of AV accidents or reduce the vehicle flow on the
roads. Meanwhile, the AV, acting as a defender, seeks to minimize the
deviations of spacing so as to ensure robustness to the attacker&apos;s actions.
Since the AV has no information about the attacker&apos;s action and due to the
infinite possibilities for data value manipulations, the outcome of the
players&apos; past interactions are fed to long-short term memory (LSTM) blocks.
Each player&apos;s LSTM block learns the expected spacing deviation resulting from
its own action and feeds it to its RL algorithm. Then, the the attacker&apos;s RL
algorithm chooses the action which maximizes the spacing deviation, while the
AV&apos;s RL algorithm tries to find the optimal action that minimizes such
deviation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferdowsi_A/0/1/0/all/0/1&quot;&gt;Aidin Ferdowsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Challita_U/0/1/0/all/0/1&quot;&gt;Ursula Challita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1&quot;&gt;Walid Saad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandayam_N/0/1/0/all/0/1&quot;&gt;Narayan B. Mandayam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01060">
<title>Multimodal Emotion Recognition for One-Minute-Gradual Emotion Challenge. (arXiv:1805.01060v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.01060</link>
<description rdf:parseType="Literal">&lt;p&gt;The continuous dimensional emotion modelled by arousal and valence can depict
complex changes of emotions. In this paper, we present our works on arousal and
valence predictions for One-Minute-Gradual (OMG) Emotion Challenge. Multimodal
representations are first extracted from videos using a variety of acoustic,
video and textual models and support vector machine (SVM) is then used for
fusion of multimodal signals to make final predictions. Our solution achieves
Concordant Correlation Coefficient (CCC) scores of 0.397 and 0.520 on arousal
and valence respectively for the validation dataset, which outperforms the
baseline systems with the best CCC scores of 0.15 and 0.23 on arousal and
valence by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Ziqi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Chenjie Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xingwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guoqiang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01157">
<title>Graph Bayesian Optimization: Algorithms, Evaluations and Applications. (arXiv:1805.01157v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.01157</link>
<description rdf:parseType="Literal">&lt;p&gt;Network structure optimization is a fundamental task in complex network
analysis. However, almost all the research on Bayesian optimization is aimed at
optimizing the objective functions with vectorial inputs. In this work, we
first present a flexible framework, denoted graph Bayesian optimization, to
handle arbitrary graphs in the Bayesian optimization community. By combining
the proposed framework with graph kernels, it can take full advantage of
implicit graph structural features to supplement explicit features guessed
according to the experience, such as tags of nodes and any attributes of
graphs. The proposed framework can identify which features are more important
during the optimization process. We apply the framework to solve four problems
including two evaluations and two applications to demonstrate its efficacy and
potential applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jiaxu Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01214">
<title>The Algorithm Selection Competition Series 2015-17. (arXiv:1805.01214v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.01214</link>
<description rdf:parseType="Literal">&lt;p&gt;The algorithm selection problem is to choose the most suitable algorithm for
solving a given problem instance and thus, it leverages the complementarity
between different approaches that is present in many areas of AI. We report on
the state of the art in algorithm selection, as defined by the Algorithm
Selection Competition series 2015 to 2017. The results of these competitions
show how the state of the art improved over the years. Although performance in
some cases is very promising, there is still room for improvement in other
cases. Finally, we provide insights into why some scenarios are hard, and pose
challenges to the community on how to advance the current state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1&quot;&gt;Marius Lindauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rijn_J/0/1/0/all/0/1&quot;&gt;Jan N. van Rijn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotthoff_L/0/1/0/all/0/1&quot;&gt;Lars Kotthoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01385">
<title>Research on the Brain-inspired Cross-media Neural Cognitive Computing Framework. (arXiv:1805.01385v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.01385</link>
<description rdf:parseType="Literal">&lt;p&gt;To address modeling problems of brain-inspired intelligence, this thesis is
focused on researching in the semantic-oriented framework design for image,
audio, language and video. The Multimedia Neural Cognitive Computing (MNCC)
model was designed based on the nervous mechanism and cognitive architecture.
Furthermore, the semantic-oriented hierarchical Cross-media Neural Cognitive
Computing (CNCC) framework was proposed based on MNCC, and formal description
and analysis for CNCC was given. It would effectively improve the performance
of semantic processing for multimedia information, and has far-reaching
significance for exploration and realization brain-inspired computing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01388">
<title>Multi-Source Fusion Operations in Subjective Logic. (arXiv:1805.01388v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.01388</link>
<description rdf:parseType="Literal">&lt;p&gt;The purpose of multi-source fusion is to combine information from more than
two evidence sources, or subjective opinions from multiple actors. For
subjective logic, a number of different fusion operators have been proposed,
each matching a fusion scenario with different assumptions. However, not all of
these operators are associative, and therefore multi-source fusion is not
well-defined for these settings. In this paper, we address this challenge, and
define multi-source fusion for weighted belief fusion (WBF) and consensus &amp;amp;
compromise fusion (CCF). For WBF, we show the definition to be equivalent to
the intuitive formulation under the bijective mapping between subjective logic
and Dirichlet evidence PDFs. For CCF, since there is no independent
generalization, we show that the resulting multi-source fusion produces valid
opinions, and explain why our generalization is sound. For completeness, we
also provide corrections to previous results for averaging and cumulative
belief fusion (ABF and CBF), as well as belief constraint fusion (BCF), which
is an extension of Dempster&apos;s rule. With our generalizations of fusion
operators, fusing information from multiple sources is now well-defined for all
different fusion types defined in subjective logic. This enables wider
applicability of subjective logic in applications where multiple actors
interact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heijden_R/0/1/0/all/0/1&quot;&gt;Rens Wouter van der Heijden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kopp_H/0/1/0/all/0/1&quot;&gt;Henning Kopp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kargl_F/0/1/0/all/0/1&quot;&gt;Frank Kargl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10105">
<title>Tensor Completion Algorithms in Big Data Analytics. (arXiv:1711.10105v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10105</link>
<description rdf:parseType="Literal">&lt;p&gt;Tensor completion is a problem of filling the missing or unobserved entries
of partially observed tensors. Due to the multidimensional character of tensors
in describing complex datasets, tensor completion algorithms and their
applications have received wide attention and achievement in areas like data
mining, computer vision, signal processing, and neuroscience. In this survey,
we provide a modern overview of recent advances in tensor completion algorithms
from the perspective of big data analytics characterized by diverse variety,
large volume, and high velocity. We characterize these advances from four
perspectives: general tensor completion algorithms, tensor completion with
auxiliary information (variety), scalable tensor completion algorithms
(volume), and dynamic tensor completion algorithms (velocity). Further, we
identify several tensor completion applications on real-world data-driven
problems and present some common experimental frameworks popularized in the
literature. Our goal is to summarize these popular methods and introduce them
to researchers and practitioners for promoting future research and
applications. We conclude with a discussion of key challenges and promising
research directions in this community for future exploration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Song_Q/0/1/0/all/0/1&quot;&gt;Qingquan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ge_H/0/1/0/all/0/1&quot;&gt;Hancheng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Caverlee_J/0/1/0/all/0/1&quot;&gt;James Caverlee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xia Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08924">
<title>Learning-Based Mean-Payoff Optimization in an Unknown MDP under Omega-Regular Constraints. (arXiv:1804.08924v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08924</link>
<description rdf:parseType="Literal">&lt;p&gt;We formalize the problem of maximizing the mean-payoff value with high
probability while satisfying a parity objective in a Markov decision process
(MDP) with unknown probabilistic transition function and unknown reward
function. Assuming the support of the unknown transition function and a lower
bound on the minimal transition probability are known in advance, we show that
in MDPs consisting of a single end component, two combinations of guarantees on
the parity and mean-payoff objectives can be achieved depending on how much
memory one is willing to use. (i) For all $\epsilon$ and $\gamma$ we can
construct an online-learning finite-memory strategy that almost-surely
satisfies the parity objective and which achieves an $\epsilon$-optimal mean
payoff with probability at least $1 - \gamma$. (ii) Alternatively, for all
$\epsilon$ and $\gamma$ there exists an online-learning infinite-memory
strategy that satisfies the parity objective surely and which achieves an
$\epsilon$-optimal mean payoff with probability at least $1 - \gamma$. We
extend the above results to MDPs consisting of more than one end component in a
natural way. Finally, we show that the aforementioned guarantees are tight,
i.e. there are MDPs for which stronger combinations of the guarantees cannot be
ensured.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kretinsky_J/0/1/0/all/0/1&quot;&gt;Jan K&amp;#x159;et&amp;#xed;nsk&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1&quot;&gt;Guillermo A. P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raskin_J/0/1/0/all/0/1&quot;&gt;Jean-Fran&amp;#xe7;ois Raskin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00909">
<title>Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review. (arXiv:1805.00909v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00909</link>
<description rdf:parseType="Literal">&lt;p&gt;The framework of reinforcement learning or optimal control provides a
mathematical formalization of intelligent decision making that is powerful and
broadly applicable. While the general form of the reinforcement learning
problem enables effective reasoning about uncertainty, the connection between
reinforcement learning and inference in probabilistic models is not immediately
obvious. However, such a connection has considerable value when it comes to
algorithm design: formalizing a problem as probabilistic inference in principle
allows us to bring to bear a wide array of approximate inference tools, extend
the model in flexible and powerful ways, and reason about compositionality and
partial observability. In this article, we will discuss how a generalization of
the reinforcement learning or optimal control problem, which is sometimes
termed maximum entropy reinforcement learning, is equivalent to exact
probabilistic inference in the case of deterministic dynamics, and variational
inference in the case of stochastic dynamics. We will present a detailed
derivation of this framework, overview prior work that has drawn on this and
related ideas to propose new reinforcement learning and control algorithms, and
describe perspectives on future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00979">
<title>modAL: A modular active learning framework for Python. (arXiv:1805.00979v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.00979</link>
<description rdf:parseType="Literal">&lt;p&gt;modAL is a modular active learning framework for Python, aimed to make active
learning research and practice simpler. Its distinguishing features are (i)
clear and modular object oriented design (ii) full compatibility with
scikit-learn models and workflows. These features make fast prototyping and
easy extensibility possible, aiding the development of real-life active
learning pipelines and novel algorithms as well. modAL is fully open source,
hosted on GitHub at https://github.com/cosmic-cortex/modAL. To assure code
quality, extensive unit tests are provided and continuous integration is
applied. In addition, a detailed documentation with several tutorials are also
available for ease of use. The framework is available in PyPI and distributed
under the MIT license.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danka_T/0/1/0/all/0/1&quot;&gt;Tivadar Danka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horvath_P/0/1/0/all/0/1&quot;&gt;Peter Horvath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00982">
<title>SVRG meets SAGA: k-SVRG --- A Tale of Limited Memory. (arXiv:1805.00982v1 [math.OC])</title>
<link>http://arxiv.org/abs/1805.00982</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, many variance reduced algorithms for empirical risk
minimization have been introduced. In contrast to vanilla SGD, these methods
converge linearly on strong convex problems. To obtain the variance reduction,
current methods either require frequent passes over the full data to recompute
gradients---without making any progress during this time (like in SVRG), or
they require memory of the same size as the input problem (like SAGA).
&lt;/p&gt;
&lt;p&gt;In this work, we propose k-SVRG, an algorithm that interpolates between those
two extremes: it makes best use of the available memory and in turn does avoid
full passes over the data without making progress. We prove linear convergence
of k-SVRG on strongly convex problems and convergence to stationary points on
non-convex problems. Numerical experiments show the effectiveness of our
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Anant Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Stich_S/0/1/0/all/0/1&quot;&gt;Sebastian U. Stich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01044">
<title>Covariate Shift Estimation based Adaptive Ensemble Learning for Handling Non-Stationarity in Motor Imagery related EEG-based Brain-Computer Interface. (arXiv:1805.01044v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01044</link>
<description rdf:parseType="Literal">&lt;p&gt;The non-stationary nature of electroencephalography (EEG) signals makes an
EEG-based brain-computer interface (BCI) a dynamic system, thus improving its
performance is a challenging task. In addition, it is well-known that due to
non-stationarity based covariate shifts, the input data distributions of
EEG-based BCI systems change during inter- and intra-session transitions, which
poses great difficulty for developments of online adaptive data-driven systems.
Ensemble learning approaches have been used previously to tackle this
challenge. However, passive scheme based implementation leads to poor
efficiency while increasing high computational cost. This paper presents a
novel integration of covariate shift estimation and unsupervised adaptive
ensemble learning (CSE-UAEL) to tackle non-stationarity in motor-imagery (MI)
related EEG classification. The proposed method first employs an exponentially
weighted moving average model to detect the covariate shifts in the common
spatial pattern features extracted from MI related brain responses. Then, a
classifier ensemble was created and updated over time to account for changes in
streaming input data distribution wherein new classifiers are added to the
ensemble in accordance with estimated shifts. Furthermore, using two publicly
available BCI-related EEG datasets, the proposed method was extensively
compared with the state-of-the-art single-classifier based passive scheme,
single-classifier based active scheme and ensemble based passive schemes. The
experimental results show that the proposed active scheme based ensemble
learning algorithm significantly enhances the BCI performance in MI
classifications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raza_H/0/1/0/all/0/1&quot;&gt;Haider Raza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathee_D/0/1/0/all/0/1&quot;&gt;Dheeraj Rathee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;ShangMing Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cecotti_H/0/1/0/all/0/1&quot;&gt;Hubert Cecotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_G/0/1/0/all/0/1&quot;&gt;Girijesh Prasad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01045">
<title>Alpha-Beta Divergence For Variational Inference. (arXiv:1805.01045v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.01045</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a variational approximation framework using direct
optimization of what is known as the {\it scale invariant Alpha-Beta
divergence} (sAB divergence). This new objective encompasses most variational
objectives that use the Kullback-Leibler, the R{\&apos;e}nyi or the gamma
divergences. It also gives access to objective functions never exploited before
in the context of variational inference. This is achieved via two easy to
interpret control parameters, which allow for a smooth interpolation over the
divergence space while trading-off properties such as mass-covering of a target
distribution and robustness to outliers in the data. Furthermore, the sAB
variational objective can be optimized directly by repurposing existing methods
for Monte Carlo computation of complex variational objectives, leading to
estimates of the divergence instead of variational lower bounds. We show the
advantages of this objective on Bayesian models for regression problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Regli_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Regli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Silva_R/0/1/0/all/0/1&quot;&gt;Ricardo Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01049">
<title>Large-Scale Unsupervised Deep Representation Learning for Brain Structure. (arXiv:1805.01049v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01049</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Learning (ML) is increasingly being used for computer aided diagnosis
of brain related disorders based on structural magnetic resonance imaging (MRI)
data. Most of such work employs biologically and medically meaningful
hand-crafted features calculated from different regions of the brain. The
construction of such highly specialized features requires a considerable amount
of time, manual oversight and careful quality control to ensure the absence of
errors in the computational process. Recent advances in Deep Representation
Learning have shown great promise in extracting highly non-linear and
information-rich features from data. In this paper, we present a novel
large-scale deep unsupervised approach to learn generic feature representations
of structural brain MRI scans, which requires no specialized domain knowledge
or manual intervention. Our method produces low-dimensional representations of
brain structure, which can be used to reconstruct brain images with very low
error and exhibit performance comparable to FreeSurfer features on various
classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1&quot;&gt;Ayush Jaiswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Dong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghavendra_C/0/1/0/all/0/1&quot;&gt;Cauligi S. Raghavendra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thompson_P/0/1/0/all/0/1&quot;&gt;Paul Thompson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01078">
<title>Exploration of Numerical Precision in Deep Neural Networks. (arXiv:1805.01078v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.01078</link>
<description rdf:parseType="Literal">&lt;p&gt;Reduced numerical precision is a common technique to reduce computational
cost in many Deep Neural Networks (DNNs). While it has been observed that DNNs
are resilient to small errors and noise, no general result exists that is
capable of predicting a given DNN system architecture&apos;s sensitivity to reduced
precision. In this project, we emulate arbitrary bit-width using a specified
floating-point representation with a truncation method, which is applied to the
neural network after each batch. We explore the impact of several model
parameters on the network&apos;s training accuracy and show results on the MNIST
dataset. We then present a preliminary theoretical investigation of the error
scaling in both forward and backward propagations. We end with a discussion of
the implications of these results as well as the potential for generalization
to other network architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhaoqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vajiac_C/0/1/0/all/0/1&quot;&gt;Catalina Vajiac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunkai Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01136">
<title>Nonparametric Learning and Optimization with Covariates. (arXiv:1805.01136v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01136</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern decision analytics frequently involves the optimization of an
objective over a finite horizon where the functional form of the objective is
unknown. The decision analyst observes covariates and tries to learn and
optimize the objective by experimenting with the decision variables. We present
a nonparametric learning and optimization policy with covariates. The policy is
based on adaptively splitting the covariate space into smaller bins
(hyper-rectangles) and learning the optimal decision in each bin. We show that
the algorithm achieves a regret of order $O(\log(T)^2 T^{(2+d)/(4+d)})$, where
$T$ is the length of the horizon and $d$ is the dimension of the covariates,
and show that no policy can achieve a regret less than $O(T^{(2+d)/(4+d)})$ and
thus demonstrate the near optimality of the proposed policy. The role of $d$ in
the regret is not seen in parametric learning problems: It highlights the
complex interaction between the nonparametric formulation and the covariate
dimension. It also suggests the decision analyst should incorporate contextual
information selectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Ningyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1&quot;&gt;Guillermo Gallego&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01156">
<title>Supervector Compression Strategies to Speed up I-Vector System Development. (arXiv:1805.01156v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1805.01156</link>
<description rdf:parseType="Literal">&lt;p&gt;The front-end factor analysis (FEFA), an extension of principal component
analysis (PPCA) tailored to be used with Gaussian mixture models (GMMs), is
currently the prevalent approach to extract compact utterance-level features
(i-vectors) for automatic speaker verification (ASV) systems. Little research
has been conducted comparing FEFA to the conventional PPCA applied to maximum a
posteriori (MAP) adapted GMM supervectors. We study several alternative
methods, including PPCA, factor analysis (FA), and two supervised approaches,
supervised PPCA (SPPCA) and the recently proposed probabilistic partial least
squares (PPLS), to compress MAP-adapted GMM supervectors. The resulting
i-vectors are used in ASV tasks with a probabilistic linear discriminant
analysis (PLDA) back-end. We experiment on two different datasets, on the
telephone condition of NIST SRE 2010 and on the recent VoxCeleb corpus
collected from YouTube videos containing celebrity interviews recorded in
various acoustical and technical conditions. The results suggest that, in terms
of ASV accuracy, the supervector compression approaches are on a par with FEFA.
The supervised approaches did not result in improved performance. In comparison
to FEFA, we obtained more than hundred-fold (100x) speedups in the total
variability model (TVM) training using the PPCA and FA supervector compression
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vestman_V/0/1/0/all/0/1&quot;&gt;Ville Vestman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1&quot;&gt;Tomi Kinnunen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01174">
<title>Optimization of computational budget for power system risk assessment. (arXiv:1805.01174v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.01174</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of maintaining high voltage power transmission
networks in security at all time, namely anticipating exceeding of thermal
limit for eventual single line disconnection (whatever its cause may be) by
running slow, but accurate, physical grid simulators. New conceptual frameworks
are calling for a probabilistic risk-based security criterion. However, these
approaches suffer from high requirements in terms of tractability. Here, we
propose a new method to assess the risk. This method uses both machine learning
techniques (artificial neural networks) and more standard simulators based on
physical laws. More specifically we train neural networks to estimate the
overall dangerousness of a grid state. A classical benchmark problem (manpower
118 buses test case) is used to show the strengths of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Donnot_B/0/1/0/all/0/1&quot;&gt;Benjamin Donnot&lt;/a&gt; (TAU, LRI), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guyon_I/0/1/0/all/0/1&quot;&gt;Isabelle Guyon&lt;/a&gt; (TAU, LRI, UP11), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marot_A/0/1/0/all/0/1&quot;&gt;Antoine Marot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schoenauer_M/0/1/0/all/0/1&quot;&gt;Marc Schoenauer&lt;/a&gt; (LRI, TAU), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Panciatici_P/0/1/0/all/0/1&quot;&gt;Patrick Panciatici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01209">
<title>Found Graph Data and Planted Vertex Covers. (arXiv:1805.01209v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1805.01209</link>
<description rdf:parseType="Literal">&lt;p&gt;A typical way in which network data is recorded is to measure all the
interactions among a specified set of core nodes; this produces a graph
containing this core together with a potentially larger set of fringe nodes
that have links to the core. Interactions between pairs of nodes in the fringe,
however, are not recorded by this process, and hence not present in the
resulting graph data. For example, a phone service provider may only have
records of calls in which at least one of the participants is a customer; this
can include calls between a customer and a non-customer, but not between pairs
of non-customers.
&lt;/p&gt;
&lt;p&gt;Knowledge of which nodes belong to the core is an important piece of metadata
that is crucial for interpreting the network dataset. But in many cases, this
metadata is not available, either because it has been lost due to difficulties
in data provenance, or because the network consists of found data obtained in
settings such as counter-surveillance. This leads to a natural algorithmic
problem, namely the recovery of the core set. Since the core set forms a vertex
cover of the graph, we essentially have a planted vertex cover problem, but
with an arbitrary underlying graph. We develop a theoretical framework for
analyzing this planted vertex cover problem, based on results in the theory of
fixed-parameter tractability, together with algorithms for recovering the core.
Our algorithms are fast, simple to implement, and out-perform several methods
based on network core-periphery structure on various real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benson_A/0/1/0/all/0/1&quot;&gt;Austin R. Benson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1&quot;&gt;Jon Kleinberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01222">
<title>audEERING&apos;s approach to the One-Minute-Gradual Emotion Challenge. (arXiv:1805.01222v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.01222</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes audEERING&apos;s submissions as well as additional
evaluations for the One-Minute-Gradual (OMG) emotion recognition challenge. We
provide the results for audio and video processing on subject (in)dependent
evaluations. On the provided Development set, we achieved 0.343 Concordance
Correlation Coefficient (CCC) for arousal (from audio) and .401 for valence
(from video).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triantafyllopoulos_A/0/1/0/all/0/1&quot;&gt;Andreas Triantafyllopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagha_H/0/1/0/all/0/1&quot;&gt;Hesam Sagha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eyben_F/0/1/0/all/0/1&quot;&gt;Florian Eyben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Schuller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01237">
<title>An Asymptotically Optimal Strategy for Constrained Multi-armed Bandit Problems. (arXiv:1805.01237v1 [math.OC])</title>
<link>http://arxiv.org/abs/1805.01237</link>
<description rdf:parseType="Literal">&lt;p&gt;For the stochastic multi-armed bandit (MAB) problem from a constrained model
that generalizes the classical one, we show that an asymptotic optimality is
achievable by a simple strategy extended from the $\epsilon_t$-greedy strategy.
We provide a finite-time lower bound on the probability of correct selection of
an optimal near-feasible arm that holds for all time steps. Under some
conditions, the bound approaches one as time $t$ goes to infinity. A particular
example sequence of $\{\epsilon_t\}$ having the asymptotic convergence rate in
the order of $(1-\frac{1}{t})^4$ that holds from a sufficiently large $t$ is
also discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hyeong Soo Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01278">
<title>Learning Pretopological Spaces to Model Complex Propagation Phenomena: A Multiple Instance Learning Approach Based on a Logical Modeling. (arXiv:1805.01278v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01278</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the problem of learning the concept of &quot;propagation&quot; in
the pretopology theoretical formalism. Our proposal is first to define the
pseudo-closure operator (modeling the propagation concept) as a logical
combination of neighborhoods. We show that learning such an operator lapses
into the Multiple Instance (MI) framework, where the learning process is
performed on bags of instances instead of individual instances. Though this
framework is well suited for this task, its use for learning a pretopological
space leads to a set of bags exponential in size. To overcome this issue we
thus propose a learning method based on a low estimation of the bags covered by
a concept under construction. As an experiment, percolation processes (forest
fires typically) are simulated and the corresponding propagation models are
learned based on a subset of observations. It reveals that the proposed MI
approach is significantly more efficient on the task of propagation model
recognition than existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caillaut_G/0/1/0/all/0/1&quot;&gt;Ga&amp;#xeb;tan Caillaut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cleuziou_G/0/1/0/all/0/1&quot;&gt;Guillaume Cleuziou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01288">
<title>Semantic Channel and Shannon&apos;s Channel Mutually Match for Multi-Label Classification. (arXiv:1805.01288v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01288</link>
<description rdf:parseType="Literal">&lt;p&gt;A group of transition probability functions form a Shannon&apos;s channel whereas
a group of truth functions form a semantic channel. Label learning is to let
semantic channels match Shannon&apos;s channels and label selection is to let
Shannon&apos;s channels match semantic channels. The Channel Matching (CM) algorithm
is provided for multi-label classification. This algorithm adheres to maximum
semantic information criterion which is compatible with maximum likelihood
criterion and regularized least squares criterion. If samples are very large,
we can directly convert Shannon&apos;s channels into semantic channels by the third
kind of Bayes&apos; theorem; otherwise, we can train truth functions with parameters
by sampling distributions. A label may be a Boolean function of some atomic
labels. For simplifying learning, we may only obtain the truth functions of
some atomic label. For a given label, instances are divided into three kinds
(positive, negative, and unclear) instead of two kinds as in popular studies so
that the problem with binary relevance is avoided. For each instance, the
classifier selects a compound label with most semantic information or richest
connotation. As a predictive model, the semantic channel does not change with
the prior probability distribution (source) of instances. It still works when
the source is changed. The classifier changes with the source, and hence can
overcome class-imbalance problem. It is shown that the old population&apos;s
increasing will change the classifier for label &quot;Old&quot; and has been impelling
the semantic evolution of &quot;Old&quot;. The CM iteration algorithm for unseen instance
classification is introduced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chenguang Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01360">
<title>Anomaly and Change Detection in Graph Streams through Constant-Curvature Manifold Embeddings. (arXiv:1805.01360v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01360</link>
<description rdf:parseType="Literal">&lt;p&gt;Mapping complex input data into suitable lower dimensional manifolds is a
common procedure in machine learning. This step is beneficial mainly for two
reasons: (1) it reduces the data dimensionality and (2) it provides a new data
representation possibly characterised by convenient geometric properties.
Euclidean spaces are by far the most widely used embedding spaces, thanks to
their well-understood structure and large availability of consolidated
inference methods. However, recent research demonstrated that many types of
complex data (e.g., those represented as graphs) are actually better described
by non-Euclidean geometries. Here, we investigate how embedding graphs on
constant-curvature manifolds (hyper-spherical and hyperbolic manifolds) impacts
on the ability to detect changes in sequences of attributed graphs. The
proposed methodology consists in embedding graphs into a geometric space and
perform change detection there by means of conventional methods for numerical
streams. The curvature of the space is a parameter that we learn to reproduce
the geometry of the original application-dependent graph space. Preliminary
experimental results show the potential capability of representing graphs by
means of curved manifold, in particular for change and anomaly detection
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambon_D/0/1/0/all/0/1&quot;&gt;Daniele Zambon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Livi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Livi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1&quot;&gt;Cesare Alippi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01367">
<title>Open Loop Execution of Tree-Search Algorithms. (arXiv:1805.01367v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.01367</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of tree-search stochastic planning algorithms where a
generative model is available, we consider on-line planning algorithms building
trees in order to recommend an action. We investigate the question of avoiding
re-planning in subsequent decision steps by directly using sub-trees as action
recommender. Firstly, we propose a method for open loop control via a new
algorithm taking the decision of re-planning or not at each time step based on
an analysis of the statistics of the sub-tree. Secondly, we show that the
probability of selecting a suboptimal action at any depth of the tree can be
upper bounded and converges towards zero. Moreover, this upper bound decays in
a logarithmic way between subsequent depths. This leads to a distinction
between node-wise optimality and state-wise optimality. Finally, we empirically
demonstrate that our method achieves a compromise between loss of performance
and computational gain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lecarpentier_E/0/1/0/all/0/1&quot;&gt;Erwan Lecarpentier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Infantes_G/0/1/0/all/0/1&quot;&gt;Guillaume Infantes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lesire_C/0/1/0/all/0/1&quot;&gt;Charles Lesire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rachelson_E/0/1/0/all/0/1&quot;&gt;Emmanuel Rachelson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01207">
<title>The $\ell_\infty$ Perturbation of HOSVD and Low Rank Tensor Denoising. (arXiv:1707.01207v4 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01207</link>
<description rdf:parseType="Literal">&lt;p&gt;The higher order singular value decomposition (HOSVD) of tensors is a
generalization of matrix SVD. The perturbation analysis of HOSVD under random
noise is more delicate than its matrix counterpart. Recent progress has been
made in Richard and Montanari (2014), Zhang and Xia (2017) and Liu et al.
(2017) demonstrating that minimax optimal singular spaces estimation and low
rank tensor recovery in $\ell_2$-norm can be obtained through polynomial time
algorithms. In this paper, we analyze the HOSVD perturbation under Gaussian
noise based on a second order method, which leads to an estimator of singular
vectors with sharp bound in $\ell_\infty$-norm. A low rank tensor denoising
estimator is then proposed which achieves a fast convergence rate
characterizing the entry-wise deviations. The advantages of these
$\ell_\infty$-norm bounds are displayed in applications including high
dimensional clustering and sub-tensor localizations. In addition, the bound
established for HOSVD also elaborates the one-sided spectral perturbation in
$\ell_\infty$-norm for unbalanced (or fat) matrices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xia_D/0/1/0/all/0/1&quot;&gt;Dong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.05929">
<title>Explaining Anomalies in Groups with Characterizing Subspace Rules. (arXiv:1708.05929v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.05929</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection has numerous applications and has been studied vastly. We
consider a complementary problem that has a much sparser literature: anomaly
description. Interpretation of anomalies is crucial for practitioners for
sense-making, troubleshooting, and planning actions. To this end, we present a
new approach called x-PACS (for eXplaining Patterns of Anomalies with
Characterizing Subspaces), which &quot;reverse-engineers&quot; the known anomalies by
identifying (1) the groups (or patterns) that they form, and (2) the
characterizing subspace and feature rules that separate each anomalous pattern
from normal instances. Explaining anomalies in groups not only saves analyst
time and gives insight into various types of anomalies, but also draws
attention to potentially critical, repeating anomalies.
&lt;/p&gt;
&lt;p&gt;In developing x-PACS, we first construct a desiderata for the anomaly
description problem. From a descriptive data mining perspective, our method
exhibits five desired properties in our desiderata. Namely, it can unearth
anomalous patterns (i) of multiple different types, (ii) hidden in arbitrary
subspaces of a high dimensional space, (iii) interpretable by the analysts,
(iv) different from normal patterns of the data, and finally (v) succinct,
providing the shortest data description. Furthermore, x-PACS is highly
parallelizable and scales linearly in terms of data size.
&lt;/p&gt;
&lt;p&gt;No existing work on anomaly description satisfies all of these properties
simultaneously. While not our primary goal, the anomalous patterns we find
serve as interpretable &quot;signatures&quot; and can be used for detection. We show the
effectiveness of x-PACS in explanation as well as detection on real-world
datasets as compared to state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macha_M/0/1/0/all/0/1&quot;&gt;Meghanath Macha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1&quot;&gt;Leman Akoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11586">
<title>Toward Multimodal Image-to-Image Translation. (arXiv:1711.11586v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11586</link>
<description rdf:parseType="Literal">&lt;p&gt;Many image-to-image translation problems are ambiguous, as a single input
image may correspond to multiple possible outputs. In this work, we aim to
model a \emph{distribution} of possible outputs in a conditional generative
modeling setting. The ambiguity of the mapping is distilled in a
low-dimensional latent vector, which can be randomly sampled at test time. A
generator learns to map the given input, combined with this latent code, to the
output. We explicitly encourage the connection between output and the latent
code to be invertible. This helps prevent a many-to-one mapping from the latent
code to the output during training, also known as the problem of mode collapse,
and produces more diverse results. We explore several variants of this approach
by employing different training objectives, network architectures, and methods
of injecting the latent code. Our proposed method encourages bijective
consistency between the latent encoding and output modes. We present a
systematic comparison of our method and other variants on both perceptual
realism and diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Richard Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_O/0/1/0/all/0/1&quot;&gt;Oliver Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1&quot;&gt;Eli Shechtman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08397">
<title>Harnessing Structures in Big Data via Guaranteed Low-Rank Matrix Estimation. (arXiv:1802.08397v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08397</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-rank modeling plays a pivotal role in signal processing and machine
learning, with applications ranging from collaborative filtering, video
surveillance, medical imaging, to dimensionality reduction and adaptive
filtering. Many modern high-dimensional data and interactions thereof can be
modeled as lying approximately in a low-dimensional subspace or manifold,
possibly with additional structures, and its proper exploitations lead to
significant reduction of costs in sensing, computation and storage. In recent
years, there is a plethora of progress in understanding how to exploit low-rank
structures using computationally efficient procedures in a provable manner,
including both convex and nonconvex approaches. On one side, convex relaxations
such as nuclear norm minimization often lead to statistically optimal
procedures for estimating low-rank matrices, where first-order methods are
developed to address the computational challenges; on the other side, there is
emerging evidence that properly designed nonconvex procedures, such as
projected gradient descent, often provide globally optimal solutions with a
much lower computational cost in many problems. This survey article will
provide a unified overview of these recent advances on low-rank matrix
estimation from incomplete measurements. Attention is paid to rigorous
characterization of the performance of these algorithms, and to problems where
the low-rank matrix have additional structural properties that require new
algorithmic designs and theoretical analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yudong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chi_Y/0/1/0/all/0/1&quot;&gt;Yuejie Chi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05753">
<title>RFCDE: Random Forests for Conditional Density Estimation. (arXiv:1804.05753v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05753</link>
<description rdf:parseType="Literal">&lt;p&gt;Random forests is a common non-parametric regression technique which performs
well for mixed-type data and irrelevant covariates, while being robust to
monotonic variable transformations. Existing random forest implementations
target regression or classification. We introduce the RFCDE package for fitting
random forest models optimized for nonparametric conditional density
estimation, including joint densities for multiple responses. This enables
analysis of conditional probability distributions which is useful for
propagating uncertainty and of joint distributions that describe relationships
between multiple responses and covariates. RFCDE is released under the MIT
open-source license and can be accessed at https://github.com/tpospisi/rfcde .
Both R and Python versions, which call a common C++ library, are available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pospisil_T/0/1/0/all/0/1&quot;&gt;Taylor Pospisil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Ann B. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06218">
<title>Hierarchical correlation reconstruction with missing data, for example for biology-inspired neuron. (arXiv:1804.06218v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06218</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning often needs to estimate density from a multidimensional data
sample, including modelling correlations between coordinates. Additionally, we
often have missing data case: that data points contain only partial information
- can miss information about some of coordinates. This article adapts rapid
parametric density estimation technique for this purpose: modelling density as
a linear combination of orthonormal functions, for which $L^2$ optimization
says that (independently) estimated coefficient for a given function is just
average over the sample of value of this function. Hierarchical correlation
reconstruction first models probability density for each separate coordinate
using all its appearances in data sample, then adds corrections from
independently modelled pairwise correlations using all samples having both
coordinates, and so on independently adding correlations for growing numbers of
variables using decreasing evidence in our data sample. A basic application of
such modelled multidimensional density can be imputation of missing
coordinates: by inserting known coordinates to the density, and taking expected
values for the missing coordinates, and maybe also variance to estimate their
uncertainty. Biological neurons are seen as able to model and predict signals -
the simplicity and flexibility of the presented approach makes it perfect for
such biology-inspired artificial neuron.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duda_J/0/1/0/all/0/1&quot;&gt;Jarek Duda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08233">
<title>N-fold Superposition: Improving Neural Networks by Reducing the Noise in Feature Maps. (arXiv:1804.08233v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08233</link>
<description rdf:parseType="Literal">&lt;p&gt;Considering the use of Fully Connected (FC) layer limits the performance of
Convolutional Neural Networks (CNNs), this paper develops a method to improve
the coupling between the convolution layer and the FC layer by reducing the
noise in Feature Maps (FMs). Our approach is divided into three steps. Firstly,
we separate all the FMs into n blocks equally. Then, the weighted summation of
FMs at the same position in all blocks constitutes a new block of FMs. Finally,
we replicate this new block into n copies and concatenate them as the input to
the FC layer. This sharing of FMs could reduce the noise in them apparently and
avert the impact by a particular FM on the specific part weight of hidden
layers, hence preventing the network from overfitting to some extent. Using the
Fermat Lemma, we prove that this method could make the global minima value
range of the loss function wider, by which makes it easier for neural networks
to converge and accelerates the convergence process. This method does not
significantly increase the amounts of network parameters (only a few more
coefficients added), and the experiments demonstrate that this method could
increase the convergence speed and improve the classification performance of
neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Q/0/1/0/all/0/1&quot;&gt;Qiang Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chao Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00521">
<title>Direct Runge-Kutta Discretization Achieves Acceleration. (arXiv:1805.00521v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00521</link>
<description rdf:parseType="Literal">&lt;p&gt;We study gradient-based optimization methods obtained by directly
discretizing a second-order ordinary differential equation (ODE) related to the
continuous limit of Nesterov&apos;s accelerated gradient method. When the function
is smooth enough, we show that acceleration can be achieved by a stable
discretization of the ODE using standard Runge-Kutta integrators. Specifically,
we prove that under Lipschitz-gradient, convexity and order-$(s+2)$
differentiability assumptions, the sequence of iterates generated by
discretizing the proposed second-order ODE converges to the optimal solution at
a rate of $\mathcal{O}({N^{-2\frac{s}{s+1}}})$, where $s$ is the order of the
Runge-Kutta numerical integrator. By increasing $s$, the convergence rate of
our method approaches the optimal rate of $\mathcal{O}({N^{-2}})$. Furthermore,
we introduce a new local flatness condition on the objective, according to
which rates even faster than $\mathcal{O}(N^{-2})$ can be achieved with
low-order integrators and only gradient information. Notably, this flatness
condition is satisfied by several standard loss functions used in machine
learning, and it may be of broader independent interest. We provide numerical
experiments that verify the theoretical rates predicted by our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingzhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mokhtari_A/0/1/0/all/0/1&quot;&gt;Aryan Mokhtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sra_S/0/1/0/all/0/1&quot;&gt;Suvrit Sra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jadbabaie_A/0/1/0/all/0/1&quot;&gt;Ali Jadbabaie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00868">
<title>A Dynamic Model for Traffic Flow Prediction Using Improved DRN. (arXiv:1805.00868v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00868</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time traffic flow prediction can not only provide travelers with
reliable traffic information and thus save time, but also assist traffic
management department to manage transportation system. It can greatly improve
the efficiency of transportation. Traditional traffic flow prediction methods
usually need a huge amount of data but still leaves a poor performance. With
the development of deep learning, researchers begin to pay attention to
artificial neural networks (ANNs) such as RNN and LSTM. However, these ANNs are
very time-consuming. In our article, we improve the Deep Residual Network and
build a dynamic model which previous researchers hardly use. Our result shows
that our model can not only be trained efficiently but also have a higher
accuracy. Additionally, our dynamic model is more suitable for practical
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zeren Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruimin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00917">
<title>A Simple Discrete-Time Survival Model for Neural Networks. (arXiv:1805.00917v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00917</link>
<description rdf:parseType="Literal">&lt;p&gt;There is currently great interest in applying neural networks to prediction
tasks in medicine. It is important for predictive models to be able to use
survival data, where each patient has a known follow-up time and
event/censoring indicator. This avoids information loss when training the model
and enables generation of predicted survival curves. In this paper, we describe
a discrete-time survival model that is designed to be used with neural
networks. The model is trained with the maximum likelihood method using
minibatch stochastic gradient descent (SGD). The use of SGD enables rapid
training speed. The model is flexible, so that the baseline hazard rate and the
effect of the input data can vary with follow-up time. It has been implemented
in the Keras deep learning framework, and source code for the model and several
examples is available online. We demonstrated the high performance of the model
by using it as part of a convolutional neural network to predict survival for
over 10,000 patients with metastatic cancer, using the full text of 1,137,317
provider notes. The model&apos;s C-index on the validation set was 0.71, which was
superior to a linear baseline model (C-index 0.69).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gensheimer_M/0/1/0/all/0/1&quot;&gt;Michael F. Gensheimer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Narasimhan_B/0/1/0/all/0/1&quot;&gt;Balasubramanian Narasimhan&lt;/a&gt;</dc:creator>
</item></rdf:RDF>