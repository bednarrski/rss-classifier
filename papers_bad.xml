<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-22T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06733"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06620"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06635"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06740"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06805"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06900"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07150"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07161"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07226"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07229"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.00782"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.00764"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03168"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06637"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06720"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06727"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06797"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06816"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06818"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06845"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06879"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06934"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07047"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07172"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07194"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07222"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1502.01068"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.03662"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.09296"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.07959"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.04081"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.04289"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.10378"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.02744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.06315"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.07287"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.07888"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.08862"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.09180"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10127"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00961"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04146"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08914"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.06733">
<title>Probabilistic Tools for the Analysis of Randomized Optimization Heuristics. (arXiv:1801.06733v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1801.06733</link>
<description rdf:parseType="Literal">&lt;p&gt;This chapter collects several probabilistic tools that proved to be useful in
the analysis of randomized search heuristics. This includes classic material
like Markov, Chebyshev and Chernoff inequalities, but also lesser known topics
like stochastic domination and coupling or Chernoff bounds for geometrically
distributed random variables and for negatively correlated random variables.
Almost all of the results presented here have appeared previously, some,
however, only in recent conference publications. While the focus is on
collecting tools for the analysis of randomized search heuristics, many of
these may be useful as well in the analysis of classic randomized algorithms or
discrete random structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06620">
<title>A high-performance analog Max-SAT solver and its application to Ramsey numbers. (arXiv:1801.06620v1 [cs.CC])</title>
<link>http://arxiv.org/abs/1801.06620</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a continuous-time analog solver for MaxSAT, a quintessential
class of NP-hard discrete optimization problems, where the task is to find a
truth assignment for a set of Boolean variables satisfying the maximum number
of given logical constraints. We show that the scaling of an invariant of the
solver&apos;s dynamics, the escape rate, as function of the number of unsatisfied
clauses can predict the global optimum value, often well before reaching the
corresponding state. We demonstrate the performance of the solver on hard
MaxSAT competition problems. We then consider the two-color Ramsey number
$R(m,m)$ problem, translate it to SAT, and apply our algorithm to the still
unknown $R(5,5)$. We find edge colorings without monochromatic 5-cliques for
complete graphs up to 42 vertices, while on 43 vertices we find colorings with
only two monochromatic 5-cliques, the best coloring found so far, supporting
the conjecture that $R(5,5) = 43$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molnar_B/0/1/0/all/0/1&quot;&gt;Botond Moln&amp;#xe1;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varga_M/0/1/0/all/0/1&quot;&gt;Melinda Varga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toroczkai_Z/0/1/0/all/0/1&quot;&gt;Zoltan Toroczkai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ercsey_Ravasz_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;ria Ercsey-Ravasz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06635">
<title>Visualization of Hyperspectral Images Using Moving Least Squares. (arXiv:1801.06635v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.06635</link>
<description rdf:parseType="Literal">&lt;p&gt;Displaying the large number of bands in a hyper spectral image on a
trichromatic monitor has been an active research topic. The visualized image
shall convey as much information as possible form the original data and
facilitate image interpretation. Most existing methods display HSIs in false
colors which contradict with human&apos;s experience and expectation. In this paper,
we propose a nonlinear approach to visualize an input HSI with natural colors
by taking advantage of a corresponding RGB image. Our approach is based on
Moving Least Squares, an interpolation scheme for reconstructing a surface from
a set of control points, which in our case is a set of matching pixels between
the HSI and the corresponding RGB image. Based on MLS, the proposed method
solves for each spectral signature a unique transformation so that the non
linear structure of the HSI can be preserved. The matching pixels between a
pair of HSI and RGB image can be reused to display other HSIs captured b the
same imaging sensor with natural colors. Experiments show that the output image
of the proposed method no only have natural colors but also maintain the visual
information necessary for human analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1&quot;&gt;Danping Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yuntao Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06740">
<title>Knowledge Representation for High-Level Norms and Violation Inference in Logic Programming. (arXiv:1801.06740v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1801.06740</link>
<description rdf:parseType="Literal">&lt;p&gt;Most of the knowledge Representation formalisms developed for representing
prescriptive norms can be categorized as either suitable for representing
either low level or high level norms.We argue that low level norm
representations do not advance the cause of autonomy in agents in the sense
that it is not the agent itself that determines the normative position it
should be at a particular time, on the account of a more general rule. In other
words an agent on some external system for a nitty gritty prescriptions of its
obligations and prohibitions. On the other hand, high level norms which have an
explicit description of a norm&apos;s precondition and have some form of
implication, do not as they exist in the literature do not support generalized
inferences about violation like low level norm representations do. This paper
presents a logical formalism for the representation of high level norms in open
societies that enable violation inferences that detail the situation in which
the norm violation took place and the identity of the norm violation. Norms are
formalized as logic programs whose heads specify what an agent is obliged or
permitted to do when a situation arises and within what time constraint of the
situation.Each norm is also assigned an identity using some reification scheme.
The body of each logic program describes the nature of the situation in which
the agent is expected to act or desist from acting. This kind of violation is
novel in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akinkunmi_B/0/1/0/all/0/1&quot;&gt;Babatunde Opeoluwa Akinkunmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babalola_M/0/1/0/all/0/1&quot;&gt;Moyin Florence Babalola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06805">
<title>Decoupled Learning for Factorial Marked Temporal Point Processes. (arXiv:1801.06805v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.06805</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces the factorial marked temporal point process model and
presents efficient learning methods. In conventional (multi-dimensional) marked
temporal point process models, event is often encoded by a single discrete
variable i.e. a marker. In this paper, we describe the factorial marked point
processes whereby time-stamped event is factored into multiple markers.
Accordingly the size of the infectivity matrix modeling the effect between
pairwise markers is in power order w.r.t. the number of the discrete marker
space. We propose a decoupled learning method with two learning procedures: i)
directly solving the model based on two techniques: Alternating Direction
Method of Multipliers and Fast Iterative Shrinkage-Thresholding Algorithm; ii)
involving a reformulation that transforms the original problem into a Logistic
Regression model for more efficient learning. Moreover, a sparse group
regularizer is added to identify the key profile features and event labels.
Empirical results on real world datasets demonstrate the efficiency of our
decoupled and reformulated method. The source code is available online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Weichang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06900">
<title>Efficient Learning of Optimal Markov Network Topology with k-Tree Modeling. (arXiv:1801.06900v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1801.06900</link>
<description rdf:parseType="Literal">&lt;p&gt;The seminal work of Chow and Liu (1968) shows that approximation of a finite
probabilistic system by Markov trees can achieve the minimum information loss
with the topology of a maximum spanning tree. Our current paper generalizes the
result to Markov networks of tree width $\leq k$, for every fixed $k\geq 2$. In
particular, we prove that approximation of a finite probabilistic system with
such Markov networks has the minimum information loss when the network topology
is achieved with a maximum spanning $k$-tree. While constructing a maximum
spanning $k$-tree is intractable for even $k=2$, we show that polynomial
algorithms can be ensured by a sufficient condition accommodated by many
meaningful applications. In particular, we prove an efficient algorithm for
learning the optimal topology of higher order correlations among random
variables that belong to an underlying linear structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Liang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1&quot;&gt;Di Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malmberg_R/0/1/0/all/0/1&quot;&gt;Russell Malmberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1&quot;&gt;Aaron Martinez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_D/0/1/0/all/0/1&quot;&gt;David Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1&quot;&gt;Matthew Wicker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hongfei Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1&quot;&gt;Liming Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07150">
<title>A Novel Weighted Distance Measure for Multi-Attributed Graph. (arXiv:1801.07150v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1801.07150</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to exponential growth of complex data, graph structure has become
increasingly important to model various entities and their interactions, with
many interesting applications including, bioinformatics, social network
analysis, etc. Depending on the complexity of the data, the underlying graph
model can be a simple directed/undirected and/or weighted/un-weighted graph to
a complex graph (aka multi-attributed graph) where vertices and edges are
labelled with multi-dimensional vectors. In this paper, we present a novel
weighted distance measure based on weighted Euclidean norm which is defined as
a function of both vertex and edge attributes, and it can be used for various
graph analysis tasks including classification and cluster analysis. The
proposed distance measure has flexibility to increase/decrease the weightage of
edge labels while calculating the distance between vertex-pairs. We have also
proposed a MAGDist algorithm, which reads multi-attributed graph stored in CSV
files containing the list of vertex vectors and edge vectors, and calculates
the distance between each vertex-pair using the proposed weighted distance
measure. Finally, we have proposed a multi-attributed similarity graph
generation algorithm, MAGSim, which reads the output of MAGDist algorithm and
generates a similarity graph that can be analysed using classification and
clustering algorithms. The significance and accuracy of the proposed distance
measure and algorithms is evaluated on Iris and Twitter data sets, and it is
found that the similarity graph generated by our proposed method yields better
clustering results than the existing similarity graph generation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abulaish_M/0/1/0/all/0/1&quot;&gt;Muhammad Abulaish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jahiruddin/0/1/0/all/0/1&quot;&gt;Jahiruddin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07161">
<title>Reasoning about multiple aspects in DLs: Semantics and Closure Construction. (arXiv:1801.07161v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.07161</link>
<description rdf:parseType="Literal">&lt;p&gt;Starting from the observation that rational closure has the undesirable
property of being an &quot;all or nothing&quot; mechanism, we here propose a
multipreferential semantics, which enriches the preferential semantics
underlying rational closure in order to separately deal with the inheritance of
different properties in an ontology with exceptions. We provide a
multipreference closure mechanism which is sound with respect to the
multipreference semantics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giordano_L/0/1/0/all/0/1&quot;&gt;Laura Giordano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gliozzi_V/0/1/0/all/0/1&quot;&gt;Valentina Gliozzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07226">
<title>Optimal Convergence for Distributed Learning with Stochastic Gradient Methods and Spectral-Regularization Algorithms. (arXiv:1801.07226v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.07226</link>
<description rdf:parseType="Literal">&lt;p&gt;We study generalization properties of distributed algorithms in the setting
of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We
first investigate distributed stochastic gradient methods (SGM), with
mini-batches and multi-passes over the data. We show that optimal
generalization error bounds can be retained for distributed SGM provided that
the partition level is not too large. We then extend our results to
spectral-regularization algorithms (SRA), including kernel ridge regression
(KRR), kernel principal component analysis, and gradient methods. Our results
are superior to the state-of-the-art theory. Particularly, our results show
that distributed SGM has a smaller theoretical computational complexity,
compared with distributed KRR and classic SGM. Moreover, even for
non-distributed SRA, they provide the first optimal, capacity-dependent
convergence rates, considering the case that the regression function may not be
in the RKHS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junhong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07229">
<title>Combinatorial framework for planning in geological exploration. (arXiv:1801.07229v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.07229</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper describes combinatorial framework for planning of geological
exploration for oil-gas fields. The suggested scheme of the geological
exploration involves the following stages: (1) building of special 4-layer
tree-like model (layer of geological exploration): productive layer, group of
productive layers, oil-gas field, oil-gas region (or group of the fields); (2)
generations of local design (exploration) alternatives for each low-layer
geological objects: conservation, additional search, independent utilization,
joint utilization; (3) multicriteria (i.e., multi-attribute) assessment of the
design (exploration) alternatives and their interrelation (compatibility) and
mapping if the obtained vector estimates into integrated ordinal scale; (4)
hierarchical design (&apos;bottom-up&apos;) of composite exploration plans for each
oil-gas field; (5) integration of the plans into region plans and (6)
aggregation of the region plans into a general exploration plan. Stages 2, 3,
4, and 5 are based on hierarchical multicriteria morphological design (HMMD)
method (assessment of ranking of alternatives, selection and composition of
alternatives into composite alternatives). The composition problem is based on
morphological clique model. Aggregation of the obtained modular alternatives
(stage 6) is based on detection of a alternatives &apos;kernel&apos; and its extension by
addition of elements (multiple choice model). In addition, the usage of
multiset estimates for alternatives is described as well. The alternative
estimates are based on expert judgment. The suggested combinatorial planning
methodology is illustrated by numerical examples for geological exploration of
Yamal peninsula.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levin_M/0/1/0/all/0/1&quot;&gt;Mark Sh. Levin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.00782">
<title>Network Structure Inference, A Survey: Motivations, Methods, and Applications. (arXiv:1610.00782v4 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1610.00782</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks represent relationships between entities in many complex systems,
spanning from online social interactions to biological cell development and
brain connectivity. In many cases, relationships between entities are
unambiguously known: are two users &apos;friends&apos; in a social network? Do two
researchers collaborate on a published paper? Do two road segments in a
transportation system intersect? These are directly observable in the system in
question. In most cases, relationship between nodes are not directly observable
and must be inferred: does one gene regulate the expression of another? Do two
animals who physically co-locate have a social bond? Who infected whom in a
disease outbreak in a population?
&lt;/p&gt;
&lt;p&gt;Existing approaches for inferring networks from data are found across many
application domains and use specialized knowledge to infer and measure the
quality of inferred network for a specific task or hypothesis. However, current
research lacks a rigorous methodology which employs standard statistical
validation on inferred models. In this survey, we examine (1) how network
representations are constructed from underlying data, (2) the variety of
questions and tasks on these representations over several domains, and (3)
validation strategies for measuring the inferred network&apos;s capability of
answering questions on the system of interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brugere_I/0/1/0/all/0/1&quot;&gt;Ivan Brugere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallagher_B/0/1/0/all/0/1&quot;&gt;Brian Gallagher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1&quot;&gt;Tanya Y. Berger-Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.00764">
<title>Hyperparameter Optimization: A Spectral Approach. (arXiv:1706.00764v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.00764</link>
<description rdf:parseType="Literal">&lt;p&gt;We give a simple, fast algorithm for hyperparameter optimization inspired by
techniques from the analysis of Boolean functions. We focus on the
high-dimensional regime where the canonical example is training a neural
network with a large number of hyperparameters. The algorithm --- an iterative
application of compressed sensing techniques for orthogonal polynomials ---
requires only uniform sampling of the hyperparameters and is thus easily
parallelizable.
&lt;/p&gt;
&lt;p&gt;Experiments for training deep neural networks on Cifar-10 show that compared
to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds
significantly improved solutions, in some cases better than what is attainable
by hand-tuning. In terms of overall running time (i.e., time required to sample
various settings of hyperparameters plus additional computation time), we are
at least an order of magnitude faster than Hyperband and Bayesian Optimization.
We also outperform Random Search 8x.
&lt;/p&gt;
&lt;p&gt;Additionally, our method comes with provable guarantees and yields the first
improvements on the sample complexity of learning decision trees in over two
decades. In particular, we obtain the first quasi-polynomial time algorithm for
learning noisy decision trees with polynomial sample complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazan_E/0/1/0/all/0/1&quot;&gt;Elad Hazan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klivans_A/0/1/0/all/0/1&quot;&gt;Adam Klivans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yang Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03168">
<title>Greenhouse: A Zero-Positive Machine Learning System for Time-Series Anomaly Detection. (arXiv:1801.03168v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03168</link>
<description rdf:parseType="Literal">&lt;p&gt;This short paper describes our ongoing research on Greenhouse - a
zero-positive machine learning system for time-series anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1&quot;&gt;Tae Jun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottschlich_J/0/1/0/all/0/1&quot;&gt;Justin Gottschlich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tatbul_N/0/1/0/all/0/1&quot;&gt;Nesime Tatbul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metcalf_E/0/1/0/all/0/1&quot;&gt;Eric Metcalf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zdonik_S/0/1/0/all/0/1&quot;&gt;Stan Zdonik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06637">
<title>Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations. (arXiv:1801.06637v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.06637</link>
<description rdf:parseType="Literal">&lt;p&gt;A long-standing problem at the interface of artificial intelligence and
applied mathematics is to devise an algorithm capable of achieving human level
or even superhuman proficiency in transforming observed data into predictive
mathematical models of the physical world. In the current era of abundance of
data and advanced machine learning capabilities, the natural question arises:
How can we automatically uncover the underlying laws of physics from
high-dimensional data generated from experiments? In this work, we put forth a
deep learning approach for discovering nonlinear partial differential equations
from scattered and potentially noisy observations in space and time.
Specifically, we approximate the unknown solution as well as the nonlinear
dynamics by two deep neural networks. The first network acts as a prior on the
unknown solution and essentially enables us to avoid numerical differentiations
which are inherently ill-conditioned and unstable. The second network
represents the nonlinear dynamics and helps us distill the mechanisms that
govern the evolution of a given spatiotemporal data-set. We test the
effectiveness of our approach for several benchmark problems spanning a number
of scientific domains and demonstrate how the proposed framework can help us
accurately learn the underlying dynamics and forecast future states of the
system. In particular, we study the Burgers&apos;, Korteweg-de Vries (KdV),
Kuramoto-Sivashinsky, nonlinear Schr\&quot;{o}dinger, and Navier-Stokes equations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raissi_M/0/1/0/all/0/1&quot;&gt;Maziar Raissi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06720">
<title>Optimal Rates for Spectral-regularized Algorithms with Least-Squares Regression over Hilbert Spaces. (arXiv:1801.06720v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.06720</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study regression problems over a separable Hilbert space
with the square loss, covering non-parametric regression over a reproducing
kernel Hilbert space. We investigate a class of spectral-regularized
algorithms, including ridge regression, principal component analysis, and
gradient methods. We prove optimal, high-probability convergence results in
terms of variants of norms for the studied algorithms, considering a capacity
assumption on the hypothesis space and a general source condition on the target
function. Consequently, we obtain almost sure convergence results with optimal
rates. Our results improve and generalize previous results, filling a
theoretical gap for the non-attainable cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junhong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06727">
<title>A Second Order Cumulant Spectrum Based Test for Strict Stationarity. (arXiv:1801.06727v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/1801.06727</link>
<description rdf:parseType="Literal">&lt;p&gt;This article develops a statistical test for the null hypothesis of strict
stationarity of a discrete time stochastic process. When the null hypothesis is
true, the second order cumulant spectrum is zero at all the discrete Fourier
frequency pairs present in the principal domain of the cumulant spectrum. The
test uses a frame (window) averaged sample estimate of the second order
cumulant spectrum to build a test statistic that has an asymptotic complex
standard normal distribution. We derive the test statistic, study the size and
power properties of the test, and demonstrate its implementation with intraday
stock market return data. The test has conservative size properties and good
power to detect varying variance and unit root in the presence of varying
variance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Patterson_D/0/1/0/all/0/1&quot;&gt;Douglas Patterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Hinich_M/0/1/0/all/0/1&quot;&gt;Melvin Hinich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Roberts_D/0/1/0/all/0/1&quot;&gt;Denisa Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06797">
<title>Depth CNNs for RGB-D scene recognition: learning from scratch better than transferring from RGB-CNNs. (arXiv:1801.06797v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.06797</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene recognition with RGB images has been extensively studied and has
reached very remarkable recognition levels, thanks to convolutional neural
networks (CNN) and large scene datasets. In contrast, current RGB-D scene data
is much more limited, so often leverages RGB large datasets, by transferring
pretrained RGB CNN models and fine-tuning with the target RGB-D dataset.
However, we show that this approach has the limitation of hardly reaching
bottom layers, which is key to learn modality-specific features. In contrast,
we focus on the bottom layers, and propose an alternative strategy to learn
depth features combining local weakly supervised training from patches followed
by global fine tuning with images. This strategy is capable of learning very
discriminative depth-specific features with limited depth images, without
resorting to Places-CNN. In addition we propose a modified CNN architecture to
further match the complexity of the model and the amount of data available. For
RGB-D scene recognition, depth and RGB features are combined by projecting them
in a common space and further leaning a multilayer classifier, which is jointly
optimized in an end-to-end network. Our framework achieves state-of-the-art
accuracy on NYU2 and SUN RGB-D in both depth only and combined RGB-D data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xinhang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1&quot;&gt;Luis Herranz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shuqiang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06816">
<title>Preferential Attachment Graphs with Planted Communities. (arXiv:1801.06816v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.06816</link>
<description rdf:parseType="Literal">&lt;p&gt;A variation of the preferential attachment random graph model of Barab\&apos;{a}si
and Albert is defined that incorporates planted communities. The graph is built
progressively, with new vertices attaching to the existing ones one-by-one. At
every step, the incoming vertex is randomly assigned a label, which represents
a community it belongs to. This vertex then chooses certain vertices as its
neighbors, with the choice of each vertex being proportional to the degree of
the vertex multiplied by an affinity depending on the labels of the new vertex
and a potential neighbor. It is shown that the fraction of half-edges attached
to vertices with a given label converges almost surely for some classes of
affinity matrices. In addition, the empirical degree distribution for the set
of vertices with a given label converges to a heavy tailed distribution, such
that the tail decay parameter can be different for different communities. Our
proof method may be of independent interest, both for the classical
Barab\&apos;{a}si -Albert model and for other possible extensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hajek_B/0/1/0/all/0/1&quot;&gt;Bruce Hajek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sankagiri_S/0/1/0/all/0/1&quot;&gt;Suryanarayana Sankagiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06818">
<title>Recovering a Hidden Community in a Preferential Attachment Graph. (arXiv:1801.06818v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.06818</link>
<description rdf:parseType="Literal">&lt;p&gt;A message passing algorithm (MP) is derived for recovering a dense subgraph
within a graph generated by a variation of the Barab\&apos;{a}si-Albert preferential
attachment model. The estimator is assumed to know the arrival times, or order
of attachment, of the vertices. The derivation of the algorithm is based on
belief propagation under an independence assumption. Two precursors to the
message passing algorithm are analyzed: the first is a degree thresholding (DT)
algorithm and the second is an algorithm based on the arrival times of the
children (C) of a given vertex, where the children of a given vertex are the
vertices that attached to it. C significantly outperforms DT, showing it is
beneficial to know the arrival times of the children, beyond simply knowing the
number of them. It is shown that for a fixed fraction of vertices in the
community $\rho$, fixed number of new edges per arriving vertex $m$, and fixed
affinity between vertices in the community $\beta$, the fraction of label
errors for either of the algorithms DT or C, or converges as $T\to\infty.$
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hajek_B/0/1/0/all/0/1&quot;&gt;Bruce Hajek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sankagiri_S/0/1/0/all/0/1&quot;&gt;Suryanarayana Sankagiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06845">
<title>Time series kernel similarities for predicting Paroxysmal Atrial Fibrillation from ECGs. (arXiv:1801.06845v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.06845</link>
<description rdf:parseType="Literal">&lt;p&gt;We tackle the problem of classifying Electrocardiography (ECG) signals with
the aim of predicting the onset of Paroxysmal Atrial Fibrillation (PAF). Atrial
fibrillation is the most common type of arrhythmia, but in many cases PAF
episodes are asymptomatic. Therefore, in order to help diagnosing PAF, it is
important to be design a suitable procedure for detecting and, more
importantly, predicting PAF episodes. We propose a method for predicting PAF
events whose first step consists of a feature extraction procedure that
represents each ECG as a multi-variate time series. Successively, we design a
classification framework based on kernel similarities for multi-variate time
series, capable of handling missing data. We consider different approaches to
perform classification in the original space of the multi-variate time series
and in an embedding space, defined by the kernel similarity measure. Our
classification results show state-of-the-art performance in terms of accuracy.
Furthermore, we demonstrate the ability to predict, with high accuracy, the PAF
onset up to 15 minutes in advance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1&quot;&gt;Filippo Maria Bianchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Livi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Livi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrante_A/0/1/0/all/0/1&quot;&gt;Alberto Ferrante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milosevic_J/0/1/0/all/0/1&quot;&gt;Jelena Milosevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malek_M/0/1/0/all/0/1&quot;&gt;Miroslaw Malek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06879">
<title>Bayesian Deep Convolutional Encoder-Decoder Networks for Surrogate Modeling and Uncertainty Quantification. (arXiv:1801.06879v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/1801.06879</link>
<description rdf:parseType="Literal">&lt;p&gt;We are interested in the development of surrogate models for uncertainty
quantification and propagation in problems governed by stochastic PDEs using a
deep convolutional encoder-decoder network in a similar fashion to approaches
considered in deep learning for image-to-image regression tasks. Since normal
neural networks are data intensive and cannot provide predictive uncertainty,
we propose a Bayesian approach to convolutional neural nets. A recently
introduced variational gradient descent algorithm based on Stein&apos;s method is
scaled to deep convolutional networks to perform approximate Bayesian inference
on millions of uncertain network parameters. This approach achieves state of
the art performance in terms of predictive accuracy and uncertainty
quantification in comparison to other approaches in Bayesian neural networks as
well as techniques that include Gaussian processes and ensemble methods even
when the training data size is relatively small. To evaluate the performance of
this approach, we consider standard uncertainty quantification benchmark
problems including flow in heterogeneous media defined in terms of limited
data-driven permeability realizations. The performance of the surrogate model
developed is very good even though there is no underlying structure shared
between the input (permeability) and output (flow/pressure) fields as is often
the case in the image-to-image regression models used in computer vision
problems. Studies are performed with an underlying stochastic input
dimensionality up to $4,225$ where most other uncertainty quantification
methods fail. Uncertainty propagation tasks are considered and the predictive
output Bayesian statistics are compared to those obtained with Monte Carlo
estimates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinhao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zabaras_N/0/1/0/all/0/1&quot;&gt;Nicholas Zabaras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06934">
<title>On the Iteration Complexity Analysis of Stochastic Primal-Dual Hybrid Gradient Approach with High Probability. (arXiv:1801.06934v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.06934</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a stochastic Primal-Dual Hybrid Gradient (PDHG)
approach for solving a wide spectrum of regularized stochastic minimization
problems, where the regularization term is composite with a linear function. It
has been recognized that solving this kind of problem is challenging since the
closed-form solution of the proximal mapping associated with the regularization
term is not available due to the imposed linear composition, and the
per-iteration cost of computing the full gradient of the expected objective
function is extremely high when the number of input data samples is
considerably large.
&lt;/p&gt;
&lt;p&gt;Our new approach overcomes these issues by exploring the special structure of
the regularization term and sampling a few data points at each iteration.
Rather than analyzing the convergence in expectation, we provide the detailed
iteration complexity analysis for the cases of both uniformly and non-uniformly
averaged iterates with high probability. This strongly supports the good
practical performance of the proposed approach. Numerical experiments
demonstrate that the efficiency of stochastic PDHG, which outperforms other
competing algorithms, as expected by the high-probability convergence analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Linbo Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianyi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Q/0/1/0/all/0/1&quot;&gt;Qi Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xicheng Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07047">
<title>News-based forecasts of macroeconomic indicators: A semantic path model for interpretable predictions. (arXiv:1801.07047v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.07047</link>
<description rdf:parseType="Literal">&lt;p&gt;The macroeconomic climate influences operations with regard to, e.g., raw
material prices, financing, supply chain utilization and demand quotas. In
order to adapt to the economic environment, decision-makers across the public
and private sectors require accurate forecasts of the economic outlook.
Existing predictive frameworks base their forecasts primarily on time series
analysis, as well as the judgments of experts. As a consequence, current
approaches are often biased and prone to error. In order to reduce forecast
errors, this paper presents an innovative methodology that extends lag
variables with unstructured data in the form of financial news: (1) we apply a
variety of models from machine learning to word counts as a high-dimensional
input. However, this approach suffers from low interpretability and
overfitting, motivating the following remedies. (2) We follow the intuition
that the economic climate is driven by general sentiments and suggest a
projection of words onto latent semantic structures as a means of feature
engineering. (3) We propose a semantic path model, together with estimation
technique based on regularization, in order to yield full interpretability of
the forecasts. We demonstrate the predictive performance of our approach by
utilizing 80,813 ad hoc announcements in order to make long-term forecasts of
up to 24 months ahead regarding key macroeconomic indicators. Back-testing
reveals a considerable reduction in forecast errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feuerriegel_S/0/1/0/all/0/1&quot;&gt;Stefan Feuerriegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gordon_J/0/1/0/all/0/1&quot;&gt;Julius Gordon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07172">
<title>Scale-invariant Feature Extraction of Neural Network and Renormalization Group Flow. (arXiv:1801.07172v1 [hep-th])</title>
<link>http://arxiv.org/abs/1801.07172</link>
<description rdf:parseType="Literal">&lt;p&gt;Theoretical understanding of how deep neural network (DNN) extracts features
from input images is still unclear, but it is widely believed that the
extraction is performed hierarchically through a process of coarse-graining. It
reminds us of the basic concept of renormalization group (RG) in statistical
physics. In order to explore possible relations between DNN and RG, we use the
Restricted Boltzmann machine (RBM) applied to Ising model and construct a flow
of model parameters (in particular, temperature) generated by the RBM. We show
that the unsupervised RBM trained by spin configurations at various
temperatures from $T=0$ to $T=6$ generates a flow along which the temperature
approaches the critical value $T_c=2.27$. This behavior is opposite to the
typical RG flow of the Ising model. By analyzing various properties of the
weight matrices of the trained RBM, we discuss why it flows towards $T_c$ and
how the RBM learns to extract features of spin configurations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Iso_S/0/1/0/all/0/1&quot;&gt;Satoshi Iso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Shiba_S/0/1/0/all/0/1&quot;&gt;Shotaro Shiba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Yokoo_S/0/1/0/all/0/1&quot;&gt;Sumito Yokoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07194">
<title>Optimizing Prediction Intervals by Tuning Random Forest via Meta-Validation. (arXiv:1801.07194v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.07194</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have shown that tuning prediction models increases prediction
accuracy and that Random Forest can be used to construct prediction intervals.
However, to our best knowledge, no study has investigated the need to, and the
manner in which one can, tune Random Forest for optimizing prediction intervals
{ this paper aims to fill this gap. We explore a tuning approach that combines
an effectively exhaustive search with a validation technique on a single Random
Forest parameter. This paper investigates which, out of eight validation
techniques, are beneficial for tuning, i.e., which automatically choose a
Random Forest configuration constructing prediction intervals that are reliable
and with a smaller width than the default configuration. Additionally, we
present and validate three meta-validation techniques to determine which are
beneficial, i.e., those which automatically chose a beneficial validation
technique. This study uses data from our industrial partner (Keymind Inc.) and
the Tukutuku Research Project, related to post-release defect prediction and
Web application effort estimation, respectively. Results from our study
indicate that: i) the default configuration is frequently unreliable, ii) most
of the validation techniques, including previously successfully adopted ones
such as 50/50 holdout and bootstrap, are counterproductive in most of the
cases, and iii) the 75/25 holdout meta-validation technique is always
beneficial; i.e., it avoids the likely counterproductive effects of validation
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayley_S/0/1/0/all/0/1&quot;&gt;Sean Bayley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falessi_D/0/1/0/all/0/1&quot;&gt;Davide Falessi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07222">
<title>Rover Descent: Learning to optimize by learning to navigate on prototypical loss surfaces. (arXiv:1801.07222v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.07222</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to optimize - the idea that we can learn from data algorithms that
optimize a numerical criterion - has recently been at the heart of a growing
number of research efforts. One of the most challenging issues within this
approach is to learn a policy that is able to optimize over classes of
functions that are fairly different from the ones that it was trained on. We
propose a novel way of framing learning to optimize as a problem of learning a
good navigation policy on a partially observable loss surface. To this end, we
develop Rover Descent, a solution that allows us to learn a fairly broad
optimization policy from training on a small set of prototypical
two-dimensional surfaces that encompasses the classically hard cases such as
valleys, plateaus, cliffs and saddles and by using strictly zero-order
information. We show that, without having access to gradient or curvature
information, we achieve state-of-the-art convergence speed on optimization
problems not presented at training time such as the Rosenbrock function and
other hard cases in two dimensions. We extend our framework to optimize over
high dimensional landscapes, while still handling only two-dimensional local
landscape information and show good preliminary results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faury_L/0/1/0/all/0/1&quot;&gt;Louis Faury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasile_F/0/1/0/all/0/1&quot;&gt;Flavian Vasile&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1502.01068">
<title>Composite convex minimization involving self-concordant-like cost functions. (arXiv:1502.01068v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1502.01068</link>
<description rdf:parseType="Literal">&lt;p&gt;The self-concordant-like property of a smooth convex function is a new
analytical structure that generalizes the self-concordant notion. While a wide
variety of important applications feature the self-concordant-like property,
this concept has heretofore remained unexploited in convex optimization. To
this end, we develop a variable metric framework of minimizing the sum of a
&quot;simple&quot; convex function and a self-concordant-like function. We introduce a
new analytic step-size selection procedure and prove that the basic gradient
algorithm has improved convergence guarantees as compared to &quot;fast&quot; algorithms
that rely on the Lipschitz gradient property. Our numerical tests with
real-data sets shows that the practice indeed follows the theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tran_Dinh_Q/0/1/0/all/0/1&quot;&gt;Quoc Tran-Dinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yen-Huan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.03662">
<title>Subspace Perspective on Canonical Correlation Analysis: Dimension Reduction and Minimax Rates. (arXiv:1605.03662v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1605.03662</link>
<description rdf:parseType="Literal">&lt;p&gt;Canonical correlation analysis (CCA) is a fundamental statistical tool for
exploring the correlation structure between two sets of random variables. In
this paper, motivated by recent success of applying CCA to learn low
dimensional representations of high dimensional objects, we propose to quantify
the estimation loss of CCA by the excess prediction loss defined through a
prediction-after-dimension-reduction framework. Such framework suggests viewing
CCA estimation as estimating the subspaces spanned by the canonical variates.
Interestedly, the proposed error metrics derived from the excess prediction
loss turn out to be closely related to the principal angles between the
subspaces spanned by the population and sample canonical variates respectively.
&lt;/p&gt;
&lt;p&gt;We characterize the non-asymptotic minimax rates under the proposed metrics,
especially the dependency of the minimax rates on the key quantities including
the dimensions, the condition number of the covariance matrices, the canonical
correlations and the eigen-gap, with minimal assumptions on the joint
covariance matrix. To the best of our knowledge, this is the first finite
sample result that captures the effect of the canonical correlations on the
minimax rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhuang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaodong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.09296">
<title>Symmetry, Saddle Points, and Global Optimization Landscape of Nonconvex Matrix Factorization. (arXiv:1612.09296v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1612.09296</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a general theory for studying the \xl{landscape} of nonconvex
\xl{optimization} with underlying symmetric structures \tz{for a class of
machine learning problems (e.g., low-rank matrix factorization, phase
retrieval, and deep linear neural networks)}. In specific, we characterize the
locations of stationary points and the null space of Hessian matrices \xl{of
the objective function} via the lens of invariant groups\removed{for associated
optimization problems, including low-rank matrix factorization, phase
retrieval, and deep linear neural networks}. As a major motivating example, we
apply the proposed general theory to characterize the global \xl{landscape} of
the \xl{nonconvex optimization in} low-rank matrix factorization problem. In
particular, we illustrate how the rotational symmetry group gives rise to
infinitely many nonisolated strict saddle points and equivalent global minima
of the objective function. By explicitly identifying all stationary points, we
divide the entire parameter space into three regions: ($\cR_1$) the region
containing the neighborhoods of all strict saddle points, where the objective
has negative curvatures; ($\cR_2$) the region containing neighborhoods of all
global minima, where the objective enjoys strong convexity along certain
directions; and ($\cR_3$) the complement of the above regions, where the
gradient has sufficiently large magnitudes. We further extend our result to the
matrix sensing problem. Such global landscape implies strong global convergence
guarantees for popular iterative algorithms with arbitrary initial solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Junwei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_R/0/1/0/all/0/1&quot;&gt;Raman Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haupt_J/0/1/0/all/0/1&quot;&gt;Jarvis Haupt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.07959">
<title>Supervised Learning of Labeled Pointcloud Differences via Cover-Tree Entropy Reduction. (arXiv:1702.07959v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1702.07959</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new algorithm, called CDER, for supervised machine learning
that merges the multi-scale geometric properties of Cover Trees with the
information-theoretic properties of entropy. CDER applies to a training set of
labeled pointclouds embedded in a common Euclidean space. If typical
pointclouds corresponding to distinct labels tend to differ at any scale in any
sub-region, CDER can identify these differences in (typically) linear time,
creating a set of distributional coordinates which act as a feature extraction
mechanism for supervised learning. We describe theoretical properties and
implementation details of CDER, and illustrate its benefits on several
synthetic examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1&quot;&gt;Abraham Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bendich_P/0/1/0/all/0/1&quot;&gt;Paul Bendich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harer_J/0/1/0/all/0/1&quot;&gt;John Harer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pieloch_A/0/1/0/all/0/1&quot;&gt;Alex Pieloch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hineman_J/0/1/0/all/0/1&quot;&gt;Jay Hineman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.04081">
<title>Feature overwriting as a finite mixture process: Evidence from comprehension data. (arXiv:1703.04081v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.04081</link>
<description rdf:parseType="Literal">&lt;p&gt;The ungrammatical sentence &quot;The key to the cabinets are on the table&quot; is
known to lead to an illusion of grammaticality. As discussed in the
meta-analysis by Jaeger et al., 2017, faster reading times are observed at the
verb are in the agreement-attraction sentence above compared to the equally
ungrammatical sentence &quot;The key to the cabinet are on the table&quot;. One
explanation for this facilitation effect is the feature percolation account:
the plural feature on cabinets percolates up to the head noun key, leading to
the illusion. An alternative account is in terms of cue-based retrieval (Lewis
&amp;amp; Vasishth, 2005), which assumes that the non-subject noun cabinets is
misretrieved due to a partial feature-match when a dependency completion
process at the auxiliary initiates a memory access for a subject with plural
marking. We present evidence for yet another explanation for the observed
facilitation. Because the second sentence has two nouns with identical number,
it is possible that these are, in some proportion of trials, more difficult to
keep distinct, leading to slower reading times at the verb in the first
sentence above; this is the feature overwriting account of Nairne, 1990. We
show that the feature overwriting proposal can be implemented as a finite
mixture process. We reanalysed ten published data-sets, fitting hierarchical
Bayesian mixture models to these data assuming a two-mixture distribution. We
show that in nine out of the ten studies, a mixture distribution corresponding
to feature overwriting furnishes a superior fit over both the feature
percolation and the cue-based retrieval accounts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vasishth_S/0/1/0/all/0/1&quot;&gt;Shravan Vasishth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jager_L/0/1/0/all/0/1&quot;&gt;Lena A. J&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nicenboim_B/0/1/0/all/0/1&quot;&gt;Bruno Nicenboim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.04289">
<title>Stochastic Gradient Descent as Approximate Bayesian Inference. (arXiv:1704.04289v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.04289</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic Gradient Descent with a constant learning rate (constant SGD)
simulates a Markov chain with a stationary distribution. With this perspective,
we derive several new results. (1) We show that constant SGD can be used as an
approximate Bayesian posterior inference algorithm. Specifically, we show how
to adjust the tuning parameters of constant SGD to best match the stationary
distribution to a posterior, minimizing the Kullback-Leibler divergence between
these two distributions. (2) We demonstrate that constant SGD gives rise to a
new variational EM algorithm that optimizes hyperparameters in complex
probabilistic models. (3) We also propose SGD with momentum for sampling and
show how to adjust the damping coefficient accordingly. (4) We analyze MCMC
algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we
quantify the approximation errors due to finite learning rates. Finally (5), we
use the stochastic process perspective to give a short proof of why Polyak
averaging is optimal. Based on this idea, we propose a scalable approximate
MCMC algorithm, the Averaged Stochastic Gradient Sampler.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hoffman_M/0/1/0/all/0/1&quot;&gt;Matthew D. Hoffman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.10378">
<title>Fair Inference On Outcomes. (arXiv:1705.10378v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.10378</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the problem of fair statistical inference
involving outcome variables. Examples include classification and regression
problems, and estimating treatment effects in randomized trials or
observational data. The issue of fairness arises in such problems where some
covariates or treatments are &quot;sensitive,&quot; in the sense of having potential of
creating discrimination. In this paper, we argue that the presence of
discrimination can be formalized in a sensible way as the presence of an effect
of a sensitive covariate on the outcome along certain causal pathways, a view
which generalizes (Pearl, 2009). A fair outcome model can then be learned by
solving a constrained optimization problem. We discuss a number of
complications that arise in classical statistical inference due to this view
and provide workarounds based on recent work in causal and semi-parametric
inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nabi_R/0/1/0/all/0/1&quot;&gt;Razieh Nabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shpitser_I/0/1/0/all/0/1&quot;&gt;Ilya Shpitser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.02744">
<title>Avoiding Discrimination through Causal Reasoning. (arXiv:1706.02744v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.02744</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work on fairness in machine learning has focused on various
statistical discrimination criteria and how they trade off. Most of these
criteria are observational: They depend only on the joint distribution of
predictor, protected attribute, features, and outcome. While convenient to work
with, observational criteria have severe inherent limitations that prevent them
from resolving matters of fairness conclusively.
&lt;/p&gt;
&lt;p&gt;Going beyond observational criteria, we frame the problem of discrimination
based on protected attributes in the language of causal reasoning. This
viewpoint shifts attention from &quot;What is the right fairness criterion?&quot; to
&quot;What do we want to assume about the causal data generating process?&quot; Through
the lens of causality, we make several contributions. First, we crisply
articulate why and when observational criteria fail, thus formalizing what was
before a matter of opinion. Second, our approach exposes previously ignored
subtleties and why they are fundamental to the problem. Finally, we put forward
natural causal non-discrimination criteria and develop algorithms that satisfy
them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kilbertus_N/0/1/0/all/0/1&quot;&gt;Niki Kilbertus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rojas_Carulla_M/0/1/0/all/0/1&quot;&gt;Mateo Rojas-Carulla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parascandolo_G/0/1/0/all/0/1&quot;&gt;Giambattista Parascandolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hardt_M/0/1/0/all/0/1&quot;&gt;Moritz Hardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Janzing_D/0/1/0/all/0/1&quot;&gt;Dominik Janzing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.06315">
<title>FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal Inference. (arXiv:1707.06315v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.06315</link>
<description rdf:parseType="Literal">&lt;p&gt;A classical problem in causal inference is that of matching, where treatment
units need to be matched to control units. Some of the main challenges in
developing matching methods arise from the tension among (i) inclusion of as
many covariates as possible in defining the matched groups, (ii) having matched
groups with enough treated and control units for a valid estimate of Average
Treatment Effect (ATE) in each group, and (iii) computing the matched pairs
efficiently for large datasets. In this paper we propose a fast method for
approximate and exact matching in causal analysis called FLAME (Fast
Large-scale Almost Matching Exactly). We define an optimization objective for
match quality, which gives preferences to matching on covariates that can be
useful for predicting the outcome while encouraging as many matches as
possible. FLAME aims to optimize our match quality measure, leveraging
techniques that are natural for query processing in the area of database
management. We provide two implementations of FLAME using SQL queries and
bit-vector techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Sudeepa Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Volfovsky_A/0/1/0/all/0/1&quot;&gt;Alexander Volfovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianyu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.07287">
<title>Learning uncertainty in regression tasks by artificial neural networks. (arXiv:1707.07287v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.07287</link>
<description rdf:parseType="Literal">&lt;p&gt;We suggest a general approach to quantification of different forms of
uncertainty in regression tasks performed by artificial neural networks. It is
based on the simultaneous training of two neural networks with a joint loss
function. One of the networks performs predictions and the other simultaneously
quantifies the uncertainty of predictions by estimating the locally averaged
loss of the first one. Unlike in many classical uncertainty quantification
methods, the targets are not assumed to be sampled from a probability
distribution of an a priori given form. We analyze how the hyperparameters
affect the learning process and, additionally, show that our method even allows
for better predictions compared to standard neural networks without uncertainty
counterparts. Finally, we show that particular cases of our approach include
maximization of log-likelihood, assuming Gaussian or Laplace noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gurevich_P/0/1/0/all/0/1&quot;&gt;Pavel Gurevich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stuke_H/0/1/0/all/0/1&quot;&gt;Hannes Stuke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.07888">
<title>Active Expansion Sampling for Learning Feasible Domains in an Unbounded Input Space. (arXiv:1708.07888v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.07888</link>
<description rdf:parseType="Literal">&lt;p&gt;Many engineering problems require identifying feasible domains under implicit
constraints. One example is finding acceptable car body styling designs based
on constraints like aesthetics and functionality. Current active-learning based
methods learn feasible domains for bounded input spaces. However, we usually
lack prior knowledge about how to set those input variable bounds. Bounds that
are too small will fail to cover all feasible domains; while bounds that are
too large will waste query budget. To avoid this problem, we introduce Active
Expansion Sampling (AES), a method that identifies (possibly disconnected)
feasible domains over an unbounded input space. AES progressively expands our
knowledge of the input space, and uses successive exploitation and exploration
stages to switch between learning the decision boundary and searching for new
feasible domains. We show that AES has a misclassification loss guarantee
within the explored region, independent of the number of iterations or labeled
samples. Thus it can be used for real-time prediction of samples&apos; feasibility
within the explored region. We evaluate AES on three test examples and compare
AES with two adaptive sampling methods -- the Neighborhood-Voronoi algorithm
and the straddle heuristic -- that operate over fixed input variable bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuge_M/0/1/0/all/0/1&quot;&gt;Mark Fuge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.08862">
<title>Bayesian Inference of Spreading Processes on Networks. (arXiv:1709.08862v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1709.08862</link>
<description rdf:parseType="Literal">&lt;p&gt;Infectious diseases are studied to understand their spreading mechanisms, to
evaluate control strategies and to predict the risk and course of future
outbreaks. Because people only interact with a small number of individuals, and
because the structure of these interactions matters for spreading processes,
the pairwise relationships between individuals in a population can be usefully
represented by a network. Although the underlying processes of transmission are
different, the network approach can be used to study the spread of pathogens in
a contact network or the spread of rumors in an online social network. We study
simulated simple and complex epidemics on synthetic networks and on two
empirical networks, a social / contact network in an Indian village and an
online social network in the U.S. Our goal is to learn simultaneously about the
spreading process parameters and the source node (first infected node) of the
epidemic, given a fixed and known network structure, and observations about
state of nodes at several points in time. Our inference scheme is based on
approximate Bayesian computation (ABC), an inference technique for complex
models with likelihood functions that are either expensive to evaluate or
analytically intractable. ABC enables us to adopt a Bayesian approach to the
problem despite the posterior distribution being very complex. Our method is
agnostic about the topology of the network and the nature of the spreading
process. It generally performs well and, somewhat counter-intuitively, the
inference problem appears to be easier on more heterogeneous network
topologies, which enhances its future applicability to real-world settings
where few networks have homogeneous topologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dutta_R/0/1/0/all/0/1&quot;&gt;Ritabrata Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mira_A/0/1/0/all/0/1&quot;&gt;Antonietta Mira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Onnela_J/0/1/0/all/0/1&quot;&gt;Jukka-Pekka Onnela&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.09180">
<title>Anatomical labeling of brain CT scan anomalies using multi-context nearest neighbor relation networks. (arXiv:1710.09180v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1710.09180</link>
<description rdf:parseType="Literal">&lt;p&gt;This work is an endeavor to develop a deep learning methodology for automated
anatomical labeling of a given region of interest (ROI) in brain computed
tomography (CT) scans. We combine both local and global context to obtain a
representation of the ROI. We then use Relation Networks (RNs) to predict the
corresponding anatomy of the ROI based on its relationship score for each
class. Further, we propose a novel strategy employing nearest neighbors
approach for training RNs. We train RNs to learn the relationship of the target
ROI with the joint representation of its nearest neighbors in each class
instead of all data-points in each class. The proposed strategy leads to better
training of RNs along with increased performance as compared to training
baseline RN network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varadarajan_S/0/1/0/all/0/1&quot;&gt;Srikrishna Varadarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1&quot;&gt;Muktabh Mayank Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grewal_M/0/1/0/all/0/1&quot;&gt;Monika Grewal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1&quot;&gt;Pulkit Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10127">
<title>Variational Inference for Gaussian Process Models with Linear Complexity. (arXiv:1711.10127v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10127</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale Gaussian process inference has long faced practical challenges
due to time and space complexity that is superlinear in dataset size. While
sparse variational Gaussian process models are capable of learning from
large-scale data, standard strategies for sparsifying the model can prevent the
approximation of complex functions. In this work, we propose a novel
variational Gaussian process model that decouples the representation of mean
and covariance functions in reproducing kernel Hilbert space. We show that this
new parametrization generalizes previous models. Furthermore, it yields a
variational inference problem that can be solved by stochastic gradient ascent
with time and space complexity that is only linear in the number of mean
function parameters, regardless of the choice of kernels, likelihoods, and
inducing points. This strategy makes the adoption of large-scale expressive
Gaussian process models possible. We run several experiments on regression
tasks and show that this decoupled approach greatly outperforms previous sparse
variational Gaussian process inference procedures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Ching-An Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boots_B/0/1/0/all/0/1&quot;&gt;Byron Boots&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00961">
<title>Learning Independent Causal Mechanisms. (arXiv:1712.00961v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00961</link>
<description rdf:parseType="Literal">&lt;p&gt;Independent causal mechanisms are a central concept in the study of causality
with implications for machine learning tasks. In this work we develop an
algorithm to recover a set of (inverse) independent mechanisms relating a
distribution transformed by the mechanisms to a reference distribution. The
approach is fully unsupervised and based on a set of experts that compete for
data to specialize and extract the mechanisms. We test and analyze the proposed
method on a series of experiments based on image transformations. Each expert
successfully maps a subset of the transformed data to the original domain, and
the learned mechanisms generalize to other domains. We discuss implications for
domain transfer and links to recent trends in generative modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parascandolo_G/0/1/0/all/0/1&quot;&gt;Giambattista Parascandolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojas_Carulla_M/0/1/0/all/0/1&quot;&gt;Mateo Rojas-Carulla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1&quot;&gt;Niki Kilbertus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04146">
<title>A Random Sample Partition Data Model for Big Data Analysis. (arXiv:1712.04146v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04146</link>
<description rdf:parseType="Literal">&lt;p&gt;Big data sets must be carefully partitioned into statistically similar data
subsets that can be used as representative samples for big data analysis tasks.
In this paper, we propose the random sample partition (RSP) data model to
represent a big data set as a set of non-overlapping data subsets, called RSP
data blocks, where each RSP data block has a probability distribution similar
to the whole big data set. Under this data model, efficient block level
sampling is used to randomly select RSP data blocks, replacing expensive record
level sampling to select sample data from a big distributed data set on a
computing cluster. We show how RSP data blocks can be employed to estimate
statistics of a big data set and build models which are equivalent to those
built from the whole big data set. In this approach, analysis of a big data set
becomes analysis of few RSP data blocks which have been generated in advance on
the computing cluster. Therefore, the new method for data analysis based on RSP
data blocks is scalable to big data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salloum_S/0/1/0/all/0/1&quot;&gt;Salman Salloum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yulin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Joshua Zhexue Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emara_T/0/1/0/all/0/1&quot;&gt;Tamer Z. Emara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Chenghao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Heping He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08914">
<title>Bayesian Nonparametric Causal Inference: Information Rates and Learning Algorithms. (arXiv:1712.08914v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08914</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the problem of estimating the causal effect of a treatment on
individual subjects from observational data, this is a central problem in
various application domains, including healthcare, social sciences, and online
advertising. Within the Neyman Rubin potential outcomes model, we use the
Kullback Leibler (KL) divergence between the estimated and true distributions
as a measure of accuracy of the estimate, and we define the information rate of
the Bayesian causal inference procedure as the (asymptotic equivalence class of
the) expected value of the KL divergence between the estimated and true
distributions as a function of the number of samples. Using Fano method, we
establish a fundamental limit on the information rate that can be achieved by
any Bayesian estimator, and show that this fundamental limit is independent of
the selection bias in the observational data. We characterize the Bayesian
priors on the potential (factual and counterfactual) outcomes that achieve the
optimal information rate. As a consequence, we show that a particular class of
priors that have been widely used in the causal inference literature cannot
achieve the optimal information rate. On the other hand, a broader class of
priors can achieve the optimal information rate. We go on to propose a prior
adaptation procedure (which we call the information based empirical Bayes
procedure) that optimizes the Bayesian prior by maximizing an information
theoretic criterion on the recovered causal effects rather than maximizing the
marginal likelihood of the observed (factual) data. Building on our analysis,
we construct an information optimal Bayesian causal inference algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alaa_A/0/1/0/all/0/1&quot;&gt;Ahmed M. Alaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item></rdf:RDF>