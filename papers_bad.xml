<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-22T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07370"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06673"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07331"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07419"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06485"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02792"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04603"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07323"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07344"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07347"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07351"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07405"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07580"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07672"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07675"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10589"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04987"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11132"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06913"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.07370">
<title>Minimizing Area and Energy of Deep Learning Hardware Design Using Collective Low Precision and Structured Compression. (arXiv:1804.07370v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.07370</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning algorithms have shown tremendous success in many recognition
tasks; however, these algorithms typically include a deep neural network (DNN)
structure and a large number of parameters, which makes it challenging to
implement them on power/area-constrained embedded platforms. To reduce the
network size, several studies investigated compression by introducing
element-wise or row-/column-/block-wise sparsity via pruning and
regularization. In addition, many recent works have focused on reducing
precision of activations and weights with some reducing down to a single bit.
However, combining various sparsity structures with binarized or
very-low-precision (2-3 bit) neural networks have not been comprehensively
explored. In this work, we present design techniques for minimum-area/-energy
DNN hardware with minimal degradation in accuracy. During training, both
binarization/low-precision and structured sparsity are applied as constraints
to find the smallest memory footprint for a given deep learning algorithm. The
DNN model for CIFAR-10 dataset with weight memory reduction of 50X exhibits
accuracy comparable to that of the floating-point counterpart. Area,
performance and energy results of DNN hardware in 40nm CMOS are reported for
the MNIST dataset. The optimized DNN that combines 8X structured compression
and 3-bit weight precision showed 98.4% accuracy at 20nJ per classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1&quot;&gt;Shihui Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1&quot;&gt;Gaurav Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkataramanaiah_S/0/1/0/all/0/1&quot;&gt;Shreyas K. Venkataramanaiah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarti_C/0/1/0/all/0/1&quot;&gt;Chaitali Chakrabarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berisha_V/0/1/0/all/0/1&quot;&gt;Visar Berisha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Jae-sun Seo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07655">
<title>Evolution of a Functionally Diverse Swarm via a Novel Decentralised Quality-Diversity Algorithm. (arXiv:1804.07655v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.07655</link>
<description rdf:parseType="Literal">&lt;p&gt;The presence of functional diversity within a group has been demonstrated to
lead to greater robustness, higher performance and increased problem-solving
ability in a broad range of studies that includes insect groups, human groups
and swarm robotics. Evolving group diversity however has proved challenging
within Evolutionary Robotics, requiring reproductive isolation and careful
attention to population size and selection mechanisms. To tackle this issue, we
introduce a novel, decentralised, variant of the MAP-Elites illumination
algorithm which is hybridised with a well-known distributed evolutionary
algorithm (mEDEA). The algorithm simultaneously evolves multiple diverse
behaviours for multiple robots, with respect to a simple token-gathering task.
Each robot in the swarm maintains a local archive defined by two pre-specified
functional traits which is shared with robots it come into contact with. We
investigate four different strategies for sharing, exploiting and combining
local archives and compare results to mEDEA. Experimental results show that in
contrast to previous claims, it is possible to evolve a functionally diverse
swarm without geographical isolation, and that the new method outperforms mEDEA
in terms of the diversity, coverage and precision of the evolved swarm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hart_E/0/1/0/all/0/1&quot;&gt;Emma Hart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steyven_A/0/1/0/all/0/1&quot;&gt;Andreas S.W. Steyven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paechter_B/0/1/0/all/0/1&quot;&gt;Ben Paechter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06673">
<title>Neon2: Finding Local Minima via First-Order Oracles. (arXiv:1711.06673v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06673</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a reduction for non-convex optimization that can (1) turn an
stationary-point finding algorithm into an local-minimum finding one, and (2)
replace the Hessian-vector product computations with only gradient
computations. It works both in the stochastic and the deterministic settings,
without hurting the algorithm&apos;s performance.
&lt;/p&gt;
&lt;p&gt;As applications, our reduction turns Natasha2 into a first-order method
without hurting its performance. It also converts SGD, GD, SCSG, and SVRG into
algorithms finding approximate local minima, outperforming some best known
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Allen-Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02342">
<title>Energy-Efficient CMOS Memristive Synapses for Mixed-Signal Neuromorphic System-on-a-Chip. (arXiv:1802.02342v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02342</link>
<description rdf:parseType="Literal">&lt;p&gt;Emerging non-volatile memory (NVM), or memristive, devices promise
energy-efficient realization of deep learning, when efficiently integrated with
mixed-signal integrated circuits on a CMOS substrate. Even though several
algorithmic challenges need to be addressed to turn the vision of memristive
Neuromorphic Systems-on-a-Chip (NeuSoCs) into reality, issues at the device and
circuit interface need immediate attention from the community. In this work, we
perform energy-estimation of a NeuSoC system and predict the desirable circuit
and device parameters for energy-efficiency optimization. Also, CMOS synapse
circuits based on the concept of CMOS memristor emulator are presented as a
system prototyping methodology, while practical memristor devices are being
developed and integrated with general-purpose CMOS. The proposed mixed-signal
memristive synapse can be designed and fabricated using standard CMOS
technologies and open doors to interesting applications in cognitive computing
circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_V/0/1/0/all/0/1&quot;&gt;Vishal Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kehan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07331">
<title>Stylistic Variation in Social Media Part-of-Speech Tagging. (arXiv:1804.07331v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.07331</link>
<description rdf:parseType="Literal">&lt;p&gt;Social media features substantial stylistic variation, raising new challenges
for syntactic analysis of online writing. However, this variation is often
aligned with author attributes such as age, gender, and geography, as well as
more readily-available social network metadata. In this paper, we report new
evidence on the link between language and social networks in the task of
part-of-speech tagging. We find that tagger error rates are correlated with
network structure, with high accuracy in some parts of the network, and lower
accuracy elsewhere. As a result, tagger accuracy depends on training from a
balanced sample of the network, rather than training on texts from a narrow
subcommunity. We also describe our attempts to add robustness to stylistic
variation, by building a mixture-of-experts model in which each expert is
associated with a region of the social network. While prior work found that
similar approaches yield performance improvements in sentiment analysis and
entity linking, we were unable to obtain performance improvements in
part-of-speech tagging, despite strong evidence for the link between
part-of-speech error rates and social network structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balusu_M/0/1/0/all/0/1&quot;&gt;Murali Raghu Babu Balusu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merghani_T/0/1/0/all/0/1&quot;&gt;Taha Merghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1&quot;&gt;Jacob Eisenstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07419">
<title>An Ensemble Generation MethodBased on Instance Hardness. (arXiv:1804.07419v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07419</link>
<description rdf:parseType="Literal">&lt;p&gt;In Machine Learning, ensemble methods have been receiving a great deal of
attention. Techniques such as Bagging and Boosting have been successfully
applied to a variety of problems. Nevertheless, such techniques are still
susceptible to the effects of noise and outliers in the training data. We
propose a new method for the generation of pools of classifiers based on
Bagging, in which the probability of an instance being selected during the
resampling process is inversely proportional to its instance hardness, which
can be understood as the likelihood of an instance being misclassified,
regardless of the choice of classifier. The goal of the proposed method is to
remove noisy data without sacrificing the hard instances which are likely to be
found on class boundaries. We evaluate the performance of the method in
nineteen public data sets, and compare it to the performance of the Bagging and
Random Subspace algorithms. Our experiments show that in high noise scenarios
the accuracy of our method is significantly better than that of Bagging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walmsley_F/0/1/0/all/0/1&quot;&gt;Felipe N. Walmsley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavalcanti_G/0/1/0/all/0/1&quot;&gt;George D. C. Cavalcanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1&quot;&gt;Dayvid V. R. Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_R/0/1/0/all/0/1&quot;&gt;Rafael M. O. Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabourin_R/0/1/0/all/0/1&quot;&gt;Robert Sabourin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06485">
<title>Robust Estimation via Robust Gradient Estimation. (arXiv:1802.06485v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06485</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide a new computationally-efficient class of estimators for risk
minimization. We show that these estimators are robust for general statistical
models: in the classical Huber epsilon-contamination model and in heavy-tailed
settings. Our workhorse is a novel robust variant of gradient descent, and we
provide conditions under which our gradient descent variant provides accurate
estimators in a general convex risk minimization problem. We provide specific
consequences of our theory for linear regression, logistic regression and for
estimation of the canonical parameters in an exponential family. These results
provide some of the first computationally tractable and provably robust
estimators for these canonical statistical models. Finally, we study the
empirical performance of our proposed methods on synthetic and real datasets,
and find that our methods convincingly outperform a variety of baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Prasad_A/0/1/0/all/0/1&quot;&gt;Adarsh Prasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suggala_A/0/1/0/all/0/1&quot;&gt;Arun Sai Suggala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balakrishnan_S/0/1/0/all/0/1&quot;&gt;Sivaraman Balakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02792">
<title>Occluded Person Re-identification. (arXiv:1804.02792v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02792</link>
<description rdf:parseType="Literal">&lt;p&gt;Person re-identification (re-id) suffers from a serious occlusion problem
when applied to crowded public places. In this paper, we propose to retrieve a
full-body person image by using a person image with occlusions. This differs
significantly from the conventional person re-id problem where it is assumed
that person images are detected without any occlusion. We thus call this new
problem the occluded person re-identitification. To address this new problem,
we propose a novel Attention Framework of Person Body (AFPB) based on deep
learning, consisting of 1) an Occlusion Simulator (OS) which automatically
generates artificial occlusions for full-body person images, and 2) multi-task
losses that force the neural network not only to discriminate a person&apos;s
identity but also to determine whether a sample is from the occluded data
distribution or the full-body data distribution. Experiments on a new occluded
person re-id dataset and three existing benchmarks modified to include
full-body person images and occluded person images show the superiority of the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Jianhuang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangcong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04603">
<title>Outline Objects using Deep Reinforcement Learning. (arXiv:1804.04603v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04603</link>
<description rdf:parseType="Literal">&lt;p&gt;Image segmentation needs both local boundary position information and global
object context information. The performance of the recent state-of-the-art
method, fully convolutional networks, reaches a bottleneck due to the neural
network limit after balancing between the two types of information
simultaneously in an end-to-end training style. To overcome this problem, we
divide the semantic image segmentation into temporal subtasks. First, we find a
possible pixel position of some object boundary; then trace the boundary at
steps within a limited length until the whole object is outlined. We present
the first deep reinforcement learning approach to semantic image segmentation,
called DeepOutline, which outperforms other algorithms in Coco detection
leaderboard in the middle and large size person category in Coco val2017
dataset. Meanwhile, it provides an insight into a divide and conquer way by
reinforcement learning on computer vision problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarcar_S/0/1/0/all/0/1&quot;&gt;Sayan Sarcar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yilin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiangshi Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07323">
<title>Nonparametric Stochastic Compositional Gradient Descent for Q-Learning in Continuous Markov Decision Problems. (arXiv:1804.07323v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07323</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider Markov Decision Problems defined over continuous state and action
spaces, where an autonomous agent seeks to learn a map from its states to
actions so as to maximize its long-term discounted accumulation of rewards. We
address this problem by considering Bellman&apos;s optimality equation defined over
action-value functions, which we reformulate into a nested non-convex
stochastic optimization problem defined over a Reproducing Kernel Hilbert Space
(RKHS). We develop a functional generalization of stochastic quasi-gradient
method to solve it, which, owing to the structure of the RKHS, admits a
parameterization in terms of scalar weights and past state-action pairs which
grows proportionately with the algorithm iteration index. To ameliorate this
complexity explosion, we apply Kernel Orthogonal Matching Pursuit to the
sequence of kernel weights and dictionaries, which yields a controllable error
in the descent direction of the underlying optimization method. We prove that
the resulting algorithm, called KQ-Learning, converges with probability 1 to a
stationary point of this problem, yielding a fixed point of the Bellman
optimality operator under the hypothesis that it belongs to the RKHS. Under
constant learning rates, we further obtain convergence to a small Bellman error
that depends on the chosen learning rates. Numerical evaluation on the
Continuous Mountain Car and Inverted Pendulum tasks yields convergent
parsimonious learned action-value functions, policies that are competitive with
the state of the art, and exhibit reliable, reproducible learning behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koppel_A/0/1/0/all/0/1&quot;&gt;Alec Koppel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolstaya_E/0/1/0/all/0/1&quot;&gt;Ekaterina Tolstaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stump_E/0/1/0/all/0/1&quot;&gt;Ethan Stump&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Alejandro Ribeiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07344">
<title>Effects of sampling skewness of the importance-weighted risk estimator on model selection. (arXiv:1804.07344v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.07344</link>
<description rdf:parseType="Literal">&lt;p&gt;Importance-weighting is a popular and well-researched technique for dealing
with sample selection bias and covariate shift. It has desirable
characteristics such as unbiasedness, consistency and low computational
complexity. However, weighting can have a detrimental effect on an estimator as
well. In this work, we empirically show that the sampling distribution of an
importance-weighted estimator can be skewed. For sample selection bias
settings, and for small sample sizes, the importance-weighted risk estimator
produces overestimates for datasets in the body of the sampling distribution,
i.e. the majority of cases, and large underestimates for data sets in the tail
of the sampling distribution. These over- and underestimates of the risk lead
to suboptimal regularization parameters when used for importance-weighted
validation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kouw_W/0/1/0/all/0/1&quot;&gt;Wouter M. Kouw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loog_M/0/1/0/all/0/1&quot;&gt;Marco Loog&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07347">
<title>Randomized ICA and LDA Dimensionality Reduction Methods for Hyperspectral Image Classification. (arXiv:1804.07347v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.07347</link>
<description rdf:parseType="Literal">&lt;p&gt;Dimensionality reduction is an important step in processing the hyperspectral
images (HSI) to overcome the curse of dimensionality problem. Linear
dimensionality reduction methods such as Independent component analysis (ICA)
and Linear discriminant analysis (LDA) are commonly employed to reduce the
dimensionality of HSI. These methods fail to capture non-linear dependency in
the HSI data, as data lies in the nonlinear manifold. To handle this, nonlinear
transformation techniques based on kernel methods were introduced for
dimensionality reduction of HSI. However, the kernel methods involve cubic
computational complexity while computing the kernel matrix, and thus its
potential cannot be explored when the number of pixels (samples) are large. In
literature a fewer number of pixels are randomly selected to partial to
overcome this issue, however this sub-optimal strategy might neglect important
information in the HSI. In this paper, we propose randomized solutions to the
ICA and LDA dimensionality reduction methods using Random Fourier features, and
we label them as RFFICA and RFFLDA. Our proposed method overcomes the
scalability issue and to handle the non-linearities present in the data more
efficiently. Experiments conducted with two real-world hyperspectral datasets
demonstrates that our proposed randomized methods outperform the conventional
kernel ICA and kernel LDA in terms overall, per-class accuracies and
computational time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jayaprakash_C/0/1/0/all/0/1&quot;&gt;Chippy Jayaprakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Damodaran_B/0/1/0/all/0/1&quot;&gt;Bharath Bhushan Damodaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+V_S/0/1/0/all/0/1&quot;&gt;Sowmya V&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soman_K/0/1/0/all/0/1&quot;&gt;K P Soman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07351">
<title>Sampling-free Uncertainty Estimation in Gated Recurrent Units with Exponential Families. (arXiv:1804.07351v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07351</link>
<description rdf:parseType="Literal">&lt;p&gt;There has recently been a concerted effort to derive mechanisms in vision and
machine learning systems to offer uncertainty estimates of the predictions they
make. Clearly, there are enormous benefits to a system that is not only
accurate but also has a sense for when it is not sure. Existing proposals
center around Bayesian interpretations of modern deep architectures -- these
are effective but can often be computationally demanding. We show how classical
ideas in the literature on exponential families on probabilistic networks
provide an excellent starting point to derive uncertainty estimates in Gated
Recurrent Units (GRU). Our proposal directly quantifies uncertainty
deterministically, without the need for costly sampling-based estimation. We
demonstrate how our model can be used to quantitatively and qualitatively
measure uncertainty in unsupervised image sequence prediction. To our
knowledge, this is the first result describing sampling-free uncertainty
estimation for powerful sequential models such as GRUs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Seong Jae Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1&quot;&gt;Ronak Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1&quot;&gt;Vikas Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07405">
<title>GritNet: Student Performance Prediction with Deep Learning. (arXiv:1804.07405v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07405</link>
<description rdf:parseType="Literal">&lt;p&gt;Student performance prediction - where a machine forecasts the future
performance of students as they interact with online coursework - is a
challenging problem. Reliable early-stage predictions of a student&apos;s future
performance could be critical to facilitate timely educational interventions
during a course. However, very few prior studies have explored this problem
from a deep learning perspective. In this paper, we recast the student
performance prediction problem as a sequential event prediction problem and
propose a new deep learning based algorithm, termed GritNet, which builds upon
the bidirectional long short term memory (BLSTM). Our results, from real
Udacity students&apos; graduation predictions, show that the GritNet not only
consistently outperforms the standard logistic-regression based method, but
that improvements are substantially pronounced in the first few weeks when
accurate predictions are most challenging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Byung-Hak Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vizitei_E/0/1/0/all/0/1&quot;&gt;Ethan Vizitei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganapathi_V/0/1/0/all/0/1&quot;&gt;Varun Ganapathi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07580">
<title>Robust and scalable learning of data manifolds with complex topologies via ElPiGraph. (arXiv:1804.07580v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.07580</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ElPiGraph, a method for approximating data distributions having
non-trivial topological features such as the existence of excluded regions or
branching structures. Unlike many existing methods, ElPiGraph is not based on
the construction of a k-nearest neighbour graph, a procedure that can perform
poorly in the case of multidimensional and noisy data. Instead, ElPiGraph
constructs elastic principal graphs in a more robust way by minimizing elastic
energy, applying graph grammars and explicitly controlling topological
complexity. Using trimmed approximation error function makes ElPiGraph
extremely robust to the presence of background noise without decreasing
computational performance and allows it to deal with complex cases of manifold
learning (for example, ElPiGraph can learn disconnected intersecting
manifolds). Thanks to the quasi-quadratic nature of the elastic function,
ElPiGraph performs almost as fast as a simple k-means clustering and,
therefore, is much more scalable than alternative methods, and can work on
large datasets containing millions of high dimensional points on a personal
computer. The excellent performance of the method opens the possibility to
apply resampling and to approximate complex data structures via principal graph
ensembles which can be used to construct consensus principal graphs. ElPiGraph
is currently implemented in five programming languages and accompanied by a
graphical user interface, which makes it a versatile tool to deal with complex
data in various fields from molecular biology, where it can be used to infer
pseudo-time trajectories from single-cell RNASeq, to astronomy, where it can be
used to approximate complex structures in the distribution of galaxies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albergante_L/0/1/0/all/0/1&quot;&gt;Luca Albergante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirkes_E/0/1/0/all/0/1&quot;&gt;Evgeny M. Mirkes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huidong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1&quot;&gt;Alexis Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faure_L/0/1/0/all/0/1&quot;&gt;Louis Faure&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barillot_E/0/1/0/all/0/1&quot;&gt;Emmanuel Barillot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinello_L/0/1/0/all/0/1&quot;&gt;Luca Pinello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1&quot;&gt;Alexander N. Gorban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zinovyev_A/0/1/0/all/0/1&quot;&gt;Andrei Zinovyev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07672">
<title>Unsupervised learning of the brain connectivity dynamic using residual D-net. (arXiv:1804.07672v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.07672</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel unsupervised learning method to learn the
brain dynamics using a deep learning architecture named residual D-net. As it
is often the case in medical research, in contrast to typical deep learning
tasks, the size of the resting-state functional Magnetic Resonance Image
(rs-fMRI) datasets for training is limited. Thus, the available data should be
very efficiently used to learn the complex patterns underneath the brain
connectivity dynamics. To address this issue, we use residual connections to
alleviate the training complexity through recurrent multi-scale representation.
We conduct two classification tasks to differentiate early and late stage Mild
Cognitive Impairment (MCI) from Normal healthy Control (NC) subjects. The
experiments verify that our proposed residual D-net indeed learns the brain
connectivity dynamics, leading to significantly higher classification accuracy
compared to previously published techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Seo_Y/0/1/0/all/0/1&quot;&gt;Youngjoo Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Morante_M/0/1/0/all/0/1&quot;&gt;Manuel Morante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kopsinis_Y/0/1/0/all/0/1&quot;&gt;Yannis Kopsinis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Theodoridis_S/0/1/0/all/0/1&quot;&gt;Sergios Theodoridis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07675">
<title>Achievable Information Rates for Nonlinear Fiber Communication via End-to-end Autoencoder Learning. (arXiv:1804.07675v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1804.07675</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning is used to compute achievable information rates (AIRs) for a
simplified fiber channel. The approach jointly optimizes the input distribution
(constellation shaping) and the auxiliary channel distribution to compute AIRs
without explicit channel knowledge in an end-to-end fashion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hager_C/0/1/0/all/0/1&quot;&gt;Christian H&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1&quot;&gt;Nil Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wymeersch_H/0/1/0/all/0/1&quot;&gt;Henk Wymeersch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10589">
<title>Contextual Outlier Interpretation. (arXiv:1711.10589v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10589</link>
<description rdf:parseType="Literal">&lt;p&gt;Outlier detection plays an essential role in many data-driven applications to
identify isolated instances that are different from the majority. While many
statistical learning and data mining techniques have been used for developing
more effective outlier detection algorithms, the interpretation of detected
outliers does not receive much attention. Interpretation is becoming
increasingly important to help people trust and evaluate the developed models
through providing intrinsic reasons why the certain outliers are chosen. It is
difficult, if not impossible, to simply apply feature selection for explaining
outliers due to the distinct characteristics of various detection models,
complicated structures of data in certain applications, and imbalanced
distribution of outliers and normal instances. In addition, the role of
contrastive contexts where outliers locate, as well as the relation between
outliers and contexts, are usually overlooked in interpretation. To tackle the
issues above, in this paper, we propose a novel Contextual Outlier
INterpretation (COIN) method to explain the abnormality of existing outliers
spotted by detectors. The interpretability for an outlier is achieved from
three aspects: outlierness score, attributes that contribute to the
abnormality, and contextual description of its neighborhoods. Experimental
results on various types of datasets demonstrate the flexibility and
effectiveness of the proposed framework compared with existing interpretation
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1&quot;&gt;Donghwa Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xia Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04987">
<title>On the Complexity of the Weighted Fused Lasso. (arXiv:1801.04987v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04987</link>
<description rdf:parseType="Literal">&lt;p&gt;The solution path of the 1D fused lasso for an $n$-dimensional input is
piecewise linear with $\mathcal{O}(n)$ segments (Hoefling et al. 2010 and
Tibshirani et al 2011). However, existing proofs of this bound do not hold for
the weighted fused lasso. At the same time, results for the generalized lasso,
of which the weighted fused lasso is a special case, allow $\Omega(3^n)$
segments (Mairal et al. 2012). In this paper, we prove that the number of
segments in the solution path of the weighted fused lasso is
$\mathcal{O}(n^2)$, and that, for some instances, it is $\Omega(n^2)$. We also
give a new, very simple, proof of the $\mathcal{O}(n)$ bound for the fused
lasso.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bento_J/0/1/0/all/0/1&quot;&gt;Jose Bento&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furmaniak_R/0/1/0/all/0/1&quot;&gt;Ralph Furmaniak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1&quot;&gt;Surjyendu Ray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11132">
<title>Notes on computational-to-statistical gaps: predictions using statistical physics. (arXiv:1803.11132v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.11132</link>
<description rdf:parseType="Literal">&lt;p&gt;In these notes we describe heuristics to predict computational-to-statistical
gaps in certain statistical problems. These are regimes in which the underlying
statistical problem is information-theoretically possible although no efficient
algorithm exists, rendering the problem essentially unsolvable for large
instances. The methods we describe here are based on mature, albeit
non-rigorous, tools from statistical physics.
&lt;/p&gt;
&lt;p&gt;These notes are based on a lecture series given by the authors at the Courant
Institute of Mathematical Sciences in New York City, on May 16th, 2017.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bandeira_A/0/1/0/all/0/1&quot;&gt;Afonso S. Bandeira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perry_A/0/1/0/all/0/1&quot;&gt;Amelia Perry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wein_A/0/1/0/all/0/1&quot;&gt;Alexander S. Wein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06913">
<title>Fast inference of deep neural networks in FPGAs for particle physics. (arXiv:1804.06913v2 [physics.ins-det] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06913</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent results at the Large Hadron Collider (LHC) have pointed to enhanced
physics capabilities through the improvement of the real-time event processing
techniques. Machine learning methods are ubiquitous and have proven to be very
powerful in LHC physics, and particle physics as a whole. However, exploration
of the use of such techniques in low-latency, low-power FPGA hardware has only
just begun. FPGA-based trigger and data acquisition (DAQ) systems have
extremely low, sub-microsecond latency requirements that are unique to particle
physics. We present a case study for neural network inference in FPGAs focusing
on a classifier for jet substructure which would enable, among many other
physics scenarios, searches for new dark sector particles and novel
measurements of the Higgs boson. While we focus on a specific example, the
lessons are far-reaching. We develop a package based on High-Level Synthesis
(HLS) called hls4ml to build machine learning models in FPGAs. The use of HLS
increases accessibility across a broad user community and allows for a drastic
decrease in firmware development time. We map out FPGA resource usage and
latency versus neural network hyperparameters to identify the problems in
particle physics that would benefit from performing neural network inference
with FPGAs. For our example jet substructure model, we fit well within the
available resources of modern FPGAs with a latency on the scale of 100 ns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Duarte_J/0/1/0/all/0/1&quot;&gt;Javier Duarte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Song Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Harris_P/0/1/0/all/0/1&quot;&gt;Philip Harris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jindariani_S/0/1/0/all/0/1&quot;&gt;Sergo Jindariani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kreinar_E/0/1/0/all/0/1&quot;&gt;Edward Kreinar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kreis_B/0/1/0/all/0/1&quot;&gt;Benjamin Kreis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ngadiuba_J/0/1/0/all/0/1&quot;&gt;Jennifer Ngadiuba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pierini_M/0/1/0/all/0/1&quot;&gt;Maurizio Pierini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rivera_R/0/1/0/all/0/1&quot;&gt;Ryan Rivera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Tran_N/0/1/0/all/0/1&quot;&gt;Nhan Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhenbin Wu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>