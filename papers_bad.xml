<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03583"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03674"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08996"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11778"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01016"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01385"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03335"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03379"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03492"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03517"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03563"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03582"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03600"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03820"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04017"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04067"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.05826"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06481"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00851"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11027"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03333"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09904"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00590"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00153"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00806"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01984"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.03035"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03349"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03404"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03430"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03432"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03461"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03467"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03482"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03547"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03551"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03555"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03571"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03664"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03677"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03688"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03763"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03803"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03816"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03836"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03884"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03962"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04028"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04037"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04047"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04090"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.03186"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.08159"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.09350"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02550"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04551"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04630"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07191"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07575"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09902"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02879"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05391"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10161"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01926"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05809"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09108"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09717"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00548"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01248"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01811"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.08095"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.01458"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05480"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.03583">
<title>IVUS-Net: An Intravascular Ultrasound Segmentation Network. (arXiv:1806.03583v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03583</link>
<description rdf:parseType="Literal">&lt;p&gt;IntraVascular UltraSound (IVUS) is one of the most effective imaging
modalities that provides assistance to experts in order to diagnose and treat
cardiovascular diseases. We address a central problem in IVUS image analysis
with Fully Convolutional Network (FCN): automatically delineate the lumen and
media-adventitia borders in IVUS images, which is crucial to shorten the
diagnosis process or benefits a faster and more accurate 3D reconstruction of
the artery. Particularly, we propose an FCN architecture, called IVUS-Net,
followed by a post-processing contour extraction step, in order to
automatically segments the interior (lumen) and exterior (media-adventitia)
regions of the human arteries. We evaluated our IVUS-Net on the test set of a
standard publicly available dataset containing 326 IVUS B-mode images with two
measurements, namely Jaccard Measure (JM) and Hausdorff Distances (HD). The
evaluation result shows that IVUS-Net outperforms the state-of-the-art lumen
and media segmentation methods by 4% to 20% in terms of HD distance. IVUS-Net
performs well on images in the test set that contain a significant amount of
major artifacts such as bifurcations, shadows, and side branches that are not
common in the training set. Furthermore, using a modern GPU, IVUS-Net segments
each IVUS frame only in 0.15 seconds. The proposed work, to the best of our
knowledge, is the first deep learning based method for segmentation of both the
lumen and the media vessel walls in 20 MHz IVUS B-mode images that achieves the
best results without any manual intervention. Code is available at
https://github.com/Kulbear/ivus-segmentation-icsm2018.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Ji Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tong_L/0/1/0/all/0/1&quot;&gt;Lin Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Faraji_M/0/1/0/all/0/1&quot;&gt;Mehdi Faraji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Basu_A/0/1/0/all/0/1&quot;&gt;Anup Basu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03674">
<title>On the Covariance-Hessian Relation in Evolution Strategies. (arXiv:1806.03674v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.03674</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider Evolution Strategies operating only with isotropic Gaussian
mutations on positive quadratic objective functions, and investigate the
covariance matrix when constructed out of selected individuals by truncation.
We prove that the covariance matrix over $(1,\lambda)$-selected decision
vectors becomes proportional to the inverse of the landscape Hessian as the
population-size $\lambda$ increases. This generalizes a previous result that
proved an equivalent phenomenon when sampling was assumed to take place in the
vicinity of the optimum. It further confirms the classical hypothesis that
statistical learning of the landscape is an inherent characteristic of standard
Evolution Strategies, and that this distinguishing capability stems only from
the usage of isotropic Gaussian mutations and rank-based selection. We provide
broad numerical validation for the proven results, and present empirical
evidence for its generalization to $(\mu,\lambda)$-selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shir_O/0/1/0/all/0/1&quot;&gt;Ofer M. Shir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yehudayoff_A/0/1/0/all/0/1&quot;&gt;Amir Yehudayoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08996">
<title>Genesis of Basic and Multi-Layer Echo State Network Recurrent Autoencoders for Efficient Data Representations. (arXiv:1804.08996v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08996</link>
<description rdf:parseType="Literal">&lt;p&gt;It is a widely accepted fact that data representations intervene noticeably
in machine learning tools. The more they are well defined the better the
performance results are. Feature extraction-based methods such as autoencoders
are conceived for finding more accurate data representations from the original
ones. They efficiently perform on a specific task in terms of 1) high accuracy,
2) large short term memory and 3) low execution time. Echo State Network (ESN)
is a recent specific kind of Recurrent Neural Network which presents very rich
dynamics thanks to its reservoir-based hidden layer. It is widely used in
dealing with complex non-linear problems and it has outperformed classical
approaches in a number of tasks including regression, classification, etc. In
this paper, the noticeable dynamism and the large memory provided by ESN and
the strength of Autoencoders in feature extraction are gathered within an ESN
Recurrent Autoencoder (ESN-RAE). In order to bring up sturdier alternative to
conventional reservoir-based networks, not only single layer basic ESN is used
as an autoencoder, but also Multi-Layer ESN (ML-ESN-RAE). The new features,
once extracted from ESN&apos;s hidden layer, are applied to classification tasks.
The classification rates rise considerably compared to those obtained when
applying the original data features. An accuracy-based comparison is performed
between the proposed recurrent AEs and two variants of an ELM feed-forward AEs
(Basic and ML) in both of noise free and noisy environments. The empirical
study reveals the main contribution of recurrent connections in improving the
classification performance results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chouikhi_N/0/1/0/all/0/1&quot;&gt;Naima Chouikhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ammar_B/0/1/0/all/0/1&quot;&gt;Boudour Ammar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alimi_A/0/1/0/all/0/1&quot;&gt;Adel M. Alimi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11778">
<title>Object Detection using Domain Randomization and Generative Adversarial Refinement of Synthetic Images. (arXiv:1805.11778v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11778</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present an application of domain randomization and
generative adversarial networks (GAN) to train a near real-time object detector
for industrial electric parts, entirely in a simulated environment. Large scale
availability of labelled real world data is typically rare and difficult to
obtain in many industrial settings. As such here, only a few hundred of
unlabelled real images are used to train a Cyclic-GAN network, in combination
with various degree of domain randomization procedures. We demonstrate that
this enables robust translation of synthetic images to the real world domain.
We show that a combination of the original synthetic (simulation) and GAN
translated images, when used for training a Mask-RCNN object detection network
achieves greater than 0.95 mean average precision in detecting and classifying
a collection of industrial electric parts. We evaluate the performance across
different combinations of training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nogues_F/0/1/0/all/0/1&quot;&gt;Fernando Camaro Nogues&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huie_A/0/1/0/all/0/1&quot;&gt;Andrew Huie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1&quot;&gt;Sakyasingha Dasgupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01016">
<title>Hierarchical Bi-level Multi-Objective Evolution of Single- and Multi-layer Echo State Network Autoencoders for Data Representations. (arXiv:1806.01016v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01016</link>
<description rdf:parseType="Literal">&lt;p&gt;Echo State Network (ESN) presents a distinguished kind of recurrent neural
networks. It is built upon a sparse, random and large hidden infrastructure
called reservoir. ESNs have succeeded in dealing with several non-linear
problems such as prediction, classification, etc. Thanks to its rich dynamics,
ESN is used as an Autoencoder (AE) to extract features from original data
representations. ESN is not only used with its basic single layer form but also
with the recently proposed Multi-Layer (ML) architecture. The well setting of
ESN (basic and ML) architectures and training parameters is a crucial and hard
labor task. Generally, a number of parameters (hidden neurons, sparsity rates,
input scaling) is manually altered to achieve minimum learning error. However,
this randomly hand crafted task, on one hand, may not guarantee best training
results and on the other hand, it can raise the network&apos;s complexity. In this
paper, a hierarchical bi-level evolutionary optimization is proposed to deal
with these issues. The first level includes a multi-objective architecture
optimization providing maximum learning accuracy while sustaining the
complexity at a reduced standard. Multi-objective Particle Swarm Optimization
(MOPSO) is used to optimize ESN structure in a way to provide a trade-off
between the network complexity decreasing and the accuracy increasing. A
pareto-front of optimal solutions is generated by the end of the MOPSO process.
These solutions present the set of candidates that succeeded in providing a
compromise between different objectives (learning error and network
complexity). At the second level, each of the solutions already found undergo a
mono-objective weights optimization to enhance the obtained pareto-front.
Empirical results ensure the effectiveness of the evolved ESN recurrent AEs
(basic and ML) for noisy and noise free data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chouikhi_N/0/1/0/all/0/1&quot;&gt;Naima Chouikhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ammar_B/0/1/0/all/0/1&quot;&gt;Boudour Ammar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alimi_A/0/1/0/all/0/1&quot;&gt;Adel M. Alimi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01385">
<title>Research on the Brain-inspired Cross-modal Neural Cognitive Computing Framework. (arXiv:1805.01385v2 [cs.NE] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1805.01385</link>
<description rdf:parseType="Literal">&lt;p&gt;To address modeling problems of brain-inspired intelligence, this thesis is
focused on researching in the semantic-oriented framework design for multimedia
and multimodal information. The Multimedia Neural Cognitive Computing (MNCC)
model was designed based on the nervous mechanism and cognitive architecture.
Furthermore, the semantic-oriented hierarchical Cross-modal Neural Cognitive
Computing (CNCC) framework was proposed based on MNCC model, and formal
description and analysis for CNCC framework was given. It would effectively
improve the performance of semantic processing for multimedia and cross-modal
information, and has far-reaching significance for exploration and realization
brain-inspired computing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03335">
<title>Randomized Prior Functions for Deep Reinforcement Learning. (arXiv:1806.03335v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03335</link>
<description rdf:parseType="Literal">&lt;p&gt;Dealing with uncertainty is essential for efficient reinforcement learning.
There is a growing literature on uncertainty estimation for deep learning from
fixed datasets, but many of the most popular approaches are poorly-suited to
sequential decision problems. Other methods, such as bootstrap sampling, have
no mechanism for uncertainty that does not come from the observed data. We
highlight why this can be a crucial shortcoming and propose a simple remedy
through addition of a randomized untrainable `prior&apos; network to each ensemble
member. We prove that this approach is efficient with linear representations,
provide simple illustrations of its efficacy with nonlinear representations and
show that this approach scales to large-scale problems far better than previous
attempts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osband_I/0/1/0/all/0/1&quot;&gt;Ian Osband&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aslanides_J/0/1/0/all/0/1&quot;&gt;John Aslanides&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cassirer_A/0/1/0/all/0/1&quot;&gt;Albin Cassirer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03379">
<title>CS-VQA: Visual Question Answering with Compressively Sensed Images. (arXiv:1806.03379v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.03379</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Question Answering (VQA) is a complex semantic task requiring both
natural language processing and visual recognition. In this paper, we explore
whether VQA is solvable when images are captured in a sub-Nyquist compressive
paradigm. We develop a series of deep-network architectures that exploit
available compressive data to increasing degrees of accuracy, and show that VQA
is indeed solvable in the compressed domain. Our results show that there is
nominal degradation in VQA performance when using compressive measurements, but
that accuracy can be recovered when VQA pipelines are used in conjunction with
state-of-the-art deep neural networks for CS reconstruction. The results
presented yield important implications for resource-constrained VQA
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Li-Chi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1&quot;&gt;Kuldeep Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1&quot;&gt;Anik Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lohit_S/0/1/0/all/0/1&quot;&gt;Suhas Lohit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayasuriya_S/0/1/0/all/0/1&quot;&gt;Suren Jayasuriya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1&quot;&gt;Pavan Turaga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03492">
<title>Explainable Deterministic MDPs. (arXiv:1806.03492v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03492</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method for a certain class of Markov Decision Processes (MDPs)
that can relate the optimal policy back to one or more reward sources in the
environment. For a given initial state, without fully computing the value
function, q-value function, or the optimal policy the algorithm can determine
which rewards will and will not be collected, whether a given reward will be
collected only once or continuously, and which local maximum within the value
function the initial state will ultimately lead to. We demonstrate that the
method can be used to map the state space to identify regions that are
dominated by one reward source and can fully analyze the state space to explain
all actions. We provide a mathematical framework to show how all of this is
possible without first computing the optimal policy or value function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertram_J/0/1/0/all/0/1&quot;&gt;Josh Bertram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1&quot;&gt;Peng Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03497">
<title>Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data for Future Prediction. (arXiv:1806.03497v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03497</link>
<description rdf:parseType="Literal">&lt;p&gt;Future predictions on sequence data (e.g., videos or audios) require the
algorithms to capture non-Markovian and compositional properties of high-level
semantics. Context-free grammars are natural choices to capture such
properties, but traditional grammar parsers (e.g., Earley parser) only take
symbolic sentences as inputs. In this paper, we generalize the Earley parser to
parse sequence data which is neither segmented nor labeled. This generalized
Earley parser integrates a grammar parser with a classifier to find the optimal
segmentation and labels, and makes top-down future predictions. Experiments
show that our method significantly outperforms other approaches for future
human activity prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qi_S/0/1/0/all/0/1&quot;&gt;Siyuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jia_B/0/1/0/all/0/1&quot;&gt;Baoxiong Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03517">
<title>A Taxonomy and Survey of Intrusion Detection System Design Techniques, Network Threats and Datasets. (arXiv:1806.03517v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1806.03517</link>
<description rdf:parseType="Literal">&lt;p&gt;With the world moving towards being increasingly dependent on computers and
automation, one of the main challenges in the current decade has been to build
secure applications, systems and networks. Alongside these challenges, the
number of threats is rising exponentially due to the attack surface increasing
through numerous interfaces offered for each service. To alleviate the impact
of these threats, researchers have proposed numerous solutions; however,
current tools often fail to adapt to ever-changing architectures, associated
threats and 0-days. This manuscript aims to provide researchers with a taxonomy
and survey of current dataset composition and current Intrusion Detection
Systems (IDS) capabilities and assets. These taxonomies and surveys aim to
improve both the efficiency of IDS and the creation of datasets to build the
next generation IDS as well as to reflect networks threats more accurately in
future datasets. To this end, this manuscript also provides a taxonomy and
survey or network threats and associated tools. The manuscript highlights that
current IDS only cover 25% of our threat taxonomy, while current datasets
demonstrate clear lack of real-network threats and attack representation, but
rather include a large number of deprecated threats, hence limiting the
accuracy of current machine learning IDS. Moreover, the taxonomies are
open-sourced to allow public contributions through a Github repository.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hindy_H/0/1/0/all/0/1&quot;&gt;Hanan Hindy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brosset_D/0/1/0/all/0/1&quot;&gt;David Brosset&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayne_E/0/1/0/all/0/1&quot;&gt;Ethan Bayne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seeam_A/0/1/0/all/0/1&quot;&gt;Amar Seeam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tachtatzis_C/0/1/0/all/0/1&quot;&gt;Christos Tachtatzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atkinson_R/0/1/0/all/0/1&quot;&gt;Robert Atkinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellekens_X/0/1/0/all/0/1&quot;&gt;Xavier Bellekens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03563">
<title>Building Bayesian Neural Networks with Blocks: On Structure, Interpretability and Uncertainty. (arXiv:1806.03563v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03563</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide simple schemes to build Bayesian Neural Networks (BNNs), block by
block, inspired by a recent idea of computation skeletons. We show how by
adjusting the types of blocks that are used within the computation skeleton, we
can identify interesting relationships with Deep Gaussian Processes (DGPs),
deep kernel learning (DKL), random features type approximation and other
topics. We give strategies to approximate the posterior via doubly stochastic
variational inference for such models which yield uncertainty estimates. We
give a detailed theoretical analysis and point out extensions that may be of
independent interest. As a special case, we instantiate our procedure to define
a Bayesian {\em additive} Neural network -- a promising strategy to identify
statistical interactions and has direct benefits for obtaining interpretable
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hao Henry Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yunyang Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Singh_V/0/1/0/all/0/1&quot;&gt;Vikas Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03582">
<title>A Scalable Framework for Trajectory Prediction. (arXiv:1806.03582v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.03582</link>
<description rdf:parseType="Literal">&lt;p&gt;Trajectory prediction (TP) is of great importance for a wide range of
location-based applications in intelligent transport systems such as
location-based advertising, route planning, traffic management, and early
warning systems. In the last few years, the widespread use of GPS navigation
systems and wireless communication technology enabled vehicles has resulted in
huge volumes of trajectory data. The task of utilizing this data employing
spatio-temporal techniques for trajectory prediction in an efficient and
accurate manner is an ongoing research problem. Existing TP approaches are
limited to short-term predictions. Moreover, they cannot handle a large volume
of trajectory data for long-term prediction. To address these limitations, we
propose a scalable clustering and Markov chain based hybrid framework, called
Traj-clusiVAT-based TP, for both short-term and long-term trajectory
prediction, which can handle a large number of overlapping trajectories in a
dense road network. In addition, Traj-clusiVAT can also determine the number of
clusters, which represent different movement behaviours in input trajectory
data. In our experiments, we compare our proposed approach with a mixed Markov
model (MMM)-based scheme, and a trajectory clustering, NETSCAN-based TP method
for both short- and long-term trajectory predictions. We performed our
experiments on two real, vehicle trajectory datasets, including a large-scale
trajectory dataset consisting of 3.28 million trajectories obtained from 15,061
taxis in Singapore over a period of one month. Experimental results on two real
trajectory datasets show that our proposed approach outperforms the existing
approaches in terms of both short- and long-term prediction performances, based
on prediction accuracy and distance error (in km).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathore_P/0/1/0/all/0/1&quot;&gt;Punit Rathore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1&quot;&gt;Dheeraj Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajasegarar_S/0/1/0/all/0/1&quot;&gt;Sutharshan Rajasegarar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palaniswami_M/0/1/0/all/0/1&quot;&gt;Marimuthu Palaniswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bezdek_J/0/1/0/all/0/1&quot;&gt;James C. Bezdek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03600">
<title>ML + FV = $\heartsuit$? A Survey on the Application of Machine Learning to Formal Verification. (arXiv:1806.03600v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1806.03600</link>
<description rdf:parseType="Literal">&lt;p&gt;Formal Verification (FV) and Machine Learning (ML) can seem incompatible due
to their opposite mathematical foundations and their use in real-life problems:
FV mostly relies on discrete mathematics and aims at ensuring correctness; ML
often relies on probabilistic models and consists of learning patterns from
training data. In this paper, we postulate that they are complementary in
practice, and explore how ML helps FV in its classical approaches: static
analysis, model-checking, theorem-proving, and SAT solving. We draw a landscape
of the current practice and catalog some of the most prominent uses of ML
inside FV tools, thus offering a new perspective on FV techniques that can help
researchers and practitioners to better locate the possible synergies. We
discuss lessons learned from our work, point to possible improvements and offer
visions for the future of the domain in the light of the science of software
and systems modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amrani_M/0/1/0/all/0/1&quot;&gt;Moussa Amrani&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucio_L/0/1/0/all/0/1&quot;&gt;Levi L&amp;#xf9;cio&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bibal_A/0/1/0/all/0/1&quot;&gt;Adrien Bibal&lt;/a&gt; (1) (University of Namur, Faculty of Computer Science, PReCiSE / NaDI, Namur, Belgium (2) fortiss GmbH, M&amp;#xfc;nchen, Germany)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03820">
<title>An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning. (arXiv:1806.03820v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.03820</link>
<description rdf:parseType="Literal">&lt;p&gt;Our goal is for AI systems to correctly identify and act according to their
human user&apos;s objectives. Cooperative Inverse Reinforcement Learning (CIRL)
formalizes this value alignment problem as a two-player game between a human
and robot, in which only the human knows the parameters of the reward function:
the robot needs to learn them as the interaction unfolds. Previous work showed
that CIRL can be solved as a POMDP, but with an action space size exponential
in the size of the reward parameter space. In this work, we exploit a specific
property of CIRL---the human is a full information agent---to derive an
optimality-preserving modification to the standard Bellman update; this reduces
the complexity of the problem by an exponential factor and allows us to relax
CIRL&apos;s assumption of human rationality. We apply this update to a variety of
POMDP solvers and find that it enables us to scale CIRL to non-trivial
problems, with larger reward parameter spaces, and larger action spaces for
both robot and human. In solutions to these larger problems, the human exhibits
pedagogic (teaching) behavior, while the robot interprets it as such and
attains higher value for the human.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_D/0/1/0/all/0/1&quot;&gt;Dhruv Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palaniappan_M/0/1/0/all/0/1&quot;&gt;Malayandi Palaniappan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fisac_J/0/1/0/all/0/1&quot;&gt;Jaime F. Fisac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1&quot;&gt;Dylan Hadfield-Menell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1&quot;&gt;Stuart Russell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1&quot;&gt;Anca D. Dragan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04017">
<title>Sheep identity recognition, age and weight estimation datasets. (arXiv:1806.04017v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.04017</link>
<description rdf:parseType="Literal">&lt;p&gt;Increased interest of scientists, producers and consumers in sheep
identification has been stimulated by the dramatic increase in population and
the urge to increase productivity. The world population is expected to exceed
9.6 million in 2050. For this reason, awareness is raised towards the necessity
of effective livestock production. Sheep is considered as one of the main of
food resources. Most of the research now is directed towards developing real
time applications that facilitate sheep identification for breed management and
gathering related information like weight and age. Weight and age are key
matrices in assessing the effectiveness of production. For this reason, visual
analysis proved recently its significant success over other approaches. Visual
analysis techniques need enough images for testing and study completion. For
this reason, collecting sheep images database is a vital step to fulfill such
objective. We provide here datasets for testing and comparing such algorithms
which are under development. Our collected dataset consists of 416 color images
for different features of sheep in different postures. Images were collected
fifty two sheep at a range of year from three months to six years. For each
sheep, two images were captured for both sides of the body, two images for both
sides of the face, one image from the top view, one image for the hip and one
image for the teeth. The collected images cover different illumination, quality
levels and angle of rotation. The allocated data set can be used to test sheep
identification, weigh estimation, and age detection algorithms. Such algorithms
are crucial for disease management, animal assessment and ownership.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelhady_A/0/1/0/all/0/1&quot;&gt;Aya Salama Abdelhady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanenin_A/0/1/0/all/0/1&quot;&gt;Aboul Ella Hassanenin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fahmy_A/0/1/0/all/0/1&quot;&gt;Aly Fahmy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04067">
<title>Adaptive Mechanism Design: Learning to Promote Cooperation. (arXiv:1806.04067v1 [cs.GT])</title>
<link>http://arxiv.org/abs/1806.04067</link>
<description rdf:parseType="Literal">&lt;p&gt;In the future, artificial learning agents are likely to become increasingly
widespread in our society. They will interact with both other learning agents
and humans in a variety of complex settings including social dilemmas. We
consider the problem of how an external agent can promote cooperation between
artificial learners by distributing additional rewards and punishments based on
observing the learners&apos; actions. We propose a rule for automatically learning
how to create right incentives by considering the players&apos; anticipated
parameter updates. Using this learning rule leads to cooperation with high
social welfare in matrix games in which the agents would otherwise learn to
defect with high probability. We show that the resulting cooperative outcome is
stable in certain games even if the planning agent is turned off after a given
number of episodes, while other games require ongoing intervention to maintain
mutual cooperation. However, even in the latter case, the amount of necessary
additional incentives decreases over time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumann_T/0/1/0/all/0/1&quot;&gt;Tobias Baumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graepel_T/0/1/0/all/0/1&quot;&gt;Thore Graepel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shawe_Taylor_J/0/1/0/all/0/1&quot;&gt;John Shawe-Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.05826">
<title>Capacity Releasing Diffusion for Speed and Locality. (arXiv:1706.05826v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1706.05826</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusions and related random walk procedures are of central importance in
many areas of machine learning, data analysis, and applied mathematics. Because
they spread mass agnostically at each step in an iterative manner, they can
sometimes spread mass &quot;too aggressively,&quot; thereby failing to find the &quot;right&quot;
clusters. We introduce a novel Capacity Releasing Diffusion (CRD) Process,
which is both faster and stays more local than the classical spectral diffusion
process. As an application, we use our CRD Process to develop an improved local
algorithm for graph clustering. Our local graph clustering method can find
local clusters in a model of clustering where one begins the CRD Process in a
cluster whose vertices are connected better internally than externally by an
$O(\log^2 n)$ factor, where $n$ is the number of nodes in the cluster. Thus,
our CRD Process is the first local graph clustering algorithm that is not
subject to the well-known quadratic Cheeger barrier. Our result requires a
certain smoothness condition, which we expect to be an artifact of our
analysis. Our empirical evaluation demonstrates improved results, in particular
for realistic social graphs where there are moderately good---but not very
good---clusters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fountoulakis_K/0/1/0/all/0/1&quot;&gt;Kimon Fountoulakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henzinger_M/0/1/0/all/0/1&quot;&gt;Monika Henzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1&quot;&gt;Michael W. Mahoney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1&quot;&gt;Satish Rao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06481">
<title>Constructing Datasets for Multi-hop Reading Comprehension Across Documents. (arXiv:1710.06481v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06481</link>
<description rdf:parseType="Literal">&lt;p&gt;Most Reading Comprehension methods limit themselves to queries which can be
answered using a single sentence, paragraph, or document. Enabling models to
combine disjoint pieces of textual evidence would extend the scope of machine
comprehension methods, but currently there exist no resources to train and test
this capability. We propose a novel task to encourage the development of models
for text understanding across multiple documents and to investigate the limits
of existing methods. In our task, a model learns to seek and combine evidence -
effectively performing multi-hop (alias multi-step) inference. We devise a
methodology to produce datasets for this task, given a collection of
query-answer pairs and thematically linked documents. Two datasets from
different domains are induced, and we identify potential pitfalls and devise
circumvention strategies. We evaluate two previously proposed competitive
models and find that one can integrate information across documents. However,
both models struggle to select relevant information, as providing documents
guaranteed to be relevant greatly improves their performance. While the models
outperform several strong baselines, their best accuracy reaches 42.9% compared
to human performance at 74.0% - leaving ample room for improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welbl_J/0/1/0/all/0/1&quot;&gt;Johannes Welbl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1&quot;&gt;Pontus Stenetorp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1&quot;&gt;Sebastian Riedel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00851">
<title>Provable defenses against adversarial examples via the convex outer adversarial polytope. (arXiv:1711.00851v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00851</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to learn deep ReLU-based classifiers that are provably
robust against norm-bounded adversarial perturbations on the training data. For
previously unseen examples, the approach is guaranteed to detect all
adversarial examples, though it may flag some non-adversarial examples as well.
The basic idea is to consider a convex outer approximation of the set of
activations reachable through a norm-bounded perturbation, and we develop a
robust optimization procedure that minimizes the worst case loss over this
outer region (via a linear program). Crucially, we show that the dual problem
to this linear program can be represented itself as a deep network similar to
the backpropagation network, leading to very efficient optimization approaches
that produce guaranteed bounds on the robust loss. The end result is that by
executing a few more forward and backward passes through a slightly modified
version of the original network (though possibly with much larger batch sizes),
we can learn a classifier that is provably robust to any norm-bounded
adversarial attack. We illustrate the approach on a number of tasks to train
classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a
convolutional classifier that provably has less than 5.8% test error for any
adversarial attack with bounded $\ell_\infty$ norm less than $\epsilon = 0.1$),
and code for all experiments in the paper is available at
https://github.com/locuslab/convex_adversarial.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1&quot;&gt;Eric Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11027">
<title>Embedding Words as Distributions with a Bayesian Skip-gram Model. (arXiv:1711.11027v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11027</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a method for embedding words as probability densities in a
low-dimensional space. Rather than assuming that a word embedding is fixed
across the entire text collection, as in standard word embedding methods, in
our Bayesian model we generate it from a word-specific prior density for each
occurrence of a given word. Intuitively, for each word, the prior density
encodes the distribution of its potential &apos;meanings&apos;. These prior densities are
conceptually similar to Gaussian embeddings. Interestingly, unlike the Gaussian
embeddings, we can also obtain context-specific densities: they encode
uncertainty about the sense of a word given its context and correspond to
posterior distributions within our model. The context-dependent densities have
many potential applications: for example, we show that they can be directly
used in the lexical substitution task. We describe an effective estimation
method based on the variational autoencoding framework. We also demonstrate
that our embeddings achieve competitive results on standard benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brazinskas_A/0/1/0/all/0/1&quot;&gt;Arthur Bra&amp;#x17e;inskas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havrylov_S/0/1/0/all/0/1&quot;&gt;Serhii Havrylov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1&quot;&gt;Ivan Titov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03333">
<title>Assumed Density Filtering Q-learning. (arXiv:1712.03333v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.03333</link>
<description rdf:parseType="Literal">&lt;p&gt;While off-policy temporal difference (TD) methods have widely been used in
reinforcement learning due to their efficiency and simple implementation, their
Bayesian counterparts have not been utilized as frequently. One reason is that
the non-linear max operation in the Bellman optimality equation makes it
difficult to define conjugate distributions over the value functions. In this
paper, we introduce a novel Bayesian approach to off-policy TD methods using
Assumed Density Filtering (ADFQ), which updates beliefs on state-action values
(Q) through an online Bayesian inference method. Uncertainty measures in the
beliefs provide a natural regularization for learning, and we show how ADFQ
reduces in a limiting case to the traditional Q-learning algorithm. Our
empirical results demonstrate that the proposed ADFQ algorithms outperform
comparable algorithms on several task domains. Moreover, our algorithms are
computationally more efficient than other existing approaches to Bayesian
reinforcement learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1&quot;&gt;Heejin Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Clark Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Daniel D. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09904">
<title>Ab initio Algorithmic Causal Deconvolution of Intertwined Programs and Networks by Generative Mechanism. (arXiv:1802.09904v6 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09904</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex data is usually produced by interacting sources with different
mechanisms. Here we introduce a parameter-free model-based approach, based upon
the seminal concept of Algorithmic Probability, that decomposes an observation
and signal into its most likely algorithmic generative sources. Our methods use
a causal calculus to infer model representations. We demonstrate the method
ability to distinguish interacting mechanisms and deconvolve them, regardless
of whether the objects produce strings, space-time evolution diagrams, images
or networks. We numerically test and evaluate our causal separation methods and
find that it can disentangle examples of observations from discrete dynamical
systems, and complex networks. We think that these causal separating techniques
can contribute to tackle the challenge of causation for estimations of better
rooted probability distributions thereby complementing more limited
statistical-oriented techniques that otherwise would lack model inference
capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenil_H/0/1/0/all/0/1&quot;&gt;Hector Zenil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiani_N/0/1/0/all/0/1&quot;&gt;Narsis A. Kiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zea_A/0/1/0/all/0/1&quot;&gt;Allan A. Zea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1&quot;&gt;Jesper Tegn&amp;#xe9;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00590">
<title>Hierarchical Imitation and Reinforcement Learning. (arXiv:1803.00590v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00590</link>
<description rdf:parseType="Literal">&lt;p&gt;We study how to effectively leverage expert feedback to learn sequential
decision-making policies. We focus on problems with sparse rewards and long
time horizons, which typically pose significant challenges in reinforcement
learning. We propose an algorithmic framework, called hierarchical guidance,
that leverages the hierarchical structure of the underlying problem to
integrate different modes of expert interaction. Our framework can incorporate
different combinations of imitation learning (IL) and reinforcement learning
(RL) at different levels, leading to dramatic reductions in both expert effort
and cost of exploration. Using long-horizon benchmarks, including Montezuma&apos;s
Revenge, we demonstrate that our approach can learn significantly faster than
hierarchical RL, and be significantly more label-efficient than standard IL. We
also theoretically analyze labeling cost for certain instantiations of our
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Hoang M. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Alekh Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1&quot;&gt;Miroslav Dud&amp;#xed;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yisong Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1&quot;&gt;Hal Daum&amp;#xe9; III&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00153">
<title>k-Space Deep Learning for Reference-free EPI Ghost Correction. (arXiv:1806.00153v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00153</link>
<description rdf:parseType="Literal">&lt;p&gt;Nyquist ghost artifacts in EPI images are originated from phase mismatch
between the even and odd echoes. However, conventional correction methods using
reference scans often produce erroneous results especially in high-field MRI
due to the non-linear and time-varying local magnetic field changes. Recently,
it was shown that the problem of ghost correction can be transformed into
k-space data interpolation problem that can be solved using the annihilating
filter-based low-rank Hankel structured matrix completion approach (ALOHA).
Another recent discovery has shown that the deep convolutional neural network
is closely related to the data-driven Hankel matrix decomposition. By
synergistically combining these findings, here we propose a k-space deep
learning approach that immediately corrects the k-space phase mismatch without
a reference scan. Reconstruction results using 7T in vivo data showed that the
proposed reference-free k-space deep learning approach for EPI ghost correction
significantly improves the image quality compared to the existing methods, and
the computing time is several orders of magnitude faster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Juyoung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yoseob Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00806">
<title>k-Space Deep Learning for Parallel MRI: Application to Time-Resolved MR Angiography. (arXiv:1806.00806v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00806</link>
<description rdf:parseType="Literal">&lt;p&gt;Time-resolved angiography with interleaved stochastic trajectories (TWIST)
has been widely used for dynamic contrast enhanced MRI (DCE-MRI). To achieve
highly accelerated acquisitions, TWIST combines the periphery of the k-space
data from several adjacent frames to reconstruct one temporal frame. However,
this view-sharing scheme limits the true temporal resolution of TWIST.
Moreover, the k-space sampling patterns have been specially designed for a
specific generalized autocalibrating partial parallel acquisition (GRAPPA)
factor so that it is not possible to reduce the number of view-sharing once the
k-data is acquired. To address these issues, this paper proposes a novel
k-space deep learning approach for parallel MRI. In particular, we have
designed our neural network so that accurate k-space interpolations are
performed simultaneously for multiple coils by exploiting the redundancies
along the coils and images. Reconstruction results using in vivo TWIST data set
confirm that the proposed method can immediately generate high-quality
reconstruction results with various choices of view- sharing, allowing us to
exploit the trade-off between spatial and temporal resolution in time-resolved
MR angiography.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_E/0/1/0/all/0/1&quot;&gt;Eunju Cha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1&quot;&gt;Eung Yeop Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01984">
<title>Learning to rank for censored survival data. (arXiv:1806.01984v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01984</link>
<description rdf:parseType="Literal">&lt;p&gt;Survival analysis is a type of semi-supervised ranking task where the target
output (the survival time) is often right-censored. Utilizing this information
is a challenge because it is not obvious how to correctly incorporate these
censored examples into a model. We study how three categories of loss
functions, namely partial likelihood methods, rank methods, and our
classification method based on a Wasserstein metric (WM) and the non-parametric
Kaplan Meier estimate of the probability density to impute the labels of
censored examples, can take advantage of this information. The proposed method
allows us to have a model that predict the probability distribution of an
event. If a clinician had access to the detailed probability of an event over
time this would help in treatment planning. For example, determining if the
risk of kidney graft rejection is constant or peaked after some time. Also, we
demonstrate that this approach directly optimizes the expected C-index which is
the most common evaluation metric for ranking survival models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luck_M/0/1/0/all/0/1&quot;&gt;Margaux Luck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sylvain_T/0/1/0/all/0/1&quot;&gt;Tristan Sylvain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1&quot;&gt;Joseph Paul Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardinal_H/0/1/0/all/0/1&quot;&gt;Heloise Cardinal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lodi_A/0/1/0/all/0/1&quot;&gt;Andrea Lodi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.03035">
<title>Context-Aware Adaptive Framework for e-Health Monitoring. (arXiv:1605.03035v1 [cs.CY] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1605.03035</link>
<description rdf:parseType="Literal">&lt;p&gt;For improving e-health services, we propose a context-aware framework to
monitor the activities of daily living of dependent persons. We define a
strategy for generating long-term realistic scenarios and a framework
containing an adaptive monitoring algorithm based on three approaches for
optimizing resource usage. The used approaches provide a deep knowledge about
the person&apos;s context by considering: the person&apos;s profile, the activities and
the relationships between activities. We evaluate the performances of our
framework and show its adaptability and significant reduction in network,
energy and processing usage over a traditional monitoring implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mshali_H/0/1/0/all/0/1&quot;&gt;Haider Mshali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemlouma_T/0/1/0/all/0/1&quot;&gt;Tayeb Lemlouma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magoni_D/0/1/0/all/0/1&quot;&gt;Damien Magoni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03342">
<title>Discovering Signals from Web Sources to Predict Cyber Attacks. (arXiv:1806.03342v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03342</link>
<description rdf:parseType="Literal">&lt;p&gt;Cyber attacks are growing in frequency and severity. Over the past year alone
we have witnessed massive data breaches that stole personal information of
millions of people and wide-scale ransomware attacks that paralyzed critical
infrastructure of several countries. Combating the rising cyber threat calls
for a multi-pronged strategy, which includes predicting when these attacks will
occur. The intuition driving our approach is this: during the planning and
preparation stages, hackers leave digital traces of their activities on both
the surface web and dark web in the form of discussions on platforms like
hacker forums, social media, blogs and the like. These data provide predictive
signals that allow anticipating cyber attacks. In this paper, we describe
machine learning techniques based on deep neural networks and autoregressive
time series models that leverage external signals from publicly available Web
sources to forecast cyber attacks. Performance of our framework across ground
truth data over real-world forecasting tasks shows that our methods yield a
significant lift or increase of F1 for the top signals on predicted cyber
attacks. Our results suggest that, when deployed, our system will be able to
provide an effective line of defense against various types of targeted cyber
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1&quot;&gt;Palash Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1&quot;&gt;KSM Tozammel Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deb_A/0/1/0/all/0/1&quot;&gt;Ashok Deb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavabi_N/0/1/0/all/0/1&quot;&gt;Nazgol Tavabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartley_N/0/1/0/all/0/1&quot;&gt;Nathan Bartley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abeliuk_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#x27;es Abeliuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1&quot;&gt;Emilio Ferrara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1&quot;&gt;Kristina Lerman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03349">
<title>An Optimal Algorithm for Online Unconstrained Submodular Maximization. (arXiv:1806.03349v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03349</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a basic problem at the interface of two fundamental fields:
submodular optimization and online learning. In the online unconstrained
submodular maximization (online USM) problem, there is a universe
$[n]=\{1,2,...,n\}$ and a sequence of $T$ nonnegative (not necessarily
monotone) submodular functions arrive over time. The goal is to design a
computationally efficient online algorithm, which chooses a subset of $[n]$ at
each time step as a function only of the past, such that the accumulated value
of the chosen subsets is as close as possible to the maximum total value of a
fixed subset in hindsight. Our main result is a polynomial-time no-$1/2$-regret
algorithm for this problem, meaning that for every sequence of nonnegative
submodular functions, the algorithm&apos;s expected total value is at least $1/2$
times that of the best subset in hindsight, up to an error term sublinear in
$T$. The factor of $1/2$ cannot be improved upon by any polynomial-time online
algorithm when the submodular functions are presented as value oracles.
Previous work on the offline problem implies that picking a subset uniformly at
random in each time step achieves zero $1/4$-regret.
&lt;/p&gt;
&lt;p&gt;A byproduct of our techniques is an explicit subroutine for the two-experts
problem that has an unusually strong regret guarantee: the total value of its
choices is comparable to twice the total value of either expert on rounds it
did not pick that expert. This subroutine may be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roughgarden_T/0/1/0/all/0/1&quot;&gt;Tim Roughgarden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Joshua R. Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03404">
<title>Deterministic Stretchy Regression. (arXiv:1806.03404v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03404</link>
<description rdf:parseType="Literal">&lt;p&gt;An extension of the regularized least-squares in which the estimation
parameters are stretchable is introduced and studied in this paper. The
solution of this ridge regression with stretchable parameters is given in
primal and dual spaces and in closed-form. Essentially, the proposed solution
stretches the covariance computation by a power term, thereby compressing or
amplifying the estimation parameters. To maintain the computation of power root
terms within the real space, an input transformation is proposed. The results
of an empirical evaluation in both synthetic and real-world data illustrate
that the proposed method is effective for compressive learning with
high-dimensional data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toh_K/0/1/0/all/0/1&quot;&gt;Kar-Ann Toh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhiping Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03430">
<title>Efficient Optimization Algorithms for Robust Principal Component Analysis and Its Variants. (arXiv:1806.03430v1 [math.OC])</title>
<link>http://arxiv.org/abs/1806.03430</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust PCA has drawn significant attention in the last decade due to its
success in numerous application domains, ranging from bio-informatics,
statistics, and machine learning to image and video processing in computer
vision. Robust PCA and its variants such as sparse PCA and stable PCA can be
formulated as optimization problems with exploitable special structures. Many
specialized efficient optimization methods have been proposed to solve robust
PCA and related problems. In this paper we review existing optimization methods
for solving convex and nonconvex relaxations/variants of robust PCA, discuss
their advantages and disadvantages, and elaborate on their convergence
behaviors. We also provide some insights for possible future research
directions including new algorithmic frameworks that might be suitable for
implementing on multi-processor setting to handle large-scale problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shiqian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Aybat_N/0/1/0/all/0/1&quot;&gt;Necdet Serhat Aybat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03432">
<title>Hierarchical Clustering with Prior Knowledge. (arXiv:1806.03432v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03432</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical clustering is a class of algorithms that seeks to build a
hierarchy of clusters. It has been the dominant approach to constructing
embedded classification schemes since it outputs dendrograms, which capture the
hierarchical relationship among members at all levels of granularity,
simultaneously. Being greedy in the algorithmic sense, a hierarchical
clustering partitions data at every step solely based on a similarity /
dissimilarity measure. The clustering results oftentimes depend on not only the
distribution of the underlying data, but also the choice of dissimilarity
measure and the clustering algorithm. In this paper, we propose a method to
incorporate prior domain knowledge about entity relationship into the
hierarchical clustering. Specifically, we use a distance function in
ultrametric space to encode the external ontological information. We show that
popular linkage-based algorithms can faithfully recover the encoded structure.
Similar to some regularized machine learning techniques, we add this distance
as a penalty term to the original pairwise distance to regulate the final
structure of the dendrogram. As a case study, we applied this method on real
data in the building of a customer behavior based product taxonomy for an
Amazon service, leveraging the information from a larger Amazon-wide browse
structure. The method is useful when one wants to leverage the relational
information from external sources, or the data used to generate the distance
matrix is noisy and sparse. Our work falls in the category of semi-supervised
or constrained clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaofei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhavala_S/0/1/0/all/0/1&quot;&gt;Satya Dhavala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03461">
<title>TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service. (arXiv:1806.03461v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1806.03461</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning methods are widely used for a variety of prediction
problems. \emph{Prediction as a service} is a paradigm in which service
providers with technological expertise and computational resources may perform
predictions for clients. However, data privacy severely restricts the
applicability of such services, unless measures to keep client data private
(even from the service provider) are designed. Equally important is to minimize
the amount of computation and communication required between client and server.
Fully homomorphic encryption offers a possible way out, whereby clients may
encrypt their data, and on which the server may perform arithmetic
computations. The main drawback of using fully homomorphic encryption is the
amount of time required to evaluate large machine learning models on encrypted
data. We combine ideas from the machine learning literature, particularly work
on binarization and sparsification of neural networks, together with
algorithmic tools to speed-up and parallelize computation using encrypted data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1&quot;&gt;Amartya Sanyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1&quot;&gt;Matt J. Kusner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gascon_A/0/1/0/all/0/1&quot;&gt;Adri&amp;#xe0; Gasc&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanade_V/0/1/0/all/0/1&quot;&gt;Varun Kanade&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03467">
<title>Orthogonal Random Forest for Heterogeneous Treatment Effect Estimation. (arXiv:1806.03467v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03467</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of estimating heterogeneous treatment effects from
observational data, where the treatment policy on the collected data was
determined by potentially many confounding observable variables. We propose
orthogonal random forest1, an algorithm that combines orthogonalization, a
technique that effectively removes the confounding effect in two-stage
estimation, with generalized random forests [Athey et al., 2017], a flexible
method for estimating treatment effect heterogeneity. We prove a consistency
rate result of our estimator in the partially linear regression model, and en
route we provide a consistency analysis for a general framework of performing
generalized method of moments (GMM) estimation. We also provide a comprehensive
empirical evaluation of our algorithms, and show that they consistently
outperform baseline approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oprescu_M/0/1/0/all/0/1&quot;&gt;Miruna Oprescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Steven Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03482">
<title>DIR-ST$^2$: Delineation of Imprecise Regions Using Spatio--Temporal--Textual Information. (arXiv:1806.03482v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1806.03482</link>
<description rdf:parseType="Literal">&lt;p&gt;An imprecise region is referred to as a geographical area without a
clearly-defined boundary in the literature. Previous clustering-based
approaches exploit spatial information to find such regions. However, the prior
studies suffer from the following two problems: the subjectivity in selecting
clustering parameters and the inclusion of a large portion of the undesirable
region (i.e., a large number of noise points). To overcome these problems, we
present DIR-ST$^2$, a novel framework for delineating an imprecise region by
iteratively performing density-based clustering, namely DBSCAN, along with not
only spatio--textual information but also temporal information on social media.
Specifically, we aim at finding a proper radius of a circle used in the
iterative DBSCAN process by gradually reducing the radius for each iteration in
which the temporal information acquired from all resulting clusters are
leveraged. Then, we propose an efficient and automated algorithm delineating
the imprecise region via hierarchical clustering. Experiment results show that
by virtue of the significant noise reduction in the region, our DIR-ST$^2$
method outperforms the state-of-the-art approach employing one-class support
vector machine in terms of the $\mathcal{F}_1$ score from comparison with
precisely-defined regions regarded as a ground truth, and returns apparently
better delineation of imprecise regions. The computational complexity of
DIR-ST$^2$ is also analytically and numerically shown.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1&quot;&gt;Cong Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1&quot;&gt;Won-Yong Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Sang-Il Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03547">
<title>Linear Spectral Estimators and an Application to Phase Retrieval. (arXiv:1806.03547v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1806.03547</link>
<description rdf:parseType="Literal">&lt;p&gt;Phase retrieval refers to the problem of recovering real- or complex-valued
vectors from magnitude measurements. The best-known algorithms for this problem
are iterative in nature and rely on so-called spectral initializers that
provide accurate initialization vectors. We propose a novel class of estimators
suitable for general nonlinear measurement systems, called linear spectral
estimators (LSPEs), which can be used to compute accurate initialization
vectors for phase retrieval problems. The proposed LSPEs not only provide
accurate initialization vectors for noisy phase retrieval systems with
structured or random measurement matrices, but also enable the derivation of
sharp and nonasymptotic mean-squared error bounds. We demonstrate the efficacy
of LSPEs on synthetic and real-world phase retrieval problems, and show that
our estimators significantly outperform existing methods for structured
measurement systems that arise in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghods_R/0/1/0/all/0/1&quot;&gt;Ramina Ghods&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1&quot;&gt;Andrew S. Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Studer_C/0/1/0/all/0/1&quot;&gt;Christoph Studer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03551">
<title>An Estimation and Analysis Framework for the Rasch Model. (arXiv:1806.03551v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03551</link>
<description rdf:parseType="Literal">&lt;p&gt;The Rasch model is widely used for item response analysis in applications
ranging from recommender systems to psychology, education, and finance. While a
number of estimators have been proposed for the Rasch model over the last
decades, the available analytical performance guarantees are mostly asymptotic.
This paper provides a framework that relies on a novel linear minimum
mean-squared error (L-MMSE) estimator which enables an exact, nonasymptotic,
and closed-form analysis of the parameter estimation error under the Rasch
model. The proposed framework provides guidelines on the number of items and
responses required to attain low estimation errors in tests or surveys. We
furthermore demonstrate its efficacy on a number of real-world collaborative
filtering datasets, which reveals that the proposed L-MMSE estimator performs
on par with state-of-the-art nonlinear estimators in terms of predictive
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lan_A/0/1/0/all/0/1&quot;&gt;Andrew S. Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chiang_M/0/1/0/all/0/1&quot;&gt;Mung Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Studer_C/0/1/0/all/0/1&quot;&gt;Christoph Studer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03555">
<title>Consistent Position Bias Estimation without Online Interventions for Learning-to-Rank. (arXiv:1806.03555v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03555</link>
<description rdf:parseType="Literal">&lt;p&gt;Presentation bias is one of the key challenges when learning from implicit
feedback in search engines, as it confounds the relevance signal with
uninformative signals due to position in the ranking, saliency, and other
presentation factors. While it was recently shown how counterfactual
learning-to-rank (LTR) approaches \cite{Joachims/etal/17a} can provably
overcome presentation bias if observation propensities are known, it remains to
show how to accurately estimate these propensities. In this paper, we propose
the first method for producing consistent propensity estimates without manual
relevance judgments, disruptive interventions, or restrictive relevance
modeling assumptions. We merely require that we have implicit feedback data
from multiple different ranking functions. Furthermore, we argue that our
estimation technique applies to an extended class of Contextual Position-Based
Propensity Models, where propensities not only depend on position but also on
observable features of the query and document. Initial simulation studies
confirm that the approach is scalable, accurate, and robust.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Aman Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaitsev_I/0/1/0/all/0/1&quot;&gt;Ivan Zaitsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1&quot;&gt;Thorsten Joachims&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03571">
<title>Stationary Geometric Graphical Model Selection. (arXiv:1806.03571v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03571</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of model selection in Gaussian Markov fields in the
sample deficient scenario. In many cases, the underlying networks are embedded
into Euclidean spaces which induces significant structure on them. Using this
natural spatial structure, we introduce the notion of spatially stationary
distributions over geometric graphs directly generalizing the notion of
stationary time series to the multidimensional setup lacking time axis. We show
that the idea of spatial stationarity leads to a dramatic decrease in the
sample complexity of the model selection compared to abstract graphs with the
same level of sparsity. For geometric graphs on randomly spread vertices and
edges of bounded length, we develop tight information-theoretic bounds on the
sample complexity and show that a finite number of independent samples is
sufficient for a consistent recovery. Finally, we develop an efficient
technique capable of reliably and consistently reconstructing graphs with a
bounded number of measurements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soloveychik_I/0/1/0/all/0/1&quot;&gt;Ilya Soloveychik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tarokh_V/0/1/0/all/0/1&quot;&gt;Vahid Tarokh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03664">
<title>Conditional Noise-Contrastive Estimation of Unnormalised Models. (arXiv:1806.03664v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03664</link>
<description rdf:parseType="Literal">&lt;p&gt;Many parametric statistical models are not properly normalised and only
specified up to an intractable partition function, which renders parameter
estimation difficult. Examples of unnormalised models are Gibbs distributions,
Markov random fields, and neural network models in unsupervised deep learning.
In previous work, the estimation principle called noise-contrastive estimation
(NCE) was introduced where unnormalised models are estimated by learning to
distinguish between data and auxiliary noise. An open question is how to best
choose the auxiliary noise distribution. We here propose a new method that
addresses this issue. The proposed method shares with NCE the idea of
formulating density estimation as a supervised learning problem but in contrast
to NCE, the proposed method leverages the observed data when generating noise
samples. The noise can thus be generated in a semi-automated manner. We first
present the underlying theory of the new method, show that score matching
emerges as a limiting case, validate the method on continuous and discrete
valued synthetic data, and show that we can expect an improved performance
compared to NCE when the data lie in a lower-dimensional manifold. Then we
demonstrate its applicability in unsupervised deep learning by estimating a
four-layer neural image model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ceylan_C/0/1/0/all/0/1&quot;&gt;Ciwan Ceylan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gutmann_M/0/1/0/all/0/1&quot;&gt;Michael U. Gutmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03677">
<title>Dissipativity Theory for Accelerating Stochastic Variance Reduction: A Unified Analysis of SVRG and Katyusha Using Semidefinite Programs. (arXiv:1806.03677v1 [math.OC])</title>
<link>http://arxiv.org/abs/1806.03677</link>
<description rdf:parseType="Literal">&lt;p&gt;Techniques for reducing the variance of gradient estimates used in stochastic
programming algorithms for convex finite-sum problems have received a great
deal of attention in recent years. By leveraging dissipativity theory from
control, we provide a new perspective on two important variance-reduction
algorithms: SVRG and its direct accelerated variant Katyusha. Our perspective
provides a physically intuitive understanding of the behavior of SVRG-like
methods via a principle of energy conservation. The tools discussed here allow
us to automate the convergence analysis of SVRG-like methods by capturing their
essential properties in small semidefinite programs amenable to standard
analysis and computational techniques. Our approach recovers existing
convergence results for SVRG and Katyusha and generalizes the theory to
alternative parameter choices. We also discuss how our approach complements the
linear coupling technique. Our combination of perspectives leads to a better
understanding of accelerated variance-reduced stochastic methods for finite-sum
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wright_S/0/1/0/all/0/1&quot;&gt;Stephen Wright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lessard_L/0/1/0/all/0/1&quot;&gt;Laurent Lessard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03688">
<title>LexNLP: Natural language processing and information extraction for legal and regulatory texts. (arXiv:1806.03688v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1806.03688</link>
<description rdf:parseType="Literal">&lt;p&gt;LexNLP is an open source Python package focused on natural language
processing and machine learning for legal and regulatory text. The package
includes functionality to (i) segment documents, (ii) identify key text such as
titles and section headings, (iii) extract over eighteen types of structured
information like distances and dates, (iv) extract named entities such as
companies and geopolitical entities, (v) transform text into features for model
training, and (vi) build unsupervised and supervised models such as word
embedding or tagging models. LexNLP includes pre-trained models based on
thousands of unit tests drawn from real documents available from the SEC EDGAR
database as well as various judicial and regulatory proceedings. LexNLP is
designed for use in both academic research and industrial applications, and is
distributed at https://github.com/LexPredict/lexpredict-lexnlp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bommarito_M/0/1/0/all/0/1&quot;&gt;Michael J Bommarito II&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katz_D/0/1/0/all/0/1&quot;&gt;Daniel Martin Katz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Detterman_E/0/1/0/all/0/1&quot;&gt;Eric M Detterman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03763">
<title>Smoothed analysis of the low-rank approach for smooth semidefinite programs. (arXiv:1806.03763v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.03763</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider semidefinite programs (SDPs) of size n with equality constraints.
In order to overcome scalability issues, Burer and Monteiro proposed a
factorized approach based on optimizing over a matrix Y of size $n$ by $k$ such
that $X = YY^*$ is the SDP variable. The advantages of such formulation are
twofold: the dimension of the optimization variable is reduced and positive
semidefiniteness is naturally enforced. However, the problem in Y is
non-convex. In prior work, it has been shown that, when the constraints on the
factorized variable regularly define a smooth manifold, provided k is large
enough, for almost all cost matrices, all second-order stationary points
(SOSPs) are optimal. Importantly, in practice, one can only compute points
which approximately satisfy necessary optimality conditions, leading to the
question: are such points also approximately optimal? To this end, and under
similar assumptions, we use smoothed analysis to show that approximate SOSPs
for a randomly perturbed objective function are approximate global optima, with
k scaling like the square root of the number of constraints (up to log
factors). We particularize our results to an SDP relaxation of phase retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pumir_T/0/1/0/all/0/1&quot;&gt;Thomas Pumir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jelassi_S/0/1/0/all/0/1&quot;&gt;Samy Jelassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boumal_N/0/1/0/all/0/1&quot;&gt;Nicolas Boumal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03803">
<title>Chaining Mutual Information and Tightening Generalization Bounds. (arXiv:1806.03803v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03803</link>
<description rdf:parseType="Literal">&lt;p&gt;Bounding the generalization error of learning algorithms has a long history,
that yet falls short in explaining various generalization successes including
those of deep learning. Two important difficulties are (i) exploiting the
dependencies between the hypotheses, (ii) exploiting the dependence between the
algorithm&apos;s input and output. Progress on the first point was made with the
chaining method, originating from the work of Kolmogorov and used in the
VC-dimension bound. More recently, progress on the second point was made with
the mutual information method by Russo and Zou &apos;15. Yet, these two methods are
currently disjoint. In this paper, we introduce a technique to combine chaining
and mutual information methods, to obtain a generalization bound that is both
algorithm-dependent and that exploits the dependencies between the hypotheses.
We provide an example in which our bound significantly outperforms both the
chaining and the mutual information bounds. As a corollary, we tighten Dudley
inequality under the knowledge that a learning algorithm chooses its output
from a small subset of hypotheses with high probability; an assumption
motivated by the performance of SGD discussed in Zhang et al. &apos;17.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asadi_A/0/1/0/all/0/1&quot;&gt;Amir R. Asadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbe_E/0/1/0/all/0/1&quot;&gt;Emmanuel Abbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verdu_S/0/1/0/all/0/1&quot;&gt;Sergio Verd&amp;#xfa;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03816">
<title>Adaptive MCMC via Combining Local Samplers. (arXiv:1806.03816v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03816</link>
<description rdf:parseType="Literal">&lt;p&gt;Markov chain Monte Carlo (MCMC) methods are widely used in machine learning.
One of the major problems with MCMC is the question of how to design chains
that mix fast over the whole space; in particular, how to select the parameters
of an MCMC algorithm. Here we take a different approach and, instead of trying
to find a single chain to sample from the whole distribution, we combine
samples from several chains run in parallel, each exploring only a few modes.
The chains are prioritized based on Stein discrepancy, which provides a good
measure of performance locally. We present a new method, based on estimating
the R\&apos;enyi entropy of subsets of the samples, to combine the samples coming
from the different samplers. The resulting algorithm is asymptotically
consistent and may lead to significant speedups, especially for multimodal
target functions, as demonstrated by our experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaloudegi_K/0/1/0/all/0/1&quot;&gt;Kiarash Shaloudegi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe1;s Gy&amp;#xf6;rgy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03836">
<title>Bayesian Model-Agnostic Meta-Learning. (arXiv:1806.03836v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03836</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to infer Bayesian posterior from a few-shot dataset is an important
step towards robust meta-learning due to the model uncertainty inherent in the
problem. In this paper, we propose a novel Bayesian model-agnostic
meta-learning method. The proposed method combines scalable gradient-based
meta-learning with nonparametric variational inference in a principled
probabilistic framework. During fast adaptation, the method is capable of
learning complex uncertainty structure beyond a point estimate or a simple
Gaussian approximation. In addition, a robust Bayesian meta-update mechanism
with a new meta-loss prevents overfitting during meta-update. Remaining an
efficient gradient-based meta-learner, the method is also model-agnostic and
simple to implement. Experiment results show the accuracy and robustness of the
proposed method in various tasks: sinusoidal regression, image classification,
active learning, and reinforcement learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taesup Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaesik Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dia_O/0/1/0/all/0/1&quot;&gt;Ousmane Dia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sungwoong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1&quot;&gt;Sungjin Ahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03884">
<title>Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis. (arXiv:1806.03884v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.03884</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimization algorithms that leverage gradient covariance information, such
as variants of natural gradient descent (Amari, 1998), offer the prospect of
yielding more effective descent directions. For models with many parameters,
the covariance matrix they are based on becomes gigantic, making them
inapplicable in their original form. This has motivated research into both
simple diagonal approximations and more sophisticated factored approximations
such as KFAC (Heskes, 2000; Martens &amp;amp; Grosse, 2015; Grosse &amp;amp; Martens, 2016). In
the present work we draw inspiration from both to propose a novel approximation
that is provably better than KFAC and amendable to cheap partial updates. It
consists in tracking a diagonal variance, not in parameter coordinates, but in
a Kronecker-factored eigenbasis, in which the diagonal approximation is likely
to be more effective. Experiments show improvements over KFAC in optimization
speed for several deep network architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1&quot;&gt;Thomas George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laurent_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;sar Laurent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouthillier_X/0/1/0/all/0/1&quot;&gt;Xavier Bouthillier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1&quot;&gt;Nicolas Ballas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1&quot;&gt;Pascal Vincent&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03962">
<title>Rotation Equivariant CNNs for Digital Pathology. (arXiv:1806.03962v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.03962</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new model for digital pathology segmentation, based on the
observation that histopathology images are inherently symmetric under rotation
and reflection. Utilizing recent findings on rotation equivariant CNNs, the
proposed model leverages these symmetries in a principled manner. We present a
visual analysis showing improved stability on predictions, and demonstrate that
exploiting rotation equivariance significantly improves tumor detection
performance on a challenging lymph node metastases dataset. We further present
a novel derived dataset to enable principled comparison of machine learning
models, in combination with an initial benchmark. Through this dataset, the
task of histopathology diagnosis becomes accessible as a challenging benchmark
for fundamental machine learning research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeling_B/0/1/0/all/0/1&quot;&gt;Bastiaan S. Veeling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linmans_J/0/1/0/all/0/1&quot;&gt;Jasper Linmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winkens_J/0/1/0/all/0/1&quot;&gt;Jim Winkens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1&quot;&gt;Taco Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04028">
<title>Adaptive Denoising of Signals with Shift-Invariant Structure. (arXiv:1806.04028v1 [math.ST])</title>
<link>http://arxiv.org/abs/1806.04028</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of discrete-time signal denoising, following the line of
research initiated by [Nem91] and further developed in [JN09, JN10, HJNO15,
OHJN16]. Previous papers considered the following setup: the signal is assumed
to admit a convolution-type linear oracle -- an unknown linear estimator in the
form of the convolution of the observations with an unknown time-invariant
filter with small $\ell_2$-norm. It was shown that such an oracle can be
&quot;mimicked&quot; by an efficiently computable non-linear convolution-type estimator,
in which the filter minimizes the Fourier-domain $\ell_\infty$-norm of the
residual, regularized by the Fourier-domain $\ell_1$-norm of the filter.
Following [OHJN16], here we study an alternative family of estimators,
replacing the $\ell_\infty$-norm of the residual with the $\ell_2$-norm. Such
estimators are found to have better statistical properties, in particular, we
prove sharp oracle inequalities for their $\ell_2$-loss. Our guarantees require
an extra assumption of approximate shift-invariance: the signal must be
$\varkappa$-close, in $\ell_2$-metric, to some shift-invariant linear subspace
with bounded dimension $s$. However, this subspace can be completely unknown,
and the remainder terms in the oracle inequalities scale at most polynomially
with $s$ and $\varkappa$. In conclusion, we show that the new assumption
implies the previously considered one, providing explicit constructions of the
convolution-type linear oracles with $\ell_2$-norm bounded in terms of
parameters $s$ and $\varkappa$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ostrovskii_D/0/1/0/all/0/1&quot;&gt;Dmitrii Ostrovskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Harchaoui_Z/0/1/0/all/0/1&quot;&gt;Zaid Harchaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Juditsky_A/0/1/0/all/0/1&quot;&gt;Anatoli Juditsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nemirovski_A/0/1/0/all/0/1&quot;&gt;Arkadi Nemirovski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04037">
<title>Neonatal EEG Interpretation and Decision Support Framework for Mobile Platforms. (arXiv:1806.04037v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1806.04037</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes and implements an intuitive and pervasive solution for
neonatal EEG monitoring assisted by sonification and deep learning AI that
provides information about neonatal brain health to all neonatal healthcare
professionals, particularly those without EEG interpretation expertise. The
system aims to increase the demographic of clinicians capable of diagnosing
abnormalities in neonatal EEG. The proposed system uses a low-cost and
low-power EEG acquisition system. An Android app provides single-channel EEG
visualization, traffic-light indication of the presence of neonatal seizures
provided by a trained, deep convolutional neural network and an algorithm for
EEG sonification, designed to facilitate the perception of changes in EEG
morphology specific to neonatal seizures. The multifaceted EEG interpretation
framework is presented and the implemented mobile platform architecture is
analyzed with respect to its power consumption and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+OSullivan_M/0/1/0/all/0/1&quot;&gt;Mark O&amp;#x27;Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gomez_S/0/1/0/all/0/1&quot;&gt;Sergi Gomez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+OShea_A/0/1/0/all/0/1&quot;&gt;Alison O&amp;#x27;Shea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Salgado_E/0/1/0/all/0/1&quot;&gt;Eduard Salgado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Huillca_K/0/1/0/all/0/1&quot;&gt;Kevin Huillca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mathieson_S/0/1/0/all/0/1&quot;&gt;Sean Mathieson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Boylan_G/0/1/0/all/0/1&quot;&gt;Geraldine Boylan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Popovici_E/0/1/0/all/0/1&quot;&gt;Emanuel Popovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Temko_A/0/1/0/all/0/1&quot;&gt;Andriy Temko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04047">
<title>High Dimensional Data Enrichment: Interpretable, Fast, and Data-Efficient. (arXiv:1806.04047v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.04047</link>
<description rdf:parseType="Literal">&lt;p&gt;High dimensional structured data enriched model describes groups of
observations by shared and per-group individual parameters, each with its own
structure such as sparsity or group sparsity. In this paper, we consider the
general form of data enrichment where data comes in a fixed but arbitrary
number of groups G. Any convex function, e.g., norms, can characterize the
structure of both shared and individual parameters. We propose an estimator for
high dimensional data enriched model and provide conditions under which it
consistently estimates both shared and individual parameters. We also delineate
sample complexity of the estimator and present high probability non-asymptotic
bound on estimation error of all parameters. Interestingly the sample
complexity of our estimator translates to conditions on both per-group sample
sizes and the total number of samples. We propose an iterative estimation
algorithm with linear convergence rate and supplement our theoretical analysis
with synthetic and real experimental results. Particularly, we show the
predictive power of data-enriched model along with its interpretable results in
anticancer drug sensitivity analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+T%2E_A/0/1/0/all/0/1&quot;&gt;Amir Asiaee T.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oyman_S/0/1/0/all/0/1&quot;&gt;Samet Oyman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Coombes_K/0/1/0/all/0/1&quot;&gt;Kevin R. Coombes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Arindam Banerjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04090">
<title>ATOMO: Communication-efficient Learning via Atomic Sparsification. (arXiv:1806.04090v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.04090</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed model training suffers from communication overheads due to
frequent gradient updates transmitted between compute nodes. To mitigate these
overheads, several studies propose the use of sparsified stochastic gradients.
We argue that these are facets of a general sparsification method that can
operate on any possible atomic decomposition. Notable examples include
element-wise, singular value, and Fourier decompositions. We present ATOMO, a
general framework for atomic sparsification of stochastic gradients. Given a
gradient, an atomic decomposition, and a sparsity budget, ATOMO gives a random
unbiased sparsification of the atoms minimizing variance. We show that methods
such as QSGD and TernGrad are special cases of ATOMO and show that sparsifiying
gradients in their singular value decomposition (SVD), rather than the
coordinate-wise one, can lead to significantly faster distributed training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sievert_S/0/1/0/all/0/1&quot;&gt;Scott Sievert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Charles_Z/0/1/0/all/0/1&quot;&gt;Zachary Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papailiopoulos_D/0/1/0/all/0/1&quot;&gt;Dimitris Papailiopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wright_S/0/1/0/all/0/1&quot;&gt;Stephen Wright&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.03186">
<title>Low-Rank Inducing Norms with Optimality Interpretations. (arXiv:1612.03186v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1612.03186</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimization problems with rank constraints appear in many diverse fields
such as control, machine learning and image analysis. Since the rank constraint
is non-convex, these problems are often approximately solved via convex
relaxations. Nuclear norm regularization is the prevailing convexifying
technique for dealing with these types of problem. This paper introduces a
family of low-rank inducing norms and regularizers which includes the nuclear
norm as a special case. A posteriori guarantees on solving an underlying rank
constrained optimization problem with these convex relaxations are provided. We
evaluate the performance of the low-rank inducing norms on three matrix
completion problems. In all examples, the nuclear norm heuristic is
outperformed by convex relaxations based on other low-rank inducing norms. For
two of the problems there exist low-rank inducing norms that succeed in
recovering the partially unknown matrix, while the nuclear norm fails. These
low-rank inducing norms are shown to be representable as semi-definite
programs. Moreover, these norms have cheaply computable proximal mappings,
which makes it possible to also solve problems of large size using first-order
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Grussler_C/0/1/0/all/0/1&quot;&gt;Christian Grussler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Giselsson_P/0/1/0/all/0/1&quot;&gt;Pontus Giselsson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.08159">
<title>McKernel: A Library for Approximate Kernel Expansions in Log-linear Time. (arXiv:1702.08159v9 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1702.08159</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel Methods Next Generation (KMNG) introduces a framework to use kernel
approximates in the mini-batch setting with SGD Optimizer as an alternative to
Deep Learning. McKernel is a C++ library for KMNG ML Large-scale. It contains a
CPU optimized implementation of the Fastfood algorithm that allows the
computation of approximated kernel expansions in log-linear time. The algorithm
requires to compute the product of Walsh Hadamard Transform (WHT) matrices. A
cache friendly SIMD Fast Walsh Hadamard Transform (FWHT) that achieves
compelling speed and outperforms current state-of-the-art methods has been
developed. McKernel allows to obtain non-linear classification combining
Fastfood and a linear classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Curto_J/0/1/0/all/0/1&quot;&gt;Joachim D. Curt&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zarza_I/0/1/0/all/0/1&quot;&gt;Irene C. Zarza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Feng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1&quot;&gt;Alexander J. Smola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1&quot;&gt;Fernando De La Torre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1&quot;&gt;Chong-Wah Ngo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.09350">
<title>Centrality measures for graphons: Accounting for uncertainty in networks. (arXiv:1707.09350v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1707.09350</link>
<description rdf:parseType="Literal">&lt;p&gt;Graphs provide a natural mathematical abstraction for systems with pairwise
interactions, and thus have become a prevalent tool across various scientific
domains. However, as uncertainty permeates data-acquisition methods, and the
size of relational datasets continues to grow, traditional graph-based
approaches are increasingly replaced by more flexible modeling paradigms. A
promising framework in this regard is that of graphons, which provide an
overarching class of non-parametric random graph models. While the theory of
graphons is already well developed, some prominent tools in network analysis
still have no counterpart within the realm of graphons. In particular, node
centrality measures, which have been successfully employed in various
applications to reveal important nodes in a network, have so far not been
defined for graphons. A key motivation for closing this gap is that centrality
measures defined at the modeling level of graphons will be inherently robust to
stochastic variations of specific graph realizations. In this work we introduce
formal definitions of centrality measures for graphons and establish their
connections to centrality measures defined on finite graphs. In particular, we
build on the theory of linear integral operators to define degree, eigenvector,
and Katz centrality functions for graphons. We further establish concentration
inequalities showing that these centrality functions arise naturally as limits
of their analogous counterparts defined on sequences of converging graphs of
increasing size. Moreover, we provide high-probability bounds on the distance
between the graphon centrality function and the centrality measures realized in
any sampled graph. We discuss and exemplify several strategies for computing
graphon centrality functions and illustrate the aforementioned concentration
inequalities through numerical experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avella_Medina_M/0/1/0/all/0/1&quot;&gt;Marco Avella-Medina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parise_F/0/1/0/all/0/1&quot;&gt;Francesca Parise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaub_M/0/1/0/all/0/1&quot;&gt;Michael T. Schaub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1&quot;&gt;Santiago Segarra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02550">
<title>Semi-Amortized Variational Autoencoders. (arXiv:1802.02550v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02550</link>
<description rdf:parseType="Literal">&lt;p&gt;Amortized variational inference (AVI) replaces instance-specific local
inference with a global inference network. While AVI has enabled efficient
training of deep generative models such as variational autoencoders (VAE),
recent empirical work suggests that inference networks can produce suboptimal
variational parameters. We propose a hybrid approach, to use AVI to initialize
the variational parameters and run stochastic variational inference (SVI) to
refine them. Crucially, the local SVI procedure is itself differentiable, so
the inference network and generative model can be trained end-to-end with
gradient-based optimization. This semi-amortized approach enables the use of
rich generative models without experiencing the posterior-collapse phenomenon
common in training VAEs for problems like text generation. Experiments show
this approach outperforms strong autoregressive and variational baselines on
standard text and image datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wiseman_S/0/1/0/all/0/1&quot;&gt;Sam Wiseman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Miller_A/0/1/0/all/0/1&quot;&gt;Andrew C. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sontag_D/0/1/0/all/0/1&quot;&gt;David Sontag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rush_A/0/1/0/all/0/1&quot;&gt;Alexander M. Rush&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04551">
<title>Analysis of Minimax Error Rate for Crowdsourcing and Its Application to Worker Clustering Model. (arXiv:1802.04551v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04551</link>
<description rdf:parseType="Literal">&lt;p&gt;While crowdsourcing has become an important means to label data, there is
great interest in estimating the ground truth from unreliable labels produced
by crowdworkers. The Dawid and Skene (DS) model is one of the most well-known
models in the study of crowdsourcing. Despite its practical popularity,
theoretical error analysis for the DS model has been conducted only under
restrictive assumptions on class priors, confusion matrices, or the number of
labels each worker provides. In this paper, we derive a minimax error rate
under more practical setting for a broader class of crowdsourcing models
including the DS model as a special case. We further propose the worker
clustering model, which is more practical than the DS model under real
crowdsourcing settings. The wide applicability of our theoretical analysis
allows us to immediately investigate the behavior of this proposed model, which
can not be analyzed by existing studies. Experimental results showed that there
is a strong similarity between the lower bound of the minimax error rate
derived by our theoretical analysis and the empirical error of the estimated
value.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Imamura_H/0/1/0/all/0/1&quot;&gt;Hideaki Imamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sato_I/0/1/0/all/0/1&quot;&gt;Issei Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04630">
<title>A probabilistic framework for multi-view feature learning with many-to-many associations via neural networks. (arXiv:1802.04630v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04630</link>
<description rdf:parseType="Literal">&lt;p&gt;A simple framework Probabilistic Multi-view Graph Embedding (PMvGE) is
proposed for multi-view feature learning with many-to-many associations so that
it generalizes various existing multi-view methods. PMvGE is a probabilistic
model for predicting new associations via graph embedding of the nodes of data
vectors with links of their associations. Multi-view data vectors with
many-to-many associations are transformed by neural networks to feature vectors
in a shared space, and the probability of new association between two data
vectors is modeled by the inner product of their feature vectors. While
existing multi-view feature learning techniques can treat only either of
many-to-many association or non-linear transformation, PMvGE can treat both
simultaneously. By combining Mercer&apos;s theorem and the universal approximation
theorem, we prove that PMvGE learns a wide class of similarity measures across
views. Our likelihood-based estimator enables efficient computation of
non-linear transformations of data vectors in large-scale datasets by minibatch
SGD, and numerical experiments illustrate that PMvGE outperforms existing
multi-view methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Okuno_A/0/1/0/all/0/1&quot;&gt;Akifumi Okuno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hada_T/0/1/0/all/0/1&quot;&gt;Tetsuya Hada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shimodaira_H/0/1/0/all/0/1&quot;&gt;Hidetoshi Shimodaira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07191">
<title>Neural Architecture Search with Bayesian Optimisation and Optimal Transport. (arXiv:1802.07191v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07191</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian Optimisation (BO) refers to a class of methods for global
optimisation of a function $f$ which is only accessible via point evaluations.
It is typically used in settings where $f$ is expensive to evaluate. A common
use case for BO in machine learning is model selection, where it is not
possible to analytically model the generalisation performance of a statistical
model, and we resort to noisy and expensive training and validation procedures
to choose the best model. Conventional BO methods have focused on Euclidean and
categorical domains, which, in the context of model selection, only permits
tuning scalar hyper-parameters of machine learning algorithms. However, with
the surge of interest in deep learning, there is an increasing demand to tune
neural network \emph{architectures}. In this work, we develop NASBOT, a
Gaussian process based BO framework for neural architecture search. To
accomplish this, we develop a distance metric in the space of neural network
architectures which can be computed efficiently via an optimal transport
program. This distance might be of independent interest to the deep learning
community as it may find applications outside of BO. We demonstrate that NASBOT
outperforms other alternatives for architecture search in several cross
validation based model selection tasks on multi-layer perceptrons and
convolutional neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kandasamy_K/0/1/0/all/0/1&quot;&gt;Kirthevasan Kandasamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1&quot;&gt;Willie Neiswanger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnabas Poczos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07575">
<title>Emulating dynamic non-linear simulators using Gaussian processes. (arXiv:1802.07575v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07575</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we examine the emulation of non-linear deterministic computer
codes where the output is a time series, possibly multivariate. Such computer
models simulate the evolution of some real-world phenomena over time, for
example models of the climate or the functioning of the human brain. The models
we are interested in are highly non-linear and exhibit tipping points,
bifurcations and chaotic behaviour. However, each simulation run could be too
time-consuming to perform analyses that require many runs, including
quantifying the variation in model output with respect to changes in the
inputs. We therefore build emulators using Gaussian processes to approximate
the output of the code. We use the Gaussian process to predict one-step ahead
in an iterative way over the whole time series. We consider a number of ways to
propagate uncertainty through the time series including both the uncertainty of
inputs to the emulators at time $t$ and the correlation between them. The
methodology is illustrated with a number of examples. These include the highly
non-linear dynamical systems described by the Lorenz and Van der Pol equations.
In both cases we will show that we not only have very good predictive
performance but also have measures of uncertainty that reflect what is known
about predictability in each system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mohammadi_H/0/1/0/all/0/1&quot;&gt;Hossein Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Challenor_P/0/1/0/all/0/1&quot;&gt;Peter Challenor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goodfellow_M/0/1/0/all/0/1&quot;&gt;Marc Goodfellow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09902">
<title>Attention-Based Guided Structured Sparsity of Deep Neural Networks. (arXiv:1802.09902v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09902</link>
<description rdf:parseType="Literal">&lt;p&gt;Network pruning is aimed at imposing sparsity in a neural network
architecture by increasing the portion of zero-valued weights for reducing its
size regarding energy-efficiency consideration and increasing evaluation speed.
In most of the conducted research efforts, the sparsity is enforced for network
pruning without any attention to the internal network characteristics such as
unbalanced outputs of the neurons or more specifically the distribution of the
weights and outputs of the neurons. That may cause severe accuracy drop due to
uncontrolled sparsity. In this work, we propose an attention mechanism that
simultaneously controls the sparsity intensity and supervised network pruning
by keeping important information bottlenecks of the network to be active. On
CIFAR-10, the proposed method outperforms the best baseline method by 6% and
reduced the accuracy drop by 2.6x at the same level of sparsity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torfi_A/0/1/0/all/0/1&quot;&gt;Amirsina Torfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirvani_R/0/1/0/all/0/1&quot;&gt;Rouzbeh A. Shirvani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1&quot;&gt;Sobhan Soleymani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1&quot;&gt;Nasser M. Nasrabadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00225">
<title>Global Convergence in Deep Learning with Variable Splitting via the Kurdyka-{\L}ojasiewicz Property. (arXiv:1803.00225v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00225</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has recently attracted a significant amount of attention due to
its great empirical success. However, the effectiveness in training deep neural
networks (DNN) remains a mystery in the associated nonconvex optimizations. In
this paper, we aim to provide some theoretical understanding on such
optimization problems. In particular, the Kurdyka-{\L}ojasiewicz (KL) property
is established for DNN training with variable splitting schemes, which leads to
the global convergence of block coordinate descent (BCD) type algorithms to a
critical point of objective functions under natural conditions of DNN. Some
existing BCD algorithms can be viewed as special cases in this framework.
Experiments further show that the proposed algorithms may find network
parameters of approximately zero training loss (error) with over-parameterized
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Jinshan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ouyang_S/0/1/0/all/0/1&quot;&gt;Shikang Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lau_T/0/1/0/all/0/1&quot;&gt;Tim Tsz-Kit Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shaobo Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02879">
<title>Deep Models of Interactions Across Sets. (arXiv:1803.02879v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02879</link>
<description rdf:parseType="Literal">&lt;p&gt;We use deep learning to model interactions across two or more sets of
objects, such as user-movie ratings, protein-drug bindings, or ternary
user-item-tag interactions. The canonical representation of such interactions
is a matrix (or a higher-dimensional tensor) with an exchangeability property:
the encoding&apos;s meaning is not changed by permuting rows or columns. We argue
that models should hence be Permutation Equivariant (PE): constrained to make
the same predictions across such permutations. We present a parameter-sharing
scheme and prove that it could not be made any more expressive without
violating PE. This scheme yields three benefits. First, we demonstrate
state-of-the-art performance on multiple matrix completion benchmarks. Second,
our models require a number of parameters independent of the numbers of
objects, and thus scale well to large datasets. Third, models can be queried
about new objects that were not available at training time, but for which
interactions have since been observed. In experiments, our models achieved
surprisingly good generalization performance on this matrix extrapolation task,
both within domains (e.g., new users and new movies drawn from the same
distribution used for training) and even across domains (e.g., predicting music
ratings after training on movies).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hartford_J/0/1/0/all/0/1&quot;&gt;Jason Hartford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Graham_D/0/1/0/all/0/1&quot;&gt;Devon R Graham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Leyton_Brown_K/0/1/0/all/0/1&quot;&gt;Kevin Leyton-Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ravanbakhsh_S/0/1/0/all/0/1&quot;&gt;Siamak Ravanbakhsh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05391">
<title>On the Universal Approximation Property and Equivalence of Stochastic Computing-based Neural Networks and Binary Neural Networks. (arXiv:1803.05391v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05391</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale deep neural networks are both memory intensive and
computation-intensive, thereby posing stringent requirements on the computing
platforms. Hardware accelerations of deep neural networks have been extensively
investigated in both industry and academia. Specific forms of binary neural
networks (BNNs) and stochastic computing based neural networks (SCNNs) are
particularly appealing to hardware implementations since they can be
implemented almost entirely with binary operations. Despite the obvious
advantages in hardware implementation, these approximate computing techniques
are questioned by researchers in terms of accuracy and universal applicability.
Also it is important to understand the relative pros and cons of SCNNs and BNNs
in theory and in actual hardware implementations. In order to address these
concerns, in this paper we prove that the &quot;ideal&quot; SCNNs and BNNs satisfy the
universal approximation property with probability 1 (due to the stochastic
behavior). The proof is conducted by first proving the property for SCNNs from
the strong law of large numbers, and then using SCNNs as a &quot;bridge&quot; to prove
for BNNs. Based on the universal approximation property, we further prove that
SCNNs and BNNs exhibit the same energy complexity. In other words, they have
the same asymptotic energy consumption with the growing of network size. We
also provide a detailed analysis of the pros and cons of SCNNs and BNNs for
hardware implementations and conclude that SCNNs are more suitable for
hardware.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiayu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1&quot;&gt;Bo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wujie Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xue Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10161">
<title>Stein Points. (arXiv:1803.10161v3 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10161</link>
<description rdf:parseType="Literal">&lt;p&gt;An important task in computational statistics and machine learning is to
approximate a posterior distribution $p(x)$ with an empirical measure supported
on a set of representative points $\{x_i\}_{i=1}^n$. This paper focuses on
methods where the selection of points is essentially deterministic, with an
emphasis on achieving accurate approximation when $n$ is small. To this end, we
present `Stein Points&apos;. The idea is to exploit either a greedy or a conditional
gradient method to iteratively minimise a kernel Stein discrepancy between the
empirical measure and $p(x)$. Our empirical results demonstrate that Stein
Points enable accurate approximation of the posterior at modest computational
cost. In addition, theoretical results are provided to establish convergence of
the method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wilson Ye Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gorham_J/0/1/0/all/0/1&quot;&gt;Jackson Gorham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois-Xavier Briol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oates_C/0/1/0/all/0/1&quot;&gt;Chris J. Oates&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01926">
<title>Scalable Magnetic Field SLAM in 3D Using Gaussian Process Maps. (arXiv:1804.01926v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01926</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method for scalable and fully 3D magnetic field simultaneous
localisation and mapping (SLAM) using local anomalies in the magnetic field as
a source of position information. These anomalies are due to the presence of
ferromagnetic material in the structure of buildings and in objects such as
furniture. We represent the magnetic field map using a Gaussian process model
and take well-known physical properties of the magnetic field into account. We
build local maps using three-dimensional hexagonal block tiling. To make our
approach computationally tractable we use reduced-rank Gaussian process
regression in combination with a Rao-Blackwellised particle filter. We show
that it is possible to obtain accurate position and orientation estimates using
measurements from a smartphone, and that our approach provides a scalable
magnetic field SLAM algorithm in terms of both computational complexity and map
storage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kok_M/0/1/0/all/0/1&quot;&gt;Manon Kok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1&quot;&gt;Arno Solin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05809">
<title>Efficient end-to-end learning for quantizable representations. (arXiv:1805.05809v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05809</link>
<description rdf:parseType="Literal">&lt;p&gt;Embedding representation learning via neural networks is at the core
foundation of modern similarity based search. While much effort has been put in
developing algorithms for learning binary hamming code representations for
search efficiency, this still requires a linear scan of the entire dataset per
each query and trades off the search accuracy through binarization. To this
end, we consider the problem of directly learning a quantizable embedding
representation and the sparse binary hash code end-to-end which can be used to
construct an efficient hash table not only providing significant search
reduction in the number of data but also achieving the state of the art search
accuracy outperforming previous state of the art deep metric learning methods.
We also show that finding the optimal sparse binary hash code in a mini-batch
can be computed exactly in polynomial time by solving a minimum cost flow
problem. Our results on Cifar-100 and on ImageNet datasets show the state of
the art search accuracy in precision@k and NMI metrics while providing up to
98X and 478X search speedup respectively over exhaustive linear search. The
source code is available at
https://github.com/maestrojeong/Deep-Hash-Table-ICML18.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1&quot;&gt;Yeonwoo Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hyun Oh Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09108">
<title>Deep Learning Estimation of Absorbed Dose for Nuclear Medicine Diagnostics. (arXiv:1805.09108v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09108</link>
<description rdf:parseType="Literal">&lt;p&gt;The distribution of energy dose from Lu$^{177}$ radiotherapy can be estimated
by convolving an image of a time-integrated activity distribution with a dose
voxel kernel (dvk) consisting of different types of tissues. This fast and
inacurate approximation is inappropriate for personalized dosimetry as it
neglects tissue heterogenity. The latter can be calculated using different
imaging techniques such as CT and SPECT combined with a time consuming
monte-carlo simulation. The aim of this study is, for the first time, an
estimation of DVKs from CT-derived density kernels (dk) via deep learning in
convolutional neural networks (cnns). The proposed cnn achieved, on the test
set, a mean intersection over union (iou) of $= 0.86$ after $308$ epochs and a
corresponding mean squared error (mse) $= 1.24 \cdot 10^{-4}$. This
generalization ability shows that the trained cnn can indeed learn the complex
transfer function from dk to dvk. Future work will evaluate dvks estimated by
cnns with full monte-carlo simulations of a whole body CT to predict patient
specific voxel dose maps.
&lt;/p&gt;
&lt;p&gt;Keywords: Deep Learning, Nuclear Medicine, Diagnostics, Machine Learning,
Statistics
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Melodia_L/0/1/0/all/0/1&quot;&gt;Luciano Melodia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09717">
<title>Learning Classifiers with Fenchel-Young Losses: Generalized Entropies, Margins, and Algorithms. (arXiv:1805.09717v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09717</link>
<description rdf:parseType="Literal">&lt;p&gt;We study in this paper Fenchel-Young losses, a generic way to construct
convex loss functions from a convex regularizer. We provide an in-depth study
of their properties in a broad setting and show that they unify many well-known
loss functions. When constructed from a generalized entropy, which includes
well-known entropies such as Shannon and Tsallis entropies, we show that
Fenchel-Young losses induce a predictive probability distribution and develop
an efficient algorithm to compute that distribution for separable entropies. We
derive conditions for generalized entropies to yield a distribution with sparse
support and losses with a separation margin. Finally, we present both primal
and dual algorithms to learn predictive models with generic Fenchel-Young
losses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blondel_M/0/1/0/all/0/1&quot;&gt;Mathieu Blondel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Martins_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Niculae_V/0/1/0/all/0/1&quot;&gt;Vlad Niculae&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00548">
<title>A Fast and Scalable Joint Estimator for Integrating Additional Knowledge in Learning Multiple Related Sparse Gaussian Graphical Models. (arXiv:1806.00548v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00548</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of including additional knowledge in estimating
sparse Gaussian graphical models (sGGMs) from aggregated samples, arising often
in bioinformatics and neuroimaging applications. Previous joint sGGM estimators
either fail to use existing knowledge or cannot scale-up to many tasks (large
$K$) under a high-dimensional (large $p$) situation. In this paper, we propose
a novel \underline{J}oint \underline{E}lementary \underline{E}stimator
incorporating additional \underline{K}nowledge (JEEK) to infer multiple related
sparse Gaussian Graphical models from large-scale heterogeneous data. Using
domain knowledge as weights, we design a novel hybrid norm as the minimization
objective to enforce the superposition of two weighted sparsity constraints,
one on the shared interactions and the other on the task-specific structural
patterns. This enables JEEK to elegantly consider various forms of existing
knowledge based on the domain at hand and avoid the need to design
knowledge-specific optimization. JEEK is solved through a fast and entry-wise
parallelizable solution that largely improves the computational efficiency of
the state-of-the-art $O(p^5K^4)$ to $O(p^2K^4)$. We conduct a rigorous
statistical analysis showing that JEEK achieves the same convergence rate
$O(\log(Kp)/n_{tot})$ as the state-of-the-art estimators that are much harder
to compute. Empirically, on multiple synthetic datasets and two real-world
data, JEEK outperforms the speed of the state-of-arts significantly while
achieving the same level of prediction accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Beilun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1&quot;&gt;Arshdeep Sekhon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yanjun Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01248">
<title>Dynamically Hierarchy Revolution: DirNet for Compressing Recurrent Neural Network on Mobile Devices. (arXiv:1806.01248v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01248</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks (RNNs) achieve cutting-edge performance on a
variety of problems. However, due to their high computational and memory
demands, deploying RNNs on resource constrained mobile devices is a challenging
task. To guarantee minimum accuracy loss with higher compression rate and
driven by the mobile resource requirement, we introduce a novel model
compression approach DirNet based on an optimized fast dictionary learning
algorithm, which 1) dynamically mines the dictionary atoms of the projection
dictionary matrix within layer to adjust the compression rate 2) adaptively
changes the sparsity of sparse codes cross the hierarchical layers.
Experimental results on language model and an ASR model trained with a 1000h
speech dataset demonstrate that our method significantly outperforms prior
approaches. Evaluated on off-the-shelf mobile devices, we are able to reduce
the size of original model by eight times with real-time model inference and
negligible accuracy loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yalin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01811">
<title>AdaGrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization. (arXiv:1806.01811v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01811</link>
<description rdf:parseType="Literal">&lt;p&gt;Adaptive gradient methods such as AdaGrad and its variants update the
stepsize in stochastic gradient descent on the fly according to the gradients
received along the way; such methods have gained widespread use in large-scale
optimization for their ability to converge robustly, without the need to fine
tune parameters such as the stepsize schedule. Yet, the theoretical guarantees
to date for AdaGrad are for online and convex optimization, which is quite
different from the offline and nonconvex setting where adaptive gradient
methods shine in practice. We bridge this gap by providing strong theoretical
guarantees in batch and stochastic setting, for the convergence of AdaGrad over
smooth, nonconvex landscapes, from any initialization of the stepsize, without
knowledge of Lipschitz constant of the gradient. We show in the stochastic
setting that AdaGrad converges to a stationary point at the optimal
$O(1/\sqrt{N})$ rate (up to a $\log(N)$ factor), and in the batch setting, at
the optimal $O(1/N)$ rate. Moreover, in both settings, the constant in the rate
matches the constant obtained as if the variance of the gradient noise and
Lipschitz constant of the gradient were known in advance and used to tune the
stepsize, up to a logarithmic factor of the mismatch between the optimal
stepsize and the stepsize used to initialize AdaGrad. In particular, our
results imply that AdaGrad is robust to both the unknown Lipschitz constant and
level of stochastic noise on the gradient, in a near-optimal sense. When there
is noise, AdaGrad converges at the rate of $O(1/\sqrt{N})$ with well-tuned
stepsize, and when there is not noise, the same algorithm converges at the rate
of $O(1/N)$ like well-tuned batch gradient descent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ward_R/0/1/0/all/0/1&quot;&gt;Rachel Ward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoxia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bottou_L/0/1/0/all/0/1&quot;&gt;Leon Bottou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.08095">
<title>Converting High-Dimensional Regression to High-Dimensional Conditional Density Estimation. (arXiv:1704.08095v1 [stat.ME] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1704.08095</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a growing demand for nonparametric conditional density estimators
(CDEs) in fields such as astronomy and economics. In astronomy, for example,
one can dramatically improve estimates of the parameters that dictate the
evolution of the Universe by working with full conditional densities instead of
regression (i.e., conditional mean) estimates. More generally, standard
regression falls short in any prediction problem where the distribution of the
response is more complex with multi-modality, asymmetry or heteroscedastic
noise. Nevertheless, much of the work on high-dimensional inference concerns
regression and classification only, whereas research on density estimation has
lagged behind. Here we propose FlexCode, a fully nonparametric approach to
conditional density estimation that reformulates CDE as a non-parametric
orthogonal series problem where the expansion coefficients are estimated by
regression. By taking such an approach, one can efficiently estimate
conditional densities and not just expectations in high dimensions by drawing
upon the success in high-dimensional regression. Depending on the choice of
regression procedure, our method can adapt to a variety of challenging
high-dimensional settings with different structures in the data (e.g., a large
number of irrelevant components and nonlinear manifold structure) as well as
different data types (e.g., functional data, mixed data types and sample sets).
We study the theoretical and empirical performance of our proposed method, and
we compare our approach with traditional conditional density estimators on
simulated as well as real-world data, such as photometric galaxy data, Twitter
data, and line-of-sight velocities in a galaxy cluster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1&quot;&gt;Rafael Izbicki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Ann B. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.01458">
<title>Image denoising with generalized Gaussian mixture model patch priors. (arXiv:1802.01458v2 [eess.IV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1802.01458</link>
<description rdf:parseType="Literal">&lt;p&gt;Patch priors have become an important component of image restoration. A
powerful approach in this category of restoration algorithms is the popular
Expected Patch Log-Likelihood (EPLL) algorithm. EPLL uses a Gaussian mixture
model (GMM) prior learned on clean image patches as a way to regularize
degraded patches. In this paper, we show that a generalized Gaussian mixture
model (GGMM) captures the underlying distribution of patches better than a GMM.
Even though GGMM is a powerful prior to combine with EPLL, the non-Gaussianity
of its components presents major challenges to be applied to a computationally
intensive process of image restoration. Specifically, each patch has to undergo
a patch classification step and a shrinkage step. These two steps can be
efficiently solved with a GMM prior but are computationally impractical when
using a GGMM prior. In this paper, we provide approximations and computational
recipes for fast evaluation of these two steps, so that EPLL can embed a GGMM
prior on an image with more than tens of thousands of patches. Our main
contribution is to analyze the accuracy of our approximations based on thorough
theoretical analysis. Our evaluations indicate that the GGMM prior is
consistently a better fit formodeling image patch distribution and performs
better on average in image denoising task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deledalle_C/0/1/0/all/0/1&quot;&gt;Charles-Alban Deledalle&lt;/a&gt; (IMB, UCSD), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Parameswaran_S/0/1/0/all/0/1&quot;&gt;Shibin Parameswaran&lt;/a&gt; (UCSD), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Truong Q. Nguyen&lt;/a&gt; (UCSD)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05480">
<title>ABC-CDE: Towards Approximate Bayesian Computation with Complex High-Dimensional Data and Limited Simulations. (arXiv:1805.05480v1 [stat.ME] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1805.05480</link>
<description rdf:parseType="Literal">&lt;p&gt;Approximate Bayesian Computation (ABC) is typically used when the likelihood
is either unavailable or intractable but where data can be simulated under
different parameter settings using a forward model. Despite the recent interest
in ABC, high-dimensional data and costly simulations still remain a bottleneck.
There is also no consensus as to how to best assess the performance of such
methods without knowing the true posterior. We show how a nonparametric
conditional density estimation (CDE) framework, which we refer to as ABC-CDE,
help address three key challenges in ABC: (i) how to efficiently estimate the
posterior distribution with limited simulations and different types of data,
(ii) how to tune and compare the performance of ABC and related methods in
estimating the posterior itself, rather than just certain properties of the
density, and (iii) how to efficiently choose among a large set of summary
statistics based on a CDE surrogate loss. We provide theoretical and empirical
evidence that justify ABC-CDE procedures that directly estimate and assess the
posterior based on an initial ABC sample, and we describe settings where
standard ABC and regression-based approaches are inadequate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1&quot;&gt;Rafael Izbicki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Ann B. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pospisil_T/0/1/0/all/0/1&quot;&gt;Taylor Pospisil&lt;/a&gt;</dc:creator>
</item></rdf:RDF>