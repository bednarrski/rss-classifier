<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-01T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11261"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11288"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11437"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11493"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11556"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.10351"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01944"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11266"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11274"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11287"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11364"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11451"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11462"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11485"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1502.03211"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.00060"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09715"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03713"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09791"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10884"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.11261">
<title>How an Electrical Engineer Became an Artificial Intelligence Researcher, a Multiphase Active Contours Analysis. (arXiv:1803.11261v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.11261</link>
<description rdf:parseType="Literal">&lt;p&gt;This essay examines how what is considered to be artificial intelligence (AI)
has changed over time and come to intersect with the expertise of the author.
Initially, AI developed on a separate trajectory, both topically and
institutionally, from pattern recognition, neural information processing,
decision and control systems, and allied topics by focusing on symbolic systems
within computer science departments rather than on continuous systems in
electrical engineering departments. The separate evolutions continued
throughout the author&apos;s lifetime, with some crossover in reinforcement learning
and graphical models, but were shocked into converging by the virality of deep
learning, thus making an electrical engineer into an AI researcher. Now that
this convergence has happened, opportunity exists to pursue an agenda that
combines learning and reasoning bridged by interpretable machine learning
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varshney_K/0/1/0/all/0/1&quot;&gt;Kush R. Varshney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11288">
<title>FutureMapping: The Computational Structure of Spatial AI Systems. (arXiv:1803.11288v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.11288</link>
<description rdf:parseType="Literal">&lt;p&gt;We discuss and predict the evolution of Simultaneous Localisation and Mapping
(SLAM) into a general geometric and semantic `Spatial AI&apos; perception capability
for intelligent embodied devices. A big gap remains between the visual
perception performance that devices such as augmented reality eyewear or
comsumer robots will require and what is possible within the constraints
imposed by real products. Co-design of algorithms, processors and sensors will
be needed. We explore the computational structure of current and future Spatial
AI algorithms and consider this within the landscape of ongoing hardware
developments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1&quot;&gt;Andrew J. Davison&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11437">
<title>A Rule for Committee Selection with Soft Diversity Constraints. (arXiv:1803.11437v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.11437</link>
<description rdf:parseType="Literal">&lt;p&gt;Committee selection with diversity or distributional constraints is a
ubiquitous problem. However, many of the formal approaches proposed so far have
certain drawbacks including (1) computationally intractability in general, and
(2) inability to suggest a solution for certain instances where the hard
constraints cannot be met. We propose a practical and polynomial-time algorithm
for diverse committee selection that draws on the idea of using soft bounds and
satisfies natural axioms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aziz_H/0/1/0/all/0/1&quot;&gt;Haris Aziz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11493">
<title>3D Pose Estimation and 3D Model Retrieval for Objects in the Wild. (arXiv:1803.11493v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.11493</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a scalable, efficient and accurate approach to retrieve 3D models
for objects in the wild. Our contribution is twofold. We first present a 3D
pose estimation approach for object categories which significantly outperforms
the state-of-the-art on Pascal3D+. Second, we use the estimated pose as a prior
to retrieve 3D models which accurately represent the geometry of objects in RGB
images. For this purpose, we render depth images from 3D models under our
predicted pose and match learned image descriptors of RGB images against those
of rendered depth images using a CNN-based multi-view metric learning approach.
In this way, we are the first to report quantitative results for 3D model
retrieval on Pascal3D+, where our method chooses the same models as human
annotators for 50% of the validation images on average. In addition, we show
that our method, which was trained purely on Pascal3D+, retrieves rich and
accurate 3D models from ShapeNet given RGB images of objects in the wild.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grabner_A/0/1/0/all/0/1&quot;&gt;Alexander Grabner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_P/0/1/0/all/0/1&quot;&gt;Peter M. Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1&quot;&gt;Vincent Lepetit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11556">
<title>Learning to Anonymize Faces for Privacy Preserving Action Detection. (arXiv:1803.11556v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.11556</link>
<description rdf:parseType="Literal">&lt;p&gt;There is an increasing concern in computer vision devices invading the
privacy of their users by recording unwanted videos. On one hand, we want the
camera systems/robots to recognize important events and assist human daily life
by understanding its videos, but on the other hand we also want to ensure that
they do not intrude people&apos;s privacy. In this paper, we propose a new
principled approach for learning a video face anonymizer. We use an adversarial
training setting in which two competing systems fight: (1) a video anonymizer
that modifies the original video to remove privacy-sensitive information (i.e.,
human face) while still trying to maximize spatial action detection
performance, and (2) a discriminator that tries to extract privacy-sensitive
information from such anonymized videos. The end result is a video anonymizer
that performs a pixel-level modification to anonymize each person&apos;s face, with
minimal effect on action detection performance. We experimentally confirm the
benefit of our approach compared to conventional hand-crafted video/face
anonymization methods including masking, blurring, and noise adding. See the
project page https://jason718.github.io/project/privacy/main.html for a demo
video and more results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhongzheng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yong Jae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1&quot;&gt;Michael S. Ryoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.10351">
<title>Joint Causal Inference from Multiple Contexts. (arXiv:1611.10351v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1611.10351</link>
<description rdf:parseType="Literal">&lt;p&gt;The gold standard for discovering causal relations is by means of
experimentation. Over the last decades, alternative methods have been proposed
that can infer causal relations between variables from certain statistical
patterns in purely observational data. We introduce Joint Causal Inference
(JCI), a novel approach to causal discovery from multiple data sets that
elegantly unifies both approaches. JCI is a causal modeling approach rather
than a specific algorithm, and it can be used in combination with any causal
discovery algorithm that can take into account certain background knowledge.
The main idea is to reduce causal discovery from multiple datasets originating
from different contexts (e.g., different experimental conditions) to causal
discovery from a single pooled dataset by adding a set of auxiliary context
variables. JCI offers the following features: it deals with several different
types of interventions in a unified fashion, it can learn intervention targets,
it pools data across different datasets which improves the statistical power of
independence tests, and by exploiting differences in distribution between
contexts it improves on the accuracy and identifiability of the predicted
causal relations. We evaluate the approach on flow cytometry data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mooij_J/0/1/0/all/0/1&quot;&gt;Joris M. Mooij&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magliacane_S/0/1/0/all/0/1&quot;&gt;Sara Magliacane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Claassen_T/0/1/0/all/0/1&quot;&gt;Tom Claassen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01944">
<title>Audio Adversarial Examples: Targeted Attacks on Speech-to-Text. (arXiv:1801.01944v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01944</link>
<description rdf:parseType="Literal">&lt;p&gt;We construct targeted audio adversarial examples on automatic speech
recognition. Given any audio waveform, we can produce another that is over
99.9% similar, but transcribes as any phrase we choose (recognizing up to 50
characters per second of audio). We apply our white-box iterative
optimization-based attack to Mozilla&apos;s implementation DeepSpeech end-to-end,
and show it has a 100% success rate. The feasibility of this attack introduce a
new domain to study adversarial examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1&quot;&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1&quot;&gt;David Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11266">
<title>Performance evaluation and hyperparameter tuning of statistical and machine-learning models using spatial data. (arXiv:1803.11266v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.11266</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine-learning algorithms have gained popularity in recent years in the
field of ecological modeling due to their promising results in predictive
performance of classification problems. While the application of such
algorithms has been highly simplified in the last years due to their
well-documented integration in commonly used statistical programming languages
such as R, there are several practical challenges in the field of ecological
modeling related to unbiased performance estimation, optimization of algorithms
using hyperparameter tuning and spatial autocorrelation. We address these
issues in the comparison of several widely used machine-learning algorithms
such as Boosted Regression Trees (BRT), k-Nearest Neighbor (WKNN), Random
Forest (RF) and Support Vector Machine (SVM) to traditional parametric
algorithms such as logistic regression (GLM) and semi-parametric ones like
generalized additive models (GAM). Different nested cross-validation methods
including hyperparameter tuning methods are used to evaluate model performances
with the aim to receive bias-reduced performance estimates. As a case study the
spatial distribution of forest disease Diplodia sapinea in the Basque Country
in Spain is investigated using common environmental variables such as
temperature, precipitation, soil or lithology as predictors. Results show that
GAM and RF (mean AUROC estimates 0.708 and 0.699) outperform all other methods
in predictive accuracy. The effect of hyperparameter tuning saturates at around
50 iterations for this data set. The AUROC differences between the bias-reduced
(spatial cross-validation) and overoptimistic (non-spatial cross-validation)
performance estimates of the GAM and RF are 0.167 (24%) and 0.213 (30%),
respectively. It is recommended to also use spatial partitioning for
cross-validation hyperparameter tuning of spatial data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schratz_P/0/1/0/all/0/1&quot;&gt;Patrick Schratz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Muenchow_J/0/1/0/all/0/1&quot;&gt;Jannes Muenchow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Richter_J/0/1/0/all/0/1&quot;&gt;Jakob Richter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brenning_A/0/1/0/all/0/1&quot;&gt;Alexander Brenning&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11274">
<title>PIMKL: Pathway Induced Multiple Kernel Learning. (arXiv:1803.11274v1 [q-bio.MN])</title>
<link>http://arxiv.org/abs/1803.11274</link>
<description rdf:parseType="Literal">&lt;p&gt;Reliable identification of molecular biomarkers is essential for accurate
patient stratification. While state-of-the-art machine learning approaches for
sample classification continue to push boundaries in terms of performance, most
of these methods are not able to integrate different data types and lack
generalization power limiting their application in a clinical setting.
Furthermore, many methods behave as black boxes, therefore we have very little
understanding about the mechanisms that lead to the prediction provided. While
opaqueness concerning machine behaviour might not be a problem in deterministic
domains, in health care, providing explanations about the molecular factors and
phenotypes that are driving the classification is crucial to build trust in the
performance of the predictive system. We propose Pathway Induced Multiple
Kernel Learning (PIMKL), a novel methodology to classify samples reliably that
can, at the same time, provide a pathway-based molecular fingerprint of the
signature that underlies the classification. PIMKL exploits prior knowledge in
the form of molecular interaction networks and annotated gene sets, by
optimizing a mixture of pathway-induced kernels using a Multiple Kernel
Learning algorithm (MKL), an approach that has demonstrated excellent
performance in different machine learning applications. After optimizing the
combination of kernels for prediction of a specific phenotype, the model
provides a stable molecular signature that can be interpreted in the light of
the ingested prior knowledge and that can be used in transfer learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Manica_M/0/1/0/all/0/1&quot;&gt;Matteo Manica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cadow_J/0/1/0/all/0/1&quot;&gt;Joris Cadow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mathis_R/0/1/0/all/0/1&quot;&gt;Roland Mathis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Martinez_M/0/1/0/all/0/1&quot;&gt;Mar&amp;#xed;a Rodr&amp;#xed;guez Mart&amp;#xed;nez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11287">
<title>A Stochastic Large-scale Machine Learning Algorithm for Distributed Features and Observations. (arXiv:1803.11287v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.11287</link>
<description rdf:parseType="Literal">&lt;p&gt;As the size of modern data sets exceeds the disk and memory capacities of a
single computer, machine learning practitioners have resorted to parallel and
distributed computing. Given that optimization is one of the pillars of machine
learning and predictive modeling, distributed optimization methods have
recently garnered ample attention, in particular when either observations or
features are distributed, but not both. We propose a general stochastic
algorithm where observations, features, and gradient components can be sampled
in a double distributed setting, i.e., with both features and observations
distributed. Very technical analyses establish convergence properties of the
algorithm under different conditions on the learning rate (diminishing to zero
or constant). Computational experiments in Spark demonstrate a superior
performance of our algorithm versus a benchmark in early iterations of the
algorithm, which is due to the stochastic components of the algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fang_B/0/1/0/all/0/1&quot;&gt;Biyi Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klabjan_D/0/1/0/all/0/1&quot;&gt;Diego Klabjan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11364">
<title>Joint Optimization Framework for Learning with Noisy Labels. (arXiv:1803.11364v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.11364</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) trained on large-scale datasets have exhibited
significant performance in image classification. Many large-scale datasets are
collected from websites, however they tend to contain inaccurate labels that
are termed as noisy labels. Training on such noisy labeled datasets causes
performance degradation because DNNs easily overfit to noisy labels. To
overcome this problem, we propose a joint optimization framework of learning
DNN parameters and estimating true labels. Our framework can correct labels
during training by alternating update of network parameters and labels. We
conduct experiments on the noisy CIFAR-10 datasets and the Clothing1M dataset.
The results indicate that our approach significantly outperforms other
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_D/0/1/0/all/0/1&quot;&gt;Daiki Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ikami_D/0/1/0/all/0/1&quot;&gt;Daiki Ikami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamasaki_T/0/1/0/all/0/1&quot;&gt;Toshihiko Yamasaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aizawa_K/0/1/0/all/0/1&quot;&gt;Kiyoharu Aizawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11451">
<title>Minimax Estimation of Quadratic Fourier Functionals. (arXiv:1803.11451v1 [math.ST])</title>
<link>http://arxiv.org/abs/1803.11451</link>
<description rdf:parseType="Literal">&lt;p&gt;We study estimation of (semi-)inner products between two nonparametric
probability distributions, given IID samples from each distribution. These
products include relatively well-studied classical $\mathcal{L}^2$ and Sobolev
inner products, as well as those induced by translation-invariant reproducing
kernels, for which we believe our results are the first. We first propose
estimators for these quantities, and the induced (semi)norms and
(pseudo)metrics. We then prove non-asymptotic upper bounds on their mean
squared error, in terms of weights both of the inner product and of the two
distributions, in the Fourier basis. Finally, we prove minimax lower bounds
that imply rate-optimality of the proposed estimators over Fourier ellipsoids.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Shashank Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sriperumbudur_B/0/1/0/all/0/1&quot;&gt;Bharath K. Sriperumbudur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnab&amp;#xe1;s P&amp;#xf3;czos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11462">
<title>Improving confidence while predicting trends in temporal disease networks. (arXiv:1803.11462v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.11462</link>
<description rdf:parseType="Literal">&lt;p&gt;For highly sensitive real-world predictive analytic applications such as
healthcare and medicine, having good prediction accuracy alone is often not
enough. These kinds of applications require a decision making process which
uses uncertainty estimation as input whenever possible. Quality of uncertainty
estimation is a subject of over or under confident prediction, which is often
not addressed in many models. In this paper we show several extensions to the
Gaussian Conditional Random Fields model, which aim to provide higher quality
uncertainty estimation. These extensions are applied to the temporal disease
graph built from the State Inpatient Database (SID) of California, acquired
from the HCUP. Our experiments demonstrate benefits of using graph information
in modeling temporal disease properties as well as improvements in uncertainty
estimation provided by given extensions of the Gaussian Conditional Random
Fields method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gligorijevic_D/0/1/0/all/0/1&quot;&gt;Djordje Gligorijevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stojanovic_J/0/1/0/all/0/1&quot;&gt;Jelena Stojanovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obradovic_Z/0/1/0/all/0/1&quot;&gt;Zoran Obradovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11485">
<title>QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning. (arXiv:1803.11485v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.11485</link>
<description rdf:parseType="Literal">&lt;p&gt;In many real-world settings, a team of agents must coordinate their behaviour
while acting in a decentralised way. At the same time, it is often possible to
train the agents in a centralised fashion in a simulated or laboratory setting,
where global state information is available and communication constraints are
lifted. Learning joint action-values conditioned on extra state information is
an attractive way to exploit centralised learning, but the best strategy for
then extracting decentralised policies is unclear. Our solution is QMIX, a
novel value-based method that can train decentralised policies in a centralised
end-to-end fashion. QMIX employs a network that estimates joint action-values
as a complex non-linear combination of per-agent values that condition only on
local observations. We structurally enforce that the joint-action value is
monotonic in the per-agent values, which allows tractable maximisation of the
joint action-value in off-policy learning, and guarantees consistency between
the centralised and decentralised policies. We evaluate QMIX on a challenging
set of StarCraft II micromanagement tasks, and show that QMIX significantly
outperforms existing value-based multi-agent reinforcement learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1&quot;&gt;Tabish Rashid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samvelyan_M/0/1/0/all/0/1&quot;&gt;Mikayel Samvelyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1&quot;&gt;Christian Schroeder de Witt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farquhar_G/0/1/0/all/0/1&quot;&gt;Gregory Farquhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1&quot;&gt;Jakob Foerster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1502.03211">
<title>An Extreme-Value Approach for Testing the Equality of Large U-Statistic Based Correlation Matrices. (arXiv:1502.03211v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1502.03211</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been an increasing interest in testing the equality of large
Pearson&apos;s correlation matrices. However, in many applications it is more
important to test the equality of large rank-based correlation matrices since
they are more robust to outliers and nonlinearity. Unlike the Pearson&apos;s case,
testing the equality of large rank-based statistics has not been well explored
and requires us to develop new methods and theory. In this paper, we provide a
framework for testing the equality of two large U-statistic based correlation
matrices, which include the rank-based correlation matrices as special cases.
Our approach exploits extreme value statistics and the Jackknife estimator for
uncertainty assessment and is valid under a fully nonparametric model.
Theoretically, we develop a theory for testing the equality of U-statistic
based correlation matrices. We then apply this theory to study the problem of
testing large Kendall&apos;s tau correlation matrices and demonstrate its
optimality. For proving this optimality, a novel construction of least
favourable distributions is developed for the correlation matrix comparison.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Cheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Han_F/0/1/0/all/0/1&quot;&gt;Fang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinsheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.00060">
<title>Exploiting gradients and Hessians in Bayesian optimization and Bayesian quadrature. (arXiv:1704.00060v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.00060</link>
<description rdf:parseType="Literal">&lt;p&gt;An exciting branch of machine learning research focuses on methods for
learning, optimizing, and integrating unknown functions that are difficult or
costly to evaluate. A popular Bayesian approach to this problem uses a Gaussian
process (GP) to construct a posterior distribution over the function of
interest given a set of observed measurements, and selects new points to
evaluate using the statistics of this posterior. Here we extend these methods
to exploit derivative information from the unknown function. We describe
methods for Bayesian optimization (BO) and Bayesian quadrature (BQ) in settings
where first and second derivatives may be evaluated along with the function
itself. We perform sampling-based inference in order to incorporate uncertainty
over hyperparameters, and show that both hyperparameter and function
uncertainty decrease much more rapidly when using derivative information.
Moreover, we introduce techniques for overcoming ill-conditioning issues that
have plagued earlier methods for gradient-enhanced Gaussian processes and
kriging. We illustrate the efficacy of these methods using applications to real
and simulated Bayesian optimization and quadrature problems, and show that
exploting derivatives can provide substantial gains over standard methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_A/0/1/0/all/0/1&quot;&gt;Anqi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aoi_M/0/1/0/all/0/1&quot;&gt;Mikio C. Aoi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pillow_J/0/1/0/all/0/1&quot;&gt;Jonathan W. Pillow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09715">
<title>Guided Machine Learning for power grid segmentation. (arXiv:1711.09715v3 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09715</link>
<description rdf:parseType="Literal">&lt;p&gt;The segmentation of large scale power grids into zones is crucial for control
room operators when managing the grid complexity near real time. In this paper
we propose a new method in two steps which is able to automatically do this
segmentation, while taking into account the real time context, in order to help
them handle shifting dynamics. Our method relies on a &quot;guided&quot; machine learning
approach. As a first step, we define and compute a task specific &quot;Influence
Graph&quot; in a guided manner. We indeed simulate on a grid state chosen
interventions, representative of our task of interest (managing active power
flows in our case). For visualization and interpretation, we then build a
higher representation of the grid relevant to this task by applying the graph
community detection algorithm \textit{Infomap} on this Influence Graph. To
illustrate our method and demonstrate its practical interest, we apply it on
commonly used systems, the IEEE-14 and IEEE-118. We show promising and original
interpretable results, especially on the previously well studied RTS-96 system
for grid segmentation. We eventually share initial investigation and results on
a large-scale system, the French power grid, whose segmentation had a
surprising resemblance with RTE&apos;s historical partitioning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marot_A/0/1/0/all/0/1&quot;&gt;Antoine Marot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tazi_S/0/1/0/all/0/1&quot;&gt;Sami Tazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Donnot_B/0/1/0/all/0/1&quot;&gt;Benjamin Donnot&lt;/a&gt; (LRI, TAU), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Panciatici_P/0/1/0/all/0/1&quot;&gt;Patrick Panciatici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03713">
<title>Optimizing Neural Networks in the Equivalence Class Space. (arXiv:1802.03713v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03713</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been widely observed that many activation functions and pooling
methods of neural network models have (positive-) rescaling-invariant property,
including ReLU, PReLU, max-pooling, and average pooling, which makes
fully-connected neural networks (FNNs) and convolutional neural networks (CNNs)
invariant to (positive) rescaling operation across layers. This may cause
unneglectable problems with their optimization: (1) different NN models could
be equivalent, but their gradients can be very different from each other; (2)
it can be proven that the loss functions may have many spurious critical points
in the redundant weight space. To tackle these problems, in this paper, we
first characterize the rescaling-invariant properties of NN models using
equivalence classes and prove that the dimension of the equivalence class space
is significantly smaller than the dimension of the original weight space. Then
we represent the loss function in the compact equivalence class space and
develop novel algorithms that conduct optimization of the NN models directly in
the equivalence class space. We call these algorithms Equivalence Class
Optimization (abbreviated as EC-Opt) algorithms. Moreover, we design efficient
tricks to compute the gradients in the equivalence class, which almost have no
extra computational complexity as compared to standard back-propagation (BP).
We conducted experimental study to demonstrate the effectiveness of our
proposed new optimization algorithms. In particular, we show that by using the
idea of EC-Opt, we can significantly improve the accuracy of the learned model
(for both FNN and CNN), as compared to using conventional stochastic gradient
descent algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meng_Q/0/1/0/all/0/1&quot;&gt;Qi Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shuxin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qiwei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tie-Yan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09791">
<title>A Common Framework for Natural Gradient and Taylor based Optimisation using Manifold Theory. (arXiv:1803.09791v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09791</link>
<description rdf:parseType="Literal">&lt;p&gt;This technical report constructs a theoretical framework to relate standard
Taylor approximation based optimisation methods with Natural Gradient (NG), a
method which is Fisher efficient with probabilistic models. Such a framework
will be shown to also provide mathematical justification to combine higher
order methods with the method of NG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haider_A/0/1/0/all/0/1&quot;&gt;Adnan Haider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10884">
<title>Structural Risk Minimization for $C^{1,1}(\mathbb{R}^d)$ Regression. (arXiv:1803.10884v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10884</link>
<description rdf:parseType="Literal">&lt;p&gt;One means of fitting functions to high-dimensional data is by providing
smoothness constraints. Recently, the following smooth function approximation
problem was proposed: given a finite set $E \subset \mathbb{R}^d$ and a
function $f: E \rightarrow \mathbb{R}$, interpolate the given information with
a function $\widehat{f} \in \dot{C}^{1, 1}(\mathbb{R}^d)$ (the class of
first-order differentiable functions with Lipschitz gradients) such that
$\widehat{f}(a) = f(a)$ for all $a \in E$, and the value of
$\mathrm{Lip}(\nabla \widehat{f})$ is minimal. An algorithm is provided that
constructs such an approximating function $\widehat{f}$ and estimates the
optimal Lipschitz constant $\mathrm{Lip}(\nabla \widehat{f})$ in the noiseless
setting.
&lt;/p&gt;
&lt;p&gt;We address statistical aspects of reconstructing the approximating function
$\widehat{f}$ from a closely-related class $C^{1, 1}(\mathbb{R}^d)$ given
samples from noisy data. We observe independent and identically distributed
samples $y(a) = f(a) + \xi(a)$ for $a \in E$, where $\xi(a)$ is a noise term
and the set $E \subset \mathbb{R}^d$ is fixed and known. We obtain uniform
bounds relating the empirical risk and true risk over the class
$\mathcal{F}_{\widetilde{M}} = \{f \in C^{1, 1}(\mathbb{R}^d) \mid
\mathrm{Lip}(\nabla f) \leq \widetilde{M}\}$, where the quantity
$\widetilde{M}$ grows with the number of samples at a rate governed by the
metric entropy of the class $C^{1, 1}(\mathbb{R}^d)$. Finally, we provide an
implementation using Vaidya&apos;s algorithm, supporting our results via numerical
experiments on simulated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gustafson_A/0/1/0/all/0/1&quot;&gt;Adam Gustafson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hirn_M/0/1/0/all/0/1&quot;&gt;Matthew Hirn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mohammed_K/0/1/0/all/0/1&quot;&gt;Kitty Mohammed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Narayanan_H/0/1/0/all/0/1&quot;&gt;Hariharan Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jason Xu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>