<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06920"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06927"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06928"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07008"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07245"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06673"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06434"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06768"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06771"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06806"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06816"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06832"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06840"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06866"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06881"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06888"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06891"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06926"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06940"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06963"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07073"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.08176"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08186"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06137"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06306"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06588"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02565"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06765"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06807"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06820"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06823"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06847"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06894"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06903"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06916"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06939"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06944"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06967"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07024"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07028"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07051"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07079"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07107"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07126"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07127"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07129"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07167"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07182"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07191"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07207"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07244"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1511.06382"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.06525"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.01541"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.01799"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06276"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.09953"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03194"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04043"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06664"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09649"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01496"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03265"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07222"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03676"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05688"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06458"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.06920">
<title>Layer-wise synapse optimization for implementing neural networks on general neuromorphic architectures. (arXiv:1802.06920v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.06920</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep artificial neural networks (ANNs) can represent a wide range of complex
functions. Implementing ANNs in Von Neumann computing systems, though, incurs a
high energy cost due to the bottleneck created between CPU and memory.
Implementation on neuromorphic systems may help to reduce energy demand.
Conventional ANNs must be converted into equivalent Spiking Neural Networks
(SNNs) in order to be deployed on neuromorphic chips. This paper presents a way
to perform this translation. We map the ANN weights to SNN synapses
layer-by-layer by forming a least-square-error approximation problem at each
layer.
&lt;/p&gt;
&lt;p&gt;An optimal set of synapse weights may then be found for a given choice of ANN
activation function and SNN neuron. Using an appropriate constrained solver, we
can generate SNNs compatible with digital, analog, or hybrid chip
architectures. We present an optimal node pruning method to allow SNN layer
sizes to be set by the designer. To illustrate this process, we convert three
ANNs, including one convolutional network, to SNNs. In all three cases, a
simple linear program solver was used. The experiments show that the resulting
networks maintain agreement with the original ANN and excellent performance on
the evaluation tasks. The networks were also reduced in size with little loss
in task performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mern_J/0/1/0/all/0/1&quot;&gt;John Mern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1&quot;&gt;Jayesh K Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1&quot;&gt;Mykel Kochenderfer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06927">
<title>On Lyapunov exponents and adversarial perturbation. (arXiv:1802.06927v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.06927</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we would like to disseminate a serendipitous discovery
involving Lyapunov exponents of a 1-D time series and their use in serving as a
filtering defense tool against a specific kind of deep adversarial
perturbation. To this end, we use the state-of-the-art CleverHans library to
generate adversarial perturbations against a standard Convolutional Neural
Network (CNN) architecture trained on the MNIST as well as the Fashion-MNIST
datasets. We empirically demonstrate how the Lyapunov exponents computed on the
flattened 1-D vector representations of the images served as highly
discriminative features that could be to pre-classify images as adversarial or
legitimate before feeding the image into the CNN for classification. We also
explore the issue of possible false-alarms when the input images are noisy in a
non-adversarial sense.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1&quot;&gt;Vinay Uday Prabhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desai_N/0/1/0/all/0/1&quot;&gt;Nishant Desai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whaley_J/0/1/0/all/0/1&quot;&gt;John Whaley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06928">
<title>Memcomputing: Leveraging memory and physics to compute efficiently. (arXiv:1802.06928v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1802.06928</link>
<description rdf:parseType="Literal">&lt;p&gt;It is well known that physical phenomena may be of great help in computing
some difficult problems efficiently. A typical example is prime factorization
that may be solved in polynomial time by exploiting quantum entanglement on a
quantum computer. There are, however, other types of (non-quantum) physical
properties that one may leverage to compute efficiently a wide range of hard
problems. In this perspective we discuss how to employ one such property,
memory (time non-locality), in a novel physics-based approach to computation:
Memcomputing. In particular, we focus on digital memcomputing machines (DMMs)
that are scalable. DMMs can be realized with non-linear dynamical systems with
memory. The latter property allows the realization of a new type of Boolean
logic, one that is self-organizing. Self-organizing logic gates are
&quot;terminal-agnostic&quot;, namely they do not distinguish between input and output
terminals. When appropriately assembled to represent a given
combinatorial/optimization problem, the corresponding self-organizing circuit
converges to the equilibrium points that express the solutions of the problem
at hand. In doing so, DMMs take advantage of the long-range order that develops
during the transient dynamics. This collective dynamical behavior, reminiscent
of a phase transition, or even the &quot;edge of chaos&quot;, is mediated by families of
classical trajectories (instantons) that connect critical points of increasing
stability in the system&apos;s phase space. The topological character of the
solution search renders DMMs robust against noise and structural disorder.
Since DMMs are non-quantum systems described by ordinary differential
equations, not only can they be built in hardware with available technology,
they can also be simulated efficiently on modern classical computers. As an
example, we will show the polynomial-time solution of the subset-sum problem
for the worst case...
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ventra_M/0/1/0/all/0/1&quot;&gt;Massimiliano Di Ventra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traversa_F/0/1/0/all/0/1&quot;&gt;Fabio L. Traversa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07008">
<title>Segmentation hi\&apos;erarchique faiblement supervis\&apos;ee. (arXiv:1802.07008v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07008</link>
<description rdf:parseType="Literal">&lt;p&gt;Image segmentation is the process of partitioning an image into a set of
meaningful regions according to some criteria. Hierarchical segmentation has
emerged as a major trend in this regard as it favors the emergence of important
regions at different scales. On the other hand, many methods allow us to have
prior information on the position of structures of interest in the images. In
this paper, we present a versatile hierarchical segmentation method that takes
into account any prior spatial information and outputs a hierarchical
segmentation that emphasizes the contours or regions of interest while
preserving the important structures in the image. An application of this method
to the weakly-supervised segmentation problem is presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fehri_A/0/1/0/all/0/1&quot;&gt;Amin Fehri&lt;/a&gt; (CMM), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Velasco_Forero_S/0/1/0/all/0/1&quot;&gt;Santiago Velasco-Forero&lt;/a&gt; (CMM), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meyer_F/0/1/0/all/0/1&quot;&gt;Fernand Meyer&lt;/a&gt; (CMM)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07034">
<title>Memetic Graph Clustering. (arXiv:1802.07034v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.07034</link>
<description rdf:parseType="Literal">&lt;p&gt;It is common knowledge that there is no single best strategy for graph
clustering, which justifies a plethora of existing approaches. In this paper,
we present a general memetic algorithm, VieClus, to tackle the graph clustering
problem. This algorithm can be adapted to optimize different objective
functions. A key component of our contribution are natural recombine operators
that employ ensemble clusterings as well as multi-level techniques. Lastly, we
combine these techniques with a scalable communication protocol, producing a
system that is able to compute high-quality solutions in a short amount of
time. We instantiate our scheme with local search for modularity and show that
our algorithm successfully improves or reproduces all entries of the 10th
DIMACS implementation~challenge under consideration using a small amount of
time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biedermann_S/0/1/0/all/0/1&quot;&gt;Sonja Biedermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henzinger_M/0/1/0/all/0/1&quot;&gt;Monika Henzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_C/0/1/0/all/0/1&quot;&gt;Christian Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuster_B/0/1/0/all/0/1&quot;&gt;Bernhard Schuster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07245">
<title>Meta-Reinforcement Learning of Structured Exploration Strategies. (arXiv:1802.07245v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07245</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploration is a fundamental challenge in reinforcement learning (RL). Many
of the current exploration methods for deep RL use task-agnostic objectives,
such as information gain or bonuses based on state visitation. However, many
practical applications of RL involve learning more than a single task, and
prior tasks can be used to inform how exploration should be performed in new
tasks. In this work, we explore how prior tasks can inform an agent about how
to explore effectively in new situations. We introduce a novel gradient-based
fast adaptation algorithm -- model agnostic exploration with structured noise
(MAESN) -- to learn exploration strategies from prior experience. The prior
experience is used both to initialize a policy and to acquire a latent
exploration space that can inject structured stochasticity into a policy,
producing exploration strategies that are informed by prior knowledge and are
more effective than random action-space noise. We show that MAESN is more
effective at learning exploration strategies when compared to prior meta-RL
methods, RL without learned exploration strategies, and task-agnostic
exploration methods. We evaluate our method on a variety of simulated tasks:
locomotion with a wheeled robot, locomotion with a quadrupedal walker, and
object manipulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendonca_R/0/1/0/all/0/1&quot;&gt;Russell Mendonca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;YuXuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06673">
<title>Neon2: Finding Local Minima via First-Order Oracles. (arXiv:1711.06673v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06673</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a reduction for non-convex optimization that can (1) turn an
stationary-point finding algorithm into an local-minimum finding one, and (2)
replace the Hessian-vector product computations with only gradient
computations. It works both in the stochastic and the deterministic settings,
without hurting the algorithm&apos;s performance.
&lt;/p&gt;
&lt;p&gt;As applications, our reduction turns Natasha2 into a first-order method
without hurting its performance. It also converts SGD, GD, SCSG, and SVRG into
algorithms finding approximate local minima, outperforming some best known
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Allen-Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06434">
<title>EffNet: An Efficient Structure for Convolutional Neural Networks. (arXiv:1801.06434v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06434</link>
<description rdf:parseType="Literal">&lt;p&gt;With the ever increasing application of Convolutional Neural Networks to
costumer products the need emerges for models which can efficiently run on
embedded, mobile hardware. Slimmer models have therefore become a hot research
topic with multiple different approaches which vary from binary networks to
revised convolution layers. We offer our contribution to the latter and propose
a novel convolution block which significantly reduces the computational burden
while surpassing the current state-of-the-art. Our model, dubbed EffNet, is
optimised for models which are slim to begin with and is created to tackle
issues in existing models such as MobileNet and ShuffleNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freeman_I/0/1/0/all/0/1&quot;&gt;Ido Freeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roese_Koerner_L/0/1/0/all/0/1&quot;&gt;Lutz Roese-Koerner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kummert_A/0/1/0/all/0/1&quot;&gt;Anton Kummert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06768">
<title>Design and software implementation of subsystems for creating and using the ontological base of a research scientist. (arXiv:1802.06768v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.06768</link>
<description rdf:parseType="Literal">&lt;p&gt;Creation of the information systems and tools for scientific research and
development support has always been one of the central directions of the
development of computer science. The main features of the modern evolution of
scientific research and development are the transdisciplinary approach and the
deep intellectualisation of all stages of the life cycle of formulation and
solution of scientific problems. The theoretical and practical aspects of the
development of perspective complex knowledge-oriented information systems and
their components are considered in the paper. The analysis of existing
scientific information systems (or current research information systems, CRIS)
and synthesis of general principles of design of the research and development
workstation environment of a researcher and its components are carried out in
the work. The functional components of knowledge-oriented information system
research and development workstation environment of a researcher are designed.
Designed and developed functional components of knowledge-oriented information
system developing research and development workstation environment,including
functional models and software implementation of the software subsystem for
creation and use of ontological knowledge base for research fellow
publications, as part of personalized knowledge base of scientific researcher.
Research in modern conditions of e-Science paradigm requires pooling scientific
community and intensive exchange of research results that may be achieved
through the use of scientific information systems. research and development
workstation environment allows to solve problems of contructivisation and
formalisation of knowledge representation, obtained during the research process
and collective accomplices interaction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palagin_O/0/1/0/all/0/1&quot;&gt;O. V. Palagin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malakhov_K/0/1/0/all/0/1&quot;&gt;K. S. Malakhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velichko_V/0/1/0/all/0/1&quot;&gt;V. Yu. Velichko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shurov_O/0/1/0/all/0/1&quot;&gt;O. S. Shurov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06771">
<title>Bayes-optimal Hierarchical Classification over Asymmetric Tree-Distance Loss. (arXiv:1802.06771v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.06771</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical classification is supervised multi-class classification problem
over the set of class labels organized according to a hierarchy. In this
report, we study the work by Ramaswamy et. al. on hierarchical classification
over symmetric tree distance loss. We extend the consistency of hierarchical
classification algorithm over asymmetric tree distance loss. We design a
$\mathcal{O}(nk\log{}n)$ algorithm to find Bayes optimal classification for a
k-ary tree as a hierarchy. We show that under reasonable assumptions over
asymmetric loss function, the Bayes optimal classification over this asymmetric
loss can be found in $\mathcal{O}(k\log{}n)$. We exploit this insight and
attempt to extend the Ova-Cascade algorithm \citet{ramaswamy2015convex} for
hierarchical classification over the asymmetric loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mekala_D/0/1/0/all/0/1&quot;&gt;Dheeraj Mekala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1&quot;&gt;Vivek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1&quot;&gt;Purushottam Kar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnick_H/0/1/0/all/0/1&quot;&gt;Harish Karnick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06806">
<title>Divide, Denoise, and Defend against Adversarial Attacks. (arXiv:1802.06806v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.06806</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks, although shown to be a successful class of machine
learning algorithms, are known to be extremely unstable to adversarial
perturbations. Improving the robustness of neural networks against these
attacks is important, especially for security-critical applications. To defend
against such attacks, we propose dividing the input image into multiple
patches, denoising each patch independently, and reconstructing the image,
without losing significant image content. This proposed defense mechanism is
non-differentiable which makes it non-trivial for an adversary to apply
gradient-based attacks. Moreover, we do not fine-tune the network with
adversarial examples, making it more robust against unknown attacks. We present
a thorough analysis of the tradeoff between accuracy and robustness against
adversarial attacks. We evaluate our method under black-box, grey-box, and
white-box settings. The proposed method outperforms the state-of-the-art by a
significant margin on the ImageNet dataset under grey-box attacks while
maintaining good accuracy on clean images. We also establish a strong baseline
for a novel white-box attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1&quot;&gt;Seyed-Mohsen Moosavi-Dezfooli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1&quot;&gt;Ashish Shrivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1&quot;&gt;Oncel Tuzel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06816">
<title>Shield: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression. (arXiv:1802.06816v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.06816</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapidly growing body of research in adversarial machine learning has
demonstrated that deep neural networks (DNNs) are highly vulnerable to
adversarially generated images. This underscores the urgent need for practical
defense that can be readily deployed to combat attacks in real-time. Observing
that many attack strategies aim to perturb image pixels in ways that are
visually imperceptible, we place JPEG compression at the core of our proposed
Shield defense framework, utilizing its capability to effectively &quot;compress
away&quot; such pixel manipulation. To immunize a DNN model from artifacts
introduced by compression, Shield &quot;vaccinates&quot; a model by re-training it with
compressed images, where different compression levels are applied to generate
multiple vaccinated models that are ultimately used together in an ensemble
defense. On top of that, Shield adds an additional layer of protection by
employing randomization at test time that compresses different regions of an
image using random compression levels, making it harder for an adversary to
estimate the transformation performed. This novel combination of vaccination,
ensembling, and randomization makes Shield a fortified multi-pronged
protection. We conducted extensive, large-scale experiments using the ImageNet
dataset, and show that our approaches eliminate up to 94% of black-box attacks
and 98% of gray-box attacks delivered by the recent, strongest attacks, such as
Carlini-Wagner&apos;s L2 and DeepFool. Our approaches are fast and work without
requiring knowledge about the model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1&quot;&gt;Nilaksh Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanbhogue_M/0/1/0/all/0/1&quot;&gt;Madhuri Shanbhogue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shang-Tse Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohman_F/0/1/0/all/0/1&quot;&gt;Fred Hohman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Li Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kounavis_M/0/1/0/all/0/1&quot;&gt;Michael E. Kounavis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06832">
<title>Deep Learning for Joint Source-Channel Coding of Text. (arXiv:1802.06832v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1802.06832</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of joint source and channel coding of structured data
such as natural language over a noisy channel. The typical approach to this
problem in both theory and practice involves performing source coding to first
compress the text and then channel coding to add robustness for the
transmission across the channel. This approach is optimal in terms of
minimizing end-to-end distortion with arbitrarily large block lengths of both
the source and channel codes when transmission is over discrete memoryless
channels. However, the optimality of this approach is no longer ensured for
documents of finite length and limitations on the length of the encoding. We
will show in this scenario that we can achieve lower word error rates by
developing a deep learning based encoder and decoder. While the approach of
separate source and channel coding would minimize bit error rates, our approach
preserves semantic information of sentences by first embedding sentences in a
semantic space where sentences closer in meaning are located closer together,
and then performing joint source and channel coding on these embeddings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farsad_N/0/1/0/all/0/1&quot;&gt;Nariman Farsad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1&quot;&gt;Milind Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldsmith_A/0/1/0/all/0/1&quot;&gt;Andrea Goldsmith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06840">
<title>Voice Impersonation using Generative Adversarial Networks. (arXiv:1802.06840v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1802.06840</link>
<description rdf:parseType="Literal">&lt;p&gt;Voice impersonation is not the same as voice transformation, although the
latter is an essential element of it. In voice impersonation, the resultant
voice must convincingly convey the impression of having been naturally produced
by the target speaker, mimicking not only the pitch and other perceivable
signal qualities, but also the style of the target speaker. In this paper, we
propose a novel neural network based speech quality- and style- mimicry
framework for the synthesis of impersonated voices. The framework is built upon
a fast and accurate generative adversarial network model. Given spectrographic
representations of source and target speakers&apos; voices, the model learns to
mimic the target speaker&apos;s voice quality and style, regardless of the
linguistic content of either&apos;s voice, generating a synthetic spectrogram from
which the time domain signal is reconstructed using the Griffin-Lim method. In
effect, this model reframes the well-known problem of style-transfer for images
as the problem of style-transfer for speech signals, while intrinsically
addressing the problem of durational variability of speech sounds. Experiments
demonstrate that the model can generate extremely convincing samples of
impersonated speech. It is even able to impersonate voices across different
genders effectively. Results are qualitatively evaluated using standard
procedures for evaluating synthesized voices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rita Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1&quot;&gt;Bhiksha Raj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06866">
<title>Expert System for Diagnosis of Chest Diseases Using Neural Networks. (arXiv:1802.06866v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.06866</link>
<description rdf:parseType="Literal">&lt;p&gt;This article represents one of the contemporary trends in the application of
the latest methods of information and communication technology for medicine
through an expert system helps the doctor to diagnose some chest diseases which
is important because of the frequent spread of chest diseases nowadays in
addition to the overlap symptoms of these diseases, which is difficult to right
diagnose by doctors with several algorithms: Forward Chaining, Backward
Chaining, Neural Network(Back Propagation). However, this system cannot replace
the doctor function, but it can help the doctor to avoid wrong diagnosis and
treatments. It can also be developed in such a way to help the novice doctors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kayali_I/0/1/0/all/0/1&quot;&gt;Ismail Kayali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06881">
<title>Automated Playtesting with Procedural Personas through MCTS with Evolved Heuristics. (arXiv:1802.06881v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.06881</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes a method for generative player modeling and its
application to the automatic testing of game content using archetypal player
models called procedural personas. Theoretically grounded in psychological
decision theory, procedural personas are implemented using a variation of Monte
Carlo Tree Search (MCTS) where the node selection criteria are developed using
evolutionary computation, replacing the standard UCB1 criterion of MCTS. Using
these personas we demonstrate how generative player models can be applied to a
varied corpus of game levels and demonstrate how different play styles can be
enacted in each level. In short, we use artificially intelligent personas to
construct synthetic playtesters. The proposed approach could be used as a tool
for automatic play testing when human feedback is not readily available or when
quick visualization of potential interactions is necessary. Possible
applications include interactive tools during game development or procedural
content generation systems where many evaluations must be conducted within a
short time span.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holmgaard_C/0/1/0/all/0/1&quot;&gt;Christoffer Holmg&amp;#xe5;rd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1&quot;&gt;Michael Cerny Green&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1&quot;&gt;Antonios Liapis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julian Togelius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06888">
<title>Superrational types. (arXiv:1802.06888v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.06888</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a formal analysis of Douglas Hofstadter&apos;s concept of
\emph{superrationality}. We start by defining superrationally justifiable
actions, and study them in symmetric games. We then model the beliefs of the
players, in a way that leads them to different choices than the usual
assumption of rationality by restricting the range of conceivable choices.
These beliefs are captured in the formal notion of \emph{type} drawn from
epistemic game theory. The theory of coalgebras is used to frame type spaces
and to account for the existence of some of them. We find conditions that
guarantee superrational outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tohme_F/0/1/0/all/0/1&quot;&gt;Fernando Tohm&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viglizzo_I/0/1/0/all/0/1&quot;&gt;Ignacio Viglizzo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06891">
<title>Fourier Policy Gradients. (arXiv:1802.06891v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.06891</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new way of deriving policy gradient updates for reinforcement
learning. Our technique, based on Fourier analysis, recasts integrals that
arise with expected policy gradients as convolutions and turns them into
multiplications. The obtained analytical solutions allow us to capture the low
variance benefits of EPG in a broad range of settings. For the critic, we treat
trigonometric and radial basis functions, two function families with the
universal approximation property. The choice of policy can be almost arbitrary,
including mixtures or hybrid continuous-discrete probability distributions.
Moreover, we derive a general family of sample-based estimators for stochastic
policy gradients, which unifies existing results on sample-based approximation.
We believe that this technique has the potential to shape the next generation
of policy gradient approaches, powered by analytical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fellows_M/0/1/0/all/0/1&quot;&gt;Matthew Fellows&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ciosek_K/0/1/0/all/0/1&quot;&gt;Kamil Ciosek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06926">
<title>Scale Optimization for Full-Image-CNN Vehicle Detection. (arXiv:1802.06926v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.06926</link>
<description rdf:parseType="Literal">&lt;p&gt;Many state-of-the-art general object detection methods make use of shared
full-image convolutional features (as in Faster R-CNN). This achieves a
reasonable test-phase computation time while enjoys the discriminative power
provided by large Convolutional Neural Network (CNN) models. Such designs excel
on benchmarks which contain natural images but which have very unnatural
distributions, i.e. they have an unnaturally high-frequency of the target
classes and a bias towards a &quot;friendly&quot; or &quot;dominant&quot; object scale. In this
paper we present further study of the use and adaptation of the Faster R-CNN
object detection method for datasets presenting natural scale distribution and
unbiased real-world object frequency. In particular, we show that better
alignment of the detector scale sensitivity to the extant distribution improves
vehicle detection performance. We do this by modifying both the selection of
Region Proposals, and through using more scale-appropriate full-image
convolution features within the CNN model. By selecting better scales in the
region proposal input and by combining feature maps through careful design of
the convolutional neural network, we improve performance on smaller objects. We
significantly increase detection AP for the KITTI dataset car class from 76.3%
on our baseline Faster R-CNN detector to 83.6% in our improved detector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shouyan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaimin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Q/0/1/0/all/0/1&quot;&gt;Qian Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yang Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1&quot;&gt;Tong Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Overett_G/0/1/0/all/0/1&quot;&gt;Gary Overett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06940">
<title>Using Automatic Generation of Relaxation Constraints to Improve the Preimage Attack on 39-step MD4. (arXiv:1802.06940v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.06940</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we construct preimage attack on the truncated variant of the
MD4 hash function. Specifically, we study the MD4-39 function defined by the
first 39 steps of the MD4 algorithm. We suggest a new attack on MD4-39, which
develops the ideas proposed by H. Dobbertin in 1998. Namely, the special
relaxation constraints are introduced in order to simplify the equations
corresponding to the problem of finding a preimage for an arbitrary MD4-39 hash
value. The equations supplemented with the relaxation constraints are then
reduced to the Boolean Satisfiability Problem (SAT) and solved using the
state-of-the-art SAT solvers. We show that the effectiveness of a set of
relaxation constraints can be evaluated using the black-box function of a
special kind. Thus, we suggest automatic method of relaxation constraints
generation by applying the black-box optimization to this function. The
proposed method made it possible to find new relaxation constraints that
contribute to a SAT-based preimage attack on MD4-39 which significantly
outperforms the competition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irina_G/0/1/0/all/0/1&quot;&gt;Gribanova Irina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_S/0/1/0/all/0/1&quot;&gt;Semenov Alexander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06963">
<title>Neural Network Ensembles to Real-time Identification of Plug-level Appliance Measurements. (arXiv:1802.06963v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.06963</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of identifying end-use electrical appliances from their
individual consumption profiles, known as the appliance identification problem,
is a primary stage in both Non-Intrusive Load Monitoring (NILM) and automated
plug-wise metering. Therefore, appliance identification has received dedicated
studies with various electric appliance signatures, classification models, and
evaluation datasets. In this paper, we propose a neural network ensembles
approach to address this problem using high resolution measurements. The models
are trained on the raw current and voltage waveforms, and thus, eliminating the
need for well engineered appliance signatures. We evaluate the proposed model
on a publicly available appliance dataset from 55 residential buildings, 11
appliance categories, and over 1000 measurements. We further study the
stability of the trained models with respect to training dataset, sampling
frequency, and variations in the steady-state operation of appliances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barsim_K/0/1/0/all/0/1&quot;&gt;Karim Said Barsim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mauch_L/0/1/0/all/0/1&quot;&gt;Lukas Mauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07073">
<title>Robust Maximization of Non-Submodular Objectives. (arXiv:1802.07073v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07073</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of maximizing a monotone set function subject to a
cardinality constraint $k$ in the setting where some number of elements $\tau$
is deleted from the returned set. The focus of this work is on the worst-case
adversarial setting. While there exist constant-factor guarantees when the
function is submodular, there are no guarantees for non-submodular objectives.
In this work, we present a new algorithm Oblivious-Greedy and prove the first
constant-factor approximation guarantees for a wider class of non-submodular
objectives. The obtained theoretical bounds are the first constant-factor
bounds that also hold in the linear regime, i.e. when the number of deletions
$\tau$ is linear in $k$. Our bounds depend on established parameters such as
the submodularity ratio and some novel ones such as the inverse curvature. We
bound these parameters for two important objectives including support selection
and variance reduction. Finally, we numerically demonstrate the robust
performance of Oblivious-Greedy for these two objectives on various datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bogunovic_I/0/1/0/all/0/1&quot;&gt;Ilija Bogunovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junyao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.08176">
<title>What is Wrong with Topic Modeling? (and How to Fix it Using Search-based Software Engineering). (arXiv:1608.08176v4 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/1608.08176</link>
<description rdf:parseType="Literal">&lt;p&gt;Context: Topic modeling finds human-readable structures in unstructured
textual data. A widely used topic modeler is Latent Dirichlet allocation. When
run on different datasets, LDA suffers from &quot;order effects&quot; i.e. different
topics are generated if the order of training data is shuffled. Such order
effects introduce a systematic error for any study. This error can relate to
misleading results;specifically, inaccurate topic descriptions and a reduction
in the efficacy of text mining classification results. Objective: To provide a
method in which distributions generated by LDA are more stable and can be used
for further analysis. Method: We use LDADE, a search-based software engineering
tool that tunes LDA&apos;s parameters using DE (Differential Evolution). LDADE is
evaluated on data from a programmer information exchange site (Stackoverflow),
title and abstract text of thousands ofSoftware Engineering (SE) papers, and
software defect reports from NASA. Results were collected across different
implementations of LDA (Python+Scikit-Learn, Scala+Spark); across different
platforms (Linux, Macintosh) and for different kinds of LDAs (VEM,or using
Gibbs sampling). Results were scored via topic stability and text mining
classification accuracy. Results: In all treatments: (i) standard LDA exhibits
very large topic instability; (ii) LDADE&apos;s tunings dramatically reduce cluster
instability; (iii) LDADE also leads to improved performances for supervised as
well as unsupervised learning. Conclusion: Due to topic instability, using
standard LDA with its &quot;off-the-shelf&quot; settings should now be depreciated. Also,
in future, we should require SE papers that use LDA to test and (if needed)
mitigate LDA topic instability. Finally, LDADE is a candidate technology for
effectively and efficiently reducing that instability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1&quot;&gt;Amritanshu Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1&quot;&gt;Wei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1&quot;&gt;Tim Menzies&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11431">
<title>Physics-guided Neural Networks (PGNN): An Application in Lake Temperature Modeling. (arXiv:1710.11431v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11431</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel framework for combining scientific knowledge of
physics-based models with neural networks to advance scientific discovery. This
framework, termed as physics-guided neural network (PGNN), leverages the output
of physics-based model simulations along with observational features to
generate predictions using a neural network architecture. Further, this paper
presents a novel framework for using physics-based loss functions in the
learning objective of neural networks, to ensure that the model predictions not
only show lower errors on the training set but are also scientifically
consistent with the known physics on the unlabeled set. We illustrate the
effectiveness of PGNN for the problem of lake temperature modeling, where
physical relationships between the temperature, density, and depth of water are
used to design a physics-based loss function. By using scientific knowledge to
guide the construction and learning of neural networks, we are able to show
that the proposed framework ensures better generalizability as well as
scientific consistency of results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karpatne_A/0/1/0/all/0/1&quot;&gt;Anuj Karpatne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watkins_W/0/1/0/all/0/1&quot;&gt;William Watkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1&quot;&gt;Jordan Read&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vipin Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08186">
<title>MAttNet: Modular Attention Network for Referring Expression Comprehension. (arXiv:1801.08186v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.08186</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address referring expression comprehension: localizing an
image region described by a natural language expression. While most recent work
treats expressions as a single unit, we propose to decompose them into three
modular components related to subject appearance, location, and relationship to
other objects. This allows us to flexibly adapt to expressions containing
different types of information in an end-to-end framework. In our model, which
we call the Modular Attention Network (MAttNet), two types of attention are
utilized: language-based attention that learns the module weights as well as
the word/phrase attention that each module should focus on; and visual
attention that allows the subject and relationship modules to focus on relevant
image components. Module weights combine scores from all three modules
dynamically to output an overall score. Experiments show that MAttNet
outperforms previous state-of-art methods by a large margin on both
bounding-box-level and pixel-level comprehension tasks. Demo and code are
provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Licheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhe Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaohui Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jimei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1&quot;&gt;Tamara L.Berg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06137">
<title>Implicit Robot-Human Communication in Adversarial and Collaborative Environments. (arXiv:1802.06137v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06137</link>
<description rdf:parseType="Literal">&lt;p&gt;Users of AI systems may rely upon them to produce plans for achieving desired
objectives. Such AI systems should be able to compute obfuscated plans whose
execution in adversarial situations protects privacy as well as legible plans
which are easy for team-members to understand in collaborative situations. We
develop a unified framework that addresses these dual problems by computing
plans with a desired level of comprehensibility from the point of view of a
partially informed observer. Our approach produces obfuscated plans with
observations that are consistent with at least &apos;k&apos; goals from a given set of
decoy goals. In addition, when the goal is known to the observer, our approach
generates obfuscated plans with observations that are diverse with at least &apos;l&apos;
candidate plans. Our approach for plan legibility produces plans that achieve a
goal while being consistent with at most &apos;j&apos; goals in a given set of
confounding goals. We provide an empirical evaluation to show the feasibility
and usefulness of our approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1&quot;&gt;Anagha Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1&quot;&gt;Siddharth Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1&quot;&gt;Subbarao Kambhampati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06306">
<title>Optimizing Interactive Systems with Data-Driven Objectives. (arXiv:1802.06306v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06306</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective optimization is essential for interactive systems to provide a
satisfactory user experience. However, it is often challenging to find an
objective to optimize for. Generally, such objectives are manually crafted and
rarely capture complex user needs accurately. Conversely, we propose an
approach that infers the objective directly from observed user interactions.
These inferences can be made regardless of prior knowledge and across different
types of user behavior. Then we introduce: Interactive System Optimizer (ISO),
a novel algorithm that uses these inferred objectives for optimization. Our
main contribution is a new general principled approach to optimizing
interactive systems using data-driven objectives. We demonstrate the high
effectiveness of ISO over several GridWorld simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grotov_A/0/1/0/all/0/1&quot;&gt;Artem Grotov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1&quot;&gt;Julia Kiseleva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1&quot;&gt;Maarten de Rijke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oosterhuis_H/0/1/0/all/0/1&quot;&gt;Harrie Oosterhuis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06588">
<title>A Machine Learning Approach to Air Traffic Route Choice Modelling. (arXiv:1802.06588v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06588</link>
<description rdf:parseType="Literal">&lt;p&gt;Air Traffic Flow and Capacity Management (ATFCM) is one of the constituent
parts of Air Traffic Management (ATM). The goal of ATFCM is to make airport and
airspace capacity meet traffic demand and, when capacity opportunities are
exhausted, optimise traffic flows to meet the available capacity. One of the
key enablers of ATFCM is the accurate estimation of future traffic demand. The
available information (schedules, flight plans, etc.) and its associated level
of uncertainty differ across the different ATFCM planning phases, leading to
qualitative differences between the types of forecasting that are feasible at
each time horizon. While abundant research has been conducted on tactical
trajectory prediction (i.e., during the day of operations), trajectory
prediction in the pre-tactical phase, when few or no flight plans are
available, has received much less attention. As a consequence, the methods
currently in use for pre-tactical traffic forecast are still rather
rudimentary, often resulting in suboptimal ATFCM decision making. This paper
proposes a machine learning approach for the prediction of airlines route
choices between two airports as a function of route characteristics, such as
flight efficiency, air navigation charges and expected level of congestion.
Different predictive models based on multinomial logistic regression and
decision trees are formulated and calibrated with historical traffic data, and
a critical evaluation of each model is conducted. We analyse the predictive
power of each model in terms of its ability to forecast traffic volumes at the
level of charging zones, proving significant potential to enhance pre-tactical
traffic forecast. We conclude by discussing the limitations and room for
improvement of the proposed approach, as well as the future developments
required to produce reliable traffic forecasts at a higher spatial and temporal
resolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcos_R/0/1/0/all/0/1&quot;&gt;Rodrigo Marcos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Cantu_O/0/1/0/all/0/1&quot;&gt;Oliva Garc&amp;#xed;a-Cant&amp;#xfa;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herranz_R/0/1/0/all/0/1&quot;&gt;Ricardo Herranz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06604">
<title>Learning High-level Representations from Demonstrations. (arXiv:1802.06604v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06604</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical learning (HL) is key to solving complex sequential decision
problems with long horizons and sparse rewards. It allows learning agents to
break-up large problems into smaller, more manageable subtasks. A common
approach to HL, is to provide the agent with a number of high-level skills that
solve small parts of the overall problem. A major open question, however, is
how to identify a suitable set of reusable skills. We propose a principled
approach that uses human demonstrations to infer a set of subgoals based on
changes in the demonstration dynamics. Using these subgoals, we decompose the
learning problem into an abstract high-level representation and a set of
low-level subtasks. The abstract description captures the overall problem
structure, while subtasks capture desired skills. We demonstrate that we can
jointly optimize over both levels of learning. We show that the resulting
method significantly outperforms previous baselines on two challenging
problems: the Atari 2600 game Montezuma&apos;s Revenge, and a simulated robotics
problem moving the ant robot through a maze.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andersen_G/0/1/0/all/0/1&quot;&gt;Garrett Andersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vrancx_P/0/1/0/all/0/1&quot;&gt;Peter Vrancx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bou_Ammar_H/0/1/0/all/0/1&quot;&gt;Haitham Bou-Ammar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02565">
<title>Applying Cooperative Machine Learning to Speed Up the Annotation of Social Signals in Large Multi-modal Corpora. (arXiv:1802.02565v1 [cs.HC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1802.02565</link>
<description rdf:parseType="Literal">&lt;p&gt;Scientific disciplines, such as Behavioural Psychology, Anthropology and
recently Social Signal Processing are concerned with the systematic exploration
of human behaviour. A typical work-flow includes the manual annotation (also
called coding) of social signals in multi-modal corpora of considerable size.
For the involved annotators this defines an exhausting and time-consuming task.
In the article at hand we present a novel method and also provide the tools to
speed up the coding procedure. To this end, we suggest and evaluate the use of
Cooperative Machine Learning (CML) techniques to reduce manual labelling
efforts by combining the power of computational capabilities and human
intelligence. The proposed CML strategy starts with a small number of labelled
instances and concentrates on predicting local parts first. Afterwards, a
session-independent classification model is created to finish the remaining
parts of the database. Confidence values are computed to guide the manual
inspection and correction of the predictions. To bring the proposed approach
into application we introduce NOVA - an open-source tool for collaborative and
machine-aided annotations. In particular, it gives labellers immediate access
to CML strategies and directly provides visual feedback on the results. Our
experiments show that the proposed method has the potential to significantly
reduce human labelling efforts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1&quot;&gt;Johannes Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baur_T/0/1/0/all/0/1&quot;&gt;Tobias Baur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valstar_M/0/1/0/all/0/1&quot;&gt;Michel F. Valstar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Schuller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1&quot;&gt;Elisabeth Andr&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06765">
<title>Interpretable VAEs for nonlinear group factor analysis. (arXiv:1802.06765v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.06765</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models have recently yielded encouraging results in producing
subjectively realistic samples of complex data. Far less attention has been
paid to making these generative models interpretable. In many scenarios,
ranging from scientific applications to finance, the observed variables have a
natural grouping. It is often of interest to understand systems of interaction
amongst these groups, and latent factor models (LFMs) are an attractive
approach. However, traditional LFMs are limited by assuming a linear
correlation structure. We present an output interpretable VAE (oi-VAE) for
grouped data that models complex, nonlinear latent-to-observed relationships.
We combine a structured VAE comprised of group-specific generators with a
sparsity-inducing prior. We demonstrate that oi-VAE yields meaningful notions
of interpretability in the analysis of motion capture and MEG data. We further
show that in these situations, the regularization inherent to oi-VAE can
actually lead to improved generalization and learned generative processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ainsworth_S/0/1/0/all/0/1&quot;&gt;Samuel Ainsworth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foti_N/0/1/0/all/0/1&quot;&gt;Nicholas Foti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Adrian KC Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1&quot;&gt;Emily Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06807">
<title>On the Complexity of Opinions and Online Discussions. (arXiv:1802.06807v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1802.06807</link>
<description rdf:parseType="Literal">&lt;p&gt;In an increasingly polarized world, demagogues who reduce complexity down to
simple arguments based on emotion are gaining in popularity. Are opinions and
online discussions falling into demagoguery? In this work, we aim to provide
computational tools to investigate this question and, by doing so, explore the
nature and complexity of online discussions and their space of opinions,
uncovering where each participant lies.
&lt;/p&gt;
&lt;p&gt;More specifically, we present a modeling framework to construct latent
representations of opinions in online discussions which are consistent with
human judgements, as measured by online voting. If two opinions are close in
the resulting latent space of opinions, it is because humans think they are
similar. Our modeling framework is theoretically grounded and establishes a
surprising connection between opinion and voting models and the sign-rank of a
matrix. Moreover, it also provides a set of practical algorithms to both
estimate the dimension of the latent space of opinions and infer where opinions
expressed by the participants of an online discussion lie in this space.
Experiments on a large dataset from Yahoo! News, Yahoo! Finance, Yahoo! Sports,
and the Newsroom app suggest that unidimensional opinion models may be often
unable to accurately represent online discussions, provide insights into human
judgements and opinions, and show that our framework is able to circumvent
language nuances such as sarcasm or humor by relying on human judgements
instead of textual analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1&quot;&gt;Utkarsh Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1&quot;&gt;Abir De&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappu_A/0/1/0/all/0/1&quot;&gt;Aasish Pappu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_M/0/1/0/all/0/1&quot;&gt;Manuel Gomez-Rodriguez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06820">
<title>Tools for higher-order network analysis. (arXiv:1802.06820v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1802.06820</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks are a fundamental model of complex systems throughout the sciences,
and network datasets are typically analyzed through lower-order connectivity
patterns described at the level of individual nodes and edges. However,
higher-order connectivity patterns captured by small subgraphs, also called
network motifs, describe the fundamental structures that control and mediate
the behavior of many complex systems. We develop three tools for network
analysis that use higher-order connectivity patterns to gain new insights into
network datasets: (1) a framework to cluster nodes into modules based on joint
participation in network motifs; (2) a generalization of the clustering
coefficient measurement to investigate higher-order closure patterns; and (3) a
definition of network motifs for temporal networks and fast algorithms for
counting them. Using these tools, we analyze data from biology, ecology,
economics, neuroscience, online social networks, scientific collaborations,
telecommunications, transportation, and the World Wide Web.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benson_A/0/1/0/all/0/1&quot;&gt;Austin R. Benson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06823">
<title>Entropy-Isomap: Manifold Learning for High-dimensional Dynamic Processes. (arXiv:1802.06823v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.06823</link>
<description rdf:parseType="Literal">&lt;p&gt;Scientific and engineering processes produce massive high-dimensional data
sets that are generated as highly non-linear transformations of an initial
state and few process parameters. Mapping such data to a low-dimensional
manifold can facilitate better understanding of the underlying process, and
ultimately their optimization. We show that off-the-shelf non-linear spectral
dimensionality methods, such as Isomap, fail for such data, primarily due to
the presence of strong temporal correlation among observations belonging to the
same process pathways. We propose a novel method, Entropy-Isomap, to address
this issue. The proposed method is successfully applied to morphology evolution
data of the organic thin film fabrication process. The resulting output is
ordered by the process variables. It allows for low-dimensional visualization
of the morphological pathways, and provides key insights to guide subsequent
design and exploration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schoeneman_F/0/1/0/all/0/1&quot;&gt;Frank Schoeneman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chandola_V/0/1/0/all/0/1&quot;&gt;Varun Chandola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Napp_N/0/1/0/all/0/1&quot;&gt;Nils Napp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wodo_O/0/1/0/all/0/1&quot;&gt;Olga Wodo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zola_J/0/1/0/all/0/1&quot;&gt;Jaroslaw Zola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06847">
<title>Distribution Matching in Variational Inference. (arXiv:1802.06847v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.06847</link>
<description rdf:parseType="Literal">&lt;p&gt;The difficulties in matching the latent posterior to the prior, balancing
powerful posteriors with computational efficiency, and the reduced flexibility
of data likelihoods are the biggest challenges in the advancement of
Variational Autoencoders. We show that these issues arise due to struggles in
marginal divergence minimization, and explore an alternative to using
conditional distributions that is inspired by Generative Adversarial Networks.
The class probability estimation that GANs offer for marginal divergence
minimization uncovers a family of VAE-GAN hybrids, which offer the promise of
addressing these major challenges in variational inference. We systematically
explore the solutions available for distribution matching, but show that these
hybrid methods do not fulfill this promise, and the trade-off between
generation and inference that they give rise to remains an ongoing research
topic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosca_M/0/1/0/all/0/1&quot;&gt;Mihaela Rosca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lakshminarayanan_B/0/1/0/all/0/1&quot;&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mohamed_S/0/1/0/all/0/1&quot;&gt;Shakir Mohamed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06875">
<title>LSALSA: efficient sparse coding in single and multiple dictionary settings. (arXiv:1802.06875v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.06875</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an efficient sparse coding (SC) framework for obtaining sparse
representation of data. The proposed framework is very general and applies to
both the single dictionary setting, where each data point is represented as a
sparse combination of the columns of one dictionary matrix, as well as the
multiple dictionary setting as given in morphological component analysis (MCA),
where the goal is to separate the data into additive parts such that each part
has distinct sparse representation within an appropriately chosen corresponding
dictionary. Both tasks have been cast as $\ell_1$-regularized optimization
problems of minimizing quadratic reconstruction error. In an effort to
accelerate traditional acquisition of sparse codes, we propose a deep learning
architecture that constitutes a trainable time-unfolded version of the Split
Augmented Lagrangian Shrinkage Algorithm (SALSA), a special case of the
alternating direction method of multipliers (ADMM). We empirically validate
both variants of the algorithm on image vision tasks and demonstrate that at
inference our networks achieve improvements in terms of the running time and
the quality of estimated sparse codes on both classic SC and MCA problems over
more common baselines. We finally demonstrate the visual advantage of our
technique on the task of source separation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cowen_B/0/1/0/all/0/1&quot;&gt;Benjamin Cowen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saridena_A/0/1/0/all/0/1&quot;&gt;Apoorva Nandini Saridena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choromanska_A/0/1/0/all/0/1&quot;&gt;Anna Choromanska&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06894">
<title>Learning Hidden Markov Models from Pairwise Co-occurrences with Applications to Topic Modeling. (arXiv:1802.06894v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1802.06894</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new algorithm for identifying the transition and emission
probabilities of a hidden Markov model (HMM) from the emitted data.
Expectation-maximization becomes computationally prohibitive for long
observation records, which are often required for identification. The new
algorithm is particularly suitable for cases where the available sample size is
large enough to accurately estimate second-order output probabilities, but not
higher-order ones. We show that if one is only able to obtain a reliable
estimate of the pairwise co-occurrence probabilities of the emissions, it is
still possible to uniquely identify the HMM if the emission probability is
\emph{sufficiently scattered}. We apply our method to hidden topic Markov
modeling, and demonstrate that we can learn topics with higher quality if
documents are modeled as observations of HMMs sharing the same emission (topic)
probability, compared to the simple but widely used bag-of-words model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kejun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xiao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sidiropoulos_N/0/1/0/all/0/1&quot;&gt;Nicholas D. Sidiropoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06903">
<title>Generalization Error Bounds with Probabilistic Guarantee for SGD in Nonconvex Optimization. (arXiv:1802.06903v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.06903</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of deep learning has led to a rising interest in the
generalization property of the stochastic gradient descent (SGD) method, and
stability is one popular approach to study it. Existing works based on
stability have studied nonconvex loss functions, but only considered the
generalization error of the SGD in expectation. In this paper, we establish
various generalization error bounds with probabilistic guarantee for the SGD.
Specifically, for both general nonconvex loss functions and gradient dominant
loss functions, we characterize the on-average stability of the iterates
generated by SGD in terms of the on-average variance of the stochastic
gradients. Such characterization leads to improved bounds for the
generalization error for SGD. We then study the regularized risk minimization
problem with strongly convex regularizers, and obtain improved generalization
error bounds for proximal SGD. With strongly convex regularizers, we further
establish the generalization error bounds for nonconvex loss functions under
proximal SGD with high-probability guarantee, i.e., exponential concentration
in probability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingbin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huishuai Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06916">
<title>Simplicial Closure and Higher-order Link Prediction. (arXiv:1802.06916v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1802.06916</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks provide a powerful formalism for modeling complex systems, by
representing the underlying set of pairwise interactions. But much of the
structure within these systems involves interactions that take place among more
than two nodes at once; for example, communication within a group rather than
person-to-person, collaboration among a team rather than a pair of co-authors,
or biological interaction between a set of molecules rather than just two. We
refer to these type of simultaneous interactions on sets of more than two nodes
as higher-order interactions; they are ubiquitous, but the empirical study of
them has lacked a general framework for evaluating higher-order models. Here we
introduce such a framework, based on link prediction, a fundamental problem in
network analysis. The traditional link prediction problem seeks to predict the
appearance of new links in a network, and here we adapt it to predict which
(larger) sets of elements will have future interactions. We study the temporal
evolution of 19 datasets from a variety of domains, and use our higher-order
formulation of link prediction to assess the types of structural features that
are most predictive of new multi-way interactions. Among our results, we find
that different domains vary considerably in their distribution of higher-order
structural parameters, and that the higher-order link prediction problem
exhibits some fundamental differences from traditional pairwise link
prediction, with a greater role for local rather than long-range information in
predicting the appearance of new interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benson_A/0/1/0/all/0/1&quot;&gt;Austin R. Benson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abebe_R/0/1/0/all/0/1&quot;&gt;Rediet Abebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaub_M/0/1/0/all/0/1&quot;&gt;Michael T.Schaub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jadbabaie_A/0/1/0/all/0/1&quot;&gt;Ali Jadbabaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1&quot;&gt;Jon Kleinberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06924">
<title>Teaching Categories to Human Learners with Visual Explanations. (arXiv:1802.06924v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.06924</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of computer-assisted teaching with explanations.
Conventional approaches for machine teaching typically only provide feedback at
the instance level e.g., the category or label of the instance. However, it is
intuitive that clear explanations from a knowledgeable teacher can
significantly improve a student&apos;s ability to learn a new concept. To address
these existing limitations, we propose a teaching framework that provides
interpretable explanations as feedback and models how the learner incorporates
this additional information. In the case of images, we show that we can
automatically generate explanations that highlight the parts of the image that
are responsible for the class label. Experiments on human learners illustrate
that, on average, participants achieve better test set performance on
challenging categorization tasks when taught with our interpretable approach
compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1&quot;&gt;Oisin Mac Aodha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shihan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1&quot;&gt;Pietro Perona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yisong Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06939">
<title>Estimator of Prediction Error Based on Approximate Message Passing for Penalized Linear Regression. (arXiv:1802.06939v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.06939</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an estimator of prediction error using an approximate message
passing (AMP) algorithm that can be applied to a broad range of sparse
penalties. Following Stein&apos;s lemma, the estimator of the generalized degrees of
freedom, which is a key quantity for the construction of the estimator of the
prediction error, is calculated at the AMP fixed point. The resulting form of
the AMP-based estimator does not depend on the penalty function, and its value
can be further improved by considering the correlation between predictors. The
proposed estimator is asymptotically unbiased when the components of the
predictors and response variables are independently generated according to a
Gaussian distribution. We examine the behaviour of the estimator for real data
under nonconvex sparse penalties, where Akaike&apos;s information criterion does not
correspond to an unbiased estimator of the prediction error. The model selected
by the proposed estimator is close to that which minimizes the true prediction
error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sakata_A/0/1/0/all/0/1&quot;&gt;Ayaka Sakata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06944">
<title>DeepThin: A Self-Compressing Library for Deep Neural Networks. (arXiv:1802.06944v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.06944</link>
<description rdf:parseType="Literal">&lt;p&gt;As the industry deploys increasingly large and complex neural networks to
mobile devices, more pressure is put on the memory and compute resources of
those devices. Deep compression, or compression of deep neural network weight
matrices, is a technique to stretch resources for such scenarios. Existing
compression methods cannot effectively compress models smaller than 1-2% of
their original size. We develop a new compression technique, DeepThin, building
on existing research in the area of low rank factorization. We identify and
break artificial constraints imposed by low rank approximations by combining
rank factorization with a reshaping process that adds nonlinearity to the
approximation function. We deploy DeepThin as a plug-gable library integrated
with TensorFlow that enables users to seamlessly compress models at different
granularities. We evaluate DeepThin on two state-of-the-art acoustic models,
TFKaldi and DeepSpeech, comparing it to previous compression work (Pruning,
HashNet, and Rank Factorization), empirical limit study approaches, and
hand-tuned models. For TFKaldi, our DeepThin networks show better word error
rates (WER) than competing methods at practically all tested compression rates,
achieving an average of 60% relative improvement over rank factorization, 57%
over pruning, 23% over hand-tuned same-size networks, and 6% over the
computationally expensive HashedNets. For DeepSpeech, DeepThin-compressed
networks achieve better test loss than all other compression methods, reaching
a 28% better result than rank factorization, 27% better than pruning, 20%
better than hand-tuned same-size networks, and 12% better than HashedNets.
DeepThin also provide inference performance benefits ranging from 2X to 14X
speedups, depending on the compression ratio and platform cache sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sotoudeh_M/0/1/0/all/0/1&quot;&gt;Matthew Sotoudeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baghsorkhi_S/0/1/0/all/0/1&quot;&gt;Sara S. Baghsorkhi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06967">
<title>Recovery of simultaneous low rank and two-way sparse coefficient matrices, a nonconvex approach. (arXiv:1802.06967v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.06967</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of recovery of matrices that are simultaneously low rank
and row and/or column sparse. Such matrices appear in recent applications in
cognitive neuroscience, imaging, computer vision, macroeconomics, and genetics.
We propose a GDT (Gradient Descent with hard Thresholding) algorithm to
efficiently recover matrices with such structure, by minimizing a bi-convex
function over a nonconvex set of constraints. We show linear convergence of the
iterates obtained by GDT to a region within statistical error of an optimal
solution. As an application of our method, we consider multi-task learning
problems and show that the statistical error rate obtained by GDT is near
optimal compared to minimax rate. Experiments demonstrate competitive
performance and much faster running speed compared to existing methods, on both
simulations and real data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Ming Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gupta_V/0/1/0/all/0/1&quot;&gt;Varun Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kolar_M/0/1/0/all/0/1&quot;&gt;Mladen Kolar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07024">
<title>Learning to Abstain via Curve Optimization. (arXiv:1802.07024v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07024</link>
<description rdf:parseType="Literal">&lt;p&gt;In practical applications of machine learning, it is often desirable to
identify and abstain on examples where the a model&apos;s predictions are likely to
be incorrect. We consider the problem of selecting a budget-constrained subset
of test examples to abstain on, with the goal of maximizing performance on the
remaining examples. We develop a novel approach to this problem by analytically
optimizing the expected marginal improvement in a desired performance metric,
such as the area under the ROC curve or Precision-Recall curve. We compare our
approach to other abstention techniques for deep learning models based on
posterior probability and uncertainty estimates obtained using test-time
dropout. On various tasks in computer vision, natural language processing, and
bioinformatics, we demonstrate the consistent effectiveness of our approach
over other techniques. We also introduce novel diagnostics based on influence
functions to understand the behavior of abstention methods in the presence of
noisy training data, and leverage the insights to propose a new influence-based
abstention method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alexandari_A/0/1/0/all/0/1&quot;&gt;Amr Alexandari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shrikumar_A/0/1/0/all/0/1&quot;&gt;Avanti Shrikumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kundaje_A/0/1/0/all/0/1&quot;&gt;Anshul Kundaje&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07028">
<title>High-Dimensional Bayesian Optimization via Additive Models with Overlapping Groups. (arXiv:1802.07028v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07028</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization (BO) is a popular technique for sequential black-box
function optimization, with applications including parameter tuning, robotics,
environmental monitoring, and more. One of the most important challenges in BO
is the development of algorithms that scale to high dimensions, which remains a
key open problem despite recent progress. In this paper, we consider the
approach of Kandasamy et al. (2015), in which the high-dimensional function
decomposes as a sum of lower-dimensional functions on subsets of the underlying
variables. In particular, we significantly generalize this approach by lifting
the assumption that the subsets are disjoint, and consider additive models with
arbitrary overlap among the subsets. By representing the dependencies via a
graph, we deduce an efficient message passing algorithm for optimizing the
acquisition function. In addition, we provide an algorithm for learning the
graph from samples based on Gibbs sampling. We empirically demonstrate the
effectiveness of our methods on both synthetic and real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolland_P/0/1/0/all/0/1&quot;&gt;Paul Rolland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarlett_J/0/1/0/all/0/1&quot;&gt;Jonathan Scarlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogunovic_I/0/1/0/all/0/1&quot;&gt;Ilija Bogunovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07051">
<title>How to Tackle an Extremely Hard Learning Problem: Learning Causal Structures from Non-Experimental Data without the Faithfulness Assumption or the Like. (arXiv:1802.07051v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07051</link>
<description rdf:parseType="Literal">&lt;p&gt;Most methods for learning causal structures from non-experimental data rely
on some assumptions of simplicity, the most famous of which is known as the
Faithfulness condition. Without assuming such conditions to begin with, we
develop a learning theory for inferring the structure of a causal Bayesian
network, and we use the theory to provide a novel justification of a certain
assumption of simplicity that is closely related to Faithfulness. Here is the
idea. With only the Markov and IID assumptions, causal learning is notoriously
too hard to achieve statistical consistency but we show that it can still
achieve a quite desirable &quot;combined&quot; mode of stochastic convergence to the
truth: having almost sure convergence to the true causal hypothesis with
respect to almost all causal Bayesian networks, together with a certain kind of
locally uniform convergence. Furthermore, every learning algorithm achieving at
least that joint mode of convergence has this property: having stochastic
convergence to the truth with respect to a causal Bayesian network $N$ only if
$N$ satisfies a certain variant of Faithfulness, known as Pearl&apos;s Minimality
condition---as if the learning algorithm were designed by assuming that
condition. This explains, for the first time, why it is not merely optional but
mandatory to assume the Minimality condition---or to proceed as if we assumed
it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hanti Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiji Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07079">
<title>Structured Uncertainty Prediction Networks. (arXiv:1802.07079v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07079</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper is the first work to propose a network to predict a structured
uncertainty distribution for a reconstructed image. Our novel model learns to
predict a full Gaussian covariance matrix for each reconstruction, which
permits efficient sampling and likelihood evaluation. We demonstrate that our
model can accurately reconstruct ground truth correlated residual distributions
for synthetic datasets and generate plausible high frequency samples for real
face images. We also illustrate the use of these predicted covariances for
structure preserving image denoising.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dorta_G/0/1/0/all/0/1&quot;&gt;Garoe Dorta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vicente_S/0/1/0/all/0/1&quot;&gt;Sara Vicente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Agapito_L/0/1/0/all/0/1&quot;&gt;Lourdes Agapito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Campbell_N/0/1/0/all/0/1&quot;&gt;Neill D.F. Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Simpson_I/0/1/0/all/0/1&quot;&gt;Ivor Simpson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07107">
<title>Learning of Optimal Forecast Aggregation in Partial Evidence Environments. (arXiv:1802.07107v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07107</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the forecast aggregation problem in repeated settings, where the
forecasts are done on a binary event. At each period multiple experts provide
forecasts about an event. The goal of the aggregator is to aggregate those
forecasts into a subjective accurate forecast. We assume that experts are
Bayesian; namely they share a common prior, each expert is exposed to some
evidence, and each expert applies Bayes rule to deduce his forecast. The
aggregator is ignorant with respect to the information structure (i.e.,
distribution over evidence) according to which experts make their prediction.
The aggregator observes the experts&apos; forecasts only. At the end of each period
the actual state is realized. We focus on the question whether the aggregator
can learn to aggregate optimally the forecasts of the experts, where the
optimal aggregation is the Bayesian aggregation that takes into account all the
information (evidence) in the system.
&lt;/p&gt;
&lt;p&gt;We consider the class of partial evidence information structures, where each
expert is exposed to a different subset of conditionally independent signals.
Our main results are positive; We show that optimal aggregation can be learned
in polynomial time in a quite wide range of instances of the partial evidence
environments. We provide a tight characterization of the instances where
learning is possible and impossible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babichenko_Y/0/1/0/all/0/1&quot;&gt;Yakov Babichenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garber_D/0/1/0/all/0/1&quot;&gt;Dan Garber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07126">
<title>On Estimating Multi-Attribute Choice Preferences using Private Signals and Matrix Factorization. (arXiv:1802.07126v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07126</link>
<description rdf:parseType="Literal">&lt;p&gt;Revealed preference theory studies the possibility of modeling an agent&apos;s
revealed preferences and the construction of a consistent utility function.
However, modeling agent&apos;s choices over preference orderings is not always
practical and demands strong assumptions on human rationality and
data-acquisition abilities. Therefore, we propose a simple generative choice
model where agents are assumed to generate the choice probabilities based on
latent factor matrices that capture their choice evaluation across multiple
attributes. Since the multi-attribute evaluation is typically hidden within the
agent&apos;s psyche, we consider a signaling mechanism where agents are provided
with choice information through private signals, so that the agent&apos;s choices
provide more insight about his/her latent evaluation across multiple
attributes. We estimate the choice model via a novel multi-stage matrix
factorization algorithm that minimizes the average deviation of the factor
estimates from choice data. Simulation results are presented to validate the
estimation performance of our proposed algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nadendla_V/0/1/0/all/0/1&quot;&gt;Venkata Sriram Siddhardh Nadendla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Langbort_C/0/1/0/all/0/1&quot;&gt;Cedric Langbort&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07127">
<title>Actions Speak Louder Than Goals: Valuing Player Actions in Soccer. (arXiv:1802.07127v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1802.07127</link>
<description rdf:parseType="Literal">&lt;p&gt;Assessing the impact of the individual actions performed by soccer players
during games is a crucial aspect of the player recruitment process.
Unfortunately, most traditional metrics fall short in addressing this task as
they either focus on rare events like shots and goals alone or fail to account
for the context in which the actions occurred. This paper introduces a novel
advanced soccer metric for valuing any type of individual player action on the
pitch, be it with or without the ball. Our metric values each player action
based on its impact on the game outcome while accounting for the circumstances
under which the action happened. When applied to on-the-ball actions like
passes, dribbles, and shots alone, our metric identifies Argentine forward
Lionel Messi, French teenage star Kylian Mbapp\&apos;e, and Belgian winger Eden
Hazard as the most effective players during the 2016/2017 season.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Decroos_T/0/1/0/all/0/1&quot;&gt;Tom Decroos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bransen_L/0/1/0/all/0/1&quot;&gt;Lotte Bransen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haaren_J/0/1/0/all/0/1&quot;&gt;Jan Van Haaren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Davis_J/0/1/0/all/0/1&quot;&gt;Jesse Davis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07129">
<title>Deep BCD-Net Using Identical Encoding-Decoding CNN Structures for Iterative Image Recovery. (arXiv:1802.07129v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07129</link>
<description rdf:parseType="Literal">&lt;p&gt;In &quot;extreme&quot; computational imaging that collects extremely undersampled or
noisy measurements, obtaining an accurate image within a reasonable computing
time is challenging. Incorporating image mapping convolutional neural networks
(CNN) to iterative image recovery has great potential to resolve this issue.
This paper 1) incorporates image mapping CNN using identical convolutional
kernels in both encoders and decoders into block coordinate descent (BCD)
optimization method -- referred to BCD-Net using identical encoding-decoding
CNN structures -- and 2) applies alternating direction method of multipliers to
train the proposed BCD-Net. Numerical experiments show that, for a) denoising
moderately low signal-to-noise-ratio images and b) extremely undersampled
magnetic resonance imaging, the proposed BCD-Net achieves (significantly) more
accurate image recovery, compared to BCD-Net using distinct encoding-decoding
structures and/or the conventional image recovery model using both wavelets and
total variation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chun_I/0/1/0/all/0/1&quot;&gt;Il Yong Chun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fessler_J/0/1/0/all/0/1&quot;&gt;Jeffrey A. Fessler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07167">
<title>High-Quality Prediction Intervals for Deep Learning: A Distribution-Free, Ensembled Approach. (arXiv:1802.07167v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07167</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are a powerful technique for learning complex functions
from data. However, their appeal in real-world applications can be hindered by
an inability to quantify the uncertainty of predictions. In this paper, the
generation of prediction intervals (PI) for quantifying uncertainty in
regression tasks is considered.
&lt;/p&gt;
&lt;p&gt;It is axiomatic that high-quality PIs should be as narrow as possible, whilst
capturing a specified portion of data. In this paper we derive a loss function
directly from this high-quality principle that requires no distributional
assumption. We show how its form derives from a likelihood principle, that it
can be used with gradient descent, and that in ensembled form, model
uncertainty is accounted for. This remedies limitations of a popular model
developed on the same high-quality principle.
&lt;/p&gt;
&lt;p&gt;Experiments are conducted on ten regression benchmark datasets. The proposed
quality-driven (QD) method is shown to outperform current state-of-the-art
uncertainty quantification methods, reducing average PI width by around 10%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pearce_T/0/1/0/all/0/1&quot;&gt;Tim Pearce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zaki_M/0/1/0/all/0/1&quot;&gt;Mohamed Zaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brintrup_A/0/1/0/all/0/1&quot;&gt;Alexandra Brintrup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neely_A/0/1/0/all/0/1&quot;&gt;Andy Neely&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07182">
<title>The Gaussian Process Autoregressive Regression Model (GPAR). (arXiv:1802.07182v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07182</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-output regression models must exploit dependencies between outputs to
maximise predictive performance. The application of Gaussian processes (GPs) to
this setting typically yields models that are computationally demanding and
have limited representational power. We present the Gaussian Process
Autoregressive Regression (GPAR) model, a scalable multi-output GP model that
is able to capture nonlinear, possibly input-varying, dependencies between
outputs in a simple and tractable way: the product rule is used to decompose
the joint distribution over the outputs into a set of conditionals, each of
which is modelled by a standard GP. GPAR&apos;s efficacy is demonstrated on a
variety of synthetic and real-world problems, outperforming existing GP models
and achieving state-of-the-art performance on the tasks with existing
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Requeima_J/0/1/0/all/0/1&quot;&gt;James Requeima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tebbutt_W/0/1/0/all/0/1&quot;&gt;Will Tebbutt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bruinsma_W/0/1/0/all/0/1&quot;&gt;Wessel Bruinsma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard E. Turner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07191">
<title>Neural Architecture Search with Bayesian Optimisation and Optimal Transport. (arXiv:1802.07191v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07191</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian Optimisation (BO) refers to a class of methods for global
optimisation of a function $f$ which is only accessible via point evaluations.
It is typically used in settings where $f$ is expensive to evaluate. A common
use case for BO in machine learning is model selection, where it is not
possible to analytically model the generalisation performance of a statistical
model, and we resort to noisy and expensive training and validation procedures
to choose the best model. Conventional BO methods have focused on Euclidean and
categorical domains, which, in the context of model selection, only permits
tuning scalar hyper-parameters of machine learning algorithms. However, with
the surge of interest in deep learning, there is an increasing demand to tune
neural network \emph{architectures}. In this work, we develop NASBOT, a
Gaussian process based BO framework for neural architecture search. To
accomplish this, we develop a distance metric in the space of neural network
architectures which can be computed efficiently via an optimal transport
program. This distance might be of independent interest to the deep learning
community as it may find applications outside of BO. We demonstrate that NASBOT
outperforms other alternatives for architecture search in several cross
validation based model selection tasks on multi-layer perceptrons and
convolutional neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kandasamy_K/0/1/0/all/0/1&quot;&gt;Kirthevasan Kandasamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1&quot;&gt;Willie Neiswanger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnabas Poczos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07207">
<title>AutoPrognosis: Automated Clinical Prognostic Modeling via Bayesian Optimization with Structured Kernel Learning. (arXiv:1802.07207v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07207</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical prognostic models derived from largescale healthcare data can inform
critical diagnostic and therapeutic decisions. To enable off-theshelf usage of
machine learning (ML) in prognostic research, we developed AUTOPROGNOSIS: a
system for automating the design of predictive modeling pipelines tailored for
clinical prognosis. AUTOPROGNOSIS optimizes ensembles of pipeline
configurations efficiently using a novel batched Bayesian optimization (BO)
algorithm that learns a low-dimensional decomposition of the pipelines
high-dimensional hyperparameter space in concurrence with the BO procedure.
This is achieved by modeling the pipelines performances as a black-box function
with a Gaussian process prior, and modeling the similarities between the
pipelines baseline algorithms via a sparse additive kernel with a Dirichlet
prior. Meta-learning is used to warmstart BO with external data from similar
patient cohorts by calibrating the priors using an algorithm that mimics the
empirical Bayes method. The system automatically explains its predictions by
presenting the clinicians with logical association rules that link patients
features to predicted risk strata. We demonstrate the utility of AUTOPROGNOSIS
using 10 major patient cohorts representing various aspects of cardiovascular
patient care.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alaa_A/0/1/0/all/0/1&quot;&gt;Ahmed M. Alaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07244">
<title>Steering Social Activity: A Stochastic Optimal Control Point Of View. (arXiv:1802.07244v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1802.07244</link>
<description rdf:parseType="Literal">&lt;p&gt;User engagement in online social networking depends critically on the level
of social activity in the corresponding platform--the number of online actions,
such as posts, shares or replies, taken by their users. Can we design
data-driven algorithms to increase social activity? At a user level, such
algorithms may increase activity by helping users decide when to take an action
to be more likely to be noticed by their peers. At a network level, they may
increase activity by incentivizing a few influential users to take more
actions, which in turn will trigger additional actions by other users. In this
paper, we model social activity using the framework of marked temporal point
processes, derive an alternate representation of these processes using
stochastic differential equations (SDEs) with jumps and, exploiting this
alternate representation, develop two efficient online algorithms with provable
guarantees to steer social activity both at a user and at a network level. In
doing so, we establish a previously unexplored connection between optimal
control of jump SDEs and doubly stochastic marked temporal point processes,
which is of independent interest. Finally, we experiment both with synthetic
and real data gathered from Twitter and show that our algorithms consistently
steer social activity more effectively than the state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zarezade_A/0/1/0/all/0/1&quot;&gt;Ali Zarezade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1&quot;&gt;Abir De&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1&quot;&gt;Utkarsh Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabiee_H/0/1/0/all/0/1&quot;&gt;Hamid R. Rabiee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_M/0/1/0/all/0/1&quot;&gt;Manuel Gomez-Rodriguez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1511.06382">
<title>Iterative Refinement of the Approximate Posterior for Directed Belief Networks. (arXiv:1511.06382v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1511.06382</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational methods that rely on a recognition network to approximate the
posterior of directed graphical models offer better inference and learning than
previous methods. Recent advances that exploit the capacity and flexibility in
this approach have expanded what kinds of models can be trained. However, as a
proposal for the posterior, the capacity of the recognition network is limited,
which can constrain the representational power of the generative model and
increase the variance of Monte Carlo estimates. To address these issues, we
introduce an iterative refinement procedure for improving the approximate
posterior of the recognition network and show that training with the refined
posterior is competitive with state-of-the-art methods. The advantages of
refinement are further evident in an increased effective sample size, which
implies a lower variance of gradient estimates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1&quot;&gt;R Devon Hjelm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;Junyoung Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Russ Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1&quot;&gt;Vince Calhoun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1&quot;&gt;Nebojsa Jojic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.06525">
<title>A Unified Framework for Low-Rank plus Sparse Matrix Recovery. (arXiv:1702.06525v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1702.06525</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a unified framework to solve general low-rank plus sparse matrix
recovery problems based on matrix factorization, which covers a broad family of
objective functions satisfying the restricted strong convexity and smoothness
conditions. Based on projected gradient descent and the double thresholding
operator, our proposed generic algorithm is guaranteed to converge to the
unknown low-rank and sparse matrices at a locally linear rate, while matching
the best-known robustness guarantee (i.e., tolerance for sparsity). At the core
of our theory is a novel structural Lipschitz gradient condition for low-rank
plus sparse matrices, which is essential for proving the linear convergence
rate of our algorithm, and we believe is of independent interest to prove fast
rates for general superposition-structured models. We illustrate the
application of our framework through two concrete examples: robust matrix
sensing and robust PCA. Experiments on both synthetic and real datasets
corroborate our theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lingxiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gu_Q/0/1/0/all/0/1&quot;&gt;Quanquan Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.01541">
<title>Soft-DTW: a Differentiable Loss Function for Time-Series. (arXiv:1703.01541v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.01541</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose in this paper a differentiable learning loss between time series,
building upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the
Euclidean distance, DTW can compare time series of variable size and is robust
to shifts or dilatations across the time dimension. To compute DTW, one
typically solves a minimal-cost alignment problem between two time series using
dynamic programming. Our work takes advantage of a smoothed formulation of DTW,
called soft-DTW, that computes the soft-minimum of all alignment costs. We show
in this paper that soft-DTW is a differentiable loss function, and that both
its value and gradient can be computed with quadratic time/space complexity
(DTW has quadratic time but linear space complexity). We show that this
regularization is particularly well suited to average and cluster time series
under the DTW geometry, a task for which our proposal significantly outperforms
existing baselines. Next, we propose to tune the parameters of a machine that
outputs time series by minimizing its fit with ground-truth labels in a
soft-DTW sense.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cuturi_M/0/1/0/all/0/1&quot;&gt;Marco Cuturi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blondel_M/0/1/0/all/0/1&quot;&gt;Mathieu Blondel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.01799">
<title>Efficient Contextual Bandits in Non-stationary Worlds. (arXiv:1708.01799v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.01799</link>
<description rdf:parseType="Literal">&lt;p&gt;Most contextual bandit algorithms minimize regret against the best fixed
policy, a questionable benchmark for non-stationary environments that are
ubiquitous in applications. In this work, we develop several efficient
contextual bandit algorithms for non-stationary environments by equipping
existing methods for i.i.d. problems with sophisticated statistical tests so as
to dynamically adapt to a change in distribution.
&lt;/p&gt;
&lt;p&gt;We analyze various standard notions of regret suited to non-stationary
environments for these algorithms, including interval regret, switching regret,
and dynamic regret. When competing with the best policy at each time, one of
our algorithms achieves regret $\mathcal{O}(\sqrt{ST})$ if there are $T$ rounds
with $S$ stationary periods, or more generally
$\mathcal{O}(\Delta^{1/3}T^{2/3})$ where $\Delta$ is some non-stationarity
measure. These results almost match the optimal guarantees achieved by an
inefficient baseline that is a variant of the classic Exp4 algorithm. The
dynamic regret result is also the first one for efficient and fully adversarial
contextual bandit.
&lt;/p&gt;
&lt;p&gt;Furthermore, while the results above require tuning a parameter based on the
unknown quantity $S$ or $\Delta$, we also develop a parameter free algorithm
achieving regret $\min\{S^{1/4}T^{3/4}, \Delta^{1/5}T^{4/5}\}$. This improves
and generalizes the best existing result $\Delta^{0.18}T^{0.82}$ by Karnin and
Anava (2016) which only holds for the two-armed bandit problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Haipeng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Chen-Yu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Alekh Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1&quot;&gt;John Langford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06276">
<title>Smooth and Sparse Optimal Transport. (arXiv:1710.06276v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06276</link>
<description rdf:parseType="Literal">&lt;p&gt;Entropic regularization is quickly emerging as a new standard in optimal
transport (OT). It enables to cast the OT computation as a differentiable and
unconstrained convex optimization problem, which can be efficiently solved
using the Sinkhorn algorithm. However, entropy keeps the transportation plan
strictly positive and therefore completely dense, unlike unregularized OT. This
lack of sparsity can be problematic in applications where the transportation
plan itself is of interest. In this paper, we explore regularizing the primal
and dual OT formulations with a strongly convex term, which corresponds to
relaxing the dual and primal constraints with smooth approximations. We show
how to incorporate squared $2$-norm and group lasso regularizations within that
framework, leading to sparse and group-sparse transportation plans. On the
theoretical side, we bound the approximation error introduced by regularizing
the primal and dual formulations. Our results suggest that, for the regularized
primal, the approximation error can often be smaller with squared $2$-norm than
with entropic regularization. We showcase our proposed framework on the task of
color transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blondel_M/0/1/0/all/0/1&quot;&gt;Mathieu Blondel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Seguy_V/0/1/0/all/0/1&quot;&gt;Vivien Seguy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rolet_A/0/1/0/all/0/1&quot;&gt;Antoine Rolet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.09953">
<title>The Error Probability of Random Fourier Features is Dimensionality Independent. (arXiv:1710.09953v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.09953</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that the error probability of reconstructing kernel matrices from
Random Fourier Features for the Gaussian kernel function is at most
$\mathcal{O}(R^{2/3} \exp(-D))$, where $D$ is the number of random features and
$R$ is the diameter of the data domain. We also provide an
information-theoretic method-independent lower bound of $\Omega(\exp(-D))$ for
$R&amp;gt;2.1$. Compared to prior work, we are the first to show that the error
probability for random Fourier features is independent of the dimensionality of
data points. As applications of our theory, we obtain dimension-independent
bounds for kernel ridge regression and support vector machines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honorio_J/0/1/0/all/0/1&quot;&gt;Jean Honorio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu-Jun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01655">
<title>Approximating Partition Functions in Constant Time. (arXiv:1711.01655v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01655</link>
<description rdf:parseType="Literal">&lt;p&gt;We study approximations of the partition function of dense graphical models.
Partition functions of graphical models play a fundamental role is statistical
physics, in statistics and in machine learning. Two of the main methods for
approximating the partition function are Markov Chain Monte Carlo and
Variational Methods. An impressive body of work in mathematics, physics and
theoretical computer science provides conditions under which Markov Chain Monte
Carlo methods converge in polynomial time. These methods often lead to
polynomial time approximation algorithms for the partition function in cases
where the underlying model exhibits correlation decay. There are very few
theoretical guarantees for the performance of variational methods. One
exception is recent results by Risteski (2016) who considered dense graphical
models and showed that using variational methods, it is possible to find an
$O(\epsilon n)$ additive approximation to the log partition function in time
$n^{O(1/\epsilon^2)}$ even in a regime where correlation decay does not hold.
&lt;/p&gt;
&lt;p&gt;We show that under essentially the same conditions, an $O(\epsilon n)$
additive approximation of the log partition function can be found in constant
time, independent of $n$. In particular, our results cover dense Ising and
Potts models as well as dense graphical models with $k$-wise interaction. They
also apply for low threshold rank models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vishesh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1&quot;&gt;Frederic Koehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mossel_E/0/1/0/all/0/1&quot;&gt;Elchanan Mossel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03194">
<title>Long-Term Sequential Prediction Using Expert Advice. (arXiv:1711.03194v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03194</link>
<description rdf:parseType="Literal">&lt;p&gt;For the prediction with experts&apos; advice setting, we consider some methods to
construct forecasting algorithms that suffer loss not much more than any expert
in the pool. In contrast to the standard approach, we investigate the case of
long-term forecasting of time series. This approach implies that each expert
issues a forecast for a time point ahead (or a time interval), and then the
master algorithm combines these forecasts into one aggregated forecast
(sequence of forecasts). We introduce two new approaches to aggregating
experts&apos; long-term interval predictions. Both are based on Vovk&apos;s aggregating
algorithm. The first approach applies the method of Mixing Past Posteriors
method to the long-term prediction. The second approach is used for the
interval forecasting and considers overlapping experts. The upper bounds for
regret of these algorithms for adversarial case are obtained. We also present
the results of numerical experiments on time series long-term prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1&quot;&gt;Alexander Korotin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyugin_V/0/1/0/all/0/1&quot;&gt;Vladimir V&amp;#x27;yugin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1&quot;&gt;Evgeny Burnaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04043">
<title>Few-Shot Learning with Graph Neural Networks. (arXiv:1711.04043v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04043</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to study the problem of few-shot learning with the prism of
inference on a partially observed graphical model, constructed from a
collection of input images whose label can be either observed or not. By
assimilating generic message-passing inference algorithms with their
neural-network counterparts, we define a graph neural network architecture that
generalizes several of the recently proposed few-shot learning models. Besides
providing improved numerical performance, our framework is easily extended to
variants of few-shot learning, such as semi-supervised or active learning,
demonstrating the ability of graph-based models to operate well on &apos;relational&apos;
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garcia_V/0/1/0/all/0/1&quot;&gt;Victor Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bruna_J/0/1/0/all/0/1&quot;&gt;Joan Bruna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06664">
<title>Predict Responsibly: Increasing Fairness by Learning To Defer. (arXiv:1711.06664v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06664</link>
<description rdf:parseType="Literal">&lt;p&gt;In many high-stakes ML applications, there are multiple decision-makers
involved, both automated and human. The interaction between these agents often
goes unaddressed in algorithmic development. In this work, we explore a simple
version of this interaction with a two-stage framework containing an automated
model and an external decision-maker. The model can choose to say IDK, and pass
the decision downstream, as explored in rejection learning. We extend this
concept by proposing learning to defer, which generalizes the rejection
learning framework by considering the effect of the other agents in the
decision-making process. We propose a learning algorithm which accounts for
potential biases held by external decision-makers in a system. Experiments on
real-world datasets demonstrate that learning to defer can make a system not
only more accurate but also less biased. Even when operated by highly biased
users, we show that deferring models can still greatly improve the fairness of
the entire system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Madras_D/0/1/0/all/0/1&quot;&gt;David Madras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pitassi_T/0/1/0/all/0/1&quot;&gt;Toniann Pitassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09649">
<title>One-Shot Coresets: The Case of k-Clustering. (arXiv:1711.09649v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09649</link>
<description rdf:parseType="Literal">&lt;p&gt;Scaling clustering algorithms to massive data sets is a challenging task.
Recently, several successful approaches based on data summarization methods,
such as coresets and sketches, were proposed. While these techniques provide
provably good and small summaries, they are inherently problem dependent - the
practitioner has to commit to a fixed clustering objective before even
exploring the data. However, can one construct small data summaries for a wide
range of clustering problems simultaneously? In this work, we affirmatively
answer this question by proposing an efficient algorithm that constructs such
one-shot summaries for k-clustering problems while retaining strong theoretical
guarantees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bachem_O/0/1/0/all/0/1&quot;&gt;Olivier Bachem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lucic_M/0/1/0/all/0/1&quot;&gt;Mario Lucic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lattanzi_S/0/1/0/all/0/1&quot;&gt;Silvio Lattanzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01496">
<title>Learning Pain from Action Unit Combinations: A Weakly Supervised Approach via Multiple Instance Learning. (arXiv:1712.01496v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01496</link>
<description rdf:parseType="Literal">&lt;p&gt;Patient pain can be detected highly reliably from facial expressions using a
set of facial muscle-based action units (AUs) defined by the Facial Action
Coding System (FACS). A key characteristic of facial expression of pain is the
simultaneous occurrence of pain-related AU combinations, whose automated
detection would be highly beneficial for efficient and practical pain
monitoring. Existing general Automated Facial Expression Recognition (AFER)
systems prove inadequate when applied specifically for detecting pain as they
either focus on detecting individual pain-related AUs but not on combinations
or they seek to bypass AU detection by training a binary pain classifier
directly on pain intensity data but are limited by lack of enough labeled data
for satisfactory training. In this paper, we propose a new approach that mimics
the strategy of human coders of decoupling pain detection into two consecutive
tasks: one performed at the individual video-frame level and the other at
video-sequence level. Using state-of-the-art AFER tools to detect single AUs at
the frame level, we propose two novel data structures to encode AU combinations
from single AU scores. Two weakly supervised learning frameworks namely
multiple instance learning (MIL) and multiple clustered instance learning
(MCIL) are employed corresponding to each data structure to learn pain from
video sequences. Experimental results show an 87% pain recognition accuracy
with 0.94 AUC (Area Under Curve) on the UNBC-McMaster Shoulder Pain Expression
dataset. Tests on long videos in a lung cancer patient video dataset
demonstrates the potential value of the proposed system for pain monitoring in
clinical settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhanli Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ansari_R/0/1/0/all/0/1&quot;&gt;Rashid Ansari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilkie_D/0/1/0/all/0/1&quot;&gt;Diana J. Wilkie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03265">
<title>More Adaptive Algorithms for Adversarial Bandits. (arXiv:1801.03265v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03265</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a novel and generic algorithm for the adversarial multi-armed
bandit problem (or more generally the combinatorial semi-bandit problem). When
instantiated differently, our algorithm achieves various new data-dependent
regret bounds improving previous work. Examples include: 1) a regret bound
depending on the variance of only the best arm; 2) a regret bound depending on
the first-order path-length of only the best arm; 3) a regret bound depending
on the sum of first-order path-lengths of all arms as well as an important
negative term, which together lead to faster convergence rates for some normal
form games with partial feedback; 4) a regret bound that simultaneously implies
small regret when the best arm has small loss and logarithmic regret when there
exists an arm whose expected loss is always smaller than those of others by a
fixed gap (e.g. the classic i.i.d. setting). In some cases, such as the last
two results, our algorithm is completely parameter-free.
&lt;/p&gt;
&lt;p&gt;The main idea of our algorithm is to apply the optimism and adaptivity
techniques to the well-known Online Mirror Descent framework with a special
log-barrier regularizer. The challenges are to come up with appropriate
optimistic predictions and correction terms in this framework. Some of our
results also crucially rely on using a sophisticated increasing learning rate
schedule.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Chen-Yu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Haipeng Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07222">
<title>Rover Descent: Learning to optimize by learning to navigate on prototypical loss surfaces. (arXiv:1801.07222v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07222</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to optimize - the idea that we can learn from data algorithms that
optimize a numerical criterion - has recently been at the heart of a growing
number of research efforts. One of the most challenging issues within this
approach is to learn a policy that is able to optimize over classes of
functions that are fairly different from the ones that it was trained on. We
propose a novel way of framing learning to optimize as a problem of learning a
good navigation policy on a partially observable loss surface. To this end, we
develop Rover Descent, a solution that allows us to learn a fairly broad
optimization policy from training on a small set of prototypical
two-dimensional surfaces that encompasses the classically hard cases such as
valleys, plateaus, cliffs and saddles and by using strictly zero-order
information. We show that, without having access to gradient or curvature
information, we achieve state-of-the-art convergence speed on optimization
problems not presented at training time such as the Rosenbrock function and
other hard cases in two dimensions. We extend our framework to optimize over
high dimensional landscapes, while still handling only two-dimensional local
landscape information and show good preliminary results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faury_L/0/1/0/all/0/1&quot;&gt;Louis Faury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasile_F/0/1/0/all/0/1&quot;&gt;Flavian Vasile&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03676">
<title>Differentiable Dynamic Programming for Structured Prediction and Attention. (arXiv:1802.03676v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03676</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic programming (DP) solves a variety of structured combinatorial
problems by iteratively breaking them down into smaller subproblems. In spite
of their versatility, DP algorithms are usually non-differentiable, which
hampers their use as a layer in neural networks trained by backpropagation. To
address this issue, we propose to smooth the max operator in the dynamic
programming recursion, using a strongly convex regularizer. This allows to
relax both the optimal value and solution of the original combinatorial
problem, and turns a broad class of DP algorithms into differentiable
operators. Theoretically, we provide a new probabilistic perspective on
backpropagating through these DP operators, and relate them to inference in
graphical models. We derive two particular instantiations of our framework, a
smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm
for time-series alignment. We showcase these instantiations on two structured
prediction tasks and on structured and sparse attention for neural machine
translation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mensch_A/0/1/0/all/0/1&quot;&gt;Arthur Mensch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blondel_M/0/1/0/all/0/1&quot;&gt;Mathieu Blondel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05688">
<title>Simulation assisted machine learning. (arXiv:1802.05688v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05688</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting how a proposed cancer treatment will affect a given tumor can be
cast as a machine learning problem, but the complexity of biological systems,
the number of potentially relevant genomic and clinical features, and the lack
of very large scale patient data repositories make this a unique challenge.
&quot;Pure data&quot; approaches to this problem are underpowered to detect
combinatorially complex interactions and are bound to uncover false
correlations despite statistical precautions taken (1). To investigate this
setting, we propose a method to integrate simulations, a strong form of prior
knowledge, into machine learning, a combination which to date has been largely
unexplored. The results of multiple simulations (under various uncertainty
scenarios) are used to compute similarity measures between every pair of
samples: sample pairs are given a high similarity score if they behave
similarly under a wide range of simulation parameters. These similarity values,
rather than the original high dimensional feature data, are used to train
kernelized machine learning algorithms such as support vector machines, thus
handling the curse-of-dimensionality that typically affects genomic machine
learning. Using four synthetic datasets of complex systems--three biological
models and one network flow optimization model--we demonstrate that when the
number of training samples is small compared to the number of features, the
simulation kernel approach dominates over no-prior-knowledge methods. In
addition to biology and medicine, this approach should be applicable to other
disciplines, such as weather forecasting, financial markets, and agricultural
management, where predictive models are sought and informative yet approximate
simulations are available. The Python SimKern software, the models (in MATLAB,
Octave, and R), and the datasets are made freely available at
https://github.com/davidcraft/SimKern .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deist_T/0/1/0/all/0/1&quot;&gt;Timo Deist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Patti_A/0/1/0/all/0/1&quot;&gt;Andrew Patti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krane_D/0/1/0/all/0/1&quot;&gt;David Krane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sorenson_T/0/1/0/all/0/1&quot;&gt;Taylor Sorenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Craft_D/0/1/0/all/0/1&quot;&gt;David Craft&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06458">
<title>A Generative Modeling Approach to Limited Channel ECG Classification. (arXiv:1802.06458v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06458</link>
<description rdf:parseType="Literal">&lt;p&gt;Processing temporal sequences is central to a variety of applications in
health care, and in particular multi-channel Electrocardiogram (ECG) is a
highly prevalent diagnostic modality that relies on robust sequence modeling.
While Recurrent Neural Networks (RNNs) have led to significant advances in
automated diagnosis with time-series data, they perform poorly when models are
trained using a limited set of channels. A crucial limitation of existing
solutions is that they rely solely on discriminative models, which tend to
generalize poorly in such scenarios. In order to combat this limitation, we
develop a generative modeling approach to limited channel ECG classification.
This approach first uses a Seq2Seq model to implicitly generate the missing
channel information, and then uses the latent representation to perform the
actual supervisory task. This decoupling enables the use of unsupervised data
and also provides highly robust metric spaces for subsequent discriminative
learning. Our experiments with the Physionet dataset clearly evidence the
effectiveness of our approach over standard RNNs in disease prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rajan_D/0/1/0/all/0/1&quot;&gt;Deepta Rajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thiagarajan_J/0/1/0/all/0/1&quot;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;</dc:creator>
</item></rdf:RDF>