<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03304"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.06699"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04214"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08521"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03370"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03481"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03487"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03491"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.04664"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.00109"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03241"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03319"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03344"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03348"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03383"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03432"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03466"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03544"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03571"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03607"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03623"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.08024"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.00577"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.02245"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.04881"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09070"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.03304">
<title>A model of reward-modulated motor learning with parallelcortical and basal ganglia pathways. (arXiv:1803.03304v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1803.03304</link>
<description rdf:parseType="Literal">&lt;p&gt;Many recent studies of the motor system are divided into two distinct
approaches: Those that investigate how motor responses are encoded in cortical
neurons&apos; firing rate dynamics and those that study the learning rules by which
mammals and songbirds develop reliable motor responses. Computationally, the
first approach is encapsulated by reservoir computing models, which can learn
intricate motor tasks and produce internal dynamics strikingly similar to those
of motor cortical neurons, but rely on biologically unrealistic learning rules.
The more realistic learning rules developed by the second approach are often
derived for simplified, discrete tasks in contrast to the intricate dynamics
that characterize real motor responses. We bridge these two approaches to
develop a biologically realistic learning rule for reservoir computing. Our
algorithm learns simulated motor tasks on which previous reservoir computing
algorithms fail, and reproduces experimental findings including those that
relate motor learning to Parkinson&apos;s disease and its treatment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pyle_R/0/1/0/all/0/1&quot;&gt;Ryan Pyle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rosenbaum_R/0/1/0/all/0/1&quot;&gt;Robert Rosenbaum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.06699">
<title>Representation Learning using Event-based STDP. (arXiv:1706.06699v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1706.06699</link>
<description rdf:parseType="Literal">&lt;p&gt;Although representation learning methods developed within the framework of
traditional neural networks are relatively mature, developing a spiking
representation model remains a challenging problem. This paper proposes an
event-based method to train a feedforward spiking neural network (SNN) layer
for extracting visual features. The method introduces a novel
spike-timing-dependent plasticity (STDP) learning rule and a threshold
adjustment rule both derived from a vector quantization-like objective function
subject to a sparsity constraint. The STDP rule is obtained by the gradient of
a vector quantization criterion that is converted to spike-based,
spatio-temporally local update rules in a spiking network of leaky,
integrate-and-fire (LIF) neurons. Independence and sparsity of the model are
achieved by the threshold adjustment rule and by a softmax function
implementing inhibition in the representation layer consisting of
WTA-thresholded spiking neurons. Together, these mechanisms implement a form of
spike-based, competitive learning. Two sets of experiments are performed on the
MNIST and natural image datasets. The results demonstrate a sparse spiking
visual representation model with low reconstruction loss comparable with
state-of-the-art visual coding approaches, yet our rule is local in both time
and space, thus biologically plausible and hardware friendly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavanaei_A/0/1/0/all/0/1&quot;&gt;Amirhossein Tavanaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1&quot;&gt;Timothee Masquelier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maida_A/0/1/0/all/0/1&quot;&gt;Anthony Maida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04214">
<title>BP-STDP: Approximating Backpropagation using Spike Timing Dependent Plasticity. (arXiv:1711.04214v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04214</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of training spiking neural networks (SNNs) is a necessary
precondition to understanding computations within the brain, a field still in
its infancy. Previous work has shown that supervised learning in multi-layer
SNNs enables bio-inspired networks to recognize patterns of stimuli through
hierarchical feature acquisition. Although gradient descent has shown
impressive performance in multi-layer (and deep) SNNs, it is generally not
considered biologically plausible and is also computationally expensive. This
paper proposes a novel supervised learning approach based on an event-based
spike-timing-dependent plasticity (STDP) rule embedded in a network of
integrate-and-fire (IF) neurons. The proposed temporally local learning rule
follows the backpropagation weight change updates applied at each time step.
This approach enjoys benefits of both accurate gradient descent and temporally
local, efficient STDP. Thus, this method is able to address some open questions
regarding accurate and efficient computations that occur in the brain. The
experimental results on the XOR problem, the Iris data, and the MNIST dataset
demonstrate that the proposed SNN performs as successfully as the traditional
NNs. Our approach also compares favorably with the state-of-the-art multi-layer
SNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavanaei_A/0/1/0/all/0/1&quot;&gt;Amirhossein Tavanaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maida_A/0/1/0/all/0/1&quot;&gt;Anthony S. Maida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08521">
<title>An Incremental Self-Organizing Architecture for Sensorimotor Learning and Prediction. (arXiv:1712.08521v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08521</link>
<description rdf:parseType="Literal">&lt;p&gt;During visuomotor tasks, robots must compensate for temporal delays inherent
in their sensorimotor processing systems. Delay compensation becomes crucial in
a dynamic environment where the visual input is constantly changing, e.g.,
during the interacting with a human demonstrator. For this purpose, the robot
must be equipped with a prediction mechanism for using the acquired perceptual
experience to estimate possible future motor commands. In this paper, we
present a novel neural network architecture that learns prototypical visuomotor
representations and provides reliable predictions on the basis of the visual
input. These predictions are used to compensate for the delayed motor behavior
in an online manner. We investigate the performance of our method with a set of
experiments comprising a humanoid robot that has to learn and generate visually
perceived arm motion trajectories. We evaluate the accuracy in terms of mean
prediction error and analyze the response of the network to novel movement
demonstrations. Additionally, we report experiments with incomplete data
sequences, showing the robustness of the proposed architecture in the case of a
noisy and faulty visual sensor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mici_L/0/1/0/all/0/1&quot;&gt;Luiza Mici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parisi_G/0/1/0/all/0/1&quot;&gt;German I. Parisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03370">
<title>Expert Finding in Heterogeneous Bibliographic Networks with Locally-trained Embeddings. (arXiv:1803.03370v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1803.03370</link>
<description rdf:parseType="Literal">&lt;p&gt;Expert finding is an important task in both industry and academia. It is
challenging to rank candidates with appropriate expertise for various queries.
In addition, different types of objects interact with one another, which
naturally forms heterogeneous information networks. We study the task of expert
finding in heterogeneous bibliographical networks based on two aspects: textual
content analysis and authority ranking. Regarding the textual content analysis,
we propose a new method for query expansion via locally-trained embedding
learning with concept hierarchy as guidance, which is particularly tailored for
specific queries with narrow semantic meanings. Compared with global embedding
learning, locally-trained embedding learning projects the terms into a latent
semantic space constrained on relevant topics, therefore it preserves more
precise and subtle information for specific queries. Considering the candidate
ranking, the heterogeneous information network structure, while being largely
ignored in the previous studies of expert finding, provides additional
information. Specifically, different types of interactions among objects play
different roles. We propose a ranking algorithm to estimate the authority of
objects in the network, treating each strongly-typed edge type individually. To
demonstrate the effectiveness of the proposed framework, we apply the proposed
method to a large-scale bibliographical dataset with over two million entries
and one million researcher candidates. The experiment results show that the
proposed framework outperforms existing methods for both general and specific
queries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1&quot;&gt;Huan Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aston Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03481">
<title>SpCoSLAM 2.0: An Improved and Scalable Online Learning of Spatial Concepts and Language Models with Mapping. (arXiv:1803.03481v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1803.03481</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel online learning algorithm, SpCoSLAM 2.0 for
spatial concepts and lexical acquisition with higher accuracy and scalability.
In previous work, we proposed SpCoSLAM as an online learning algorithm based on
the Rao--Blackwellized particle filter. However, this conventional algorithm
had problems such as the decrease of the estimation accuracy due to the
influence of the early stages of learning as well as the increase of the
computational complexity with the increase of the training data. Therefore, we
first develop an improved algorithm by introducing new techniques such as
rejuvenation. Next, we develop a scalable algorithm to reduce the calculation
time while maintaining a higher accuracy than the conventional algorithm. In
the experiment, we evaluate and compare the estimation accuracy and calculation
time of the proposed algorithm, conventional online algorithm, and batch
learning. The experimental results demonstrate that the proposed algorithm not
only exceeds the accuracy of the conventional algorithm but also capable of
achieving an accuracy comparable to that of batch learning. In addition, the
proposed algorithm showed that the calculation time does not depend on the
amount of training data and becomes constant for each step with the scalable
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taniguchi_A/0/1/0/all/0/1&quot;&gt;Akira Taniguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagiwara_Y/0/1/0/all/0/1&quot;&gt;Yoshinobu Hagiwara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1&quot;&gt;Tadahiro Taniguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inamura_T/0/1/0/all/0/1&quot;&gt;Tetsunari Inamura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03487">
<title>Cooperative Starting Movement Detection of Cyclists Using Convolutional Neural Networks and a Boosted Stacking Ensemble. (arXiv:1803.03487v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.03487</link>
<description rdf:parseType="Literal">&lt;p&gt;In future, vehicles and other traffic participants will be interconnected and
equipped with various types of sensors, allowing for cooperation on different
levels, such as situation prediction or intention detection. In this article we
present a cooperative approach for starting movement detection of cyclists
using a boosted stacking ensemble approach realizing feature- and decision
level cooperation. We introduce a novel method based on a 3D Convolutional
Neural Network (CNN) to detect starting motions on image sequences by learning
spatio-temporal features. The CNN is complemented by a smart device based
starting movement detection originating from smart devices carried by the
cyclist. Both model outputs are combined in a stacking ensemble approach using
an extreme gradient boosting classifier resulting in a fast and yet robust
cooperative starting movement detector. We evaluate our cooperative approach on
real-world data originating from experiments with 49 test subjects consisting
of 84 starting motions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bieshaar_M/0/1/0/all/0/1&quot;&gt;Maarten Bieshaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1&quot;&gt;Stefan Zernetsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubert_A/0/1/0/all/0/1&quot;&gt;Andreas Hubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1&quot;&gt;Bernhard Sick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1&quot;&gt;Konrad Doll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03491">
<title>Valuing knowledge, information and agency in Multi-agent Reinforcement Learning: a case study in smart buildings. (arXiv:1803.03491v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1803.03491</link>
<description rdf:parseType="Literal">&lt;p&gt;Increasing energy efficiency in buildings can reduce costs and emissions
substantially. Historically, this has been treated as a local, or single-agent,
optimization problem. However, many buildings utilize the same types of thermal
equipment e.g. electric heaters and hot water vessels. During operation,
occupants in these buildings interact with the equipment differently thereby
driving them to diverse regions in the state-space. Reinforcement learning
agents can learn from these interactions, recorded as sensor data, to optimize
the overall energy efficiency. However, if these agents operate individually at
a household level, they can not exploit the replicated structure in the
problem. In this paper, we demonstrate that this problem can indeed benefit
from multi-agent collaboration by making use of targeted exploration of the
state-space allowing for better generalization. We also investigate trade-offs
between integrating human knowledge and additional sensors. Results show that
savings of over 40% are possible with collaborative multi-agent systems making
use of either expert knowledge or additional sensors with no loss of occupant
comfort. We find that such multi-agent systems comfortably outperform
comparable single agent systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazmi_H/0/1/0/all/0/1&quot;&gt;Hussain Kazmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1&quot;&gt;Johan Suykens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Driesen_J/0/1/0/all/0/1&quot;&gt;Johan Driesen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.04664">
<title>Online Spatial Concept and Lexical Acquisition with Simultaneous Localization and Mapping. (arXiv:1704.04664v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1704.04664</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an online learning algorithm based on a
Rao-Blackwellized particle filter for spatial concept acquisition and mapping.
We have proposed a nonparametric Bayesian spatial concept acquisition model
(SpCoA). We propose a novel method (SpCoSLAM) integrating SpCoA and FastSLAM in
the theoretical framework of the Bayesian generative model. The proposed method
can simultaneously learn place categories and lexicons while incrementally
generating an environmental map. Furthermore, the proposed method has scene
image features and a language model added to SpCoA. In the experiments, we
tested online learning of spatial concepts and environmental maps in a novel
environment of which the robot did not have a map. Then, we evaluated the
results of online learning of spatial concepts and lexical acquisition. The
experimental results demonstrated that the robot was able to more accurately
learn the relationships between words and the place in the environmental map
incrementally by using the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taniguchi_A/0/1/0/all/0/1&quot;&gt;Akira Taniguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagiwara_Y/0/1/0/all/0/1&quot;&gt;Yoshinobu Hagiwara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1&quot;&gt;Tadahiro Taniguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inamura_T/0/1/0/all/0/1&quot;&gt;Tetsunari Inamura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.00109">
<title>A Labelling Framework for Probabilistic Argumentation. (arXiv:1708.00109v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1708.00109</link>
<description rdf:parseType="Literal">&lt;p&gt;The combination of argumentation and probability paves the way to new
accounts of qualitative and quantitative uncertainty, thereby offering new
theoretical and applicative opportunities. Due to a variety of interests,
probabilistic argumentation is approached in the literature with different
frameworks, pertaining to structured and abstract argumentation, and with
respect to diverse types of uncertainty, in particular the uncertainty on the
credibility of the premises, the uncertainty about which arguments to consider,
and the uncertainty on the acceptance status of arguments or statements.
Towards a general framework for probabilistic argumentation, we investigate a
labelling-oriented framework encompassing a basic setting for rule-based
argumentation and its (semi-) abstract account, along with diverse types of
uncertainty. Our framework provides a systematic treatment of various kinds of
uncertainty and of their relationships and allows us to back or question
assertions from the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riveret_R/0/1/0/all/0/1&quot;&gt;Regis Riveret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baroni_P/0/1/0/all/0/1&quot;&gt;Pietro Baroni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Governatori_G/0/1/0/all/0/1&quot;&gt;Guido Governatori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rotolo_A/0/1/0/all/0/1&quot;&gt;Antonino Rotolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sartor_G/0/1/0/all/0/1&quot;&gt;Giovanni Sartor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03241">
<title>Efficient Algorithms for Outlier-Robust Regression. (arXiv:1803.03241v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03241</link>
<description rdf:parseType="Literal">&lt;p&gt;We give the first polynomial-time algorithm for performing linear or
polynomial regression resilient to adversarial corruptions in both examples and
labels.
&lt;/p&gt;
&lt;p&gt;Given a sufficiently large (polynomial-size) training set drawn i.i.d. from
distribution D and subsequently corrupted on some fraction of points, our
algorithm outputs a linear function whose squared error is close to the squared
error of the best-fitting linear function with respect to D, assuming that the
marginal distribution of D over the input space is \emph{certifiably
hypercontractive}. This natural property is satisfied by many well-studied
distributions such as Gaussian, strongly log-concave distributions and, uniform
distribution on the hypercube among others. We also give a simple statistical
lower bound showing that some distributional assumption is necessary to succeed
in this setting.
&lt;/p&gt;
&lt;p&gt;These results are the first of their kind and were not known to be even
information-theoretically possible prior to our work.
&lt;/p&gt;
&lt;p&gt;Our approach is based on the sum-of-squares (SoS) method and is inspired by
the recent applications of the method for parameter recovery problems in
unsupervised learning. Our algorithm can be seen as a natural convex relaxation
of the following conceptually simple non-convex optimization problem: find a
linear function and a large subset of the input corrupted sample such that the
least squares loss of the function over the subset is minimized over all
possible large subsets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klivans_A/0/1/0/all/0/1&quot;&gt;Adam Klivans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kothari_P/0/1/0/all/0/1&quot;&gt;Pravesh K. Kothari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meka_R/0/1/0/all/0/1&quot;&gt;Raghu Meka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03319">
<title>Efficient Loss-Based Decoding On Graphs For Extreme Classification. (arXiv:1803.03319v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.03319</link>
<description rdf:parseType="Literal">&lt;p&gt;In extreme classification problems, learning algorithms are required to map
instances to labels from an extremely large label set. We build on a recent
extreme classification framework with logarithmic time and space, and on a
general approach for error correcting output coding (ECOC), and introduce a
flexible and efficient approach accompanied by bounds. Our framework employs
output codes induced by graphs, and offers a tradeoff between accuracy and
model size. We show how to find the sweet spot of this tradeoff using only the
training data. Our experimental study demonstrates the validity of our
assumptions and claims, and shows the superiority of our method compared with
state-of-the-art algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evron_I/0/1/0/all/0/1&quot;&gt;Itay Evron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moroshko_E/0/1/0/all/0/1&quot;&gt;Edward Moroshko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crammer_K/0/1/0/all/0/1&quot;&gt;Koby Crammer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03344">
<title>Robust MCMC Sampling with Non-Gaussian and Hierarchical Priors in High Dimensions. (arXiv:1803.03344v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1803.03344</link>
<description rdf:parseType="Literal">&lt;p&gt;A key problem in inference for high dimensional unknowns is the design of
sampling algorithms whose performance scales favourably with the dimension of
the unknown. A typical setting in which these problems arise is the area of
Bayesian inverse problems. In such problems, which include graph-based
learning, nonparametric regression and PDE-based inversion, the unknown can be
viewed as an infinite-dimensional parameter (such as a function) that has been
discretised. This results in a high-dimensional space for inference. Here we
study robustness of an MCMC algorithm for posterior inference; this refers to
MCMC convergence rates that do not deteriorate as the discretisation becomes
finer. When a Gaussian prior is employed there is a known methodology for the
design of robust MCMC samplers. However, one often requires more flexibility
than a Gaussian prior can provide: hierarchical models are used to enable
inference of parameters underlying a Gaussian prior; or non-Gaussian priors,
such as Besov, are employed to induce sparse MAP estimators; or deep Gaussian
priors are used to represent other non-Gaussian phenomena; and piecewise
constant functions, which are necessarily non-Gaussian, are required for
classification problems. The purpose of this article is to show that the
simulation technology available for Gaussian priors can be exported to such
non-Gaussian priors. The underlying methodology is based on a white noise
representation of the unknown. This is exploited both for robust posterior
sampling and for joint inference of the function and parameters involved in the
specification of its prior, in which case our framework borrows strength from
the well-developed non-centred methodology for Bayesian hierarchical models.
The desired robustness of the proposed sampling algorithms is supported by some
theory and by extensive numerical evidence from several challenging problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_V/0/1/0/all/0/1&quot;&gt;Victor Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dunlop_M/0/1/0/all/0/1&quot;&gt;Matthew M. Dunlop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papaspiliopoulos_O/0/1/0/all/0/1&quot;&gt;Omiros Papaspiliopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stuart_A/0/1/0/all/0/1&quot;&gt;Andrew M. Stuart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03348">
<title>Joint Estimation and Inference for Data Integration Problems based on Multiple Multi-layered Gaussian Graphical Models. (arXiv:1803.03348v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03348</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid development of high-throughput technologies has enabled the
generation of data from biological or disease processes that span multiple
layers, like genomic, proteomic or metabolomic data, and further pertain to
multiple sources, like disease subtypes or experimental conditions. In this
work, we propose a general statistical framework based on Gaussian graphical
models for horizontal (i.e. across conditions or subtypes) and vertical (i.e.
across different layers containing data on molecular compartments) integration
of information in such datasets. We start with decomposing the multi-layer
problem into a series of two-layer problems. For each two-layer problem, we
model the outcomes at a node in the lower layer as dependent on those of other
nodes in that layer, as well as all nodes in the upper layer. We use a
combination of neighborhood selection and group-penalized regression to obtain
sparse estimates of all model parameters. Following this, we develop a
debiasing technique and asymptotic distributions of inter-layer directed edge
weights that utilize already computed neighborhood selection coefficients for
nodes in the upper layer. Subsequently, we establish global and simultaneous
testing procedures for these edge weights. Performance of the proposed
methodology is evaluated on synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Majumdar_S/0/1/0/all/0/1&quot;&gt;Subhabrata Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Michailidis_G/0/1/0/all/0/1&quot;&gt;George Michailidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03383">
<title>High-Accuracy Low-Precision Training. (arXiv:1803.03383v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.03383</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-precision computation is often used to lower the time and energy cost of
machine learning, and recently hardware accelerators have been developed to
support it. Still, it has been used primarily for inference - not training.
Previous low-precision training algorithms suffered from a fundamental
tradeoff: as the number of bits of precision is lowered, quantization noise is
added to the model, which limits statistical accuracy. To address this issue,
we describe a simple low-precision stochastic gradient descent variant called
HALP. HALP converges at the same theoretical rate as full-precision algorithms
despite the noise introduced by using low precision throughout execution. The
key idea is to use SVRG to reduce gradient variance, and to combine this with a
novel technique called bit centering to reduce quantization error. We show that
on the CPU, HALP can run up to $4 \times$ faster than full-precision SVRG and
can match its convergence trajectory. We implemented HALP in TensorQuant, and
show that it exceeds the validation performance of plain low-precision SGD on
two deep learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1&quot;&gt;Christopher De Sa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leszczynski_M/0/1/0/all/0/1&quot;&gt;Megan Leszczynski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marzoev_A/0/1/0/all/0/1&quot;&gt;Alana Marzoev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aberger_C/0/1/0/all/0/1&quot;&gt;Christopher R. Aberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olukotun_K/0/1/0/all/0/1&quot;&gt;Kunle Olukotun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1&quot;&gt;Christopher R&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03432">
<title>Bayesian Optimization for Dynamic Problems. (arXiv:1803.03432v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03432</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose practical extensions to Bayesian optimization for solving dynamic
problems. We model dynamic objective functions using spatiotemporal Gaussian
process priors which capture all the instances of the functions over time. Our
extensions to Bayesian optimization use the information learnt from this model
to guide the tracking of a temporally evolving minimum. By exploiting temporal
correlations, the proposed method also determines when to make evaluations, how
fast to make those evaluations, and it induces an appropriate budget of steps
based on the available information. Lastly, we evaluate our technique on
synthetic and real-world problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nyikosa_F/0/1/0/all/0/1&quot;&gt;Favour M. Nyikosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osborne_M/0/1/0/all/0/1&quot;&gt;Michael A. Osborne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen J. Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03466">
<title>A Stochastic Semismooth Newton Method for Nonsmooth Nonconvex Optimization. (arXiv:1803.03466v1 [math.OC])</title>
<link>http://arxiv.org/abs/1803.03466</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a globalized stochastic semismooth Newton method for
solving stochastic optimization problems involving smooth nonconvex and
nonsmooth convex terms in the objective function. We assume that only noisy
gradient and Hessian information of the smooth part of the objective function
is available via calling stochastic first and second order oracles. The
proposed method can be seen as a hybrid approach combining stochastic
semismooth Newton steps and stochastic proximal gradient steps. Two inexact
growth conditions are incorporated to monitor the convergence and the
acceptance of the semismooth Newton steps and it is shown that the algorithm
converges globally to stationary points in expectation. Moreover, under
standard assumptions and utilizing random matrix concentration inequalities, we
prove that the proposed approach locally turns into a pure stochastic
semismooth Newton method and converges r-superlinearly with high probability.
We present numerical results and comparisons on $\ell_1$-regularized logistic
regression and nonconvex binary classification that demonstrate the efficiency
of our algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Milzarek_A/0/1/0/all/0/1&quot;&gt;Andre Milzarek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xiantao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cen_S/0/1/0/all/0/1&quot;&gt;Shicong Cen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wen_Z/0/1/0/all/0/1&quot;&gt;Zaiwen Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ulbrich_M/0/1/0/all/0/1&quot;&gt;Michael Ulbrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03544">
<title>Explaining Black-box Android Malware Detection. (arXiv:1803.03544v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.03544</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine-learning models have been recently used for detecting malicious
Android applications, reporting impressive performances on benchmark datasets,
even when trained only on features statically extracted from the application,
such as system calls and permissions. However, recent findings have highlighted
the fragility of such in-vitro evaluations with benchmark datasets, showing
that very few changes to the content of Android malware may suffice to evade
detection. How can we thus trust that a malware detector performing well on
benchmark data will continue to do so when deployed in an operating
environment? To mitigate this issue, the most popular Android malware detectors
use linear, explainable machine-learning models to easily identify the most
influential features contributing to each decision. In this work, we generalize
this approach to any black-box machine- learning model, by leveraging a
gradient-based approach to identify the most influential local features. This
enables using nonlinear models to potentially increase accuracy without
sacrificing interpretability of decisions. Our approach also highlights the
global characteristics learned by the model to discriminate between benign and
malware applications. Finally, as shown by our empirical analysis on a popular
Android malware detection task, it also helps identifying potential
vulnerabilities of linear and nonlinear models against adversarial
manipulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melis_M/0/1/0/all/0/1&quot;&gt;Marco Melis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maiorca_D/0/1/0/all/0/1&quot;&gt;Davide Maiorca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1&quot;&gt;Battista Biggio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giacinto_G/0/1/0/all/0/1&quot;&gt;Giorgio Giacinto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1&quot;&gt;Fabio Roli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03571">
<title>City-wide Analysis of Electronic Health Records Reveals Gender and Age Biases in the Administration of Known Drug-Drug Interactions. (arXiv:1803.03571v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1803.03571</link>
<description rdf:parseType="Literal">&lt;p&gt;From a public-health perspective, the occurrence of drug-drug-interactions
(DDI) from multiple drug prescriptions is a serious problem, especially in the
elderly population. This is true both for individuals and the system itself
since patients with complications due to DDI will likely re-enter the system at
a costlier level. We conducted an 18-month study of DDI occurrence in Blumenau
(Brazil; pop. 340,000) using city-wide drug dispensing data from both primary
and secondary-care level. Our goal is also to identify possible risk factors in
a large population, ultimately characterizing the burden of DDI for patients,
doctors and the public system itself. We found 181 distinct DDI being
prescribed concomitantly to almost 5% of the city population. We also
discovered that women are at a 60% risk increase of DDI when compared to men,
while only having a 6% co-administration risk increase. Analysis of the DDI
co-occurrence network reveals which DDI pairs are most associated with the
observed greater DDI risk for females, demonstrating that contraception and
hormone therapy are not the main culprits of the gender disparity, which is
maximized after the reproductive years. Furthermore, DDI risk increases
dramatically with age, with patients age 70-79 having a 50-fold risk increase
in comparison to patients aged 0-19. Interestingly, several null models
demonstrate that this risk increase is not due to increased polypharmacy with
age. Finally, we demonstrate that while the number of drugs and
co-administrations help predict a patient&apos;s number of DDI ($R^2=.413$), they
are not sufficient to flag these patients accurately, which we achieve by
training classifiers with additional data (MCC=.83,F1=.72). These results
demonstrate that accurate warning systems for known DDI can be devised for
public and private systems alike, resulting in substantial prevention of
DDI-related ADR and savings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Correia_R/0/1/0/all/0/1&quot;&gt;Rion Brattig Correia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_L/0/1/0/all/0/1&quot;&gt;Luciana P. de Ara&amp;#xfa;jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattos_M/0/1/0/all/0/1&quot;&gt;Mauro M. Mattos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wild_D/0/1/0/all/0/1&quot;&gt;David Wild&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocha_L/0/1/0/all/0/1&quot;&gt;Luis M. Rocha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03607">
<title>On Generation of Adversarial Examples using Convex Programming. (arXiv:1803.03607v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.03607</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been observed that deep learning architectures tend to make erroneous
decisions with high reliability for particularly designed adversarial
instances. In this work, we show that the perturbation analysis of these
architectures provides a method for generating adversarial instances by convex
programming which, for classification tasks, recovers variants of existing
non-adaptive adversarial methods. The proposed method can be used for the
design of adversarial noise under various desirable constraints and different
types of networks. Furthermore, the core idea of this method is that neural
networks can be well approximated by a linear function. Experiments show the
competitive performance of the obtained algorithms, in terms of fooling ratio,
when benchmarked with well-known adversarial methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balda_E/0/1/0/all/0/1&quot;&gt;Emilio Rafael Balda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behboodi_A/0/1/0/all/0/1&quot;&gt;Arash Behboodi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathar_R/0/1/0/all/0/1&quot;&gt;Rudolph Mathar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03623">
<title>Hourly-Similarity Based Solar Forecasting Using Multi-Model Machine Learning Blending. (arXiv:1803.03623v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03623</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing penetration of solar power into power systems,
forecasting becomes critical in power system operations. In this paper, an
hourly-similarity (HS) based method is developed for 1-hour-ahead (1HA) global
horizontal irradiance (GHI) forecasting. This developed method utilizes diurnal
patterns, statistical distinctions between different hours, and hourly
similarities in solar data to improve the forecasting accuracy. The HS-based
method is built by training multiple two-layer multi-model forecasting
framework (MMFF) models independently with the same-hour subsets. The final
optimal model is a combination of MMFF models with the best-performed blending
algorithm at every hour. At the forecasting stage, the most suitable model is
selected to perform the forecasting subtask of a certain hour. The HS-based
method is validated by 1-year data with six solar features collected by the
National Renewable Energy Laboratory (NREL). Results show that the HS-based
method outperforms the non-HS (all-in-one) method significantly with the same
MMFF architecture, wherein the optimal HS- based method outperforms the best
all-in-one method by 10.94% and 7.74% based on the normalized mean absolute
error and normalized root mean square error, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Cong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.08024">
<title>EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces. (arXiv:1611.08024v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1611.08024</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain computer interfaces (BCI) enable direct communication with a computer,
using neural activity as the control signal. This signal is generally chosen
from a variety of well-studied electroencephalogram (EEG) signals. For a given
BCI paradigm, feature extractors and classifiers are tailored to the distinct
characteristics of its expected EEG control signal, limiting its application to
that specific signal. Convolutional Neural Networks (CNNs), which have been
used in computer vision and speech recognition to perform automatic feature
extraction and classification, have successfully been applied to EEG-based
BCIs; however, they have mainly been applied to single BCI paradigms and thus
it remains unclear how these architectures generalize to other paradigms. Here,
we ask if we can design a single CNN architecture to accurately classify EEG
signals from different BCI paradigms, while simultaneously being as compact as
possible (defined as the number of parameters in the model). In this work we
introduce EEGNet, a compact convolutional network for EEG-based BCIs. We
introduce the use of depthwise and separable convolutions to more efficiently
extract relevant features for EEG-based BCIs. We compare EEGNet, both for
within-subject and cross-subject classification, to current state-of-the-art
approaches across four BCI paradigms: P300 visual-evoked potentials,
error-related negativity responses (ERN), movement-related cortical potentials
(MRCP), and sensory motor rhythms (SMR). We show that EEGNet generalizes across
paradigms better than, and achieves comparably high performance to, traditional
approaches, while simultaneously fitting up to two orders of magnitude fewer
parameters. We also demonstrate ways to visualize the contents of a trained
EEGNet model to enable interpretation of the learned features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawhern_V/0/1/0/all/0/1&quot;&gt;Vernon J. Lawhern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solon_A/0/1/0/all/0/1&quot;&gt;Amelia J. Solon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waytowich_N/0/1/0/all/0/1&quot;&gt;Nicholas R. Waytowich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordon_S/0/1/0/all/0/1&quot;&gt;Stephen M. Gordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hung_C/0/1/0/all/0/1&quot;&gt;Chou P. Hung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lance_B/0/1/0/all/0/1&quot;&gt;Brent J. Lance&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.00577">
<title>Generalization Properties of Doubly Stochastic Learning Algorithms. (arXiv:1707.00577v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.00577</link>
<description rdf:parseType="Literal">&lt;p&gt;Doubly stochastic learning algorithms are scalable kernel methods that
perform very well in practice. However, their generalization properties are not
well understood and their analysis is challenging since the corresponding
learning sequence may not be in the hypothesis space induced by the kernel. In
this paper, we provide an in-depth theoretical analysis for different variants
of doubly stochastic learning algorithms within the setting of nonparametric
regression in a reproducing kernel Hilbert space and considering the square
loss. Particularly, we derive convergence results on the generalization error
for the studied algorithms either with or without an explicit penalty term. To
the best of our knowledge, the derived results for the unregularized variants
are the first of this kind, while the results for the regularized variants
improve those in the literature. The novelties in our proof are a sample error
bound that requires controlling the trace norm of a cumulative operator, and a
refined analysis of bounding initial error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junhong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1&quot;&gt;Lorenzo Rosasco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.02245">
<title>Linear-Time Sequence Classification using Restricted Boltzmann Machines. (arXiv:1710.02245v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.02245</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification of sequence data is the topic of interest for dynamic Bayesian
models and Recurrent Neural Networks (RNNs). While the former can explicitly
model the temporal dependencies between class variables, the latter have a
capability of learning representations. Several attempts have been made to
improve performance by combining these two approaches or increasing the
processing capability of the hidden units in RNNs. This often results in
complex models with a large number of learning parameters. In this paper, a
compact model is proposed which offers both representation learning and
temporal inference of class variables by rolling Restricted Boltzmann Machines
(RBMs) and class variables over time. We address the key issue of
intractability in this variant of RBMs by optimising a conditional
distribution, instead of a joint distribution. Experiments reported in the
paper on melody modelling and optical character recognition show that the
proposed model can outperform the state-of-the-art. Also, the experimental
results on optical character recognition, part-of-speech tagging and text
chunking demonstrate that our model is comparable to recurrent neural networks
with complex memory gates while requiring far fewer parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1&quot;&gt;Son N. Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherla_S/0/1/0/all/0/1&quot;&gt;Srikanth Cherla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcez_A/0/1/0/all/0/1&quot;&gt;Artur Garcez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weyde_T/0/1/0/all/0/1&quot;&gt;Tillman Weyde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.04881">
<title>User Modelling for Avoiding Overfitting in Interactive Knowledge Elicitation for Prediction. (arXiv:1710.04881v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/1710.04881</link>
<description rdf:parseType="Literal">&lt;p&gt;In human-in-the-loop machine learning, the user provides information beyond
that in the training data. Many algorithms and user interfaces have been
designed to optimize and facilitate this human--machine interaction; however,
fewer studies have addressed the potential defects the designs can cause.
Effective interaction often requires exposing the user to the training data or
its statistics. The design of the system is then critical, as this can lead to
double use of data and overfitting, if the user reinforces noisy patterns in
the data. We propose a user modelling methodology, by assuming simple rational
behaviour, to correct the problem. We show, in a user study with 48
participants, that the method improves predictive performance in a sparse
linear regression sentiment analysis task, where graded user knowledge on
feature relevance is elicited. We believe that the key idea of inferring user
knowledge with probabilistic user models has general applicability in guarding
against overfitting and improving interactive machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daee_P/0/1/0/all/0/1&quot;&gt;Pedram Daee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peltola_T/0/1/0/all/0/1&quot;&gt;Tomi Peltola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vehtari_A/0/1/0/all/0/1&quot;&gt;Aki Vehtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1&quot;&gt;Samuel Kaski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09070">
<title>Fast Cosmic Web Simulations with Generative Adversarial Networks. (arXiv:1801.09070v2 [astro-ph.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1801.09070</link>
<description rdf:parseType="Literal">&lt;p&gt;Dark matter in the universe evolves through gravity to form a complex network
of halos, filaments, sheets and voids, that is known as the cosmic web.
Computational models of the underlying physical processes, such as classical
N-body simulations, are extremely resource intensive, as they track the action
of gravity in an expanding universe using billions of particles as tracers of
the cosmic matter distribution. Therefore, upcoming cosmology experiments will
face a computational bottleneck that may limit the exploitation of their full
scientific potential. To address this challenge, we demonstrate the application
of a machine learning technique called Generative Adversarial Networks (GAN) to
learn models that can efficiently generate new, physically realistic
realizations of the cosmic web. Our training set is a small, representative
sample of 2D image snapshots from N-body simulations of size 500 and 100 Mpc.
We show that the GAN-produced results are qualitatively and quantitatively very
similar to the originals. Generation of a new cosmic web realization with a GAN
takes a fraction of a second, compared to the many hours needed by the N-body
technique. We anticipate that GANs will therefore play an important role in
providing extremely fast and precise simulations of cosmic web in the era of
large cosmological surveys, such as Euclid and LSST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Andres C Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kacprzak_T/0/1/0/all/0/1&quot;&gt;Tomasz Kacprzak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Lucchi_A/0/1/0/all/0/1&quot;&gt;Aurelien Lucchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Amara_A/0/1/0/all/0/1&quot;&gt;Adam Amara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Sgier_R/0/1/0/all/0/1&quot;&gt;Raphael Sgier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Fluri_J/0/1/0/all/0/1&quot;&gt;Janis Fluri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Hofmann_T/0/1/0/all/0/1&quot;&gt;Thomas Hofmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Refregier_A/0/1/0/all/0/1&quot;&gt;Alexandre R&amp;#xe9;fr&amp;#xe9;gier&lt;/a&gt;</dc:creator>
</item></rdf:RDF>