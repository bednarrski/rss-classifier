<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-15T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04054"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05752"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05768"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.07561"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.06374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.02314"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.03748"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04837"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05554"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05573"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05589"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05621"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05776"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05784"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05867"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05897"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.09033"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.03003"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.01955"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.03658"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05104"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1709.04054">
<title>Shifting Mean Activation Towards Zero with Bipolar Activation Functions. (arXiv:1709.04054v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04054</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a simple extension to the ReLU-family of activation functions that
allows them to shift the mean activation across a layer towards zero. Combined
with proper weight initialization, this alleviates the need for normalization
layers. We explore the training of deep vanilla recurrent neural networks
(RNNs) with up to 144 layers, and show that bipolar activation functions help
learning in this setting. On the Penn Treebank and Text8 language modeling
tasks we obtain competitive results, improving on the best reported results for
non-gated networks. In experiments with convolutional neural networks without
batch normalization, we find that bipolar activations produce a faster drop in
training error, and results in a lower test error on the CIFAR-10
classification task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eidnes_L/0/1/0/all/0/1&quot;&gt;Lars Eidnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nokland_A/0/1/0/all/0/1&quot;&gt;Arild N&amp;#xf8;kland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05752">
<title>Rearrangement with Nonprehensile Manipulation Using Deep Reinforcement Learning. (arXiv:1803.05752v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1803.05752</link>
<description rdf:parseType="Literal">&lt;p&gt;Rearranging objects on a tabletop surface by means of nonprehensile
manipulation is a task which requires skillful interaction with the physical
world. Usually, this is achieved by precisely modeling physical properties of
the objects, robot, and the environment for explicit planning. In contrast, as
explicitly modeling the physical environment is not always feasible and
involves various uncertainties, we learn a nonprehensile rearrangement strategy
with deep reinforcement learning based on only visual feedback. For this, we
model the task with rewards and train a deep Q-network. Our potential
field-based heuristic exploration strategy reduces the amount of collisions
which lead to suboptimal outcomes and we actively balance the training set to
avoid bias towards poor examples. Our training process leads to quicker
learning and better performance on the task as compared to uniform exploration
and standard experience replay. We demonstrate empirical evidence from
simulation that our method leads to a success rate of 85%, show that our system
can cope with sudden changes of the environment, and compare our performance
with human level performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Weihao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stork_J/0/1/0/all/0/1&quot;&gt;Johannes A. Stork&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1&quot;&gt;Danica Kragic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Michael Y. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hang_K/0/1/0/all/0/1&quot;&gt;Kaiyu Hang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05768">
<title>PAC-Reasoning in Relational Domains. (arXiv:1803.05768v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.05768</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of predicting plausible missing facts in relational
data, given a set of imperfect logical rules. In particular, our aim is to
provide bounds on the (expected) number of incorrect inferences that are made
in this way. Since for classical inference it is in general impossible to bound
this number in a non-trivial way, we consider two inference relations that
weaken, but remain close in spirit to classical inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuzelka_O/0/1/0/all/0/1&quot;&gt;Ondrej Kuzelka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1&quot;&gt;Jesse Davis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1&quot;&gt;Steven Schockaert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.07561">
<title>A-NICE-MC: Adversarial Training for MCMC. (arXiv:1706.07561v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.07561</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing Markov Chain Monte Carlo (MCMC) methods are either based on
general-purpose and domain-agnostic schemes which can lead to slow convergence,
or hand-crafting of problem-specific proposals by an expert. We propose
A-NICE-MC, a novel method to train flexible parametric Markov chain kernels to
produce samples with desired properties. First, we propose an efficient
likelihood-free adversarial training method to train a Markov chain and mimic a
given data distribution. Then, we leverage flexible volume preserving flows to
obtain parametric kernels for MCMC. Using a bootstrap approach, we show how to
train efficient Markov chains to sample from a prescribed posterior
distribution by iteratively improving the quality of both the model and the
samples. A-NICE-MC provides the first framework to automatically design
efficient domain-specific MCMC proposals. Empirical results demonstrate that
A-NICE-MC combines the strong guarantees of MCMC with the expressiveness of
deep neural networks, and is able to significantly outperform competing methods
such as Hamiltonian Monte Carlo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jiaming Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shengjia Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.06374">
<title>On a Formal Model of Safe and Scalable Self-driving Cars. (arXiv:1708.06374v5 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1708.06374</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, car makers and tech companies have been racing towards self
driving cars. It seems that the main parameter in this race is who will have
the first car on the road. The goal of this paper is to add to the equation two
additional crucial parameters. The first is standardization of safety assurance
--- what are the minimal requirements that every self-driving car must satisfy,
and how can we verify these requirements. The second parameter is scalability
--- engineering solutions that lead to unleashed costs will not scale to
millions of cars, which will push interest in this field into a niche academic
corner, and drive the entire field into a &quot;winter of autonomous driving&quot;. In
the first part of the paper we propose a white-box, interpretable, mathematical
model for safety assurance, which we call Responsibility-Sensitive Safety
(RSS). In the second part we describe a design of a system that adheres to our
safety assurance requirements and is scalable to millions of cars.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1&quot;&gt;Shai Shalev-Shwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shammah_S/0/1/0/all/0/1&quot;&gt;Shaked Shammah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1&quot;&gt;Amnon Shashua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.02314">
<title>Representation Learning for Visual-Relational Knowledge Graphs. (arXiv:1709.02314v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.02314</link>
<description rdf:parseType="Literal">&lt;p&gt;A visual-relational knowledge graph (KG) is a multi-relational graph whose
entities are associated with images. We introduce ImageGraph, a KG with 1,330
relation types, 14,870 entities, and 829,931 images. Visual-relational KGs lead
to novel probabilistic query types where images are treated as first-class
citizens. Both the prediction of relations between unseen images and
multi-relational image retrieval can be formulated as query types in a
visual-relational KG. We approach the problem of answering such queries with a
novel combination of deep convolutional networks and models for learning
knowledge graph embeddings. The resulting models can answer queries such as
&quot;How are these two unseen images related to each other?&quot; We also explore a
zero-shot learning scenario where an image of an entirely new entity is linked
with multiple relations to entities of an existing KG. The multi-relational
grounding of unseen entity images into a knowledge graph serves as the
description of such an entity. We conduct experiments to demonstrate that the
proposed deep architectures in combination with KG embedding objectives can
answer the visual-relational queries efficiently and accurately.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onoro_Rubio_D/0/1/0/all/0/1&quot;&gt;Daniel O&amp;#xf1;oro-Rubio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Duran_A/0/1/0/all/0/1&quot;&gt;Alberto Garc&amp;#xed;a-Dur&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_R/0/1/0/all/0/1&quot;&gt;Roberto Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_Sastre_R/0/1/0/all/0/1&quot;&gt;Roberto J. L&amp;#xf3;pez-Sastre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.03748">
<title>Emergent Complexity via Multi-Agent Competition. (arXiv:1710.03748v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.03748</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning algorithms can train agents that solve problems in
complex, interesting environments. Normally, the complexity of the trained
agent is closely related to the complexity of the environment. This suggests
that a highly capable agent requires a complex environment for training. In
this paper, we point out that a competitive multi-agent environment trained
with self-play can produce behaviors that are far more complex than the
environment itself. We also point out that such environments come with a
natural curriculum, because for any skill level, an environment full of agents
of this level will have the right level of difficulty. This work introduces
several competitive multi-agent environments where agents compete in a 3D world
with simulated physics. The trained agents learn a wide variety of complex and
interesting skills, even though the environment themselves are relatively
simple. The skills include behaviors such as running, blocking, ducking,
tackling, fooling opponents, kicking, and defending using both arms and legs. A
highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_T/0/1/0/all/0/1&quot;&gt;Trapit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pachocki_J/0/1/0/all/0/1&quot;&gt;Jakub Pachocki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sidor_S/0/1/0/all/0/1&quot;&gt;Szymon Sidor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutskever_I/0/1/0/all/0/1&quot;&gt;Ilya Sutskever&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1&quot;&gt;Igor Mordatch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04837">
<title>Learning the Joint Representation of Heterogeneous Temporal Events for Clinical Endpoint Prediction. (arXiv:1803.04837v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04837</link>
<description rdf:parseType="Literal">&lt;p&gt;The availability of a large amount of electronic health records (EHR)
provides huge opportunities to improve health care service by mining these
data. One important application is clinical endpoint prediction, which aims to
predict whether a disease, a symptom or an abnormal lab test will happen in the
future according to patients&apos; history records. This paper develops deep
learning techniques for clinical endpoint prediction, which are effective in
many practical applications. However, the problem is very challenging since
patients&apos; history records contain multiple heterogeneous temporal events such
as lab tests, diagnosis, and drug administrations. The visiting patterns of
different types of events vary significantly, and there exist complex nonlinear
relationships between different events. In this paper, we propose a novel model
for learning the joint representation of heterogeneous temporal events. The
model adds a new gate to control the visiting rates of different events which
effectively models the irregular patterns of different events and their
nonlinear correlations. Experiment results with real-world clinical data on the
tasks of predicting death and abnormal lab tests prove the effectiveness of our
proposed approach over competitive baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Luchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jianhao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zichang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05554">
<title>Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models. (arXiv:1803.05554v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1803.05554</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning a Bayesian network (BN) from data can be useful for decision-making
or discovering causal relationships. However, traditional methods often fail in
modern applications, which exhibit a larger number of observed variables than
data points. The resulting uncertainty about the underlying network as well as
the desire to incorporate prior information recommend a Bayesian approach to
learning the BN, but the highly combinatorial structure of BNs poses a striking
challenge for inference. The current state-of-the-art methods such as order
MCMC are faster than previous methods but prevent the use of many natural
structural priors and still have running time exponential in the maximum
indegree of the true directed acyclic graph (DAG) of the BN. We here propose an
alternative posterior approximation based on the observation that, if we
incorporate empirical conditional independence tests, we can focus on a
high-probability DAG associated with each order of the vertices. We show that
our method allows the desired flexibility in prior specification, removes
timing dependence on the maximum indegree and yields provably good posterior
approximations; in addition, we show that it achieves superior accuracy,
scalability, and sampler mixing on several datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Agrawal_R/0/1/0/all/0/1&quot;&gt;Raj Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Broderick_T/0/1/0/all/0/1&quot;&gt;Tamara Broderick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Uhler_C/0/1/0/all/0/1&quot;&gt;Caroline Uhler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05573">
<title>Improving GANs Using Optimal Transport. (arXiv:1803.05573v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.05573</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Optimal Transport GAN (OT-GAN), a variant of generative
adversarial nets minimizing a new metric measuring the distance between the
generator distribution and the data distribution. This metric, which we call
mini-batch energy distance, combines optimal transport in primal form with an
energy distance defined in an adversarially learned feature space, resulting in
a highly discriminative distance function with unbiased mini-batch gradients.
Experimentally we show OT-GAN to be highly stable when trained with large
mini-batches, and we present state-of-the-art results on several popular
benchmark problems for image generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1&quot;&gt;Tim Salimans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Han Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1&quot;&gt;Alec Radford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1&quot;&gt;Dimitris Metaxas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05589">
<title>Variational Message Passing with Structured Inference Networks. (arXiv:1803.05589v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.05589</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent efforts on combining deep models with probabilistic graphical models
are promising in providing flexible models that are also easy to interpret. We
propose a variational message-passing algorithm for variational inference in
such models. We make three contributions. First, we propose structured
inference networks that incorporate the structure of the graphical model in the
inference network of variational auto-encoders (VAE). Second, we establish
conditions under which such inference networks enable fast amortized inference
similar to VAE. Finally, we derive a variational message passing algorithm to
perform efficient natural-gradient inference while retaining the efficiency of
the amortized inference. By simultaneously enabling structured, amortized, and
natural-gradient inference for deep structured models, our method simplifies
and generalizes existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hubacher_N/0/1/0/all/0/1&quot;&gt;Nicolas Hubacher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05621">
<title>Proximal SCOPE for Distributed Sparse Learning: Better Data Partition Implies Faster Convergence Rate. (arXiv:1803.05621v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.05621</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed sparse learning with a cluster of multiple machines has attracted
much attention in machine learning, especially for large-scale applications
with high-dimensional data. One popular way to implement sparse learning is to
use $L_1$ regularization. In this paper, we propose a novel method, called
proximal \mbox{SCOPE}~(\mbox{pSCOPE}), for distributed sparse learning with
$L_1$ regularization. pSCOPE is based on a \underline{c}ooperative
\underline{a}utonomous \underline{l}ocal \underline{l}earning~(\mbox{CALL})
framework. In the \mbox{CALL} framework of \mbox{pSCOPE}, we find that the data
partition affects the convergence of the learning procedure, and subsequently
we define a metric to measure the goodness of a data partition. Based on the
defined metric, we theoretically prove that pSCOPE is convergent with a linear
convergence rate if the data partition is good enough. We also prove that
better data partition implies faster convergence rate. Furthermore, pSCOPE is
also communication efficient. Experimental results on real data sets show that
pSCOPE can outperform other state-of-the-art distributed methods for sparse
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shen-Yi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Gong-Duo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming-Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wu-Jun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05776">
<title>Gaussian Processes Over Graphs. (arXiv:1803.05776v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.05776</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Gaussian processes for signals over graphs (GPG) using the apriori
knowledge that the target vectors lie over a graph. We incorporate this
information using a graph- Laplacian based regularization which enforces the
target vectors to have a specific profile in terms of graph Fourier transform
coeffcients, for example lowpass or bandpass graph signals. We discuss how the
regularization affects the mean and the variance in the prediction output. In
particular, we prove that the predictive variance of the GPG is strictly
smaller than the conventional Gaussian process (GP) for any non-trivial graph.
We validate our concepts by application to various real-world graph signals.
Our experiments show that the performance of the GPG is superior to GP for
small training data sizes and under noisy training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Venkitaraman_A/0/1/0/all/0/1&quot;&gt;Arun Venkitaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chatterjee_S/0/1/0/all/0/1&quot;&gt;Saikat Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haandel_P/0/1/0/all/0/1&quot;&gt;Peter Ha&amp;#xe4;ndel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05784">
<title>Minimax optimal rates for Mondrian trees and forests. (arXiv:1803.05784v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.05784</link>
<description rdf:parseType="Literal">&lt;p&gt;Introduced by Breiman (2001), Random Forests are widely used as
classification and regression algorithms. While being initially designed as
batch algorithms, several variants have been proposed to handle online
learning. One particular instance of such forests is the Mondrian Forest, whose
trees are built using the so-called Mondrian process, therefore allowing to
easily update their construction in a streaming fashion. In this paper, we
study Mondrian Forests in a batch setting and prove their consistency assuming
a proper tuning of the lifetime sequence. A thorough theoretical study of
Mondrian partitions allows us to derive an upper bound for the risk of Mondrian
Forests, which turns out to be the minimax optimal rate for both Lipschitz and
twice differentiable regression functions. These results are actually the first
to state that some particular random forests achieve minimax rates \textit{in
arbitrary dimension}, paving the way to a refined theoretical analysis and thus
a deeper understanding of these black box algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mourtada_J/0/1/0/all/0/1&quot;&gt;Jaouad Mourtada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gaiffas_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Ga&amp;#xef;ffas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scornet_E/0/1/0/all/0/1&quot;&gt;Erwan Scornet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05867">
<title>Capturing Structure Implicitly from Time-Series having Limited Data. (arXiv:1803.05867v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.05867</link>
<description rdf:parseType="Literal">&lt;p&gt;Scientific fields such as insider-threat detection and highway-safety
planning often lack sufficient amounts of time-series data to estimate
statistical models for the purpose of scientific discovery. Moreover, the
available limited data are quite noisy. This presents a major challenge when
estimating time-series models that are robust to overfitting and have
well-calibrated uncertainty estimates. Most of the current literature in these
fields involve visualizing the time-series for noticeable structure and hard
coding them into pre-specified parametric functions. This approach is
associated with two limitations. First, given that such trends may not be
easily noticeable in small data, it is difficult to explicitly incorporate
expressive structure into the models during formulation. Second, it is
difficult to know $\textit{a priori}$ the most appropriate functional form to
use. To address these limitations, a nonparametric Bayesian approach was
proposed to implicitly capture hidden structure from time series having limited
data. The proposed model, a Gaussian process with a spectral mixture kernel,
precludes the need to pre-specify a functional form and hard code trends, is
robust to overfitting and has well-calibrated uncertainty estimates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Emaasit_D/0/1/0/all/0/1&quot;&gt;Daniel Emaasit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Johnson_M/0/1/0/all/0/1&quot;&gt;Matthew Johnson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05897">
<title>Contrasting information theoretic decompositions of modulatory and arithmetic interactions in neural information processing systems. (arXiv:1803.05897v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1803.05897</link>
<description rdf:parseType="Literal">&lt;p&gt;Biological and artificial neural systems are composed of many local
processors, and their capabilities depend upon the transfer function that
relates each local processor&apos;s outputs to its inputs. This paper uses a recent
advance in the foundations of information theory to study the properties of
local processors that use contextual input to amplify or attenuate transmission
of information about their driving inputs. This advance enables the information
transmitted by processors with two distinct inputs to be decomposed into those
components unique to each input, that shared between the two inputs, and that
which depends on both though it is in neither, i.e. synergy. The decompositions
that we report here show that contextual modulation has information processing
properties that contrast with those of all four simple arithmetic operators,
that it can take various forms, and that the form used in our previous studies
of artificial neural nets composed of local processors with both driving and
contextual inputs is particularly well-suited to provide the distinctive
capabilities of contextual modulation under a wide range of conditions. We
argue that the decompositions reported here could be compared with those
obtained from empirical neurobiological and psychophysical data under
conditions thought to reflect contextual modulation. That would then shed new
light on the underlying processes involved. Finally, we suggest that such
decompositions could aid the design of context-sensitive machine learning
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kay_J/0/1/0/all/0/1&quot;&gt;Jim W. Kay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_W/0/1/0/all/0/1&quot;&gt;William A. Phillips&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.09033">
<title>Operator Variational Inference. (arXiv:1610.09033v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.09033</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational inference is an umbrella term for algorithms which cast Bayesian
inference as optimization. Classically, variational inference uses the
Kullback-Leibler divergence to define the optimization. Though this divergence
has been widely used, the resultant posterior approximation can suffer from
undesirable statistical properties. To address this, we reexamine variational
inference from its roots as an optimization problem. We use operators, or
functions of functions, to design variational objectives. As one example, we
design a variational objective with a Langevin-Stein operator. We develop a
black box algorithm, operator variational inference (OPVI), for optimizing any
operator objective. Importantly, operators enable us to make explicit the
statistical and computational tradeoffs for variational inference. We can
characterize different properties of variational objectives, such as objectives
that admit data subsampling---allowing inference to scale to massive data---as
well as objectives that admit variational programs---a rich class of posterior
approximations that does not require a tractable density. We illustrate the
benefits of OPVI on a mixture model and a generative model of images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1&quot;&gt;Rajesh Ranganath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Altosaar_J/0/1/0/all/0/1&quot;&gt;Jaan Altosaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tran_D/0/1/0/all/0/1&quot;&gt;Dustin Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.03003">
<title>Tick: a Python library for statistical learning, with a particular emphasis on time-dependent modelling. (arXiv:1707.03003v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.03003</link>
<description rdf:parseType="Literal">&lt;p&gt;Tick is a statistical learning library for Python~3, with a particular
emphasis on time-dependent models, such as point processes, and tools for
generalized linear models and survival analysis. The core of the library is an
optimization module providing model computational classes, solvers and proximal
operators for regularization. tick relies on a C++ implementation and
state-of-the-art optimization algorithms to provide very fast computations in a
single node multi-core setting. Source code and documentation can be downloaded
from https://github.com/X-DataInitiative/tick
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bacry_E/0/1/0/all/0/1&quot;&gt;Emmanuel Bacry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bompaire_M/0/1/0/all/0/1&quot;&gt;Martin Bompaire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gaiffas_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Ga&amp;#xef;ffas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poulsen_S/0/1/0/all/0/1&quot;&gt;Soren Poulsen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.01955">
<title>Wasserstein Dictionary Learning: Optimal Transport-based unsupervised non-linear dictionary learning. (arXiv:1708.01955v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.01955</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new nonlinear dictionary learning method for
histograms in the probability simplex. The method leverages optimal transport
theory, in the sense that our aim is to reconstruct histograms using so-called
displacement interpolations (a.k.a. Wasserstein barycenters) between dictionary
atoms; such atoms are themselves synthetic histograms in the probability
simplex. Our method simultaneously estimates such atoms, and, for each
datapoint, the vector of weights that can optimally reconstruct it as an
optimal transport barycenter of such atoms. Our method is computationally
tractable thanks to the addition of an entropic regularization to the usual
optimal transportation problem, leading to an approximation scheme that is
efficient, parallel and simple to differentiate. Both atoms and weights are
learned using a gradient-based descent method. Gradients are obtained by
automatic differentiation of the generalized Sinkhorn iterations that yield
barycenters with entropic smoothing. Because of its formulation relying on
Wasserstein barycenters instead of the usual matrix product between dictionary
and codes, our method allows for nonlinear relationships between atoms and the
reconstruction of input data. We illustrate its application in several
different image processing settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmitz_M/0/1/0/all/0/1&quot;&gt;Morgan A. Schmitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Heitz_M/0/1/0/all/0/1&quot;&gt;Matthieu Heitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bonneel_N/0/1/0/all/0/1&quot;&gt;Nicolas Bonneel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mboula_F/0/1/0/all/0/1&quot;&gt;Fred Maurice Ngol&amp;#xe8; Mboula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Coeurjolly_D/0/1/0/all/0/1&quot;&gt;David Coeurjolly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cuturi_M/0/1/0/all/0/1&quot;&gt;Marco Cuturi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peyre_G/0/1/0/all/0/1&quot;&gt;Gabriel Peyr&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Starck_J/0/1/0/all/0/1&quot;&gt;Jean-Luc Starck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.03658">
<title>End-to-End Waveform Utterance Enhancement for Direct Evaluation Metrics Optimization by Fully Convolutional Neural Networks. (arXiv:1709.03658v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.03658</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech enhancement model is used to map a noisy speech to a clean speech. In
the training stage, an objective function is often adopted to optimize the
model parameters. However, in most studies, there is an inconsistency between
the model optimization criterion and the evaluation criterion on the enhanced
speech. For example, in measuring speech intelligibility, most of the
evaluation metric is based on a short-time objective intelligibility (STOI)
measure, while the frame based minimum mean square error (MMSE) between
estimated and clean speech is widely used in optimizing the model. Due to the
inconsistency, there is no guarantee that the trained model can provide optimal
performance in applications. In this study, we propose an end-to-end
utterance-based speech enhancement framework using fully convolutional neural
networks (FCN) to reduce the gap between the model optimization and evaluation
criterion. Because of the utterance-based optimization, temporal correlation
information of long speech segments, or even at the entire utterance level, can
be considered when perception-based objective functions are used for the direct
optimization. As an example, we implement the proposed FCN enhancement
framework to optimize the STOI measure. Experimental results show that the STOI
of test speech is better than conventional MMSE-optimized speech due to the
consistency between the training and evaluation target. Moreover, by
integrating the STOI in model optimization, the intelligibility of human
subjects and automatic speech recognition (ASR) system on the enhanced speech
is also substantially improved compared to those generated by the MMSE
criterion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fu_S/0/1/0/all/0/1&quot;&gt;Szu-Wei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao-Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tsao_Y/0/1/0/all/0/1&quot;&gt;Yu Tsao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xugang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kawai_H/0/1/0/all/0/1&quot;&gt;Hisashi Kawai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04924">
<title>Dense Limit of the Dawid-Skene Model for Crowdsourcing and Regions of Sub-optimality of Message Passing Algorithms. (arXiv:1803.04924v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04924</link>
<description rdf:parseType="Literal">&lt;p&gt;Crowdsourcing is a strategy to categorize data through the contribution of
many individuals. A wide range of theoretical and algorithmic contributions are
based on the model of Dawid and Skene [1]. Recently it was shown in [2,3] that,
in certain regimes, belief propagation is asymptotically optimal for data
generated from the Dawid-Skene model. This paper is motivated by this recent
progress. We analyze the dense limit of the Dawid-Skene model. It is shown that
it belongs to a larger class of low-rank matrix estimation problems for which
it is possible to express the asymptotic, Bayes-optimal, performance in a
simple closed form. In the dense limit the mapping to a low-rank matrix
estimation problem provides an approximate message passing algorithm that
solves the problem algorithmically. We identify the regions where the algorithm
efficiently computes the Bayes-optimal estimates. Our analysis refines the
results of [2,3] about optimality of message passing algorithms by
characterizing regions of parameters where these algorithms do not match the
Bayes-optimal performance. We further study numerically the performance of
approximate message passing, derived in the dense limit, on sparse instances
and carry out experiments on a real world dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmidt_C/0/1/0/all/0/1&quot;&gt;Christian Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zdeborova_L/0/1/0/all/0/1&quot;&gt;Lenka Zdeborov&amp;#xe1;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05104">
<title>Bucket Renormalization for Approximate Inference. (arXiv:1803.05104v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05104</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic graphical models are a key tool in machine learning
applications. Computing the partition function, i.e., normalizing constant, is
a fundamental task of statistical inference but it is generally computationally
intractable, leading to extensive study of approximation methods. Iterative
variational methods are a popular and successful family of approaches. However,
even state of the art variational methods can return poor results or fail to
converge on difficult instances. In this paper, we instead consider computing
the partition function via sequential summation over variables. We develop
robust approximate algorithms by combining ideas from mini-bucket elimination
with tensor network and renormalization group methods from statistical physics.
The resulting &quot;convergence-free&quot; methods show good empirical performance on
both synthetic and real-world benchmark models, even for difficult instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ahn_S/0/1/0/all/0/1&quot;&gt;Sungsoo Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chertkov_M/0/1/0/all/0/1&quot;&gt;Michael Chertkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jinwoo Shin&lt;/a&gt;</dc:creator>
</item></rdf:RDF>