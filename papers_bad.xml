<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-09-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01213"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01220"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01266"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01479"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01524"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01575"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01577"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10081"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01133"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01229"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01293"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01353"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01354"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01357"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01434"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01471"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01485"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01605"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01625"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1510.00084"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.00252"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.01551"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.02436"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.09627"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.02893"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03832"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08323"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02081"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01357"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04256"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06296"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09638"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1707.01213">
<title>Data-Driven Sparse Structure Selection for Deep Neural Networks. (arXiv:1707.01213v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01213</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional neural networks have liberated its extraordinary power on
various tasks. However, it is still very challenging to deploy state-of-the-art
models into real-world applications due to their high computational complexity.
How can we design a compact and effective network without massive experiments
and expert knowledge? In this paper, we propose a simple and effective
framework to learn and prune deep models in an end-to-end manner. In our
framework, a new type of parameter -- scaling factor is first introduced to
scale the outputs of specific structures, such as neurons, groups or residual
blocks. Then we add sparsity regularizations on these factors, and solve this
optimization problem by a modified stochastic Accelerated Proximal Gradient
(APG) method. By forcing some of the factors to zero, we can safely remove the
corresponding structures, thus prune the unimportant parts of a CNN. Comparing
with other structure selection methods that may need thousands of trials or
iterative fine-tuning, our method is trained fully end-to-end in one training
pass without bells and whistles. We evaluate our method, Sparse Structure
Selection with several state-of-the-art CNNs, and demonstrate very promising
results with adaptive depth and width selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zehao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Naiyan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01220">
<title>Vulcan: A Monte Carlo Algorithm for Large Chance Constrained MDPs with Risk Bounding Functions. (arXiv:1809.01220v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.01220</link>
<description rdf:parseType="Literal">&lt;p&gt;Chance Constrained Markov Decision Processes maximize reward subject to a
bounded probability of failure, and have been frequently applied for planning
with potentially dangerous outcomes or unknown environments. Solution
algorithms have required strong heuristics or have been limited to relatively
small problems with up to millions of states, because the optimal action to
take from a given state depends on the probability of failure in the rest of
the policy, leading to a coupled problem that is difficult to solve. In this
paper we examine a generalization of a CCMDP that trades off probability of
failure against reward through a functional relationship. We derive a
constraint that can be applied to each state history in a policy individually,
and which guarantees that the chance constraint will be satisfied. The approach
decouples states in the CCMDP, so that large problems can be solved
efficiently. We then introduce Vulcan, which uses our constraint in order to
apply Monte Carlo Tree Search to CCMDPs. Vulcan can be applied to problems
where it is unfeasible to generate the entire state space, and policies must be
returned in an anytime manner. We show that Vulcan and its variants run tens to
hundreds of times faster than linear programming methods, and over ten times
faster than heuristic based methods, all without the need for a heuristic, and
returning solutions with a mean suboptimality on the order of a few percent.
Finally, we use Vulcan to solve for a chance constrained policy in a CCMDP with
over $10^{13}$ states in 3 minutes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayton_B/0/1/0/all/0/1&quot;&gt;Benjamin J Ayton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1&quot;&gt;Brian C Williams&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01266">
<title>Coverage-Guided Fuzzing for Deep Neural Networks. (arXiv:1809.01266v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1809.01266</link>
<description rdf:parseType="Literal">&lt;p&gt;In company with the data explosion over the past decade, deep neural network
(DNN) based software has experienced unprecedented leap and is becoming the key
driving force of many novel industrial applications, including many
safety-critical scenarios such as autonomous driving. Despite great success
achieved in various human intelligence tasks, similar to traditional software,
DNNs could also exhibit incorrect behaviors caused by hidden defects causing
severe accidents and losses. In this paper, we propose an automated fuzz
testing framework for hunting potential defects of general-purpose DNNs. It
performs metamorphic mutation to generate new semantically preserved tests, and
leverages multiple plugable coverage criteria as feedback to guide the test
generation from different perspectives. To be scalable towards practical-sized
DNNs, our framework maintains tests in batch, and prioritizes the tests
selection based on active feedback. The effectiveness of our framework is
extensively investigated on 3 popular datasets (MNIST, CIFAR-10, ImageNet) and
7 DNNs with diverse complexities, under large set of 6 coverage criteria as
feedback. The large-scale experiments demonstrate that our fuzzing framework
can (1) significantly boost the coverage with guidance; (2) generate useful
tests to detect erroneous behaviors and facilitate the DNN model quality
evaluation; (3) accurately capture potential defects during DNN quantization
for platform migration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaofei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1&quot;&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongxu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1&quot;&gt;Minhui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jianjun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianxiong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1&quot;&gt;Simon See&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01479">
<title>UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification. (arXiv:1809.01479v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1809.01479</link>
<description rdf:parseType="Literal">&lt;p&gt;The Fact Extraction and VERification (FEVER) shared task was launched to
support the development of systems able to verify claims by extracting
supporting or refuting facts from raw text. The shared task organizers provide
a large-scale dataset for the consecutive steps involved in claim verification,
in particular, document retrieval, fact extraction, and claim classification.
In this paper, we present our claim verification pipeline approach, which,
according to the preliminary results, scored third in the shared task, out of
23 competing systems. For the document retrieval, we implemented a new entity
linking approach. In order to be able to rank candidate facts and classify a
claim on the basis of several selected facts, we introduce two extensions to
the Enhanced LSTM (ESIM).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanselowski_A/0/1/0/all/0/1&quot;&gt;Andreas Hanselowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zile Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sorokin_D/0/1/0/all/0/1&quot;&gt;Daniil Sorokin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiller_B/0/1/0/all/0/1&quot;&gt;Benjamin Schiller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_C/0/1/0/all/0/1&quot;&gt;Claudia Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1&quot;&gt;Iryna Gurevych&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01524">
<title>Modeling human intuitions about liquid flow with particle-based simulation. (arXiv:1809.01524v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.01524</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans can easily describe, imagine, and, crucially, predict a wide variety
of behaviors of liquids--splashing, squirting, gushing, sloshing, soaking,
dripping, draining, trickling, pooling, and pouring--despite tremendous
variability in their material and dynamical properties. Here we propose and
test a computational model of how people perceive and predict these liquid
dynamics, based on coarse approximate simulations of fluids as collections of
interacting particles. Our model is analogous to a &quot;game engine in the head&quot;,
drawing on techniques for interactive simulations (as in video games) that
optimize for efficiency and natural appearance rather than physical accuracy.
In two behavioral experiments, we found that the model accurately captured
people&apos;s predictions about how liquids flow among complex solid obstacles, and
was significantly better than two alternatives based on simple heuristics and
deep neural networks. Our model was also able to explain how people&apos;s
predictions varied as a function of the liquids&apos; properties (e.g., viscosity
and stickiness). Together, the model and empirical results extend the recent
proposal that human physical scene understanding for the dynamics of rigid,
solid objects can be supported by approximate probabilistic simulation, to the
more complex and unexplored domain of fluid dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bates_C/0/1/0/all/0/1&quot;&gt;Christopher J. Bates&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yildirim_I/0/1/0/all/0/1&quot;&gt;Ilker Yildirim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1&quot;&gt;Peter Battaglia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01560">
<title>Reinforcement Learning under Threats. (arXiv:1809.01560v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01560</link>
<description rdf:parseType="Literal">&lt;p&gt;In several reinforcement learning (RL) scenarios, mainly in security
settings, there may be adversaries trying to interfere with the reward
generating process. In this paper, we introduce Threatened Markov Decision
Processes (TMDPs), which provide a framework to support a decision maker
against a potential adversary in RL. Furthermore, we propose a level-$k$
thinking scheme resulting in a new learning framework to deal with TMDPs. After
introducing our framework and deriving theoretical results, relevant empirical
evidence is given via extensive experiments, showing the benefits of accounting
for adversaries while the agent learns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallego_V/0/1/0/all/0/1&quot;&gt;V&amp;#xed;ctor Gallego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naveiro_R/0/1/0/all/0/1&quot;&gt;Roi Naveiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Insua_D/0/1/0/all/0/1&quot;&gt;David R&amp;#xed;os Insua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01575">
<title>Bounded Rational Decision-Making with Adaptive Neural Network Priors. (arXiv:1809.01575v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.01575</link>
<description rdf:parseType="Literal">&lt;p&gt;Bounded rationality investigates utility-optimizing decision-makers with
limited information-processing power. In particular, information theoretic
bounded rationality models formalize resource constraints abstractly in terms
of relative Shannon information, namely the Kullback-Leibler Divergence between
the agents&apos; prior and posterior policy. Between prior and posterior lies an
anytime deliberation process that can be instantiated by sample-based
evaluations of the utility function through Markov Chain Monte Carlo (MCMC)
optimization. The most simple model assumes a fixed prior and can relate
abstract information-theoretic processing costs to the number of sample
evaluations. However, more advanced models would also address the question of
learning, that is how the prior is adapted over time such that generated prior
proposals become more efficient. In this work we investigate generative neural
networks as priors that are optimized concurrently with anytime sample-based
decision-making processes such as MCMC. We evaluate this approach on toy
examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hihn_H/0/1/0/all/0/1&quot;&gt;Heinke Hihn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottwald_S/0/1/0/all/0/1&quot;&gt;Sebastian Gottwald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_D/0/1/0/all/0/1&quot;&gt;Daniel A. Braun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01577">
<title>From Bayesian Inference to Logical Bayesian Inference: A New Mathematical Frame for Semantic Communication and Machine Learning. (arXiv:1809.01577v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.01577</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian Inference (BI) uses the Bayes&apos; posterior whereas Logical Bayesian
Inference (LBI) uses the truth function or membership function as the inference
tool. LBI was proposed because BI was not compatible with the classical Bayes&apos;
prediction and didn&apos;t use logical probability and hence couldn&apos;t express
semantic meaning. In LBI, statistical probability and logical probability are
strictly distinguished, used at the same time, and linked by the third kind of
Bayes&apos; Theorem. The Shannon channel consists of a set of transition probability
functions whereas the semantic channel consists of a set of truth functions.
When a sample is large enough, we can directly derive the semantic channel from
Shannon&apos;s channel. Otherwise, we can use parameters to construct truth
functions and use the Maximum Semantic Information (MSI) criterion to optimize
the truth functions. The MSI criterion is equivalent to the Maximum Likelihood
(ML) criterion, and compatible with the Regularized Least Square (RLS)
criterion. By matching the two channels one with another, we can obtain the
Channels&apos; Matching (CM) algorithm. This algorithm can improve multi-label
classifications, maximum likelihood estimations (including unseen instance
classifications), and mixture models. In comparison with BI, LBI 1) uses the
prior P(X) of X instead of that of Y or {\theta} and fits cases where the
source P(X) changes, 2) can be used to solve the denotations of labels, and 3)
is more compatible with the classical Bayes&apos; prediction and likelihood method.
LBI also provides a confirmation measure between -1 and 1 for induction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chenguang Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01604">
<title>Merging datasets through deep learning. (arXiv:1809.01604v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01604</link>
<description rdf:parseType="Literal">&lt;p&gt;Merging datasets is a key operation for data analytics. A frequent
requirement for merging is joining across columns that have different surface
forms for the same entity (e.g., the name of a person might be represented as
&quot;Douglas Adams&quot; or &quot;Adams, Douglas&quot;). Similarly, ontology alignment can require
recognizing distinct surface forms of the same entity, especially when
ontologies are independently developed. However, data management systems are
currently limited to performing merges based on string equality, or at best
using string similarity. We propose an approach to performing merges based on
deep learning models. Our approach depends on (a) creating a deep learning
model that maps surface forms of an entity into a set of vectors such that
alternate forms for the same entity are closest in vector space, (b) indexing
these vectors using a nearest neighbors algorithm to find the forms that can be
potentially joined together. To build these models, we had to adapt techniques
from metric learning due to the characteristics of the data; specifically we
describe novel sample selection techniques and loss functions that work for
this problem. To evaluate our approach, we used Wikidata as ground truth and
built models from datasets with approximately 1.1M people&apos;s names (200K
identities) and 130K company names (70K identities). We developed models that
allow for joins with precision@1 of .75-.81 and recall of .74-.81. We make the
models available for aligning people or companies across multiple datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivas_K/0/1/0/all/0/1&quot;&gt;Kavitha Srinivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gale_A/0/1/0/all/0/1&quot;&gt;Abraham Gale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolby_J/0/1/0/all/0/1&quot;&gt;Julian Dolby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10081">
<title>DeepJDOT: Deep Joint Distribution Optimal Transport for Unsupervised Domain Adaptation. (arXiv:1803.10081v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10081</link>
<description rdf:parseType="Literal">&lt;p&gt;In computer vision, one is often confronted with problems of domain shifts,
which occur when one applies a classifier trained on a source dataset to target
data sharing similar characteristics (e.g. same classes), but also different
latent data structures (e.g. different acquisition conditions). In such a
situation, the model will perform poorly on the new data, since the classifier
is specialized to recognize visual cues specific to the source domain. In this
work we explore a solution, named DeepJDOT, to tackle this problem: through a
measure of discrepancy on joint deep representations/labels based on optimal
transport, we not only learn new data representations aligned between the
source and target domain, but also simultaneously preserve the discriminative
information used by the classifier. We applied DeepJDOT to a series of visual
recognition tasks, where it compares favorably against state-of-the-art deep
domain adaptation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1&quot;&gt;Bharath Bhushan Damodaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kellenberger_B/0/1/0/all/0/1&quot;&gt;Benjamin Kellenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Flamary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1&quot;&gt;Devis Tuia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1&quot;&gt;Nicolas Courty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01133">
<title>Automated bird sound recognition in realistic settings. (arXiv:1809.01133v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1809.01133</link>
<description rdf:parseType="Literal">&lt;p&gt;We evaluated the effectiveness of an automated bird sound identification
system in a situation that emulates a realistic, typical application. We
trained classification algorithms on a crowd-sourced collection of bird audio
recording data and restricted our training methods to be completely free of
manual intervention. The approach is hence directly applicable to the analysis
of multiple species collections, with labelling provided by crowd-sourced
collection. We evaluated the performance of the bird sound recognition system
on a realistic number of candidate classes, corresponding to real conditions.
We investigated the use of two canonical classification methods, chosen due to
their widespread use and ease of interpretation, namely a k Nearest Neighbour
(kNN) classifier with histogram-based features and a Support Vector Machine
(SVM) with time-summarisation features. We further investigated the use of a
certainty measure, derived from the output probabilities of the classifiers, to
enhance the interpretability and reliability of the class decisions. Our
results demonstrate that both identification methods achieved similar
performance, but we argue that the use of the kNN classifier offers somewhat
more flexibility. Furthermore, we show that employing an outcome certainty
measure provides a valuable and consistent indicator of the reliability of
classification results. Our use of generic training data and our investigation
of probabilistic classification methodologies that can flexibly address the
variable number of candidate species/classes that are expected to be
encountered in the field, directly contribute to the development of a practical
bird sound identification system with potentially global application. Further,
we show that certainty measures associated with identification outcomes can
significantly contribute to the practical usability of the overall system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadopoulos_T/0/1/0/all/0/1&quot;&gt;Timos Papadopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen J. Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1&quot;&gt;Katherine J. Willis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01225">
<title>Compositional Stochastic Average Gradient for Machine Learning and Related Applications. (arXiv:1809.01225v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01225</link>
<description rdf:parseType="Literal">&lt;p&gt;Many machine learning, statistical inference, and portfolio optimization
problems require minimization of a composition of expected value functions
(CEVF). Of particular interest is the finite-sum versions of such compositional
optimization problems (FS-CEVF). Compositional stochastic variance reduced
gradient (C-SVRG) methods that combine stochastic compositional gradient
descent (SCGD) and stochastic variance reduced gradient descent (SVRG) methods
are the state-of-the-art methods for FS-CEVF problems. We introduce
compositional stochastic average gradient descent (C-SAG) a novel extension of
the stochastic average gradient method (SAG) to minimize composition of
finite-sum functions. C-SAG, like SAG, estimates gradient by incorporating
memory of previous gradient information. We present theoretical analyses of
C-SAG which show that C-SAG, like SAG, and C-SVRG, achieves a linear
convergence rate when the objective function is strongly convex; However, C-CAG
achieves lower oracle query complexity per iteration than C-SVRG. Finally, we
present results of experiments showing that C-SAG converges substantially
faster than full gradient (FG), as well as C-SVRG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_T/0/1/0/all/0/1&quot;&gt;Tsung-Yu Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+EL_Manzalawy_Y/0/1/0/all/0/1&quot;&gt;Yasser EL-Manzalawy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yiwei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honavar_V/0/1/0/all/0/1&quot;&gt;Vasant Honavar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01229">
<title>t-Exponential Memory Networks for Question-Answering Machines. (arXiv:1809.01229v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01229</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep learning have brought to the fore models that can
make multiple computational steps in the service of completing a task; these
are capable of describ- ing long-term dependencies in sequential data. Novel
recurrent attention models over possibly large external memory modules
constitute the core mechanisms that enable these capabilities. Our work
addresses learning subtler and more complex underlying temporal dynamics in
language modeling tasks that deal with sparse sequential data. To this end, we
improve upon these recent advances, by adopting concepts from the field of
Bayesian statistics, namely variational inference. Our proposed approach
consists in treating the network parameters as latent variables with a prior
distribution imposed over them. Our statistical assumptions go beyond the
standard practice of postulating Gaussian priors. Indeed, to allow for handling
outliers, which are prevalent in long observed sequences of multivariate data,
multivariate t-exponential distributions are imposed. On this basis, we proceed
to infer corresponding posteriors; these can be used for inference and
prediction at test time, in a way that accounts for the uncertainty in the
available sparse training data. Specifically, to allow for our approach to best
exploit the merits of the t-exponential family, our method considers a new
t-divergence measure, which generalizes the concept of the Kullback-Leibler
divergence. We perform an extensive experimental evaluation of our approach,
using challenging language modeling benchmarks, and illustrate its superiority
over existing state-of-the-art techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolias_K/0/1/0/all/0/1&quot;&gt;Kyriakos Tolias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatzis_S/0/1/0/all/0/1&quot;&gt;Sotirios Chatzis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01293">
<title>Stochastic Particle-Optimization Sampling and the Non-Asymptotic Convergence Theory. (arXiv:1809.01293v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.01293</link>
<description rdf:parseType="Literal">&lt;p&gt;Particle-optimization sampling (POS) is a recently developed technique to
generate high-quality samples from a target distribution by iteratively
updating a set of interactive particles. A representative algorithm is the
Stein variational gradient descent (SVGD). Though obtaining significant
empirical success, the {\em non-asymptotic} convergence behavior of SVGD
remains unknown. In this paper, we generalize POS to a stochasticity setting by
injecting random noise in particle updates, called stochastic
particle-optimization sampling (SPOS). Standard SVGD can be regarded as a
special case of our framework. Notably, for the first time, we develop
non-asymptotic convergence theory for the SPOS framework (which includes SVGD),
characterizing the bias of a sample approximation w.r.t. the numbers of
particles and iterations under both convex- and noncovex-energy-function
settings. Remarkably, we provide theoretical understand of a pitfall of SVGD
that can be avoided in the proposed SPOS framework, i.e., particles tent to
collapse to a local mode in SVGD under some particular conditions. Our theory
is based on the analysis of nonlinear stochastic differential equations, which
serves as an extension and a complemented development to the asymptotic
convergence theory for SVGD such as [1].
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changyou Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01353">
<title>IKA: Independent Kernel Approximator. (arXiv:1809.01353v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01353</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes a new method for low rank kernel approximation called
IKA. The main advantage of IKA is that it produces a function $\psi(x)$ defined
as a linear combination of arbitrarily chosen functions. In contrast the
approximation produced by Nystr\&quot;om method is a linear combination of kernel
evaluations. The proposed method consistently outperformed Nystr\&quot;om method in
a comparison on the STL-10 dataset. Numerical results are reproducible using
the source code available at https://gitlab.com/matteo-ronchetti/IKA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronchetti_M/0/1/0/all/0/1&quot;&gt;Matteo Ronchetti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01354">
<title>Semantic Human Matting. (arXiv:1809.01354v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.01354</link>
<description rdf:parseType="Literal">&lt;p&gt;Human matting, high quality extraction of humans from natural images, is
crucial for a wide variety of applications. Since the matting problem is
severely under-constrained, most previous methods require user interactions to
take user designated trimaps or scribbles as constraints. This user-in-the-loop
nature makes them difficult to be applied to large scale data or time-sensitive
scenarios. In this paper, instead of using explicit user input constraints, we
employ implicit semantic constraints learned from data and propose an automatic
human matting algorithm (SHM). SHM is the first algorithm that learns to
jointly fit both semantic information and high quality details with deep
networks. In practice, simultaneously learning both coarse semantics and fine
details is challenging. We propose a novel fusion strategy which naturally
gives a probabilistic estimation of the alpha matte. We also construct a very
large dataset with high quality annotations consisting of 35,513 unique
foregrounds to facilitate the learning and evaluation of human matting.
Extensive experiments on this dataset and plenty of real images show that SHM
achieves comparable results with state-of-the-art interactive matting methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Quan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tiezheng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Kun Gai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01357">
<title>Zero Shot Learning for Code Education: Rubric Sampling with Deep Learning Inference. (arXiv:1809.01357v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01357</link>
<description rdf:parseType="Literal">&lt;p&gt;In modern computer science education, massive open online courses (MOOCs) log
thousands of hours of data about how students solve coding challenges. Being so
rich in data, these platforms have garnered the interest of the machine
learning community, with many new algorithms attempting to autonomously provide
feedback to help future students learn. But what about those first hundred
thousand students? In most educational contexts (i.e. classrooms), assignments
do not have enough historical data for supervised learning. In this paper, we
introduce a human-in-the-loop &quot;rubric sampling&quot; approach to tackle the &quot;zero
shot&quot; feedback challenge. We are able to provide autonomous feedback for the
first students working on an introductory programming assignment with accuracy
that substantially outperforms data-hungry algorithms and approaches human
level fidelity. Rubric sampling requires minimal teacher effort, can associate
feedback with specific parts of a student&apos;s solution and can articulate a
student&apos;s misconceptions in the language of the instructor. Deep learning
inference enables rubric sampling to further improve as more assignment
specific student data is acquired. We demonstrate our results on a novel
dataset from Code.org, the world&apos;s largest programming education platform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Mike Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mosse_M/0/1/0/all/0/1&quot;&gt;Milan Mosse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1&quot;&gt;Noah Goodman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piech_C/0/1/0/all/0/1&quot;&gt;Chris Piech&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01434">
<title>Stellar Cluster Detection using GMM with Deep Variational Autoencoder. (arXiv:1809.01434v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01434</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting stellar clusters have always been an important research problem in
Astronomy. Although images do not convey very detailed information in detecting
stellar density enhancements, we attempt to understand if new machine learning
techniques can reveal patterns that would assist in drawing better inferences
from the available image data. This paper describes an unsupervised approach in
detecting star clusters using Deep Variational Autoencoder combined with a
Gaussian Mixture Model. We show that our method works significantly well in
comparison with state-of-the-art detection algorithm in recognizing a variety
of star clusters even in the presence of noise and distortion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karmakar_A/0/1/0/all/0/1&quot;&gt;Arnab Karmakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_D/0/1/0/all/0/1&quot;&gt;Deepak Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tej_A/0/1/0/all/0/1&quot;&gt;Anandmayee Tej&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01471">
<title>Chest X-ray Inpainting with Deep Generative Models. (arXiv:1809.01471v1 [cs.GR])</title>
<link>http://arxiv.org/abs/1809.01471</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks have been successfully applied to inpainting
in natural images. However, the current state-of-the-art models have not yet
been widely adopted in the medical imaging domain. In this paper, we
investigate the performance of three recently published deep learning based
inpainting models: context encoders, semantic image inpainting, and the
contextual attention model, applied to chest x-rays, as the chest exam is the
most commonly performed radiological procedure. We train these generative
models on 1.2M 128 $\times$ 128 patches from 60K healthy x-rays, and learn to
predict the center 64 $\times$ 64 region in each patch. We test the models on
both the healthy and abnormal radiographs. We evaluate the results by visual
inspection and comparing the PSNR scores. The outputs of the models are in most
cases highly realistic. We show that the methods have potential to enhance and
detect abnormalities. In addition, we perform a 2AFC observer study and show
that an experienced human observer performs poorly in detecting inpainted
regions, particularly those generated by the contextual attention model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sogancioglu_E/0/1/0/all/0/1&quot;&gt;Ecem Sogancioglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belli_D/0/1/0/all/0/1&quot;&gt;Davide Belli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ginneken_B/0/1/0/all/0/1&quot;&gt;Bram van Ginneken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01485">
<title>Blind Community Detection from Low-rank Excitations of a Graph Filter. (arXiv:1809.01485v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1809.01485</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers a novel framework to detect communities in a graph from
the observation of signals at its nodes. We model the observed signals as noisy
outputs of an unknown network process -- represented as a graph filter -- that
is excited by a set of low-rank inputs. Rather than learning the precise
parameters of the graph itself, the proposed method retrieves the community
structure directly; Furthermore, as in blind system identification methods, it
does not require knowledge of the system excitation. The paper shows that
communities can be detected by applying spectral clustering to the low-rank
output covariance matrix obtained from the graph signals. The performance
analysis indicates that the community detection accuracy depends on the
spectral properties of the graph filter considered. Furthermore, we show that
the accuracy can be improved via a low-rank matrix decomposition method when
the excitation signals are known. Numerical experiments demonstrate that our
approach is effective for analyzing network data from diffusion, consumers, and
social dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wai_H/0/1/0/all/0/1&quot;&gt;Hoi-To Wai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1&quot;&gt;Santiago Segarra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozdaglar_A/0/1/0/all/0/1&quot;&gt;Asuman E. Ozdaglar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scaglione_A/0/1/0/all/0/1&quot;&gt;Anna Scaglione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jadbabaie_A/0/1/0/all/0/1&quot;&gt;Ali Jadbabaie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01605">
<title>Anomaly Detection in the Presence of Missing Values. (arXiv:1809.01605v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01605</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard methods for anomaly detection assume that all features are observed
at both learning time and prediction time. Such methods cannot process data
containing missing values. This paper studies five strategies for handling
missing values in test queries: (a) mean imputation, (b) MAP imputation, (c)
reduction (reduced-dimension anomaly detectors via feature bagging), (d)
marginalization (for density estimators only), and (e) proportional
distribution (for tree-based methods only). Our analysis suggests that MAP
imputation and proportional distribution should give better results than mean
imputation, reduction, and marginalization. These hypotheses are largely
confirmed by experimental studies on synthetic data and on anomaly detection
benchmark data sets using the Isolation Forest (IF), LODA, and EGMM anomaly
detection algorithms. However, marginalization worked surprisingly well for
EGMM, and there are exceptions where reduction works well on some benchmark
problems. We recommend proportional distribution for IF, MAP imputation for
LODA, and marginalization for EGMM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dietterich_T/0/1/0/all/0/1&quot;&gt;Thomas G. Dietterich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemicheal_T/0/1/0/all/0/1&quot;&gt;Tadesse Zemicheal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01625">
<title>Gene Shaving using influence function of a kernel method. (arXiv:1809.01625v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.01625</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying significant subsets of the genes, gene shaving is an essential
and challenging issue for biomedical research for a huge number of genes and
the complex nature of biological networks,. Since positive definite kernel
based methods on genomic information can improve the prediction of diseases, in
this paper we proposed a new method, &quot;kernel gene shaving (kernel canonical
correlation analysis (kernel CCA) based gene shaving). This problem is
addressed using the influence function of the kernel CCA. To investigate the
performance of the proposed method in a comparison of three popular gene
selection methods (T-test, SAM and LIMMA), we were used extensive simulated and
real microarray gene expression datasets. The performance measures AUC was
computed for each of the methods. The achievement of the proposed method has
improved than the three well-known gene selection methods. In real data
analysis, the proposed method identified a subsets of $210$ genes out of $2000$
genes. The network of these genes has significantly more interactions than
expected, which indicates that they may function in a concerted effort on colon
cancer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Md. Ashad Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shahjama_M/0/1/0/all/0/1&quot;&gt;Mohammad Shahjama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md. Ferdush Rahman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1510.00084">
<title>A Direct Approach for Sparse Quadratic Discriminant Analysis. (arXiv:1510.00084v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1510.00084</link>
<description rdf:parseType="Literal">&lt;p&gt;Quadratic discriminant analysis (QDA) is a standard tool for classification
due to its simplicity and flexibility. Because the number of its parameters
scales quadratically with the number of the variables, QDA is not practical,
however, when the dimensionality is relatively large. To address this, we
propose a novel procedure named DA-QDA for QDA in analyzing high-dimensional
data. Formulated in a simple and coherent framework, DA-QDA aims to directly
estimate the key quantities in the Bayes discriminant function including
quadratic interactions and a linear index of the variables for classification.
Under appropriate sparsity assumptions, we establish consistency results for
estimating the interactions and the linear index, and further demonstrate that
the misclassification rate of our procedure converges to the optimal Bayes
risk, even when the dimensionality is exponentially high with respect to the
sample size. An efficient algorithm based on the alternating direction method
of multipliers (ADMM) is developed for finding interactions, which is much
faster than its competitor in the literature. The promising performance of
DA-QDA is illustrated via extensive simulation studies and the analysis of four
real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Binyan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Leng_C/0/1/0/all/0/1&quot;&gt;Chenlei Leng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.00252">
<title>Fast Rates for General Unbounded Loss Functions: from ERM to Generalized Bayes. (arXiv:1605.00252v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1605.00252</link>
<description rdf:parseType="Literal">&lt;p&gt;We present new excess risk bounds for general unbounded loss functions
including log loss and squared loss, where the distribution of the losses may
be heavy-tailed. The bounds hold for general estimators, but they are optimized
when applied to $\eta$-generalized Bayesian, MDL, and empirical risk
minimization estimators. In the case of log loss, the bounds imply convergence
rates for generalized Bayesian inference under misspecification in terms of a
generalization of the Hellinger metric as long as the learning rate $\eta$ is
set correctly. For general loss functions, our bounds rely on two separate
conditions: the $v$-GRIP (generalized reversed information projection)
conditions, which control the lower tail of the excess loss; and the newly
introduced witness condition, which controls the upper tail. The parameter $v$
in the $v$-GRIP conditions determines the achievable rate and is akin to the
exponent in the Tsybakov margin condition and the Bernstein condition for
bounded losses, which the $v$-GRIP conditions generalize; favorable $v$ in
combination with small model complexity leads to $\tilde{O}(1/n)$ rates. The
witness condition allows us to connect the excess risk to an &apos;annealed&apos; version
thereof, by which we generalize several previous results connecting Hellinger
and R\&apos;enyi divergence to KL divergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grunwald_P/0/1/0/all/0/1&quot;&gt;Peter D. Gr&amp;#xfc;nwald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_N/0/1/0/all/0/1&quot;&gt;Nishant A. Mehta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.01551">
<title>Deep learning in color: towards automated quark/gluon jet discrimination. (arXiv:1612.01551v3 [hep-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1612.01551</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence offers the potential to automate challenging
data-processing tasks in collider physics. To establish its prospects, we
explore to what extent deep learning with convolutional neural networks can
discriminate quark and gluon jets better than observables designed by
physicists. Our approach builds upon the paradigm that a jet can be treated as
an image, with intensity given by the local calorimeter deposits. We supplement
this construction by adding color to the images, with red, green and blue
intensities given by the transverse momentum in charged particles, transverse
momentum in neutral particles, and pixel-level charged particle counts.
Overall, the deep networks match or outperform traditional jet variables. We
also find that, while various simulations produce different quark and gluon
jets, the neural networks are surprisingly insensitive to these differences,
similar to traditional observables. This suggests that the networks can extract
robust physical information from imperfect simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Komiske_P/0/1/0/all/0/1&quot;&gt;Patrick T. Komiske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Metodiev_E/0/1/0/all/0/1&quot;&gt;Eric M. Metodiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Schwartz_M/0/1/0/all/0/1&quot;&gt;Matthew D. Schwartz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.02436">
<title>Nonlinear Information Bottleneck. (arXiv:1705.02436v7 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1705.02436</link>
<description rdf:parseType="Literal">&lt;p&gt;Information bottleneck [IB] is a technique for extracting information in some
`input&apos; random variable that is relevant for predicting some different &apos;output&apos;
random variable. IB works by encoding the input in a compressed &apos;bottleneck
variable&apos; from which the output can then be accurately decoded. IB can be
difficult to compute in practice, and has been mainly developed for two limited
cases: (1) discrete random variables with small state spaces, and (2)
continuous random variables that are jointly Gaussian distributed (in which
case the encoding and decoding maps are linear). We propose a method to perform
IB in more general domains. Our approach can be applied to discrete or
continuous inputs and outputs, and allows for nonlinear encoding and decoding
maps. The method uses a novel upper bound on the IB objective, derived using a
non-parametric estimator of mutual information and a variational approximation.
We show how to implement the method using neural networks and gradient-based
optimization, and demonstrate its performance on the MNIST dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolchinsky_A/0/1/0/all/0/1&quot;&gt;Artemy Kolchinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tracey_B/0/1/0/all/0/1&quot;&gt;Brendan D. Tracey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolpert_D/0/1/0/all/0/1&quot;&gt;David H. Wolpert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.09627">
<title>Deep learning bank distress from news and numerical financial data. (arXiv:1706.09627v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.09627</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we focus our attention on the exploitation of the information
contained in financial news to enhance the performance of a classifier of bank
distress. Such information should be analyzed and inserted into the predictive
model in the most efficient way and this task deals with all the issues related
to text analysis and specifically analysis of news media. Among the different
models proposed for such purpose, we investigate one of the possible deep
learning approaches, based on a doc2vec representation of the textual data, a
kind of neural network able to map the sequential and symbolic text input onto
a reduced latent semantic space. Afterwards, a second supervised neural network
is trained combining news data with standard financial figures to classify
banks whether in distressed or tranquil states, based on a small set of known
distress events. Then the final aim is not only the improvement of the
predictive performance of the classifier but also to assess the importance of
news data in the classification process. Does news data really bring more
useful information not contained in standard financial variables? Our results
seem to confirm such hypothesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cerchiello_P/0/1/0/all/0/1&quot;&gt;Paola Cerchiello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nicola_G/0/1/0/all/0/1&quot;&gt;Giancarlo Nicola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ronnqvist_S/0/1/0/all/0/1&quot;&gt;Samuel Ronnqvist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sarlin_P/0/1/0/all/0/1&quot;&gt;Peter Sarlin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.02893">
<title>Convolutional Dictionary Learning: A Comparative Review and New Algorithms. (arXiv:1709.02893v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.02893</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional sparse representations are a form of sparse representation with
a dictionary that has a structure that is equivalent to convolution with a set
of linear filters. While effective algorithms have recently been developed for
the convolutional sparse coding problem, the corresponding dictionary learning
problem is substantially more challenging. Furthermore, although a number of
different approaches have been proposed, the absence of thorough comparisons
between them makes it difficult to determine which of them represents the
current state of the art. The present work both addresses this deficiency and
proposes some new approaches that outperform existing ones in certain contexts.
A thorough set of performance comparisons indicates a very wide range of
performance differences among the existing and proposed methods, and clearly
identifies those that are the most effective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Cardona_C/0/1/0/all/0/1&quot;&gt;Cristina Garcia-Cardona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wohlberg_B/0/1/0/all/0/1&quot;&gt;Brendt Wohlberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03832">
<title>Quadrature-based features for kernel approximation. (arXiv:1802.03832v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03832</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of improving kernel approximation via randomized
feature maps. These maps arise as Monte Carlo approximation to integral
representations of kernel functions and scale up kernel methods for larger
datasets. Based on an efficient numerical integration technique, we propose a
unifying approach that reinterprets the previous random features methods and
extends to better estimates of the kernel approximation. We derive the
convergence behaviour and conduct an extensive empirical study that supports
our hypothesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munkhoeva_M/0/1/0/all/0/1&quot;&gt;Marina Munkhoeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapushev_Y/0/1/0/all/0/1&quot;&gt;Yermek Kapushev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1&quot;&gt;Evgeny Burnaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1&quot;&gt;Ivan Oseledets&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08323">
<title>Deep learning algorithm for data-driven simulation of noisy dynamical system. (arXiv:1802.08323v2 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08323</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a deep learning model, DE-LSTM, for the simulation of a stochastic
process with an underlying nonlinear dynamics. The deep learning model aims to
approximate the probability density function of a stochastic process via
numerical discretization and the underlying nonlinear dynamics is modeled by
the Long Short-Term Memory (LSTM) network. It is shown that, when the numerical
discretization is used, the function estimation problem can be solved by a
multi-label classification problem. A penalized maximum log likelihood method
is proposed to impose a smoothness condition in the prediction of the
probability distribution. We show that the time evolution of the probability
distribution can be computed by a high-dimensional integration of the
transition probability of the LSTM internal states. A Monte Carlo algorithm to
approximate the high-dimensional integration is outlined. The behavior of
DE-LSTM is thoroughly investigated by using the Ornstein-Uhlenbeck process and
noisy observations of nonlinear dynamical systems; Mackey-Glass time series and
forced Van der Pol oscillator. It is shown that DE-LSTM makes a good prediction
of the probability distribution without assuming any distributional properties
of the stochastic process. For a multiple-step forecast of the Mackey-Glass
time series, the prediction uncertainty, denoted by the 95\% confidence
interval, first grows, then dynamically adjusts following the evolution of the
system, while in the simulation of the forced Van der Pol oscillator, the
prediction uncertainty does not grow in time even for a 3,000-step forecast.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yeo_K/0/1/0/all/0/1&quot;&gt;Kyongmin Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Melnyk_I/0/1/0/all/0/1&quot;&gt;Igor Melnyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02081">
<title>Adaptive Diffusions for Scalable Learning over Graphs. (arXiv:1804.02081v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02081</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion-based classifiers such as those relying on the Personalized
PageRank and the Heat kernel, enjoy remarkable classification accuracy at
modest computational requirements. Their performance however is affected by the
extent to which the chosen diffusion captures a typically unknown label
propagation mechanism, that can be specific to the underlying graph, and
potentially different for each class. The present work introduces a
disciplined, data-efficient approach to learning class-specific diffusion
functions adapted to the underlying network topology. The novel learning
approach leverages the notion of &quot;landing probabilities&quot; of class-specific
random walks, which can be computed efficiently, thereby ensuring scalability
to large graphs. This is supported by rigorous analysis of the properties of
the model as well as the proposed algorithms. Furthermore, a robust version of
the classifier facilitates learning even in noisy environments.
&lt;/p&gt;
&lt;p&gt;Classification tests on real networks demonstrate that adapting the diffusion
function to the given graph and observed labels, significantly improves the
performance over fixed diffusions; reaching -- and many times surpassing -- the
classification accuracy of computationally heavier state-of-the-art competing
methods, that rely on node embeddings and deep neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Berberidis_D/0/1/0/all/0/1&quot;&gt;Dimitris Berberidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nikolakopoulos_A/0/1/0/all/0/1&quot;&gt;Athanasios N. Nikolakopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06834">
<title>Subspace Estimation from Incomplete Observations: A High-Dimensional Analysis. (arXiv:1805.06834v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.06834</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a high-dimensional analysis of three popular algorithms, namely,
Oja&apos;s method, GROUSE and PETRELS, for subspace estimation from streaming and
highly incomplete observations. We show that, with proper time scaling, the
time-varying principal angles between the true subspace and its estimates given
by the algorithms converge weakly to deterministic processes when the ambient
dimension $n$ tends to infinity. Moreover, the limiting processes can be
exactly characterized as the unique solutions of certain ordinary differential
equations (ODEs). A finite sample bound is also given, showing that the rate of
convergence towards such limits is $\mathcal{O}(1/\sqrt{n})$. In addition to
providing asymptotically exact predictions of the dynamic performance of the
algorithms, our high-dimensional analysis yields several insights, including an
asymptotic equivalence between Oja&apos;s method and GROUSE, and a precise scaling
relationship linking the amount of missing data to the signal-to-noise ratio.
By analyzing the solutions of the limiting ODEs, we also establish phase
transition phenomena associated with the steady-state performance of these
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1&quot;&gt;Yonina C. Eldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yue M. Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01357">
<title>A recurrent multi-scale approach to RBG-D Object Recognition. (arXiv:1808.01357v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1808.01357</link>
<description rdf:parseType="Literal">&lt;p&gt;Technological development aims to produce generations of increasingly
efficient robots able to perform complex tasks. This requires considerable
efforts, from the scientific community, to find new algorithms that solve
computer vision problems, such as object recognition. The diffusion of RGB-D
cameras directed the study towards the research of new architectures able to
exploit the RGB and Depth information. The project that is developed in this
thesis concerns the realization of a new end-to-end architecture for the
recognition of RGB-D objects called RCFusion. Our method generates compact and
highly discriminative multi-modal features by combining complementary RGB and
depth information representing different levels of abstraction. We evaluate our
method on standard object recognition datasets, RGB-D Object Dataset and
JHUIT-50. The experiments performed show that our method outperforms the
existing approaches and establishes new state-of-the-art results for both
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Planamente_M/0/1/0/all/0/1&quot;&gt;Mirco Planamente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loghmani_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Loghmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1&quot;&gt;Barbara Caputo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04256">
<title>CT Super-resolution GAN Constrained by the Identical, Residual, and Cycle Learning Ensemble(GAN-CIRCLE). (arXiv:1808.04256v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/1808.04256</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed tomography (CT) is widely used in screening, diagnosis, and
image-guided therapy for both clinical and research purposes. Since CT involves
ionizing radiation, an overarching thrust of related technical research is
development of novel methods enabling ultrahigh quality imaging with fine
structural details while reducing the X-ray radiation. In this paper, we
present a semi-supervised deep learning approach to accurately recover
high-resolution (HR) CT images from low-resolution (LR) counterparts.
Specifically, with the generative adversarial network (GAN) as the building
block, we enforce the cycle-consistency in terms of the Wasserstein distance to
establish a nonlinear end-to-end mapping from noisy LR input images to denoised
and deblurred HR outputs. We also include the joint constraints in the loss
function to facilitate structural preservation. In this deep imaging process,
we incorporate deep convolutional neural network (CNN), residual learning, and
network in network techniques for feature extraction and restoration. In
contrast to the current trend of increasing network depth and complexity to
boost the CT imaging performance, which limit its real-world applications by
imposing considerable computational and memory overheads, we apply a parallel
$1\times1$ CNN to compress the output of the hidden layer and optimize the
number of layers and the number of filters for each convolutional layer.
Quantitative and qualitative evaluations demonstrate that our proposed model is
accurate, efficient and robust for super-resolution (SR) image restoration from
noisy LR input images. In particular, we validate our composite SR networks on
three large-scale CT datasets, and obtain promising results as compared to the
other state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenyu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoliu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shan_H/0/1/0/all/0/1&quot;&gt;Hongming Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ju_S/0/1/0/all/0/1&quot;&gt;Shenghong Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuiyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cong_W/0/1/0/all/0/1&quot;&gt;Wenxiang Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vannier_M/0/1/0/all/0/1&quot;&gt;Michael W. Vannier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saha_P/0/1/0/all/0/1&quot;&gt;Punam K. Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Ge Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06296">
<title>Universal Stagewise Learning for Non-Convex Problems with Convergence on Averaged Solutions. (arXiv:1808.06296v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06296</link>
<description rdf:parseType="Literal">&lt;p&gt;Although stochastic gradient descent (SGD) method and its variants (e.g.,
stochastic momentum methods, AdaGrad) are the choice of algorithms for solving
non-convex problems (especially deep learning), there still remain big gaps
between the theory and the practice with many questions unresolved. For
example, there is still a lack of theories of convergence for SGD and its
variants that use stagewise step size and return an averaged solution in
practice. In addition, theoretical insights of why adaptive step size of
AdaGrad could improve non-adaptive step size of {\sgd} is still missing for
non-convex optimization. This paper aims to address these questions and fill
the gap between theory and practice. We propose a universal stagewise
optimization framework for a broad family of {\bf non-smooth non-convex}
(namely weakly convex) problems with the following key features: (i) at each
stage any suitable stochastic convex optimization algorithms (e.g., SGD or
AdaGrad) that return an averaged solution can be employed for minimizing a
regularized convex problem; (ii) the step size is decreased in a stagewise
manner; (iii) an averaged solution is returned as the final solution that is
selected from all stagewise averaged solutions with sampling probabilities {\it
increasing} as the stage number. Our theoretical results of stagewise AdaGrad
exhibit its adaptive convergence, therefore shed insights on its faster
convergence for problems with sparse stochastic gradients than stagewise SGD.
To the best of our knowledge, these new results are the first of their kind for
addressing the unresolved issues of existing theories mentioned earlier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bowen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09638">
<title>Replay attack spoofing detection system using replay noise by multi-task learning. (arXiv:1808.09638v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/1808.09638</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a spoofing detection system for replay attack using
replay noise. In many previous studies across various domains, noise has been
reduced. However, in replay attack, we hypothesize that noise is the prominent
feature which is different with original signal and it can be one of the keys
to find whether a signal has been spoofed. We define the noise that is caused
by the replay attack as replay noise. Specifically, the noise of playback
devices, recording environments, and recording devices, is included in the
replay noise. We explore the effectiveness of training a deep neural network
simultaneously for replay attack spoofing detection and replay noise
classification. Multi-task learning was exploited to embed spoofing detection
and replay noise classification in the code layer. The experiment results on
the ASVspoof2017 datasets demonstrate that the performance of our proposed
system is relatively improved 30% on the evaluation set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shim_H/0/1/0/all/0/1&quot;&gt;Hye-Jin Shim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Jee-weon Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heo_H/0/1/0/all/0/1&quot;&gt;Hee-Soo Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sunghyun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Ha-Jin Yu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>