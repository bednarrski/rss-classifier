<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-31T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11669"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08989"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11583"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11615"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11622"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11761"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.01307"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00420"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03654"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02884"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00468"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11620"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11632"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11648"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11679"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11694"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11697"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11718"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11805"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11809"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11824"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11836"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11876"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11880"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.03774"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.08263"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.02329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.03269"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10733"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04153"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.01610"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03752"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08898"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03234"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01050"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02612"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03121"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04458"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08010"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06007"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07754"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08197"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.11669">
<title>Optimization by Pairwise Linkage Detection, Incremental Linkage Set, and Restricted / Back Mixing: DSMGA-II. (arXiv:1807.11669v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.11669</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a new evolutionary algorithm, called DSMGA-II, to
efficiently solve optimization problems via exploiting problem substructures.
The proposed algorithm adopts pairwise linkage detection and stores the
information in the form of dependency structure matrix (DSM). A new linkage
model, called the incremental linkage set, is then constructed by using the
DSM. Inspired by the idea of optimal mixing, the restricted mixing and the back
mixing are proposed. The former aims at efficient exploration under certain
constrains. The latter aims at exploitation by refining the DSM so as to reduce
unnecessary evaluations. Experimental results show that DSMGA-II outperforms
LT-GOMEA and hBOA in terms of number of function evaluations on the
concatenated/folded/cyclic trap problems, NK-landscape problems with various
degrees of overlapping, 2D Ising spin-glass problems, and MAX-SAT. The
investigation of performance comparison with P3 is also included.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_S/0/1/0/all/0/1&quot;&gt;Shih-Huan Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tian-Li Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08989">
<title>Enhancing Gaussian Estimation of Distribution Algorithm by Exploiting Evolution Direction with Archive. (arXiv:1802.08989v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08989</link>
<description rdf:parseType="Literal">&lt;p&gt;As a typical model-based evolutionary algorithm (EA), estimation of
distribution algorithm (EDA) possesses unique characteristics and has been
widely applied to global optimization. However, the common-used Gaussian EDA
(GEDA) usually suffers from premature convergence which severely limits its
search efficiency. This study first systematically analyses the reasons for the
deficiency of the traditional GEDA, then tries to enhance its performance by
exploiting its evolution direction, and finally develops a new GEDA variant
named EDA2. Instead of only utilizing some good solutions produced in the
current generation when estimating the Gaussian model, EDA2 preserves a certain
number of high-quality solutions generated in previous generations into an
archive and takes advantage of these historical solutions to assist estimating
the covariance matrix of Gaussian model. By this means, the evolution direction
information hidden in the archive is naturally integrated into the estimated
model which in turn can guide EDA2 towards more promising solution regions.
Moreover, the new estimation method significantly reduces the population size
of EDA2 since it needs fewer individuals in the current population for model
estimation. As a result, a fast convergence can be achieved. To verify the
efficiency of EDA2, we tested it on a variety of benchmark functions and
compared it with several state-of-the-art EAs, including IPOP-CMAES, AMaLGaM,
three high-powered DE algorithms, and a new PSO algorithm. The experimental
results demonstrate that EDA2 is efficient and competitive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yongsheng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhigang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1&quot;&gt;Xianghua Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zuren Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;An Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11583">
<title>Testing the Efficient Network TRaining (ENTR) Hypothesis: initially reducing training image size makes Convolutional Neural Network training for image recognition tasks more efficient. (arXiv:1807.11583v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.11583</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNN) for image recognition tasks are seeing
rapid advances in the available architectures and how networks are trained
based on large computational infrastructure and standard datasets with millions
of images. In contrast, performance and time constraints for example, of small
devices and free cloud GPUs necessitate efficient network training (i.e.,
highest accuracy in the shortest inference time possible), often on small
datasets. Here, we hypothesize that initially decreasing image size during
training makes the training process more efficient, because pre-shaping weights
with small images and later utilizing these weights with larger images reduces
initial network parameters and total inference time. We test this Efficient
Network TRaining (ENTR) Hypothesis by training pre-trained Residual Network
(ResNet) models (ResNet18, 34, &amp;amp; 50) on three small datasets (steel
microstructures, bee images, and geographic aerial images) with a free cloud
GPU. Based on three training regimes of i) not, ii) gradually or iii) in one
step increasing image size over the training process, we show that initially
reducing image size increases training efficiency consistently across datasets
and networks. We interpret these results mechanistically in the framework of
regularization theory. Support for the ENTR hypothesis is an important
contribution, because network efficiency improvements for image recognition
tasks are needed for practical applications. In the future, it will be exciting
to see how the ENTR hypothesis holds for large standard datasets like ImageNet
or CIFAR, to better understand the underlying mechanisms, and how these results
compare to other fields such as structural learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wanger_T/0/1/0/all/0/1&quot;&gt;Thomas Cherico Wanger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frohn_P/0/1/0/all/0/1&quot;&gt;Peter Frohn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11615">
<title>Semantic DMN: Formalizing and Reasoning About Decisions in the Presence of Background Knowledg. (arXiv:1807.11615v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.11615</link>
<description rdf:parseType="Literal">&lt;p&gt;The Decision Model and Notation (DMN) is a recent OMG standard for the
elicitation and representation of decision models, and for managing their
interconnection with business processes. DMN builds on the notion of decision
table, and their combination into more complex decision requirements graphs
(DRGs), which bridge between business process models and decision logic models.
DRGs may rely on additional, external business knowledge models, whose
functioning is not part of the standard. In this work, we consider one of the
most important types of business knowledge, namely background knowledge that
conceptually accounts for the structural aspects of the domain of interest, and
propose decision requirement knowledge bases (DKBs), where DRGs are modeled in
DMN, and domain knowledge is captured by means of first-order logic with
datatypes. We provide a logic-based semantics for such an integration, and
formalize different DMN reasoning tasks for DKBs. We then consider background
knowledge formulated as a description logic ontology with datatypes, and show
how the main verification tasks for DMN in this enriched setting, can be
formalized as standard DL reasoning services, and actually carried out in
ExpTime. We discuss the effectiveness of our framework on a case study in
maritime security. This work is under consideration in Theory and Practice of
Logic Programming (TPLP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calvanese_D/0/1/0/all/0/1&quot;&gt;Diego Calvanese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_M/0/1/0/all/0/1&quot;&gt;Marlon Dumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Maria Maggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montali_M/0/1/0/all/0/1&quot;&gt;Marco Montali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11622">
<title>Count-Based Exploration with the Successor Representation. (arXiv:1807.11622v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11622</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of exploration in reinforcement learning is well-understood in
the tabular case and many sample-efficient algorithms are known. Nevertheless,
it is often unclear how the algorithms in the tabular setting can be extended
to tasks with large state-spaces where generalization is required. Recent
promising developments generally depend on problem-specific density models or
handcrafted features. In this paper we introduce a simple approach for
exploration that allows us to develop theoretically justified algorithms in the
tabular case but that also give us intuitions for new algorithms applicable to
settings where function approximation is required. Our approach and its
underlying theory is based on the substochastic successor representation, a
concept we develop here. While the traditional successor representation is a
representation that defines state generalization by the similarity of successor
states, the substochastic successor representation is also able to implicitly
count the number of times each state (or feature) has been observed. This
extension connects two until now disjoint areas of research. We show in
traditional tabular domains (RiverSwim and SixArms) that our algorithm
empirically performs as well as other sample-efficient algorithms. We then
describe a deep reinforcement learning algorithm inspired by these ideas and
show that it matches the performance of recent pseudo-count-based methods in
hard exploration Atari 2600 games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1&quot;&gt;Marlos C. Machado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellemare_M/0/1/0/all/0/1&quot;&gt;Marc G. Bellemare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowling_M/0/1/0/all/0/1&quot;&gt;Michael Bowling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11761">
<title>A First Experiment on Including Text Literals in KGloVe. (arXiv:1807.11761v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.11761</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph embedding models produce embedding vectors for entities and relations
in Knowledge Graphs, often without taking literal properties into account. We
show an initial idea based on the combination of global graph structure with
additional information provided by textual information in properties. Our
initial experiment shows that this approach might be useful, but does not
clearly outperform earlier approaches when evaluated on machine learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1&quot;&gt;Michael Cochez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garofalo_M/0/1/0/all/0/1&quot;&gt;Martina Garofalo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenssen_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xf4;me Len&amp;#xdf;en&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrino_M/0/1/0/all/0/1&quot;&gt;Maria Angela Pellegrino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.01307">
<title>Scene Grammars, Factor Graphs, and Belief Propagation. (arXiv:1606.01307v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1606.01307</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a general framework for probabilistic modeling of complex scenes
and inference from ambiguous observations. The approach is motivated by
applications in image analysis and is based on the use of priors defined by
stochastic grammars. We define a class of grammars that capture relationships
between the objects in a scene and provide important contextual cues for
statistical inference. The distribution over scenes defined by a probabilistic
scene grammar can be represented by a graphical model and this construction can
be used for efficient inference with loopy belief propagation.
&lt;/p&gt;
&lt;p&gt;We show experimental results with two different applications. One application
involves the reconstruction of binary contour maps. Another application
involves detecting and localizing faces in images. In both applications the
same framework leads to robust inference algorithms that can effectively
combine local information to reason about a scene.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_J/0/1/0/all/0/1&quot;&gt;Jeroen Chua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felzenszwalb_P/0/1/0/all/0/1&quot;&gt;Pedro F. Felzenszwalb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00420">
<title>Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. (arXiv:1802.00420v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.00420</link>
<description rdf:parseType="Literal">&lt;p&gt;We identify obfuscated gradients, a kind of gradient masking, as a phenomenon
that leads to a false sense of security in defenses against adversarial
examples. While defenses that cause obfuscated gradients appear to defeat
iterative optimization-based attacks, we find defenses relying on this effect
can be circumvented. We describe characteristic behaviors of defenses
exhibiting the effect, and for each of the three types of obfuscated gradients
we discover, we develop attack techniques to overcome it. In a case study,
examining non-certified white-box-secure defenses at ICLR 2018, we find
obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on
obfuscated gradients. Our new attacks successfully circumvent 6 completely, and
1 partially, in the original threat model each paper considers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athalye_A/0/1/0/all/0/1&quot;&gt;Anish Athalye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1&quot;&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1&quot;&gt;David Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03654">
<title>Beyond the One Step Greedy Approach in Reinforcement Learning. (arXiv:1802.03654v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03654</link>
<description rdf:parseType="Literal">&lt;p&gt;The famous Policy Iteration algorithm alternates between policy improvement
and policy evaluation. Implementations of this algorithm with several variants
of the latter evaluation stage, e.g, $n$-step and trace-based returns, have
been analyzed in previous works. However, the case of multiple-step lookahead
policy improvement, despite the recent increase in empirical evidence of its
strength, has to our knowledge not been carefully analyzed yet. In this work,
we introduce the first such analysis. Namely, we formulate variants of
multiple-step policy improvement, derive new algorithms using these definitions
and prove their convergence. Moreover, we show that recent prominent
Reinforcement Learning algorithms are, in fact, instances of our framework. We
thus shed light on their empirical success and give a recipe for deriving new
algorithms for future study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1&quot;&gt;Yonathan Efroni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalal_G/0/1/0/all/0/1&quot;&gt;Gal Dalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherrer_B/0/1/0/all/0/1&quot;&gt;Bruno Scherrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1&quot;&gt;Shie Mannor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02884">
<title>Synthesis in pMDPs: A Tale of 1001 Parameters. (arXiv:1803.02884v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02884</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers parametric Markov decision processes (pMDPs) whose
transitions are equipped with affine functions over a finite set of parameters.
The synthesis problem is to find a parameter valuation such that the
instantiated pMDP satisfies a specification under all strategies. We show that
this problem can be formulated as a quadratically-constrained quadratic program
(QCQP) and is non-convex in general. To deal with the NP-hardness of such
problems, we exploit a convex-concave procedure (CCP) to iteratively obtain
local optima. An appropriate interplay between CCP solvers and probabilistic
model checkers creates a procedure --- realized in the open-source tool
PROPhESY --- that solves the synthesis problem for models with thousands of
parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cubuktepe_M/0/1/0/all/0/1&quot;&gt;Murat Cubuktepe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jansen_N/0/1/0/all/0/1&quot;&gt;Nils Jansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junges_S/0/1/0/all/0/1&quot;&gt;Sebastian Junges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katoen_J/0/1/0/all/0/1&quot;&gt;Joost-Pieter Katoen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1&quot;&gt;Ufuk Topcu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00468">
<title>Automated Directed Fairness Testing. (arXiv:1807.00468v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00468</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness is a critical trait in decision making. As machine-learning models
are increasingly being used in sensitive application domains (e.g. education
and employment) for decision making, it is crucial that the decisions computed
by such models are free of unintended bias. But how can we automatically
validate the fairness of arbitrary machine-learning models? For a given
machine-learning model and a set of sensitive input parameters, our AEQUITAS
approach automatically discovers discriminatory inputs that highlight fairness
violation. At the core of AEQUITAS are three novel strategies to employ
probabilistic search over the input space with the objective of uncovering
fairness violation. Our AEQUITAS approach leverages inherent robustness
property in common machine-learning models to design and implement scalable
test generation methodologies. An appealing feature of our generated test
inputs is that they can be systematically added to the training set of the
underlying model and improve its fairness. To this end, we design a fully
automated module that guarantees to improve the fairness of the underlying
model.
&lt;/p&gt;
&lt;p&gt;We implemented AEQUITAS and we have evaluated it on six state-of-the-art
classifiers, including a classifier that was designed with fairness
constraints. We show that AEQUITAS effectively generates inputs to uncover
fairness violation in all the subject classifiers and systematically improves
the fairness of the respective models using the generated test inputs. In our
evaluation, AEQUITAS generates up to 70% discriminatory inputs (w.r.t. the
total number of inputs generated) and leverages these inputs to improve the
fairness up to 94%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udeshi_S/0/1/0/all/0/1&quot;&gt;Sakshi Udeshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1&quot;&gt;Pryanshu Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1&quot;&gt;Sudipta Chattopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11560">
<title>Efficient Gauss-Newton-Krylov momentum conservation constrained PDE-LDDMM using the band-limited vector field parameterization. (arXiv:1807.11560v1 [math.OC])</title>
<link>http://arxiv.org/abs/1807.11560</link>
<description rdf:parseType="Literal">&lt;p&gt;The class of non-rigid registration methods proposed in the framework of
PDE-constrained Large Deformation Diffeomorphic Metric Mapping is a
particularly interesting family of physically meaningful diffeomorphic
registration methods. PDE-constrained LDDMM methods are formulated as
constrained variational problems, where the different physical models are
imposed using the associated partial differential equations as hard
constraints. Inexact Newton-Krylov optimization has shown an excellent
numerical accuracy and an extraordinarily fast convergence rate in this
framework. However, the Galerkin representation of the non-stationary velocity
fields does not provide proper geodesic paths. In a previous work, we proposed
a method for PDE-constrained LDDMM parameterized in the space of initial
velocity fields under the EPDiff equation. The proposed method provided
geodesics in the framework of PDE-constrained LDDMM, and it showed performance
competitive to benchmark PDE-constrained LDDMM and EPDiff-LDDMM methods.
However, the major drawback of this method was the large memory load inherent
to PDE-constrained LDDMM methods and the increased computational time with
respect to the benchmark methods. In this work we optimize the computational
complexity of the method using the band-limited vector field parameterization
closing the loop with our previous works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hernandez_M/0/1/0/all/0/1&quot;&gt;Monica Hernandez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11620">
<title>K-medoids Clustering of Data Sequences with Composite Distributions. (arXiv:1807.11620v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11620</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies clustering of data sequences using the k-medoids
algorithm. All the data sequences are assumed to be generated from
\emph{unknown} continuous distributions, which form clusters with each cluster
containing a composite set of closely located distributions (based on a certain
distance metric between distributions). The maximum intra-cluster distance is
assumed to be smaller than the minimum inter-cluster distance, and both values
are assumed to be known. The goal is to group the data sequences together if
their underlying generative distributions (which are unknown) belong to one
cluster. Distribution distance metrics based k-medoids algorithms are proposed
for known and unknown number of distribution clusters. Upper bounds on the
error probability and convergence results in the large sample regime are also
provided. It is shown that the error probability decays exponentially fast as
the number of samples in each data sequence goes to infinity. The error
exponent has a simple form regardless of the distance metric applied when
certain conditions are satisfied. In particular, the error exponent is
characterized when either the Kolmogrov-Smirnov distance or the maximum mean
discrepancy are used as the distance metric. Simulation results are provided to
validate the analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tiexing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qunwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucci_D/0/1/0/all/0/1&quot;&gt;Donald J. Bucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingbin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Biao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varshney_P/0/1/0/all/0/1&quot;&gt;Pramod K. Varshney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11632">
<title>Scaling and bias codes for modeling speaker-adaptive DNN-based speech synthesis systems. (arXiv:1807.11632v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1807.11632</link>
<description rdf:parseType="Literal">&lt;p&gt;Most neural-network based speaker-adaptive acoustic models for speech
synthesis can be categorized into either layer-based or input-code approaches.
Although both approaches have their own pros and cons, most existing works on
speaker adaptation focus on improving one or the other. In this paper, after we
first systematically overview the common principles of neural-network based
speaker-adaptive models, we show that these approaches can be represented in a
unified framework and can be generalized further. More specifically, we
introduce the use of scaling and bias codes as generalized means for
speaker-adaptive transformation. By utilizing these codes, we can create a more
efficient factorized speaker-adaptive model and capture advantages of both
approaches while reducing their disadvantages. The experiments show that the
proposed method can improve the performance of speaker adaptation compared with
speaker adaptation based on the conventional input code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luong_H/0/1/0/all/0/1&quot;&gt;Hieu-Thi Luong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11648">
<title>Composable Core-sets for Determinant Maximization Problems via Spectral Spanners. (arXiv:1807.11648v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1807.11648</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a spectral generalization of classical combinatorial graph spanners
to the spectral setting. Given a set of vectors $V\subseteq \Re^d$, we say a
set $U\subseteq V$ is an $\alpha$-spectral spanner if for all $v\in V$ there is
a probability distribution $\mu_v$ supported on $U$ such that $$vv^\intercal
\preceq \alpha\cdot\mathbb{E}_{u\sim\mu_v} uu^\intercal.$$ We show that any set
$V$ has an $\tilde{O}(d)$-spectral spanner of size $\tilde{O}(d)$ and this
bound is almost optimal in the worst case.
&lt;/p&gt;
&lt;p&gt;We use spectral spanners to study composable core-sets for spectral problems.
We show that for many objective functions one can use a spectral spanner,
independent of the underlying functions, as a core-set and obtain almost
optimal composable core-sets. For example, for the determinant maximization
problem we obtain an $\tilde{O}(k)^k$-composable core-set and we show that this
is almost optimal in the worst case.
&lt;/p&gt;
&lt;p&gt;Our algorithm is a spectral analogue of the classical greedy algorithm for
finding (combinatorial) spanners in graphs. We expect that our spanners find
many other applications in distributed or parallel models of computation. Our
proof is spectral. As a side result of our techniques, we show that the rank of
diagonally dominant lower-triangular matrices are robust under `small
perturbations&apos; which could be of independent interests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1&quot;&gt;Piotr Indyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahabadi_S/0/1/0/all/0/1&quot;&gt;Sepideh Mahabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gharan_S/0/1/0/all/0/1&quot;&gt;Shayan Oveis Gharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezaei_A/0/1/0/all/0/1&quot;&gt;Alireza Rezaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11679">
<title>Wasserstein GAN and Waveform Loss-based Acoustic Model Training for Multi-speaker Text-to-Speech Synthesis Systems Using a WaveNet Vocoder. (arXiv:1807.11679v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1807.11679</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent neural networks such as WaveNet and sampleRNN that learn directly from
speech waveform samples have achieved very high-quality synthetic speech in
terms of both naturalness and speaker similarity even in multi-speaker
text-to-speech synthesis systems. Such neural networks are being used as an
alternative to vocoders and hence they are often called neural vocoders. The
neural vocoder uses acoustic features as local condition parameters, and these
parameters need to be accurately predicted by another acoustic model. However,
it is not yet clear how to train this acoustic model, which is problematic
because the final quality of synthetic speech is significantly affected by the
performance of the acoustic model. Significant degradation happens, especially
when predicted acoustic features have mismatched characteristics compared to
natural ones. In order to reduce the mismatched characteristics between natural
and generated acoustic features, we propose frameworks that incorporate either
a conditional generative adversarial network (GAN) or its variant, Wasserstein
GAN with gradient penalty (WGAN-GP), into multi-speaker speech synthesis that
uses the WaveNet vocoder. We also extend the GAN frameworks and use the
discretized mixture logistic loss of a well-trained WaveNet in addition to mean
squared error and adversarial losses as parts of objective functions.
Experimental results show that acoustic models trained using the WGAN-GP
framework using back-propagated discretized-mixture-of-logistics (DML) loss
achieves the highest subjective evaluation scores in terms of both quality and
speaker similarity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Takaki_S/0/1/0/all/0/1&quot;&gt;Shinji Takaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luong_H/0/1/0/all/0/1&quot;&gt;Hieu-Thi Luong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saito_D/0/1/0/all/0/1&quot;&gt;Daisuke Saito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Minematsu_N/0/1/0/all/0/1&quot;&gt;Nobuaki Minematsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11694">
<title>Spectrum concentration in deep residual learning: a free probability appproach. (arXiv:1807.11694v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11694</link>
<description rdf:parseType="Literal">&lt;p&gt;We revisit the initialization of deep residual networks (ResNets) by
introducing a novel analytical tool in free probability to the community of
deep learning. This tool deals with non-Hermitian random matrices, rather than
their conventional Hermitian counterparts in the literature. As a consequence,
this new tool enables us to evaluate the singular value spectrum of the
input-output Jacobian of a fully- connected deep ResNet for both linear and
nonlinear cases. With the powerful tool of free probability, we conduct an
asymptotic analysis of the spectrum on the single-layer case, and then extend
this analysis to the multi-layer case of an arbitrary number of layers. In
particular, we propose to rescale the classical random initialization by the
number of residual units, so that the spectrum has the order of $O(1)$, when
compared with the large width and depth of the network. We empirically
demonstrate that the proposed initialization scheme learns at a speed of orders
of magnitudes faster than the classical ones, and thus attests a strong
practical relevance of this investigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zenan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1&quot;&gt;Robert C. Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11697">
<title>Multimodal Deep Domain Adaptation. (arXiv:1807.11697v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11697</link>
<description rdf:parseType="Literal">&lt;p&gt;Typically a classifier trained on a given dataset (source domain) does not
performs well if it is tested on data acquired in a different setting (target
domain). This is the problem that domain adaptation (DA) tries to overcome and,
while it is a well explored topic in computer vision, it is largely ignored in
robotic vision where usually visual classification methods are trained and
tested in the same domain. Robots should be able to deal with unknown
environments, recognize objects and use them in the correct way, so it is
important to explore the domain adaptation scenario also in this context. The
goal of the project is to define a benchmark and a protocol for multi-modal
domain adaptation that is valuable for the robot vision community. With this
purpose some of the state-of-the-art DA methods are selected: Deep Adaptation
Network (DAN), Domain Adversarial Training of Neural Network (DANN), Automatic
Domain Alignment Layers (AutoDIAL) and Adversarial Discriminative Domain
Adaptation (ADDA). Evaluations have been done using different data types: RGB
only, depth only and RGB-D over the following datasets, designed for the
robotic community: RGB-D Object Dataset (ROD), Web Object Dataset (WOD),
Autonomous Robot Indoor Dataset (ARID), Big Berkeley Instance Recognition
Dataset (BigBIRD) and Active Vision Dataset. Although progresses have been made
on the formulation of effective adaptation algorithms and more realistic object
datasets are available, the results obtained show that, training a sufficiently
good object classifier, especially in the domain adaptation scenario, is still
an unsolved problem. Also the best way to combine depth with RGB informations
to improve the performance is a point that needs to be investigated more.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucci_S/0/1/0/all/0/1&quot;&gt;Silvia Bucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loghmani_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Loghmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1&quot;&gt;Barbara Caputo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11718">
<title>Using Feature Grouping as a Stochastic Regularizer for High-Dimensional Noisy Data. (arXiv:1807.11718v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11718</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of complex models --with many parameters-- is challenging with
high-dimensional small-sample problems: indeed, they face rapid overfitting.
Such situations are common when data collection is expensive, as in
neuroscience, biology, or geology. Dedicated regularization can be crafted to
tame overfit, typically via structured penalties. But rich penalties require
mathematical expertise and entail large computational costs. Stochastic
regularizers such as dropout are easier to implement: they prevent overfitting
by random perturbations. Used inside a stochastic optimizer, they come with
little additional cost. We propose a structured stochastic regularization that
relies on feature grouping. Using a fast clustering algorithm, we define a
family of groups of features that capture feature covariations. We then
randomly select these groups inside a stochastic gradient descent loop. This
procedure acts as a structured regularizer for high-dimensional correlated data
without additional computational cost and it has a denoising effect. We
demonstrate the performance of our approach for logistic regression both on a
sample-limited face image dataset with varying additive noise and on a typical
high-dimensional learning problem, brain image classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aydore_S/0/1/0/all/0/1&quot;&gt;Sergul Aydore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thirion_B/0/1/0/all/0/1&quot;&gt;Bertrand Thirion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grisel_O/0/1/0/all/0/1&quot;&gt;Olivier Grisel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varoquaux_G/0/1/0/all/0/1&quot;&gt;Gael Varoquaux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11805">
<title>Disaster Monitoring using Unmanned Aerial Vehicles and Deep Learning. (arXiv:1807.11805v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11805</link>
<description rdf:parseType="Literal">&lt;p&gt;Monitoring of disasters is crucial for mitigating their effects on the
environment and human population, and can be facilitated by the use of unmanned
aerial vehicles (UAV), equipped with camera sensors that produce aerial photos
of the areas of interest. A modern technique for recognition of events based on
aerial photos is deep learning. In this paper, we present the state of the art
work related to the use of deep learning techniques for disaster
identification. We demonstrate the potential of this technique in identifying
disasters with high accuracy, by means of a relatively simple deep learning
model. Based on a dataset of 544 images (containing disaster images such as
fires, earthquakes, collapsed buildings, tsunami and flooding, as well as
non-disaster scenes), our results show an accuracy of 91% achieved, indicating
that deep learning, combined with UAV equipped with camera sensors, have the
potential to predict disasters with high accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamilaris_A/0/1/0/all/0/1&quot;&gt;Andreas Kamilaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prenafeta_Boldu_F/0/1/0/all/0/1&quot;&gt;Francesc X. Prenafeta-Bold&amp;#xfa;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11809">
<title>Deep learning in agriculture: A survey. (arXiv:1807.11809v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11809</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning constitutes a recent, modern technique for image processing and
data analysis, with promising results and large potential. As deep learning has
been successfully applied in various domains, it has recently entered also the
domain of agriculture. In this paper, we perform a survey of 40 research
efforts that employ deep learning techniques, applied to various agricultural
and food production challenges. We examine the particular agricultural problems
under study, the specific models and frameworks employed, the sources, nature
and pre-processing of data used, and the overall performance achieved according
to the metrics used at each work under study. Moreover, we study comparisons of
deep learning with other existing popular techniques, in respect to differences
in classification or regression performance. Our findings indicate that deep
learning provides high accuracy, outperforming existing commonly used image
processing techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamilaris_A/0/1/0/all/0/1&quot;&gt;Andreas Kamilaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prenafeta_Boldu_F/0/1/0/all/0/1&quot;&gt;Francesc X. Prenafeta-Boldu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11824">
<title>t-SNE-CUDA: GPU-Accelerated t-SNE and its Applications to Modern Data. (arXiv:1807.11824v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11824</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern datasets and models are notoriously difficult to explore and analyze
due to their inherent high dimensionality and massive numbers of samples.
Existing visualization methods which employ dimensionality reduction to two or
three dimensions are often inefficient and/or ineffective for these datasets.
This paper introduces t-SNE-CUDA, a GPU-accelerated implementation of
t-distributed Symmetric Neighbor Embedding (t-SNE) for visualizing datasets and
models. t-SNE-CUDA significantly outperforms current implementations with
50-700x speedups on the CIFAR-10 and MNIST datasets. These speedups enable, for
the first time, visualization of the neural network activations on the entire
ImageNet dataset - a feat that was previously computationally intractable. We
also demonstrate visualization performance in the NLP domain by visualizing the
GloVe embedding vectors. From these visualizations, we can draw interesting
conclusions about using the L2 metric in these embedding spaces. t-SNE-CUDA is
publicly available athttps://github.com/CannyLab/tsne-cuda
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1&quot;&gt;David M. Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1&quot;&gt;Roshan Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Forrest Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1&quot;&gt;John F. Canny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11836">
<title>Inferring the ground truth through crowdsourcing. (arXiv:1807.11836v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11836</link>
<description rdf:parseType="Literal">&lt;p&gt;Universally valid ground truth is almost impossible to obtain or would come
at a very high cost. For supervised learning without universally valid ground
truth, a recommended approach is applying crowdsourcing: Gathering a large data
set annotated by multiple individuals of varying possibly expertise levels and
inferring the ground truth data to be used as labels to train the classifier.
Nevertheless, due to the sensitivity of the problem at hand (e.g. mitosis
detection in breast cancer histology images), the obtained data needs
verification and proper assessment before being used for classifier training.
Even in the context of organic computing systems, an indisputable ground truth
might not always exist. Therefore, it should be inferred through the
aggregation and verification of the local knowledge of each autonomous agent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Char_J/0/1/0/all/0/1&quot;&gt;Jean Pierre Char&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11876">
<title>Predicting Solution Summaries to Integer Linear Programs under Imperfect Information with Machine Learning. (arXiv:1807.11876v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11876</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper provides a methodological contribution at the intersection of
machine learning and operations research. Namely, we propose a methodology to
quickly predict solution summaries (i.e., solution descriptions at a given
level of detail) to discrete stochastic optimization problems. We approximate
the solutions based on supervised learning and the training dataset consists of
a large number of deterministic problems that have been solved independently
and offline. Uncertainty regarding a missing subset of the inputs is addressed
through sampling and aggregation methods.
&lt;/p&gt;
&lt;p&gt;Our motivating application concerns booking decisions of intermodal
containers on double-stack trains. Under perfect information, this is the
so-called load planning problem and it can be formulated by means of integer
linear programming. However, the formulation cannot be used for the application
at hand because of the restricted computational budget and unknown container
weights. The results show that standard deep learning algorithms allow one to
predict descriptions of solutions with high accuracy in very short time
(milliseconds or less).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larsen_E/0/1/0/all/0/1&quot;&gt;Eric Larsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lachapelle_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Lachapelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frejinger_E/0/1/0/all/0/1&quot;&gt;Emma Frejinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1&quot;&gt;Simon Lacoste-Julien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lodi_A/0/1/0/all/0/1&quot;&gt;Andrea Lodi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11880">
<title>Stochastic Gradient Descent with Biased but Consistent Gradient Estimators. (arXiv:1807.11880v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.11880</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic gradient descent (SGD), which dates back to the 1950s, is one of
the most popular and effective approaches for performing stochastic
optimization. Research on SGD resurged recently in machine learning for
optimizing convex loss functions as well as training nonconvex deep neural
networks. The theory assumes that one can easily compute an unbiased gradient
estimator, which is usually the case due to the sample average nature of
empirical risk minimization. There exist, however, many scenarios (e.g., graph
learning) where an unbiased estimator may be as expensive to compute as the
full gradient, because training examples are interconnected. In a recent work,
Chen et al. (2018) proposed using a consistent gradient estimator as an
economic alternative. Encouraged by empirical success, we show, in a general
setting, that consistent estimators result in the same convergence behavior as
do unbiased ones. Our analysis covers strongly convex, convex, and nonconvex
objectives. This work opens several new research directions, including the
development of more efficient SGD updates with consistent estimators and the
design of efficient training algorithms for large-scale graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luss_R/0/1/0/all/0/1&quot;&gt;Ronny Luss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.03774">
<title>Parallelizing Stochastic Gradient Descent for Least Squares Regression: mini-batching, averaging, and model misspecification. (arXiv:1610.03774v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.03774</link>
<description rdf:parseType="Literal">&lt;p&gt;This work characterizes the benefits of averaging schemes widely used in
conjunction with stochastic gradient descent (SGD). In particular, this work
provides a sharp analysis of: (1) mini-batching, a method of averaging many
samples of a stochastic gradient to both reduce the variance of the stochastic
gradient estimate and for parallelizing SGD and (2) tail-averaging, a method
involving averaging the final few iterates of SGD to decrease the variance in
SGD&apos;s final iterate. This work presents non-asymptotic excess risk bounds for
these schemes for the stochastic approximation problem of least squares
regression.
&lt;/p&gt;
&lt;p&gt;Furthermore, this work establishes a precise problem-dependent extent to
which mini-batch SGD yields provable near-linear parallelization speedups over
SGD with batch size one. This allows for understanding learning rate versus
batch size tradeoffs for the final iterate of an SGD method. These results are
then utilized in providing a highly parallelizable SGD method that obtains the
minimax risk with nearly the same number of serial updates as batch gradient
descent, improving significantly over existing SGD methods. A non-asymptotic
analysis of communication efficient parallelization schemes such as
model-averaging/parameter mixing methods is then provided.
&lt;/p&gt;
&lt;p&gt;Finally, this work sheds light on some fundamental differences in SGD&apos;s
behavior when dealing with agnostic noise in the (non-realizable) least squares
regression problem. In particular, the work shows that the stepsizes that
ensure minimax risk for the agnostic case must be a function of the noise
properties.
&lt;/p&gt;
&lt;p&gt;This paper builds on the operator view of analyzing SGD methods, introduced
by Defossez and Bach (2015), followed by developing a novel analysis in
bounding these operators to characterize the excess risk. These techniques are
of broader interest in analyzing computational aspects of stochastic
approximation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jain_P/0/1/0/all/0/1&quot;&gt;Prateek Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kakade_S/0/1/0/all/0/1&quot;&gt;Sham M. Kakade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kidambi_R/0/1/0/all/0/1&quot;&gt;Rahul Kidambi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Netrapalli_P/0/1/0/all/0/1&quot;&gt;Praneeth Netrapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sidford_A/0/1/0/all/0/1&quot;&gt;Aaron Sidford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.08263">
<title>Efficient Manifold and Subspace Approximations with Spherelets. (arXiv:1706.08263v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.08263</link>
<description rdf:parseType="Literal">&lt;p&gt;Data lying in a high dimensional ambient space are commonly thought to have a
much lower intrinsic dimension. In particular, the data may be concentrated
near a lower-dimensional subspace or manifold. There is an immense literature
focused on approximating the unknown subspace, and in exploiting such
approximations in clustering, data compression, and building of predictive
models. Most of the literature relies on approximating subspaces using a
locally linear, and potentially multiscale, dictionary. In this article, we
propose a simple and general alternative, which instead uses pieces of spheres,
or spherelets, to locally approximate the unknown subspace. Theory is developed
showing that spherelets can produce lower covering numbers and MSEs for many
manifolds. We develop spherical principal components analysis (SPCA). Results
relative to state-of-the-art competitors show gains in ability to accurately
approximate the subspace with fewer components. In addition, unlike most
competitors, our approach can be used for data denoising and can efficiently
embed new data without retraining. The methods are illustrated with standard
toy manifold learning examples, and applications to multiple real data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Didong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mukhopadhyay_M/0/1/0/all/0/1&quot;&gt;Minerva Mukhopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dunson_D/0/1/0/all/0/1&quot;&gt;David B. Dunson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.02329">
<title>Deep Q-Learning for Self-Organizing Networks Fault Management and Radio Performance Improvement. (arXiv:1707.02329v4 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/1707.02329</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an algorithm to automate fault management in an outdoor cellular
network using deep reinforcement learning (RL) against wireless impairments.
This algorithm enables the cellular network cluster to self-heal by allowing RL
to learn how to improve the downlink signal to interference plus noise ratio
and spectral efficiency through exploration and exploitation of various alarm
corrective actions. The main contributions of this paper are to 1) introduce a
deep RL-based fault handling algorithm which self-organizing networks can
implement in a polynomial runtime and 2) show that this fault management method
can improve the radio link performance in a realistic network setup. Simulation
results show that our proposed learns an action sequence to clear alarms and
improve the performance in the cellular cluster better than existing
algorithms, even against the randomness of the network fault occurrences and
user movements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mismar_F/0/1/0/all/0/1&quot;&gt;Faris B. Mismar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1&quot;&gt;Brian L. Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.03269">
<title>Q-Learning Algorithm for VoLTE Closed-Loop Power Control in Indoor Small Cells. (arXiv:1707.03269v5 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/1707.03269</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a reinforcement learning (RL) based closed loop power control
algorithm for the downlink of the voice over LTE (VoLTE) radio bearer for an
indoor environment served by small cells. The main contributions of our paper
are to 1) use RL to solve performance tuning problems in an indoor cellular
network for voice bearers and 2) show that our derived lower bound loss in
effective SINR is sufficient for VoLTE power control purposes in practical
cellular networks. In our simulation, the proposed RL-based power control
algorithm significantly improves both voice retainability and mean opinion
score compared to current industry standards. The improvement is due to
maintaining an effective downlink signal to interference plus noise ratio
against adverse network operational issues and faults.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mismar_F/0/1/0/all/0/1&quot;&gt;Faris B. Mismar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1&quot;&gt;Brian L. Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10733">
<title>DS*: Tighter Lifting-Free Convex Relaxations for Quadratic Matching Problems. (arXiv:1711.10733v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10733</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we study convex relaxations of quadratic optimisation problems
over permutation matrices. While existing semidefinite programming approaches
can achieve remarkably tight relaxations, they have the strong disadvantage
that they lift the original $n {\times} n$-dimensional variable to an $n^2
{\times} n^2$-dimensional variable, which limits their practical applicability.
In contrast, here we present a lifting-free convex relaxation that is provably
at least as tight as existing (lifting-free) convex relaxations. We demonstrate
experimentally that our approach is superior to existing convex and non-convex
methods for various problems, including image arrangement and multi-graph
matching.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bernard_F/0/1/0/all/0/1&quot;&gt;Florian Bernard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Moeller_M/0/1/0/all/0/1&quot;&gt;Michael Moeller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04153">
<title>Bayesian Quadrature for Multiple Related Integrals. (arXiv:1801.04153v7 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04153</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian probabilistic numerical methods are a set of tools providing
posterior distributions on the output of numerical methods. The use of these
methods is usually motivated by the fact that they can represent our
uncertainty due to incomplete/finite information about the continuous
mathematical problem being approximated. In this paper, we demonstrate that
this paradigm can provide additional advantages, such as the possibility of
transferring information between several numerical methods. This allows users
to represent uncertainty in a more faithful manner and, as a by-product,
provide increased numerical efficiency. We propose the first such numerical
method by extending the well-known Bayesian quadrature algorithm to the case
where we are interested in computing the integral of several related functions.
We then prove convergence rates for the method in the well-specified and
misspecified cases, and demonstrate its efficiency in the context of
multi-fidelity models for complex engineering systems and a problem of global
illumination in computer graphics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xi_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois-Xavier Briol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1&quot;&gt;Mark Girolami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.01610">
<title>Fast and accurate approximation of the full conditional for gamma shape parameters. (arXiv:1802.01610v2 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1802.01610</link>
<description rdf:parseType="Literal">&lt;p&gt;The gamma distribution arises frequently in Bayesian models, but there is not
an easy-to-use conjugate prior for the shape parameter of a gamma. This
inconvenience is usually dealt with by using either Metropolis-Hastings moves,
rejection sampling methods, or numerical integration. However, in models with a
large number of shape parameters, these existing methods are slower or more
complicated than one would like, making them burdensome in practice. It turns
out that the full conditional distribution of the gamma shape parameter is well
approximated by a gamma distribution, even for small sample sizes, when the
prior on the shape parameter is also a gamma distribution. This article
introduces a quick and easy algorithm for finding a gamma distribution that
approximates the full conditional distribution of the shape parameter. We
empirically demonstrate the speed and accuracy of the approximation across a
wide range of conditions. If exactness is required, the approximation can be
used as a proposal distribution for Metropolis-Hastings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Miller_J/0/1/0/all/0/1&quot;&gt;Jeffrey W. Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03752">
<title>Supervised classification of Dermatological diseases by Deep learning. (arXiv:1802.03752v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03752</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a deep-learning based efficient classifier for common
dermatological conditions, aimed at people without easy access to skin
specialists. We report approximately 80% accuracy, in a situation where primary
care doctors have attained 57% success rate, according to recent literature.
The rationale of its design is centered on deploying and updating it on
handheld devices in near future. Dermatological diseases are common in every
population and have a wide spectrum in severity. With a shortage of
dermatological expertise being observed in several countries, machine learning
solutions can augment medical services and advise regarding existence of common
diseases. The paper implements supervised classification of nine distinct
conditions which have high occurrence in East Asian countries. Our current
attempt establishes that deep learning based techniques are viable avenues for
preliminary information to aid patients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Sourav Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yamasaki_T/0/1/0/all/0/1&quot;&gt;Toshihiko Yamasaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Imaizumi_H/0/1/0/all/0/1&quot;&gt;Hideaki Imaizumi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08898">
<title>Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo. (arXiv:1802.08898v3 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08898</link>
<description rdf:parseType="Literal">&lt;p&gt;Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from
high-dimensional distributions in Statistics and Machine learning. HMC is known
to run very efficiently in practice and its popular second-order &quot;leapfrog&quot;
implementation has long been conjectured to run in $d^{1/4}$ gradient
evaluations. Here we show that this conjecture is true when sampling from
strongly log-concave target distributions that satisfy a weak third-order
regularity property associated with the input data. Our regularity condition is
weaker than the Lipschitz Hessian property and allows us to show faster
convergence bounds for a much larger class of distributions than would be
possible with the usual Lipschitz Hessian constant alone. Important
distributions that satisfy our regularity condition include posterior
distributions used in Bayesian logistic regression for which the data satisfies
an &quot;incoherence&quot; property. Our result compares favorably with the best
available bounds for the class of strongly log-concave distributions, which
grow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, our
simulations on synthetic data suggest that, when our regularity condition is
satisfied, leapfrog HMC performs better than its competitors -- both in terms
of accuracy and the number of gradient evaluations it requires.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mangoubi_O/0/1/0/all/0/1&quot;&gt;Oren Mangoubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1&quot;&gt;Nisheeth K. Vishnoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03234">
<title>Improving Optimization for Models With Continuous Symmetry Breaking. (arXiv:1803.03234v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03234</link>
<description rdf:parseType="Literal">&lt;p&gt;Many loss functions in representation learning are invariant under a
continuous symmetry transformation. For example, the loss function of word
embeddings (Mikolov et al., 2013) remains unchanged if we simultaneously rotate
all word and context embedding vectors. We show that representation learning
models for time series possess an approximate continuous symmetry that leads to
slow convergence of gradient descent. We propose a new optimization algorithm
that speeds up convergence using ideas from gauge theory in physics. Our
algorithm leads to orders of magnitude faster convergence and to more
interpretable representations, as we show for dynamic extensions of matrix
factorization and word embedding models. We further present an example
application of our proposed algorithm that translates modern words into their
historic equivalents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bamler_R/0/1/0/all/0/1&quot;&gt;Robert Bamler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01050">
<title>Training VAEs Under Structured Residuals. (arXiv:1804.01050v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01050</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational auto-encoders (VAEs) are a popular and powerful deep generative
model. Previous works on VAEs have assumed a factorized likelihood model,
whereby the output uncertainty of each pixel is assumed to be independent. This
approximation is clearly limited as demonstrated by observing a residual image
from a VAE reconstruction, which often possess a high level of structure. This
paper demonstrates a novel scheme to incorporate a structured Gaussian
likelihood prediction network within the VAE that allows the residual
correlations to be modeled. Our novel architecture, with minimal increase in
complexity, incorporates the covariance matrix prediction within the VAE. We
also propose a new mechanism for allowing structured uncertainty on color
images. Furthermore, we provide a scheme for effectively training this model,
and include some suggestions for improving performance in terms of efficiency
or modeling longer range correlations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dorta_G/0/1/0/all/0/1&quot;&gt;Garoe Dorta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vicente_S/0/1/0/all/0/1&quot;&gt;Sara Vicente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Agapito_L/0/1/0/all/0/1&quot;&gt;Lourdes Agapito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Campbell_N/0/1/0/all/0/1&quot;&gt;Neill D.F. Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Simpson_I/0/1/0/all/0/1&quot;&gt;Ivor Simpson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02612">
<title>Dimensionality-Driven Learning with Noisy Labels. (arXiv:1806.02612v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02612</link>
<description rdf:parseType="Literal">&lt;p&gt;Datasets with significant proportions of noisy (incorrect) class labels
present challenges for training accurate Deep Neural Networks (DNNs). We
propose a new perspective for understanding DNN generalization for such
datasets, by investigating the dimensionality of the deep representation
subspace of training samples. We show that from a dimensionality perspective,
DNNs exhibit quite distinctive learning styles when trained with clean labels
versus when trained with a proportion of noisy labels. Based on this finding,
we develop a new dimensionality-driven learning strategy, which monitors the
dimensionality of subspaces during training and adapts the loss function
accordingly. We empirically demonstrate that our approach is highly tolerant to
significant proportions of noisy labels, and can effectively learn
low-dimensional local subspaces that capture the data distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xingjun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yisen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houle_M/0/1/0/all/0/1&quot;&gt;Michael E. Houle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1&quot;&gt;Sarah M. Erfani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijewickrema_S/0/1/0/all/0/1&quot;&gt;Sudanthi Wijewickrema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1&quot;&gt;James Bailey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03121">
<title>Machine Learning CICY Threefolds. (arXiv:1806.03121v3 [hep-th] UPDATED)</title>
<link>http://arxiv.org/abs/1806.03121</link>
<description rdf:parseType="Literal">&lt;p&gt;The latest techniques from Neural Networks and Support Vector Machines (SVM)
are used to investigate geometric properties of Complete Intersection
Calabi-Yau (CICY) threefolds, a class of manifolds that facilitate string model
building. An advanced neural network classifier and SVM are employed to (1)
learn Hodge numbers and report a remarkable improvement over previous efforts,
(2) query for favourability, and (3) predict discrete symmetries, a highly
imbalanced problem to which both Synthetic Minority Oversampling Technique
(SMOTE) and permutations of the CICY matrix are used to decrease the class
imbalance and improve performance. In each case study, we employ a genetic
algorithm to optimise the hyperparameters of the neural network. We demonstrate
that our approach provides quick diagnostic tools capable of shortlisting
quasi-realistic string models based on compactification over smooth CICYs and
further supports the paradigm that classes of problems in algebraic geometry
can be machine learned.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Bull_K/0/1/0/all/0/1&quot;&gt;Kieran Bull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yang-Hui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Jejjala_V/0/1/0/all/0/1&quot;&gt;Vishnu Jejjala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Mishra_C/0/1/0/all/0/1&quot;&gt;Challenger Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04458">
<title>Sparse Stochastic Zeroth-Order Optimization with an Application to Bandit Structured Prediction. (arXiv:1806.04458v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04458</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic zeroth-order (SZO), or gradient-free, optimization allows to
optimize arbitrary functions by relying only on function evaluations under
parameter perturbations, however, the iteration complexity of SZO methods
suffers a factor proportional to the dimensionality of the perturbed function.
We show that in scenarios with natural sparsity patterns as in structured
prediction applications, this factor can be reduced to the expected number of
active features over input-output pairs. We give a general proof that applies
sparse SZO optimization to Lipschitz-continuous, nonconvex, stochastic
objectives, and present an experimental evaluation on linear bandit structured
prediction tasks with sparse word-based feature representations that confirm
our theoretical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sokolov_A/0/1/0/all/0/1&quot;&gt;Artem Sokolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hitschler_J/0/1/0/all/0/1&quot;&gt;Julian Hitschler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Riezler_S/0/1/0/all/0/1&quot;&gt;Stefan Riezler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08010">
<title>Fairness Without Demographics in Repeated Loss Minimization. (arXiv:1806.08010v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.08010</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models (e.g., speech recognizers) are usually trained to
minimize average loss, which results in representation disparity---minority
groups (e.g., non-native speakers) contribute less to the training objective
and thus tend to suffer higher loss. Worse, as model accuracy affects user
retention, a minority group can shrink over time. In this paper, we first show
that the status quo of empirical risk minimization (ERM) amplifies
representation disparity over time, which can even make initially fair models
unfair. To mitigate this, we develop an approach based on distributionally
robust optimization (DRO), which minimizes the worst case risk over all
distributions close to the empirical distribution. We prove that this approach
controls the risk of the minority group at each time step, in the spirit of
Rawlsian distributive justice, while remaining oblivious to the identity of the
groups. We demonstrate that DRO prevents disparity amplification on examples
where ERM fails, and show improvements in minority group user satisfaction in a
real-world text autocomplete task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori B. Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srivastava_M/0/1/0/all/0/1&quot;&gt;Megha Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Namkoong_H/0/1/0/all/0/1&quot;&gt;Hongseok Namkoong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09048">
<title>A classification point-of-view about conditional Kendall&apos;s tau. (arXiv:1806.09048v2 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1806.09048</link>
<description rdf:parseType="Literal">&lt;p&gt;We show how the problem of estimating conditional Kendall&apos;s tau can be
rewritten as a classification task. Conditional Kendall&apos;s tau is a conditional
dependence parameter that is a characteristic of a given pair of random
variables. The goal is to predict whether the pair is concordant (value of $1$)
or discordant (value of $-1$) conditionally on some covariates. We prove the
consistency and the asymptotic normality of a family of penalized approximate
maximum likelihood estimators, including the equivalent of the logit and probit
regressions in our framework. Then, we detail specific algorithms adapting
usual machine learning techniques, including nearest neighbors, decision trees,
random forests and neural networks, to the setting of the estimation of
conditional Kendall&apos;s tau. A small simulation study compares their finite
sample properties. Finally, we apply all these estimators to a dataset of
European stock indices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Derumigny_A/0/1/0/all/0/1&quot;&gt;Alexis Derumigny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fermanian_J/0/1/0/all/0/1&quot;&gt;Jean-David Fermanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06007">
<title>On Lebesgue Integral Quadrature. (arXiv:1807.06007v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/1807.06007</link>
<description rdf:parseType="Literal">&lt;p&gt;A new type of quadrature is developed. The Gauss quadrature, for a given
measure, finds optimal values of a function&apos;s argument (nodes) and the
corresponding weights. In contrast, the Lebesgue quadrature developed in this
paper, finds optimal values of function (value-nodes) and the corresponding
weights. The Gauss quadrature groups sums by function argument; it can be
viewed as a $n$-point discrete measure, producing the Riemann integral. The
Lebesgue quadrature groups sums by function value; it can be viewed as a
$n$-point discrete distribution, producing the Lebesgue integral.
Mathematically, the problem is reduced to a generalized eigenvalue problem:
Lebesgue quadrature value-nodes are the eigenvalues and the corresponding
weights are the square of the averaged eigenvectors. A numerical estimation of
an integral as the Lebesgue integral is especially advantageous when analyzing
irregular and stochastic processes. The approach separates the outcome
(value-nodes) and the probability of the outcome (weight). For this reason, it
is especially well-suited for the study of non--Gaussian processes. The
software implementing the theory is available from the authors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Malyshkin_V/0/1/0/all/0/1&quot;&gt;Vladislav Gennadievich Malyshkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07754">
<title>Learning the effect of latent variables in Gaussian Graphical models with unobserved variables. (arXiv:1807.07754v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.07754</link>
<description rdf:parseType="Literal">&lt;p&gt;The edge structure of the graph defining an undirected graphical model
describes precisely the structure of dependence between the variables in the
graph. In many applications, the dependence structure is unknown and it is
desirable to learn it from data, often because it is a preliminary step to be
able to ascertain causal effects. This problem, known as structure learning, is
hard in general, but for Gaussian graphical models it is slightly easier
because the structure of the graph is given by the sparsity pattern of the
precision matrix of the joint distribution, and because independence coincides
with decorrelation. A major difficulty too often ignored in structure learning
is the fact that if some variables are not observed, the marginal dependence
graph over the observed variables will possibly be significantly more complex
and no longer reflect the direct dependencies that are potentially associated
with causal effects. In this work, we consider a family of latent variable
Gaussian graphical models in which the graph of the joint distribution between
observed and unobserved variables is sparse, and the unobserved variables are
conditionally independent given the others. Prior work was able to recover the
connectivity between observed variables, but could only identify the subspace
spanned by unobserved variables, whereas we propose a convex optimization
formulation based on structured matrix sparsity to estimate the complete
connectivity of the complete graph including unobserved variables, given the
knowledge of the number of missing variables, and a priori knowledge of their
level of connectivity. Our formulation is supported by a theoretical result of
identifiability of the latent dependence structure for sparse graphs in the
infinite data limit. We propose an algorithm leveraging recent active set
methods, which performs well in the experiments on synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vinyes_M/0/1/0/all/0/1&quot;&gt;Marina Vinyes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Obozinski_G/0/1/0/all/0/1&quot;&gt;Guillaume Obozinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08197">
<title>On Numerical Estimation of Joint Probability Distribution from Lebesgue Integral Quadratures. (arXiv:1807.08197v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/1807.08197</link>
<description rdf:parseType="Literal">&lt;p&gt;An important application of Lebesgue integral quadrature[1] is developed.
Given two random processes, $f(x)$ and $g(x)$, two generalized eigenvalue
problems can be formulated and solved. In addition to obtaining two Lebesgue
quadratures (for $f$ and $g$) from two eigenproblems, the projections of $f$--
and $g$-- eigenvectors on each other allow to build a joint distribution
estimator. Two kinds of estimators are obtained: value--correlation
$V_{f_i;g_j}$, similar to the regular correlation concept, and a new one,
probability--correlation $P_{f_i;g_j}$. The theory is implemented numerically;
the software is available under the GPLv3 license.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Malyshkin_V/0/1/0/all/0/1&quot;&gt;Vladislav Gennadievich Malyshkin&lt;/a&gt;</dc:creator>
</item></rdf:RDF>