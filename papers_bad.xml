<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01844"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.06474"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01739"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01801"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01953"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01970"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02037"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1602.07857"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.08954"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.09382"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03846"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01670"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01750"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01771"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01774"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01808"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01827"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01889"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01969"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02033"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02078"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02089"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.06972"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.00707"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03065"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04846"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01257"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02603"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11274"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01851"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02185"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02954"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08295"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10175"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10586"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11038"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00243"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00905"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.01844">
<title>Pontogammarus Maeoticus Swarm Optimization: A Metaheuristic Optimization Algorithm. (arXiv:1807.01844v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.01844</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, metaheuristic optimization algorithms are used to find the global
optima in difficult search spaces. Pontogammarus Maeoticus Swarm Optimization
(PMSO) is a metaheuristic algorithm imitating aquatic nature and foraging
behavior. Pontogammarus Maeoticus, also called Gammarus in short, is a tiny
creature found mostly in coast of Caspian Sea in Iran. In this algorithm,
global optima is modeled as sea edge (coast) to which Gammarus creatures are
willing to move in order to rest from sea waves and forage in sand. Sea waves
satisfy exploration and foraging models exploitation. The strength of sea wave
is determined according to distance of Gammarus from sea edge. The angles of
waves applied on several particles are set randomly helping algorithm not be
stuck in local bests. Meanwhile, the neighborhood of particles change
adaptively resulting in more efficient progress in searching. The proposed
algorithm, although is applicable on any optimization problem, is experimented
for partially shaded solar PV array. Experiments on CEC05 benchmarks, as well
as solar PV array, show the effectiveness of this optimization algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghojogh_B/0/1/0/all/0/1&quot;&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharifian_S/0/1/0/all/0/1&quot;&gt;Saeed Sharifian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.06474">
<title>Learned Primal-dual Reconstruction. (arXiv:1707.06474v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1707.06474</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the Learned Primal-Dual algorithm for tomographic reconstruction.
The algorithm accounts for a (possibly non-linear) forward operator in a deep
neural network by unrolling a proximal primal-dual optimization method, but
where the proximal operators have been replaced with convolutional neural
networks. The algorithm is trained end-to-end, working directly from raw
measured data and it does not depend on any initial reconstruction such as FBP.
&lt;/p&gt;
&lt;p&gt;We compare performance of the proposed method on low dose CT reconstruction
against FBP, TV, and deep learning based post-processing of FBP. For the
Shepp-Logan phantom we obtain &amp;gt;6dB PSNR improvement against all compared
methods. For human phantoms the corresponding improvement is 6.6dB over TV and
2.2dB over learned post-processing along with a substantial improvement in the
SSIM. Finally, our algorithm involves only ten forward-back-projection
computations, making the method feasible for time critical clinical
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Adler_J/0/1/0/all/0/1&quot;&gt;Jonas Adler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Oktem_O/0/1/0/all/0/1&quot;&gt;Ozan &amp;#xd6;ktem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01739">
<title>Proximal algorithms for large-scale statistical modeling and optimal sensor/actuator selection. (arXiv:1807.01739v1 [math.OC])</title>
<link>http://arxiv.org/abs/1807.01739</link>
<description rdf:parseType="Literal">&lt;p&gt;Several problems in modeling and control of stochastically-driven dynamical
systems can be cast as regularized semi-definite programs. We examine two such
representative problems and show that they can be formulated in a similar
manner. The first, in statistical modeling, seeks to reconcile observed
statistics by suitably and minimally perturbing prior dynamics. The second,
seeks to optimally select sensors and actuators for control purposes. To
address modeling and control of large-scale systems we develop a unified
algorithmic framework using proximal methods. Our customized algorithms exploit
problem structure and allow handling statistical modeling, as well as sensor
and actuator selection, for substantially larger scales than what is amenable
to current general-purpose solvers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zare_A/0/1/0/all/0/1&quot;&gt;Armin Zare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dhingra_N/0/1/0/all/0/1&quot;&gt;Neil K. Dhingra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jovanovic_M/0/1/0/all/0/1&quot;&gt;Mihailo R. Jovanovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Georgiou_T/0/1/0/all/0/1&quot;&gt;Tryphon T. Georgiou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01801">
<title>Feature-based reformulation of entities in triple pattern queries. (arXiv:1807.01801v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.01801</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs encode uniquely identifiable entities to other entities or
literal values by means of relationships, thus enabling semantically rich
querying over the stored data. Typically, the semantics of such queries are
often crisp thereby resulting in crisp answers. Query log statistics show that
a majority of the queries issued to knowledge graphs are often entity centric
queries. When a user needs additional answers the state-of-the-art in assisting
users is to rewrite the original query resulting in a set of approximations.
Several strategies have been proposed in past to address this. They typically
move up the taxonomy to relax a specific element to a more generic element.
Entities don&apos;t have a taxonomy and they end up being generalized. To address
this issue, in this paper, we propose an entity centric reformulation strategy
that utilizes schema information and entity features present in the graph to
suggest rewrites. Once the features are identified, the entity in concern is
reformulated as a set of features. Since entities can have a large number of
features, we introduce strategies that select the top-k most relevant and
{informative ranked features and augment them to the original query to create a
valid reformulation. We then evaluate our approach by showing that our
reformulation strategy produces results that are more informative when compared
with state-of-the-art
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viswanathan_A/0/1/0/all/0/1&quot;&gt;Amar Viswanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mel_G/0/1/0/all/0/1&quot;&gt;Geeth de Mel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1&quot;&gt;James A.Hendler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01953">
<title>Lattice based Conceptual Spaces to Explore Cognitive Functionalities for Prosthetic Arm. (arXiv:1807.01953v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.01953</link>
<description rdf:parseType="Literal">&lt;p&gt;Upper limb Prosthetic can be viewed as an independent cognitive system in
order to develop a conceptual space. In this paper, we provide a detailed
analogical reasoning of prosthetic arm to build the conceptual spaces with the
help of the theory called geometric framework of conceptual spaces proposed by
Gardenfors. Terminologies of conceptual spaces such as concepts, similarities,
properties, quality dimensions and prototype are applied for a specific
prosthetic system and conceptual space is built for prosthetic arm. Concept
lattice traversals are used on the lattice represented conceptual spaces.
Cognitive functionalities such as generalization (Similarities) and
specialization (Differences) are achieved in the lattice represented conceptual
space. This might well prove to design intelligent prosthetics to assist
challenged humans. Geometric framework of conceptual spaces holds similar
concepts closer in geometric structures in a way similar to concept lattices.
Hence, we also propose to use concept lattice to represent concepts of
geometric framework of conceptual spaces. Also, we extend our discussion with
our insights on conceptual spaces of bidirectional hand prosthetics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishwarya_M/0/1/0/all/0/1&quot;&gt;M S Ishwarya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherukuri_A/0/1/0/all/0/1&quot;&gt;Aswani Kumar Cherukuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01970">
<title>Arcades: A deep model for adaptive decision making in voice controlled smart-home. (arXiv:1807.01970v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01970</link>
<description rdf:parseType="Literal">&lt;p&gt;In a voice-controlled smart-home, a controller must respond not only to
user&apos;s requests but also according to the interaction context. This paper
describes Arcades, a system which uses deep reinforcement learning to extract
context from a graphical representation of home automation system and to update
continuously its behavior to the user&apos;s one. This system is robust to changes
in the environment (sensor breakdown or addition) through its graphical
representation (scale well) and the reinforcement mechanism (adapt well). The
experiments on realistic data demonstrate that this method promises to reach
long life context-aware control of smart-home.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brenon_A/0/1/0/all/0/1&quot;&gt;Alexis Brenon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Portet_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Portet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vacher_M/0/1/0/all/0/1&quot;&gt;Michel Vacher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02037">
<title>TFLMS: Large Model Support in TensorFlow by Graph Rewriting. (arXiv:1807.02037v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02037</link>
<description rdf:parseType="Literal">&lt;p&gt;While accelerators such as GPUs have limited memory, deep neural networks are
becoming larger and will not fit with the memory limitation of accelerators for
training. We propose an approach to tackle this problem by rewriting the
computational graph of a neural network, in which swap-out and swap-in
operations are inserted to temporarily store intermediate results on CPU
memory. In particular, we first revise the concept of a computational graph by
defining a concrete semantics for variables in a graph. We then formally show
how to derive swap-out and swap-in operations from an existing graph and
present rules to optimize the graph. To realize our approach, we developed a
module in TensorFlow, named TFLMS. TFLMS is published as a pull request in the
TensorFlow repository for contributing to the TensorFlow community. With TFLMS,
we were able to train ResNet-50 and 3DUnet with 4.7x and 2x larger batch size,
respectively. In particular, we were able to train 3DUNet using images of size
of $192^3$ for image segmentation, which, without TFLMS, had been done only by
dividing the images to smaller images, which affects the accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Tung D. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imai_H/0/1/0/all/0/1&quot;&gt;Haruki Imai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negishi_Y/0/1/0/all/0/1&quot;&gt;Yasushi Negishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawachiya_K/0/1/0/all/0/1&quot;&gt;Kiyokuni Kawachiya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1602.07857">
<title>Modeling cumulative biological phenomena with Suppes-Bayes Causal Networks. (arXiv:1602.07857v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1602.07857</link>
<description rdf:parseType="Literal">&lt;p&gt;Several diseases related to cell proliferation are characterized by the
accumulation of somatic DNA changes, with respect to wildtype conditions.
Cancer and HIV are two common examples of such diseases, where the mutational
load in the cancerous/viral population increases over time. In these cases,
selective pressures are often observed along with competition, cooperation and
parasitism among distinct cellular clones. Recently, we presented a
mathematical framework to model these phenomena, based on a combination of
Bayesian inference and Suppes&apos; theory of probabilistic causation, depicted in
graphical structures dubbed Suppes-Bayes Causal Networks (SBCNs). SBCNs are
generative probabilistic graphical models that recapitulate the potential
ordering of accumulation of such DNA changes during the progression of the
disease. Such models can be inferred from data by exploiting likelihood-based
model-selection strategies with regularization. In this paper we discuss the
theoretical foundations of our approach and we investigate in depth the
influence on the model-selection task of: (i) the poset based on Suppes&apos; theory
and (ii) different regularization strategies. Furthermore, we provide an
example of application of our framework to HIV genetic data highlighting the
valuable insights provided by the inferred.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramazzotti_D/0/1/0/all/0/1&quot;&gt;Daniele Ramazzotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graudenzi_A/0/1/0/all/0/1&quot;&gt;Alex Graudenzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caravagna_G/0/1/0/all/0/1&quot;&gt;Giulio Caravagna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antoniotti_M/0/1/0/all/0/1&quot;&gt;Marco Antoniotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.08954">
<title>Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs. (arXiv:1606.08954v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1606.08954</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a transition-based parser that jointly produces syntactic and
semantic dependencies. It learns a representation of the entire algorithm
state, using stack long short-term memories. Our greedy inference algorithm has
linear time, including feature extraction. On the CoNLL 2008--9 English shared
tasks, we obtain the best published parsing performance among models that
jointly learn syntax and semantics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1&quot;&gt;Swabha Swayamdipta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1&quot;&gt;Miguel Ballesteros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1&quot;&gt;Chris Dyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1&quot;&gt;Noah A. Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.09382">
<title>Distributed Robust Subspace Recovery. (arXiv:1705.09382v3 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/1705.09382</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose distributed solutions to the problem of Robust Subspace Recovery
(RSR). Our setting assumes a huge dataset in an ad hoc network without a
central processor, where each node has access only to one chunk of the dataset.
Furthermore, part of the whole dataset lies around a low-dimensional subspace
and the other part is composed of outliers that lie away from that subspace.
The goal is to recover the underlying subspace for the whole dataset, without
transferring the data itself between the nodes. We first apply the
Consensus-Based Gradient method to the Geometric Median Subspace algorithm for
RSR. For this purpose, we propose an iterative solution for the local dual
minimization problem and establish its r-linear convergence. We then explain
how to distributedly implement the Reaper and Fast Median Subspace algorithms
for RSR. The proposed algorithms display competitive performance on both
synthetic and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Huroyan_V/0/1/0/all/0/1&quot;&gt;Vahan Huroyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lerman_G/0/1/0/all/0/1&quot;&gt;Gilad Lerman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03846">
<title>&quot;Dave...I can assure you...that it&apos;s going to be all right...&quot; -- A definition, case for, and survey of algorithmic assurances in human-autonomy trust relationships. (arXiv:1711.03846v3 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03846</link>
<description rdf:parseType="Literal">&lt;p&gt;Those who design, use, and are otherwise affected by advanced, technologies
like artificially intelligent, autonomous systems want to know that these
systems will perform correctly, understand the reasons behind their actions,
and know how to use them appropriately. In short: they want to be able to trust
such systems. Consequently, designers have devised various kinds of assurances
for assessing trust. Typically, however, these assessments are ad hoc, and have
not been formally related to each other or to formal trust models. This paper
presents a survey of algorithmic assurances that allow users to calibrate their
trust in autonomous artificially intelligent agents and use such autonomous
agents more appropriately. To this end algorithmic assurances are first
formally defined, and classified, from the perspective of formally modeled
trust relationships. The survey is then performed using research from related
communities such as machine learning, human-computer interaction, human-robot
interaction, e-commerce, and others. The literature for different classes of
assurances are identified with seven different levels of integration for
artificially intelligent agents; these classes are useful for practitioners and
system designers. Recommendations and directions for future work are also
presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Israelsen_B/0/1/0/all/0/1&quot;&gt;Brett W Israelsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nisar R Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01670">
<title>Encoding Spatial Relations from Natural Language. (arXiv:1807.01670v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01670</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural language processing has made significant inroads into learning the
semantics of words through distributional approaches, however representations
learnt via these methods fail to capture certain kinds of information implicit
in the real world. In particular, spatial relations are encoded in a way that
is inconsistent with human spatial reasoning and lacking invariance to
viewpoint changes. We present a system capable of capturing the semantics of
spatial relations such as behind, left of, etc from natural language. Our key
contributions are a novel multi-modal objective based on generating images of
scenes from their textual descriptions, and a new dataset on which to train it.
We demonstrate that internal representations are robust to meaning preserving
transformations of descriptions (paraphrase invariance), while viewpoint
invariance is an emergent property of the system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramalho_T/0/1/0/all/0/1&quot;&gt;Tiago Ramalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocisky_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Ko&amp;#x10d;isk&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Besse_F/0/1/0/all/0/1&quot;&gt;Frederic Besse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1&quot;&gt;S. M. Ali Eslami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melis_G/0/1/0/all/0/1&quot;&gt;G&amp;#xe1;bor Melis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viola_F/0/1/0/all/0/1&quot;&gt;Fabio Viola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1&quot;&gt;Phil Blunsom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hermann_K/0/1/0/all/0/1&quot;&gt;Karl Moritz Hermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01750">
<title>Accelerated First-order Methods on the Wasserstein Space for Bayesian Inference. (arXiv:1807.01750v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.01750</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider doing Bayesian inference by minimizing the KL divergence on the
2-Wasserstein space $\mathcal{P}_2$. By exploring the Riemannian structure of
$\mathcal{P}_2$, we develop two inference methods by simulating the gradient
flow on $\mathcal{P}_2$ via updating particles, and an acceleration method that
speeds up all such particle-simulation-based inference methods. Moreover we
analyze the approximation flexibility of such methods, and conceive a novel
bandwidth selection method for the kernel that they use. We note that
$\mathcal{P}_2$ is quite abstract and general so that our methods can make
closer approximation, while it still has a rich structure that enables
practical implementation. Experiments show the effectiveness of the two
proposed methods and the improvement of convergence by the acceleration method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhuo_J/0/1/0/all/0/1&quot;&gt;Jingwei Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Pengyu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01771">
<title>Direct Uncertainty Prediction with Applications to Healthcare. (arXiv:1807.01771v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01771</link>
<description rdf:parseType="Literal">&lt;p&gt;Large labeled datasets for supervised learning are frequently constructed by
assigning each instance to multiple human evaluators, and this leads to
disagreement in the labels associated with a single instance. Here we consider
the question of predicting the level of disagreement for a given instance, and
we find an interesting phenomenon: direct prediction of uncertainty performs
better than the two-step process of training a classifier and then using the
classifier outputs to derive an uncertainty. We show stronger performance for
predicting disagreement via this direct method both in a synthetic setting
whose parameters we can fully control, and in a paradigmatic healthcare
application involving multiple labels assigned by medical domain experts. We
further show implications for allocating additional labeling effort toward
instances with the greatest levels of predicted disagreement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1&quot;&gt;Maithra Raghu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blumer_K/0/1/0/all/0/1&quot;&gt;Katy Blumer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayres_R/0/1/0/all/0/1&quot;&gt;Rory Sayres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obermeyer_Z/0/1/0/all/0/1&quot;&gt;Ziad Obermeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullainathan_S/0/1/0/all/0/1&quot;&gt;Sendhil Mullainathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1&quot;&gt;Jon Kleinberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01774">
<title>BOHB: Robust and Efficient Hyperparameter Optimization at Scale. (arXiv:1807.01774v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01774</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern deep learning methods are very sensitive to many hyperparameters, and,
due to the long training times of state-of-the-art models, vanilla Bayesian
hyperparameter optimization is typically computationally infeasible. On the
other hand, bandit-based configuration evaluation approaches based on random
search lack guidance and do not converge to the best configurations as quickly.
Here, we propose to combine the benefits of both Bayesian optimization and
bandit-based methods, in order to achieve the best of both worlds: strong
anytime performance and fast convergence to optimal configurations. We propose
a new practical state-of-the-art hyperparameter optimization method, which
consistently outperforms both Bayesian optimization and Hyperband on a wide
range of problem types, including high-dimensional toy functions, support
vector machines, feed-forward neural networks, Bayesian neural networks, deep
reinforcement learning, and convolutional neural networks. Our method is robust
and versatile, while at the same time being conceptually simple and easy to
implement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falkner_S/0/1/0/all/0/1&quot;&gt;Stefan Falkner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_A/0/1/0/all/0/1&quot;&gt;Aaron Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01808">
<title>Discrete Sampling using Semigradient-based Product Mixtures. (arXiv:1807.01808v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01808</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of inference in discrete probabilistic models, that
is, distributions over subsets of a finite ground set. These encompass a range
of well-known models in machine learning, such as determinantal point processes
and Ising models. Locally-moving Markov chain Monte Carlo algorithms, such as
the Gibbs sampler, are commonly used for inference in such models, but their
convergence is, at times, prohibitively slow. This is often caused by
state-space bottlenecks that greatly hinder the movement of such samplers. We
propose a novel sampling strategy that uses a specific mixture of product
distributions to propose global moves and, thus, accelerate convergence.
Furthermore, we show how to construct such a mixture using semigradient
information. We illustrate the effectiveness of combining our sampler with
existing ones, both theoretically on an example model, as well as practically
on three models learned from real-world data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gotovos_A/0/1/0/all/0/1&quot;&gt;Alkis Gotovos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1&quot;&gt;Hamed Hassani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01827">
<title>Learning Theory and Algorithms for Revenue Management in Sponsored Search. (arXiv:1807.01827v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.01827</link>
<description rdf:parseType="Literal">&lt;p&gt;Online advertisement is the main source of revenue for Internet business.
Advertisers are typically ranked according to a score that takes into account
their bids and potential click-through rates(eCTR). Generally, the likelihood
that a user clicks on an ad is often modeled by optimizing for the click
through rates rather than the performance of the auction in which the click
through rates will be used. This paper attempts to eliminate this
dis-connection by proposing loss functions for click modeling that are based on
final auction performance.In this paper, we address two feasible metrics (AUC^R
and SAUC) to evaluate the on-line RPM (revenue per mille) directly rather than
the CTR. And then, we design an explicit ranking function by incorporating the
calibration fac-tor and price-squashed factor to maximize the revenue. Given
the power of deep networks, we also explore an implicit optimal ranking
function with deep model. Lastly, various experiments with two real world
datasets are presented. In particular, our proposed methods perform better than
the state-of-the-art methods with regard to the revenue of the platform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lulu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huahui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guanhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Shaola Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiaonan Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yi Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01889">
<title>Learning in Variational Autoencoders with Kullback-Leibler and Renyi Integral Bounds. (arXiv:1807.01889v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.01889</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose two novel bounds for the log-likelihood based on
Kullback-Leibler and the R\&apos;{e}nyi divergences, which can be used for
variational inference and in particular for the training of Variational
AutoEncoders. Our proposal is motivated by the difficulties encountered in
training VAEs on continuous datasets with high contrast images, such as those
with handwritten digits and characters, where numerical issues often appear
unless noise is added, either to the dataset during training or to the
generative model given by the decoder. The new bounds we propose, which are
obtained from the maximization of the likelihood of an interval for the
observations, allow numerically stable training procedures without the
necessity of adding any extra source of noise to the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarbu_S/0/1/0/all/0/1&quot;&gt;Septimia S&amp;#xe2;rbu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volpi_R/0/1/0/all/0/1&quot;&gt;Riccardo Volpi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1&quot;&gt;Alexandra Pe&amp;#x15f;te&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malago_L/0/1/0/all/0/1&quot;&gt;Luigi Malag&amp;#xf2;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01969">
<title>Variational Bayesian dropout: pitfalls and fixes. (arXiv:1807.01969v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.01969</link>
<description rdf:parseType="Literal">&lt;p&gt;Dropout, a stochastic regularisation technique for training of neural
networks, has recently been reinterpreted as a specific type of approximate
inference algorithm for Bayesian neural networks. The main contribution of the
reinterpretation is in providing a theoretical framework useful for analysing
and extending the algorithm. We show that the proposed framework suffers from
several issues; from undefined or pathological behaviour of the true posterior
related to use of improper priors, to an ill-defined variational objective due
to singularity of the approximating distribution relative to the true
posterior. Our analysis of the improper log uniform prior used in variational
Gaussian dropout suggests the pathologies are generally irredeemable, and that
the algorithm still works only because the variational formulation annuls some
of the pathologies. To address the singularity issue, we proffer Quasi-KL (QKL)
divergence, a new approximate inference objective for approximation of
high-dimensional distributions. We show that motivations for variational
Bernoulli dropout based on discretisation and noise have QKL as a limit.
Properties of QKL are studied both theoretically and on a simple practical
example which shows that the QKL-optimal approximation of a full rank Gaussian
with a degenerate one naturally leads to the Principal Component Analysis
solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hron_J/0/1/0/all/0/1&quot;&gt;Jiri Hron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Matthews_A/0/1/0/all/0/1&quot;&gt;Alexander G. de G. Matthews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghahramani_Z/0/1/0/all/0/1&quot;&gt;Zoubin Ghahramani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02033">
<title>Consistent Generative Query Networks. (arXiv:1807.02033v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.02033</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic video prediction is usually framed as an extrapolation problem
where the goal is to sample a sequence of consecutive future image frames
conditioned on a sequence of observed past frames. For the most part,
algorithms for this task generate future video frames sequentially in an
autoregressive fashion, which is slow and requires the input and output to be
consecutive. We introduce a model that overcomes these drawbacks -- it learns
to generate a global latent representation from an arbitrary set of frames
within a video. This representation can then be used to simultaneously and
efficiently sample any number of temporally consistent frames at arbitrary
time-points in the video. We apply our model to synthetic video prediction
tasks and achieve results that are comparable to state-of-the-art video
prediction models. In addition, we demonstrate the flexibility of our model by
applying it to 3D scene reconstruction where we condition on location instead
of time. To the best of our knowledge, our model is the first to provide
flexible and coherent prediction on stochastic video datasets, as well as
consistent 3D scene samples. Please check the project website
https://bit.ly/2jX7Vyu to view scene reconstructions and videos produced by our
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Ananya Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1&quot;&gt;S. M. Ali Eslami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezende_D/0/1/0/all/0/1&quot;&gt;Danilo J. Rezende&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garnelo_M/0/1/0/all/0/1&quot;&gt;Marta Garnelo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viola_F/0/1/0/all/0/1&quot;&gt;Fabio Viola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lockhart_E/0/1/0/all/0/1&quot;&gt;Edward Lockhart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1&quot;&gt;Murray Shanahan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02078">
<title>Goal-oriented Trajectories for Efficient Exploration. (arXiv:1807.02078v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02078</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploration is a difficult challenge in reinforcement learning and even
recent state-of-the art curiosity-based methods rely on the simple
epsilon-greedy strategy to generate novelty. We argue that pure random walks do
not succeed to properly expand the exploration area in most environments and
propose to replace single random action choices by random goals selection
followed by several steps in their direction. This approach is compatible with
any curiosity-based exploration and off-policy reinforcement learning agents
and generates longer and safer trajectories than individual random actions. To
illustrate this, we present a task-independent agent that learns to reach
coordinates in screen frames and demonstrate its ability to explore with the
game Super Mario Bros. improving significantly the score of a baseline DQN
agent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pardo_F/0/1/0/all/0/1&quot;&gt;Fabio Pardo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levdik_V/0/1/0/all/0/1&quot;&gt;Vitaly Levdik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kormushev_P/0/1/0/all/0/1&quot;&gt;Petar Kormushev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02089">
<title>Contextual Bandits under Delayed Feedback. (arXiv:1807.02089v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.02089</link>
<description rdf:parseType="Literal">&lt;p&gt;Delayed feedback is an ubiquitous problem in many industrial systems
employing bandit algorithms. Most of those systems seek to optimize binary
indicators as clicks. In that case, when the reward is not sent immediately,
the learner cannot distinguish a negative signal from a not-yet-sent positive
one: she might be waiting for a feedback that will never come. In this paper,
we define and address the contextual bandit problem with delayed and censored
feedback by providing a new UCB-based algorithm. In order to demonstrate its
effectiveness, we provide a finite time regret analysis and an empirical
evaluation that compares it against a baseline commonly used in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vernade_C/0/1/0/all/0/1&quot;&gt;Claire Vernade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carpentier_A/0/1/0/all/0/1&quot;&gt;Alexandra Carpentier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zappella_G/0/1/0/all/0/1&quot;&gt;Giovanni Zappella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ermis_B/0/1/0/all/0/1&quot;&gt;Beyza Ermis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brueckner_M/0/1/0/all/0/1&quot;&gt;Michael Brueckner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.06972">
<title>Approximations of the Restless Bandit Problem. (arXiv:1702.06972v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1702.06972</link>
<description rdf:parseType="Literal">&lt;p&gt;The multi-armed restless bandit problem is studied in the case where the
pay-off distributions are stationary $\varphi$-mixing. This version of the
problem provides a more realistic model for most real-world applications, but
cannot be optimally solved in practice, since it is known to be PSPACE-hard.
The objective of this paper is to characterize a sub-class of the problem where
{\em good} approximate solutions can be found using tractable approaches.
Specifically, it is shown that under some conditions on the $\varphi$-mixing
coefficients, a modified version of UCB can prove effective. The main challenge
is that, unlike in the i.i.d. setting, the distributions of the sampled
pay-offs may not have the same characteristics as those of the original bandit
arms. In particular, the $\varphi$-mixing property does not necessarily carry
over. This is overcome by carefully controlling the effect of a sampling policy
on the pay-off distributions. Some of the proof techniques developed in this
paper can be more generally used in the context of online sampling under
dependence. Proposed algorithms are accompanied with corresponding regret
analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Grunewalder_S/0/1/0/all/0/1&quot;&gt;Steffen Grunewalder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Khaleghi_A/0/1/0/all/0/1&quot;&gt;Azadeh Khaleghi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.00707">
<title>ELFI: Engine for Likelihood-Free Inference. (arXiv:1708.00707v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.00707</link>
<description rdf:parseType="Literal">&lt;p&gt;Engine for Likelihood-Free Inference (ELFI) is a Python software library for
performing likelihood-free inference (LFI). ELFI provides a convenient syntax
for arranging components in LFI, such as priors, simulators, summaries or
distances, to a network called ELFI graph. The components can be implemented in
a wide variety of languages. The stand-alone ELFI graph can be used with any of
the available inference methods without modifications. A central method
implemented in ELFI is Bayesian Optimization for Likelihood-Free Inference
(BOLFI), which has recently been shown to accelerate likelihood-free inference
up to several orders of magnitude by surrogate-modelling the distance. ELFI
also has an inbuilt support for output data storing for reuse and analysis, and
supports parallelization of computation from multiple cores up to a cluster
environment. ELFI is designed to be extensible and provides interfaces for
widening its functionality. This makes the adding of new inference methods to
ELFI straightforward and automatically compatible with the inbuilt features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lintusaari_J/0/1/0/all/0/1&quot;&gt;Jarno Lintusaari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vuollekoski_H/0/1/0/all/0/1&quot;&gt;Henri Vuollekoski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kangasraasio_A/0/1/0/all/0/1&quot;&gt;Antti Kangasr&amp;#xe4;&amp;#xe4;si&amp;#xf6;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Skyten_K/0/1/0/all/0/1&quot;&gt;Kusti Skyt&amp;#xe9;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jarvenpaa_M/0/1/0/all/0/1&quot;&gt;Marko J&amp;#xe4;rvenp&amp;#xe4;&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marttinen_P/0/1/0/all/0/1&quot;&gt;Pekka Marttinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gutmann_M/0/1/0/all/0/1&quot;&gt;Michael U. Gutmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vehtari_A/0/1/0/all/0/1&quot;&gt;Aki Vehtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Corander_J/0/1/0/all/0/1&quot;&gt;Jukka Corander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kaski_S/0/1/0/all/0/1&quot;&gt;Samuel Kaski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03065">
<title>Generating Realistic Geology Conditioned on Physical Measurements with Generative Adversarial Networks. (arXiv:1802.03065v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03065</link>
<description rdf:parseType="Literal">&lt;p&gt;An important problem in geostatistics is to build models of the subsurface of
the Earth given physical measurements at sparse spatial locations. Typically,
this is done using spatial interpolation methods or by reproducing patterns
from a reference image. However, these algorithms fail to produce realistic
patterns and do not exhibit the wide range of uncertainty inherent in the
prediction of geology. In this paper, we show how semantic inpainting with
Generative Adversarial Networks can be used to generate varied realizations of
geology which honor physical measurements while matching the expected
geological patterns. In contrast to other algorithms, our method scales well
with the number of data points and mimics a distribution of patterns as opposed
to a single pattern or image. The generated conditional samples are state of
the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dupont_E/0/1/0/all/0/1&quot;&gt;Emilien Dupont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tuanfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tilke_P/0/1/0/all/0/1&quot;&gt;Peter Tilke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Lin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bailey_W/0/1/0/all/0/1&quot;&gt;William Bailey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04846">
<title>State Space Gaussian Processes with Non-Gaussian Likelihood. (arXiv:1802.04846v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04846</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide a comprehensive overview and tooling for GP modeling with
non-Gaussian likelihoods using state space methods. The state space formulation
allows for solving one-dimensional GP models in $\mathcal{O}(n)$ time and
memory complexity. While existing literature has focused on the connection
between GP regression and state space methods, the computational primitives
allowing for inference using general likelihoods in combination with the
Laplace approximation (LA), variational Bayes (VB), and assumed density
filtering (ADF, a.k.a. single-sweep expectation propagation, EP) schemes has
been largely overlooked. We present means of combining the efficient
$\mathcal{O}(n)$ state space methodology with existing inference methods. We
extend existing methods, and provide unifying code implementing all approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nickisch_H/0/1/0/all/0/1&quot;&gt;Hannes Nickisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Solin_A/0/1/0/all/0/1&quot;&gt;Arno Solin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grigorievskiy_A/0/1/0/all/0/1&quot;&gt;Alexander Grigorievskiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01257">
<title>Nonnegative Matrix Factorization for Signal and Data Analytics: Identifiability, Algorithms, and Applications. (arXiv:1803.01257v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01257</link>
<description rdf:parseType="Literal">&lt;p&gt;Nonnegative matrix factorization (NMF) has become a workhorse for signal and
data analytics, triggered by its model parsimony and interpretability. Perhaps
a bit surprisingly, the understanding to its model identifiability---the major
reason behind the interpretability in many applications such as topic mining
and hyperspectral imaging---had been rather limited until recent years.
Beginning from the 2010s, the identifiability research of NMF has progressed
considerably: Many interesting and important results have been discovered by
the signal processing (SP) and machine learning (ML) communities. NMF
identifiability has a great impact on many aspects in practice, such as
ill-posed formulation avoidance and performance-guaranteed algorithm design. On
the other hand, there is no tutorial paper that introduces NMF from an
identifiability viewpoint. In this paper, we aim at filling this gap by
offering a comprehensive and deep tutorial on model identifiability of NMF as
well as the connections to algorithms and applications. This tutorial will help
researchers and graduate students grasp the essence and insights of NMF,
thereby avoiding typical `pitfalls&apos; that are often times due to unidentifiable
NMF formulations. This paper will also help practitioners pick/design suitable
factorization tools for their own problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xiao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kejun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sidiropoulos_N/0/1/0/all/0/1&quot;&gt;Nicholas D. Sidiropoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wing-Kin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02603">
<title>Gaussian Process Latent Variable Alignment Learning. (arXiv:1803.02603v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02603</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a model that can automatically learn alignments between
high-dimensional data in an unsupervised manner. Our proposed method casts
alignment learning in a framework where both alignment and data are modelled
simultaneously. Further, we automatically infer groupings of different types of
sequence within the same dataset. We derive a probabilistic model built on
non-parametric priors that allows for flexible warps while at the same time
providing means to specify interpretable constraints. We demonstrate the
efficacy of our approach with superior quantitative performance to the
state-of-the-art approaches and provide examples to illustrate the versatility
of our model in automatic inference of sequence groupings, absent from previous
approaches, as well as easy specification of high level priors for different
modalities of data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kazlauskaite_I/0/1/0/all/0/1&quot;&gt;Ieva Kazlauskaite&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ek_C/0/1/0/all/0/1&quot;&gt;Carl Henrik Ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Campbell_N/0/1/0/all/0/1&quot;&gt;Neill D. F. Campbell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11274">
<title>PIMKL: Pathway Induced Multiple Kernel Learning. (arXiv:1803.11274v3 [q-bio.MN] UPDATED)</title>
<link>http://arxiv.org/abs/1803.11274</link>
<description rdf:parseType="Literal">&lt;p&gt;Reliable identification of molecular biomarkers is essential for accurate
patient stratification. While state-of-the-art machine learning approaches for
sample classification continue to push boundaries in terms of performance, most
of these methods are not able to integrate different data types and lack
generalization power, limiting their application in a clinical setting.
Furthermore, many methods behave as black boxes, and we have very little
understanding about the mechanisms that lead to the prediction. While
opaqueness concerning machine behaviour might not be a problem in deterministic
domains, in health care, providing explanations about the molecular factors and
phenotypes that are driving the classification is crucial to build trust in the
performance of the predictive system. We propose Pathway Induced Multiple
Kernel Learning (PIMKL), a novel methodology to reliably classify samples that
can also help gain insights into the molecular mechanisms that underlie the
classification. PIMKL exploits prior knowledge in the form of a molecular
interaction network and annotated gene sets, by optimizing a mixture of
pathway-induced kernels using a Multiple Kernel Learning (MKL) algorithm, an
approach that has demonstrated excellent performance in different machine
learning applications. After optimizing the combination of kernels for
prediction of a specific phenotype, the model provides a stable molecular
signature that can be interpreted in the light of the ingested prior knowledge
and that can be used in transfer learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Manica_M/0/1/0/all/0/1&quot;&gt;Matteo Manica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cadow_J/0/1/0/all/0/1&quot;&gt;Joris Cadow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mathis_R/0/1/0/all/0/1&quot;&gt;Roland Mathis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Martinez_M/0/1/0/all/0/1&quot;&gt;Mar&amp;#xed;a Rodr&amp;#xed;guez Mart&amp;#xed;nez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01851">
<title>Pathwise Derivatives Beyond the Reparameterization Trick. (arXiv:1806.01851v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01851</link>
<description rdf:parseType="Literal">&lt;p&gt;We observe that gradients computed via the reparameterization trick are in
direct correspondence with solutions of the transport equation in the formalism
of optimal transport. We use this perspective to compute (approximate) pathwise
gradients for probability distributions not directly amenable to the
reparameterization trick: Gamma, Beta, and Dirichlet. We further observe that
when the reparameterization trick is applied to the Cholesky-factorized
multivariate Normal distribution, the resulting gradients are suboptimal in the
sense of optimal transport. We derive the optimal gradients and show that they
have reduced variance in a Gaussian Process regression task. We demonstrate
with a variety of synthetic experiments and stochastic variational inference
tasks that our pathwise gradients are competitive with other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jankowiak_M/0/1/0/all/0/1&quot;&gt;Martin Jankowiak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Obermeyer_F/0/1/0/all/0/1&quot;&gt;Fritz Obermeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02185">
<title>Boosting Black Box Variational Inference. (arXiv:1806.02185v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02185</link>
<description rdf:parseType="Literal">&lt;p&gt;Approximating a probability density in a tractable manner is a central task
in Bayesian statistics. Variational Inference (VI) is a popular technique that
achieves tractability by choosing a relatively simple variational family.
Borrowing ideas from the classic boosting framework, recent approaches attempt
to \emph{boost} VI by replacing the selection of a single density with a
greedily constructed mixture of densities. In order to guarantee convergence,
previous works impose stringent assumptions that require significant effort for
practitioners. Specifically, they require a custom implementation of the greedy
step (called the LMO) for every probabilistic model with respect to an
unnatural variational family of truncated distributions. Our work fixes these
issues with novel theoretical and algorithmic insights. On the theoretical
side, we show that boosting VI satisfies a relaxed smoothness assumption which
is sufficient for the convergence of the functional Frank-Wolfe (FW) algorithm.
Furthermore, we rephrase the LMO problem and propose to maximize the Residual
ELBO (RELBO) which replaces the standard ELBO optimization in VI. These
theoretical enhancements allow for black box implementation of the boosting
subroutine. Finally, we present a stopping criterion drawn from the duality gap
in the classic FW analyses and exhaustive experiments to illustrate the
usefulness of our theoretical and algorithmic contributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dresdner_G/0/1/0/all/0/1&quot;&gt;Gideon Dresdner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khanna_R/0/1/0/all/0/1&quot;&gt;Rajiv Khanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Valera_I/0/1/0/all/0/1&quot;&gt;Isabel Valera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1&quot;&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02954">
<title>Using Social Network Information in Bayesian Truth Discovery. (arXiv:1806.02954v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02954</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the problem of truth discovery based on opinions from multiple
agents who may be unreliable or biased. We consider the case where agents&apos;
reliabilities or biases are correlated if they belong to the same community,
which defines a group of agents with similar opinions regarding a particular
event. An agent can belong to different communities for different events, and
these communities are unknown a priori. We incorporate knowledge of the agents&apos;
social network in our truth discovery framework and develop Laplace variational
inference methods to estimate agents&apos; reliabilities, communities, and the event
states. We also develop a stochastic variational inference method to scale our
model to large social networks. Simulations and experiments on real data
suggest that when observations are sparse, our proposed methods perform better
than several other inference methods, including majority voting, TruthFinder,
AccuSim, the Confidence-Aware Truth Discovery method, the Bayesian Classifier
Combination (BCC) method, and the Community BCC method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jielong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junshan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_W/0/1/0/all/0/1&quot;&gt;Wee Peng Tay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08295">
<title>How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments. (arXiv:1806.08295v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.08295</link>
<description rdf:parseType="Literal">&lt;p&gt;Consistently checking the statistical significance of experimental results is
one of the mandatory methodological steps to address the so-called
&quot;reproducibility crisis&quot; in deep reinforcement learning. In this tutorial
paper, we explain how the number of random seeds relates to the probabilities
of statistical errors. For both the t-test and the bootstrap confidence
interval test, we recall theoretical guidelines to determine the number of
random seeds one should use to provide a statistically significant comparison
of the performance of two algorithms. Finally, we discuss the influence of
deviations from the assumptions usually made by statistical tests. We show that
they can lead to inaccurate evaluations of statistical errors and provide
guidelines to counter these negative effects. We make our code available to
perform the tests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Colas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1&quot;&gt;Olivier Sigaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10175">
<title>The Sparse Recovery Autoencoder. (arXiv:1806.10175v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.10175</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear encoding of sparse vectors is widely popular, but is most commonly
data-independent -- missing any possible extra (but a-priori unknown) structure
beyond sparsity. In this paper we present a new method to learn linear encoders
that adapt to data, while still performing well with the widely used $\ell_1$
decoder. The convex $\ell_1$ decoder prevents gradient propagation as needed in
standard autoencoder training. Our method is based on the insight that
unfolding the convex decoder into $T$ projected gradient steps can address this
issue. Our method can be seen as a data-driven way to learn a compressed
sensing matrix. Our experiments show that there is indeed additional structure
beyond sparsity in several real datasets. Our autoencoder is able to discover
it and exploit it to create excellent reconstructions with fewer measurements
compared to the previous state of the art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shanshan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dimakis_A/0/1/0/all/0/1&quot;&gt;Alexandros G. Dimakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sanghavi_S/0/1/0/all/0/1&quot;&gt;Sujay Sanghavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Felix X. Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Holtmann_Rice_D/0/1/0/all/0/1&quot;&gt;Daniel Holtmann-Rice&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Storcheus_D/0/1/0/all/0/1&quot;&gt;Dmitry Storcheus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rostamizadeh_A/0/1/0/all/0/1&quot;&gt;Afshin Rostamizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sanjiv Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10586">
<title>Approximability of Discriminators Implies Diversity in GANs. (arXiv:1806.10586v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.10586</link>
<description rdf:parseType="Literal">&lt;p&gt;While Generative Adversarial Networks (GANs) have empirically produced
impressive results on learning complex real-world distributions, recent work
has shown that they suffer from lack of diversity or mode collapse. The
theoretical work of Arora et al. suggests a dilemma about GANs&apos; statistical
properties: powerful discriminators cause overfitting, whereas weak
discriminators cannot detect mode collapse.
&lt;/p&gt;
&lt;p&gt;In contrast, we show in this paper that GANs can in principle learn
distributions in Wasserstein distance (or KL-divergence in many cases) with
polynomial sample complexity, if the discriminator class has strong
distinguishing power against the particular generator class (instead of against
all possible generators). For various generator classes such as mixture of
Gaussians, exponential families, and invertible neural networks generators, we
design corresponding discriminators (which are often neural nets of specific
architectures) such that the Integral Probability Metric (IPM) induced by the
discriminators can provably approximate the Wasserstein distance and/or
KL-divergence. This implies that if the training is successful, then the
learned distribution is close to the true distribution in Wasserstein distance
or KL divergence, and thus cannot drop modes. Our preliminary experiments show
that on synthetic datasets the test IPM is well correlated with KL divergence,
indicating that the lack of diversity may be caused by the sub-optimality in
optimization instead of statistical inefficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yu Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1&quot;&gt;Andrej Risteski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11038">
<title>Neural Network Cognitive Engine for Autonomous and Distributed Underlay Dynamic Spectrum Access. (arXiv:1806.11038v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.11038</link>
<description rdf:parseType="Literal">&lt;p&gt;An important challenge in underlay dynamic spectrum access (DSA) is how to
establish an interference limit for the primary network (PN) and how cognitive
radios (CRs) in the secondary network (SN) become aware of their created
interference on the PN, especially when there is no exchange of information
between the primary and the secondary networks. This challenge is addressed in
this paper by present- ing a fully autonomous and distributed underlay DSA
scheme where each CR operates based on predicting its transmission effect on
the PN. The scheme is based on a cognitive engine with an artificial neural
network that predicts, without exchanging information between the networks, the
adaptive modulation and coding configuration for the primary link nearest to a
transmitting CR. By managing the tradeoff between the effect of the SN on the
PN and the achievable throughput at the SN, the presented technique maintains
the change in the PN relative average throughput within a prescribed maximum
value, while also finding transmit settings for the CRs that result in
throughput as large as allowed by the PN interference limit. Moreover, the
proposed technique increases the CRs transmission opportunities compared to a
scheme that can only estimate the modulation scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_F/0/1/0/all/0/1&quot;&gt;Fatemeh Shah Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwasinski_A/0/1/0/all/0/1&quot;&gt;Andres Kwasinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00243">
<title>chemmodlab: A Cheminformatics Modeling Laboratory for Fitting and Assessing Machine Learning Models. (arXiv:1807.00243v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00243</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of chemmodlab is to streamline the fitting and assessment pipeline
for many machine learning models in R, making it easy for researchers to
compare the utility of new models. While focused on implementing methods for
model fitting and assessment that have been accepted by experts in the
cheminformatics field, all of the methods in chemmodlab have broad utility for
the machine learning community. chemmodlab contains several assessment
utilities including a plotting function that constructs accumulation curves and
a function that computes many performance measures. The most novel feature of
chemmodlab is the ease with which statistically significant performance
differences for many machine learning models is presented by means of the
multiple comparisons similarity plot. Differences are assessed using repeated
k-fold cross validation where blocking increases precision and multiplicity
adjustments are applied.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hughes_Oliver_J/0/1/0/all/0/1&quot;&gt;Jeremy R. Ash Jacqueline M. Hughes-Oliver&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00905">
<title>Learning under selective labels in the presence of expert consistency. (arXiv:1807.00905v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00905</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the problem of learning under selective labels in the context of
algorithm-assisted decision making. Selective labels is a pervasive selection
bias problem that arises when historical decision making blinds us to the true
outcome for certain instances. Examples of this are common in many
applications, ranging from predicting recidivism using pre-trial release data
to diagnosing patients. In this paper we discuss why selective labels often
cannot be effectively tackled by standard methods for adjusting for sample
selection bias, even if there are no unobservables. We propose a data
augmentation approach that can be used to either leverage expert consistency to
mitigate the partial blindness that results from selective labels, or to
empirically validate whether learning under such framework may lead to
unreliable models prone to systemic discrimination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+De_Arteaga_M/0/1/0/all/0/1&quot;&gt;Maria De-Arteaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1&quot;&gt;Artur Dubrawski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chouldechova_A/0/1/0/all/0/1&quot;&gt;Alexandra Chouldechova&lt;/a&gt;</dc:creator>
</item></rdf:RDF>