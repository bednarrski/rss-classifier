<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07697"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.09832"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.04404"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05734"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07257"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07461"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07489"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07512"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07564"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07606"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07687"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06768"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07295"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07307"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07309"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07330"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07372"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07389"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07400"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07434"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07444"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07481"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07510"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07513"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07528"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07572"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07575"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07581"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07648"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.01559"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.07892"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.00074"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.06972"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.08085"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.11140"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05413"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04715"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04911"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05074"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05968"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07182"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.02727"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06942"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.07697">
<title>Approximation Algorithms for Cascading Prediction Models. (arXiv:1802.07697v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07697</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an approximation algorithm that takes a pool of pre-trained models
as input and produces from it a cascaded model with similar accuracy but lower
average-case cost. Applied to state-of-the-art ImageNet classification models,
this yields up to a 2x reduction in floating point multiplications, and up to a
6x reduction in average-case memory I/O. The auto-generated cascades exhibit
intuitive properties, such as using lower-resolution input for easier images
and requiring higher prediction confidence when using a computationally cheaper
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Streeter_M/0/1/0/all/0/1&quot;&gt;Matthew Streeter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.09832">
<title>Model based learning for accelerated, limited-view 3D photoacoustic tomography. (arXiv:1708.09832v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1708.09832</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep learning for tomographic reconstructions have shown
great potential to create accurate and high quality images with a considerable
speed-up. In this work we present a deep neural network that is specifically
designed to provide high resolution 3D images from restricted photoacoustic
measurements. The network is designed to represent an iterative scheme and
incorporates gradient information of the data fit to compensate for limited
view artefacts. Due to the high complexity of the photoacoustic forward
operator, we separate training and computation of the gradient information. A
suitable prior for the desired image structures is learned as part of the
training. The resulting network is trained and tested on a set of segmented
vessels from lung CT scans and then applied to in-vivo photoacoustic
measurement data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1&quot;&gt;Andreas Hauptmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucka_F/0/1/0/all/0/1&quot;&gt;Felix Lucka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Betcke_M/0/1/0/all/0/1&quot;&gt;Marta Betcke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1&quot;&gt;Nam Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adler_J/0/1/0/all/0/1&quot;&gt;Jonas Adler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cox_B/0/1/0/all/0/1&quot;&gt;Ben Cox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beard_P/0/1/0/all/0/1&quot;&gt;Paul Beard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arridge_S/0/1/0/all/0/1&quot;&gt;Simon Arridge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.04404">
<title>Sum-Product-Quotient Networks. (arXiv:1710.04404v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.04404</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel tractable generative model that extends Sum-Product
Networks (SPNs) and significantly boosts their power. We call it
Sum-Product-Quotient Networks (SPQNs), whose core concept is to incorporate
conditional distributions into the model by direct computation using quotient
nodes, e.g. $P(A|B) = \frac{P(A,B)}{P(B)}$. We provide sufficient conditions
for the tractability of SPQNs that generalize and relax the decomposable and
complete tractability conditions of SPNs. These relaxed conditions give rise to
an exponential boost to the expressive efficiency of our model, i.e. we prove
that there are distributions which SPQNs can compute efficiently but require
SPNs to be of exponential size. Thus, we narrow the gap in expressivity between
tractable graphical models and other Neural Network-based generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharir_O/0/1/0/all/0/1&quot;&gt;Or Sharir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1&quot;&gt;Amnon Shashua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05734">
<title>Chipmunk: A Systolically Scalable 0.9 mm${}^2$, 3.08 Gop/s/mW @ 1.2 mW Accelerator for Near-Sensor Recurrent Neural Network Inference. (arXiv:1711.05734v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05734</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks (RNNs) are state-of-the-art in voice
awareness/understanding and speech recognition. On-device computation of RNNs
on low-power mobile and wearable devices would be key to applications such as
zero-latency voice-based human-machine interfaces. Here we present Chipmunk, a
small (&amp;lt;1 mm${}^2$) hardware accelerator for Long-Short Term Memory RNNs in UMC
65 nm technology capable to operate at a measured peak efficiency up to 3.08
Gop/s/mW at 1.24 mW peak power. To implement big RNN models without incurring
in huge memory transfer overhead, multiple Chipmunk engines can cooperate to
form a single systolic array. In this way, the Chipmunk architecture in a 75
tiles configuration can achieve real-time phoneme extraction on a demanding RNN
topology proposed by Graves et al., consuming less than 13 mW of average power.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conti_F/0/1/0/all/0/1&quot;&gt;Francesco Conti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavigelli_L/0/1/0/all/0/1&quot;&gt;Lukas Cavigelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulin_G/0/1/0/all/0/1&quot;&gt;Gianna Paulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susmelj_I/0/1/0/all/0/1&quot;&gt;Igor Susmelj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07257">
<title>Predicting Natural Hazards with Neuronal Networks. (arXiv:1802.07257v1 [eess.IV])</title>
<link>http://arxiv.org/abs/1802.07257</link>
<description rdf:parseType="Literal">&lt;p&gt;Gravitational mass flows, such as avalanches, debris flows and rockfalls are
common events in alpine regions with high impact on transport routes. Within
the last few decades, hazard zone maps have been developed to systematically
approach this threat. These maps mark vulnerable zones in habitable areas to
allow effective planning of hazard mitigation measures and development of
settlements. Hazard zone maps have shown to be an effective tool to reduce
fatalities during extreme events. They are created in a complex process, based
on experience, empirical models, physical simulations and historical data. The
generation of such maps is therefore expensive and limited to crucially
important regions, e.g. permanently inhabited areas.
&lt;/p&gt;
&lt;p&gt;In this work we interpret the task of hazard zone mapping as a classification
problem. Every point in a specific area has to be classified according to its
vulnerability. On a regional scale this leads to a segmentation problem, where
the total area has to be divided in the respective hazard zones. The recent
developments in artificial intelligence, namely convolutional neuronal
networks, have led to major improvement in a very similar task, image
classification and semantic segmentation, i.e. computer vision. We use a
convolutional neuronal network to identify terrain formations with the
potential for catastrophic snow avalanches and label points in their reach as
vulnerable. Repeating this procedure for all points allows us to generate an
artificial hazard zone map. We demonstrate that the approach is feasible and
promising based on the hazard zone map of the Tirolean Oberland. However, more
training data and further improvement of the method is required before such
techniques can be applied reliably.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rauter_M/0/1/0/all/0/1&quot;&gt;Matthias Rauter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Winkler_D/0/1/0/all/0/1&quot;&gt;Daniel Winkler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07461">
<title>Emergence of Structured Behaviors from Curiosity-Based Intrinsic Motivation. (arXiv:1802.07461v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07461</link>
<description rdf:parseType="Literal">&lt;p&gt;Infants are experts at playing, with an amazing ability to generate novel
structured behaviors in unstructured environments that lack clear extrinsic
reward signals. We seek to replicate some of these abilities with a neural
network that implements curiosity-driven intrinsic motivation. Using a simple
but ecologically naturalistic simulated environment in which the agent can move
and interact with objects it sees, the agent learns a world model predicting
the dynamic consequences of its actions. Simultaneously, the agent learns to
take actions that adversarially challenge the developing world model, pushing
the agent to explore novel and informative interactions with its environment.
We demonstrate that this policy leads to the self-supervised emergence of a
spectrum of complex behaviors, including ego motion prediction, object
attention, and object gathering. Moreover, the world model that the agent
learns supports improved performance on object dynamics prediction and
localization tasks. Our results are a proof-of-principle that computational
models of intrinsic motivation might account for key features of developmental
visuomotor learning in infants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haber_N/0/1/0/all/0/1&quot;&gt;Nick Haber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mrowca_D/0/1/0/all/0/1&quot;&gt;Damian Mrowca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1&quot;&gt;Daniel L. K. Yamins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07489">
<title>Epistemic Graphs for Representing and Reasoning with Positive and Negative Influences of Arguments. (arXiv:1802.07489v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.07489</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces epistemic graphs as a generalization of the epistemic
approach to probabilistic argumentation. In these graphs, an argument can be
believed or disbelieved up to a given degree, thus providing a more
fine--grained alternative to the standard Dung&apos;s approaches when it comes to
determining the status of a given argument. Furthermore, the flexibility of the
epistemic approach allows us to both model the rationale behind the existing
semantics as well as completely deviate from them when required. Epistemic
graphs can model both attack and support as well as relations that are neither
support nor attack. The way other arguments influence a given argument is
expressed by the epistemic constraints that can restrict the belief we have in
an argument with a varying degree of specificity. The fact that we can specify
the rules under which arguments should be evaluated and we can include
constraints between unrelated arguments permits the framework to be more
context--sensitive. It also allows for better modelling of imperfect agents,
which can be important in multi--agent applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hunter_A/0/1/0/all/0/1&quot;&gt;Anthony Hunter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polberg_S/0/1/0/all/0/1&quot;&gt;Sylwia Polberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thimm_M/0/1/0/all/0/1&quot;&gt;Matthias Thimm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07512">
<title>Density Weighted Connectivity of Grass Pixels in Image Frames for Biomass Estimation. (arXiv:1802.07512v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.07512</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate estimation of the biomass of roadside grasses plays a significant
role in applications such as fire-prone region identification. Current
solutions heavily depend on field surveys, remote sensing measurements and
image processing using reference markers, which often demand big investments of
time, effort and cost. This paper proposes Density Weighted Connectivity of
Grass Pixels (DWCGP) to automatically estimate grass biomass from roadside
image data. The DWCGP calculates the length of continuously connected grass
pixels along a vertical orientation in each image column, and then weights the
length by the grass density in a surrounding region of the column. Grass pixels
are classified using feedforward artificial neural networks and the dominant
texture orientation at every pixel is computed using multi-orientation Gabor
wavelet filter vote. Evaluations on a field survey dataset show that the DWCGP
reduces Root-Mean-Square Error from 5.84 to 5.52 by additionally considering
grass density on top of grass height. The DWCGP shows robustness to
non-vertical grass stems and to changes of both Gabor filter parameters and
surrounding region widths. It also has performance close to human observation
and higher than eight baseline approaches, as well as promising results for
classifying low vs. high fire risk and identifying fire-prone road regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Ligang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_B/0/1/0/all/0/1&quot;&gt;Brijesh Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stockwell_D/0/1/0/all/0/1&quot;&gt;David Stockwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1&quot;&gt;Sujan Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07564">
<title>Clipped Action Policy Gradient. (arXiv:1802.07564v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07564</link>
<description rdf:parseType="Literal">&lt;p&gt;Many continuous control tasks have bounded action spaces and clip
out-of-bound actions before execution. Policy gradient methods often optimize
policies as if actions were not clipped. We propose clipped action policy
gradient (CAPG) as an alternative policy gradient estimator that exploits the
knowledge of actions being clipped to reduce the variance in estimation. We
prove that CAPG is unbiased and achieves lower variance than the original
estimator that ignores action bounds. Experimental results demonstrate that
CAPG generally outperforms the original estimator, indicating its promise as a
better policy gradient estimator for continuous control tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujita_Y/0/1/0/all/0/1&quot;&gt;Yasuhiro Fujita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maeda_S/0/1/0/all/0/1&quot;&gt;Shin-ichi Maeda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07606">
<title>Ordered Preference Elicitation Strategies for Supporting Multi-Objective Decision Making. (arXiv:1802.07606v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07606</link>
<description rdf:parseType="Literal">&lt;p&gt;In multi-objective decision planning and learning, much attention is paid to
producing optimal solution sets that contain an optimal policy for every
possible user preference profile. We argue that the step that follows, i.e,
determining which policy to execute by maximising the user&apos;s intrinsic utility
function over this (possibly infinite) set, is under-studied. This paper aims
to fill this gap. We build on previous work on Gaussian processes and pairwise
comparisons for preference modelling, extend it to the multi-objective decision
support scenario, and propose new ordered preference elicitation strategies
based on ranking and clustering. Our main contribution is an in-depth
evaluation of these strategies using computer and human-based experiments. We
show that our proposed elicitation strategies outperform the currently used
pairwise methods, and found that users prefer ranking most. Our experiments
further show that utilising monotonicity information in GPs by using a linear
prior mean at the start and virtual comparisons to the nadir and ideal points,
increases performance. We demonstrate our decision support framework in a
real-world study on traffic regulation, conducted with the city of Amsterdam.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zintgraf_L/0/1/0/all/0/1&quot;&gt;Luisa M Zintgraf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roijers_D/0/1/0/all/0/1&quot;&gt;Diederik M Roijers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linders_S/0/1/0/all/0/1&quot;&gt;Sjoerd Linders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jonker_C/0/1/0/all/0/1&quot;&gt;Catholijn M Jonker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowe_A/0/1/0/all/0/1&quot;&gt;Ann Now&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07687">
<title>Stochastic Video Generation with a Learned Prior. (arXiv:1802.07687v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.07687</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating video frames that accurately predict future world states is
challenging. Existing approaches either fail to capture the full distribution
of outcomes, or yield blurry generations, or both. In this paper we introduce
an unsupervised video generation model that learns a prior model of uncertainty
in a given environment. Video frames are generated by drawing samples from this
prior and combining them with a deterministic estimate of the future frame. The
approach is simple and easily trained end-to-end on a variety of datasets.
Sample generations are both varied and sharp, even many frames into the future,
and compare favorably to those from existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denton_E/0/1/0/all/0/1&quot;&gt;Emily Denton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1&quot;&gt;Rob Fergus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06768">
<title>Design and software implementation of subsystems for creating and using the ontological base of a research scientist. (arXiv:1802.06768v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06768</link>
<description rdf:parseType="Literal">&lt;p&gt;Creation of the information systems and tools for scientific research and
development support has always been one of the central directions of the
development of computer science. The main features of the modern evolution of
scientific research and development are the transdisciplinary approach and the
deep intellectualisation of all stages of the life cycle of formulation and
solution of scientific problems. The theoretical and practical aspects of the
development of perspective complex knowledge-oriented information systems and
their components are considered in the paper. The analysis of existing
scientific information systems (or current research information systems, CRIS)
and synthesis of general principles of design of the research and development
workstation environment of a researcher and its components are carried out in
the work. The functional components of knowledge-oriented information system
research and development workstation environment of a researcher are designed.
Designed and developed functional components of knowledge-oriented information
system developing research and development workstation environment,including
functional models and software implementation of the software subsystem for
creation and use of ontological knowledge base for research fellow
publications, as part of personalized knowledge base of scientific researcher.
Research in modern conditions of e-Science paradigm requires pooling scientific
community and intensive exchange of research results that may be achieved
through the use of scientific information systems. research and development
workstation environment allows to solve problems of contructivisation and
formalisation of knowledge representation, obtained during the research process
and collective accomplices interaction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palagin_O/0/1/0/all/0/1&quot;&gt;O. V. Palagin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malakhov_K/0/1/0/all/0/1&quot;&gt;K. S. Malakhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velichko_V/0/1/0/all/0/1&quot;&gt;V. Yu. Velichko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shurov_O/0/1/0/all/0/1&quot;&gt;O. S. Shurov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07295">
<title>Attack Strength vs. Detectability Dilemma in Adversarial Machine Learning. (arXiv:1802.07295v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07295</link>
<description rdf:parseType="Literal">&lt;p&gt;As the prevalence and everyday use of machine learning algorithms, along with
our reliance on these algorithms grow dramatically, so do the efforts to attack
and undermine these algorithms with malicious intent, resulting in a growing
interest in adversarial machine learning. A number of approaches have been
developed that can render a machine learning algorithm ineffective through
poisoning or other types of attacks. Most attack algorithms typically use
sophisticated optimization approaches, whose objective function is designed to
cause maximum damage with respect to accuracy and performance of the algorithm
with respect to some task. In this effort, we show that while such an objective
function is indeed brutally effective in causing maximum damage on an embedded
feature selection task, it often results in an attack mechanism that can be
easily detected with an embarrassingly simple novelty or outlier detection
algorithm. We then propose an equally simple yet elegant solution by adding a
regularization term to the attacker&apos;s objective function that penalizes
outlying attack points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frederickson_C/0/1/0/all/0/1&quot;&gt;Christopher Frederickson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moore_M/0/1/0/all/0/1&quot;&gt;Michael Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dawson_G/0/1/0/all/0/1&quot;&gt;Glenn Dawson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Polikar_R/0/1/0/all/0/1&quot;&gt;Robi Polikar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07307">
<title>Unsupervised Phase Mapping of X-ray Diffraction Data by Nonnegative Matrix Factorization Integrated with Custom Clustering. (arXiv:1802.07307v1 [cond-mat.mtrl-sci])</title>
<link>http://arxiv.org/abs/1802.07307</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing large X-ray diffraction (XRD) datasets is a key step in
high-throughput mapping of the compositional phase diagrams of combinatorial
materials libraries. Optimizing and automating this task can help accelerate
the process of discovery of materials with novel and desirable properties.
Here, we report a new method for pattern analysis and phase extraction of XRD
datasets. The method expands the Nonnegative Matrix Factorization method, which
has been used previously to analyze such datasets, by combining it with custom
clustering and cross-correlation algorithms. This new method is capable of
robust determination of the number of basis patterns present in the data which,
in turn, enables straightforward identification of any possible peak-shifted
patterns. Peak-shifting arises due to continuous change in the lattice
constants as a function of composition, and is ubiquitous in XRD datasets from
composition spread libraries. Successful identification of the peak-shifted
patterns allows proper quantification and classification of the basis XRD
patterns, which is necessary in order to decipher the contribution of each
unique single-phase structure to the multi-phase regions. The process can be
utilized to determine accurately the compositional phase diagram of a system
under study. The presented method is applied to one synthetic and one
experimental dataset, and demonstrates robust accuracy and identification
abilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Stanev_V/0/1/0/all/0/1&quot;&gt;Valentin Stanev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Vesselinov_V/0/1/0/all/0/1&quot;&gt;Velimir V. Vesselinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Kusne_A/0/1/0/all/0/1&quot;&gt;A. Gilad Kusne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Antoszewski_G/0/1/0/all/0/1&quot;&gt;Graham Antoszewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Takeuchi_I/0/1/0/all/0/1&quot;&gt;Ichiro Takeuchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Alexandrov_B/0/1/0/all/0/1&quot;&gt;Boian S. Alexandrov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07309">
<title>Detection limits in the high-dimensional spiked rectangular model. (arXiv:1802.07309v1 [math.ST])</title>
<link>http://arxiv.org/abs/1802.07309</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of detecting the presence of a single unknown spike in a
rectangular data matrix, in a high-dimensional regime where the spike has fixed
strength and the aspect ratio of the matrix converges to a finite limit. This
situation comprises Johnstone&apos;s spiked covariance model. We analyze the
likelihood ratio of the spiked model against an &quot;all noise&quot; null model of
reference, and show it has asymptotically Gaussian fluctuations in a region
below---but in general not up to---the so-called BBP threshold from random
matrix theory. Our result parallels earlier findings of Onatski et al.\ (2013)
and Johnstone-Onatski (2015) for spherical spikes. We present a probabilistic
approach capable of treating generic product priors. In particular, sparsity in
the spike is allowed. Our approach operates through the general principle of
the cavity method from spin-glass theory. The question of the maximal parameter
region where asymptotic normality is expected to hold is left open. This region
is shaped by the prior in a non-trivial way. We conjecture that this is the
entire paramagnetic phase of an associated spin-glass model, and is defined by
the vanishing of the replica-symmetric solution of Lesieur et al.\ (2015).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Alaoui_A/0/1/0/all/0/1&quot;&gt;Ahmed El Alaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07329">
<title>Bayesian Incremental Learning for Deep Neural Networks. (arXiv:1802.07329v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07329</link>
<description rdf:parseType="Literal">&lt;p&gt;In industrial machine learning pipelines, data often arrive in parts.
Particularly in the case of deep neural networks, it may be too expensive to
train the model from scratch each time, so one would rather use a previously
learned model and the new data to improve performance. However, deep neural
networks are prone to getting stuck in a suboptimal solution when trained on
only new data as compared to the full dataset. Our work focuses on a continuous
learning setup where the task is always the same and new parts of data arrive
sequentially. We apply a Bayesian approach to update the posterior
approximation with each new piece of data and find this method to outperform
the traditional approach in our experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kochurov_M/0/1/0/all/0/1&quot;&gt;Max Kochurov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garipov_T/0/1/0/all/0/1&quot;&gt;Timur Garipov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Podoprikhin_D/0/1/0/all/0/1&quot;&gt;Dmitry Podoprikhin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Molchanov_D/0/1/0/all/0/1&quot;&gt;Dmitry Molchanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ashukha_A/0/1/0/all/0/1&quot;&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vetrov_D/0/1/0/all/0/1&quot;&gt;Dmitry Vetrov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07330">
<title>A folded model for compositional data analysis. (arXiv:1802.07330v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07330</link>
<description rdf:parseType="Literal">&lt;p&gt;A folded type model is developed for analyzing compositional data. The
proposed model, which is based upon the $\alpha$-transformation for
compositional data, provides a new and flexible class of distributions for
modeling data defined on the simplex sample space. Despite its rather seemingly
complex structure, employment of the EM algorithm guarantees efficient
parameter estimation. The model is validated through simulation studies and
examples which illustrate that the proposed model performs better in terms of
capturing the data structure, when compared to the popular logistic normal
distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tsagris_M/0/1/0/all/0/1&quot;&gt;Michail Tsagris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stewart_C/0/1/0/all/0/1&quot;&gt;Connie Stewart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07372">
<title>Sample Complexity of Stochastic Variance-Reduced Cubic Regularization for Nonconvex Optimization. (arXiv:1802.07372v1 [math.OC])</title>
<link>http://arxiv.org/abs/1802.07372</link>
<description rdf:parseType="Literal">&lt;p&gt;The popular cubic regularization (CR) method converges with first- and
second-order optimality guarantee for nonconvex optimization, but encounters a
high sample complexity issue for solving large-scale problems. Various
sub-sampling variants of CR have been proposed to improve the sample
complexity.In this paper, we propose a stochastic variance-reduced
cubic-regularized (SVRC) Newton&apos;s method under both sampling with and without
replacement schemes. We characterize the per-iteration sample complexity bounds
which guarantee the same rate of convergence as that of CR for nonconvex
optimization. Furthermore, our method achieves a total Hessian sample
complexity of $\mathcal{O}(N^{8/11} \epsilon^{-3/2})$ and
$\mathcal{O}(N^{3/4}\epsilon^{-3/2})$ respectively under sampling without and
with replacement, which improve that of CR as well as other sub-sampling
variant methods via the variance reduction scheme. Our result also suggests
that sampling without replacement yields lower sample complexity than that of
sampling with replacement. We further compare the practical performance of SVRC
with other cubic regularization methods via experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingbin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lan_G/0/1/0/all/0/1&quot;&gt;Guanghui Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07389">
<title>3LC: Lightweight and Effective Traffic Compression for Distributed Machine Learning. (arXiv:1802.07389v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07389</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance and efficiency of distributed machine learning (ML) depends
significantly on how long it takes for nodes to exchange state changes.
Overly-aggressive attempts to reduce communication often sacrifice final model
accuracy and necessitate additional ML techniques to compensate for this loss,
limiting their generality. Some attempts to reduce communication incur high
computation overhead, which makes their performance benefits visible only over
slow networks.
&lt;/p&gt;
&lt;p&gt;We present 3LC, a lossy compression scheme for state change traffic that
strikes balance between multiple goals: traffic reduction, accuracy,
computation overhead, and generality. It combines three new
techniques---3-value quantization with sparsity multiplication, quartic
encoding, and zero-run encoding---to leverage strengths of quantization and
sparsification techniques and avoid their drawbacks. It achieves a data
compression ratio of up to 39--107X, almost the same test accuracy of trained
models, and high compression speed. Distributed ML frameworks can employ 3LC
without modifications to existing ML algorithms. Our experiments show that 3LC
reduces wall-clock training time of ResNet-110--based image classifiers for
CIFAR-10 on a 10-GPU cluster by up to 16--23X compared to TensorFlow&apos;s baseline
design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1&quot;&gt;Hyeontaek Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andersen_D/0/1/0/all/0/1&quot;&gt;David G. Andersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaminsky_M/0/1/0/all/0/1&quot;&gt;Michael Kaminsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07400">
<title>Direct Learning to Rank and Rerank. (arXiv:1802.07400v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07400</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-to-rank techniques have proven to be extremely useful for
prioritization problems, where we rank items in order of their estimated
probabilities, and dedicate our limited resources to the top-ranked items. This
work exposes a serious problem with the state of learning-to-rank algorithms,
which is that they are based on convex proxies that lead to poor
approximations. We then discuss the possibility of &quot;exact&quot; reranking algorithms
based on mathematical programming. We prove that a relaxed version of the
&quot;exact&quot; problem has the same optimal solution, and provide an empirical
analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yining Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07434">
<title>Nonparametric Bayesian Sparse Graph Linear Dynamical Systems. (arXiv:1802.07434v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07434</link>
<description rdf:parseType="Literal">&lt;p&gt;A nonparametric Bayesian sparse graph linear dynamical system (SGLDS) is
proposed to model sequentially observed multivariate data. SGLDS uses the
Bernoulli-Poisson link together with a gamma process to generate an infinite
dimensional sparse random graph to model state transitions. Depending on the
sparsity pattern of the corresponding row and column of the graph affinity
matrix, a latent state of SGLDS can be categorized as either a non-dynamic
state or a dynamic one. A normal-gamma construction is used to shrink the
energy captured by the non-dynamic states, while the dynamic states can be
further categorized into live, absorbing, or noise-injection states, which
capture different types of dynamical components of the underlying time series.
The state-of-the-art performance of SGLDS is demonstrated with experiments on
both synthetic and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kalantari_R/0/1/0/all/0/1&quot;&gt;Rahi Kalantari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghosh_J/0/1/0/all/0/1&quot;&gt;Joydeep Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07444">
<title>Scaling-up Split-Merge MCMC with Locality Sensitive Sampling (LSS). (arXiv:1802.07444v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07444</link>
<description rdf:parseType="Literal">&lt;p&gt;Split-Merge MCMC (Monte Carlo Markov Chain) is one of the essential and
popular variants of MCMC for problems when an MCMC state consists of an unknown
number of components or clusters. It is well known that state-of-the-art
methods for split-merge MCMC do not scale well. Strategies for rapid mixing
requires smart and informative proposals to reduce the rejection rate. However,
all known smart proposals involve cost at least linear in the size of the data
$ \ge O(N)$, to suggest informative transitions. Thus, the cost of each
iteration is prohibitive for massive scale datasets. It is further known that
uninformative but computationally efficient proposals, such as random
split-merge, leads to extremely slow convergence. This tradeoff between mixing
time and per update cost seems hard to get around. In this paper, we get around
this tradeoff by utilizing simple similarity information, such as cosine
similarity, between the entity vectors to design a proposal distribution. Such
information is readily available in almost all applications. We show that the
recent use of locality sensitive hashing for efficient adaptive sampling can be
leveraged to obtain a computationally efficient pseudo-marginal MCMC. The new
split-merge MCMC has constant time update, just like random split-merge, and at
the same time the proposal is informative and needs significantly fewer
iterations than random split-merge. Overall, we obtain a sweet tradeoff between
convergence and per update cost. As a direct consequence, our proposal, named
LSHSM, is around 10x faster than the state-of-the-art sampling methods on both
synthetic datasets and two large real datasets KDDCUP and PubMed with several
millions of entities and thousands of cluster centers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1&quot;&gt;Anshumali Shrivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07481">
<title>Dual Extrapolation for Faster Lasso Solvers. (arXiv:1802.07481v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07481</link>
<description rdf:parseType="Literal">&lt;p&gt;Convex sparsity-inducing regularizations are ubiquitous in high-dimension
machine learning, but their non-differentiability requires the use of iterative
solvers. To accelerate such solvers, state-of-the-art approaches consist in
reducing the size of the optimization problem at hand. In the context of
regression, this can be achieved either by discarding irrelevant features
(screening techniques) or by prioritizing features likely to be included in the
support of the solution (working set techniques). Duality comes into play at
several steps in these techniques. Here, we propose an extrapolation technique
starting from a sequence of iterates in the dual that leads to the construction
of an improved dual point. This enables a tighter control of optimality as used
in stopping criterion, as well as better screening performance of Gap Safe
rules. Finally, we propose a working set strategy based on an aggressive use of
Gap Safe rules and our new dual point construction, which improves
state-of-the-art time performance on Lasso problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mathurin_M/0/1/0/all/0/1&quot;&gt;Massias Mathurin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alexandre_G/0/1/0/all/0/1&quot;&gt;Gramfort Alexandre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Joseph_S/0/1/0/all/0/1&quot;&gt;Salmon Joseph&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07510">
<title>Spectrally approximating large graphs with smaller graphs. (arXiv:1802.07510v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07510</link>
<description rdf:parseType="Literal">&lt;p&gt;How does coarsening affect the spectrum of a general graph? We provide
conditions such that the principal eigenvalues and eigenspaces of a coarsened
and original graph Laplacian matrices are close. The achieved approximation is
shown to depend on standard graph-theoretic properties, such as the degree and
eigenvalue distributions, as well as on the ratio between the coarsened and
actual graph sizes. Our results carry implications for learning methods that
utilize coarsening. For the particular case of spectral clustering, they imply
that coarse eigenvectors can be used to derive good quality assignments even
without refinement---this phenomenon was previously observed, but lacked formal
justification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1&quot;&gt;Andreas Loukas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vandergheynst_P/0/1/0/all/0/1&quot;&gt;Pierre Vandergheynst&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07513">
<title>Adversarial classification: An adversarial risk analysis approach. (arXiv:1802.07513v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07513</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification problems in security settings are usually contemplated as
confrontations in which one or more adversaries try to fool a classifier to
obtain a benefit. Most approaches to such adversarial classification problems
have focused on game theoretical ideas with strong underlying common knowledge
assumptions, which are actually not realistic in security domains. We provide
an alternative framework to such problem based on adversarial risk analysis,
which we illustrate with several examples. Computational and implementation
issues are discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Naveiro_R/0/1/0/all/0/1&quot;&gt;Roi Naveiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Redondo_A/0/1/0/all/0/1&quot;&gt;Alberto Redondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Insua_D/0/1/0/all/0/1&quot;&gt;David R&amp;#xed;os Insua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ruggeri_F/0/1/0/all/0/1&quot;&gt;Fabrizio Ruggeri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07528">
<title>Subspace-Induced Gaussian Processes. (arXiv:1802.07528v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07528</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new Gaussian process (GP) regression model where the covariance
kernel is indexed or parameterized by a sufficient dimension reduction subspace
of a reproducing kernel Hilbert space. The covariance kernel will be low-rank
while capturing the statistical dependency of the response to the covariates,
this affords significant improvement in computational efficiency as well as
potential reduction in the variance of predictions. We develop a fast
Expectation-Maximization algorithm for estimating the parameters of the
subspace-induced Gaussian process (SIGP). Extensive results on real data show
that SIGP can outperform the standard full GP even with a low rank-$m$, $m\leq
3$, inducing subspace.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zilong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Sayan Mukherjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07572">
<title>Information Theoretic Co-Training. (arXiv:1802.07572v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.07572</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces an information theoretic co-training objective for
unsupervised learning. We consider the problem of predicting the future. Rather
than predict future sensations (image pixels or sound waves) we predict
&quot;hypotheses&quot; to be confirmed by future sensations. More formally, we assume a
population distribution on pairs $(x,y)$ where we can think of $x$ as a past
sensation and $y$ as a future sensation. We train both a predictor model
$P_\Phi(z|x)$ and a confirmation model $P_\Psi(z|y)$ where we view $z$ as
hypotheses (when predicted) or facts (when confirmed). For a population
distribution on pairs $(x,y)$ we focus on the problem of measuring the mutual
information between $x$ and $y$. By the data processing inequality this mutual
information is at least as large as the mutual information between $x$ and $z$
under the distribution on triples $(x,z,y)$ defined by the confirmation model
$P_\Psi(z|y)$. The information theoretic training objective for $P_\Phi(z|x)$
and $P_\Psi(z|y)$ can be viewed as a form of co-training where we want the
prediction from $x$ to match the confirmation from $y$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAllester_D/0/1/0/all/0/1&quot;&gt;David McAllester&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07575">
<title>Emulating dynamic non-linear simulators using Gaussian processes. (arXiv:1802.07575v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07575</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we examine the emulation of non-linear deterministic computer
codes where the output is a time series, possibly multivariate. Such computer
models simulate the evolution of some real-world phenomena over time, for
example models of the climate or the functioning of the human brain. The models
we are interested in are highly non-linear and exhibit tipping points,
bifurcations and chaotic behaviour. Each simulation run is too time-consuming
to perform naive uncertainty quantification. We therefore build emulators using
Gaussian processes to model the output of the code. We use the Gaussian process
to predict one-step ahead in an iterative way over the whole time series. We
consider a number of ways to propagate uncertainty through the time series
including both the uncertainty of inputs to the emulators at time t and the
correlation between them. The methodology is illustrated with a number of
examples. These include the highly non-linear dynamical systems described by
the Lorenz and Van der Pol equations. In both cases we will show that we not
only have very good predictive performance but also have measures of
uncertainty that reflect what is known about predictability in each system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mohammadi_H/0/1/0/all/0/1&quot;&gt;Hossein Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Challenor_P/0/1/0/all/0/1&quot;&gt;Peter Challenor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goodfellow_M/0/1/0/all/0/1&quot;&gt;Marc Goodfellow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07581">
<title>Universal Hypothesis Testing with Kernels: Asymptotically Optimal Tests for Goodness of Fit. (arXiv:1802.07581v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.07581</link>
<description rdf:parseType="Literal">&lt;p&gt;We characterize the asymptotic performance of nonparametric goodness of fit
testing, otherwise known as the universal hypothesis testing that dates back to
Hoeffding (1965). The exponential decay rate of the type-II error probability
is used as the asymptotic performance metric, hence an optimal test achieves
the maximum decay rate subject to a constant level constraint on the type-I
error probability. We show that two classes of Maximum Mean Discrepancy (MMD)
based tests attain this optimality on $\mathbb R^d$, while a Kernel Stein
Discrepancy (KSD) based test achieves a weaker one under this criterion. In the
finite sample regime, these tests have similar statistical performance in our
experiments, while the KSD based test is more computationally efficient. Key to
our approach are Sanov&apos;s theorem from large deviation theory and recent results
on the weak convergence properties of the MMD and KSD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shengyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Biao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_P/0/1/0/all/0/1&quot;&gt;Pengfei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhitang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07648">
<title>Scalable and Robust Sparse Subspace Clustering Using Randomized Clustering and Multilayer Graphs. (arXiv:1802.07648v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.07648</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse subspace clustering (SSC) is one of the current state-of-the-art
method for partitioning data points into the union of subspaces, with strong
theoretical guarantees. However, it is not practical for large data sets as it
requires solving a LASSO problem for each data point, where the number of
variables in each LASSO problem is the number of data points. To improve the
scalability of SSC, we propose to select a few sets of anchor points using a
randomized hierarchical clustering method, and, for each set of anchor points,
solve the LASSO problems for each data point allowing only anchor points to
have a non-zero weight (this reduces drastically the number of variables). This
generates a multilayer graph where each layer corresponds to a different set of
anchor points. Using the Grassmann manifold of orthogonal matrices, the shared
connectivity among the layers is summarized within a single subspace. Finally,
we use $k$-means clustering within that subspace to cluster the data points,
similarly as done by spectral clustering in SSC. We show on both synthetic and
real-world data sets that the proposed method not only allows SSC to scale to
large-scale data sets, but that it is also much more robust as it performs
significantly better on noisy data and on data with close susbspaces and
outliers, while it is not prone to oversegmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdolali_M/0/1/0/all/0/1&quot;&gt;Maryam Abdolali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gillis_N/0/1/0/all/0/1&quot;&gt;Nicolas Gillis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmati_M/0/1/0/all/0/1&quot;&gt;Mohammad Rahmati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.01559">
<title>High-dimensional Bayesian inference via the Unadjusted Langevin Algorithm. (arXiv:1605.01559v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1605.01559</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider in this paper the problem of sampling a high-dimensional
probability distribution $\pi$ having a density \wrt\ the Lebesgue measure on
$\mathbb{R}^d$, known up to a normalization factor $x \mapsto \pi(x)=
\mathrm{e}^{-U(x)}/\int_{\mathbb{R}^d} \mathrm{e}^{-U(y)} \mathrm{d}y$. Such
problem naturally occurs for example in Bayesian inference and machine
learning. Under the assumption that $U$ is continuously differentiable, $\nabla
U$ is globally Lipschitz and $U$ is strongly convex, we obtain non-asymptotic
bounds for the convergence to stationarity in Wasserstein distance of order $2$
and total variation distance of the sampling method based on the Euler
discretization of the Langevin stochastic differential equation, for both
constant and decreasing step sizes. The dependence on the dimension of the
state space of the obtained bounds is studied to demonstrate the applicability
of this method. The convergence of an appropriately weighted empirical measure
is also investigated and bounds for the mean square error and exponential
deviation inequality are reported for functions which are measurable and
bounded. An illustration to Bayesian inference for binary regression is
presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Durmus_A/0/1/0/all/0/1&quot;&gt;Alain Durmus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Moulines_E/0/1/0/all/0/1&quot;&gt;Eric Moulines&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.07892">
<title>Optimizing Recurrent Neural Networks Architectures under Time Constraints. (arXiv:1608.07892v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1608.07892</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural network (RNN)&apos;s architecture is a key factor influencing its
performance. We propose algorithms to optimize hidden sizes under running time
constraint. We convert the discrete optimization into a subset selection
problem. By novel transformations, the objective function becomes submodular
and constraint becomes supermodular. A greedy algorithm with bounds is
suggested to solve the transformed problem. And we show how transformations
influence the bounds. To speed up optimization, surrogate functions are
proposed which balance exploration and exploitation. Experiments show that our
algorithms can find more accurate models or faster models than manually tuned
state-of-the-art and random search. We also compare popular RNN architectures
using our algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Junqi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Ziang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kun Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changshui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.00074">
<title>Neural Network Architecture Optimization through Submodularity and Supermodularity. (arXiv:1609.00074v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1609.00074</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models&apos; architectures, including depth and width, are key
factors influencing models&apos; performance, such as test accuracy and computation
time. This paper solves two problems: given computation time budget, choose an
architecture to maximize accuracy, and given accuracy requirement, choose an
architecture to minimize computation time. We convert this architecture
optimization into a subset selection problem. With accuracy&apos;s submodularity and
computation time&apos;s supermodularity, we propose efficient greedy optimization
algorithms. The experiments demonstrate our algorithm&apos;s ability to find more
accurate models or faster models. By analyzing architecture evolution with
growing time budget, we discuss relationships among accuracy, time and
architecture, and give suggestions on neural network architecture design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Junqi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Ziang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kun Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changshui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.06972">
<title>Measuring Sample Quality with Diffusions. (arXiv:1611.06972v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.06972</link>
<description rdf:parseType="Literal">&lt;p&gt;Stein&apos;s method for measuring convergence to a continuous target distribution
relies on an operator characterizing the target and Stein factor bounds on the
solutions of an associated differential equation. While such operators and
bounds are readily available for a diversity of univariate targets, few
multivariate targets have been analyzed. We introduce a new class of
characterizing operators based on Ito diffusions and develop explicit
multivariate Stein factor bounds for any target with a fast-coupling Ito
diffusion. As example applications, we develop computable and
convergence-determining diffusion Stein discrepancies for log-concave,
heavy-tailed, and multimodal targets and use these quality measures to select
the hyperparameters of biased Markov chain Monte Carlo (MCMC) samplers, compare
random and deterministic quadrature rules, and quantify bias-variance tradeoffs
in approximate MCMC. Our results establish a near-linear relationship between
diffusion Stein discrepancies and Wasserstein distances, improving upon past
work even for strongly log-concave targets. The exposed relationship between
Stein factors and Markov process coupling may be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gorham_J/0/1/0/all/0/1&quot;&gt;Jackson Gorham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duncan_A/0/1/0/all/0/1&quot;&gt;Andrew B. Duncan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vollmer_S/0/1/0/all/0/1&quot;&gt;Sebastian J. Vollmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.08085">
<title>Reducing Crowdsourcing to Graphon Estimation, Statistically. (arXiv:1703.08085v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.08085</link>
<description rdf:parseType="Literal">&lt;p&gt;Inferring the correct answers to binary tasks based on multiple noisy answers
in an unsupervised manner has emerged as the canonical question for micro-task
crowdsourcing or more generally aggregating opinions. In graphon estimation,
one is interested in estimating edge intensities or probabilities between nodes
using a single snapshot of a graph realization. In the recent literature, there
has been exciting development within both of these topics. In the context of
crowdsourcing, the key intellectual challenge is to understand whether a given
task can be more accurately denoised by aggregating answers collected from
other different tasks. In the context of graphon estimation, precise
information limits and estimation algorithms remain of interest. In this paper,
we utilize a statistical reduction from crowdsourcing to graphon estimation to
advance the state-of-art for both of these challenges. We use concepts from
graphon estimation to design an algorithm that achieves better performance than
the {\em majority voting} scheme for a setup that goes beyond the {\em rank
one} models considered in the literature. We use known explicit lower bounds
for crowdsourcing to provide refined lower bounds for graphon estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shah_D/0/1/0/all/0/1&quot;&gt;Devavrat Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Christina Lee Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.11140">
<title>Variational Sequential Monte Carlo. (arXiv:1705.11140v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.11140</link>
<description rdf:parseType="Literal">&lt;p&gt;Many recent advances in large scale probabilistic inference rely on
variational methods. The success of variational approaches depends on (i)
formulating a flexible parametric family of distributions, and (ii) optimizing
the parameters to find the member of this family that most closely approximates
the exact posterior. In this paper we present a new approximating family of
distributions, the variational sequential Monte Carlo (VSMC) family, and show
how to optimize it in variational inference. VSMC melds variational inference
(VI) and sequential Monte Carlo (SMC), providing practitioners with flexible,
accurate, and powerful Bayesian inference. The VSMC family is a variational
family that can approximate the posterior arbitrarily well, while still
allowing for efficient optimization of its parameters. We demonstrate its
utility on state space models, stochastic volatility models for financial data,
and deep Markov models of brain neural circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Naesseth_C/0/1/0/all/0/1&quot;&gt;Christian A. Naesseth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Linderman_S/0/1/0/all/0/1&quot;&gt;Scott W. Linderman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1&quot;&gt;Rajesh Ranganath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05413">
<title>Combinatorial Preconditioners for Proximal Algorithms on Graphs. (arXiv:1801.05413v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1801.05413</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel preconditioning technique for proximal optimization
methods that relies on graph algorithms to construct effective preconditioners.
Such combinatorial preconditioners arise from partitioning the graph into
forests. We prove that certain decompositions lead to a theoretically optimal
condition number. We also show how ideal decompositions can be realized using
matroid partitioning and propose efficient greedy variants thereof for
large-scale problems. Coupled with specialized solvers for the resulting scaled
proximal subproblems, the preconditioned algorithm achieves competitive
performance in machine learning and vision applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mollenhoff_T/0/1/0/all/0/1&quot;&gt;Thomas M&amp;#xf6;llenhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zhenzhang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04715">
<title>Online Variance Reduction for Stochastic Optimization. (arXiv:1802.04715v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04715</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern stochastic optimization methods often rely on uniform sampling which
is agnostic to the underlying characteristics of the data. This might degrade
the convergence by yielding estimates that suffer from a high variance. A
possible remedy is to employ non-uniform importance sampling techniques, which
take the structure of the dataset into account. In this work, we investigate a
recently proposed setting which poses variance reduction as an online
optimization problem with bandit feedback. We devise a novel and efficient
algorithm for this setting that finds a sequence of importance sampling
distributions competitive with the best fixed distribution in hindsight, the
first result of this kind. While we present our method for sampling datapoints,
it naturally extends to selecting coordinates or even blocks of thereof.
Empirical validations underline the benefits of our method in several settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Borsos_Z/0/1/0/all/0/1&quot;&gt;Zal&amp;#xe1;n Borsos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Levy_K/0/1/0/all/0/1&quot;&gt;Kfir Y. Levy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04911">
<title>Linear-Time Algorithm for Learning Large-Scale Sparse Graphical Models. (arXiv:1802.04911v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04911</link>
<description rdf:parseType="Literal">&lt;p&gt;The sparse inverse covariance estimation problem is commonly solved using an
$\ell_{1}$-regularized Gaussian maximum likelihood estimator known as
&quot;graphical lasso&quot;, but its computational cost becomes prohibitive for large
data sets. A recent line of results showed--under mild assumptions--that the
graphical lasso estimator can be retrieved by soft-thresholding the sample
covariance matrix and solving a maximum determinant matrix completion (MDMC)
problem. This paper proves an extension of this result, and describes a
Newton-CG algorithm to efficiently solve the MDMC problem. Assuming that the
thresholded sample covariance matrix is sparse with a sparse Cholesky
factorization, we prove that the algorithm converges to an $\epsilon$-accurate
solution in $O(n\log(1/\epsilon))$ time and $O(n)$ memory. The algorithm is
highly efficient in practice: we solve the associated MDMC problems with as
many as 200,000 variables to 7-9 digits of accuracy in less than an hour on a
standard laptop computer running MATLAB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Richard Y. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fattahi_S/0/1/0/all/0/1&quot;&gt;Salar Fattahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sojoudi_S/0/1/0/all/0/1&quot;&gt;Somayeh Sojoudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05074">
<title>L4: Practical loss-based stepsize adaptation for deep learning. (arXiv:1802.05074v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05074</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a stepsize adaptation scheme for stochastic gradient descent. It
operates directly with the loss function and rescales the gradient in order to
make fixed predicted progress on the loss. We demonstrate its capabilities by
strongly improving the performance of Adam and Momentum optimizers. The
enhanced optimizers with default hyperparameters consistently outperform their
constant stepsize counterparts, even the best ones, without a measurable
increase in computational cost. The performance is validated on multiple
architectures including ResNets and the Differential Neural Computer. A
prototype implementation as a TensorFlow optimizer is released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolinek_M/0/1/0/all/0/1&quot;&gt;Michal Rolinek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martius_G/0/1/0/all/0/1&quot;&gt;Georg Martius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05968">
<title>Information Theory: A Tutorial Introduction. (arXiv:1802.05968v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05968</link>
<description rdf:parseType="Literal">&lt;p&gt;Shannon&apos;s mathematical theory of communication defines fundamental limits on
how much information can be transmitted between the different components of any
man-made or biological system. This paper is an informal but rigorous
introduction to the main ideas implicit in Shannon&apos;s theory. An annotated
reading list is provided for further reading.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_J/0/1/0/all/0/1&quot;&gt;James V Stone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07182">
<title>The Gaussian Process Autoregressive Regression Model (GPAR). (arXiv:1802.07182v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07182</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-output regression models must exploit dependencies between outputs to
maximise predictive performance. The application of Gaussian processes (GPs) to
this setting typically yields models that are computationally demanding and
have limited representational power. We present the Gaussian Process
Autoregressive Regression (GPAR) model, a scalable multi-output GP model that
is able to capture nonlinear, possibly input-varying, dependencies between
outputs in a simple and tractable way: the product rule is used to decompose
the joint distribution over the outputs into a set of conditionals, each of
which is modelled by a standard GP. GPAR&apos;s efficacy is demonstrated on a
variety of synthetic and real-world problems, outperforming existing GP models
and achieving state-of-the-art performance on the tasks with existing
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Requeima_J/0/1/0/all/0/1&quot;&gt;James Requeima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tebbutt_W/0/1/0/all/0/1&quot;&gt;Will Tebbutt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bruinsma_W/0/1/0/all/0/1&quot;&gt;Wessel Bruinsma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard E. Turner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.02727">
<title>Machine learning modeling of superconducting critical temperature. (arXiv:1709.02727v2 [cond-mat.supr-con] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1709.02727</link>
<description rdf:parseType="Literal">&lt;p&gt;Superconductivity has been the focus of enormous research effort since its
discovery more than a century ago. Yet, some features of this unique phenomenon
remain poorly understood; prime among these is the connection between
superconductivity and chemical/structural properties of materials. To bridge
the gap, several machine learning schemes are developed herein to model the
critical temperatures ($T_{\mathrm{c}}$) of the 12,000+ known superconductors
available via the SuperCon database. Materials are first divided into two
classes based on their $T_{\mathrm{c}}$ values, above and below 10 K, and a
classification model predicting this label is trained. The model uses
coarse-grained features based only on the chemical compositions. It shows
strong predictive power, with out-of-sample accuracy of about 92%. Separate
regression models are developed to predict the values of $T_{\mathrm{c}}$ for
cuprate, iron-based, and &quot;low-$T_{\mathrm{c}}$&quot; compounds. These models also
demonstrate good performance, with learned predictors offering potential
insights into the mechanisms behind superconductivity in different families of
materials. To improve the accuracy and interpretability of these models, new
features are incorporated using materials data from the AFLOW Online
Repositories. Finally, the classification and regression models are combined
into a single integrated pipeline and employed to search the entire Inorganic
Crystallographic Structure Database (ICSD) for potential new superconductors.
We identify more than 30 non-cuprate and non-iron-based oxides as candidate
materials.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Stanev_V/0/1/0/all/0/1&quot;&gt;Valentin Stanev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Oses_C/0/1/0/all/0/1&quot;&gt;Corey Oses&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Kusne_A/0/1/0/all/0/1&quot;&gt;A. Gilad Kusne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Rodriguez_E/0/1/0/all/0/1&quot;&gt;Efrain Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Paglione_J/0/1/0/all/0/1&quot;&gt;Johnpierre Paglione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Curtarolo_S/0/1/0/all/0/1&quot;&gt;Stefano Curtarolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Takeuchi_I/0/1/0/all/0/1&quot;&gt;Ichiro Takeuchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06942">
<title>Comparison Based Learning from Weak Oracles. (arXiv:1802.06942v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1802.06942</link>
<description rdf:parseType="Literal">&lt;p&gt;There is increasing interest in learning algorithms that involve interaction
between human and machine. Comparison-based queries are among the most natural
ways to get feedback from humans. A challenge in designing comparison-based
interactive learning algorithms is coping with noisy answers. The most common
fix is to submit a query several times, but this is not applicable in many
situations due to its prohibitive cost and due to the unrealistic assumption of
independent noise in different repetitions of the same query.
&lt;/p&gt;
&lt;p&gt;In this paper, we introduce a new weak oracle model, where a non-malicious
user responds to a pairwise comparison query only when she is quite sure about
the answer. This model is able to mimic the behavior of a human in noise-prone
regions. We also consider the application of this weak oracle model to the
problem of content search (a variant of the nearest neighbor search problem)
through comparisons. More specifically, we aim at devising efficient algorithms
to locate a target object in a database equipped with a dissimilarity metric
via invocation of the weak comparison oracle. We propose two algorithms termed
WORCS-I and WORCS-II (Weak-Oracle Comparison-based Search), which provably
locate the target object in a number of comparisons close to the entropy of the
target distribution. While WORCS-I provides better theoretical guarantees,
WORCS-II is applicable to more technically challenging scenarios where the
algorithm has limited access to the ranking dissimilarity between objects. A
series of experiments validate the performance of our proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazemi_E/0/1/0/all/0/1&quot;&gt;Ehsan Kazemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1&quot;&gt;Sanjoy Dasgupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1&quot;&gt;Amin Karbasi&lt;/a&gt;</dc:creator>
</item></rdf:RDF>