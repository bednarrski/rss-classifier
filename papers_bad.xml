<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-16T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05093"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05208"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05267"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05319"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05364"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05429"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05443"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05554"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05650"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.04742"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08760"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00997"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02702"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05012"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05092"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05184"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05212"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05260"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05320"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05435"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05448"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05697"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05741"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05804"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.03074"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.03076"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09956"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01508"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02792"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03824"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04326"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05044"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04789"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05090"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05133"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05146"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05170"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05214"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05345"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05402"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05433"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05474"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05482"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05484"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05494"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05515"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05544"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05589"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05753"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05774"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05805"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05810"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1508.01939"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.02549"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.02724"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.06114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.08683"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.05374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.04191"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.05342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.09953"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04846"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.05093">
<title>Heterogeneous Multilayer Generalized Operational Perceptron. (arXiv:1804.05093v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.05093</link>
<description rdf:parseType="Literal">&lt;p&gt;The traditional Multilayer Perceptron (MLP) using McCulloch-Pitts neuron
model is inherently limited to a set of neuronal activities, i.e., linear
weighted sum followed by nonlinear thresholding step. Previously, Generalized
Operational Perceptron (GOP) was proposed to extend conventional perceptron
model by defining a diverse set of neuronal activities to imitate a generalized
model of biological neurons. Together with GOP, Progressive Operational
Perceptron (POP) algorithm was proposed to optimize a pre-defined template of
multiple homogeneous layers in a layerwise manner. In this paper, we propose an
efficient algorithm to learn a compact, fully heterogeneous multilayer network
that allows each individual neuron, regardless of the layer, to have distinct
characteristics. Based on the complexity of the problem, the proposed algorithm
operates in a progressive manner on a neuronal level, searching for a compact
topology, not only in terms of depth but also width, i.e., the number of
neurons in each layer. The proposed algorithm is shown to outperform other
related learning methods in extensive experiments on several classification
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1&quot;&gt;Dat Thanh Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1&quot;&gt;Alexandros Iosifidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05208">
<title>On Asynchronous Non-Dominated Sorting for Steady-State Multiobjective Evolutionary Algorithms. (arXiv:1804.05208v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1804.05208</link>
<description rdf:parseType="Literal">&lt;p&gt;In parallel and distributed environments, generational evolutionary
algorithms often do not exploit the full potential of the computation system
since they have to wait until the entire population is evaluated before
starting selection procedures. Steady-state algorithms are often seen as a
solution to this problem, since fitness evaluation can be done by multiple
threads in an asynchronous way. However, if the algorithm updates its state in
a complicated way, the threads will eventually have to wait until this update
finishes. State update procedures that are computationally expensive are common
in multiobjective evolutionary algorithms.
&lt;/p&gt;
&lt;p&gt;We have implemented an asynchronous steady-state version of the NSGA-II
algorithm. Its most expensive part, non-dominated sorting, determines the time
needed to update the state. We turned the existing incremental non-dominated
sorting algorithm into an asynchronous one using several concurrency
techniques: a single entry-level lock, finer-grained locks working with
non-domination levels, and a non-blocking approach using compare-and-set
operations. Our experimental results reveal the trade-off between the
work-efficiency of the algorithm and the achieved amount of parallelism.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yakupov_I/0/1/0/all/0/1&quot;&gt;Ilya Yakupov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buzdalov_M/0/1/0/all/0/1&quot;&gt;Maxim Buzdalov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05267">
<title>Low-Precision Floating-Point Schemes for Neural Network Training. (arXiv:1804.05267v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.05267</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of low-precision fixed-point arithmetic along with stochastic
rounding has been proposed as a promising alternative to the commonly used
32-bit floating point arithmetic to enhance training neural networks training
in terms of performance and energy efficiency. In the first part of this paper,
the behaviour of the 12-bit fixed-point arithmetic when training a
convolutional neural network with the CIFAR-10 dataset is analysed, showing
that such arithmetic is not the most appropriate for the training phase. After
that, the paper presents and evaluates, under the same conditions, alternative
low-precision arithmetics, starting with the 12-bit floating-point arithmetic.
These two representations are then leveraged using local scaling in order to
increase accuracy and get closer to the baseline 32-bit floating-point
arithmetic. Finally, the paper introduces a simplified model in which both the
outputs and the gradients of the neural networks are constrained to
power-of-two values, just using 7 bits for their representation. The evaluation
demonstrates a minimal loss in accuracy for the proposed Power-of-Two neural
network, avoiding the use of multiplications and divisions and thereby,
significantly reducing the training time as well as the energy consumption and
memory requirements during the training and inference phases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortiz_M/0/1/0/all/0/1&quot;&gt;Marc Ortiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cristal_A/0/1/0/all/0/1&quot;&gt;Adri&amp;#xe1;n Cristal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayguade_E/0/1/0/all/0/1&quot;&gt;Eduard Ayguad&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_M/0/1/0/all/0/1&quot;&gt;Marc Casas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05319">
<title>Particle Swarm Optimization: A survey of historical and recent developments with hybridization perspectives. (arXiv:1804.05319v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.05319</link>
<description rdf:parseType="Literal">&lt;p&gt;Particle Swarm Optimization (PSO) is a metaheuristic global optimization
paradigm that has gained prominence in the last two decades due to its ease of
application in unsupervised, complex multidimensional problems which cannot be
solved using traditional deterministic algorithms. The canonical particle swarm
optimizer is based on the flocking behavior and social co-operation of birds
and fish schools and draws heavily from the evolutionary behavior of these
organisms. This paper serves to provide a thorough survey of the PSO algorithm
with special emphasis on the development, deployment and improvements of its
most basic as well as some of the state-of-the-art implementations. Concepts
and directions on choosing the inertia weight, constriction factor, cognition
and social weights and perspectives on convergence, parallelization, elitism,
niching and discrete optimization as well as neighborhood topologies are
outlined. Hybridization attempts with other evolutionary and swarm paradigms in
selected applications are covered and an up-to-date review is put forward for
the interested reader.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1&quot;&gt;Saptarshi Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basak_S/0/1/0/all/0/1&quot;&gt;Sanchita Basak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_R/0/1/0/all/0/1&quot;&gt;Richard Alan Peters II&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05364">
<title>Data-efficient Neuroevolution with Kernel-Based Surrogate Models. (arXiv:1804.05364v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.05364</link>
<description rdf:parseType="Literal">&lt;p&gt;Surrogate-assistance approaches have long been used in computationally
expensive domains to improve the data-efficiency of optimization algorithms.
Neuroevolution, however, has so far resisted the application of these
techniques because it requires the surrogate model to make fitness predictions
based on variable topologies, instead of a vector of parameters. Our main
insight is that we can sidestep this problem by using kernel-based surrogate
models, which require only the definition of a distance measure between
individuals. Our second insight is that the well-established Neuroevolution of
Augmenting Topologies (NEAT) algorithm provides a computationally efficient
distance measure between dissimilar networks in the form of &quot;compatibility
distance&quot;, initially designed to maintain topological diversity. Combining
these two ideas, we introduce a surrogate-assisted neuroevolution algorithm
that combines NEAT and a surrogate model built using a compatibility distance
kernel. We demonstrate the data-efficiency of this new algorithm on the low
dimensional cart-pole swing-up problem, as well as the higher dimensional
half-cheetah running task. In both tasks the surrogate-assisted variant
achieves the same or better results with several times fewer function
evaluations as the original NEAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaier_A/0/1/0/all/0/1&quot;&gt;Adam Gaier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asteroth_A/0/1/0/all/0/1&quot;&gt;Alexander Asteroth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Mouret&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05429">
<title>Gnowee: A Hybrid Metaheuristic Optimization Algorithm for Constrained, Black Box, Combinatorial Mixed-Integer Design. (arXiv:1804.05429v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.05429</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Gnowee, a modular, Python-based, open-source hybrid
metaheuristic optimization algorithm (Available from
https://github.com/SlaybaughLab/Gnowee). Gnowee is designed for rapid
convergence to nearly globally optimum solutions for complex, constrained
nuclear engineering problems with mixed-integer and combinatorial design
vectors and high-cost, noisy, discontinuous, black box objective function
evaluations. Gnowee&apos;s hybrid metaheuristic framework is a new combination of a
set of diverse, robust heuristics that appropriately balance diversification
and intensification strategies across a wide range of optimization problems.
&lt;/p&gt;
&lt;p&gt;This novel algorithm was specifically developed to optimize complex nuclear
design problems; the motivating research problem was the design of material
stack-ups to modify neutron energy spectra to specific targeted spectra for
applications in nuclear medicine, technical nuclear forensics, nuclear physics,
etc. However, there are a wider range of potential applications for this
algorithm both within the nuclear community and beyond. To demonstrate Gnowee&apos;s
behavior for a variety of problem types, comparisons between Gnowee and several
well-established metaheuristic algorithms are made for a set of eighteen
continuous, mixed-integer, and combinatorial benchmarks. These results
demonstrate Gnoweee to have superior flexibility and convergence
characteristics over a wide range of design spaces. We anticipate this wide
range of applicability will make this algorithm desirable for many complex
engineering applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bevins_J/0/1/0/all/0/1&quot;&gt;James Bevins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slaybaugh_R/0/1/0/all/0/1&quot;&gt;Rachel Slaybaugh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05443">
<title>Better Fixed-Arity Unbiased Black-Box Algorithms. (arXiv:1804.05443v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.05443</link>
<description rdf:parseType="Literal">&lt;p&gt;In their GECCO&apos;12 paper, Doerr and Doerr proved that the $k$-ary unbiased
black-box complexity of OneMax on $n$ bits is $O(n/k)$ for $2\le k\le\log_2 n$.
We propose an alternative strategy for achieving this unbiased black-box
complexity when $3\le k\le\log_2 n$. While it is based on the same idea of
block-wise optimization, it uses $k$-ary unbiased operators in a different way.
&lt;/p&gt;
&lt;p&gt;For each block of size $2^{k-1}-1$ we set up, in $O(k)$ queries, a virtual
coordinate system, which enables us to use an arbitrary unrestricted algorithm
to optimize this block. This is possible because this coordinate system
introduces a bijection between unrestricted queries and a subset of $k$-ary
unbiased operators. We note that this technique does not depend on OneMax being
solved and can be used in more general contexts.
&lt;/p&gt;
&lt;p&gt;This together constitutes an algorithm which is conceptually simpler than the
one by Doerr and Doerr, and at the same time achieves better constant factors
in the asymptotic notation. Our algorithm works in $(2+o(1))\cdot n/(k-1)$,
where $o(1)$ relates to $k$. Our experimental evaluation of this algorithm
shows its efficiency already for $3\le k\le6$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulanova_N/0/1/0/all/0/1&quot;&gt;Nina Bulanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buzdalov_M/0/1/0/all/0/1&quot;&gt;Maxim Buzdalov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05554">
<title>BinarEye: An Always-On Energy-Accuracy-Scalable Binary CNN Processor With All Memory On Chip in 28nm CMOS. (arXiv:1804.05554v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1804.05554</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces BinarEye: a digital processor for always-on Binary
Convolutional Neural Networks. The chip maximizes data reuse through a Neuron
Array exploiting local weight Flip-Flops. It stores full network models and
feature maps and hence requires no off-chip bandwidth, which leads to a 230
1b-TOPS/W peak efficiency. Its 3 levels of flexibility - (a) weight
reconfiguration, (b) a programmable network depth and (c) a programmable
network width - allow trading energy for accuracy depending on the task&apos;s
requirements. BinarEye&apos;s full system input-to-label energy consumption ranges
from 14.4uJ/f for 86% CIFAR-10 and 98% owner recognition down to 0.92uJ/f for
94% face detection at up to 1700 frames per second. This is 3-12-70x more
efficient than the state-of-the-art at on-par accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moons_B/0/1/0/all/0/1&quot;&gt;Bert Moons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bankman_D/0/1/0/all/0/1&quot;&gt;Daniel Bankman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lita Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murmann_B/0/1/0/all/0/1&quot;&gt;Boris Murmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verhelst_M/0/1/0/all/0/1&quot;&gt;Marian Verhelst&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05650">
<title>Theory of Parameter Control for Discrete Black-Box Optimization: Provable Performance Gains Through Dynamic Parameter Choices. (arXiv:1804.05650v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.05650</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter control aims at realizing performance gains through a dynamic
choice of the parameters which determine the behavior of the underlying
optimization algorithm. In the context of evolutionary algorithms this research
line has for a long time been dominated by empirical approaches. With the
significant advances in running time analysis achieved in the last ten years,
the parameter control question has become accessible to theoretical
investigations. A number of running time results for a broad range of different
parameter control mechanisms have been obtained in recent years. This book
chapter surveys these works, and puts them into context, by proposing an
updated classification scheme for parameter control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_C/0/1/0/all/0/1&quot;&gt;Carola Doerr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.04742">
<title>Imposing higher-level Structure in Polyphonic Music Generation using Convolutional Restricted Boltzmann Machines and Constraints. (arXiv:1612.04742v4 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/1612.04742</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a method for imposing higher-level structure on generated,
polyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a
generative model is combined with gradient descent constraint optimisation to
provide further control over the generation process. Among other things, this
allows for the use of a &quot;template&quot; piece, from which some structural properties
can be extracted, and transferred as constraints to the newly generated
material. The sampling process is guided with Simulated Annealing to avoid
local optima, and to find solutions that both satisfy the constraints, and are
relatively stable with respect to the C-RBM. Results show that with this
approach it is possible to control the higher-level self-similarity structure,
the meter, and the tonal properties of the resulting musical piece, while
preserving its local musical coherence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lattner_S/0/1/0/all/0/1&quot;&gt;Stefan Lattner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grachten_M/0/1/0/all/0/1&quot;&gt;Maarten Grachten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1&quot;&gt;Gerhard Widmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08760">
<title>Sensitivity and Generalization in Neural Networks: an Empirical Study. (arXiv:1802.08760v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08760</link>
<description rdf:parseType="Literal">&lt;p&gt;In practice it is often found that large over-parameterized neural networks
generalize better than their smaller counterparts, an observation that appears
to conflict with classical notions of function complexity, which typically
favor smaller models. In this work, we investigate this tension between
complexity and generalization through an extensive empirical exploration of two
natural metrics of complexity related to sensitivity to input perturbations.
Our experiments survey thousands of models with various fully-connected
architectures, optimizers, and other hyper-parameters, as well as four
different image classification datasets.
&lt;/p&gt;
&lt;p&gt;We find that trained neural networks are more robust to input perturbations
in the vicinity of the training data manifold, as measured by the norm of the
input-output Jacobian of the network, and that it correlates well with
generalization. We further establish that factors associated with poor
generalization $-$ such as full-batch training or using random labels $-$
correspond to lower robustness, while factors associated with good
generalization $-$ such as data augmentation and ReLU non-linearities $-$ give
rise to more robust functions. Finally, we demonstrate how the input-output
Jacobian norm can be predictive of generalization at the level of individual
test points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Novak_R/0/1/0/all/0/1&quot;&gt;Roman Novak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bahri_Y/0/1/0/all/0/1&quot;&gt;Yasaman Bahri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abolafia_D/0/1/0/all/0/1&quot;&gt;Daniel A. Abolafia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pennington_J/0/1/0/all/0/1&quot;&gt;Jeffrey Pennington&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1&quot;&gt;Jascha Sohl-Dickstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00997">
<title>A computational perspective of the role of Thalamus in cognition. (arXiv:1803.00997v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00997</link>
<description rdf:parseType="Literal">&lt;p&gt;Thalamus has traditionally been considered as only a relay source of cortical
inputs, with hierarchically organized cortical circuits serially transforming
thalamic signals to cognitively-relevant representations. Given the absence of
local excitatory connections within the thalamus, the notion of thalamic
`relay&apos; seemed like a reasonable description over the last several decades.
Recent advances in experimental approaches and theory provide a broader
perspective on the role of the thalamus in cognitively-relevant cortical
computations, and suggest that only a subset of thalamic circuit motifs fit the
relay description. Here, we discuss this perspective and highlight the
potential role for the thalamus in dynamic selection of cortical
representations through a combination of intrinsic thalamic computations and
output signals that change cortical network functional parameters. We suggest
that through the contextual modulation of cortical computation, thalamus and
cortex jointly optimize the information/cost tradeoff in an emergent fashion.
We emphasize that coordinated experimental and theoretical efforts will provide
a path to understanding the role of the thalamus in cognition, along with an
understanding to augment cognitive capacity in health and disease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dehghani_N/0/1/0/all/0/1&quot;&gt;Nima Dehghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wimmer_R/0/1/0/all/0/1&quot;&gt;Ralf D. Wimmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02702">
<title>Ordinal Pooling Networks: For Preserving Information over Shrinking Feature Maps. (arXiv:1804.02702v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02702</link>
<description rdf:parseType="Literal">&lt;p&gt;In the framework of convolutional neural networks that lie at the heart of
deep learning, downsampling is often performed with a max-pooling operation
that only retains the element with maximum activation, while completely
discarding the information contained in other elements in a pooling region. To
address this issue, a novel pooling scheme, Ordinal Pooling Network (OPN), is
introduced in this work. OPN rearranges all the elements of a pooling region in
a sequence and assigns different weights to these elements based upon their
orders in the sequence, where the weights are learned via the gradient-based
optimisation. The results of our small-scale experiments on image
classification task demonstrate that this scheme leads to a consistent
improvement in the accuracy over max-pooling operation. This improvement is
expected to increase in deeper networks, where several layers of pooling become
necessary.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Ashwani Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05012">
<title>Representing smooth functions as compositions of near-identity functions with implications for deep network optimization. (arXiv:1804.05012v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05012</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that any smooth bi-Lipschitz $h$ can be represented exactly as a
composition $h_m \circ ... \circ h_1$ of functions $h_1,...,h_m$ that are close
to the identity in the sense that each $\left(h_i-\mathrm{Id}\right)$ is
Lipschitz, and the Lipschitz constant decreases inversely with the number $m$
of functions composed. This implies that $h$ can be represented to any accuracy
by a deep residual network whose nonlinear layers compute functions with a
small Lipschitz constant. Next, we consider nonlinear regression with a
composition of near-identity nonlinear maps. We show that, regarding Fr\&apos;echet
derivatives with respect to the $h_1,...,h_m$, any critical point of a
quadratic criterion in this near-identity region must be a global minimizer. In
contrast, if we consider derivatives with respect to parameters of a fixed-size
residual network with sigmoid activation functions, we show that there are
near-identity critical points that are suboptimal, even in the realizable case.
Informally, this means that functional gradient methods for residual networks
cannot get stuck at suboptimal critical points corresponding to near-identity
layers, whereas parametric gradient methods for sigmoidal residual networks
suffer from suboptimal critical points in the near-identity region.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1&quot;&gt;Peter L. Bartlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_S/0/1/0/all/0/1&quot;&gt;Steven N. Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_P/0/1/0/all/0/1&quot;&gt;Philip M. Long&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05092">
<title>A new robust feature selection method using variance-based sensitivity analysis. (arXiv:1804.05092v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.05092</link>
<description rdf:parseType="Literal">&lt;p&gt;Excluding irrelevant features in a pattern recognition task plays an
important role in maintaining a simpler machine learning model and optimizing
the computational efficiency. Nowadays with the rise of large scale datasets,
feature selection is in great demand as it becomes a central issue when facing
high-dimensional datasets. The present study provides a new measure of saliency
for features by employing a Sensitivity Analysis (SA) technique called the
extended Fourier amplitude sensitivity test, and a well-trained Feedforward
Neural Network (FNN) model, which ultimately leads to the selection of a
promising optimal feature subset. Ideas of the paper are mainly demonstrated
based on adopting FNN model for feature selection in classification problems.
But in the end, a generalization framework is discussed in order to give
insights into the usage in regression problems as well as expressing how other
function approximate models can be deployed. Effectiveness of the proposed
method is verified by result analysis and data visualization for a series of
experiments over several well-known datasets drawn from UCI machine learning
repository.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadeghyan_S/0/1/0/all/0/1&quot;&gt;Saman Sadeghyan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05184">
<title>Not all Embeddings are created Equal: Extracting Entity-specific Substructures for RDF Graph Embedding. (arXiv:1804.05184v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.05184</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Graphs (KGs) are becoming essential to information systems that
require access to structured data. Several approaches have been recently
proposed, for obtaining vector representations of KGs suitable for Machine
Learning tasks, based on identifying and extracting relevant graph
substructures using uniform and biased random walks. However, such approaches
lead to representations comprising mostly &quot;popular&quot;, instead of &quot;relevant&quot;,
entities in the KG. In KGs, in which different types of entities often exist
(such as in Linked Open Data), a given target entity may have its own distinct
set of most &quot;relevant&quot; nodes and edges. We propose specificity as an accurate
measure of identifying most relevant, entity-specific, nodes and edges. We
develop a scalable method based on bidirectional random walks to compute
specificity. Our experimental evaluation results show that specificity-based
biased random walks extract more &quot;meaningful&quot; (in terms of size and relevance)
RDF substructures compared to the state-of-the-art and, the graph embedding
learned from the extracted substructures, outperform existing techniques in the
task of entity recommendation in DBpedia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeed_M/0/1/0/all/0/1&quot;&gt;Muhammad Rizwan Saeed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chelmis_C/0/1/0/all/0/1&quot;&gt;Charalampos Chelmis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasanna_V/0/1/0/all/0/1&quot;&gt;Viktor K. Prasanna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05212">
<title>Combining Difficulty Ranking with Multi-Armed Bandits to Sequence Educational Content. (arXiv:1804.05212v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.05212</link>
<description rdf:parseType="Literal">&lt;p&gt;As e-learning systems become more prevalent, there is a growing need for them
to accommodate individual differences between students. This paper addresses
the problem of how to personalize educational content to students in order to
maximize their learning gains over time. We present a new computational
approach to this problem called MAPLE (Multi-Armed Bandits based
Personalization for Learning Environments) that combines difficulty ranking
with multi-armed bandits. Given a set of target questions MAPLE estimates the
expected learning gains for each question and uses an exploration-exploitation
strategy to choose the next question to pose to the student. It maintains a
personalized ranking over the difficulties of question in the target set which
is used in two ways: First, to obtain initial estimates over the learning gains
for the set of questions. Second, to update the estimates over time based on
the students responses. We show in simulations that MAPLE was able to improve
students&apos; learning gains compared to approaches that sequence questions in
increasing level of difficulty, or rely on content experts. When implemented in
a live e-learning system in the wild, MAPLE showed promising results. This work
demonstrates the efficacy of using stochastic approaches to the sequencing
problem when augmented with information about question difficulty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segal_A/0/1/0/all/0/1&quot;&gt;Avi Segal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+David_Y/0/1/0/all/0/1&quot;&gt;Yossi Ben David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1&quot;&gt;Joseph Jay Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gal_K/0/1/0/all/0/1&quot;&gt;Kobi Gal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalom_Y/0/1/0/all/0/1&quot;&gt;Yaar Shalom&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05260">
<title>ClassiNet -- Predicting Missing Features for Short-Text Classification. (arXiv:1804.05260v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.05260</link>
<description rdf:parseType="Literal">&lt;p&gt;The fundamental problem in short-text classification is \emph{feature
sparseness} -- the lack of feature overlap between a trained model and a test
instance to be classified. We propose \emph{ClassiNet} -- a network of
classifiers trained for predicting missing features in a given instance, to
overcome the feature sparseness problem. Using a set of unlabeled training
instances, we first learn binary classifiers as feature predictors for
predicting whether a particular feature occurs in a given instance. Next, each
feature predictor is represented as a vertex $v_i$ in the ClassiNet where a
one-to-one correspondence exists between feature predictors and vertices. The
weight of the directed edge $e_{ij}$ connecting a vertex $v_i$ to a vertex
$v_j$ represents the conditional probability that given $v_i$ exists in an
instance, $v_j$ also exists in the same instance. We show that ClassiNets
generalize word co-occurrence graphs by considering implicit co-occurrences
between features. We extract numerous features from the trained ClassiNet to
overcome feature sparseness. In particular, for a given instance $\vec{x}$, we
find similar features from ClassiNet that did not appear in $\vec{x}$, and
append those features in the representation of $\vec{x}$. Moreover, we propose
a method based on graph propagation to find features that are indirectly
related to a given short-text. We evaluate ClassiNets on several benchmark
datasets for short-text classification. Our experimental results show that by
using ClassiNet, we can statistically significantly improve the accuracy in
short-text classification tasks, without having to use any external resources
such as thesauri for finding related features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1&quot;&gt;Danushka Bollegala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atanasov_V/0/1/0/all/0/1&quot;&gt;Vincent Atanasov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maehara_T/0/1/0/all/0/1&quot;&gt;Takanori Maehara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawarabayashi_K/0/1/0/all/0/1&quot;&gt;Ken-ichi Kawarabayashi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05320">
<title>Generative Adversarial Network based Autoencoder: Application to fault detection problem for closed loop dynamical systems. (arXiv:1804.05320v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.05320</link>
<description rdf:parseType="Literal">&lt;p&gt;Fault detection problem for closed loop uncertain dynamical systems, is
investigated in this paper, using different deep learning based methods.
Traditional classifier based method does not perform well, because of the
inherent difficulty of detecting system level faults for closed loop dynamical
system. Specifically, acting controller in any closed loop dynamical system,
works to reduce the effect of system level faults. A novel Generative
Adversarial based deep Autoencoder is designed to classify datasets under
normal and faulty operating conditions. This proposed network performs
significantly well when compared to any available classifier based methods, and
moreover, does not require labeled fault incorporated datasets for training
purpose. Finally, this aforementioned network&apos;s performance is tested on a high
complexity building energy system dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_I/0/1/0/all/0/1&quot;&gt;Indrasis Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1&quot;&gt;Rudrasis Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vrabie_D/0/1/0/all/0/1&quot;&gt;Draguna Vrabie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05435">
<title>What Happened? Leveraging VerbNet to Predict the Effects of Actions in Procedural Text. (arXiv:1804.05435v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.05435</link>
<description rdf:parseType="Literal">&lt;p&gt;Our goal is to answer questions about paragraphs describing processes (e.g.,
photosynthesis). Texts of this genre are challenging because the effects of
actions are often implicit (unstated), requiring background knowledge and
inference to reason about the changing world states. To supply this knowledge,
we leverage VerbNet to build a rulebase (called the Semantic Lexicon) of the
preconditions and effects of actions, and use it along with commonsense
knowledge of persistence to answer questions about change. Our evaluation shows
that our system, ProComp, significantly outperforms two strong reading
comprehension (RC) baselines. Our contributions are two-fold: the Semantic
Lexicon rulebase itself, and a demonstration of how a simulation-based approach
to machine reading can outperform RC methods that rely on surface cues alone.
&lt;/p&gt;
&lt;p&gt;Since this work was performed, we have developed neural systems that
outperform ProComp, described elsewhere (Dalvi et al., NAACL&apos;18). However, the
Semantic Lexicon remains a novel and potentially useful resource, and its
integration with neural systems remains a currently unexplored opportunity for
further improvements in machine reading about processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1&quot;&gt;Peter Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalvi_B/0/1/0/all/0/1&quot;&gt;Bhavana Dalvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1&quot;&gt;Niket Tandon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05448">
<title>Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions for Video Captioning. (arXiv:1804.05448v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.05448</link>
<description rdf:parseType="Literal">&lt;p&gt;A major challenge for video captioning is to combine audio and visual cues.
Existing multi-modal fusion methods have shown encouraging results in video
understanding. However, the temporal structures of multiple modalities at
different granularities are rarely explored, and how to selectively fuse the
multi-modal representations at different levels of details remains uncharted.
In this paper, we propose a novel hierarchically aligned cross-modal attention
(HACA) framework to learn and selectively fuse both global and local temporal
dynamics of different modalities. Furthermore, for the first time, we validate
the superior performance of the deep audio features on the video captioning
task. Finally, our HACA model significantly outperforms the previous best
systems and achieves new state-of-the-art results on the widely used MSR-VTT
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuan-Fang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05655">
<title>Active Learning for Efficient Testing of Student Programs. (arXiv:1804.05655v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1804.05655</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose an automated method to identify semantic bugs in
student programs, called ATAS, which builds upon the recent advances in both
symbolic execution and active learning. Symbolic execution is a program
analysis technique which can generate test cases through symbolic constraint
solving. Our method makes use of a reference implementation of the task as its
sole input. We compare our method with a symbolic execution-based baseline on 6
programming tasks retrieved from CodeForces comprising a total of 23K student
submissions. We show an average improvement of over 2.5x over the baseline in
terms of runtime (thus making it more suitable for online evaluation), without
a significant degradation in evaluation accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastogi_I/0/1/0/all/0/1&quot;&gt;Ishan Rastogi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1&quot;&gt;Aditya Kanade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shevade_S/0/1/0/all/0/1&quot;&gt;Shirish Shevade&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05697">
<title>A stigmergy-based analysis of city hotspots to discover trends and anomalies in urban transportation usage. (arXiv:1804.05697v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.05697</link>
<description rdf:parseType="Literal">&lt;p&gt;A key aspect of a sustainable urban transportation system is the
effectiveness of transportation policies. To be effective, a policy has to
consider a broad range of elements, such as pollution emission, traffic flow,
and human mobility. Due to the complexity and variability of these elements in
the urban area, to produce effective policies remains a very challenging task.
With the introduction of the smart city paradigm, a widely available amount of
data can be generated in the urban spaces. Such data can be a fundamental
source of knowledge to improve policies because they can reflect the
sustainability issues underlying the city. In this context, we propose an
approach to exploit urban positioning data based on stigmergy, a bio-inspired
mechanism providing scalar and temporal aggregation of samples. By employing
stigmergy, samples in proximity with each other are aggregated into a
functional structure called trail. The trail summarizes relevant dynamics in
data and allows matching them, providing a measure of their similarity.
Moreover, this mechanism can be specialized to unfold specific dynamics.
Specifically, we identify high-density urban areas (i.e hotspots), analyze
their activity over time, and unfold anomalies. Moreover, by matching activity
patterns, a continuous measure of the dissimilarity with respect to the typical
activity pattern is provided. This measure can be used by policy makers to
evaluate the effect of policies and change them dynamically. As a case study,
we analyze taxi trip data gathered in Manhattan from 2013 to 2015.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alfeo_A/0/1/0/all/0/1&quot;&gt;Antonio L. Alfeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cimino_M/0/1/0/all/0/1&quot;&gt;Mario G. C. A. Cimino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egidi_S/0/1/0/all/0/1&quot;&gt;Sara Egidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1&quot;&gt;Bruno Lepri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaglini_G/0/1/0/all/0/1&quot;&gt;Gigliola Vaglini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05741">
<title>Decision Provenance: Capturing data flow for accountable systems. (arXiv:1804.05741v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1804.05741</link>
<description rdf:parseType="Literal">&lt;p&gt;Demand is growing for more accountability in the technological systems that
increasingly occupy our world. However, the complexity of many of these systems
- often systems of systems - poses accountability challenges. This is because
the details and nature of the data flows that interconnect and drive systems,
which often occur across technical and organisational boundaries, tend to be
opaque. This paper argues that data provenance methods show much promise as a
technical means for increasing the transparency of these interconnected
systems. Given concerns with the ever-increasing levels of automated and
algorithmic decision-making, we make the case for decision provenance. This
involves exposing the &apos;decision pipeline&apos; by tracking the chain of inputs to,
and flow-on effects from, the decisions and actions taken within these systems.
This paper proposes decision provenance as a means to assist in raising levels
of accountability, discusses relevant legal conceptions, and indicates some
practical considerations for moving forward.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1&quot;&gt;Jatinder Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cobbe_J/0/1/0/all/0/1&quot;&gt;Jennifer Cobbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norval_C/0/1/0/all/0/1&quot;&gt;Chris Norval&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05804">
<title>Safe Motion Planning in Unknown Environments: Optimality Benchmarks and Tractable Policies. (arXiv:1804.05804v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1804.05804</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the problem of planning a safe (i.e., collision-free)
trajectory from an initial state to a goal region when the obstacle space is
a-priori unknown and is incrementally revealed online, e.g., through
line-of-sight perception. Despite its ubiquitous nature, this formulation of
motion planning has received relatively little theoretical investigation, as
opposed to the setup where the environment is assumed known. A fundamental
challenge is that, unlike motion planning with known obstacles, it is not even
clear what an optimal policy to strive for is. Our contribution is threefold.
First, we present a notion of optimality for safe planning in unknown
environments in the spirit of comparative (as opposed to competitive) analysis,
with the goal of obtaining a benchmark that is, at least conceptually,
attainable. Second, by leveraging this theoretical benchmark, we derive a
pseudo-optimal class of policies that can seamlessly incorporate any amount of
prior or learned information while still guaranteeing the robot never collides.
Finally, we demonstrate the practicality of our algorithmic approach in
numerical experiments using a range of environment types and dynamics,
including a comparison with a state of the art method. A key aspect of our
framework is that it automatically and implicitly weighs exploration versus
exploitation in a way that is optimal with respect to the information
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janson_L/0/1/0/all/0/1&quot;&gt;Lucas Janson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tommy Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1&quot;&gt;Marco Pavone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.03074">
<title>Structural Learning of Probabilistic Graphical Models of Cumulative Phenomena. (arXiv:1703.03074v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.03074</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the critical issues when adopting Bayesian networks (BNs) to model
dependencies among random variables is to &quot;learn&quot; their structure. This is a
well-known NP-hard problem in its most general and classical formulation, which
is furthermore complicated by known pitfalls such as the issue of I-equivalence
among different structures. In this work we restrict the investigation to a
specific class of networks, i.e., those representing the dynamics of phenomena
characterized by the monotonic accumulation of events. Such phenomena allow to
set specific structural constraints based on Suppes&apos; theory of probabilistic
causation and, accordingly, to define constrained BNs, named Suppes-Bayes
Causal Networks (SBCNs). Within this framework, we study the structure learning
of SBCNs via extensive simulations with various state-of-the-art search
strategies, such as canonical local search techniques and Genetic Algorithms.
This investigation is intended to be an extension and an in-depth clarification
of our previous works on SBCN structure learning. Among the main results, we
show that Suppes&apos; constraints do simplify the learning task, by reducing the
solution search space and providing a temporal ordering on the variables, which
simplifies the complications derived by I-equivalent structures. Finally, we
report on tradeoffs among different optimization techniques that can be used to
learn SBCNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramazzotti_D/0/1/0/all/0/1&quot;&gt;Daniele Ramazzotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nobile_M/0/1/0/all/0/1&quot;&gt;Marco S. Nobile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antoniotti_M/0/1/0/all/0/1&quot;&gt;Marco Antoniotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graudenzi_A/0/1/0/all/0/1&quot;&gt;Alex Graudenzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.03076">
<title>Causal Data Science for Financial Stress Testing. (arXiv:1703.03076v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.03076</link>
<description rdf:parseType="Literal">&lt;p&gt;The most recent financial upheavals have cast doubt on the adequacy of some
of the conventional quantitative risk management strategies, such as VaR (Value
at Risk), in many common situations. Consequently, there has been an increasing
need for verisimilar financial stress testings, namely simulating and analyzing
financial portfolios in extreme, albeit rare scenarios. Unlike conventional
risk management which exploits statistical correlations among financial
instruments, here we focus our analysis on the notion of probabilistic
causation, which is embodied by Suppes-Bayes Causal Networks (SBCNs); SBCNs are
probabilistic graphical models that have many attractive features in terms of
more accurate causal analysis for generating financial stress scenarios. In
this paper, we present a novel approach for conducting stress testing of
financial portfolios based on SBCNs in combination with classical machine
learning classification tools. The resulting method is shown to be capable of
correctly discovering the causal relationships among financial factors that
affect the portfolios and thus, simulating stress testing scenarios with a
higher accuracy and lower computational complexity than conventional Monte
Carlo Simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1&quot;&gt;Gelin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bud Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramazzotti_D/0/1/0/all/0/1&quot;&gt;Daniele Ramazzotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09956">
<title>Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning. (arXiv:1803.09956v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09956</link>
<description rdf:parseType="Literal">&lt;p&gt;Skilled robotic manipulation benefits from complex synergies between
non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing
can help rearrange cluttered objects to make space for arms and fingers;
likewise, grasping can help displace objects to make pushing movements more
precise and collision-free. In this work, we demonstrate that it is possible to
discover and learn these synergies from scratch through model-free deep
reinforcement learning. Our method involves training two fully convolutional
networks that map from visual observations to actions: one infers the utility
of pushes for a dense pixel-wise sampling of end effector orientations and
locations, while the other does the same for grasping. Both networks are
trained jointly in a Q-learning framework and are entirely self-supervised by
trial and error, where rewards are provided from successful grasps. In this
way, our policy learns pushing motions that enable future grasps, while
learning grasps that can leverage past pushes. During picking experiments in
both simulation and real-world scenarios, we find that our system quickly
learns complex behaviors amid challenging cases of clutter, and achieves better
grasping success rates and picking efficiencies than baseline alternatives
after only a few hours of training. We further demonstrate that our method is
capable of generalizing to novel objects. Qualitative results (videos), code,
pre-trained models, and simulation environments are available at
&lt;a href=&quot;http://vpg.cs.princeton.edu&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Andy Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuran Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welker_S/0/1/0/all/0/1&quot;&gt;Stefan Welker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Johnny Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Alberto Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1&quot;&gt;Thomas Funkhouser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01508">
<title>The Tsetlin Machine - A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic. (arXiv:1804.01508v6 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01508</link>
<description rdf:parseType="Literal">&lt;p&gt;Although simple individually, artificial neurons provide state-of-the-art
performance when interconnected in deep networks. Unknown to many, there exists
an arguably even simpler and more versatile learning mechanism, namely, the
Tsetlin Automaton. Merely by means of a single integer as memory, it learns the
optimal action in stochastic environments. In this paper, we introduce the
Tsetlin Machine, which solves complex pattern recognition problems with
easy-to-interpret propositional formulas, composed by a collective of Tsetlin
Automata. To eliminate the longstanding problem of vanishing signal-to-noise
ratio, the Tsetlin Machine orchestrates the automata using a novel game. Our
theoretical analysis establishes that the Nash equilibria of the game are
aligned with the propositional formulas that provide optimal pattern
recognition accuracy. This translates to learning without local optima, only
global ones. We argue that the Tsetlin Machine finds the propositional formula
that provides optimal accuracy, with probability arbitrarily close to unity. In
four distinct benchmarks, the Tsetlin Machine outperforms both Neural Networks,
SVMs, Random Forests, the Naive Bayes Classifier and Logistic Regression. It
further turns out that the accuracy advantage of the Tsetlin Machine increases
with lack of data. The Tsetlin Machine has a significant computational
performance advantage since both inputs, patterns, and outputs are expressed as
bits, while recognition of patterns relies on bit manipulation. The combination
of accuracy, interpretability, and computational simplicity makes the Tsetlin
Machine a promising tool for a wide range of domains, including safety-critical
medicine. Being the first of its kind, we believe the Tsetlin Machine will
kick-start completely new paths of research, with a potentially significant
impact on the AI field and the applications of AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1&quot;&gt;Ole-Christoffer Granmo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02792">
<title>Occluded Person Re-identification. (arXiv:1804.02792v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02792</link>
<description rdf:parseType="Literal">&lt;p&gt;Person re-identification (re-id) suffers from a serious occlusion problem
when applied to crowded public places. In this paper, we propose to retrieve a
full-body person image by using a person image with occlusions. This differs
significantly from the conventional person re-id problem where it is assumed
that person images are detected without any occlusion. We thus call this new
problem the occluded person re-identitification. To address this new problem,
we propose a novel Attention Framework of Person Body (AFPB) based on deep
learning, consisting of 1) an Occlusion Simulator (OS) which automatically
generates artificial occlusions for full-body person images, and 2) multi-task
losses that force the neural network not only to discriminate a person&apos;s
identity but also to determine whether a sample is from the occluded data
distribution or the full-body data distribution. Experiments on a new occluded
person re-id dataset and three existing benchmarks modified to include
full-body person images and occluded person images show the superiority of the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Jianhuang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangcong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03824">
<title>Reference-less Measure of Faithfulness for Grammatical Error Correction. (arXiv:1804.03824v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.03824</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose {\sc USim}, a semantic measure for Grammatical Error Correction
(GEC) that measures the semantic faithfulness of the output to the source,
thereby complementing existing reference-less measures (RLMs) for measuring the
output&apos;s grammaticality. {\sc USim} operates by comparing the semantic symbolic
structure of the source and the correction, without relying on manually-curated
references. Our experiments establish the validity of {\sc USim}, by showing
that (1) semantic annotation can be consistently applied to ungrammatical text;
(2) valid corrections obtain a high {\sc USim} similarity score to the source;
and (3) invalid corrections obtain a lower score.\footnote{Our code is
available in \url{https://github.com/borgr/USim}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1&quot;&gt;Leshem Choshen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1&quot;&gt;Omri Abend&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04326">
<title>STAIR Actions: A Video Dataset of Everyday Home Actions. (arXiv:1804.04326v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04326</link>
<description rdf:parseType="Literal">&lt;p&gt;A new large-scale video dataset for human action recognition, called STAIR
Actions is introduced. STAIR Actions contains 100 categories of action labels
representing fine-grained everyday home actions so that it can be applied to
research in various home tasks such as nursing, caring, and security. In STAIR
Actions, each video has a single action label. Moreover, for each action
category, there are around 1,000 videos that were obtained from YouTube or
produced by crowdsource workers. The duration of each video is mostly five to
six seconds. The total number of videos is 102,462. We explain how we
constructed STAIR Actions and show the characteristics of STAIR Actions
compared to existing datasets for human action recognition. Experiments with
three major models for action recognition show that STAIR Actions can train
large models and achieve good performance. STAIR Actions can be downloaded from
&lt;a href=&quot;http://actions.stair.center&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshikawa_Y/0/1/0/all/0/1&quot;&gt;Yuya Yoshikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiaqing Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeuchi_A/0/1/0/all/0/1&quot;&gt;Akikazu Takeuchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05044">
<title>Monitoring and Executing Workflows in Linked Data Environments. (arXiv:1804.05044v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05044</link>
<description rdf:parseType="Literal">&lt;p&gt;The W3C&apos;s Web of Things working group is aimed at addressing the
interoperability problem on the Internet of Things using Linked Data as uniform
interface. While Linked Data paves the way towards combining such devices into
integrated applications, traditional solutions for specifying the control flow
of applications do not work seamlessly with Linked Data. We therefore tackle
the problem of the specification, execution, and monitoring of applications in
the context of Linked Data. We present a novel approach that combines
workflows, semantic reasoning, and RESTful interaction into one integrated
solution. We contribute to the state of the art by (1) defining an ontology for
describing workflow models and instances, (2) providing operational semantics
for the ontology that allows for the execution and monitoring of workflow
instances, (3) presenting a benchmark to evaluate our solution. Moreover, we
showcase how we used the ontology and the operational semantics to monitor
pilots executing workflows in virtual aircraft cockpits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kafer_T/0/1/0/all/0/1&quot;&gt;Tobias K&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harth_A/0/1/0/all/0/1&quot;&gt;Andreas Harth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04789">
<title>Successful Nash Equilibrium Agent for a 3-Player Imperfect-Information Game. (arXiv:1804.04789v1 [cs.GT] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1804.04789</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating strong agents for games with more than two players is a major open
problem in AI. Common approaches are based on approximating game-theoretic
solution concepts such as Nash equilibrium, which have strong theoretical
guarantees in two-player zero-sum games, but no guarantees in non-zero-sum
games or in games with more than two players. We describe an agent that is able
to defeat a variety of realistic opponents using an exact Nash equilibrium
strategy in a 3-player imperfect-information game. This shows that, despite a
lack of theoretical guarantees, agents based on Nash equilibrium strategies can
be successful in multiplayer games after all.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganzfried_S/0/1/0/all/0/1&quot;&gt;Sam Ganzfried&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_A/0/1/0/all/0/1&quot;&gt;Austin Nowak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinales_J/0/1/0/all/0/1&quot;&gt;Joannier Pinales&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05090">
<title>Regularized Singular Value Decomposition and Application to Recommender System. (arXiv:1804.05090v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.05090</link>
<description rdf:parseType="Literal">&lt;p&gt;Singular value decomposition (SVD) is the mathematical basis of principal
component analysis (PCA). Together, SVD and PCA are one of the most widely used
mathematical formalism/decomposition in machine learning, data mining, pattern
recognition, artificial intelligence, computer vision, signal processing, etc.
In recent applications, regularization becomes an increasing trend. In this
paper, we present a regularized SVD (RSVD), present an efficient computational
algorithm, and provide several theoretical analysis. We show that although RSVD
is non-convex, it has a closed-form global optimal solution. Finally, we apply
RSVD to the application of recommender system and experimental result show that
RSVD outperforms SVD significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shuai Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Chris Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_F/0/1/0/all/0/1&quot;&gt;Feiping Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05133">
<title>A Latent Gaussian Mixture Model for Clustering Longitudinal Data. (arXiv:1804.05133v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1804.05133</link>
<description rdf:parseType="Literal">&lt;p&gt;Finite mixture models have become a popular tool for clustering. Amongst
other uses, they have been applied for clustering longitudinal data and
clustering high-dimensional data. In the latter case, a latent Gaussian mixture
model is sometimes used. Although there has been much work on clustering using
latent variables and on clustering longitudinal data, respectively, there has
been a paucity of work that combines these features. An approach is developed
for clustering longitudinal data with many time points based on an extension of
the mixture of common factor analyzers model. A variation of the
expectation-maximization algorithm is used for parameter estimation and the
Bayesian information criterion is used for model selection. The approach is
illustrated using real and simulated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bierling_V/0/1/0/all/0/1&quot;&gt;Vanessa S.E. Bierling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McNicholas_P/0/1/0/all/0/1&quot;&gt;Paul D. McNicholas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05146">
<title>General-purpose validation and model selection when estimating individual treatment effects. (arXiv:1804.05146v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.05146</link>
<description rdf:parseType="Literal">&lt;p&gt;Practitioners in medicine, business, political science, and other fields are
increasingly aware that decisions should be personalized to each patient,
customer, or voter. A given treatment (e.g. a drug or advertisement) should be
administered only to those who will respond most positively, and certainly not
to those who will be harmed by it. Individual-level treatment effects (ITEs)
can be estimated with tools adapted from machine learning, but different models
can yield contradictory estimates. Unlike risk prediction models, however,
treatment effect models cannot be easily evaluated against each other using a
held-out test set because the true treatment effect itself is never directly
observed. Besides outcome prediction accuracy, several approaches that use
held-out data to evaluate treatment effects models have been proposed, but they
are largely unknown or cloistered within disciplines. We present a review of
these approaches and demonstrate theoretical relationships among them. We
demonstrate their behavior using simulations of both randomized and
observational data. Based on our empirical and theoretical results, we advocate
for the standardized use of estimated decision value for individual treatment
effect model selection and validation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schuler_A/0/1/0/all/0/1&quot;&gt;Alejandro Schuler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nigam Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05170">
<title>Universal Model-free Information Extraction. (arXiv:1804.05170v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.05170</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian approaches have been used extensively in scientific and engineering
research to quantify uncertainty and extract information. However, its
model-dependent nature means that when the a priori model is incomplete or
unavailable, there is a severe risk that Bayesian approaches will yield
misleading results. Here, we propose a universal model-free information
extraction approach, capable of reliably recovering target signals from complex
responses. This breakthrough leverages on a data-centric approach, whereby
measured data is reconfigured to create an enriched observable space, which in
turn is mapped to a well-adapted manifold, thereby detecting crucial
information via a reconstructed low-rank phase-space. A Koopman operator is
used to transform hidden and complex nonlinear dynamics to linear one, which
enables us to detect hidden event of interest from rapidly evolving systems,
and relate it to either unobservable stimulus or anomalous behaviour. Thanks to
its data-driven nature, our method excludes completely any prior knowledge on
governing dynamics. We benchmark the astonishing accuracy of our method on
three diverse and challenging problems in: biology, medicine, and engineering.
In all cases, our approach outperforms existing state-of-the-art methods, of
both Bayesian and non-Bayesian type. By creating a new reliable information
analysis paradigm, it is suitable for ubiquitous nonlinear dynamical systems or
end-users with little expertise, which permits the unbiased understanding of
various mechanisms in the real world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yueheng Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Weisi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chenglin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05214">
<title>Fast Optimal Bandwidth Selection for RBF Kernel using Reproducing Kernel Hilbert Space Operators for Kernel Based Classifiers. (arXiv:1804.05214v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.05214</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel based methods have shown effective performance in many remote sensing
classification tasks. However their performance significantly depend on its
hyper-parameters. The conventional technique to estimate the parameter comes
with high computational complexity. Thus, the objective of this letter is to
propose an fast and efficient method to select the bandwidth parameter of the
Gaussian kernel in the kernel based classification methods. The proposed method
is developed based on the operators in the reproducing kernel Hilbert space and
it is evaluated on Support vector machines and PerTurbo classification method.
Experiments conducted with hyperspectral datasets show that our proposed method
outperforms the state-of-art method in terms in computational time and
classification performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Damodaran_B/0/1/0/all/0/1&quot;&gt;Bharath Bhushan Damodaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05316">
<title>From CDF to PDF --- A Density Estimation Method for High Dimensional Data. (arXiv:1804.05316v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.05316</link>
<description rdf:parseType="Literal">&lt;p&gt;CDF2PDF is a method of PDF estimation by approximating CDF. The original idea
of it was previously proposed in [1] called SIC. However, SIC requires
additional hyper-parameter tunning, and no algorithms for computing higher
order derivative from a trained NN are provided in [1]. CDF2PDF improves SIC by
avoiding the time-consuming hyper-parameter tuning part and enabling higher
order derivative computation to be done in polynomial time. Experiments of this
method for one-dimensional data shows promising results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengdong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05345">
<title>Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds. (arXiv:1804.05345v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.05345</link>
<description rdf:parseType="Literal">&lt;p&gt;The deployment of state-of-the-art neural networks containing millions of
parameters to resource-constrained platforms may be prohibitive in terms of
both time and space. In this work, we present an efficient coresets-based
neural network compression algorithm that provably sparsifies the parameters of
a trained feedforward neural network in a manner that approximately preserves
the network&apos;s output. Our approach is based on an importance sampling scheme
that judiciously defines a sampling distribution over the neural network
parameters, and as a result, retains parameters of high importance while
discarding redundant ones. Our method and analysis introduce an empirical
notion of sensitivity and extend traditional coreset constructions to the
application of compressing parameters. Our theoretical analysis establishes
both instance-dependent and -independent bounds on the size of the resulting
compressed neural network as a function of the user-specified tolerance and
failure probability parameters. As a corollary to our practical compression
algorithm, we obtain novel generalization bounds that may provide novel
insights on the generalization properties of neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baykal_C/0/1/0/all/0/1&quot;&gt;Cenk Baykal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1&quot;&gt;Lucas Liebenwein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilitschenski_I/0/1/0/all/0/1&quot;&gt;Igor Gilitschenski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_D/0/1/0/all/0/1&quot;&gt;Dan Feldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1&quot;&gt;Daniela Rus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05402">
<title>Approximating the covariance ellipsoid. (arXiv:1804.05402v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.05402</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore ways in which the covariance ellipsoid ${\cal B}=\{v \in
\mathbb{R}^d : \mathbb{E} &amp;lt;X,v&amp;gt;^2 \leq 1\}$ of a centred random vector $X$ in
$\mathbb{R}^d$ can be approximated by a simple set. The data one is given for
constructing the approximating set consists of $X_1,...,X_N$ that are
independent and distributed as $X$.
&lt;/p&gt;
&lt;p&gt;We present a general method that can be used to construct such approximations
and implement it for two types of approximating sets. We first construct a
(random) set ${\cal K}$ defined by a union of intersections of slabs
$H_{z,\alpha}=\{v \in \mathbb{R}^d : |&amp;lt;z,v&amp;gt;| \leq \alpha\}$ (and therefore
${\cal K}$ is actually the output of a neural network with two hidden layers).
The slabs are generated using $X_1,...,X_N$, and under minimal assumptions on
$X$ (e.g., $X$ can be heavy-tailed) it suffices that $N = c_1d
\eta^{-4}\log(2/\eta)$ to ensure that $(1-\eta) {\cal K} \subset {\cal B}
\subset (1+\eta){\cal K}$. In some cases (e.g., if $X$ is rotation invariant
and has marginals that are well behaved in some weak sense), a smaller sample
size suffices: $N = c_1d\eta^{-2}\log(2/\eta)$.
&lt;/p&gt;
&lt;p&gt;We then show that if the slabs are replaced by randomly generated ellipsoids
defined using $X_1,...,X_N$, the same degree of approximation is true when $N
\geq c_2d\eta^{-2}\log(2/\eta)$.
&lt;/p&gt;
&lt;p&gt;The construction we use is based on the small-ball method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mendelson_S/0/1/0/all/0/1&quot;&gt;Shahar Mendelson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05433">
<title>Adaptivity for Regularized Kernel Methods by Lepskii&apos;s Principle. (arXiv:1804.05433v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.05433</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of {\it adaptivity} in the framework of reproducing
kernel Hilbert space (RKHS) regression. More precisely, we analyze estimators
arising from a linear regularization scheme $g_\lam$. In practical
applications, an important task is to choose the regularization parameter
$\lam$ appropriately, i.e. based only on the given data and independently on
unknown structural assumptions on the regression function. An attractive
approach avoiding data-splitting is the {\it Lepskii Principle} (LP), also
known as the {\it Balancing Principle} is this setting. We show that a modified
parameter choice based on (LP) is minimax optimal adaptive, up to
$\log\log(n)$. A convenient result is the fact that balancing in $L^2(\nu)-$
norm, which is easiest, automatically gives optimal balancing in all stronger
norms, interpolating between $L^2(\nu)$ and the RKHS. An analogous result is
open for other classical approaches to data dependent choices of the
regularization parameter, e.g. for Hold-Out.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mucke_N/0/1/0/all/0/1&quot;&gt;Nicole M&amp;#xfc;cke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05474">
<title>A Direct Sum Result for the Information Complexity of Learning. (arXiv:1804.05474v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.05474</link>
<description rdf:parseType="Literal">&lt;p&gt;How many bits of information are required to PAC learn a class of hypotheses
of VC dimension $d$? The mathematical setting we follow is that of Bassily et
al. (2018), where the value of interest is the mutual information
$\mathrm{I}(S;A(S))$ between the input sample $S$ and the hypothesis outputted
by the learning algorithm $A$. We introduce a class of functions of VC
dimension $d$ over the domain $\mathcal{X}$ with information complexity at
least $\Omega\left(d\log \log \frac{|\mathcal{X}|}{d}\right)$ bits for any
consistent and proper algorithm (deterministic or random). Bassily et al.
proved a similar (but quantitatively weaker) result for the case $d=1$.
&lt;/p&gt;
&lt;p&gt;The above result is in fact a special case of a more general phenomenon we
explore. We define the notion of information complexity of a given class of
functions $\mathcal{H}$. Intuitively, it is the minimum amount of information
that an algorithm for $\mathcal{H}$ must retain about its input to ensure
consistency and properness. We prove a direct sum result for information
complexity in this context; roughly speaking, the information complexity sums
when combining several classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachum_I/0/1/0/all/0/1&quot;&gt;Ido Nachum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafer_J/0/1/0/all/0/1&quot;&gt;Jonathan Shafer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yehudayoff_A/0/1/0/all/0/1&quot;&gt;Amir Yehudayoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05482">
<title>Binary Matrix Factorization via Dictionary Learning. (arXiv:1804.05482v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.05482</link>
<description rdf:parseType="Literal">&lt;p&gt;Matrix factorization is a key tool in data analysis; its applications include
recommender systems, correlation analysis, signal processing, among others.
Binary matrices are a particular case which has received significant attention
for over thirty years, especially within the field of data mining. Dictionary
learning refers to a family of methods for learning overcomplete basis (also
called frames) in order to efficiently encode samples of a given type; this
area, now also about twenty years old, was mostly developed within the signal
processing field. In this work we propose two binary matrix factorization
methods based on a binary adaptation of the dictionary learning paradigm to
binary matrices. The proposed algorithms focus on speed and scalability; they
work with binary factors combined with bit-wise operations and a few auxiliary
integer ones. Furthermore, the methods are readily applicable to online binary
matrix factorization. Another important issue in matrix factorization is the
choice of rank for the factors; we address this model selection problem with an
efficient method based on the Minimum Description Length principle. Our
preliminary results show that the proposed methods are effective at producing
interpretable factorizations of various data types of different nature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramirez_I/0/1/0/all/0/1&quot;&gt;Ignacio Ramirez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05484">
<title>Block Mean Approximation for Efficient Second Order Optimization. (arXiv:1804.05484v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.05484</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced optimization algorithms such as Newton method and AdaGrad benefit
from second order derivative or second order statistics to achieve better
descent directions and faster convergence rates. At their heart, such
algorithms need to compute the inverse or inverse square root of a matrix whose
size is quadratic of the dimensionality of the search space. For high
dimensional search spaces, the matrix inversion or inversion of square root
becomes overwhelming which in turn demands for approximate methods. In this
work, we propose a new matrix approximation method which divides a matrix into
blocks and represents each block by one or two numbers. The method allows
efficient computation of matrix inverse and inverse square root. We apply our
method to AdaGrad in training deep neural networks. Experiments show
encouraging results compared to the diagonal approximation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1&quot;&gt;Mehrtash Harandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1&quot;&gt;Richard Hartley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05494">
<title>conformalClassification: A Conformal Prediction R Package for Classification. (arXiv:1804.05494v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.05494</link>
<description rdf:parseType="Literal">&lt;p&gt;The conformalClassification package implements Transductive Conformal
Prediction (TCP) and Inductive Conformal Prediction (ICP) for classification
problems. Conformal Prediction (CP) is a framework that complements the
predictions of machine learning algorithms with reliable measures of
confidence. TCP gives results with higher validity than ICP, however ICP is
computationally faster than TCP. The package conformalClassification is built
upon the random forest method, where votes of the random forest for each class
are considered as the conformity scores for each data point. Although the main
aim of the conformalClassification package is to generate CP errors (p-values)
for classification problems, the package also implements various diagnostic
measures such as deviation from validity, error rate, efficiency, observed
fuzziness and calibration plots. In future releases, we plan to extend the
package to use other machine learning algorithms, (e.g. support vector
machines) for model fitting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gauraha_N/0/1/0/all/0/1&quot;&gt;Niharika Gauraha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spjuth_O/0/1/0/all/0/1&quot;&gt;Ola Spjuth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05515">
<title>Learning Simple Thresholded Features with Sparse Support Recovery. (arXiv:1804.05515v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.05515</link>
<description rdf:parseType="Literal">&lt;p&gt;The thresholded feature has recently emerged as an extremely efficient, yet
rough empirical approximation, of the time-consuming sparse coding inference
process. Such an approximation has not yet been rigorously examined, and
standard dictionaries often lead to non-optimal performance when used for
computing thresholded features. In this paper, we first present two theoretical
recovery guarantees for the thresholded feature to exactly recover the nonzero
support of the sparse code. Motivated by them, we then formulate the Dictionary
Learning for Thresholded Features (DLTF) model, which learns an optimized
dictionary for applying the thresholded feature. In particular, for the $(k,
2)$ norm involved, a novel proximal operator with log-linear time complexity
$O(m\log m)$ is derived. We evaluate the performance of DLTF on a vast range of
synthetic and real-data tasks, where DLTF demonstrates remarkable efficiency,
effectiveness and robustness in all experiments. In addition, we briefly
discuss the potential link between DLTF and deep learning building blocks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haichuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Ding Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05544">
<title>Building robust prediction models for defective sensor data using Artificial Neural Networks. (arXiv:1804.05544v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.05544</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting the health of components in complex dynamic systems such as an
automobile poses numerous challenges. The primary aim of such predictive
systems is to use the high-dimensional data acquired from different sensors and
predict the state-of-health of a particular component, e.g., brake pad. The
classical approach involves selecting a smaller set of relevant sensor signals
using feature selection and using them to train a machine learning algorithm.
However, this fails to address two prominent problems: (1) sensors are
susceptible to failure when exposed to extreme conditions over a long periods
of time; (2) sensors are electrical devices that can be affected by noise or
electrical interference. Using the failed and noisy sensor signals as inputs
largely reduce the prediction accuracy. To tackle this problem, it is
advantageous to use the information from all sensor signals, so that the
failure of one sensor can be compensated by another. In this work, we propose
an Artificial Neural Network (ANN) based framework to exploit the information
from a large number of signals. Secondly, our framework introduces a data
augmentation approach to perform accurate predictions in spite of noisy
signals. The plausibility of our framework is validated on real life industrial
application from Robert Bosch GmbH.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shekar_A/0/1/0/all/0/1&quot;&gt;Arvind Kumar Shekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe1;udio Rebelo de S&amp;#xe1;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_H/0/1/0/all/0/1&quot;&gt;Hugo Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1&quot;&gt;Carlos Soares&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05589">
<title>SPSA-FSR: Simultaneous Perturbation Stochastic Approximation for Feature Selection and Ranking. (arXiv:1804.05589v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.05589</link>
<description rdf:parseType="Literal">&lt;p&gt;This manuscript presents the following: (1) an improved version of the Binary
Simultaneous Perturbation Stochastic Approximation (SPSA) Method for feature
selection in machine learning (Aksakalli and Malekipirbazari, Pattern
Recognition Letters, Vol. 75, 2016) based on non-monotone iteration gains
computed via the Barzilai and Borwein (BB) method, (2) its adaptation for
feature ranking, and (3) comparison against popular methods on public benchmark
datasets. The improved method, which we call SPSA-FSR, dramatically reduces the
number of iterations required for convergence without impacting solution
quality. SPSA-FSR can be used for feature ranking and feature selection both
for classification and regression problems. After a review of the current
state-of-the-art, we discuss our improvements in detail and present three sets
of computational experiments: (1) comparison of SPSA-FS as a (wrapper) feature
selection method against sequential methods as well as genetic algorithms, (2)
comparison of SPSA-FS as a feature ranking method in a classification setting
against random forest importance, chi-squared, and information main methods,
and (3) comparison of SPSA-FS as a feature ranking method in a regression
setting against minimum redundancy maximum relevance (MRMR), RELIEF, and linear
correlation methods. The number of features in the datasets we use range from a
few dozens to a few thousands. Our results indicate that SPSA-FS converges to a
good feature set in no more than 100 iterations and therefore it is quite fast
for a wrapper method. SPSA-FS also outperforms popular feature selection as
well as feature ranking methods in majority of test cases, sometimes by a large
margin, and it stands as a promising new feature selection and ranking method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yenice_Z/0/1/0/all/0/1&quot;&gt;Zeren D. Yenice&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Adhikari_N/0/1/0/all/0/1&quot;&gt;Niranjan Adhikari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wong_Y/0/1/0/all/0/1&quot;&gt;Yong Kai Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aksakalli_V/0/1/0/all/0/1&quot;&gt;Vural Aksakalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gumus_A/0/1/0/all/0/1&quot;&gt;Alev Taskin Gumus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abbasi_B/0/1/0/all/0/1&quot;&gt;Babak Abbasi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05753">
<title>RFCDE: Random Forests for Conditional Density Estimation. (arXiv:1804.05753v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.05753</link>
<description rdf:parseType="Literal">&lt;p&gt;Random forests is a common non-parametric regression technique which performs
well for mixed-type data and irrelevant covariates, while being robust to
monotonic variable transformations. Existing random forest implementations
target regression or classification. We introduce the RFCDE package for fitting
random forest models optimized for nonparametric conditional density
estimation, including joint densities for multiple responses. This enables
analysis of conditional probability distributions which is useful for
propagating uncertainty and of joint distributions that describe relationships
between multiple responses and covariates. RFCDE is released under the MIT
open-source license and can be accessed at https://github.com/tpospisi/rfcde.
Both R and Python versions, which call a common C++ library, are available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pospisil_T/0/1/0/all/0/1&quot;&gt;Taylor Pospisil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Ann B. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05774">
<title>BELIEF: A distance-based redundancy-proof feature selection method for Big Data. (arXiv:1804.05774v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.05774</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advent of Big Data era, data reduction methods are highly demanded
given its ability to simplify huge data, and ease complex learning processes.
Concretely, algorithms that are able to filter relevant dimensions from a set
of millions are of huge importance. Although effective, these techniques suffer
from the &quot;scalability&quot; curse as well.
&lt;/p&gt;
&lt;p&gt;In this work, we propose a distributed feature weighting algorithm, which is
able to rank millions of features in parallel using large samples. This method,
inspired by the well-known RELIEF algorithm, introduces a novel redundancy
elimination measure that provides similar schemes to those based on entropy at
a much lower cost. It also allows smooth scale up when more instances are
demanded in feature estimations. Empirical tests performed on our method show
its estimation ability in manifold huge sets --both in number of features and
instances--, as well as its simplified runtime cost (specially, at the
redundancy detection step).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramirez_Gallego_S/0/1/0/all/0/1&quot;&gt;Sergio Ram&amp;#xed;rez-Gallego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1&quot;&gt;Salvador Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_N/0/1/0/all/0/1&quot;&gt;Ning Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrera_F/0/1/0/all/0/1&quot;&gt;Francisco Herrera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05805">
<title>Global Robustness Evaluation of Deep Neural Networks with Provable Guarantees for L0 Norm. (arXiv:1804.05805v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.05805</link>
<description rdf:parseType="Literal">&lt;p&gt;Deployment of deep neural networks (DNNs) in safety or security-critical
systems demands provable guarantees on their correct behaviour. One example is
the robustness of image classification decisions, defined as the invariance of
the classification for a given input over a small neighbourhood of images
around the input. Here we focus on the L_0 norm, and study the problem of
quantifying the global robustness of a trained DNN, where global robustness is
defined as the expectation of the maximum safe radius over a testing dataset.
We first show that the problem is NP-hard, and then propose an approach to
iteratively generate lower and upper bounds on the network&apos;s robustness. The
approach is anytime, i.e., it returns intermediate bounds and robustness
estimates that are gradually, but strictly, improved as the computation
proceeds; tensor-based, i.e., the computation is conducted over a set of inputs
simultaneously, instead of one by one, to enable efficient GPU computation; and
has provable guarantees, i.e., both the bounds and the robustness estimates can
converge to their optimal values. Finally, we demonstrate the utility of the
proposed approach in practice to compute tight bounds by applying and adapting
the anytime algorithm to a set of challenging problems, including global
robustness evaluation, guidance for the design of robust DNNs, competitive
$L_0$ attacks, generation of saliency maps for model interpretability, and test
generation for DNNs. We release the code of all case studies via Github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Youcheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kroening_D/0/1/0/all/0/1&quot;&gt;Daniel Kroening&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1&quot;&gt;Marta Kwiatkowska&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05810">
<title>Robust Physical Adversarial Attack on Faster R-CNN Object Detector. (arXiv:1804.05810v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.05810</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the ability to directly manipulate image pixels in the digital input
space, an adversary can easily generate imperceptible perturbations to fool a
Deep Neural Network (DNN) image classifier, as demonstrated in prior work. In
this work, we tackle the more challenging problem of crafting physical
adversarial perturbations to fool image-based object detectors like Faster
R-CNN. Attacking an object detector is more difficult than attacking an image
classifier, as it needs to mislead the classification results in multiple
bounding boxes with different scales. Extending the digital attack to the
physical world adds another layer of difficulty, because it requires the
perturbation to be robust enough to survive real-world distortions due to
different viewing distances and angles, lighting conditions, and camera
limitations. We show that the Expectation over Transformation technique, which
was originally proposed to enhance the robustness of adversarial perturbations
in image classification, can be successfully adapted to the object detection
setting. Our approach can generate adversarially perturbed stop signs that are
consistently mis-detected by Faster R-CNN as other objects, posing a potential
threat to autonomous vehicles and other safety-critical computer vision
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shang-Tse Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornelius_C/0/1/0/all/0/1&quot;&gt;Cory Cornelius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1&quot;&gt;Jason Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1508.01939">
<title>Model Assisted Variable Clustering: Minimax-optimal Recovery and Algorithms. (arXiv:1508.01939v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1508.01939</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-based clustering defines population level clusters relative to a model
that embeds notions of similarity. Algorithms tailored to such models yield
estimated clusters with a clear statistical interpretation. We take this view
here and introduce the class of $G$-block covariance models as a background
model for variable clustering. In such models, two variables in a cluster are
deemed similar if they have similar associations will all other variables. This
can arise, for instance, when groups of variables are noise corrupted versions
of the same latent factor. We quantify the difficulty of clustering data
generated from a $G$-block covariance model in terms of cluster proximity,
measured with respect to two related, but different, cluster separation
metrics. We derive minimax cluster separation thresholds, which are the metric
values below which no algorithm can recover the model-defined clusters exactly,
and show that they are different for the two metrics. We therefore develop two
algorithms, COD and PECOK, tailored to G-block covariance models, and study
their minimax-optimality with respect to each metric. Of independent interest
is the fact that the analysis of the PECOK algorithm, which is based on a
corrected convex relaxation of the popular $K$-means algorithm, provides the
first statistical analysis of such algorithms for variable clustering.
Additionally, we contrast our methods with another popular clustering method,
spectral clustering, specialized to variable clustering, and show that ensuring
exact cluster recovery via this method requires clusters to have a higher
separation, relative to the minimax threshold. Extensive simulation studies, as
well as our data analyses, confirm the applicability of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bunea_F/0/1/0/all/0/1&quot;&gt;Florentina Bunea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Giraud_C/0/1/0/all/0/1&quot;&gt;Christophe Giraud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Royer_M/0/1/0/all/0/1&quot;&gt;Martin Royer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Verzelen_N/0/1/0/all/0/1&quot;&gt;Nicolas Verzelen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.02549">
<title>Sampling Requirements and Accelerated Schemes for Sparse Linear Regression with Orthogonal Least-Squares. (arXiv:1608.02549v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1608.02549</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of inferring a sparse vector from random linear
combinations of its components. We propose the Accelerated Orthogonal
Least-Squares (AOLS) algorithm that improves performance of the well-known
Orthogonal Least-Squares (OLS) algorithm while requiring significantly lower
computational costs. While OLS greedily selects columns of the coefficient
matrix that correspond to non-zero components of the sparse vector, AOLS
employs a novel computationally efficient procedure that speeds up the search
by anticipating future selections via choosing $L$ columns in each step, where
$L$ is an adjustable hyper-parameter. We analyze the performance of AOLS and
establish lower bounds on the probability of exact recovery for both noiseless
and noisy random linear measurements. In the noiseless scenario, it is shown
that when the coefficients are samples from a Gaussian distribution, AOLS with
high probability recovers a $k$-sparse $m$-dimensional sparse vector using
${\cal O}(k\log \frac{m}{k+L-1})$ measurements. Similar result is established
for the bounded-noise scenario where an additional condition on the smallest
nonzero element of the unknown vector is required. The asymptotic sampling
complexity of AOLS is lower than the asymptotic sampling complexity of the
existing sparse reconstruction algorithms. In simulations, AOLS is compared to
state-of-the-art sparse recovery techniques and shown to provide better
performance in terms of accuracy, running time, or both. Finally, we consider
an application of AOLS to clustering high-dimensional data lying on the union
of low-dimensional subspaces and demonstrate its superiority over existing
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hashemi_A/0/1/0/all/0/1&quot;&gt;Abolfazl Hashemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vikalo_H/0/1/0/all/0/1&quot;&gt;Haris Vikalo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.02724">
<title>Tensor SVD: Statistical and Computational Limits. (arXiv:1703.02724v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1703.02724</link>
<description rdf:parseType="Literal">&lt;p&gt;Tensors, or high-order arrays, attract much attention in recent research. In
this paper, we propose a general framework for tensor principal component
analysis (tensor PCA), which focuses on the methodology and theory for
extracting the hidden low-rank structure from the high-dimensional tensor data.
A unified solution is provided to tensor PCA in both statistical limits and
computational costs. The problem exhibits three different phases according to
the signal-noise-ratio (SNR). In particular, with strong SNR, we propose a fast
spectral power iteration method that achieves the minimax optimal rate of
convergence in estimation; with weak SNR, the information-theoretical lower
bound shows that it is impossible to have consistent estimation in general;
with moderate SNR, we show that the non-convex maximum likelihood estimation
provides optimal solution, but with NP-hard computational cost; moreover, under
the hardness hypothesis the hypergraphic planted clique detection, there are no
polynomial-time algorithms performing consistently in general. Simulation
studies show that the proposed spectral power iteration method has good
performs under a variety of settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xia_D/0/1/0/all/0/1&quot;&gt;Dong Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.06114">
<title>Deep Sets. (arXiv:1703.06114v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.06114</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of designing models for machine learning tasks defined
on \emph{sets}. In contrast to traditional approach of operating on fixed
dimensional vectors, we consider objective functions defined on sets that are
invariant to permutations. Such problems are widespread, ranging from
estimation of population statistics \cite{poczos13aistats}, to anomaly
detection in piezometer data of embankment dams \cite{Jung15Exploration}, to
cosmology \cite{Ntampaka16Dynamical,Ravanbakhsh16ICML1}. Our main theorem
characterizes the permutation invariant functions and provides a family of
functions to which any permutation invariant objective function must belong.
This family of functions has a special structure which enables us to design a
deep network architecture that can operate on sets and which can be deployed on
a variety of scenarios including both unsupervised and supervised learning
tasks. We also derive the necessary and sufficient conditions for permutation
equivariance in deep models. We demonstrate the applicability of our method on
population statistic estimation, point cloud classification, set expansion, and
outlier detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1&quot;&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1&quot;&gt;Satwik Kottur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravanbakhsh_S/0/1/0/all/0/1&quot;&gt;Siamak Ravanbakhsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnabas Poczos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1&quot;&gt;Alexander Smola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.08683">
<title>Matrix Completion and Related Problems via Strong Duality. (arXiv:1704.08683v4 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1704.08683</link>
<description rdf:parseType="Literal">&lt;p&gt;This work studies the strong duality of non-convex matrix factorization
problems: we show that under certain dual conditions, these problems and its
dual have the same optimum. This has been well understood for convex
optimization, but little was known for non-convex problems. We propose a novel
analytical framework and show that under certain dual conditions, the optimal
solution of the matrix factorization program is the same as its bi-dual and
thus the global optimality of the non-convex program can be achieved by solving
its bi-dual which is convex. These dual conditions are satisfied by a wide
class of matrix factorization problems, although matrix factorization problems
are hard to solve in full generality. This analytical framework may be of
independent interest to non-convex optimization more broadly.
&lt;/p&gt;
&lt;p&gt;We apply our framework to two prototypical matrix factorization problems:
matrix completion and robust Principal Component Analysis (PCA). These are
examples of efficiently recovering a hidden matrix given limited reliable
observations of it. Our framework shows that exact recoverability and strong
duality hold with nearly-optimal sample complexity guarantees for matrix
completion and robust PCA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1&quot;&gt;Maria-Florina Balcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingyu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1&quot;&gt;David P. Woodruff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.05374">
<title>Expected Policy Gradients. (arXiv:1706.05374v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.05374</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose expected policy gradients (EPG), which unify stochastic policy
gradients (SPG) and deterministic policy gradients (DPG) for reinforcement
learning. Inspired by expected sarsa, EPG integrates across the action when
estimating the gradient, instead of relying only on the action in the sampled
trajectory. We establish a new general policy gradient theorem, of which the
stochastic and deterministic policy gradient theorems are special cases. We
also prove that EPG reduces the variance of the gradient estimates without
requiring deterministic policies and, for the Gaussian case, with no
computational overhead. Finally, we show that it is optimal in a certain sense
to explore with a Gaussian policy such that the covariance is proportional to
the exponential of the scaled Hessian of the critic with respect to the
actions. We present empirical results confirming that this new form of
exploration substantially outperforms DPG with the Ornstein-Uhlenbeck heuristic
in four challenging MuJoCo domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ciosek_K/0/1/0/all/0/1&quot;&gt;Kamil Ciosek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.04191">
<title>Distributionally Ambiguous Optimization Techniques for Batch Bayesian Optimization. (arXiv:1707.04191v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.04191</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel, theoretically-grounded, acquisition function for Batch
Bayesian optimization informed by insights from distributionally ambiguous
optimization. Our acquisition function is a lower bound on the well-known
Expected Improvement function, which requires evaluation of a Gaussian
Expectation over a multivariate piecewise affine function. Our bound is
computed instead by evaluating the best-case expectation over all probability
distributions consistent with the same mean and variance as the original
Gaussian distribution. Unlike alternative approaches, including Expected
Improvement, our proposed acquisition function avoids multi-dimensional
integrations entirely, and can be computed exactly - even on large batch sizes
- as the solution of a tractable convex optimization problem. Our suggested
acquisition function can also be optimized efficiently, since first and second
derivative information can be calculated inexpensively as by-products of the
acquisition function calculation itself. We derive various novel theorems that
ground our work theoretically and we demonstrate superior performance via
simple motivating examples, benchmark functions and real-world problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rontsis_N/0/1/0/all/0/1&quot;&gt;Nikitas Rontsis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osborne_M/0/1/0/all/0/1&quot;&gt;Michael A. Osborne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goulart_P/0/1/0/all/0/1&quot;&gt;Paul J. Goulart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.05342">
<title>An optimal unrestricted learning procedure. (arXiv:1707.05342v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.05342</link>
<description rdf:parseType="Literal">&lt;p&gt;We study learning problems involving arbitrary classes of functions $F$,
distributions $X$ and targets $Y$. Because proper learning procedures, i.e.,
procedures that are only allowed to select functions in $F$, tend to perform
poorly unless the problem satisfies some additional structural property (e.g.,
that $F$ is convex), we consider unrestricted learning procedures that are free
to choose functions outside the given class.
&lt;/p&gt;
&lt;p&gt;We present a new unrestricted procedure that is optimal in a very strong
sense: the required sample complexity is essentially the best one can hope for,
and the estimate holds for (almost) any problem, including heavy-tailed
situations. Moreover, the sample complexity coincides with the what one would
expect if $F$ were convex, even when $F$ is not. And if $F$ is convex, the
procedure turns out to be proper. Thus, the unrestricted procedure is actually
optimal in both realms, for convex classes as a proper procedure and for
arbitrary classes as an unrestricted procedure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mendelson_S/0/1/0/all/0/1&quot;&gt;Shahar Mendelson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.09953">
<title>The Error Probability of Random Fourier Features is Dimensionality Independent. (arXiv:1710.09953v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.09953</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that the error probability of reconstructing kernel matrices from
Random Fourier Features for the Gaussian kernel function is at most
$\mathcal{O}(R^{2/3} \exp(-D))$, where $D$ is the number of random features and
$R$ is the diameter of the data domain. We also provide an
information-theoretic method-independent lower bound of $\Omega(\exp(-D))$ for
$R&amp;gt;2.1$. Compared to prior work, we are the first to show that the error
probability for random Fourier features is independent of the dimensionality of
data points. As applications of our theory, we obtain dimension-independent
bounds for kernel ridge regression and support vector machines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honorio_J/0/1/0/all/0/1&quot;&gt;Jean Honorio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu-Jun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04846">
<title>State Space Gaussian Processes with Non-Gaussian Likelihood. (arXiv:1802.04846v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04846</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide a comprehensive overview and tooling for GP modeling with
non-Gaussian likelihoods using state space methods. The state space formulation
allows for solving one-dimensional GP models in $\mathcal{O}(n)$ time and
memory complexity. While existing literature has focused on the connection
between GP regression and state space methods, the computational primitives
allowing for inference using general likelihoods in combination with the
Laplace approximation (LA), variational Bayes (VB), and assumed density
filtering (ADF, a.k.a. single-sweep expectation propagation, EP) schemes has
been largely overlooked. We present means of combining the efficient
$\mathcal{O}(n)$ state space methodology with existing inference methods. We
extend existing methods, and provide unifying code implementing all approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nickisch_H/0/1/0/all/0/1&quot;&gt;Hannes Nickisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Solin_A/0/1/0/all/0/1&quot;&gt;Arno Solin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grigorievskiy_A/0/1/0/all/0/1&quot;&gt;Alexander Grigorievskiy&lt;/a&gt;</dc:creator>
</item></rdf:RDF>