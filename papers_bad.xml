<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06445"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06459"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06500"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06554"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06581"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06644"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06773"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07055"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07067"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.03459"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04049"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.06196"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.02238"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10125"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09381"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04590"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07444"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05768"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06396"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06401"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06441"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06449"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06453"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06460"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06510"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06518"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06521"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06531"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06561"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06586"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06589"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06675"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06716"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06727"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06795"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06852"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06898"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06917"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06952"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06969"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06971"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06989"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06992"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07054"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1505.03898"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.04608"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.02060"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.07531"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.00483"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06462"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10570"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11070"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11439"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09203"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02950"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08667"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08735"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04051"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04926"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06058"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05999"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.06445">
<title>Datalog: Bag Semantics via Set Semantics. (arXiv:1803.06445v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1803.06445</link>
<description rdf:parseType="Literal">&lt;p&gt;Duplicates in data management are common and problematic. In this work, we
present a translation of Datalog under bag semantics into a well-behaved
extension of Datalog (the so-called warded Datalog+-) under set semantics. From
a theoretical point of view, this allows us to reason on bag semantics by
making use of the well-established theoretical foundations of set semantics.
From a practical point of view, this allows us to handle the bag semantics of
Datalog by powerful, existing query engines for the required extension of
Datalog. Moreover, this translation has the potential for further extensions --
above all to capture the bag semantics of the semantic web query language
SPARQL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1&quot;&gt;Leopoldo Bertossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottlob_G/0/1/0/all/0/1&quot;&gt;Georg Gottlob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pichler_R/0/1/0/all/0/1&quot;&gt;Reinhard Pichler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06459">
<title>Learning to Cluster for Proposal-Free Instance Segmentation. (arXiv:1803.06459v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.06459</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposed a novel learning objective to train a deep neural network
to perform end-to-end image pixel clustering. We applied the approach to
instance segmentation, which is at the intersection of image semantic
segmentation and object detection. We utilize the most fundamental property of
instance labeling -- the pairwise relationship between pixels -- as the
supervision to formulate the learning objective, then apply it to train a fully
convolutional network (FCN) for learning to perform pixel-wise clustering. The
resulting clusters can be used as the instance labeling directly. To support
labeling of an unlimited number of instance, we further formulate ideas from
graph coloring theory into the proposed learning objective. The evaluation on
the Cityscapes dataset demonstrates strong performance and therefore proof of
the concept. Moreover, our approach won the second place in the lane detection
competition of 2017 CVPR Autonomous Driving Challenge, and was the top
performer without using external data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1&quot;&gt;Yen-Chang Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1&quot;&gt;Zsolt Kira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiawei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06500">
<title>Argumentation theory for mathematical argument. (arXiv:1803.06500v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.06500</link>
<description rdf:parseType="Literal">&lt;p&gt;To adequately model mathematical arguments the analyst must be able to
represent the mathematical objects under discussion and the relationships
between them, as well as inferences drawn about these objects and relationships
as the discourse unfolds. We introduce a framework with these properties, which
has been applied to both mathematical dialogues and expository texts. The
framework can recover salient elements of discourse at, and within, the
sentence level, as well as the way mathematical content connects to form larger
argumentative structures. We show how the framework might be used to support
computational reasoning, and argue that it provides a more natural way to
examine the process of proving theorems than do Lamport&apos;s structured proofs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corneli_J/0/1/0/all/0/1&quot;&gt;Joseph Corneli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_U/0/1/0/all/0/1&quot;&gt;Ursula Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murray_Rust_D/0/1/0/all/0/1&quot;&gt;Dave Murray-Rust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nesin_G/0/1/0/all/0/1&quot;&gt;Gabriela Rino Nesin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pease_A/0/1/0/all/0/1&quot;&gt;Alison Pease&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06554">
<title>Fusion of an Ensemble of Augmented Image Detectors for Robust Object Detection. (arXiv:1803.06554v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.06554</link>
<description rdf:parseType="Literal">&lt;p&gt;A significant challenge in object detection is accurate identification of an
object&apos;s position in image space, whereas one algorithm with one set of
parameters is usually not enough, and the fusion of multiple algorithms and/or
parameters can lead to more robust results. Herein, a new computational
intelligence fusion approach based on the dynamic analysis of agreement among
object detection outputs is proposed. Furthermore, we propose an online versus
just in training image augmentation strategy. Experiments comparing the results
both with and without fusion are presented. We demonstrate that the augmented
and fused combination results are the best, with respect to higher accuracy
rates and reduction of outlier influences. The approach is demonstrated in the
context of cone, pedestrian and box detection for Advanced Driver Assistance
Systems (ADAS) applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1&quot;&gt;Pan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_J/0/1/0/all/0/1&quot;&gt;John E. Ball&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1&quot;&gt;Derek T. Anderson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06581">
<title>Variational Knowledge Graph Reasoning. (arXiv:1803.06581v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.06581</link>
<description rdf:parseType="Literal">&lt;p&gt;Inferring missing links in knowledge graphs (KG) has attracted a lot of
attention from the research community. In this paper, we tackle a practical
query answering task involving predicting the relation of a given entity pair.
We frame this prediction problem as an inference problem in a probabilistic
graphical model and aim at resolving it from a variational inference
perspective. In order to model the relation between the query entity pair, we
assume that there exist underlying latent variables (assemble of all paths
connecting these two nodes) in the KG, which carries the equivalent semantics
of their relation. However, due to the intractability of connections in large
KGs, we propose to use variation inference to maximize the evidence lower
bound. More specifically, our framework (\textsc{Diva}) is composed of three
modules, i.e. a posterior approximator, a prior (path finder), and a likelihood
(path reasoner). By using variational inference, we are able to incorporate
them closely into a unified architecture and jointly optimize them to perform
KG reasoning. With active interactions among these sub-modules, \textsc{Diva}
is better at handling noise and cope with more complex reasoning scenarios. In
order to evaluate our method, we conduct the experiment of the link prediction
task on NELL-995 and FB15K datasets and achieve state-of-the-art performances
on both datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wenhan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xifeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06644">
<title>Computing and Testing Pareto Optimal Committees. (arXiv:1803.06644v1 [cs.GT])</title>
<link>http://arxiv.org/abs/1803.06644</link>
<description rdf:parseType="Literal">&lt;p&gt;Selecting a set of alternatives based on the preferences of agents is an
important problem in committee selection and beyond. Among the various criteria
put forth for the desirability of a committee, Pareto optimality is a minimal
and important requirement. As asking agents to specify their preferences over
exponentially many subsets of alternatives is practically infeasible, we assume
that each agent specifies a weak order on single alternatives, from which a
preference relation over subsets is derived using some preference extension. We
consider five prominent extensions (responsive, downward lexicographic, upward
lexicographic, best, and worst). For each of them, we consider the
corresponding Pareto optimality notion, and we study the complexity of
computing and verifying Pareto optimal outcomes. We also consider strategic
issues: for four of the set extensions, we present a linear-time, Pareto
optimal and strategyproof algorithm that even works for weak preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aziz_H/0/1/0/all/0/1&quot;&gt;Haris Aziz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_J/0/1/0/all/0/1&quot;&gt;Jerome Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monnot_J/0/1/0/all/0/1&quot;&gt;Jerome Monnot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06773">
<title>Composable Deep Reinforcement Learning for Robotic Manipulation. (arXiv:1803.06773v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.06773</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-free deep reinforcement learning has been shown to exhibit good
performance in domains ranging from video games to simulated robotic
manipulation and locomotion. However, model-free methods are known to perform
poorly when the interaction time with the environment is limited, as is the
case for most real-world robotic tasks. In this paper, we study how maximum
entropy policies trained using soft Q-learning can be applied to real-world
robotic manipulation. The application of this method to real-world manipulation
is facilitated by two important features of soft Q-learning. First, soft
Q-learning can learn multimodal exploration strategies by learning policies
represented by expressive energy-based models. Second, we show that policies
learned with soft Q-learning can be composed to create new policies, and that
the optimality of the resulting policy can be bounded in terms of the
divergence between the composed policies. This compositionality provides an
especially valuable tool for real-world manipulation, where constructing new
policies by composing existing skills can provide a large gain in efficiency
over training from scratch. Our experimental evaluation demonstrates that soft
Q-learning is substantially more sample efficient than prior model-free deep
reinforcement learning methods, and that compositionality can be performed for
both simulated and real-world tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haarnoja_T/0/1/0/all/0/1&quot;&gt;Tuomas Haarnoja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1&quot;&gt;Vitchyr Pong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Aurick Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalal_M/0/1/0/all/0/1&quot;&gt;Murtaza Dalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07055">
<title>Simple random search provides a competitive approach to reinforcement learning. (arXiv:1803.07055v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.07055</link>
<description rdf:parseType="Literal">&lt;p&gt;A common belief in model-free reinforcement learning is that methods based on
random search in the parameter space of policies exhibit significantly worse
sample complexity than those that explore the space of actions. We dispel such
beliefs by introducing a random search method for training static, linear
policies for continuous control problems, matching state-of-the-art sample
efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a
nearly optimal controller for a challenging instance of the Linear Quadratic
Regulator, a classical problem in control theory, when the dynamics are not
known. Computationally, our random search algorithm is at least 15 times more
efficient than the fastest competing model-free methods on these benchmarks. We
take advantage of this computational efficiency to evaluate the performance of
our method over hundreds of random seeds and many different hyperparameter
configurations for each benchmark task. Our simulations highlight a high
variability in performance in these benchmark tasks, suggesting that commonly
used estimations of sample efficiency do not adequately evaluate the
performance of RL algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mania_H/0/1/0/all/0/1&quot;&gt;Horia Mania&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guy_A/0/1/0/all/0/1&quot;&gt;Aurelia Guy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Recht_B/0/1/0/all/0/1&quot;&gt;Benjamin Recht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07067">
<title>Setting up a Reinforcement Learning Task with a Real-World Robot. (arXiv:1803.07067v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.07067</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning is a promising approach to developing hard-to-engineer
adaptive solutions for complex and diverse robotic tasks. However, learning
with real-world robots is often unreliable and difficult, which resulted in
their low adoption in reinforcement learning research. This difficulty is
worsened by the lack of guidelines for setting up learning tasks with robots.
In this work, we develop a learning task with a UR5 robotic arm to bring to
light some key elements of a task setup and study their contributions to the
challenges with robots. We find that learning performance can be highly
sensitive to the setup, and thus oversights and omissions in setup details can
make effective learning, reproducibility, and fair comparison hard. Our study
suggests some mitigating steps to help future experimenters avoid difficulties
and pitfalls. We show that highly reliable and repeatable experiments can be
performed in our setup, indicating the possibility of reinforcement learning
research extensively based on real-world robots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1&quot;&gt;A. Rupam Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korenkevych_D/0/1/0/all/0/1&quot;&gt;Dmytro Korenkevych&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komer_B/0/1/0/all/0/1&quot;&gt;Brent J. Komer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergstra_J/0/1/0/all/0/1&quot;&gt;James Bergstra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.03459">
<title>Optimal Auctions through Deep Learning. (arXiv:1706.03459v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/1706.03459</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing an auction that maximizes expected revenue is an intricate task.
Indeed, as of today--despite major efforts and impressive progress over the
past few years--only the single-item case is fully understood. In this work, we
initiate the exploration of the use of tools from deep learning on this topic.
The design objective is revenue optimal, dominant-strategy incentive compatible
auctions. We show that multi-layer neural networks can learn almost-optimal
auctions for settings for which there are analytical solutions, such as
Myerson&apos;s auction for a single item, Manelli and Vincent&apos;s mechanism for a
single bidder with additive preferences over two items, or Yao&apos;s auction for
two additive bidders with binary support distributions and multiple items, even
if no prior knowledge about the form of optimal auctions is encoded in the
network and the only feedback during training is revenue and regret. We further
show how characterization results, even rather implicit ones such as Rochet&apos;s
characterization through induced utilities and their gradients, can be
leveraged to obtain more precise fits to the optimal design. We conclude by
demonstrating the potential of deep learning for deriving optimal auctions with
high revenue for poorly understood problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutting_P/0/1/0/all/0/1&quot;&gt;Paul D&amp;#xfc;tting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhe Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1&quot;&gt;Harikrishna Narasimhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1&quot;&gt;David C. Parkes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04049">
<title>Information Design in Crowdfunding under Thresholding Policies. (arXiv:1709.04049v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04049</link>
<description rdf:parseType="Literal">&lt;p&gt;Crowdfunding has emerged as a prominent way for entrepreneurs to secure
funding without sophisticated intermediation. In crowdfunding, an entrepreneur
often has to decide how to disclose the campaign status in order to collect as
many contributions as possible. Such decisions are difficult to make primarily
due to incomplete information. We propose information design as a tool to help
the entrepreneur to improve revenue by influencing backers&apos; beliefs. We
introduce a heuristic algorithm to dynamically compute information-disclosure
policies for the entrepreneur, followed by an empirical evaluation to
demonstrate its competitiveness over the widely-adopted immediate-disclosure
policy. Our results demonstrate that despite its ease of implementation, the
immediate-disclosure policy is not optimal when backers follow thresholding
policies. With appropriate heuristics, an entrepreneur can benefit from dynamic
information disclosure. Our work sheds light on information design in a dynamic
setting where agents make decisions using thresholding policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wen Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crandall_J/0/1/0/all/0/1&quot;&gt;Jacob W. Crandall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Ke Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopes_C/0/1/0/all/0/1&quot;&gt;Cristina V. Lopes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.06196">
<title>Online algorithms for POMDPs with continuous state, action, and observation spaces. (arXiv:1709.06196v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.06196</link>
<description rdf:parseType="Literal">&lt;p&gt;Online solvers for partially observable Markov decision processes have been
applied to problems with large discrete state spaces, but continuous state,
action, and observation spaces remain a challenge. This paper begins by
investigating double progressive widening (DPW) as a solution to this
challenge. However, we prove that this modification alone is not sufficient
because the belief representations in the search tree collapse to a single
particle causing the algorithm to converge to a policy that is suboptimal
regardless of the computation time. This paper proposes and evaluates two new
algorithms, POMCPOW and PFT-DPW, that overcome this deficiency by using
weighted particle filtering. Simulation results show that these modifications
allow the algorithms to be successful where previous approaches fail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunberg_Z/0/1/0/all/0/1&quot;&gt;Zachary Sunberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1&quot;&gt;Mykel Kochenderfer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.02238">
<title>How Much Chemistry Does a Deep Neural Network Need to Know to Make Accurate Predictions?. (arXiv:1710.02238v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.02238</link>
<description rdf:parseType="Literal">&lt;p&gt;The meteoric rise of deep learning models in computer vision research, having
achieved human-level accuracy in image recognition tasks is firm evidence of
the impact of representation learning of deep neural networks. In the chemistry
domain, recent advances have also led to the development of similar CNN models,
such as Chemception, that is trained to predict chemical properties using
images of molecular drawings. In this work, we investigate the effects of
systematically removing and adding localized domain-specific information to the
image channels of the training data. By augmenting images with only 3
additional basic information, and without introducing any architectural
changes, we demonstrate that an augmented Chemception (AugChemception)
outperforms the original model in the prediction of toxicity, activity, and
solvation free energy. Then, by altering the information content in the images,
and examining the resulting model&apos;s performance, we also identify two distinct
learning patterns in predicting toxicity/activity as compared to solvation free
energy. These patterns suggest that Chemception is learning about its tasks in
the manner that is consistent with established knowledge. Thus, our work
demonstrates that advanced chemical knowledge is not a pre-requisite for deep
learning models to accurately predict complex chemical properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goh_G/0/1/0/all/0/1&quot;&gt;Garrett B. Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siegel_C/0/1/0/all/0/1&quot;&gt;Charles Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vishnu_A/0/1/0/all/0/1&quot;&gt;Abhinav Vishnu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hodas_N/0/1/0/all/0/1&quot;&gt;Nathan O. Hodas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baker_N/0/1/0/all/0/1&quot;&gt;Nathan Baker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10125">
<title>Learning to cluster in order to transfer across domains and tasks. (arXiv:1711.10125v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10125</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel method to perform transfer learning across
domains and tasks, formulating it as a problem of learning to cluster. The key
insight is that, in addition to features, we can transfer similarity
information and this is sufficient to learn a similarity function and
clustering network to perform both domain adaptation and cross-task transfer
learning. We begin by reducing categorical information to pairwise constraints,
which only considers whether two instances belong to the same class or not.
This similarity is category-agnostic and can be learned from data in the source
domain using a similarity network. We then present two novel approaches for
performing transfer learning using this similarity function. First, for
unsupervised domain adaptation, we design a new loss function to regularize
classification with a constrained clustering loss, hence learning a clustering
network with the transferred similarity metric generating the training inputs.
Second, for cross-task learning (i.e., unsupervised clustering with unseen
categories), we propose a framework to reconstruct and estimate the number of
semantic clusters, again using the clustering network. Since the similarity
network is noisy, the key is to use a robust clustering algorithm, and we show
that our formulation is more robust than the alternative constrained and
unconstrained clustering approaches. Using this method, we first show state of
the art results for the challenging cross-task problem, applied on Omniglot and
ImageNet. Our results show that we can reconstruct semantic clusters with high
accuracy. We then evaluate the performance of cross-domain transfer using
images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both
datasets. Our approach doesn&apos;t explicitly deal with domain discrepancy. If we
combine with a domain adaptation loss, it shows further improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1&quot;&gt;Yen-Chang Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1&quot;&gt;Zsolt Kira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02034">
<title>SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for Predicting Chemical Properties. (arXiv:1712.02034v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.02034</link>
<description rdf:parseType="Literal">&lt;p&gt;Chemical databases store information in text representations, and the SMILES
format is a universal standard used in many cheminformatics software. Encoded
in each SMILES string is structural information that can be used to predict
complex chemical properties. In this work, we develop SMILES2vec, a deep RNN
that automatically learns features from SMILES to predict chemical properties,
without the need for additional explicit feature engineering. Using Bayesian
optimization methods to tune the network architecture, we show that an
optimized SMILES2vec model can serve as a general-purpose neural network for
predicting distinct chemical properties including toxicity, activity,
solubility and solvation energy, while also outperforming contemporary MLP
neural networks that uses engineered features. Furthermore, we demonstrate
proof-of-concept of interpretability by developing an explanation mask that
localizes on the most important characters used in making a prediction. When
tested on the solubility dataset, it identified specific parts of a chemical
that is consistent with established first-principles knowledge with an accuracy
of 88%. Our work demonstrates that neural networks can learn technically
accurate chemical concept and provide state-of-the-art accuracy, making
interpretable deep neural networks a useful tool of relevance to the chemical
industry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goh_G/0/1/0/all/0/1&quot;&gt;Garrett B. Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hodas_N/0/1/0/all/0/1&quot;&gt;Nathan O. Hodas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siegel_C/0/1/0/all/0/1&quot;&gt;Charles Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vishnu_A/0/1/0/all/0/1&quot;&gt;Abhinav Vishnu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09381">
<title>Ray RLlib: A Framework for Distributed Reinforcement Learning. (arXiv:1712.09381v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.09381</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) training involves the deep nesting of highly
irregular computation patterns, each of which typically exhibits opportunities
for distributed computation. Current RL libraries offer parallelism at the
level of the entire program, coupling all algorithm components together and
making existing implementations difficult to scale, combine, and reuse. We
argue for distributing RL components in a composable way by adapting algorithms
for top-down hierarchical control, thereby encapsulating parallelism and
resource requirements within short-running compute tasks.
&lt;/p&gt;
&lt;p&gt;We demonstrate this principle by building RLlib on top of a task-based
framework and show that we can implement a wide range of state-of-the art
algorithms on top of a small set of general abstractions. These abstractions
are key to composability and reuse in RLlib and do not come at the cost of
performance---in our experiments, RLlib matches or exceeds the performance of
highly optimized reference implementations. Ray RLlib is available as part of
Ray at https://github.com/ray-project/ray/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_E/0/1/0/all/0/1&quot;&gt;Eric Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liaw_R/0/1/0/all/0/1&quot;&gt;Richard Liaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moritz_P/0/1/0/all/0/1&quot;&gt;Philipp Moritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishihara_R/0/1/0/all/0/1&quot;&gt;Robert Nishihara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_R/0/1/0/all/0/1&quot;&gt;Roy Fox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1&quot;&gt;Ken Goldberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1&quot;&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1&quot;&gt;Ion Stoica&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04590">
<title>Frame-Recurrent Video Super-Resolution. (arXiv:1801.04590v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04590</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in video super-resolution have shown that convolutional
neural networks combined with motion compensation are able to merge information
from multiple low-resolution (LR) frames to generate high-quality images.
Current state-of-the-art methods process a batch of LR frames to generate a
single high-resolution (HR) frame and run this scheme in a sliding window
fashion over the entire video, effectively treating the problem as a large
number of separate multi-frame super-resolution tasks. This approach has two
main weaknesses: 1) Each input frame is processed and warped multiple times,
increasing the computational cost, and 2) each output frame is estimated
independently conditioned on the input frames, limiting the system&apos;s ability to
produce temporally consistent results.
&lt;/p&gt;
&lt;p&gt;In this work, we propose an end-to-end trainable frame-recurrent video
super-resolution framework that uses the previously inferred HR estimate to
super-resolve the subsequent frame. This naturally encourages temporally
consistent results and reduces the computational cost by warping only one image
in each step. Furthermore, due to its recurrent nature, the proposed method has
the ability to assimilate a large number of previous frames without increased
computational demands. Extensive evaluations and comparisons with previous
methods validate the strengths of our approach and demonstrate that the
proposed framework is able to significantly outperform the current state of the
art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajjadi_M/0/1/0/all/0/1&quot;&gt;Mehdi S. M. Sajjadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1&quot;&gt;Raviteja Vemulapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1&quot;&gt;Matthew Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07444">
<title>Scaling-up Split-Merge MCMC with Locality Sensitive Sampling (LSS). (arXiv:1802.07444v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07444</link>
<description rdf:parseType="Literal">&lt;p&gt;Split-Merge MCMC (Monte Carlo Markov Chain) is one of the essential and
popular variants of MCMC for problems when an MCMC state consists of an unknown
number of components. It is well known that state-of-the-art methods for
split-merge MCMC do not scale well. Strategies for rapid mixing requires smart
and informative proposals to reduce the rejection rate. However, all known
smart proposals involve expensive operations to suggest informative
transitions. As a result, the cost of each iteration is prohibitive for massive
scale datasets. It is further known that uninformative but computationally
efficient proposals, such as random split-merge, leads to extremely slow
convergence. This tradeoff between mixing time and per update cost seems hard
to get around. In this paper, we get around this tradeoff by utilizing simple
similarity information, such as cosine similarity, between the entity vectors
to design a proposal distribution. Such information is readily available in
almost all applications. We show that the recent use of locality sensitive
hashing for efficient adaptive sampling can be leveraged to obtain a
computationally efficient pseudo-marginal MCMC. The new split-merge MCMC has
cheap proposal which is also informative and needs significantly fewer
iterations than random split-merge. Overall, we obtain a sweet tradeoff between
convergence and per update cost. As a direct consequence, our proposal, named
LSHSM, is around 5x faster than the state-of-the-art sampling methods on both
synthetic datasets and two large real datasets KDDCUP and PubMed with several
millions of entities and thousands of clusters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1&quot;&gt;Anshumali Shrivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05768">
<title>PAC-Reasoning in Relational Domains. (arXiv:1803.05768v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.05768</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of predicting plausible missing facts in relational
data, given a set of imperfect logical rules. In particular, our aim is to
provide bounds on the (expected) number of incorrect inferences that are made
in this way. Since for classical inference it is in general impossible to bound
this number in a non-trivial way, we consider two inference relations that
weaken, but remain close in spirit to classical inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuzelka_O/0/1/0/all/0/1&quot;&gt;Ondrej Kuzelka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1&quot;&gt;Jesse Davis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1&quot;&gt;Steven Schockaert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06386">
<title>Forecasting Economics and Financial Time Series: ARIMA vs. LSTM. (arXiv:1803.06386v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.06386</link>
<description rdf:parseType="Literal">&lt;p&gt;Forecasting time series data is an important subject in economics, business,
and finance. Traditionally, there are several techniques to effectively
forecast the next lag of time series data such as univariate Autoregressive
(AR), univariate Moving Average (MA), Simple Exponential Smoothing (SES), and
more notably Autoregressive Integrated Moving Average (ARIMA) with its many
variations. In particular, ARIMA model has demonstrated its outperformance in
precision and accuracy of predicting the next lags of time series. With the
recent advancement in computational power of computers and more importantly
developing more advanced machine learning algorithms and approaches such as
deep learning, new algorithms are developed to forecast time series data. The
research question investigated in this article is that whether and how the
newly developed deep learning-based algorithms for forecasting time series
data, such as &quot;Long Short-Term Memory (LSTM)&quot;, are superior to the traditional
algorithms. The empirical studies conducted and reported in this article show
that deep learning-based algorithms such as LSTM outperform traditional-based
algorithms such as ARIMA model. More specifically, the average reduction in
error rates obtained by LSTM is between 84 - 87 percent when compared to ARIMA
indicating the superiority of LSTM to ARIMA. Furthermore, it was noticed that
the number of training times, known as &quot;epoch&quot; in deep learning, has no effect
on the performance of the trained forecast model and it exhibits a truly random
behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siami_Namini_S/0/1/0/all/0/1&quot;&gt;Sima Siami-Namini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namin_A/0/1/0/all/0/1&quot;&gt;Akbar Siami Namin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06396">
<title>Reviving and Improving Recurrent Back-Propagation. (arXiv:1803.06396v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.06396</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we revisit the recurrent back-propagation (RBP) algorithm,
discuss the conditions under which it applies as well as how to satisfy them in
deep neural networks. We show that RBP can be unstable and propose two variants
based on conjugate gradient on the normal equations (CG-RBP) and Neumann series
(Neumann-RBP). We further investigate the relationship between Neumann-RBP and
back propagation through time (BPTT) and its truncated version (TBPTT). Our
Neumann-RBP has the same time complexity as TBPTT but only requires constant
memory, whereas TBPTT&apos;s memory cost scales linearly with the number of
truncation steps. We examine all RBP variants along with BPTT and TBPTT in
three different application domains: associative memory with continuous
Hopfield networks, document classification in citation networks using graph
neural networks and hyperparameter optimization for fully connected networks.
All experiments demonstrate that RBPs, especially the Neumann-RBP variant, are
efficient and effective for optimizing convergent recurrent neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1&quot;&gt;Renjie Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1&quot;&gt;Ethan Fetaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lisa Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;KiJung Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pitkow_X/0/1/0/all/0/1&quot;&gt;Xaq Pitkow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06401">
<title>Evaluating Conditional Cash Transfer Policies with Machine Learning Methods. (arXiv:1803.06401v1 [econ.EM])</title>
<link>http://arxiv.org/abs/1803.06401</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an out-of-sample prediction comparison between major
machine learning models and the structural econometric model. Over the past
decade, machine learning has established itself as a powerful tool in many
prediction applications, but this approach is still not widely adopted in
empirical economic studies. To evaluate the benefits of this approach, I use
the most common machine learning algorithms, CART, C4.5, LASSO, random forest,
and adaboost, to construct prediction models for a cash transfer experiment
conducted by the Progresa program in Mexico, and I compare the prediction
results with those of a previous structural econometric study. Two prediction
tasks are performed in this paper: the out-of-sample forecast and the long-term
within-sample simulation. For the out-of-sample forecast, both the mean
absolute error and the root mean square error of the school attendance rates
found by all machine learning models are smaller than those found by the
structural model. Random forest and adaboost have the highest accuracy for the
individual outcomes of all subgroups. For the long-term within-sample
simulation, the structural model has better performance than do all of the
machine learning models. The poor within-sample fitness of the machine learning
model results from the inaccuracy of the income and pregnancy prediction
models. The result shows that the machine learning model performs better than
does the structural model when there are many data to learn; however, when the
data are limited, the structural model offers a more sensible prediction. The
findings of this paper show promise for adopting machine learning in economic
policy analyses in the era of big data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tzai-Shuen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06441">
<title>A Novel Blaschke Unwinding Adaptive Fourier Decomposition based Signal Compression Algorithm with Application on ECG Signals. (arXiv:1803.06441v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1803.06441</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel signal compression algorithm based on the
Blaschke unwinding adaptive Fourier decomposition (AFD). The Blaschke unwinding
AFD is a newly developed signal decomposition theory. It utilizes the
Nevanlinna factorization and the maximal selection principle in each
decomposition step, and achieves a faster convergence rate with higher
fidelity. The proposed compression algorithm is applied to the
electrocardiogram signal. To assess the performance of the proposed compression
algorithm, in addition to the generic assessment criteria, we consider the less
discussed criteria related to the clinical needs -- for the heart rate
variability analysis purpose, how accurate the R peak information is preserved
is evaluated. The experiments are conducted on the MIT-BIH arrhythmia benchmark
database. The results show that the proposed algorithm performs better than
other state-of-the-art approaches. Meanwhile, it also well preserves the R peak
information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chunyu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hau-tieng Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06449">
<title>Note: Variational Encoding of Protein Dynamics Benefits from Maximizing Latent Autocorrelation. (arXiv:1803.06449v1 [physics.chem-ph])</title>
<link>http://arxiv.org/abs/1803.06449</link>
<description rdf:parseType="Literal">&lt;p&gt;As deep Variational Auto-Encoder (VAE) frameworks become more widely used for
modeling biomolecular simulation data, we emphasize the capability of the VAE
architecture to concurrently maximize the timescale of the latent space while
inferring a reduced coordinate, which assists in finding slow processes as
according to the variational approach to conformational dynamics. We
additionally provide evidence that the VDE framework (Hern\&apos;andez et al.,
2017), which uses this autocorrelation loss along with a time-lagged
reconstruction loss, obtains a variationally optimized latent coordinate in
comparison with related loss functions. We thus recommend leveraging the
autocorrelation of the latent space while training neural network models of
biomolecular simulation data to better represent slow processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wayment_Steele_H/0/1/0/all/0/1&quot;&gt;Hannah K. Wayment-Steele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pande_V/0/1/0/all/0/1&quot;&gt;Vijay S. Pande&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06453">
<title>Constrained Deep Learning using Conditional Gradient and Applications in Computer Vision. (arXiv:1803.06453v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.06453</link>
<description rdf:parseType="Literal">&lt;p&gt;A number of results have recently demonstrated the benefits of incorporating
various constraints when training deep architectures in vision and machine
learning. The advantages range from guarantees for statistical generalization
to better accuracy to compression. But support for general constraints within
widely used libraries remains scarce and their broader deployment within many
applications that can benefit from them remains under-explored. Part of the
reason is that Stochastic gradient descent (SGD), the workhorse for training
deep neural networks, does not natively deal with constraints with global scope
very well. In this paper, we revisit a classical first order scheme from
numerical optimization, Conditional Gradients (CG), that has, thus far had
limited applicability in training deep models. We show via rigorous analysis
how various constraints can be naturally handled by modifications of this
algorithm. We provide convergence guarantees and show a suite of immediate
benefits that are possible -- from training ResNets with fewer layers but
better accuracy simply by substituting in our version of CG to faster training
of GANs with 50% fewer epochs in image inpainting applications to provably
better generalization guarantees using efficiently implementable forms of
recently proposed regularizers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1&quot;&gt;Sathya N. Ravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1&quot;&gt;Tuan Dinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lokhande_V/0/1/0/all/0/1&quot;&gt;Vishnu Sai Rao Lokhande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1&quot;&gt;Vikas Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06460">
<title>Mean Reverting Portfolios via Penalized OU-Likelihood Estimation. (arXiv:1803.06460v1 [q-fin.PM])</title>
<link>http://arxiv.org/abs/1803.06460</link>
<description rdf:parseType="Literal">&lt;p&gt;We study an optimization-based approach to con- struct a mean-reverting
portfolio of assets. Our objectives are threefold: (1) design a portfolio that
is well-represented by an Ornstein-Uhlenbeck process with parameters estimated
by maximum likelihood, (2) select portfolios with desirable characteristics of
high mean reversion and low variance, and (3) select a parsimonious portfolio,
i.e. find a small subset of a larger universe of assets that can be used for
long and short positions. We present the full problem formulation, a
specialized algorithm that exploits partial minimization, and numerical
examples using both simulated and empirical price data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jize Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Leung_T/0/1/0/all/0/1&quot;&gt;Tim Leung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Aravkin_A/0/1/0/all/0/1&quot;&gt;Aleksandr Y. Aravkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06510">
<title>Hidden Integrality of SDP Relaxation for Sub-Gaussian Mixture Models. (arXiv:1803.06510v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.06510</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of estimating the discrete clustering structures
under Sub-Gaussian Mixture Models. Our main results establish a hidden
integrality property of a semidefinite programming (SDP) relaxation for this
problem: while the optimal solutions to the SDP are not integer-valued in
general, their estimation errors can be upper bounded in terms of the error of
an idealized integer program. The error of the integer program, and hence that
of the SDP, are further shown to decay exponentially in the signal-to-noise
ratio. To the best of our knowledge, this is the first exponentially decaying
error bound for convex relaxations of mixture models, and our results reveal
the &quot;global-to-local&quot; mechanism that drives the performance of the SDP
relaxation.
&lt;/p&gt;
&lt;p&gt;A corollary of our results shows that in certain regimes the SDP solutions
are in fact integral and exact, improving on existing exact recovery results
for convex relaxations. More generally, our results establish sufficient
conditions for the SDP to correctly recover the cluster memberships of
$(1-\delta)$ fraction of the points for any $\delta\in(0,1)$. As a special
case, we show that under the $d$-dimensional Stochastic Ball Model, SDP
achieves non-trivial (sometimes exact) recovery when the center separation is
as small as $\sqrt{1/d}$, which complements previous exact recovery results
that require constant separation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fei_Y/0/1/0/all/0/1&quot;&gt;Yingjie Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yudong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06518">
<title>Provable Convex Co-clustering of Tensors. (arXiv:1803.06518v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1803.06518</link>
<description rdf:parseType="Literal">&lt;p&gt;Cluster analysis is a fundamental tool for pattern discovery of complex
heterogeneous data. Prevalent clustering methods mainly focus on vector or
matrix-variate data and are not applicable to general-order tensors, which
arise frequently in modern scientific and business applications. Moreover,
there is a gap between statistical guarantees and computational efficiency for
existing tensor clustering solutions due to the nature of their non-convex
formulations. In this work, we bridge this gap by developing a provable convex
formulation of tensor co-clustering. Our convex co-clustering (CoCo) estimator
enjoys stability guarantees and is both computationally and storage efficient.
We further establish a non-asymptotic error bound for the CoCo estimator, which
reveals a surprising &quot;blessing of dimensionality&quot; phenomenon that does not
exist in vector or matrix-variate cluster analysis. Our theoretical findings
are supported by extensive simulated studies. Finally, we apply the CoCo
estimator to the cluster analysis of advertisement click tensor data from a
major online company. Our clustering results provide meaningful business
insights to improve advertising effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chi_E/0/1/0/all/0/1&quot;&gt;Eric C. Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gaines_B/0/1/0/all/0/1&quot;&gt;Brian R. Gaines&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Will Wei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hua Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06521">
<title>Learning Mixtures of Product Distributions via Higher Multilinear Moments. (arXiv:1803.06521v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.06521</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning mixtures of $k$ binary product distributions is a central problem in
computational learning theory, but one where there are wide gaps between the
best known algorithms and lower bounds (even for restricted families of
algorithms). We narrow many of these gaps by developing novel insights about
how to reason about higher order multilinear moments. Our results include:
&lt;/p&gt;
&lt;p&gt;1) An $n^{O(k^2)}$ time algorithm for learning mixtures of binary product
distributions, giving the first improvement on the $n^{O(k^3)}$ time algorithm
of Feldman, O&apos;Donnell and Servedio
&lt;/p&gt;
&lt;p&gt;2) An $n^{\Omega(\sqrt{k})}$ statistical query lower bound, improving on the
$n^{\Omega(\log k)}$ lower bound that is based on connections to sparse parity
with noise
&lt;/p&gt;
&lt;p&gt;3) An $n^{O(\log k)}$ time algorithm for learning mixtures of $k$ subcubes.
This special case can still simulate many other hard learning problems, but is
much richer than any of them alone. As a corollary, we obtain more flexible
algorithms for learning decision trees under the uniform distribution, that
work with stochastic transitions, when we are only given positive examples and
with a polylogarithmic number of samples for any fixed $k$.
&lt;/p&gt;
&lt;p&gt;Our algorithms are based on a win-win analysis where we either build a basis
for the moments or locate a degeneracy that can be used to simplify the
problem, which we believe will have applications to other learning problems
over discrete domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sitan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1&quot;&gt;Ankur Moitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06531">
<title>Topology Estimation using Graphical Models in Multi-Phase Power Distribution Grids. (arXiv:1803.06531v1 [cs.SY])</title>
<link>http://arxiv.org/abs/1803.06531</link>
<description rdf:parseType="Literal">&lt;p&gt;Distribution grid is the medium and low voltage part of a large power system.
Structurally, the majority of distribution networks operate radially, such that
energized lines form a collection of trees, i.e. forest, with a substation
being at the root of any tree. The operational topology/forest may change from
time to time, however tracking these changes, even though important for the
distribution grid operation and control, is hindered by limited real-time
monitoring. This paper develops a learning framework to reconstruct radial
operational structure of the distribution grid from synchronized voltage
measurements in the grid subject to the exogenous fluctuations in nodal power
consumption. To detect operational lines our learning algorithm uses
conditional independence tests for continuous random variables that is
applicable to a wide class of probability distributions of the nodal
consumption and Gaussian injections in particular. Moreover, our algorithm
applies to the practical case of unbalanced three-phase power flow. Algorithm
performance is validated on AC power flow simulations over IEEE distribution
grid test cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deka_D/0/1/0/all/0/1&quot;&gt;Deepjyoti Deka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chertkov_M/0/1/0/all/0/1&quot;&gt;Michael Chertkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Backhaus_S/0/1/0/all/0/1&quot;&gt;Scott Backhaus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06561">
<title>Multi-device, Multi-tenant Model Selection with GP-EI. (arXiv:1803.06561v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.06561</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization is the core technique behind the emergence of AutoML,
which holds the promise of automatically searching for models and
hyperparameters to make machine learning techniques more accessible. As such
services are moving towards the cloud, we ask -- {\em When multiple AutoML
users share the same computational infrastructure, how should we allocate
resources to maximize the &quot;global happiness&quot; of all users?}
&lt;/p&gt;
&lt;p&gt;We focus on GP-EI, one of the most popular algorithms for automatic model
selection and hyperparameter tuning, and develop a novel multi-device,
multi-tenant extension that is aware of \emph{multiple} computation devices and
multiple users sharing the same set of computation devices. Theoretically,
given $N$ users and $M$ devices, we obtain a regret bound of $O((\text{\bf
{MIU}}(T,K) + M)\frac{N^2}{M})$, where $\text{\bf {MIU}}(T,K)$ refers to the
maximal incremental uncertainty up to time $T$ for the covariance matrix $K$.
Empirically, we evaluate our algorithm on two applications of automatic model
selection, and show that our algorithm significantly outperforms the strategy
of serving users independently. Moreover, when multiple computation devices are
available, we achieve near-linear speedup when the number of users is much
larger than the number of devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlas_B/0/1/0/all/0/1&quot;&gt;Bojan Karlas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1&quot;&gt;Jie Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Ce Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06586">
<title>Structural query-by-committee. (arXiv:1803.06586v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.06586</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we describe a framework that unifies many different interactive
learning tasks. We present a generalization of the {\it query-by-committee}
active learning algorithm for this setting, and we study its consistency and
rate of convergence, both theoretically and empirically, with and without
noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tosh_C/0/1/0/all/0/1&quot;&gt;Christopher Tosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1&quot;&gt;Sanjoy Dasgupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06589">
<title>Early Hospital Mortality Prediction using Vital Signals. (arXiv:1803.06589v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.06589</link>
<description rdf:parseType="Literal">&lt;p&gt;Early hospital mortality prediction is critical as intensivists strive to
make efficient medical decisions about the severely ill patients staying in
intensive care units. As a result, various methods have been developed to
address this problem based on clinical records. However, some of the laboratory
test results are time-consuming and need to be processed. In this paper, we
propose a novel method to predict mortality using features extracted from the
heart signals of patients within the first hour of ICU admission. In order to
predict the risk, quantitative features have been computed based on the heart
rate signals of ICU patients. Each signal is described in terms of 12
statistical and signal-based features. The extracted features are fed into
eight classifiers: decision tree, linear discriminant, logistic regression,
support vector machine (SVM), random forest, boosted trees, Gaussian SVM, and
K-nearest neighborhood (K-NN). To derive insight into the performance of the
proposed method, several experiments have been conducted using the well-known
clinical dataset named Medical Information Mart for Intensive Care III
(MIMIC-III). The experimental results demonstrate the capability of the
proposed method in terms of precision, recall, F1-score, and area under the
receiver operating characteristic curve (AUC). The decision tree classifier
satisfies both accuracy and interpretability better than the other classifiers,
producing an F1-score and AUC equal to 0.91 and 0.93, respectively. It
indicates that heart rate signals can be used for predicting mortality in
patients in the ICU, achieving a comparable performance with existing
predictions that rely on high dimensional features from clinical records which
need to be processed and may contain missing information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadeghi_R/0/1/0/all/0/1&quot;&gt;Reza Sadeghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1&quot;&gt;Tanvi Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romine_W/0/1/0/all/0/1&quot;&gt;William Romine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06675">
<title>Rare Feature Selection in High Dimensions. (arXiv:1803.06675v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1803.06675</link>
<description rdf:parseType="Literal">&lt;p&gt;It is common in modern prediction problems for many predictor variables to be
counts of rarely occurring events. This leads to design matrices in which many
columns are highly sparse. The challenge posed by such &quot;rare features&quot; has
received little attention despite its prevalence in diverse areas, ranging from
natural language processing (e.g., rare words) to biology (e.g., rare species).
We show, both theoretically and empirically, that not explicitly accounting for
the rareness of features can greatly reduce the effectiveness of an analysis.
We next propose a framework for aggregating rare features into denser features
in a flexible manner that creates better predictors of the response. Our
strategy leverages side information in the form of a tree that encodes feature
similarity.
&lt;/p&gt;
&lt;p&gt;We apply our method to data from TripAdvisor, in which we predict the
numerical rating of a hotel based on the text of the associated review. Our
method achieves high accuracy by making effective use of rare words; by
contrast, the lasso is unable to identify highly predictive words if they are
too rare. A companion R package, called rare, implements our new estimator,
using the alternating direction method of multipliers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xiaohan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bien_J/0/1/0/all/0/1&quot;&gt;Jacob Bien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06716">
<title>High Dimensional Linear Regression using Lattice Basis Reduction. (arXiv:1803.06716v1 [math.ST])</title>
<link>http://arxiv.org/abs/1803.06716</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a high dimensional linear regression problem where the goal is to
efficiently recover an unknown vector $\beta^*$ from $n$ noisy linear
observations $Y=X\beta^*+W \in \mathbb{R}^n$, for known $X \in \mathbb{R}^{n
\times p}$ and unknown $W \in \mathbb{R}^n$. Unlike most of the literature on
this model we make no sparsity assumption on $\beta^*$. Instead we adopt a
regularization based on assuming that the underlying vectors $\beta^*$ have
rational entries with the same denominator $Q \in \mathbb{Z}_{&amp;gt;0}$. We call
this $Q$-rationality assumption.
&lt;/p&gt;
&lt;p&gt;We propose a new polynomial-time algorithm for this task which is based on
the seminal Lenstra-Lenstra-Lovasz (LLL) lattice basis reduction algorithm. We
establish that under the $Q$-rationality assumption, our algorithm recovers
exactly the vector $\beta^*$ for a large class of distributions for the iid
entries of $X$ and non-zero noise $W$. We prove that it is successful under
small noise, even when the learner has access to only one observation ($n=1$).
Furthermore, we prove that in the case of the Gaussian white noise for $W$,
$n=o\left(p/\log p\right)$ and $Q$ sufficiently large, our algorithm tolerates
a nearly optimal information-theoretic level of the noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gamarnik_D/0/1/0/all/0/1&quot;&gt;David Gamarnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zadik_I/0/1/0/all/0/1&quot;&gt;Ilias Zadik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06727">
<title>Aggregating Strategies for Long-term Forecasting. (arXiv:1803.06727v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.06727</link>
<description rdf:parseType="Literal">&lt;p&gt;The article is devoted to investigating the application of aggregating
algorithms to the problem of the long-term forecasting. We examine the classic
aggregating algorithms based on the exponential reweighing. For the general
Vovk&apos;s aggregating algorithm we provide its generalization for the long-term
forecasting. For the special basic case of Vovk&apos;s algorithm we provide its two
modifications for the long-term forecasting. The first one is theoretically
close to an optimal algorithm and is based on replication of independent
copies. It provides the time-independent regret bound with respect to the best
expert in the pool. The second one is not optimal but is more practical and has
$O(\sqrt{T})$ regret bound, where $T$ is the length of the game.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1&quot;&gt;Alexander Korotin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyugin_V/0/1/0/all/0/1&quot;&gt;Vladimir V&amp;#x27;yugin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1&quot;&gt;Evgeny Burnaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06795">
<title>Nonlocal Low-Rank Tensor Factor Analysis for Image Restoration. (arXiv:1803.06795v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.06795</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-rank signal modeling has been widely leveraged to capture non-local
correlation in image processing applications. We propose a new method that
employs low-rank tensor factor analysis for tensors generated by grouped image
patches. The low-rank tensors are fed into the alternative direction multiplier
method (ADMM) to further improve image reconstruction. The motivating
application is compressive sensing (CS), and a deep convolutional architecture
is adopted to approximate the expensive matrix inversion in CS applications. An
iterative algorithm based on this low-rank tensor factorization strategy,
called NLR-TFA, is presented in detail. Experimental results on noiseless and
noisy CS measurements demonstrate the superiority of the proposed approach,
especially at low CS sampling rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06852">
<title>Confounder Detection in High Dimensional Linear Models using First Moments of Spectral Measures. (arXiv:1803.06852v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.06852</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the confounder detection problem in the linear model,
where the target variable $Y$ is predicted using its $n$ potential causes
$X_n=(x_1,...,x_n)^T$. Based on an assumption of rotation invariant generating
process of the model, recent study shows that the spectral measure induced by
the regression coefficient vector with respect to the covariance matrix of
$X_n$ is close to a uniform measure in purely causal cases, but it differs from
a uniform measure characteristically in the presence of a scalar confounder.
Then, analyzing spectral measure pattern could help to detect confounding. In
this paper, we propose to use the first moment of the spectral measure for
confounder detection. We calculate the first moment of the regression vector
induced spectral measure, and compare it with the first moment of a uniform
spectral measure, both defined with respect to the covariance matrix of $X_n$.
The two moments coincide in non-confounding cases, and differ from each other
in the presence of confounding. This statistical causal-confounding asymmetry
can be used for confounder detection. Without the need of analyzing the
spectral measure pattern, our method does avoid the difficulty of metric choice
and multiple parameter optimization. Experiments on synthetic and real data
show the performance of this method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Furui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chan_L/0/1/0/all/0/1&quot;&gt;Laiwan Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06898">
<title>A Mixture of Views Network with Applications to the Classification of Breast Microcalcifications. (arXiv:1803.06898v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.06898</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we examine data fusion methods for multi-view data
classification. We present a decision concept which explicitly takes into
account the input multi-view structure, where for each case there is a
different subset of relevant views. This data fusion concept, which we dub
Mixture of Views, is implemented by a special purpose neural network
architecture. It is demonstrated on the task of classifying breast
microcalcifications as benign or malignant based on CC and MLO mammography
views. The single view decisions are combined by a data-driven decision,
according to the relevance of each view in a given case, into a global
decision. The method is evaluated on a large multi-view dataset extracted from
the standardized digital database for screening mammography (DDSM). The
experimental results show that our method outperforms previously suggested
fusion methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shachor_Y/0/1/0/all/0/1&quot;&gt;Yaniv Shachor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenspan_H/0/1/0/all/0/1&quot;&gt;Hayit Greenspan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1&quot;&gt;Jacob Goldberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06917">
<title>Universal features of price formation in financial markets: perspectives from Deep Learning. (arXiv:1803.06917v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/1803.06917</link>
<description rdf:parseType="Literal">&lt;p&gt;Using a large-scale Deep Learning approach applied to a high-frequency
database containing billions of electronic market quotes and transactions for
US equities, we uncover nonparametric evidence for the existence of a universal
and stationary price formation mechanism relating the dynamics of supply and
demand for a stock, as revealed through the order book, to subsequent
variations in its market price. We assess the model by testing its
out-of-sample predictions for the direction of price moves given the history of
price and order flow, across a wide range of stocks and time periods. The
universal price formation model is shown to exhibit a remarkably stable
out-of-sample prediction accuracy across time, for a wide range of stocks from
different sectors. Interestingly, these results also hold for stocks which are
not part of the training sample, showing that the relations captured by the
model are universal and not asset-specific.
&lt;/p&gt;
&lt;p&gt;The universal model --- trained on data from all stocks --- outperforms, in
terms of out-of-sample prediction accuracy, asset-specific linear and nonlinear
models trained on time series of any given stock, showing that the universal
nature of price formation weighs in favour of pooling together financial data
from various stocks, rather than designing asset- or sector-specific models as
commonly done. Standard data normalizations based on volatility, price level or
average spread, or partitioning the training data into sectors or categories
such as large/small tick stocks, do not improve training results. On the other
hand, inclusion of price and order flow history over many past observations is
shown to improve forecasting performance, showing evidence of path-dependence
in price dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Sirignano_J/0/1/0/all/0/1&quot;&gt;Justin Sirignano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Cont_R/0/1/0/all/0/1&quot;&gt;Rama Cont&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06952">
<title>Asymmetric kernel in Gaussian Processes for learning target variance. (arXiv:1803.06952v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.06952</link>
<description rdf:parseType="Literal">&lt;p&gt;This work incorporates the multi-modality of the data distribution into a
Gaussian Process regression model. We approach the problem from a
discriminative perspective by learning, jointly over the training data, the
target space variance in the neighborhood of a certain sample through metric
learning. We start by using data centers rather than all training samples.
Subsequently, each center selects an individualized kernel metric. This enables
each center to adjust the kernel space in its vicinity in correspondence with
the topology of the targets --- a multi-modal approach. We additionally add
descriptiveness by allowing each center to learn a precision matrix. We
demonstrate empirically the reliability of the model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pintea_S/0/1/0/all/0/1&quot;&gt;Silvia L. Pintea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1&quot;&gt;Jan C. van Gemert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1&quot;&gt;Arnold W. M. Smeulders&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06969">
<title>Comparing Dynamics: Deep Neural Networks versus Glassy Systems. (arXiv:1803.06969v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.06969</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze numerically the training dynamics of deep neural networks (DNN) by
using methods developed in statistical physics of glassy systems. The two main
issues we address are the complexity of the loss-landscape and of the dynamics
within it, and to what extent DNNs share similarities with glassy systems. Our
findings, obtained for different architectures and datasets, suggest that
during the training process the dynamics slows down because of an increasingly
large number of flat directions. At large times, when the loss is approaching
zero, the system diffuses at the bottom of the landscape. Despite some
similarities with the dynamics of mean-field glassy systems, in particular, the
absence of barrier crossing, we find distinctive dynamical behaviors in the two
cases, showing that the statistical properties of the corresponding loss and
energy landscapes are different. In contrast, when the network is
under-parametrized we observe a typical glassy behavior, thus suggesting the
existence of different phases depending on whether the network is
under-parametrized or over-parametrized.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baity_Jesi_M/0/1/0/all/0/1&quot;&gt;M. Baity-Jesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sagun_L/0/1/0/all/0/1&quot;&gt;L. Sagun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Geiger_M/0/1/0/all/0/1&quot;&gt;M. Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spigler_S/0/1/0/all/0/1&quot;&gt;S. Spigler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arous_G/0/1/0/all/0/1&quot;&gt;G. Ben Arous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cammarota_C/0/1/0/all/0/1&quot;&gt;C. Cammarota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+LeCun_Y/0/1/0/all/0/1&quot;&gt;Y. LeCun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wyart_M/0/1/0/all/0/1&quot;&gt;M. Wyart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Biroli_G/0/1/0/all/0/1&quot;&gt;G. Biroli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06971">
<title>What Doubling Tricks Can and Can&apos;t Do for Multi-Armed Bandits. (arXiv:1803.06971v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.06971</link>
<description rdf:parseType="Literal">&lt;p&gt;An online reinforcement learning algorithm is anytime if it does not need to
know in advance the horizon T of the experiment. A well-known technique to
obtain an anytime algorithm from any non-anytime algorithm is the &quot;Doubling
Trick&quot;. In the context of adversarial or stochastic multi-armed bandits, the
performance of an algorithm is measured by its regret, and we study two
families of sequences of growing horizons (geometric and exponential) to
generalize previously known results that certain doubling tricks can be used to
conserve certain regret bounds. In a broad setting, we prove that a geometric
doubling trick can be used to conserve (minimax) bounds in $R\_T = O(\sqrt{T})$
but cannot conserve (distribution-dependent) bounds in $R\_T = O(\log T)$. We
give insights as to why exponential doubling tricks may be better, as they
conserve bounds in $R\_T = O(\log T)$, and are close to conserving bounds in
$R\_T = O(\sqrt{T})$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Besson_L/0/1/0/all/0/1&quot;&gt;Lilian Besson&lt;/a&gt; (IETR), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kaufmann_E/0/1/0/all/0/1&quot;&gt;Emilie Kaufmann&lt;/a&gt; (SEQUEL, CNRS)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06989">
<title>Numerical Integration on Graphs: where to sample and how to weigh. (arXiv:1803.06989v1 [math.ST])</title>
<link>http://arxiv.org/abs/1803.06989</link>
<description rdf:parseType="Literal">&lt;p&gt;Let $G=(V,E,w)$ be a finite, connected graph with weighted edges. We are
interested in the problem of finding a subset $W \subset V$ of vertices and
weights $a_w$ such that $$ \frac{1}{|V|}\sum_{v \in V}^{}{f(v)} \sim \sum_{w
\in W}{a_w f(w)}$$ for functions $f:V \rightarrow \mathbb{R}$ that are `smooth&apos;
with respect to the geometry of the graph. The main application are problems
where $f$ is known to somehow depend on the underlying graph but is expensive
to evaluate on even a single vertex. We prove an inequality showing that the
integration problem can be rewritten as a geometric problem (`the optimal
packing of heat balls&apos;). We discuss how one would construct approximate
solutions of the heat ball packing problem; numerical examples demonstrate the
efficiency of the method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Linderman_G/0/1/0/all/0/1&quot;&gt;George C. Linderman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Steinerberger_S/0/1/0/all/0/1&quot;&gt;Stefan Steinerberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06992">
<title>Estimating the intrinsic dimension of datasets by a minimal neighborhood information. (arXiv:1803.06992v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.06992</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing large volumes of high-dimensional data is an issue of fundamental
importance in data science, molecular simulations and beyond. Several
approaches work on the assumption that the important content of a dataset
belongs to a manifold whose Intrinsic Dimension (ID) is much lower than the
crude large number of coordinates. Such manifold is generally twisted and
curved, in addition points on it will be non-uniformly distributed: two factors
that make the identification of the ID and its exploitation really hard. Here
we propose a new ID estimator using only the distance of the first and the
second nearest neighbor of each point in the sample. This extreme minimality
enables us to reduce the effects of curvature, of density variation, and the
resulting computational cost. The ID estimator is theoretically exact in
uniformly distributed datasets, and provides consistent measures in general.
When used in combination with block analysis, it allows discriminating the
relevant dimensions as a function of the block size. This allows estimating the
ID even when the data lie on a manifold perturbed by a high-dimensional noise,
a situation often encountered in real world data sets. We demonstrate the
usefulness of the approach on molecular simulations and image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Facco_E/0/1/0/all/0/1&quot;&gt;Elena Facco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+dErrico_M/0/1/0/all/0/1&quot;&gt;Maria d&amp;#x27;Errico&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Alex Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laio_A/0/1/0/all/0/1&quot;&gt;Alessandro Laio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07054">
<title>Optimal link prediction with matrix logistic regression. (arXiv:1803.07054v1 [math.ST])</title>
<link>http://arxiv.org/abs/1803.07054</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of link prediction, based on partial observation of a
large network, and on side information associated to its vertices. The
generative model is formulated as a matrix logistic regression. The performance
of the model is analysed in a high-dimensional regime under a structural
assumption. The minimax rate for the Frobenius-norm risk is established and a
combinatorial estimator based on the penalised maximum likelihood approach is
shown to achieve it. Furthermore, it is shown that this rate cannot be attained
by any (randomised) algorithm computable in polynomial time under a
computational complexity assumption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Baldin_N/0/1/0/all/0/1&quot;&gt;Nicolai Baldin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Berthet_Q/0/1/0/all/0/1&quot;&gt;Quentin Berthet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1505.03898">
<title>Pinball Loss Minimization for One-bit Compressive Sensing: Convex Models and Algorithms. (arXiv:1505.03898v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1505.03898</link>
<description rdf:parseType="Literal">&lt;p&gt;The one-bit quantization is implemented by one single comparator that
operates at low power and a high rate. Hence one-bit compressive sensing
(1bit-CS) becomes attractive in signal processing. When measurements are
corrupted by noise during signal acquisition and transmission, 1bit-CS is
usually modeled as minimizing a loss function with a sparsity constraint. The
one-sided $\ell_1$ loss and the linear loss are two popular loss functions for
1bit-CS. To improve the decoding performance on noisy data, we consider the
pinball loss, which provides a bridge between the one-sided $\ell_1$ loss and
the linear loss. Using the pinball loss, two convex models, an elastic-net
pinball model and its modification with the $\ell_1$-norm constraint, are
proposed. To efficiently solve them, the corresponding dual coordinate ascent
algorithms are designed and their convergence is proved. The numerical
experiments confirm the effectiveness of the proposed algorithms and the
performance of the pinball loss minimization for 1bit-CS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Lei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Ming Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1&quot;&gt;Johan A.K. Suykens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.04608">
<title>Recursive nearest agglomeration (ReNA): fast clustering for approximation of structured signals. (arXiv:1609.04608v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1609.04608</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we revisit fast dimension reduction approaches, as with random
projections and random sampling. Our goal is to summarize the data to decrease
computational costs and memory footprint of subsequent analysis. Such dimension
reduction can be very efficient when the signals of interest have a strong
structure, such as with images. We focus on this setting and investigate
feature clustering schemes for data reductions that capture this structure. An
impediment to fast dimension reduction is that good clustering comes with large
algorithmic costs. We address it by contributing a linear-time agglomerative
clustering scheme, Recursive Nearest Agglomeration (ReNA). Unlike existing fast
agglomerative schemes, it avoids the creation of giant clusters. We empirically
validate that it approximates the data as well as traditional
variance-minimizing clustering schemes that have a quadratic complexity. In
addition, we analyze signal approximation with feature clustering and show that
it can remove noise, improving subsequent analysis steps. As a consequence,
data reduction by clustering features with ReNA yields very fast and accurate
models, enabling to process large datasets on budget. Our theoretical analysis
is backed by extensive experiments on publicly-available data that illustrate
the computation efficiency and the denoising properties of the resulting
dimension reduction scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hoyos_Idrobo_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Hoyos-Idrobo&lt;/a&gt; (PARIETAL, NEUROSPIN), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Varoquaux_G/0/1/0/all/0/1&quot;&gt;Ga&amp;#xeb;l Varoquaux&lt;/a&gt; (PARIETAL, NEUROSPIN), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kahn_J/0/1/0/all/0/1&quot;&gt;Jonas Kahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thirion_B/0/1/0/all/0/1&quot;&gt;Bertrand Thirion&lt;/a&gt; (PARIETAL)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.02060">
<title>Angle-Based Joint and Individual Variation Explained. (arXiv:1704.02060v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.02060</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrative analysis of disparate data blocks measured on a common set of
experimental subjects is a major challenge in modern data analysis. This data
structure naturally motivates the simultaneous exploration of the joint and
individual variation within each data block resulting in new insights. For
instance, there is a strong desire to integrate the multiple genomic data sets
in The Cancer Genome Atlas to characterize the common and also the unique
aspects of cancer genetics and cell biology for each source. In this paper we
introduce Angle-Based Joint and Individual Variation Explained capturing both
joint and individual variation within each data block. This is a major
improvement over earlier approaches to this challenge in terms of a new
conceptual understanding, much better adaption to data heterogeneity and a fast
linear algebra computation. Important mathematical contributions are the use of
score subspaces as the principal descriptors of variation structure and the use
of perturbation theory as the guide for variation segmentation. This leads to
an exploratory data analysis method which is insensitive to the heterogeneity
among data blocks and does not require separate normalization. An application
to cancer data reveals different behaviors of each type of signal in
characterizing tumor subtypes. An application to a mortality data set reveals
interesting historical lessons. Software and data are available at GitHub
&amp;lt;https://github.com/MeileiJiang/AJIVE_Project&amp;gt;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_Q/0/1/0/all/0/1&quot;&gt;Qing Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Meilei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hannig_J/0/1/0/all/0/1&quot;&gt;Jan Hannig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marron_J/0/1/0/all/0/1&quot;&gt;J. S. Marron&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.07531">
<title>Sufficient Markov Decision Processes with Alternating Deep Neural Networks. (arXiv:1704.07531v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1704.07531</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in mobile computing technologies have made it possible to monitor
and apply data-driven interventions across complex systems in real time. Markov
decision processes (MDPs) are the primary model for sequential decision
problems with a large or indefinite time horizon. Choosing a representation of
the underlying decision process that is both Markov and low-dimensional is
non-trivial. We propose a method for constructing a low-dimensional
representation of the original decision process for which: 1. the MDP model
holds; 2. a decision strategy that maximizes mean utility when applied to the
low-dimensional representation also maximizes mean utility when applied to the
original process. We use a deep neural network to define a class of potential
process representations and estimate the process of lowest dimension within
this class. The method is illustrated using data from a mobile study on heavy
drinking and smoking among college students.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longshaokan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laber_E/0/1/0/all/0/1&quot;&gt;Eric B. Laber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Witkiewitz_K/0/1/0/all/0/1&quot;&gt;Katie Witkiewitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.00483">
<title>Iteratively Linearized Reweighted Alternating Direction Method of Multipliers for a Class of Nonconvex Problems. (arXiv:1709.00483v3 [cs.NA] UPDATED)</title>
<link>http://arxiv.org/abs/1709.00483</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider solving a class of nonconvex and nonsmooth
problems frequently appearing in signal processing and machine learning
research. The traditional alternating direction method of multipliers
encounters troubles in both mathematics and computations in solving the
nonconvex and nonsmooth subproblem. In view of this, we propose a reweighted
alternating direction method of multipliers. In this algorithm, all subproblems
are convex and easy to solve. We also provide several guarantees for the
convergence and prove that the algorithm globally converges to a critical point
of an auxiliary function with the help of the Kurdyka-{\L}ojasiewicz property.
Several numerical results are presented to demonstrate the efficiency of the
proposed algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lizhi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wei Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06462">
<title>S-Isomap++: Multi Manifold Learning from Streaming Data. (arXiv:1710.06462v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06462</link>
<description rdf:parseType="Literal">&lt;p&gt;Manifold learning based methods have been widely used for non-linear
dimensionality reduction (NLDR). However, in many practical settings, the need
to process streaming data is a challenge for such methods, owing to the high
computational complexity involved. Moreover, most methods operate under the
assumption that the input data is sampled from a single manifold, embedded in a
high dimensional space. We propose a method for streaming NLDR when the
observed data is either sampled from multiple manifolds or irregularly sampled
from a single manifold. We show that existing NLDR methods, such as Isomap,
fail in such situations, primarily because they rely on smoothness and
continuity of the underlying manifold, which is violated in the scenarios
explored in this paper. However, the proposed algorithm is able to learn
effectively in presence of multiple, and potentially intersecting, manifolds,
while allowing for the input data to arrive as a massive stream.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mahapatra_S/0/1/0/all/0/1&quot;&gt;Suchismit Mahapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chandola_V/0/1/0/all/0/1&quot;&gt;Varun Chandola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10570">
<title>Weight Initialization of Deep Neural Networks(DNNs) using Data Statistics. (arXiv:1710.10570v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10570</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) form the backbone of almost every
state-of-the-art technique in the fields such as computer vision, speech
processing, and text analysis. The recent advances in computational technology
have made the use of DNNs more practical. Despite the overwhelming performances
by DNN and the advances in computational technology, it is seen that very few
researchers try to train their models from the scratch. Training of DNNs still
remains a difficult and tedious job. The main challenges that researchers face
during training of DNNs are the vanishing/exploding gradient problem and the
highly non-convex nature of the objective function which has up to million
variables. The approaches suggested in He and Xavier solve the vanishing
gradient problem by providing a sophisticated initialization technique. These
approaches have been quite effective and have achieved good results on standard
datasets, but these same approaches do not work very well on more practical
datasets. We think the reason for this is not making use of data statistics for
initializing the network weights. Optimizing such a high dimensional loss
function requires careful initialization of network weights. In this work, we
propose a data dependent initialization and analyze its performance against the
standard initialization techniques such as He and Xavier. We performed our
experiments on some practical datasets and the results show our algorithm&apos;s
superior classification accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koturwar_S/0/1/0/all/0/1&quot;&gt;Saiprasad Koturwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merchant_S/0/1/0/all/0/1&quot;&gt;Shabbir Merchant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11070">
<title>Convergence Rates of Latent Topic Models Under Relaxed Identifiability Conditions. (arXiv:1710.11070v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11070</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we study the frequentist convergence rate for the Latent
Dirichlet Allocation (Blei et al., 2003) topic models. We show that the maximum
likelihood estimator converges to one of the finitely many equivalent
parameters in Wasserstein&apos;s distance metric at a rate of $n^{-1/4}$ without
assuming separability or non-degeneracy of the underlying topics and/or the
existence of more than three words per document, thus generalizing the previous
works of Anandkumar et al. (2012, 2014) from an information-theoretical
perspective. We also show that the $n^{-1/4}$ convergence rate is optimal in
the worst case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yining Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11439">
<title>Statistical Speech Enhancement Based on Probabilistic Integration of Variational Autoencoder and Non-Negative Matrix Factorization. (arXiv:1710.11439v4 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11439</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a statistical method of single-channel speech enhancement
that uses a variational autoencoder (VAE) as a prior distribution on clean
speech. A standard approach to speech enhancement is to train a deep neural
network (DNN) to take noisy speech as input and output clean speech. Although
this supervised approach requires a very large amount of pair data for
training, it is not robust against unknown environments. Another approach is to
use non-negative matrix factorization (NMF) based on basis spectra trained on
clean speech in advance and those adapted to noise on the fly. This
semi-supervised approach, however, causes considerable signal distortion in
enhanced speech due to the unrealistic assumption that speech spectrograms are
linear combinations of the basis spectra. Replacing the poor linear generative
model of clean speech in NMF with a VAE---a powerful nonlinear deep generative
model---trained on clean speech, we formulate a unified probabilistic
generative model of noisy speech. Given noisy speech as observed data, we can
sample clean speech from its posterior distribution. The proposed method
outperformed the conventional DNN-based method in unseen noisy environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bando_Y/0/1/0/all/0/1&quot;&gt;Yoshiaki Bando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mimura_M/0/1/0/all/0/1&quot;&gt;Masato Mimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Itoyama_K/0/1/0/all/0/1&quot;&gt;Katsutoshi Itoyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshii_K/0/1/0/all/0/1&quot;&gt;Kazuyoshi Yoshii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawahara_T/0/1/0/all/0/1&quot;&gt;Tatsuya Kawahara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09203">
<title>Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations. (arXiv:1712.09203v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.09203</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that the gradient descent algorithm provides an implicit
regularization effect in the learning of over-parameterized matrix
factorization models and one-hidden-layer neural networks with quadratic
activations. Concretely, we show that given $\tilde{O}(dr^{2})$ random linear
measurements of a rank $r$ positive semidefinite matrix $X^{\star}$, we can
recover $X^{\star}$ by parameterizing it by $UU^\top$ with $U\in \mathbb
R^{d\times d}$ and minimizing the squared loss, even if $r \ll d$. We prove
that starting from a small initialization, gradient descent recovers
$X^{\star}$ in $\tilde{O}(\sqrt{r})$ iterations approximately. The results
solve the conjecture of Gunasekar et al.&apos;17 under the restricted isometry
property. The technique can be applied to analyzing neural networks with
one-hidden-layer quadratic activations with some technical modifications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02950">
<title>Adversarial Deep Learning for Robust Detection of Binary Encoded Malware. (arXiv:1801.02950v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02950</link>
<description rdf:parseType="Literal">&lt;p&gt;Malware is constantly adapting in order to avoid detection. Model based
malware detectors, such as SVM and neural networks, are vulnerable to so-called
adversarial examples which are modest changes to detectable malware that allows
the resulting malware to evade detection. Continuous-valued methods that are
robust to adversarial examples of images have been developed using saddle-point
optimization formulations. We are inspired by them to develop similar methods
for the discrete, e.g. binary, domain which characterizes the features of
malware. A specific extra challenge of malware is that the adversarial examples
must be generated in a way that preserves their malicious functionality. We
introduce methods capable of generating functionally preserved adversarial
malware examples in the binary domain. Using the saddle-point formulation, we
incorporate the adversarial examples into the training of models that are
robust to them. We evaluate the effectiveness of the methods and others in the
literature on a set of Portable Execution~(PE) files. Comparison prompts our
introduction of an online measure computed during training to assess general
expectation of robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Dujaili_A/0/1/0/all/0/1&quot;&gt;Abdullah Al-Dujaili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1&quot;&gt;Alex Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemberg_E/0/1/0/all/0/1&quot;&gt;Erik Hemberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OReilly_U/0/1/0/all/0/1&quot;&gt;Una-May O&amp;#x27;Reilly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08667">
<title>Double/De-Biased Machine Learning Using Regularized Riesz Representers. (arXiv:1802.08667v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08667</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide adaptive inference methods for linear functionals of sparse linear
approximations to the conditional expectation function. Examples of such
functionals include average derivatives, policy effects, average treatment
effects, and many others. The construction relies on building Neyman-orthogonal
equations that are approximately invariant to perturbations of the nuisance
parameters, including the Riesz representer for the linear functionals. We use
L1-regularized methods to learn approximations to the regression function and
the Riesz representer, and construct the estimator for the linear functionals
as the solution to the orthogonal estimating equations. We establish that under
weak assumptions the estimator concentrates in a 1/root n neighborhood of the
target with deviations controlled by the normal laws, and the estimator attains
the semi-parametric efficiency bound in many cases. In particular, either the
approximation to the regression function or the approximation to the Riesz
representer can be &quot;dense&quot; as long as one of them is sufficiently &quot;sparse&quot;. Our
main results are non-asymptotic and imply asymptotic uniform validity over
large classes of models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chernozhukov_V/0/1/0/all/0/1&quot;&gt;Victor Chernozhukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Newey_W/0/1/0/all/0/1&quot;&gt;Whitney Newey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Robins_J/0/1/0/all/0/1&quot;&gt;James Robins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08735">
<title>A DIRT-T Approach to Unsupervised Domain Adaptation. (arXiv:1802.08735v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08735</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain adaptation refers to the problem of leveraging labeled data in a
source domain to learn an accurate model in a target domain where labels are
scarce or unavailable. A recent approach for finding a common representation of
the two domains is via domain adversarial training (Ganin &amp;amp; Lempitsky, 2015),
which attempts to induce a feature extractor that matches the source and target
feature distributions in some feature space. However, domain adversarial
training faces two critical limitations: 1) if the feature extraction function
has high-capacity, then feature distribution matching is a weak constraint, 2)
in non-conservative domain adaptation (where no single classifier can perform
well in both the source and target domains), training the model to do well on
the source domain hurts performance on the target domain. In this paper, we
address these issues through the lens of the cluster assumption, i.e., decision
boundaries should not cross high-density data regions. We propose two novel and
related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model,
which combines domain adversarial training with a penalty term that punishes
the violation the cluster assumption; 2) the Decision-boundary Iterative
Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model
as initialization and employs natural gradient steps to further minimize the
cluster assumption violation. Extensive empirical results demonstrate that the
combination of these two models significantly improve the state-of-the-art
performance on the digit, traffic sign, and Wi-Fi recognition domain adaptation
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shu_R/0/1/0/all/0/1&quot;&gt;Rui Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bui_H/0/1/0/all/0/1&quot;&gt;Hung H. Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Narui_H/0/1/0/all/0/1&quot;&gt;Hirokazu Narui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04051">
<title>Representation Learning over Dynamic Graphs. (arXiv:1803.04051v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04051</link>
<description rdf:parseType="Literal">&lt;p&gt;How can we effectively encode evolving information over dynamic graphs into
low-dimensional representations? In this paper, we propose DyRep, an inductive
deep representation learning framework that learns a set of functions to
efficiently produce low-dimensional node embeddings that evolves over time. The
learned embeddings drive the dynamics of two key processes namely,
communication and association between nodes in dynamic graphs. These processes
exhibit complex nonlinear dynamics that evolve at different time scales and
subsequently contribute to the update of node embeddings. We employ a
time-scale dependent multivariate point process model to capture these
dynamics. We devise an efficient unsupervised learning procedure and
demonstrate that our approach significantly outperforms representative
baselines on two real-world datasets for the problem of dynamic link prediction
and event time prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_R/0/1/0/all/0/1&quot;&gt;Rakshit Trivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1&quot;&gt;Mehrdad Farajtabar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswal_P/0/1/0/all/0/1&quot;&gt;Prasenjeet Biswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04926">
<title>Active Reinforcement Learning with Monte-Carlo Tree Search. (arXiv:1803.04926v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04926</link>
<description rdf:parseType="Literal">&lt;p&gt;Active Reinforcement Learning (ARL) is a twist on RL where the agent observes
reward information only if it pays a cost. This subtle change makes exploration
substantially more challenging. Powerful principles in RL like optimism,
Thompson sampling, and random exploration do not help with ARL. We relate ARL
in tabular environments to Bayes-Adaptive MDPs. We provide an ARL algorithm
using Monte-Carlo Tree Search that is asymptotically Bayes optimal.
Experimentally, this algorithm is near-optimal on small Bandit problems and
MDPs. On larger MDPs it outperforms a Q-learner augmented with specialised
heuristics for ARL. By analysing exploration behaviour in detail, we uncover
obstacles to scaling up simulation-based algorithms for ARL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulze_S/0/1/0/all/0/1&quot;&gt;Sebastian Schulze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1&quot;&gt;Owain Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06058">
<title>Constant-Time Predictive Distributions for Gaussian Processes. (arXiv:1803.06058v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06058</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most compelling features of Gaussian process (GP) regression is
its ability to provide well calibrated posterior distributions. Recent advances
in inducing point methods have drastically sped up marginal likelihood and
posterior mean computations, leaving posterior covariance estimation and
sampling as the remaining computational bottlenecks. In this paper we address
this shortcoming by using the Lanczos decomposition algorithm to rapidly
approximate the predictive covariance matrix. Our approach, which we refer to
as LOVE (LanczOs Variance Estimates), substantially reduces the time and space
complexity over any previous method. In practice, it can compute predictive
covariances up to 2,000 times faster and draw samples 18,000 time faster than
existing methods, all without sacrificing accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1&quot;&gt;Geoff Pleiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Jacob R. Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1&quot;&gt;Kilian Q. Weinberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05999">
<title>Escaping Saddles with Stochastic Gradients. (arXiv:1803.05999v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1803.05999</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze the variance of stochastic gradients along negative curvature
directions in certain non-convex machine learning models and show that
stochastic gradients exhibit a strong component along these directions.
Furthermore, we show that - contrary to the case of isotropic noise - this
variance is proportional to the magnitude of the corresponding eigenvalues and
not decreasing in the dimensionality. Based upon this observation we propose a
new assumption under which we show that the injection of explicit, isotropic
noise usually applied to make gradient descent escape saddle points can
successfully be replaced by a simple SGD step. Additionally - and under the
same condition - we derive the first convergence rate for plain SGD to a
second-order stationary point in a number of iterations that is independent of
the problem dimension.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daneshmand_H/0/1/0/all/0/1&quot;&gt;Hadi Daneshmand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1&quot;&gt;Jonas Kohler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1&quot;&gt;Aurelien Lucchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1&quot;&gt;Thomas Hofmann&lt;/a&gt;</dc:creator>
</item></rdf:RDF>