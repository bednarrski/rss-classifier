<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-16T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04602"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03891"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01937"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01942"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05373"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06104"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06146"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06148"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06250"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06368"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.03937"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00705"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05081"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06095"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06208"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06230"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06258"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06332"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06350"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06439"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06441"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.03391"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.08024"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.01445"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01572"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09007"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06218"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08841"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01955"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04784"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05857"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.04602">
<title>On the organization of grid and place cells: Neural de-noising via subspace learning. (arXiv:1712.04602v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04602</link>
<description rdf:parseType="Literal">&lt;p&gt;Place cells in the hippocampus are active when an animal visits a certain
location (referred to as a place field) within an environment. Grid cells in
the medial entorhinal cortex (MEC) respond at multiple locations, with firing
fields that form a periodic and hexagonal tiling of the environment. The joint
activity of grid and place cell populations, as a function of location, forms a
neural code for space. An ensemble of codes is generated by varying grid and
place cell population parameters. For each code in this ensemble, codewords are
generated by stimulating a network with a discrete set of locations. In this
manuscript, we develop an understanding of the relationships between coding
theoretic properties of these combined populations and code construction
parameters. These relationships are revisited by measuring the performances of
biologically realizable algorithms implemented by networks of place and grid
cell populations, as well as constraint neurons, which perform de-noising
operations. Objectives of this work include the investigation of coding
theoretic limitations of the mammalian neural code for location and how
communication between grid and place cell networks may improve the accuracy of
each population&apos;s representation. Simulations demonstrate that de-noising
mechanisms analyzed here can significantly improve fidelity of this neural
representation of space. Further, patterns observed in connectivity of each
population of simulated cells suggest that
inter-hippocampal-medial-entorhinal-cortical connectivity decreases downward
along the dorsoventral axis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Schwartz_D/0/1/0/all/0/1&quot;&gt;David M. Schwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Koyluoglu_O/0/1/0/all/0/1&quot;&gt;O. Ozan Koyluoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03891">
<title>Multifunctionality in embodied agents: Three levels of neural reuse. (arXiv:1802.03891v4 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03891</link>
<description rdf:parseType="Literal">&lt;p&gt;The brain in conjunction with the body is able to adapt to new environments
and perform multiple behaviors through reuse of neural resources and transfer
of existing behavioral traits. Although mechanisms that underlie this ability
are not well understood, they are largely attributed to neuromodulation. In
this work, we demonstrate that an agent can be multifunctional using the same
sensory and motor systems across behaviors, in the absence of modulatory
mechanisms. Further, we lay out the different levels at which neural reuse can
occur through a dynamical filtering of the brain-body-environment system&apos;s
operation: structural network, autonomous dynamics, and transient dynamics.
Notably, transient dynamics reuse could only be explained by studying the
brain-body-environment system as a whole and not just the brain. The
multifunctional agent we present here demonstrates neural reuse at all three
levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Candadai_M/0/1/0/all/0/1&quot;&gt;Madhavun Candadai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izquierdo_E/0/1/0/all/0/1&quot;&gt;Eduardo Izquierdo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01937">
<title>Superconducting Optoelectronic Neurons III: Synaptic Plasticity. (arXiv:1805.01937v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1805.01937</link>
<description rdf:parseType="Literal">&lt;p&gt;As a means of dynamically reconfiguring the synaptic weight of a
superconducting optoelectronic loop neuron, a superconducting flux storage loop
is inductively coupled to the synaptic current bias of the neuron. A standard
flux memory cell is used to achieve a binary synapse, and loops capable of
storing many flux quanta are used to enact multi-stable synapses. Circuits are
designed to implement supervised learning wherein current pulses add or remove
flux from the loop to strengthen or weaken the synaptic weight. Designs are
presented for circuits with hundreds of intermediate synaptic weights between
minimum and maximum strengths. Circuits for implementing unsupervised learning
are modeled using two photons to strengthen and two photons to weaken the
synaptic weight via Hebbian and anti-Hebbian learning rules, and techniques are
proposed to control the learning rate. Implementation of short-term plasticity,
homeostatic plasticity, and metaplasticity in loop neurons is discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shainline_J/0/1/0/all/0/1&quot;&gt;Jeffrey M. Shainline&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCaughan_A/0/1/0/all/0/1&quot;&gt;Adam N. McCaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_S/0/1/0/all/0/1&quot;&gt;Sonia M. Buckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donnelly_C/0/1/0/all/0/1&quot;&gt;Christine A. Donnelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castellanos_Beltran_M/0/1/0/all/0/1&quot;&gt;Manuel Castellanos-Beltran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_M/0/1/0/all/0/1&quot;&gt;Michael L. Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirin_R/0/1/0/all/0/1&quot;&gt;Richard P. Mirin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1&quot;&gt;Sae Woo Nam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01942">
<title>Superconducting Optoelectronic Neurons V: Networks and Scaling. (arXiv:1805.01942v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1805.01942</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks of superconducting optoelectronic neurons are investigated for their
near-term technological potential and long-term physical limitations. Networks
with short average path length, high clustering coefficient, and power-law
degree distribution are designed using a growth model that assigns connections
between new and existing nodes based on spatial distance as well as degree of
existing nodes. The network construction algorithm is scalable to arbitrary
levels of network hierarchy and achieves systems with fractal spatial
properties and efficient wiring. By modeling the physical size of
superconducting optoelectronic neurons, we calculate the area of these
networks. A system with 8100 neurons and 330,430 total synapses will fit on a
1\,cm $\times$ 1\,cm die. Systems of millions of neurons with hundreds of
millions of synapses will fit on a 300\,mm wafer. For multi-wafer assemblies,
communication at light speed enables a neuronal pool the size of a large data
center comprising 100 trillion neurons with coherent oscillations at 1\,MHz.
Assuming a power law frequency distribution, as is necessary for self-organized
criticality, we calculate the power consumption of the networks. We find the
use of single photons for communication and superconducting circuits for
computation leads to power density low enough to be cooled by liquid $^4$He for
networks of any scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shainline_J/0/1/0/all/0/1&quot;&gt;Jeffrey M. Shainline&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiles_J/0/1/0/all/0/1&quot;&gt;Jeff Chiles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_S/0/1/0/all/0/1&quot;&gt;Sonia M. Buckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCaughan_A/0/1/0/all/0/1&quot;&gt;Adam N. McCaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirin_R/0/1/0/all/0/1&quot;&gt;Richard P. Mirin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1&quot;&gt;Sae Woo Nam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05373">
<title>DeepEM: Deep 3D ConvNets With EM For Weakly Supervised Pulmonary Nodule Detection. (arXiv:1805.05373v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05373</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently deep learning has been witnessing widespread adoption in various
medical image applications. However, training complex deep neural nets requires
large-scale datasets labeled with ground truth, which are often unavailable in
many medical image domains. For instance, to train a deep neural net to detect
pulmonary nodules in lung computed tomography (CT) images, current practice is
to manually label nodule locations and sizes in many CT images to construct a
sufficiently large training dataset, which is costly and difficult to scale. On
the other hand, electronic medical records (EMR) contain plenty of partial
information on the content of each medical image. In this work, we explore how
to tap this vast, but currently unexplored data source to improve pulmonary
nodule detection. We propose DeepEM, a novel deep 3D ConvNet framework
augmented with expectation-maximization (EM), to mine weakly supervised labels
in EMRs for pulmonary nodule detection. Experimental results show that DeepEM
can lead to 1.5\% and 3.9\% average improvement in free-response receiver
operating characteristic (FROC) scores on LUNA16 and Tianchi datasets,
respectively, demonstrating the utility of incomplete information in EMRs for
improving deep learning
algorithms.\footnote{https://github.com/uci-cbcl/DeepEM-for-Weakly-Supervised-Detection.git}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vang_Y/0/1/0/all/0/1&quot;&gt;Yeeleng S. Vang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yufang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06104">
<title>Privacy Preservation in Location-Based Services: A Novel Metric and Attack Model. (arXiv:1805.06104v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1805.06104</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have seen rising needs for location-based services in our
everyday life. Aside from the many advantages provided by these services, they
have caused serious concerns regarding the location privacy of users. An
adversary such as an untrusted location-based server can monitor the queried
locations by a user to infer critical information such as the user&apos;s home
address, health conditions, shopping habits, etc. To address this issue,
dummy-based algorithms have been developed to increase the anonymity of users,
and thus, protecting their privacy. Unfortunately, the existing algorithms only
consider a limited amount of side information known by an adversary which may
face more serious challenges in practice. In this paper, we incorporate a new
type of side information based on consecutive location changes of users and
propose a new metric called transition-entropy to investigate the location
privacy preservation, followed by two algorithms to improve the
transition-entropy for a given dummy generation algorithm. Then, we develop an
attack model based on the Viterbi algorithm which can significantly threaten
the location privacy of the users. Next, in order to protect the users from
Viterbi attack, we propose an algorithm called robust dummy generation (RDG)
which can resist against the Viterbi attack while maintaining a high
performance in terms of the privacy metrics introduced in the paper. All the
algorithms are applied and analyzed on a real-life dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaham_S/0/1/0/all/0/1&quot;&gt;Sina Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Ming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zihuai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06146">
<title>Optimized Computation Offloading Performance in Virtual Edge Computing Systems via Deep Reinforcement Learning. (arXiv:1805.06146v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.06146</link>
<description rdf:parseType="Literal">&lt;p&gt;To improve the quality of computation experience for mobile devices,
mobile-edge computing (MEC) is a promising paradigm by providing computing
capabilities in close proximity within a sliced radio access network (RAN),
which supports both traditional communication and MEC services. Nevertheless,
the design of computation offloading policies for a virtual MEC system remains
challenging. Specifically, whether to execute a computation task at the mobile
device or to offload it for MEC server execution should adapt to the
time-varying network dynamics. In this paper, we consider MEC for a
representative mobile user in an ultra-dense sliced RAN, where multiple base
stations (BSs) are available to be selected for computation offloading. The
problem of solving an optimal computation offloading policy is modelled as a
Markov decision process, where our objective is to maximize the long-term
utility performance whereby an offloading decision is made based on the task
queue state, the energy queue state as well as the channel qualities between MU
and BSs. To break the curse of high dimensionality in state space, we first
propose a double deep Q-network (DQN) based strategic computation offloading
algorithm to learn the optimal policy without knowing a priori knowledge of
network dynamics. Then motivated by the additive structure of the utility
function, a Q-function decomposition technique is combined with the double DQN,
which leads to novel learning algorithm for the solving of stochastic
computation offloading. Numerical experiments show that our proposed learning
algorithms achieve a significant improvement in computation offloading
performance compared with the baseline policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xianfu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Honggang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Celimuge Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shiwen Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yusheng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1&quot;&gt;Mehdi Bennis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06148">
<title>Critical Points to Determine Persistence Homology. (arXiv:1805.06148v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.06148</link>
<description rdf:parseType="Literal">&lt;p&gt;Computation of the simplicial complexes of a large point cloud often relies
on extracting a sample, to reduce the associated computational burden. The
study considers sampling critical points of a Morse function associated to a
point cloud, to approximate the Vietoris-Rips complex or the witness complex
and compute persistence homology. The effectiveness of the novel approach is
compared with the farthest point sampling, in a context of classifying human
face images into ethnics groups using persistence homology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asirimath_C/0/1/0/all/0/1&quot;&gt;Charmin Asirimath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratnayake_J/0/1/0/all/0/1&quot;&gt;Jayampathy Ratnayake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weeraddana_C/0/1/0/all/0/1&quot;&gt;Chathuranga Weeraddana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06250">
<title>Learning Representations of Spatial Displacement through Sensorimotor Prediction. (arXiv:1805.06250v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1805.06250</link>
<description rdf:parseType="Literal">&lt;p&gt;Robots act in their environment through sequences of continuous motor
commands. Because of the dimensionality of the motor space, as well as the
infinite possible combinations of successive motor commands, agents need
compact representations that capture the structure of the resulting
displacements. In the case of an autonomous agent with no a priori knowledge
about its sensorimotor apparatus, this compression has to be learned. We
propose to use Recurrent Neural Networks to encode motor sequences into a
compact representation, which is used to predict the consequence of motor
sequences in term of sensory changes. We show that sensory prediction can
successfully guide the compression of motor sequences into representations that
are organized topologically in term of spatial displacement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortiz_M/0/1/0/all/0/1&quot;&gt;Michael Garcia Ortiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laflaquiere_A/0/1/0/all/0/1&quot;&gt;Alban Laflaqui&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06368">
<title>Strict Very Fast Decision Tree: a memory conservative algorithm for data stream mining. (arXiv:1805.06368v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.06368</link>
<description rdf:parseType="Literal">&lt;p&gt;Dealing with memory and time constraints are current challenges when learning
from data streams with a massive amount of data. Many algorithms have been
proposed to handle these difficulties, among them, the Very Fast Decision Tree
(VFDT) algorithm. Although the VFDT has been widely used in data stream mining,
in the last years, several authors have suggested modifications to increase its
performance, putting aside memory concerns by proposing memory-costly
solutions. Besides, most data stream mining solutions have been centred around
ensembles, which combine the memory costs of their weak learners, usually
VFDTs. To reduce the memory cost, keeping the predictive performance, this
study proposes the Strict VFDT (SVFDT), a novel algorithm based on the VFDT.
The SVFDT algorithm minimises unnecessary tree growth, substantially reducing
memory usage and keeping competitive predictive performance. Moreover, since it
creates much more shallow trees than VFDT, SVFDT can achieve a shorter
processing time. Experiments were carried out comparing the SVFDT with the VFDT
in 11 benchmark data stream datasets. This comparison assessed the trade-off
between accuracy, memory, and processing time. Statistical analysis showed that
the proposed algorithm obtained similar predictive performance and
significantly reduced processing time and memory use. Thus, SVFDT is a suitable
option for data stream mining with memory and time limitations, recommended as
a weak learner in ensemble-based solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_V/0/1/0/all/0/1&quot;&gt;Victor Guilherme Turrisi da Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leon_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Carlos Ponce de Leon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_F/0/1/0/all/0/1&quot;&gt;Ferreira de Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junior_S/0/1/0/all/0/1&quot;&gt;Sylvio Barbon Junior&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.03937">
<title>PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning. (arXiv:1710.03937v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.03937</link>
<description rdf:parseType="Literal">&lt;p&gt;We present PRM-RL, a hierarchical method for long-range navigation task
completion that combines sampling based path planning with reinforcement
learning (RL). The RL agents learn short-range, point-to-point navigation
policies that capture robot dynamics and task constraints without knowledge of
the large-scale topology. Next, the sampling-based planners provide roadmaps
which connect robot configurations that can be successfully navigated by the RL
agent. The same RL agents are used to control the robot under the direction of
the planning, enabling long-range navigation. We use the Probabilistic Roadmaps
(PRMs) for the sampling-based planner. The RL agents are constructed using
feature-based and deep neural net policies in continuous state and action
spaces. We evaluate PRM-RL, both in simulation and on-robot, on two navigation
tasks with non-trivial robot dynamics: end-to-end differential drive indoor
navigation in office environments, and aerial cargo delivery in urban
environments with load displacement constraints. Our results show improvement
in task completion over both RL agents on their own and traditional
sampling-based planners. In the indoor navigation task, PRM-RL successfully
completes up to 215 m long trajectories under noisy sensor conditions, and the
aerial cargo delivery completes flights over 1000 m without violating the task
constraints in an environment 63 million times larger than used in training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1&quot;&gt;Aleksandra Faust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramirez_O/0/1/0/all/0/1&quot;&gt;Oscar Ramirez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiser_M/0/1/0/all/0/1&quot;&gt;Marek Fiser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oslund_K/0/1/0/all/0/1&quot;&gt;Kenneth Oslund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francis_A/0/1/0/all/0/1&quot;&gt;Anthony Francis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davidson_J/0/1/0/all/0/1&quot;&gt;James Davidson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tapia_L/0/1/0/all/0/1&quot;&gt;Lydia Tapia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00705">
<title>Investigating Audio, Visual, and Text Fusion Methods for End-to-End Automatic Personality Prediction. (arXiv:1805.00705v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00705</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a tri-modal architecture to predict Big Five personality trait
scores from video clips with different channels for audio, text, and video
data. For each channel, stacked Convolutional Neural Networks are employed. The
channels are fused both on decision-level and by concatenating their respective
fully connected layers. It is shown that a multimodal fusion approach
outperforms each single modality channel, with an improvement of 9.4\% over the
best individual modality (video). Full backpropagation is also shown to be
better than a linear combination of modalities, meaning complex interactions
between modalities can be leveraged to build better models. Furthermore, we can
see the prediction relevance of each modality for each trait. The described
model can be used to increase the emotional intelligence of virtual agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampman_O/0/1/0/all/0/1&quot;&gt;Onno Kampman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1&quot;&gt;Elham J. Barezi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertero_D/0/1/0/all/0/1&quot;&gt;Dario Bertero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1&quot;&gt;Pascale Fung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05081">
<title>Constructing Narrative Event Evolutionary Graph for Script Event Prediction. (arXiv:1805.05081v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05081</link>
<description rdf:parseType="Literal">&lt;p&gt;Script event prediction requires a model to predict the subsequent event
given an existing event context. Previous models based on event pairs or event
chains cannot make full use of dense event connections, which may limit their
capability of event prediction. To remedy this, we propose constructing an
event graph to better utilize the event network information for script event
prediction. In particular, we first extract narrative event chains from large
quantities of news corpus, and then construct a narrative event evolutionary
graph (NEEG) based on the extracted chains. NEEG can be seen as a knowledge
base that describes event evolutionary principles and patterns. To solve the
inference problem on NEEG, we present a scaled graph neural network (SGNN) to
model event interactions and learn better event representations. Instead of
computing the representations on the whole graph, SGNN processes only the
concerned nodes each time, which makes our model feasible to large-scale
graphs. By comparing the similarity between input context event representations
and candidate event representations, we can choose the most reasonable
subsequent event. Experimental results on widely used New York Times corpus
demonstrate that our model significantly outperforms state-of-the-art baseline
methods, by using standard multiple choice narrative cloze evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06095">
<title>Semi-Blind Inference of Topologies and Dynamical Processes over Graphs. (arXiv:1805.06095v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.06095</link>
<description rdf:parseType="Literal">&lt;p&gt;Network science provides valuable insights across numerous disciplines
including sociology, biology, neuroscience and engineering. A task of major
practical importance in these application domains is inferring the network
structure from noisy observations at a subset of nodes. Available methods for
topology inference typically assume that the process over the network is
observed at all nodes. However, application-specific constraints may prevent
acquiring network-wide observations. Alleviating the limited flexibility of
existing approaches, this work advocates structural models for graph processes
and develops novel algorithms for joint inference of the network topology and
processes from partial nodal observations. Structural equation models (SEMs)
and structural vector autoregressive models (SVARMs) have well-documented
merits in identifying even directed topologies of complex graphs; while SEMs
capture contemporaneous causal dependencies among nodes, SVARMs further account
for time-lagged influences. This paper develops algorithms that iterate between
inferring directed graphs that &quot;best&quot; fit the data, and estimating the network
processes at reduced computational complexity by leveraging tools related to
Kalman smoothing. To further accommodate delay-sensitive applications, an
online joint inference approach is put forth that even tracks time-evolving
topologies. Furthermore, conditions for identifying the network topology given
partial observations are specified. It is proved that the required number of
observations for unique identification reduces significantly when the network
structure is sparse. Numerical tests with synthetic as well as real datasets
corroborate the effectiveness of the novel approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ioannidis_V/0/1/0/all/0/1&quot;&gt;Vassilis N. Ioannidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yanning Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06208">
<title>CDM: Compound dissimilarity measure and an application to fingerprinting-based positioning. (arXiv:1805.06208v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1805.06208</link>
<description rdf:parseType="Literal">&lt;p&gt;A non-vector-based dissimilarity measure is proposed by combining
vector-based distance metrics and set operations. This proposed compound
dissimilarity measure (CDM) is applicable to quantify similarity of collections
of attribute/feature pairs where not all attributes are present in all
collections. This is a typical challenge in the context of e.g.,
fingerprinting-based positioning (FbP). Compared to vector-based distance
metrics (e.g., Minkowski), the merits of the proposed CDM are i) the data do
not need to be converted to vectors of equal dimension, ii) shared and unshared
attributes can be weighted differently within the assessment, and iii)
additional degrees of freedom within the measure allow to adapt its properties
to application needs in a data-driven way. We indicate the validity of the
proposed CDM by demonstrating the improvements of the positioning performance
of fingerprinting-based WLAN indoor positioning using four different datasets,
three of them publicly available. When processing these datasets using CDM
instead of conventional distance metrics the accuracy of identifying buildings
and floors improves by about 5% on average. The 2d positioning errors in terms
of root mean squared error (RMSE) are reduced by a factor of two, and the
percentage of position solutions with less than 2m error improves by over 10%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Caifa Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wieser_A/0/1/0/all/0/1&quot;&gt;Andreas Wieser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06230">
<title>Towards Explaining Anomalies: A Deep Taylor Decomposition of One-Class Models. (arXiv:1805.06230v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.06230</link>
<description rdf:parseType="Literal">&lt;p&gt;A common machine learning task is to discriminate between normal and
anomalous data points. In practice, it is not always sufficient to reach high
accuracy at this task, one also would like to understand why a given data point
has been predicted in a certain way. We present a new principled approach for
one-class SVMs that decomposes outlier predictions in terms of input variables.
The method first recomposes the one-class model as a neural network with
distance functions and min-pooling, and then performs a deep Taylor
decomposition (DTD) of the model output. The proposed One-Class DTD is
applicable to a number of common distance-based SVM kernels and is able to
reliably explain a wide set of data anomalies. Furthermore, it outperforms
baselines such as sensitivity analysis, nearest neighbor, or simple edge
detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kauffmann_J/0/1/0/all/0/1&quot;&gt;Jacob Kauffmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Montavon_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;goire Montavon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06258">
<title>Structured nonlinear variable selection. (arXiv:1805.06258v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.06258</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate structured sparsity methods for variable selection in
regression problems where the target depends nonlinearly on the inputs. We
focus on general nonlinear functions not limiting a priori the function space
to additive models. We propose two new regularizers based on partial
derivatives as nonlinear equivalents of group lasso and elastic net. We
formulate the problem within the framework of learning in reproducing kernel
Hilbert spaces and show how the variational problem can be reformulated into a
more practical finite dimensional equivalent. We develop a new algorithm
derived from the ADMM principles that relies solely on closed forms of the
proximal operators. We explore the empirical properties of our new algorithm
for Nonlinear Variable Selection based on Derivatives (NVSD) on a set of
experiments and confirm favourable properties of our structured-sparsity models
and the algorithm in terms of both prediction and variable selection accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gregorova_M/0/1/0/all/0/1&quot;&gt;Magda Gregorov&amp;#xe1;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kalousis_A/0/1/0/all/0/1&quot;&gt;Alexandros Kalousis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marchand_Maillet_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Marchand-Maillet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06332">
<title>Multi-task Learning for Macromolecule Classification, Segmentation and Coarse Structural Recovery in Cryo-Tomography. (arXiv:1805.06332v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.06332</link>
<description rdf:parseType="Literal">&lt;p&gt;Cellular Electron Cryo-Tomography (CECT) is a powerful 3D imaging tool for
studying the native structure and organization of macromolecules inside single
cells. For systematic recognition and recovery of macromolecular structures
captured by CECT, methods for several important tasks such as subtomogram
classification and semantic segmentation have been developed. However, the
recognition and recovery of macromolecular structures are still very difficult
due to high molecular structural diversity, crowding molecular environment, and
the imaging limitations of CECT. In this paper, we propose a novel multi-task
3D convolutional neural network model for simultaneous classification,
segmentation, and coarse structural recovery of macromolecules of interest in
subtomograms. In our model, the learned image features of one task are shared
and thereby mutually reinforce the learning of other tasks. Evaluated on
realistically simulated and experimental CECT data, our multi-task learning
model outperformed all single-task learning methods for classification and
segmentation. In addition, we demonstrate that our model can generalize to
discover, segment and recover novel structures that do not exist in the
training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiangrui Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaiwen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Min Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06350">
<title>Approximating the Void: Learning Stochastic Channel Models from Observation with Variational Generative Adversarial Networks. (arXiv:1805.06350v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.06350</link>
<description rdf:parseType="Literal">&lt;p&gt;Channel modeling is a critical topic when considering designing, learning, or
evaluating the performance of any communications system. Prior work in manual
modulation system design or learned modulation system design has largely
focused on simplified analytic channel models such as additive white Gaussian
noise (AWGN) or Rayleigh fading channels, in more recent work we consider the
usage of generative adversarial networks (GANs) to jointly approximate of a
wireless channel response and design an efficient encoding and decoding of
information to robustly survive it. In this paper, we focus more specifically
on characterizing how well a GAN can capture the stochastic nature of a typical
wireless channel response, and the topic of effectively designing the network
and loss function to accurately capture its stochastic behavior in a
probabilistic sense. We illustrate the problems with certain approaches and
share results capturing the performance of such as system over a range channel
distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OShea_T/0/1/0/all/0/1&quot;&gt;Timothy J. O&amp;#x27;Shea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1&quot;&gt;Tamoghna Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+West_N/0/1/0/all/0/1&quot;&gt;Nathan West&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06386">
<title>Neural Multi-scale Image Compression. (arXiv:1805.06386v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.06386</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents a new lossy image compression method that utilizes the
multi-scale features of natural images. Our model consists of two networks:
multi-scale lossy autoencoder and parallel multi-scale lossless coder. The
multi-scale lossy autoencoder extracts the multi-scale image features to
quantized variables and the parallel multi-scale lossless coder enables rapid
and accurate lossless coding of the quantized variables via encoding/decoding
the variables in parallel. Our proposed model achieves comparable performance
to the state-of-the-art model on Kodak and RAISE-1k dataset images, and it
encodes a PNG image of size $768 \times 512$ in 70 ms with a single GPU and a
single CPU process and decodes it into a high-fidelity image in approximately
200 ms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nakanishi_K/0/1/0/all/0/1&quot;&gt;Ken Nakanishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maeda_S/0/1/0/all/0/1&quot;&gt;Shin-ichi Maeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Miyato_T/0/1/0/all/0/1&quot;&gt;Takeru Miyato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Okanohara_D/0/1/0/all/0/1&quot;&gt;Daisuke Okanohara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06439">
<title>Prediction Rule Reshaping. (arXiv:1805.06439v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.06439</link>
<description rdf:parseType="Literal">&lt;p&gt;Two methods are proposed for high-dimensional shape-constrained regression
and classification. These methods reshape pre-trained prediction rules to
satisfy shape constraints like monotonicity and convexity. The first method can
be applied to any pre-trained prediction rule, while the second method deals
specifically with random forests. In both cases, efficient algorithms are
developed for computing the estimators, and experiments are performed to
demonstrate their performance on four datasets. We find that reshaping methods
enforce shape constraints without compromising predictive accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bonakdarpour_M/0/1/0/all/0/1&quot;&gt;Matt Bonakdarpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chatterjee_S/0/1/0/all/0/1&quot;&gt;Sabyasachi Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barber_R/0/1/0/all/0/1&quot;&gt;Rina Foygel Barber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lafferty_J/0/1/0/all/0/1&quot;&gt;John Lafferty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06441">
<title>Regularized Finite Dimensional Kernel Sobolev Discrepancy. (arXiv:1805.06441v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.06441</link>
<description rdf:parseType="Literal">&lt;p&gt;We show in this note that the Sobolev Discrepancy introduced in Mroueh et al
in the context of generative adversarial networks, is actually the weighted
negative Sobolev norm $||.||_{\dot{H}^{-1}(\nu_q)}$, that is known to linearize
the Wasserstein $W_2$ distance and plays a fundamental role in the dynamic
formulation of optimal transport of Benamou and Brenier. Given a Kernel with
finite dimensional feature map we show that the Sobolev discrepancy can be
approximated from finite samples. Assuming this discrepancy is finite, the
error depends on the approximation error in the function space induced by the
finite dimensional feature space kernel and on a statistical error due to the
finite sample approximation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1&quot;&gt;Youssef Mroueh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.03391">
<title>Unbiased split variable selection for random survival forests using maximally selected rank statistics. (arXiv:1605.03391v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1605.03391</link>
<description rdf:parseType="Literal">&lt;p&gt;The most popular approach for analyzing survival data is the Cox regression
model. The Cox model may, however, be misspecified, and its proportionality
assumption may not always be fulfilled. An alternative approach for survival
prediction is random forests for survival outcomes. The standard split
criterion for random survival forests is the log-rank test statistics, which
favors splitting variables with many possible split points. Conditional
inference forests avoid this split variable selection bias. However, linear
rank statistics are utilized by default in conditional inference forests to
select the optimal splitting variable, which cannot detect non-linear effects
in the independent variables. An alternative is to use maximally selected rank
statistics for the split point selection. As in conditional inference forests,
splitting variables are compared on the p-value scale. However, instead of the
conditional Monte-Carlo approach used in conditional inference forests, p-value
approximations are employed. We describe several p-value approximations and the
implementation of the proposed random forest approach. A simulation study
demonstrates that unbiased split variable selection is possible. However, there
is a trade-off between unbiased split variable selection and runtime. In
benchmark studies of prediction performance on simulated and real datasets the
new method performs better than random survival forests if informative
dichotomous variables are combined with uninformative variables with more
categories and better than conditional inference forests if non-linear
covariate effects are included. In a runtime comparison the method proves to be
computationally faster than both alternatives, if a simple p-value
approximation is used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wright_M/0/1/0/all/0/1&quot;&gt;Marvin N. Wright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dankowski_T/0/1/0/all/0/1&quot;&gt;Theresa Dankowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ziegler_A/0/1/0/all/0/1&quot;&gt;Andreas Ziegler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.08024">
<title>EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces. (arXiv:1611.08024v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1611.08024</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain computer interfaces (BCI) enable direct communication with a computer,
using neural activity as the control signal. This neural signal is generally
chosen from a variety of well-studied electroencephalogram (EEG) signals. For a
given BCI paradigm, feature extractors and classifiers are tailored to the
distinct characteristics of its expected EEG control signal, limiting its
application to that specific signal. Convolutional Neural Networks (CNNs),
which have been used in computer vision and speech recognition, have
successfully been applied to EEG-based BCIs; however, they have mainly been
applied to single BCI paradigms and thus it remains unclear how these
architectures generalize to other paradigms. Here, we ask if we can design a
single CNN architecture to accurately classify EEG signals from different BCI
paradigms, while simultaneously being as compact as possible. In this work we
introduce EEGNet, a compact convolutional network for EEG-based BCIs. We
introduce the use of depthwise and separable convolutions to construct an
EEG-specific model which encapsulates well-known EEG feature extraction
concepts for BCI. We compare EEGNet to current state-of-the-art approaches
across four BCI paradigms: P300 visual-evoked potentials, error-related
negativity responses (ERN), movement-related cortical potentials (MRCP), and
sensory motor rhythms (SMR). We show that EEGNet generalizes across paradigms
better than the reference algorithms when only limited training data is
available. We demonstrate three different approaches to visualize the contents
of a trained EEGNet model to enable interpretation of the learned features. Our
results suggest that EEGNet is robust enough to learn a wide variety of
interpretable features over a range of BCI tasks, suggesting that the observed
performances were not due to artifact or noise sources in the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawhern_V/0/1/0/all/0/1&quot;&gt;Vernon J. Lawhern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solon_A/0/1/0/all/0/1&quot;&gt;Amelia J. Solon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waytowich_N/0/1/0/all/0/1&quot;&gt;Nicholas R. Waytowich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordon_S/0/1/0/all/0/1&quot;&gt;Stephen M. Gordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hung_C/0/1/0/all/0/1&quot;&gt;Chou P. Hung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lance_B/0/1/0/all/0/1&quot;&gt;Brent J. Lance&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.01445">
<title>Batched Large-scale Bayesian Optimization in High-dimensional Spaces. (arXiv:1706.01445v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.01445</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization (BO) has become an effective approach for black-box
function optimization problems when function evaluations are expensive and the
optimum can be achieved within a relatively small number of queries. However,
many cases, such as the ones with high-dimensional inputs, may require a much
larger number of observations for optimization. Despite an abundance of
observations thanks to parallel experiments, current BO techniques have been
limited to merely a few thousand observations. In this paper, we propose
ensemble Bayesian optimization (EBO) to address three current challenges in BO
simultaneously: (1) large-scale observations; (2) high dimensional input
spaces; and (3) selections of batch queries that balance quality and diversity.
The key idea of EBO is to operate on an ensemble of additive Gaussian process
models, each of which possesses a randomized strategy to divide and conquer. We
show unprecedented, previously impossible results of scaling up BO to tens of
thousands of observations within minutes of computation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gehring_C/0/1/0/all/0/1&quot;&gt;Clement Gehring&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohli_P/0/1/0/all/0/1&quot;&gt;Pushmeet Kohli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01572">
<title>Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert Spaces. (arXiv:1712.01572v2 [math.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01572</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer operators such as the Perron--Frobenius or Koopman operator play an
important role in the global analysis of complex dynamical systems. The
eigenfunctions of these operators can be used to detect metastable sets, to
project the dynamics onto the dominant slow processes, or to separate
superimposed signals. We extend transfer operator theory to reproducing kernel
Hilbert spaces and show that these operators are related to Hilbert space
representations of conditional distributions, known as conditional mean
embeddings in the machine learning community. Moreover, numerical methods to
compute empirical estimates of these embeddings are akin to data-driven methods
for the approximation of transfer operators such as extended dynamic mode
decomposition and its variants. One main benefit of the presented kernel-based
approaches is that these methods can be applied to any domain where a
similarity measure given by a kernel is available. We illustrate the results
with the aid of guiding examples and highlight potential applications in
molecular dynamics as well as video and text data analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Klus_S/0/1/0/all/0/1&quot;&gt;Stefan Klus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Schuster_I/0/1/0/all/0/1&quot;&gt;Ingmar Schuster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Muandet_K/0/1/0/all/0/1&quot;&gt;Krikamol Muandet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08655">
<title>Travel time tomography with adaptive dictionaries. (arXiv:1712.08655v3 [physics.geo-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08655</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a 2D travel time tomography method which regularizes the inversion
by modeling groups of slowness pixels from discrete slowness maps, called
patches, as sparse linear combinations of atoms from a dictionary. We propose
to use dictionary learning during the inversion to adapt dictionaries to
specific slowness maps. This patch regularization, called the local model, is
integrated into the overall slowness map, called the global model. The local
model considers small-scale variations using a sparsity constraint and the
global model considers larger-scale features constrained using $\ell_2$
regularization. This strategy in a locally-sparse travel time tomography (LST)
approach enables simultaneous modeling of smooth and discontinuous slowness
features. This is in contrast to conventional tomography methods, which
constrain models to be exclusively smooth or discontinuous. We develop a
$\textit{maximum a posteriori}$ formulation for LST and exploit the sparsity of
slowness patches using dictionary learning. The LST approach compares favorably
with smoothness and total variation regularization methods on densely, but
irregularly sampled synthetic slowness maps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bianco_M/0/1/0/all/0/1&quot;&gt;Michael Bianco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gerstoft_P/0/1/0/all/0/1&quot;&gt;Peter Gerstoft&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09007">
<title>Stochastic Multi-armed Bandits in Constant Space. (arXiv:1712.09007v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1712.09007</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the stochastic bandit problem in the sublinear space setting,
where one cannot record the win-loss record for all $K$ arms. We give an
algorithm using $O(1)$ words of space with regret \[
&lt;/p&gt;
&lt;p&gt;\sum_{i=1}^{K}\frac{1}{\Delta_i}\log \frac{\Delta_i}{\Delta}\log T \] where
$\Delta_i$ is the gap between the best arm and arm $i$ and $\Delta$ is the gap
between the best and the second-best arms. If the rewards are bounded away from
$0$ and $1$, this is within an $O(\log 1/\Delta)$ factor of the optimum regret
possible without space constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liau_D/0/1/0/all/0/1&quot;&gt;David Liau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1&quot;&gt;Eric Price&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Ger Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06218">
<title>Hierarchical correlation reconstruction with missing data, for example for biology-inspired neuron. (arXiv:1804.06218v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06218</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning often needs to model density from a multidimensional data
sample, including correlations between coordinates. Additionally, we often have
missing data case: that data points can miss values for some of coordinates.
This article adapts rapid parametric density estimation approach for this
purpose: modelling density as a linear combination of orthonormal functions,
for which $L^2$ optimization says that (independently) estimated coefficient
for a given function is just average over the sample of value of this function.
Hierarchical correlation reconstruction first models probability density for
each separate coordinate using all its appearances in data sample, then adds
corrections from independently modelled pairwise correlations using all samples
having both coordinates, and so on independently adding correlations for
growing numbers of variables using decreasing evidence in our data sample. A
basic application of such modelled multidimensional density can be imputation
of missing coordinates: by inserting known coordinates to the density, and
taking expected values for the missing coordinates, and maybe also variance to
estimate their uncertainty. Presented method can be compared with cascade
correlations approach, offering several advantages in flexibility and accuracy.
It can be also used as artificial neuron: maximizing prediction capabilities
for only local behavior - modelling and predicting local connections.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duda_J/0/1/0/all/0/1&quot;&gt;Jarek Duda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08841">
<title>Between hard and soft thresholding: optimal iterative thresholding algorithms. (arXiv:1804.08841v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08841</link>
<description rdf:parseType="Literal">&lt;p&gt;Iterative thresholding algorithms seek to optimize a differentiable objective
function over a sparsity or rank constraint by alternating between gradient
steps that reduce the objective, and thresholding steps that enforce the
constraint. This work examines the choice of the thresholding operator, and
asks whether it is possible to achieve stronger guarantees than what is
possible with hard thresholding. We develop the notion of relative concavity of
a thresholding operator, a quantity that characterizes the convergence
performance of any thresholding operator on the target optimization problem.
Surprisingly, we find that commonly used thresholding operators, such as hard
thresholding and soft thresholding, are suboptimal in terms of convergence
guarantees. Instead, a general class of thresholding operators, lying between
hard thresholding and soft thresholding, is shown to be optimal with the
strongest possible convergence guarantee among all thresholding operators.
Examples of this general class includes $\ell_q$ thresholding with appropriate
choices of $q$, and a newly defined {\em reciprocal thresholding} operator. In
the setting of sparse linear regression, this new class of thresholding
operators achieve a squared prediction error of order
$\kappa\cdot\frac{\sigma^2s_0\log(d)}{n}$ with a $O(\kappa s_0)$-sparse
estimator, where $\kappa$ is the sparse condition number.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haoyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barber_R/0/1/0/all/0/1&quot;&gt;Rina Foygel Barber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01955">
<title>Improve Uncertainty Estimation for Unknown Classes in Bayesian Neural Networks with Semi-Supervised /One Set Classification. (arXiv:1805.01955v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.01955</link>
<description rdf:parseType="Literal">&lt;p&gt;Although deep neural network (DNN) has achieved many state-of-the-art
results, estimating the uncertainty presented in the DNN model and the data is
a challenging task. Problems related to uncertainty such as classifying unknown
classes (class which does not appear in the training data) data as known class
with high confidence, is critically concerned in the safety domain area (e.g,
autonomous driving, medical diagnosis). In this paper, we show that applying
current Bayesian Neural Network (BNN) techniques alone does not effectively
capture the uncertainty. To tackle this problem, we introduce a simple way to
improve the BNN by using one class classification (in this paper, we use the
term &quot;set classification&quot; instead). We empirically show the result of our
method on an experiment which involves three datasets: MNIST, notMNIST and
FMNIST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_B/0/1/0/all/0/1&quot;&gt;Buu Phan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04784">
<title>Nonlinear Metric Learning through Geodesic Interpolation within Lie Groups. (arXiv:1805.04784v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.04784</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a nonlinear distance metric learning scheme based
on the fusion of component linear metrics. Instead of merging displacements at
each data point, our model calculates the velocities induced by the component
transformations, via a geodesic interpolation on a Lie transfor- mation group.
Such velocities are later summed up to produce a global transformation that is
guaranteed to be diffeomorphic. Consequently, pair-wise distances computed this
way conform to a smooth and spatially varying metric, which can greatly benefit
k-NN classification. Experiments on synthetic and real datasets demonstrate the
effectiveness of our model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhewei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bibo Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_C/0/1/0/all/0/1&quot;&gt;Charles D. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jundong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05857">
<title>On the glassy nature of the hard phase in inference problems. (arXiv:1805.05857v1 [cond-mat.dis-nn] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1805.05857</link>
<description rdf:parseType="Literal">&lt;p&gt;An algorithmically hard phase was described in a range of inference problems:
even if the signal can be reconstructed with a small error from an information
theoretic point of view, known algorithms fail unless the noise-to-signal ratio
is sufficiently small. This hard phase is typically understood as a metastable
branch of the dynamical evolution of message passing algorithms. In this work
we study the metastable branch for a prototypical inference problem, the
low-rank matrix factorization, that presents a hard phase. We show that for
noise-to-signal ratios that are below the information theoretic threshold, the
posterior measure is composed of an exponential number of metastable glassy
states and we compute their entropy, called the complexity. We show that this
glassiness extends even slightly below the algorithmic threshold below which
the well-known approximate message passing (AMP) algorithm is able to closely
reconstruct the signal. Counter-intuitively, we find that the performance of
the AMP algorithm is not improved by taking into account the glassy nature of
the hard phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Antenucci_F/0/1/0/all/0/1&quot;&gt;Fabrizio Antenucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Franz_S/0/1/0/all/0/1&quot;&gt;Silvio Franz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Urbani_P/0/1/0/all/0/1&quot;&gt;Pierfrancesco Urbani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zdeborova_L/0/1/0/all/0/1&quot;&gt;Lenka Zdeborov&amp;#xe1;&lt;/a&gt;</dc:creator>
</item></rdf:RDF>