<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01712"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01756"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01963"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00656"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01640"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01793"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01874"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.05122"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10574"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01001"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01526"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01557"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01592"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01619"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01620"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01684"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01825"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01882"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01900"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01926"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01955"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1409.5178"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.01271"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.02436"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.10306"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.04410"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.07113"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10925"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06845"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09851"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00183"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09539"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00445"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.01712">
<title>Variational Rejection Sampling. (arXiv:1804.01712v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.01712</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning latent variable models with stochastic variational inference is
challenging when the approximate posterior is far from the true posterior, due
to high variance in the gradient estimates. We propose a novel rejection
sampling step that discards samples from the variational posterior which are
assigned low likelihoods by the model. Our approach provides an arbitrarily
accurate approximation of the true posterior at the expense of extra
computation. Using a new gradient estimator for the resulting unnormalized
proposal distribution, we achieve average improvements of 3.71 nats and 0.21
nats over state-of-the-art single-sample and multi-sample alternatives
respectively for estimating marginal log-likelihoods using sigmoid belief
networks on the MNIST dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gummadi_R/0/1/0/all/0/1&quot;&gt;Ramki Gummadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lazaro_Gredilla_M/0/1/0/all/0/1&quot;&gt;Miguel Lazaro-Gredilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schuurmans_D/0/1/0/all/0/1&quot;&gt;Dale Schuurmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01756">
<title>The Kanerva Machine: A Generative Distributed Memory. (arXiv:1804.01756v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.01756</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an end-to-end trained memory system that quickly adapts to new
data and generates samples like them. Inspired by Kanerva&apos;s sparse distributed
memory, it has a robust distributed reading and writing mechanism. The memory
is analytically tractable, which enables optimal on-line compression via a
Bayesian update-rule. We formulate it as a hierarchical conditional generative
model, where memory provides a rich data-dependent prior distribution.
Consequently, the top-down memory and bottom-up perception are combined to
produce the code representing an observation. Empirically, we demonstrate that
the adaptive memory significantly improves generative models trained on both
the Omniglot and CIFAR datasets. Compared with the Differentiable Neural
Computer (DNC) and its variants, our memory model has greater capacity and is
significantly easier to train.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wayne_G/0/1/0/all/0/1&quot;&gt;Greg Wayne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Graves_A/0/1/0/all/0/1&quot;&gt;Alex Graves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lillicrap_T/0/1/0/all/0/1&quot;&gt;Timothy Lillicrap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01963">
<title>Automated Classification of Text Sentiment. (arXiv:1804.01963v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.01963</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to identify sentiment in text, referred to as sentiment analysis,
is one which is natural to adult humans. This task is, however, not one which a
computer can perform by default. Identifying sentiments in an automated,
algorithmic manner will be a useful capability for business and research in
their search to understand what consumers think about their products or
services and to understand human sociology. Here we propose two new Genetic
Algorithms (GAs) for the task of automated text sentiment analysis. The GAs
learn whether words occurring in a text corpus are either sentiment or
amplifier words, and their corresponding magnitude. Sentiment words, such as
&apos;horrible&apos;, add linearly to the final sentiment. Amplifier words in contrast,
which are typically adjectives/adverbs like &apos;very&apos;, multiply the sentiment of
the following word. This increases, decreases or negates the sentiment of the
following word. The sentiment of the full text is then the sum of these terms.
This approach grows both a sentiment and amplifier dictionary which can be
reused for other purposes and fed into other machine learning algorithms. We
report the results of multiple experiments conducted on large Amazon data sets.
The results reveal that our proposed approach was able to outperform several
public and/or commercial sentiment analysis algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dufourq_E/0/1/0/all/0/1&quot;&gt;Emmanuel Dufourq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassett_B/0/1/0/all/0/1&quot;&gt;Bruce A. Bassett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00656">
<title>mQAPViz: A divide-and-conquer multi-objective optimization algorithm to compute large data visualizations. (arXiv:1804.00656v1 [cs.HC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1804.00656</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern digital products and services are instrumental in understanding users
activities and behaviors. In doing so, we have to extract relevant
relationships and patterns from extensive data collections efficiently. Data
visualization algorithms are essential tools in transforming data into
narratives. Unfortunately, very few visualization algorithms can handle a
significant amount of data. In this study, we address the visualization of
large-scale datasets as a multi-objective optimization problem. We propose
mQAPViz, a divide-and-conquer multi-objective optimization algorithm to compute
large-scale data visualizations. Our method employs the Multi-Objective
Quadratic Assignment Problem (mQAP) as the mathematical foundation to solve the
visualization task at hand. The algorithm applies advanced machine learning
sampling techniques and efficient data structures to scale to millions of data
objects. The divide-and-conquer strategy can efficiently handle millions of
objects which the algorithm allocates onto a layout that allows the
visualization of a whole dataset. Experimental results on real-world and large
datasets demonstrate that mQAPViz is a competitive alternative to compute
large-scale visualizations that we can employ to inform the development and
improvement of digital applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanhueza_C/0/1/0/all/0/1&quot;&gt;Claudio Sanhueza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jimenez_F/0/1/0/all/0/1&quot;&gt;Francia Jim&amp;#xe9;nez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berretta_R/0/1/0/all/0/1&quot;&gt;Regina Berretta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moscato_P/0/1/0/all/0/1&quot;&gt;Pablo Moscato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01640">
<title>Hypertree Decompositions Revisited for PGMs. (arXiv:1804.01640v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.01640</link>
<description rdf:parseType="Literal">&lt;p&gt;We revisit the classical problem of exact inference on probabilistic
graphical models (PGMs). Our algorithm is based on recent worst-case optimal
database join algorithms, which can be asymptotically faster than traditional
data processing methods. We present the first empirical evaluation of these new
algorithms via JoinInfer, a new exact inference engine. We empirically explore
the properties of the data for which our engine can be expected to outperform
traditional inference engines refining current theoretical notions. Further,
JoinInfer outperforms existing state-of-the-art inference engines (ACE, IJGP
and libDAI) on some standard benchmark datasets by up to a factor of 630x.
Finally, we propose a promising data-driven heuristic that extends JoinInfer to
automatically tailor its parameters and/or switch to the traditional inference
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arun_A/0/1/0/all/0/1&quot;&gt;Aarthy Shivram Arun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaraman_S/0/1/0/all/0/1&quot;&gt;Sai Vikneshwar Mani Jayaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1&quot;&gt;Christopher R&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudra_A/0/1/0/all/0/1&quot;&gt;Atri Rudra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01793">
<title>End-to-End Saliency Mapping via Probability Distribution Prediction. (arXiv:1804.01793v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.01793</link>
<description rdf:parseType="Literal">&lt;p&gt;Most saliency estimation methods aim to explicitly model low-level
conspicuity cues such as edges or blobs and may additionally incorporate
top-down cues using face or text detection. Data-driven methods for training
saliency models using eye-fixation data are increasingly popular, particularly
with the introduction of large-scale datasets and deep architectures. However,
current methods in this latter paradigm use loss functions designed for
classification or regression tasks whereas saliency estimation is evaluated on
topographical maps. In this work, we introduce a new saliency map model which
formulates a map as a generalized Bernoulli distribution. We then train a deep
architecture to predict such maps using novel loss functions which pair the
softmax activation function with measures designed to compute distances between
probability distributions. We show in extensive experiments the effectiveness
of such loss functions over standard ones on four public benchmark datasets,
and demonstrate improved performance over state-of-the-art saliency methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jetley_S/0/1/0/all/0/1&quot;&gt;Saumya Jetley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murray_N/0/1/0/all/0/1&quot;&gt;Naila Murray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vig_E/0/1/0/all/0/1&quot;&gt;Eleonora Vig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01874">
<title>A Human Mixed Strategy Approach to Deep Reinforcement Learning. (arXiv:1804.01874v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.01874</link>
<description rdf:parseType="Literal">&lt;p&gt;In 2015, Google&apos;s DeepMind announced an advancement in creating an autonomous
agent based on deep reinforcement learning (DRL) that could beat a professional
player in a series of 49 Atari games. However, the current manifestation of DRL
is still immature, and has significant drawbacks. One of DRL&apos;s imperfections is
its lack of &quot;exploration&quot; during the training process, especially when working
with high-dimensional problems. In this paper, we propose a mixed strategy
approach that mimics behaviors of human when interacting with environment, and
create a &quot;thinking&quot; agent that allows for more efficient exploration in the DRL
training process. The simulation results based on the Breakout game show that
our scheme achieves a higher probability of obtaining a maximum score than does
the baseline DRL algorithm, i.e., the asynchronous advantage actor-critic
method. The proposed scheme therefore can be applied effectively to solving a
complicated task in a real-world application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1&quot;&gt;Ngoc Duy Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1&quot;&gt;Saeid Nahavandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thanh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.05122">
<title>Bib2vec: An Embedding-based Search System for Bibliographic Information. (arXiv:1706.05122v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1706.05122</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel embedding model that represents relationships among
several elements in bibliographic information with high representation ability
and flexibility. Based on this model, we present a novel search system that
shows the relationships among the elements in the ACL Anthology Reference
Corpus. The evaluation results show that our model can achieve a high
prediction ability and produce reasonable search results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoneda_T/0/1/0/all/0/1&quot;&gt;Takuma Yoneda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mori_K/0/1/0/all/0/1&quot;&gt;Koki Mori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miwa_M/0/1/0/all/0/1&quot;&gt;Makoto Miwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sasaki_Y/0/1/0/all/0/1&quot;&gt;Yutaka Sasaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10574">
<title>A reinforcement learning algorithm for building collaboration in multi-agent systems. (arXiv:1711.10574v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10574</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a proof-of concept study for demonstrating the viability
of building collaboration among multiple agents through standard Q learning
algorithm embedded in particle swarm optimisation. Collaboration is formulated
to be achieved among the agents via some sort competition, where the agents are
expected to balance their action in such a way that none of them drifts away of
the team and none intervene any fellow neighbours territory. Particles are
devised with Q learning algorithm for self training to learn how to act as
members of a swarm and how to produce collaborative/collective behaviours. The
produced results are supportive to the algorithmic structures suggesting that a
substantive collaboration can be build via proposed learning algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aydin_M/0/1/0/all/0/1&quot;&gt;Mehmet Emin Aydin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fellows_R/0/1/0/all/0/1&quot;&gt;Ryan Fellows&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01001">
<title>Characterizing and Computing Causes for Query Answers in Databases from Database Repairs and Repair Programs. (arXiv:1712.01001v3 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01001</link>
<description rdf:parseType="Literal">&lt;p&gt;A correspondence between database tuples as causes for query answers in
databases and tuple-based repairs of inconsistent databases with respect to
denial constraints has already been established. In this work, answer-set
programs that specify repairs of databases are used as a basis for solving
computational and reasoning problems about causes. Here, causes are also
introduced at the attribute level by appealing to a both null-based and
attribute-based repair semantics. The corresponding repair programs are
presented, and they are used as a basis for computation and reasoning about
attribute-level causes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1&quot;&gt;Leopoldo Bertossi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01526">
<title>End-to-End DNN Training with Block Floating Point Arithmetic. (arXiv:1804.01526v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.01526</link>
<description rdf:parseType="Literal">&lt;p&gt;DNNs are ubiquitous datacenter workloads, requiring orders of magnitude more
computing power from servers than traditional workloads. As such, datacenter
operators are forced to adopt domain-specific accelerators that employ
half-precision floating-point (FP) numeric representations to improve
arithmetic density. Unfortunately, even these representations are not dense
enough, and are, therefore, sub-optimal for DNNs. We propose a hybrid approach
that employs dense block floating-point (BFP) arithmetic on dot product
computations and FP arithmetic elsewhere. While using BFP improves the
performance of dot product operations, that compose most of DNN computations,
allowing values to freely float between dot product operations leads to a
better choice of tensor exponents when converting values to back BFP. We show
that models trained with hybrid BFP-FP arithmetic either match or outperform
their FP32 counterparts, leading to more compact models and denser arithmetic
in computing platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drumond_M/0/1/0/all/0/1&quot;&gt;Mario Drumond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falsafi_B/0/1/0/all/0/1&quot;&gt;Babak Falsafi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01557">
<title>Qualit\&quot;atsma{\ss}e bin\&quot;arer Klassifikationen im Bereich kriminalprognostischer Instrumente der vierten Generation. (arXiv:1804.01557v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1804.01557</link>
<description rdf:parseType="Literal">&lt;p&gt;This master&apos;s thesis discusses an important issue regarding how algorithmic
decision making (ADM) is used in crime forecasting. In America forecasting
tools are widely used by judiciary systems for making decisions about risk
offenders based on criminal justice for risk offenders. By making use of such
tools, the judiciary relies on ADM in order to make error free judgement on
offenders. For this purpose, one of the quality measures for machine learning
techniques which is widly used, the $AUC$ (area under curve), is compared to
and contrasted for results with the $PPV_k$ (positive predictive value).
Keeping in view the criticality of judgement along with a high dependency on
tools offering ADM, it is necessary to evaluate risk tools that aid in decision
making based on algorithms. In this methodology, such an evaluation is
conducted by implementing a common machine learning approach called binary
classifier, as it determines the binary outcome of the underlying juristic
question. This thesis showed that the $PPV_k$ (positive predictive value)
technique models the decision of judges much better than the $AUC$. Therefore,
this research has investigated whether there exists a classifier for which the
$PPV_k$ deviates from $AUC$ by a large proportion. It could be shown that the
deviation can rise up to 0.75. In order to test this deviation on an already in
used Classifier, data from the fourth generation risk assement tool COMPAS was
used. The result were were quite alarming as the two measures derivate from
each other by 0.48. In this study, the risk assessment evaluation of the
forecasting tools was successfully conducted, carefully reviewed and examined.
Additionally, it is also discussed whether such systems used for the purpose of
making decisions should be socially accepted or not.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krafft_T/0/1/0/all/0/1&quot;&gt;Tobias D. Krafft&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01592">
<title>Identification of Shallow Neural Networks by Fewest Samples. (arXiv:1804.01592v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.01592</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the uniform approximation of sums of ridge functions $\sum_{i=1}^m
g_i(a_i\cdot x)$ on ${\mathbb R}^d$, representing the shallowest form of
feed-forward neural network, from a small number of query samples, under mild
smoothness assumptions on the functions $g_i$&apos;s and near-orthogonality of the
ridge directions $a_i$&apos;s. The sample points are randomly generated and are
universal, in the sense that the sampled queries on those points will allow the
proposed recovery algorithms to perform a uniform approximation of any sum of
ridge functions with high-probability. Our general approximation strategy is
developed as a sequence of algorithms to perform individual sub-tasks. We first
approximate the span of the ridge directions. Then we use a straightforward
substitution, which reduces the dimensionality of the problem from $d$ to $m$.
The core of the construction is then the approximation of ridge directions
expressed in terms of rank-$1$ matrices $a_i \otimes a_i$, realized by
formulating their individual identification as a suitable nonlinear program,
maximizing the spectral norm of certain competitors constrained over the unit
Frobenius sphere. The final step is then to approximate the functions
$g_1,\dots,g_m$ by $\hat g_1,\dots,\hat g_m$. Higher order differentiation, as
used in our construction, of sums of ridge functions or of their compositions,
as in deeper neural network, yields a natural connection between neural network
weight identification and tensor product decomposition identification. In the
case of the shallowest feed-forward neural network, we show that second order
differentiation and tensors of order two (i.e., matrices) suffice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fornasier_M/0/1/0/all/0/1&quot;&gt;Massimo Fornasier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vybiral_J/0/1/0/all/0/1&quot;&gt;Jan Vyb&amp;#xed;ral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Daubechies_I/0/1/0/all/0/1&quot;&gt;Ingrid Daubechies&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01619">
<title>Stability and Convergence Trade-off of Iterative Optimization Algorithms. (arXiv:1804.01619v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.01619</link>
<description rdf:parseType="Literal">&lt;p&gt;The overall performance or expected excess risk of an iterative machine
learning algorithm can be decomposed into training error and generalization
error. While the former is controlled by its convergence analysis, the latter
can be tightly handled by algorithmic stability. The machine learning community
has a rich history investigating convergence and stability separately. However,
the question about the trade-off between these two quantities remains open. In
this paper, we show that for any iterative algorithm at any iteration, the
overall performance is lower bounded by the minimax statistical error over an
appropriately chosen loss function class. This implies an important trade-off
between convergence and stability of the algorithm -- a faster converging
algorithm has to be less stable, and vice versa. As a direct consequence of
this fundamental tradeoff, new convergence lower bounds can be derived for
classes of algorithms constrained with different stability bounds. In
particular, when the loss function is convex (or strongly convex) and smooth,
we discuss the stability upper bounds of gradient descent (GD) and stochastic
gradient descent and their variants with decreasing step sizes. For Nesterov&apos;s
accelerated gradient descent (NAG) and heavy ball method (HB), we provide
stability upper bounds for the quadratic loss function. Applying existing
stability upper bounds for the gradient methods in our trade-off framework, we
obtain lower bounds matching the well-established convergence upper bounds up
to constants for these algorithms and conjecture similar lower bounds for NAG
and HB. Finally, we numerically demonstrate the tightness of our stability
bounds in terms of exponents in the rate and also illustrate via a simulated
logistic regression problem that our stability bounds reflect the
generalization errors better than the simple uniform convergence bounds for GD
and NAG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuansi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01620">
<title>Active covariance estimation by random sub-sampling of variables. (arXiv:1804.01620v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.01620</link>
<description rdf:parseType="Literal">&lt;p&gt;We study covariance matrix estimation for the case of partially observed
random vectors, where different samples contain different subsets of vector
coordinates. Each observation is the product of the variable of interest with a
$0-1$ Bernoulli random variable. We analyze an unbiased covariance estimator
under this model, and derive an error bound that reveals relations between the
sub-sampling probabilities and the entries of the covariance matrix. We apply
our analysis in an active learning framework, where the expected number of
observed variables is small compared to the dimension of the vector of
interest, and propose a design of optimal sub-sampling probabilities and an
active covariance matrix estimation algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pavez_E/0/1/0/all/0/1&quot;&gt;Eduardo Pavez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ortega_A/0/1/0/all/0/1&quot;&gt;Antonio Ortega&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01684">
<title>Using a Classifier Ensemble for Proactive Quality Monitoring and Control: the impact of the choice of classifiers types, selection criterion, and fusion process. (arXiv:1804.01684v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.01684</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent times, the manufacturing processes are faced with many external or
internal (the increase of customized product rescheduling , process
reliability,..) changes. Therefore, monitoring and quality management
activities for these manufacturing processes are difficult. Thus, the managers
need more proactive approaches to deal with this variability. In this study, a
proactive quality monitoring and control approach based on classifiers to
predict defect occurrences and provide optimal values for factors critical to
the quality processes is proposed. In a previous work (Noyel et al. 2013), the
classification approach had been used in order to improve the quality of a
lacquering process at a company plant; the results obtained are promising, but
the accuracy of the classification model used needs to be improved. One way to
achieve this is to construct a committee of classifiers (referred to as an
ensemble) to obtain a better predictive model than its constituent models.
However, the selection of the best classification methods and the construction
of the final ensemble still poses a challenging issue. In this study, we focus
and analyze the impact of the choice of classifier types on the accuracy of the
classifier ensemble; in addition, we explore the effects of the selection
criterion and fusion process on the ensemble accuracy as well. Several fusion
scenarios were tested and compared based on a real-world case. Our results show
that using an ensemble classification leads to an increase in the accuracy of
the classifier models. Consequently, the monitoring and control of the
considered real-world case can be improved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_P/0/1/0/all/0/1&quot;&gt;Philippe Thomas&lt;/a&gt; (CRAN), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haouzi_H/0/1/0/all/0/1&quot;&gt;Hind Bril El Haouzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suhner_M/0/1/0/all/0/1&quot;&gt;Marie-Christine Suhner&lt;/a&gt; (CRAN), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Thomas&lt;/a&gt; (CRAN), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_E/0/1/0/all/0/1&quot;&gt;Emmanuel Zimmermann&lt;/a&gt; (CRAN), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noyel_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe9;lanie Noyel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01825">
<title>Evaluating Hospital Case Cost Prediction Models Using Azure Machine Learning Studio. (arXiv:1804.01825v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.01825</link>
<description rdf:parseType="Literal">&lt;p&gt;Ability for accurate hospital case cost modelling and prediction is critical
for efficient health care financial management and budgetary planning. A
variety of regression machine learning algorithms are known to be effective for
health care cost predictions. The purpose of this experiment was to build an
Azure Machine Learning Studio tool for rapid assessment of multiple types of
regression models. The tool offers environment for comparing 14 types of
regression models in a unified experiment: linear regression, Bayesian linear
regression, decision forest regression, boosted decision tree regression,
neural network regression, Poisson regression, Gaussian processes for
regression, gradient boosted machine, nonlinear least squares regression,
projection pursuit regression, random forest regression, robust regression,
robust regression with mm-type estimators, support vector regression. The tool
presents assessment results arranged by model accuracy in a single table using
five performance metrics. Evaluation of regression machine learning models for
performing hospital case cost prediction demonstrated advantage of robust
regression model, boosted decision tree regression and decision forest
regression. The operational tool has been published to the web and openly
available for experiments and extensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botchkarev_A/0/1/0/all/0/1&quot;&gt;Alexei Botchkarev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01882">
<title>Hyperbolic Entailment Cones for Learning Hierarchical Embeddings. (arXiv:1804.01882v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.01882</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning graph representations via low-dimensional embeddings that preserve
relevant network properties is an important class of problems in machine
learning. We here present a novel method to embed directed acyclic graphs.
Following prior work, we first advocate for using hyperbolic spaces which
provably model tree-like structures better than Euclidean geometry. Second, we
view hierarchical relations as partial orders defined using a family of nested
geodesically convex cones. We prove that these entailment cones admit an
optimal shape with a closed form expression both in the Euclidean and
hyperbolic spaces. Moreover, they canonically define the embedding learning
process. Experiments show significant improvements of our method over strong
recent baselines both in terms of representational capacity and generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganea_O/0/1/0/all/0/1&quot;&gt;Octavian-Eugen Ganea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becigneul_G/0/1/0/all/0/1&quot;&gt;Gary B&amp;#xe9;cigneul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1&quot;&gt;Thomas Hofmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01900">
<title>Large Scale Local Online Similarity/Distance Learning Framework based on Passive/Aggressive. (arXiv:1804.01900v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.01900</link>
<description rdf:parseType="Literal">&lt;p&gt;Similarity/Distance measures play a key role in many machine learning,
pattern recognition, and data mining algorithms, which leads to the emergence
of metric learning field. Many metric learning algorithms learn a global
distance function from data that satisfy the constraints of the problem.
However, in many real-world datasets that the discrimination power of features
varies in the different regions of input space, a global metric is often unable
to capture the complexity of the task. To address this challenge, local metric
learning methods are proposed that learn multiple metrics across the different
regions of input space. Some advantages of these methods are high flexibility
and the ability to learn a nonlinear mapping but typically achieves at the
expense of higher time requirement and overfitting problem. To overcome these
challenges, this research presents an online multiple metric learning
framework. Each metric in the proposed framework is composed of a global and a
local component learned simultaneously. Adding a global component to a local
metric efficiently reduce the problem of overfitting. The proposed framework is
also scalable with both sample size and the dimension of input data. To the
best of our knowledge, this is the first local online similarity/distance
learning framework based on PA (Passive/Aggressive). In addition, for
scalability with the dimension of input data, DRP (Dual Random Projection) is
extended for local online learning in the present work. It enables our methods
to be run efficiently on high-dimensional datasets, while maintains their
predictive performance. The proposed framework provides a straightforward local
extension to any global online similarity/distance learning algorithm based on
PA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamdan_B/0/1/0/all/0/1&quot;&gt;Baida Hamdan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zabihzadeh_D/0/1/0/all/0/1&quot;&gt;Davood Zabihzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reza_M/0/1/0/all/0/1&quot;&gt;Monsefi Reza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01926">
<title>Scalable Magnetic Field SLAM in 3D Using Gaussian Process Maps. (arXiv:1804.01926v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1804.01926</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method for scalable and fully 3D magnetic field simultaneous
localisation and mapping (SLAM) using local anomalies in the magnetic field as
a source of position information. These anomalies are due to the presence of
ferromagnetic material in the structure of buildings and in objects such as
furniture. We represent the magnetic field map using a Gaussian process model
and take well-known physical properties of the magnetic field into account. We
build local magnetic field maps using three-dimensional hexagonal block tiling.
To make our approach computationally tractable we use reduced-rank Gaussian
process regression in combination with a Rao--Blackwellised particle filter. We
show that it is possible to obtain accurate position and orientation estimates
using measurements from a smartphone, and that our approach provides a scalable
magnetic SLAM algorithm in terms of both computational complexity and map
storage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kok_M/0/1/0/all/0/1&quot;&gt;Manon Kok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1&quot;&gt;Arno Solin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01955">
<title>Explanations of model predictions with live and breakDown packages. (arXiv:1804.01955v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.01955</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex models are commonly used in predictive modeling. In this paper we
present R packages that can be used to explain predictions from complex black
box models and attribute parts of these predictions to input features. We
introduce two new approaches and corresponding packages for such attribution,
namely live and breakDown. We also compare their results with existing
implementations of state of the art solutions, namely lime that implements
Locally Interpretable Model-agnostic Explanations and ShapleyR that implements
Shapley values.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Staniak_M/0/1/0/all/0/1&quot;&gt;Mateusz Staniak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Biecek_P/0/1/0/all/0/1&quot;&gt;Przemyslaw Biecek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1409.5178">
<title>Model-based Kernel Sum Rule: Kernel Bayesian Inference with Probabilistic Models. (arXiv:1409.5178v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1409.5178</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel Bayesian inference is a powerful nonparametric approach to performing
Bayesian inference in reproducing kernel Hilbert spaces or feature spaces. In
this approach, kernel means are estimated instead of probability distributions,
and these estimates can be used for subsequent probabilistic operations (as for
inference in graphical models) or in computing the expectations of smooth
functions, for instance. Various algorithms for kernel Bayesian inference have
been obtained by combining basic rules such as the kernel sum rule (KSR),
kernel chain rule, kernel product rule and kernel Bayes&apos; rule. However, the
current framework only deals with fully nonparametric inference (i.e., all
conditional relations are learned nonparametrically), and it does not allow for
flexible combinations of nonparametric and parametric inference, which are
practically important. Our contribution is in providing a novel technique to
realize such combinations. We introduce a new KSR referred to as the
model-based KSR (Mb-KSR), which employs the sum rule in feature spaces under a
parametric setting. Incorporating the Mb-KSR into existing kernel Bayesian
framework provides a richer framework for hybrid (nonparametric and parametric)
kernel Bayesian inference. As a practical application, we propose a novel
filtering algorithm for state space models based on the Mb-KSR, which combines
the nonparametric learning of an observation process using kernel mean
embedding and the additive Gaussian noise model for a state transition process.
While we focus on additive Gaussian noise models in this study, the idea can be
extended to other noise models, such as the Cauchy and alpha-stable noise
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nishiyama_Y/0/1/0/all/0/1&quot;&gt;Yu Nishiyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kanagawa_M/0/1/0/all/0/1&quot;&gt;Motonobu Kanagawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1&quot;&gt;Arthur Gretton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fukumizu_K/0/1/0/all/0/1&quot;&gt;Kenji Fukumizu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.01271">
<title>Generalized Random Forests. (arXiv:1610.01271v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1610.01271</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose generalized random forests, a method for non-parametric
statistical estimation based on random forests (Breiman, 2001) that can be used
to fit any quantity of interest identified as the solution to a set of local
moment equations. Following the literature on local maximum likelihood
estimation, our method considers a weighted set of nearby training examples;
however, instead of using classical kernel weighting functions that are prone
to a strong curse of dimensionality, we use an adaptive weighting function
derived from a forest designed to express heterogeneity in the specified
quantity of interest. We propose a flexible, computationally efficient
algorithm for growing generalized random forests, develop a large sample theory
for our method showing that our estimates are consistent and asymptotically
Gaussian, and provide an estimator for their asymptotic variance that enables
valid confidence intervals. We use our approach to develop new methods for
three statistical tasks: non-parametric quantile regression, conditional
average partial effect estimation, and heterogeneous treatment effect
estimation via instrumental variables. A software implementation, grf for R and
C++, is available from CRAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Athey_S/0/1/0/all/0/1&quot;&gt;Susan Athey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tibshirani_J/0/1/0/all/0/1&quot;&gt;Julie Tibshirani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wager_S/0/1/0/all/0/1&quot;&gt;Stefan Wager&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.02436">
<title>Nonlinear Information Bottleneck. (arXiv:1705.02436v5 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1705.02436</link>
<description rdf:parseType="Literal">&lt;p&gt;Information bottleneck [IB] is a technique for extracting information in some
`input&apos; random variable that is relevant for predicting some different &apos;output&apos;
random variable. IB works by encoding the input in a compressed &apos;bottleneck
variable&apos; from which the output can then be accurately decoded. IB can be
difficult to compute in practice, and has been mainly developed for two limited
cases: (1) discrete random variables with small state spaces, and (2)
continuous random variables that are jointly Gaussian distributed (in which
case the encoding and decoding maps are linear). We propose a method to perform
IB in more general domains. Our approach can be applied to discrete or
continuous inputs and outputs, and allows for nonlinear encoding and decoding
maps. The method uses a novel upper bound on the IB objective, derived using a
non-parametric estimator of mutual information and a variational approximation.
We show how to implement the method using neural networks and gradient-based
optimization, and demonstrate its performance on the MNIST dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolchinsky_A/0/1/0/all/0/1&quot;&gt;Artemy Kolchinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tracey_B/0/1/0/all/0/1&quot;&gt;Brendan D. Tracey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolpert_D/0/1/0/all/0/1&quot;&gt;David H. Wolpert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.10306">
<title>Auto-Encoding Sequential Monte Carlo. (arXiv:1705.10306v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.10306</link>
<description rdf:parseType="Literal">&lt;p&gt;We build on auto-encoding sequential Monte Carlo (AESMC): a method for model
and proposal learning based on maximizing the lower bound to the log marginal
likelihood in a broad family of structured probabilistic models. Our approach
relies on the efficiency of sequential Monte Carlo (SMC) for performing
inference in structured probabilistic models and the flexibility of deep neural
networks to model complex conditional probability distributions. We develop
additional theoretical insights and introduce a new training procedure which
improves both model and proposal learning. We demonstrate that our approach
provides a fast, easy-to-implement and scalable means for simultaneous model
learning and proposal adaptation in deep generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Tuan Anh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Igl_M/0/1/0/all/0/1&quot;&gt;Maximilian Igl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1&quot;&gt;Tom Rainforth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jin_T/0/1/0/all/0/1&quot;&gt;Tom Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wood_F/0/1/0/all/0/1&quot;&gt;Frank Wood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.04410">
<title>A strong converse bound for multiple hypothesis testing, with applications to high-dimensional estimation. (arXiv:1706.04410v3 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1706.04410</link>
<description rdf:parseType="Literal">&lt;p&gt;In statistical inference problems, we wish to obtain lower bounds on the
minimax risk, that is to bound the performance of any possible estimator. A
standard technique to obtain risk lower bounds involves the use of Fano&apos;s
inequality. In an information-theoretic setting, it is known that Fano&apos;s
inequality typically does not give a sharp converse result (error lower bound)
for channel coding problems. Moreover, recent work has shown that an argument
based on binary hypothesis testing gives tighter results. We adapt this
technique to the statistical setting, and argue that Fano&apos;s inequality can
always be replaced by this approach to obtain tighter lower bounds that can be
easily computed and are asymptotically sharp. We illustrate our technique in
three applications: density estimation, active learning of a binary classifier,
and compressed sensing, obtaining tighter risk lower bounds in each case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkataramanan_R/0/1/0/all/0/1&quot;&gt;Ramji Venkataramanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_O/0/1/0/all/0/1&quot;&gt;Oliver Johnson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.07113">
<title>Adversarial Variational Optimization of Non-Differentiable Simulators. (arXiv:1707.07113v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.07113</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex computer simulators are increasingly used across fields of science as
generative models tying parameters of an underlying theory to experimental
observations. Inference in this setup is often difficult, as simulators rarely
admit a tractable density or likelihood function. We introduce Adversarial
Variational Optimization (AVO), a likelihood-free inference algorithm for
fitting a non-differentiable generative model incorporating ideas from
generative adversarial networks, variational optimization and empirical Bayes.
We adapt the training procedure of Wasserstein GANs by replacing the
differentiable generative network with a domain-specific simulator. We solve
the resulting non-differentiable minimax problem by minimizing variational
upper bounds of the two adversarial objectives. Effectively, the procedure
results in learning a proposal distribution over simulator parameters, such
that the Wasserstein distance between the marginal distribution of the
synthetic data and the empirical distribution of observed data is minimized. We
present results of the method with simulators producing both discrete and
continuous data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Louppe_G/0/1/0/all/0/1&quot;&gt;Gilles Louppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cranmer_K/0/1/0/all/0/1&quot;&gt;Kyle Cranmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10925">
<title>Deep Image Prior. (arXiv:1711.10925v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10925</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional networks have become a popular tool for image generation
and restoration. Generally, their excellent performance is imputed to their
ability to learn realistic image priors from a large number of example images.
In this paper, we show that, on the contrary, the structure of a generator
network is sufficient to capture a great deal of low-level image statistics
prior to any learning. In order to do so, we show that a randomly-initialized
neural network can be used as a handcrafted prior with excellent results in
standard inverse problems such as denoising, super-resolution, and inpainting.
Furthermore, the same prior can be used to invert deep neural representations
to diagnose them, and to restore images based on flash-no flash input pairs.
&lt;/p&gt;
&lt;p&gt;Apart from its diverse applications, our approach highlights the inductive
bias captured by standard generator network architectures. It also bridges the
gap between two very popular families of image restoration methods:
learning-based methods using deep convolutional networks and learning-free
methods based on handcrafted image priors such as self-similarity. Code and
supplementary material are available at
https://dmitryulyanov.github.io/deep_image_prior .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulyanov_D/0/1/0/all/0/1&quot;&gt;Dmitry Ulyanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1&quot;&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lempitsky_V/0/1/0/all/0/1&quot;&gt;Victor Lempitsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06845">
<title>Time series kernel similarities for predicting Paroxysmal Atrial Fibrillation from ECGs. (arXiv:1801.06845v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06845</link>
<description rdf:parseType="Literal">&lt;p&gt;We tackle the problem of classifying Electrocardiography (ECG) signals with
the aim of predicting the onset of Paroxysmal Atrial Fibrillation (PAF). Atrial
fibrillation is the most common type of arrhythmia, but in many cases PAF
episodes are asymptomatic. Therefore, in order to help diagnosing PAF, it is
important to design procedures for detecting and, more importantly, predicting
PAF episodes. We propose a method for predicting PAF events whose first step
consists of a feature extraction procedure that represents each ECG as a
multi-variate time series. Successively, we design a classification framework
based on kernel similarities for multi-variate time series, capable of handling
missing data. We consider different approaches to perform classification in the
original space of the multi-variate time series and in an embedding space,
defined by the kernel similarity measure. We achieve a classification accuracy
comparable with state of the art methods, with the additional advantage of
detecting the PAF onset up to 15 minutes in advance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1&quot;&gt;Filippo Maria Bianchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Livi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Livi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrante_A/0/1/0/all/0/1&quot;&gt;Alberto Ferrante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milosevic_J/0/1/0/all/0/1&quot;&gt;Jelena Milosevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malek_M/0/1/0/all/0/1&quot;&gt;Miroslaw Malek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09851">
<title>Cross-type Biomedical Named Entity Recognition with Deep Multi-Task Learning. (arXiv:1801.09851v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1801.09851</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivation: Biomedical named entity recognition (BioNER) is the most
fundamental task in biomedical text mining. State-of-the-art BioNER systems
often require handcrafted features specifically designed for each type of
biomedical entities. This feature generation process requires intensive labors
from biomedical and linguistic experts, and makes it difficult to adapt these
systems to new biomedical entity types. Although recent studies explored using
neural network models for BioNER to free experts from manual feature
generation, these models still require substantial human efforts to annotate
massive training data.
&lt;/p&gt;
&lt;p&gt;Results: We propose a multi-task learning framework for BioNER that is based
on neural network models to save human efforts. We build a global model by
collectively training multiple models that share parameters, each model
capturing the characteristics of a different biomedical entity type. In
experiments on five BioNER benchmark datasets covering four major biomedical
entity types, our model outperforms state-of-the-art systems and other neural
network models by a large margin, even when only limited training data are
available. Further analysis shows that the large performance gains come from
sharing character- and word-level information between different biomedical
entities. The approach creates new opportunities for text-mining approaches to
help biomedical scientists better exploit knowledge in biomedical literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1&quot;&gt;Marinka Zitnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1&quot;&gt;Jingbo Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1&quot;&gt;Curtis Langlotz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00183">
<title>Learning with Correntropy-induced Losses for Regression with Mixture of Symmetric Stable Noise. (arXiv:1803.00183v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00183</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, correntropy and its applications in machine learning have
been drawing continuous attention owing to its merits in dealing with
non-Gaussian noise and outliers. However, theoretical understanding of
correntropy, especially in the statistical learning context, is still limited.
In this study, within the statistical learning framework, we investigate
correntropy based regression in the presence of non-Gaussian noise or outliers.
To this purpose, we first introduce mixture of symmetric stable noise, which
include Gaussian noise, Cauchy noise, and the mixture of Gaussian noise as
special cases, to model non-Gaussian noise and outliers. We demonstrate that
under the mixture of symmetric stable noise assumption, correntropy based
regression can learn the conditional mean function or the conditional median
function well without requiring the finite variance assumption of the noise. In
particular, we establish learning rates for correntropy based regression
estimators that are asymptotically of type $\mathcal{O}(n^{-1})$. We believe
that the present study completes our understanding towards correntropy based
regression from a statistical learning viewpoint, and may also shed some light
on robust statistical learning for regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yunlong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Y/0/1/0/all/0/1&quot;&gt;Yiming Ying&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09539">
<title>Revisiting First-Order Convex Optimization Over Linear Spaces. (arXiv:1803.09539v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09539</link>
<description rdf:parseType="Literal">&lt;p&gt;Two popular examples of first-order optimization methods over linear spaces
are coordinate descent and matching pursuit algorithms, with their randomized
variants. While the former targets the optimization by moving along
coordinates, the latter considers a generalized notion of directions.
Exploiting the connection between the two algorithms, we present a unified
analysis of both, providing affine invariant sublinear $\mathcal{O}(1/t)$ rates
on smooth objectives and linear convergence on strongly convex objectives. As a
byproduct of our affine invariant analysis of matching pursuit, our rates for
steepest coordinate descent are the tightest known. Furthermore, we show the
first accelerated convergence rate $\mathcal{O}(1/t^2)$ for matching pursuit on
convex objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Anant Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karimireddy_S/0/1/0/all/0/1&quot;&gt;Sai Praneeth Karimireddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1&quot;&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stich_S/0/1/0/all/0/1&quot;&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00445">
<title>On the Computation of Kantorovich-Wasserstein Distances between 2D-Histograms by Uncapacitated Minimum Cost Flows. (arXiv:1804.00445v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00445</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a method to compute the Kantorovich distance, that
is, the Wasserstein distance of order one, between a pair of two-dimensional
histograms. Recent works in Computer Vision and Machine Learning have shown the
benefits of measuring Wasserstein distances of order one between histograms
with $N$ bins, by solving a classical transportation problem on (very large)
complete bipartite graphs with $N$ nodes and $N^2$ edges. The main contribution
of our work is to approximate the original transportation problem by an
uncapacitated min cost flow problem on a reduced flow network of size $O(N)$.
More precisely, when the distance among the bin centers is measured with the
1-norm or the $\infty$-norm, our approach provides an optimal solution. When
the distance amongst bins is measured with the 2-norm: (i) we derive a
quantitative estimate on the error between optimal and approximate solution;
(ii) given the error, we construct a reduced flow network of size $O(N)$. We
numerically show the benefits of our approach by computing Wasserstein
distances of order one on a set of grey scale images used as benchmarks in the
literature. We show how our approach scales with the size of the images with
1-norm, 2-norm and $\infty$-norm ground distances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bassetti_F/0/1/0/all/0/1&quot;&gt;Federico Bassetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gualandi_S/0/1/0/all/0/1&quot;&gt;Stefano Gualandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Veneroni_M/0/1/0/all/0/1&quot;&gt;Marco Veneroni&lt;/a&gt;</dc:creator>
</item></rdf:RDF>