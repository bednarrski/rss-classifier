<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-28T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10375"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10459"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.04745"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00258"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01243"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09869"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09877"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10609"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10648"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03390"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06382"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09473"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08784"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10231"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10254"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10266"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10274"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10309"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10311"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10366"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10415"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10535"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10554"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10586"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10639"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10647"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10705"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10746"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1407.4729"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1601.06207"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.00535"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.02329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.08333"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.10106"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.05241"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02475"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04851"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03848"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07028"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09468"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09518"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09704"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09974"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10161"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.10225">
<title>Light Gated Recurrent Units for Speech Recognition. (arXiv:1803.10225v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1803.10225</link>
<description rdf:parseType="Literal">&lt;p&gt;A field that has directly benefited from the recent advances in deep learning
is Automatic Speech Recognition (ASR). Despite the great achievements of the
past decades, however, a natural and robust human-machine speech interaction
still appears to be out of reach, especially in challenging environments
characterized by significant noise and reverberation. To improve robustness,
modern speech recognizers often employ acoustic models based on Recurrent
Neural Networks (RNNs), that are naturally able to exploit large time contexts
and long-term speech modulations. It is thus of great interest to continue the
study of proper techniques for improving the effectiveness of RNNs in
processing speech signals.
&lt;/p&gt;
&lt;p&gt;In this paper, we revise one of the most popular RNN models, namely Gated
Recurrent Units (GRUs), and propose a simplified architecture that turned out
to be very effective for ASR. The contribution of this work is two-fold: First,
we analyze the role played by the reset gate, showing that a significant
redundancy with the update gate occurs. As a result, we propose to remove the
former from the GRU design, leading to a more efficient and compact single-gate
model. Second, we propose to replace hyperbolic tangent with ReLU activations.
This variation couples well with batch normalization and could help the model
learn long-term dependencies without numerical issues.
&lt;/p&gt;
&lt;p&gt;Results show that the proposed architecture, called Light GRU (Li-GRU), not
only reduces the per-epoch training time by more than 30% over a standard GRU,
but also consistently improves the recognition accuracy across different tasks,
input features, noisy conditions, as well as across different ASR paradigms,
ranging from standard DNN-HMM speech recognizers to end-to-end CTC models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ravanelli_M/0/1/0/all/0/1&quot;&gt;Mirco Ravanelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brakel_P/0/1/0/all/0/1&quot;&gt;Philemon Brakel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Omologo_M/0/1/0/all/0/1&quot;&gt;Maurizio Omologo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10316">
<title>Evolutionary Multi-objective Optimization of Real-Time Strategy Micro. (arXiv:1803.10316v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.10316</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate an evolutionary multi-objective approach to good micro for
real-time strategy games. Good micro helps a player win skirmishes and is one
of the keys to developing better real-time strategy game play. In prior work,
the same multi-objective approach of maximizing damage done while minimizing
damage received was used to evolve micro for a group of ranged units versus a
group of melee units. We extend this work to consider groups composed from two
types of units. Specifically, this paper uses evolutionary multi-objective
optimization to generate micro for one group composed from both ranged and
melee units versus another group of ranged and melee units. Our micro behavior
representation uses influence maps to represent enemy spatial information and
potential fields generated from distance, health, and weapons cool down to
guide unit movement. Experimental results indicate that our multi-objective
approach leads to a Pareto front of diverse high-quality micro encapsulating
multiple possible tactics. This range of micro provided by the Pareto front
enables a human or AI player to trade-off among short term tactics that better
suit the player&apos;s longer term strategy - for example, choosing to minimize
friendly unit damage at the cost of only lightly damaging the enemy versus
maximizing damage to the enemy units at the cost of increased damage to
friendly units. We believe that our results indicate the usefulness of
potential fields as a representation, and of evolutionary multi-objective
optimization as an approach, for generating good micro.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_R/0/1/0/all/0/1&quot;&gt;Rahul Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghantous_J/0/1/0/all/0/1&quot;&gt;Joseph Ghantous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louis_S/0/1/0/all/0/1&quot;&gt;Sushil Louis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Siming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10375">
<title>On the Algorithmic Power of Spiking Neural Networks. (arXiv:1803.10375v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.10375</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Neural Networks (SNN) are mathematical models in neuroscience to
describe the dynamics among a set of neurons which interact with each other by
firing spike signals to each other. Interestingly, recent works observed that
for an integrate-and-fire model, when configured appropriately (e.g., after the
parameters are learned properly), the neurons&apos; firing rate, i.e., converges to
an optimal solution of Lasso and certain quadratic optimization problems. Thus,
SNN can be viewed as a natural algorithm for solving such convex optimization
problems. However, theoretical understanding of SNN algorithms remains limited.
In particular, only the convergence result for the Lasso problem is known, but
the bounds of the convergence rate remain unknown. Therefore, we do not know
any explicit complexity bounds for SNN algorithms.
&lt;/p&gt;
&lt;p&gt;In this work, we investigate the algorithmic power of the integrate-and-fire
SNN model after the parameters are properly learned/configured. In particular,
we explore what algorithms SNN can implement. We start by formulating a clean
discrete-time SNN model to facilitate the algorithmic study. We consider two
SNN dynamics and obtain the following results.
&lt;/p&gt;
&lt;p&gt;* We first consider an arguably simplest SNN dynamics with a threshold
spiking rule, which we call simple SNN. We show that simple SNN solves the
least square problem for a matrix $A\in\mathbb{R}^{m\times n}$ and vector
$\mathbf{b} \in \mathbb{R}^m$ with timestep complexity $O(\kappa n/\epsilon)$.
&lt;/p&gt;
&lt;p&gt;* For the under-determined case, we observe that simple SNN may solve the
$\ell_1$ minimization problem using an interesting primal-dual algorithm, which
solves the dual problem by a gradient-based algorithm while updates the primal
solution along the way. We analyze a variant dynamics and use simulation to
serve as partial evidence to support the conjecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_C/0/1/0/all/0/1&quot;&gt;Chi-Ning Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_K/0/1/0/all/0/1&quot;&gt;Kai-Min Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chi-Jen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10459">
<title>Graphite: Iterative Generative Modeling of Graphs. (arXiv:1803.10459v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.10459</link>
<description rdf:parseType="Literal">&lt;p&gt;Graphs are a fundamental abstraction for modeling relational data. However,
graphs are discrete and combinatorial in nature, and learning representations
suitable for machine learning tasks poses statistical and computational
challenges. In this work, we propose Graphite an algorithmic framework for
unsupervised learning of representations over nodes in a graph using deep
latent variable generative models. Our model is based on variational
autoencoders (VAE), and differs from existing VAE frameworks for data
modalities such as images, speech, and text in the use of graph neural networks
for parameterizing both the generative model (i.e., decoder) and inference
model (i.e., encoder). The use of graph neural networks directly incorporates
inductive biases due to the spatial, local structure of graphs directly in the
generative model. Moreover, we draw novel connections between graph neural
networks and approximate inference via kernel embeddings of distributions. We
demonstrate empirically that Graphite outperforms state-of-the-art approaches
for the tasks of density estimation, link prediction, and node classification
on synthetic and benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zweig_A/0/1/0/all/0/1&quot;&gt;Aaron Zweig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.04745">
<title>Weight-based Fish School Search algorithm for Many-Objective Optimization. (arXiv:1708.04745v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1708.04745</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimization problems with more than one objective consist in a very
attractive topic for researchers due to its applicability in real-world
situations. Over the years, the research effort in the Computational
Intelligence field resulted in algorithms able to achieve good results by
solving problems with more than one conflicting objective. However, these
techniques do not exhibit the same performance as the number of objectives
increases and become greater than 3. This paper proposes an adaptation of the
metaheuristic Fish School Search to solve optimization problems with many
objectives. This adaptation is based on the division of the school in clusters
that are specialized in solving a single-objective problem generated by the
decomposition of the original problem. For this, we used concepts and ideas
often employed by state-of-the-art algorithms, namely: (i) reference points and
lines in the objectives space; (ii) clustering process; and (iii) the
decomposition technique Penalty-based Boundary Intersection. The proposed
algorithm was compared with two state-of-the-art bio-inspired algorithms.
Moreover, a version of the proposed technique tailored to solve multi-modal
problems was also presented. The experiments executed have shown that the
performance obtained by both versions is competitive with state-of-the-art
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neto_F/0/1/0/all/0/1&quot;&gt;F. B. Lima Neto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albuquerque_I/0/1/0/all/0/1&quot;&gt;I. M. C. Albuquerque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filho_J/0/1/0/all/0/1&quot;&gt;J. B. Monteiro Filho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00258">
<title>Smooth Neighbors on Teacher Graphs for Semi-supervised Learning. (arXiv:1711.00258v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00258</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently proposed self-ensembling methods have achieved promising results
in deep semi-supervised learning, which penalize inconsistent predictions of
unlabeled data under different perturbations. However, they only consider
adding perturbations to each single data point, while ignoring the connections
between data samples. In this paper, we propose a novel method, called Smooth
Neighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on
the predictions of the teacher model, i.e., the implicit self-ensemble of
models. Then the graph serves as a similarity measure with respect to which the
representations of &quot;similar&quot; neighboring points are learned to be smooth on the
low-dimensional manifold. We achieve state-of-the-art results on
semi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for
CIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular,
the improvements are significant when the labels are fewer. For the
non-augmented MNIST with only 20 labels, the error rate is reduced from
previous 4.81% to 1.36%. Our method also shows robustness to noisy labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yucen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yong Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01243">
<title>ReBNet: Residual Binarized Neural Network. (arXiv:1711.01243v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01243</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes ReBNet, an end-to-end framework for training
reconfigurable binary neural networks on software and developing efficient
accelerators for execution on FPGA. Binary neural networks offer an intriguing
opportunity for deploying large-scale deep learning models on
resource-constrained devices. Binarization reduces the memory footprint and
replaces the power-hungry matrix-multiplication with light-weight XnorPopcount
operations. However, binary networks suffer from a degraded accuracy compared
to their fixed-point counterparts. We show that the state-of-the-art methods
for optimizing binary networks accuracy, significantly increase the
implementation cost and complexity. To compensate for the degraded accuracy
while adhering to the simplicity of binary networks, we devise the first
reconfigurable scheme that can adjust the classification accuracy based on the
application. Our proposition improves the classification accuracy by
representing features with multiple levels of residual binarization. Unlike
previous methods, our approach does not exacerbate the area cost of the
hardware accelerator. Instead, it provides a tradeoff between throughput and
accuracy while the area overhead of multi-level binarization is negligible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghasemzadeh_M/0/1/0/all/0/1&quot;&gt;Mohammad Ghasemzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samragh_M/0/1/0/all/0/1&quot;&gt;Mohammad Samragh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1&quot;&gt;Farinaz Koushanfar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09869">
<title>Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs. (arXiv:1711.09869v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09869</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel deep learning-based framework to tackle the challenge of
semantic segmentation of large-scale point clouds of millions of points. We
argue that the organization of 3D point clouds can be efficiently captured by a
structure called superpoint graph (SPG), derived from a partition of the
scanned scene into geometrically homogeneous elements. SPGs offer a compact yet
rich representation of contextual relationships between object parts, which is
then exploited by a graph convolutional network. Our framework sets a new state
of the art for segmenting outdoor LiDAR scans (+11.9 and +8.8 mIoU points for
both Semantic3D test sets), as well as indoor scans (+12.4 mIoU points for the
S3DIS dataset).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1&quot;&gt;Loic Landrieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simonovsky_M/0/1/0/all/0/1&quot;&gt;Martin Simonovsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09877">
<title>DRACO: Robust Distributed Training via Redundant Gradients. (arXiv:1803.09877v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09877</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed model training is vulnerable to worst-case system failures and
adversarial compute nodes, i.e., nodes that use malicious updates to corrupt
the global model stored at a parameter server (PS). To tolerate node failures
and adversarial attacks, recent work suggests using variants of the geometric
median to aggregate distributed updates at the PS, in place of bulk averaging.
Although median-based update rules are robust to adversarial nodes, their
computational cost can be prohibitive in large-scale settings and their
convergence guarantees often require relatively strong assumptions.
&lt;/p&gt;
&lt;p&gt;In this work, we present DRACO, a scalable framework for robust distributed
training that uses ideas from coding theory. In DRACO, each compute node
evaluates redundant gradients that are then used by the parameter server to
eliminate the effects of adversarial updates. We present problem-independent
robustness guarantees for DRACO and show that the model it produces is
identical to the one trained in the adversary-free setup. We provide extensive
experiments on real datasets and distributed setups across a variety of
large-scale models, where we show that DRACO is several times to orders of
magnitude faster than median-based approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lingjiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Charles_Z/0/1/0/all/0/1&quot;&gt;Zachary Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papailiopoulos_D/0/1/0/all/0/1&quot;&gt;Dimitris Papailiopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10609">
<title>The fifth &apos;CHiME&apos; Speech Separation and Recognition Challenge: Dataset, task and baselines. (arXiv:1803.10609v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1803.10609</link>
<description rdf:parseType="Literal">&lt;p&gt;The CHiME challenge series aims to advance robust automatic speech
recognition (ASR) technology by promoting research at the interface of speech
and language processing, signal processing , and machine learning. This paper
introduces the 5th CHiME Challenge, which considers the task of distant
multi-microphone conversational ASR in real home environments. Speech material
was elicited using a dinner party scenario with efforts taken to capture data
that is representative of natural conversational speech and recorded by 6
Kinect microphone arrays and 4 binaural microphone pairs. The challenge
features a single-array track and a multiple-array track and, for each track,
distinct rankings will be produced for systems focusing on robustness with
respect to distant-microphone capture vs. systems attempting to address all
aspects of the task including conversational language modeling. We discuss the
rationale for the challenge and provide a detailed description of the data
collection procedure, the task, and the baseline systems for array
synchronization, speech enhancement, and conventional and end-to-end ASR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barker_J/0/1/0/all/0/1&quot;&gt;Jon Barker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1&quot;&gt;Shinji Watanabe&lt;/a&gt; (CLSP), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_E/0/1/0/all/0/1&quot;&gt;Emmanuel Vincent&lt;/a&gt; (MULTISPEECH), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trmal_J/0/1/0/all/0/1&quot;&gt;Jan Trmal&lt;/a&gt; (CLSP)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10648">
<title>A Distributed Extension of the Turing Machine. (arXiv:1803.10648v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.10648</link>
<description rdf:parseType="Literal">&lt;p&gt;The Turing Machine has two implicit properties that depend on its underlying
notion of computing: the format is fully determinate and computations are
information preserving. Distributed representations lack these properties and
cannot be fully captured by Turing&apos;s standard model. To address this limitation
a distributed extension of the Turing Machine is introduced in this paper. In
the extended machine, functions and abstractions are expressed extensionally
and computations are entropic. The machine is applied to the definition of an
associative memory, with its corresponding memory register, recognition and
retrieval operations. The memory is tested with an experiment for storing and
recognizing hand written digits with satisfactory results. The experiment can
be seen as a proof of concept that information can be stored and processed
effectively in a highly distributed fashion using a symbolic but not fully
determinate format. The new machine augments the symbolic mode of computing
with consequences on the way Church Thesis is understood. The paper is
concluded with a discussion of some implications of the extended machine for
Artificial Intelligence and Cognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1&quot;&gt;Luis A. Pineda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03390">
<title>NAG: Network for Adversary Generation. (arXiv:1712.03390v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.03390</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial perturbations can pose a serious threat for deploying machine
learning systems. Recent works have shown existence of image-agnostic
perturbations that can fool classifiers over most natural images. Existing
methods present optimization approaches that solve for a fooling objective with
an imperceptibility constraint to craft the perturbations. However, for a given
classifier, they generate one perturbation at a time, which is a single
instance from the manifold of adversarial perturbations. Also, in order to
build robust models, it is essential to explore the manifold of adversarial
perturbations. In this paper, we propose for the first time, a generative
approach to model the distribution of adversarial perturbations. The
architecture of the proposed model is inspired from that of GANs and is trained
using fooling and diversity objectives. Our trained generator network attempts
to capture the distribution of adversarial perturbations for a given classifier
and readily generates a wide variety of such perturbations. Our experimental
evaluation demonstrates that perturbations crafted by our model (i) achieve
state-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver
excellent cross model generalizability. Our work can be deemed as an important
step in the process of inferring about the complex manifolds of adversarial
perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1&quot;&gt;Konda Reddy Mopuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ojha_U/0/1/0/all/0/1&quot;&gt;Utkarsh Ojha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_U/0/1/0/all/0/1&quot;&gt;Utsav Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1&quot;&gt;R. Venkatesh Babu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06382">
<title>Scalable Alignment Kernels via Space-Efficient Feature Maps. (arXiv:1802.06382v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06382</link>
<description rdf:parseType="Literal">&lt;p&gt;String kernels are attractive data analysis tools for analyzing string data.
Among them, alignment kernels are known for their high prediction accuracies in
string classifications when tested in combination with SVMs in various
applications. However, alignment kernels have a crucial drawback in that they
scale poorly due to their quadratic computation complexity in the number of
input strings, which limits large-scale applications in practice. We present
the first approximation named ESP+SFM for alignment kernels by leveraging a
metric embedding named edit-sensitive parsing (ESP) and space-efficient feature
maps (SFM) for random Fourier features (RFF) for large-scale string analyses.
Input strings are projected into vectors of RFF by leveraging ESP and SFM.
Then, SVMs are trained on the projected vectors, which enables to significantly
improve the scalability of alignment kernels while preserving their prediction
accuracies. We experimentally test ESP+ SFM on its ability to learn SVMs for
large-scale string classifications with various massive string data, and we
demonstrate the superior performance of ESP+SFM with respect to prediction
accuracy, scalability and computation efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabei_Y/0/1/0/all/0/1&quot;&gt;Yasuo Tabei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamanishi_Y/0/1/0/all/0/1&quot;&gt;Yoshihiro Yamanishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pagh_R/0/1/0/all/0/1&quot;&gt;Rasmus Pagh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09473">
<title>code2vec: Learning Distributed Representations of Code. (arXiv:1803.09473v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09473</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a neural model for representing snippets of code as continuous
distributed vectors. The main idea is to represent code as a collection of
paths in its abstract syntax tree, and aggregate these paths, in a smart and
scalable way, into a single fixed-length $\textit{code vector}$, which can be
used to predict semantic properties of the snippet.
&lt;/p&gt;
&lt;p&gt;We demonstrate the effectiveness of our approach by using it to predict a
method&apos;s name from the vector representation of its body. We evaluate our
approach by training a model on a dataset of $14$M methods. We show that code
vectors trained on this dataset can predict method names from files that were
completely unobserved during training. Furthermore, we show that our model
learns useful method name vectors that capture semantic similarities,
combinations, and analogies.
&lt;/p&gt;
&lt;p&gt;Comparing previous techniques over the same data set, our approach obtains a
relative improvement of over $75\%$, being the first to successfully predict
method names based on a large, cross-project, corpus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1&quot;&gt;Uri Alon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zilberstein_M/0/1/0/all/0/1&quot;&gt;Meital Zilberstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1&quot;&gt;Omer Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1&quot;&gt;Eran Yahav&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08784">
<title>From Random Differential Equations to Structural Causal Models: the stochastic case. (arXiv:1803.08784v2 [cs.AI] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1803.08784</link>
<description rdf:parseType="Literal">&lt;p&gt;Random Differential Equations provide a natural extension of Ordinary
Differential Equations to the stochastic setting. We show how, and under which
conditions, every equilibrium state of a Random Differential Equation (RDE) can
be described by a Structural Causal Model (SCM), while pertaining the causal
semantics. This provides an SCM that captures the stochastic and causal
behavior of the RDE, which can model both cycles and confounders. This enables
the study of the equilibrium states of the RDE by applying the theory and
statistical tools available for SCMs, for example, marginalizations and Markov
properties, as we illustrate by means of an example. Our work thus provides a
direct connection between two fields that so far have been developing in
isolation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bongers_S/0/1/0/all/0/1&quot;&gt;Stephan Bongers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mooij_J/0/1/0/all/0/1&quot;&gt;Joris M. Mooij&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10231">
<title>Safe end-to-end imitation learning for model predictive control. (arXiv:1803.10231v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10231</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the use of Bayesian networks, which provide both a mean value and
an uncertainty estimate as output, to enhance the safety of learned control
policies under circumstances in which a test-time input differs significantly
from the training set. Our algorithm combines reinforcement learning and
end-to-end imitation learning to simultaneously learn a control policy as well
as a threshold over the predictive uncertainty of the learned model, with no
hand-tuning required. Corrective action, such as a return of control to the
model predictive controller or human expert, is taken when the uncertainty
threshold is exceeded. We validate our method on fully-observable and
vision-based partially-observable systems using cart-pole and autonomous
driving simulations using deep convolutional Bayesian neural networks. We
demonstrate that our method is robust to uncertainty resulting from varying
system dynamics as well as from partial state observability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Keuntaek Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saigol_K/0/1/0/all/0/1&quot;&gt;Kamil Saigol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1&quot;&gt;Evangelos Theodorou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10254">
<title>Disease-Atlas: Navigating Disease Trajectories with Deep Learning. (arXiv:1803.10254v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.10254</link>
<description rdf:parseType="Literal">&lt;p&gt;Joint models for longitudinal and time-to-event data are commonly used in
longitudinal studies to forecast disease trajectories over time. While there
are many advantages to joint modeling, the standard forms suffer from
limitations that arise from a fixed model specification, and computational
difficulties when applied to high-dimensional datasets. In this paper, we
propose a deep learning approach to address these limitations, enhancing
existing methods with the inherent flexibility and scalability of deep neural
networks, while retaining the benefits of joint modeling. Using longitudinal
data from two real-world medical datasets, we demonstrate improvements in
performance and scalability, as well as robustness in the presence of
irregularly sampled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lim_B/0/1/0/all/0/1&quot;&gt;Bryan Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10266">
<title>Privacy-preserving Prediction. (arXiv:1803.10266v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10266</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring differential privacy of models learned from sensitive user data is
an important goal that has been studied extensively in recent years. It is now
known that for some basic learning problems, especially those involving
high-dimensional data, producing an accurate private model requires much more
data than learning without privacy. At the same time, in many applications it
is not necessary to expose the model itself. Instead users may be allowed to
query the prediction model on their inputs only through an appropriate
interface. Here we formulate the problem of ensuring privacy of individual
predictions and investigate the overheads required to achieve it in several
standard models of classification and regression.
&lt;/p&gt;
&lt;p&gt;We first describe a simple baseline approach based on training several models
on disjoint subsets of data and using standard private aggregation techniques
to predict. We show that this approach has nearly optimal sample complexity for
(realizable) PAC learning of any class of Boolean functions. At the same time,
without strong assumptions on the data distribution, the aggregation step
introduces a substantial overhead. We demonstrate that this overhead can be
avoided for the well-studied class of thresholds on a line and for a number of
standard settings of convex regression. The analysis of our algorithm for
learning thresholds relies crucially on strong generalization guarantees that
we establish for all differentially private prediction algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dwork_C/0/1/0/all/0/1&quot;&gt;Cynthia Dwork&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1&quot;&gt;Vitaly Feldman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10274">
<title>A Study of Clustering Techniques and Hierarchical Matrix Formats for Kernel Ridge Regression. (arXiv:1803.10274v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10274</link>
<description rdf:parseType="Literal">&lt;p&gt;We present memory-efficient and scalable algorithms for kernel methods used
in machine learning. Using hierarchical matrix approximations for the kernel
matrix the memory requirements, the number of floating point operations, and
the execution time are drastically reduced compared to standard dense linear
algebra routines. We consider both the general $\mathcal{H}$ matrix
hierarchical format as well as Hierarchically Semi-Separable (HSS) matrices.
Furthermore, we investigate the impact of several preprocessing and clustering
techniques on the hierarchical matrix compression. Effective clustering of the
input leads to a ten-fold increase in efficiency of the compression. The
algorithms are implemented using the STRUMPACK solver library. These results
confirm that --- with correct tuning of the hyperparameters --- classification
using kernel ridge regression with the compressed matrix does not lose
prediction accuracy compared to the exact --- not compressed --- kernel matrix
and that our approach can be extended to $\mathcal{O}(1M)$ datasets, for which
computation with the full kernel matrix becomes prohibitively expensive. We
present numerical experiments in a distributed memory environment up to 1,024
processors of the NERSC&apos;s Cori supercomputer using well-known datasets to the
machine learning community that range from dimension 8 up to 784.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rebrova_E/0/1/0/all/0/1&quot;&gt;Elizaveta Rebrova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavez_G/0/1/0/all/0/1&quot;&gt;Gustavo Chavez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghysels_P/0/1/0/all/0/1&quot;&gt;Pieter Ghysels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoye Sherry Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10309">
<title>Canonical Correlation Analysis of Datasets with a Common Source Graph. (arXiv:1803.10309v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10309</link>
<description rdf:parseType="Literal">&lt;p&gt;Canonical correlation analysis (CCA) is a powerful technique for discovering
whether or not hidden sources are commonly present in two (or more) datasets.
Its well-appreciated merits include dimensionality reduction, clustering,
classification, feature selection, and data fusion. The standard CCA however,
does not exploit the geometry of the common sources, which may be available
from the given data or can be deduced from (cross-) correlations. In this
paper, this extra information provided by the common sources generating the
data is encoded in a graph, and is invoked as a graph regularizer. This leads
to a novel graph-regularized CCA approach, that is termed graph (g) CCA. The
novel gCCA accounts for the graph-induced knowledge of common sources, while
minimizing the distance between the wanted canonical variables. Tailored for
diverse practical settings where the number of data is smaller than the data
vector dimensions, the dual formulation of gCCA is also developed. One such
setting includes kernels that are incorporated to account for nonlinear data
dependencies. The resultant graph-kernel (gk) CCA is also obtained in closed
form. Finally, corroborating image classification tests over several real
datasets are presented to showcase the merits of the novel linear, dual, and
kernel approaches relative to competing alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jia Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yanning Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10311">
<title>How Developers Iterate on Machine Learning Workflows -- A Survey of the Applied Machine Learning Literature. (arXiv:1803.10311v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10311</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning workflow development is anecdotally regarded to be an
iterative process of trial-and-error with humans-in-the-loop. However, we are
not aware of quantitative evidence corroborating this popular belief. A
quantitative characterization of iteration can serve as a benchmark for machine
learning workflow development in practice, and can aid the development of
human-in-the-loop machine learning systems. To this end, we conduct a
small-scale survey of the applied machine learning literature from five
distinct application domains. We collect and distill statistics on the role of
iteration within machine learning workflow development, and report preliminary
trends and insights from our investigation, as a starting point towards this
benchmark. Based on our findings, we finally describe desiderata for effective
and versatile human-in-the-loop machine learning systems that can cater to
users in diverse domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_D/0/1/0/all/0/1&quot;&gt;Doris Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Litian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuchen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parameswaran_A/0/1/0/all/0/1&quot;&gt;Aditya Parameswaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10342">
<title>Classification of crystallization outcomes using deep convolutional neural networks. (arXiv:1803.10342v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/1803.10342</link>
<description rdf:parseType="Literal">&lt;p&gt;The Machine Recognition of Crystallization Outcomes (MARCO) initiative has
assembled roughly half a million annotated images of macromolecular
crystallization experiments from various sources and setups. Here,
state-of-the-art machine learning algorithms are trained and tested on
different parts of this data set. We find that more than 94% of the test images
can be correctly labeled, irrespective of their experimental origin. Because
crystal recognition is key to high-density screening and the systematic
analysis of crystallization experiments, this approach opens the door to both
industrial and fundamental research applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bruno_A/0/1/0/all/0/1&quot;&gt;Andrew E. Bruno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Charbonneau_P/0/1/0/all/0/1&quot;&gt;Patrick Charbonneau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Newman_J/0/1/0/all/0/1&quot;&gt;Janet Newman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Snell_E/0/1/0/all/0/1&quot;&gt;Edward H. Snell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+So_D/0/1/0/all/0/1&quot;&gt;David R. So&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Vanhoucke_V/0/1/0/all/0/1&quot;&gt;Vincent Vanhoucke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Williams_S/0/1/0/all/0/1&quot;&gt;Shawn Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wilson_J/0/1/0/all/0/1&quot;&gt;Julie Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10366">
<title>Smoothed Online Convex Optimization in High Dimensions via Online Balanced Descent. (arXiv:1803.10366v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10366</link>
<description rdf:parseType="Literal">&lt;p&gt;We study Smoothed Online Convex Optimization, a version of online convex
optimization where the learner incurs a penalty for changing her actions
between rounds. Given a known $\Omega(\sqrt{d})$ lower bound on the competitive
ratio of any online algorithm, where $d$ is the dimension of the action space,
we ask under what conditions this bound can be beaten. We introduce a novel
algorithmic framework for this problem, Online Balanced Descent (OBD), which
works by iteratively projecting the previous point onto a carefully chosen
level set of the current cost function so as to balance the switching costs and
hitting costs. We demonstrate the generality of the OBD framework by showing
how, with different choices of &quot;balance,&quot; OBD can improve upon state-of-the-art
performance guarantees for both competitive ratio and regret; in particular,
OBD is the first algorithm to achieve a dimension-free competitive ratio, $3 +
O(1/\alpha)$, for locally polyhedral costs, where $\alpha$ measures the
&quot;steepness&quot; of the costs. We also prove bounds on the dynamic regret of OBD
when the balance is performed in the dual space that are dimension-free and
imply that OBD has sublinear static regret.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Niangjun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_G/0/1/0/all/0/1&quot;&gt;Gautam Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wierman_A/0/1/0/all/0/1&quot;&gt;Adam Wierman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10415">
<title>A Better Resource Allocation Algorithm with Semi-Bandit Feedback. (arXiv:1803.10415v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10415</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a sequential resource allocation problem between a fixed number of
arms. On each iteration the algorithm distributes a resource among the arms in
order to maximize the expected success rate. Allocating more of the resource to
a given arm increases the probability that it succeeds, yet with a cut-off. We
follow Lattimore et al. (2014) and assume that the probability increases
linearly until it equals one, after which allocating more of the resource is
wasteful. These cut-off values are fixed and unknown to the learner. We present
an algorithm for this problem and prove a regret upper bound of $O(\log n)$
improving over the best known bound of $O(\log^2 n)$. Lower bounds we prove
show that our upper bound is tight. Simulations demonstrate the superiority of
our algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dagan_Y/0/1/0/all/0/1&quot;&gt;Yuval Dagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crammer_K/0/1/0/all/0/1&quot;&gt;Koby Crammer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10535">
<title>Estimating causal effects of time-dependent exposures on a binary endpoint in a high-dimensional setting. (arXiv:1803.10535v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1803.10535</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the intervention calculus when the DAG is absent (IDA) method was
developed to estimate lower bounds of causal effects from observational
high-dimensional data. Originally it was introduced to assess the effect of
baseline biomarkers which do not vary over time. However, in many clinical
settings, measurements of biomarkers are repeated at fixed time points during
treatment exposure and, therefore, this method need to be extended. The purpose
of this paper is then to extend the first step of the IDA, the Peter Clarks
(PC)-algorithm, to a time-dependent exposure in the context of a binary
outcome. We generalised the PC-algorithm for taking into account the
chronological order of repeated measurements of the exposure and propose to
apply the IDA with our new version, the chronologically ordered PC-algorithm
(COPC-algorithm). A simulation study has been performed before applying the
method for estimating causal effects of time-dependent immunological biomarkers
on toxicity, death and progression in patients with metastatic melanoma. The
simulation study showed that the completed partially directed acyclic graphs
(CPDAGs) obtained using COPC-algorithm were structurally closer to the true
CPDAG than CPDAGs obtained using PC-algorithm. Also, causal effects were more
accurate when they were estimated based on CPDAGs obtained using
COPC-algorithm. Moreover, CPDAGs obtained by COPC-algorithm allowed removing
non-chronologic arrows with a variable measured at a time t pointing to a
variable measured at a time t&apos; where t&apos;&amp;lt; t. Bidirected edges were less present
in CPDAGs obtained with the COPC-algorithm, supporting the fact that there was
less variability in causal effects estimated from these CPDAGs. The
COPC-algorithm provided CPDAGs that keep the chronological structure present in
the data, thus allowed to estimate lower bounds of the causal effect of
time-dependent biomarkers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Asvatourian_V/0/1/0/all/0/1&quot;&gt;Vah&amp;#xe9; Asvatourian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Coutzac_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;lia Coutzac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chaput_N/0/1/0/all/0/1&quot;&gt;Nathalie Chaput&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Robert_C/0/1/0/all/0/1&quot;&gt;Caroline Robert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Michiels_S/0/1/0/all/0/1&quot;&gt;Stefan Michiels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lanoy_E/0/1/0/all/0/1&quot;&gt;Emilie Lanoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10554">
<title>Joint PLDA for Simultaneous Modeling of Two Factors. (arXiv:1803.10554v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10554</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic linear discriminant analysis (PLDA) is a method used for
biometric problems like speaker or face recognition that models the variability
of the samples using two latent variables, one that depends on the class of the
sample and another one that is assumed independent across samples and models
the within-class variability. In this work, we propose a generalization of PLDA
that enables joint modeling of two sample-dependent factors: the class of
interest and a nuisance condition. The approach does not change the basic form
of PLDA but rather modifies the training procedure to consider the dependency
across samples of the latent variable that models within-class variability.
While the identity of the nuisance condition is needed during training, it is
not needed during testing since we propose a scoring procedure that
marginalizes over the corresponding latent variable. We show results on a
multilingual speaker-verification task, where the language spoken is considered
a nuisance condition. We show that the proposed joint PLDA approach leads to
significant performance gains in this task for two different datasets, in
particular when the training data contains mostly or only monolingual speakers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1&quot;&gt;Luciana Ferrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McLaren_M/0/1/0/all/0/1&quot;&gt;Mitchell McLaren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10586">
<title>Stochastic Variational Inference with Gradient Linearization. (arXiv:1803.10586v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.10586</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational inference has experienced a recent surge in popularity owing to
stochastic approaches, which have yielded practical tools for a wide range of
model classes. A key benefit is that stochastic variational inference obviates
the tedious process of deriving analytical expressions for closed-form variable
updates. Instead, one simply needs to derive the gradient of the log-posterior,
which is often much easier. Yet for certain model classes, the log-posterior
itself is difficult to optimize using standard gradient techniques. One such
example are random field models, where optimization based on gradient
linearization has proven popular, since it speeds up convergence significantly
and can avoid poor local optima. In this paper we propose stochastic
variational inference with gradient linearization (SVIGL). It is similarly
convenient as standard stochastic variational inference - all that is required
is a local linearization of the energy gradient. Its benefit over stochastic
variational inference with conventional gradient methods is a clear improvement
in convergence speed, while yielding comparable or even better variational
approximations in terms of KL divergence. We demonstrate the benefits of SVIGL
in three applications: Optical flow estimation, Poisson-Gaussian denoising, and
3D surface reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plotz_T/0/1/0/all/0/1&quot;&gt;Tobias Pl&amp;#xf6;tz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wannenwetsch_A/0/1/0/all/0/1&quot;&gt;Anne S. Wannenwetsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_S/0/1/0/all/0/1&quot;&gt;Stefan Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10639">
<title>On Learning Graphs with Edge-Detecting Queries. (arXiv:1803.10639v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10639</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning a general graph $G=(V,E)$ using
edge-detecting queries, where the number of vertices $|V|=n$ is given to the
learner. The information theoretic lower bound gives $m\log n$ for the number
of queries, where $m=|E|$ is the number of edges. In case the number of edges
$m$ is also given to the learner, Angluin-Chen&apos;s Las Vegas algorithm
\cite{AC08} runs in $4$ rounds and detects the edges in $O(m\log n)$ queries.
In the other harder case where the number of edges $m$ is unknown, their
algorithm runs in $5$ rounds and asks $O(m\log n+\sqrt{m}\log^2 n)$ queries.
There have been two open problems: \emph{(i)} can the number of queries be
reduced to $O(m\log n)$ in the second case, and, \emph{(ii)} can the number of
rounds be reduced without substantially increasing the number of queries (in
both cases). For the first open problem (when $m$ is unknown) we give two
algorithms. The first is an $O(1)$-round Las Vegas algorithm that asks $m\log
n+\sqrt{m}(\log^{[k]}n)\log n$ queries for any constant $k$ where
$\log^{[k]}n=\log \stackrel{k}{\cdots} \log n$. The second is an
$O(\log^*n)$-round Las Vegas algorithm that asks $O(m\log n)$ queries. This
solves the first open problem for any practical $n$, for example,
$n&amp;lt;2^{65536}$. We also show that no deterministic algorithm can solve this
problem in a constant number of rounds. To solve the second problem we study
the case when $m$ is known. We first show that any non-adaptive Monte Carlo
algorithm (one-round) must ask at least $\Omega(m^2\log n)$ queries, and any
two-round Las Vegas algorithm must ask at least $m^{4/3-o(1)}\log n$ queries on
average. We then give two two-round Monte Carlo algorithms, the first asks
$O(m^{4/3}\log n)$ queries for any $n$ and $m$, and the second asks $O(m\log
n)$ queries when $n&amp;gt;2^m$. Finally, we give a $3$-round Monte Carlo algorithm
that asks $O(m\log n)$ queries for any $n$ and $m$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abasi_H/0/1/0/all/0/1&quot;&gt;Hasan Abasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bshouty_N/0/1/0/all/0/1&quot;&gt;Nader H. Bshouty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10647">
<title>Active Metric Learning for Supervised Classification. (arXiv:1803.10647v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10647</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering and classification critically rely on distance metrics that
provide meaningful comparisons between data points. We present mixed-integer
optimization approaches to find optimal distance metrics that generalize the
Mahalanobis metric extensively studied in the literature. Additionally, we
generalize and improve upon leading methods by removing reliance on
pre-designated &quot;target neighbors,&quot; &quot;triplets,&quot; and &quot;similarity pairs.&quot; Another
salient feature of our method is its ability to enable active learning by
recommending precise regions to sample after an optimal metric is computed to
improve classification performance. This targeted acquisition can significantly
reduce computational burden by ensuring training data completeness,
representativeness, and economy. We demonstrate classification and
computational performance of the algorithms through several simple and
intuitive examples, followed by results on real image and medical datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumaran_K/0/1/0/all/0/1&quot;&gt;Krishnan Kumaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papageorgiou_D/0/1/0/all/0/1&quot;&gt;Dimitri Papageorgiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yutong Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Minhan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takac_M/0/1/0/all/0/1&quot;&gt;Martin Tak&amp;#xe1;&amp;#x10d;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10705">
<title>Semi-supervised learning for structured regression on partially observed attributed graphs. (arXiv:1803.10705v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10705</link>
<description rdf:parseType="Literal">&lt;p&gt;Conditional probabilistic graphical models provide a powerful framework for
structured regression in spatio-temporal datasets with complex correlation
patterns. However, in real-life applications a large fraction of observations
is often missing, which can severely limit the representational power of these
models. In this paper we propose a Marginalized Gaussian Conditional Random
Fields (m-GCRF) structured regression model for dealing with missing labels in
partially observed temporal attributed graphs. This method is aimed at learning
with both labeled and unlabeled parts and effectively predicting future values
in a graph. The method is even capable of learning from nodes for which the
response variable is never observed in history, which poses problems for many
state-of-the-art models that can handle missing data. The proposed model is
characterized for various missingness mechanisms on 500 synthetic graphs. The
benefits of the new method are also demonstrated on a challenging application
for predicting precipitation based on partial observations of climate variables
in a temporal graph that spans the entire continental US. We also show that the
method can be useful for optimizing the costs of data collection in climate
applications via active reduction of the number of weather stations to
consider. In experiments on these real-world and synthetic datasets we show
that the proposed model is consistently more accurate than alternative
semi-supervised structured models, as well as models that either use imputation
to deal with missing values or simply ignore them altogether.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stojanovic_J/0/1/0/all/0/1&quot;&gt;Jelena Stojanovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jovanovic_M/0/1/0/all/0/1&quot;&gt;Milos Jovanovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gligorijevic_D/0/1/0/all/0/1&quot;&gt;Djordje Gligorijevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obradovic_Z/0/1/0/all/0/1&quot;&gt;Zoran Obradovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10746">
<title>Pseudo-marginal Bayesian inference for supervised Gaussian process latent variable models. (arXiv:1803.10746v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.10746</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a Bayesian framework for inference with a supervised version of
the Gaussian process latent variable model. The framework overcomes the high
correlations between latent variables and hyperparameters by using an unbiased
pseudo estimate for the marginal likelihood that approximately integrates over
the latent variables. This is used to construct a Markov Chain to explore the
posterior of the hyperparameters. We demonstrate the procedure on simulated and
real examples, showing its ability to capture uncertainty and multimodality of
the hyperparameters and improved uncertainty quantification in predictions when
compared with variational inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gadd_C/0/1/0/all/0/1&quot;&gt;Charles Gadd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wade_S/0/1/0/all/0/1&quot;&gt;Sara Wade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Akeel Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grammatopoulos_D/0/1/0/all/0/1&quot;&gt;Dimitris Grammatopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1407.4729">
<title>Sparse Partially Linear Additive Models. (arXiv:1407.4729v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1407.4729</link>
<description rdf:parseType="Literal">&lt;p&gt;The generalized partially linear additive model (GPLAM) is a flexible and
interpretable approach to building predictive models. It combines features in
an additive manner, allowing each to have either a linear or nonlinear effect
on the response. However, the choice of which features to treat as linear or
nonlinear is typically assumed known. Thus, to make a GPLAM a viable approach
in situations in which little is known $a~priori$ about the features, one must
overcome two primary model selection challenges: deciding which features to
include in the model and determining which of these features to treat
nonlinearly. We introduce the sparse partially linear additive model (SPLAM),
which combines model fitting and $both$ of these model selection challenges
into a single convex optimization problem. SPLAM provides a bridge between the
lasso and sparse additive models. Through a statistical oracle inequality and
thorough simulation, we demonstrate that SPLAM can outperform other methods
across a broad spectrum of statistical regimes, including the high-dimensional
($p\gg N$) setting. We develop efficient algorithms that are applied to real
data sets with half a million samples and over 45,000 features with excellent
predictive performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lou_Y/0/1/0/all/0/1&quot;&gt;Yin Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bien_J/0/1/0/all/0/1&quot;&gt;Jacob Bien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Caruana_R/0/1/0/all/0/1&quot;&gt;Rich Caruana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gehrke_J/0/1/0/all/0/1&quot;&gt;Johannes Gehrke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1601.06207">
<title>Rectified Gaussian Scale Mixtures and the Sparse Non-Negative Least Squares Problem. (arXiv:1601.06207v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1601.06207</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we develop a Bayesian evidence maximization framework to solve
the sparse non-negative least squares (S-NNLS) problem. We introduce a family
of probability densities referred to as the Rectified Gaussian Scale Mixture
(R- GSM) to model the sparsity enforcing prior distribution for the solution.
The R-GSM prior encompasses a variety of heavy-tailed densities such as the
rectified Laplacian and rectified Student- t distributions with a proper choice
of the mixing density. We utilize the hierarchical representation induced by
the R-GSM prior and develop an evidence maximization framework based on the
Expectation-Maximization (EM) algorithm. Using the EM based method, we estimate
the hyper-parameters and obtain a point estimate for the solution. We refer to
the proposed method as rectified sparse Bayesian learning (R-SBL). We provide
four R- SBL variants that offer a range of options for computational complexity
and the quality of the E-step computation. These methods include the Markov
chain Monte Carlo EM, linear minimum mean-square-error estimation, approximate
message passing and a diagonal approximation. Using numerical experiments, we
show that the proposed R-SBL method outperforms existing S-NNLS solvers in
terms of both signal and support recovery performance, and is also very robust
against the structure of the design matrix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nalci_A/0/1/0/all/0/1&quot;&gt;Alican Nalci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedorov_I/0/1/0/all/0/1&quot;&gt;Igor Fedorov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Shoukairi_M/0/1/0/all/0/1&quot;&gt;Maher Al-Shoukairi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Thomas T. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_B/0/1/0/all/0/1&quot;&gt;Bhaskar D. Rao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.00535">
<title>Human Interaction with Recommendation Systems. (arXiv:1703.00535v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.00535</link>
<description rdf:parseType="Literal">&lt;p&gt;Many recommendation algorithms rely on user data to generate recommendations.
However, these recommendations also affect the data obtained from future users.
This work aims to understand the effects of this dynamic interaction. We
propose a simple model where users with heterogeneous preferences arrive over
time. Based on this model, we prove that naive estimators, i.e. those which
ignore this feedback loop, are not consistent. We show that consistent
estimators are efficient in the presence of myopic agents. Our results are
validated using extensive simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmit_S/0/1/0/all/0/1&quot;&gt;Sven Schmit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Riquelme_C/0/1/0/all/0/1&quot;&gt;Carlos Riquelme&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.02329">
<title>Deep Q-Learning for Self-Organizing Networks Fault Management and Radio Performance Improvement. (arXiv:1707.02329v3 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/1707.02329</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to improve the radio link performance in a wireless
network using a deep Q-Learning based algorithm. In this paper, we use this
reinforcement learning model to allow the wireless network cluster to self-heal
by performing certain fault management actions which improves the radio link
performance of this wireless network. The main contributions of this paper are:
1) introduce a radio performance tuning algorithm that self-organizing networks
can implement in a polynomial runtime, 2) employ deep reinforcement learning to
perform fault management, and 3) show that this fault management method can
improve the radio link performance in a realistic network setup. Simulation
results show that an optimal action sequence to clear alarms is feasible even
against the randomness of the network faults and user movements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mismar_F/0/1/0/all/0/1&quot;&gt;Faris B. Mismar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1&quot;&gt;Brian L. Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.08333">
<title>Framing U-Net via Deep Convolutional Framelets: Application to Sparse-view CT. (arXiv:1708.08333v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1708.08333</link>
<description rdf:parseType="Literal">&lt;p&gt;X-ray computed tomography (CT) using sparse projection views is a recent
approach to reduce the radiation dose. However, due to the insufficient
projection views, an analytic reconstruction approach using the filtered back
projection (FBP) produces severe streaking artifacts. Recently, deep learning
approaches using large receptive field neural networks such as U-Net have
demonstrated impressive performance for sparse- view CT reconstruction.
However, theoretical justification is still lacking. Inspired by the recent
theory of deep convolutional framelets, the main goal of this paper is,
therefore, to reveal the limitation of U-Net and propose new multi-resolution
deep learning schemes. In particular, we show that the alternative U- Net
variants such as dual frame and the tight frame U-Nets satisfy the so-called
frame condition which make them better for effective recovery of high frequency
edges in sparse view- CT. Using extensive experiments with real patient data
set, we demonstrate that the new network architectures provide better
reconstruction performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yoseob Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.10106">
<title>What is the Machine Learning?. (arXiv:1709.10106v2 [hep-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1709.10106</link>
<description rdf:parseType="Literal">&lt;p&gt;Applications of machine learning tools to problems of physical interest are
often criticized for producing sensitivity at the expense of transparency. To
address this concern, we explore a data planing procedure for identifying
combinations of variables -- aided by physical intuition -- that can
discriminate signal from background. Weights are introduced to smooth away the
features in a given variable(s). New networks are then trained on this modified
data. Observed decreases in sensitivity diagnose the variable&apos;s discriminating
power. Planing also allows the investigation of the linear versus non-linear
nature of the boundaries between signal and background. We demonstrate the
efficacy of this approach using a toy example, followed by an application to an
idealized heavy resonance scenario at the Large Hadron Collider. By unpacking
the information being utilized by these algorithms, this method puts in context
what it means for a machine to learn.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Spencer Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Cohen_T/0/1/0/all/0/1&quot;&gt;Timothy Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Ostdiek_B/0/1/0/all/0/1&quot;&gt;Bryan Ostdiek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.05241">
<title>Robust Decentralized Learning Using ADMM with Unreliable Agents. (arXiv:1710.05241v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.05241</link>
<description rdf:parseType="Literal">&lt;p&gt;Many machine learning problems can be formulated as consensus optimization
problems which can be solved efficiently via a cooperative multi-agent system.
However, the agents in the system can be unreliable due to a variety of
reasons: noise, faults and attacks. Thus, providing falsified data leads the
optimization process in a wrong direction, and degrades the performance of
distributed machine learning algorithms. This paper considers the problem of
decentralized learning using ADMM in the presence of unreliable agents. First,
we rigorously analyze the effect of falsified updates (in ADMM learning
iterations) on the convergence behavior of multi-agent system. We show that the
algorithm linearly converges to a neighborhood of the optimal solution under
certain conditions and characterize the neighborhood size analytically. Next,
we provide guidelines for network structure design to achieve a faster
convergence. We also provide necessary conditions on the falsified updates for
exact convergence to the optimal solution. Finally, to mitigate the influence
of unreliable agents, we propose a robust variant of ADMM and show its
resilience to unreliable agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qunwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1&quot;&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldhahn_R/0/1/0/all/0/1&quot;&gt;Ryan Goldhahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_P/0/1/0/all/0/1&quot;&gt;Priyadip Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varshney_P/0/1/0/all/0/1&quot;&gt;Pramod K. Varshney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.02475">
<title>Bayesian model and dimension reduction for uncertainty propagation: applications in random media. (arXiv:1711.02475v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.02475</link>
<description rdf:parseType="Literal">&lt;p&gt;Well-established methods for the solution of stochastic partial differential
equations (SPDEs) typically struggle in problems with high-dimensional
inputs/outputs. Such difficulties are only amplified in large-scale
applications where even a few tens of full-order model runs are impracticable.
While dimensionality reduction can alleviate some of these issues, it is not
known which and how many features of the (high-dimensional) input are actually
predictive of the (high-dimensional) output. In this paper, we advocate a
Bayesian formulation that is capable of performing simultaneous dimension and
model-order reduction. It consists of a component that encodes the
high-dimensional input into a low-dimensional set of feature functions by
employing sparsity-enforcing priors and a decoding component that makes use of
the solution of a coarse-grained model in order to reconstruct that of the
full-order model. Both components are represented with latent variables in a
probabilistic graphical model and are simultaneously trained using Stochastic
Variational Inference methods. The model is capable of quantifying the
predictive uncertainty due to the information loss that unavoidably takes place
in any model-order/dimension reduction as well as the uncertainty arising from
finite-sized training datasets. We demonstrate its capabilities in the context
of random media where fine-scale fluctuations can give rise to random inputs
with tens of thousands of variables. With a few tens of full-order model
simulations, the proposed model is capable of identifying salient physical
features and produce sharp predictions under different boundary conditions of
the full output which itself consists of thousands of components.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grigo_C/0/1/0/all/0/1&quot;&gt;Constantin Grigo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Koutsourelakis_P/0/1/0/all/0/1&quot;&gt;Phaedon-Stelios Koutsourelakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04851">
<title>Learning and Visualizing Localized Geometric Features Using 3D-CNN: An Application to Manufacturability Analysis of Drilled Holes. (arXiv:1711.04851v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04851</link>
<description rdf:parseType="Literal">&lt;p&gt;3D Convolutional Neural Networks (3D-CNN) have been used for object
recognition based on the voxelized shape of an object. However, interpreting
the decision making process of these 3D-CNNs is still an infeasible task. In
this paper, we present a unique 3D-CNN based Gradient-weighted Class Activation
Mapping method (3D-GradCAM) for visual explanations of the distinct local
geometric features of interest within an object. To enable efficient learning
of 3D geometries, we augment the voxel data with surface normals of the object
boundary. We then train a 3D-CNN with this augmented data and identify the
local features critical for decision-making using 3D GradCAM. An application of
this feature identification framework is to recognize difficult-to-manufacture
drilled hole features in a complex CAD geometry. The framework can be extended
to identify difficult-to-manufacture features at multiple spatial scales
leading to a real-time design for manufacturability decision support system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghadai_S/0/1/0/all/0/1&quot;&gt;Sambit Ghadai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balu_A/0/1/0/all/0/1&quot;&gt;Aditya Balu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krishnamurthy_A/0/1/0/all/0/1&quot;&gt;Adarsh Krishnamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumik Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03848">
<title>Region Detection in Markov Random Fields: Gaussian Case. (arXiv:1802.03848v7 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03848</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of model selection in Gaussian Markov fields in the
sample deficient scenario. The benchmark information-theoretic results in the
case of d-regular graphs require the number of samples to be at least
proportional to the logarithm of the number of vertices to allow consistent
graph recovery. When the number of samples is less than this amount, reliable
detection of all edges is impossible. In many applications, it is more
important to learn the distribution of the edge (coupling) parameters over the
network than the specific locations of the edges. Assuming that the entire
graph can be partitioned into a number of spatial regions with similar edge
parameters and reasonably regular boundaries, we develop new
information-theoretic sample complexity bounds and show that a bounded number
of samples can be sufficient to consistently recover these regions. Finally, we
introduce and analyze an efficient region growing algorithm capable of
recovering the regions with high accuracy. We show that it is consistent and
demonstrate its performance benefits in synthetic simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soloveychik_I/0/1/0/all/0/1&quot;&gt;Ilya Soloveychik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tarokh_V/0/1/0/all/0/1&quot;&gt;Vahid Tarokh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07028">
<title>High-Dimensional Bayesian Optimization via Additive Models with Overlapping Groups. (arXiv:1802.07028v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07028</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization (BO) is a popular technique for sequential black-box
function optimization, with applications including parameter tuning, robotics,
environmental monitoring, and more. One of the most important challenges in BO
is the development of algorithms that scale to high dimensions, which remains a
key open problem despite recent progress. In this paper, we consider the
approach of Kandasamy et al. (2015), in which the high-dimensional function
decomposes as a sum of lower-dimensional functions on subsets of the underlying
variables. In particular, we significantly generalize this approach by lifting
the assumption that the subsets are disjoint, and consider additive models with
arbitrary overlap among the subsets. By representing the dependencies via a
graph, we deduce an efficient message passing algorithm for optimizing the
acquisition function. In addition, we provide an algorithm for learning the
graph from samples based on Gibbs sampling. We empirically demonstrate the
effectiveness of our methods on both synthetic and real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolland_P/0/1/0/all/0/1&quot;&gt;Paul Rolland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarlett_J/0/1/0/all/0/1&quot;&gt;Jonathan Scarlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogunovic_I/0/1/0/all/0/1&quot;&gt;Ilija Bogunovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07329">
<title>Bayesian Incremental Learning for Deep Neural Networks. (arXiv:1802.07329v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07329</link>
<description rdf:parseType="Literal">&lt;p&gt;In industrial machine learning pipelines, data often arrive in parts.
Particularly in the case of deep neural networks, it may be too expensive to
train the model from scratch each time, so one would rather use a previously
learned model and the new data to improve performance. However, deep neural
networks are prone to getting stuck in a suboptimal solution when trained on
only new data as compared to the full dataset. Our work focuses on a continuous
learning setup where the task is always the same and new parts of data arrive
sequentially. We apply a Bayesian approach to update the posterior
approximation with each new piece of data and find this method to outperform
the traditional approach in our experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kochurov_M/0/1/0/all/0/1&quot;&gt;Max Kochurov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garipov_T/0/1/0/all/0/1&quot;&gt;Timur Garipov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Podoprikhin_D/0/1/0/all/0/1&quot;&gt;Dmitry Podoprikhin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Molchanov_D/0/1/0/all/0/1&quot;&gt;Dmitry Molchanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ashukha_A/0/1/0/all/0/1&quot;&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vetrov_D/0/1/0/all/0/1&quot;&gt;Dmitry Vetrov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09468">
<title>Clipping free attacks against artificial neural networks. (arXiv:1803.09468v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09468</link>
<description rdf:parseType="Literal">&lt;p&gt;During the last years, a remarkable breakthrough has been made in AI domain
thanks to artificial deep neural networks that achieved a great success in many
machine learning tasks in computer vision, natural language processing, speech
recognition, malware detection and so on. However, they are highly vulnerable
to easily crafted adversarial examples. Many investigations have pointed out
this fact and different approaches have been proposed to generate attacks while
adding a limited perturbation to the original data. The most robust known
method so far is the so called C&amp;amp;W attack [1]. Nonetheless, a countermeasure
known as feature squeezing coupled with ensemble defense showed that most of
these attacks can be destroyed [6]. In this paper, we present a new method we
call Centered Initial Attack (CIA) whose advantage is twofold : first, it
insures by construction the maximum perturbation to be smaller than a threshold
fixed beforehand, without the clipping process that degrades the quality of
attacks. Second, it is robust against recently introduced defenses such as
feature squeezing, JPEG encoding and even against a voting ensemble of
defenses. While its application is not limited to images, we illustrate this
using five of the current best classifiers on ImageNet dataset among which two
are adversarialy retrained on purpose to be robust against attacks. With a
fixed maximum perturbation of only 1.5% on any pixel, around 80% of attacks
(targeted) fool the voting ensemble defense and nearly 100% when the
perturbation is only 6%. While this shows how it is difficult to defend against
CIA attacks, the last section of the paper gives some guidelines to limit their
impact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Addad_B/0/1/0/all/0/1&quot;&gt;Boussad Addad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kodjabachian_J/0/1/0/all/0/1&quot;&gt;Jerome Kodjabachian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1&quot;&gt;Christophe Meyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09518">
<title>Fr\&apos;echet ChemblNet Distance: A metric for generative models for molecules. (arXiv:1803.09518v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09518</link>
<description rdf:parseType="Literal">&lt;p&gt;The new wave of successful generative models in machine learning has
increased the interest in deep learning driven de novo drug design. However,
assessing the performance of such generative models is notoriously difficult.
Metrics that are typically used to assess the performance of such generative
models are the percentage of chemically valid molecules or the similarity to
real molecules in terms of particular descriptors, such as the partition
coefficient (logP) or druglikeness. However, method comparison is difficult
because of the inconsistent use of evaluation metrics, the necessity for
multiple metrics, and the fact that some of these measures can easily be
tricked by simple rule-based systems. We propose a novel distance measure
between two sets of molecules, called Fr\&apos;echet ChemblNet distance (FCD), that
can be used as an evaluation metric for generative models. The FCD is similar
to a recently established performance metric for comparing image generation
methods, the Fr\&apos;echet Inception Distance (FID). Whereas the FID uses one of
the hidden layers of InceptionNet, the FCD utilizes the penultimate layer of a
deep neural network called &quot;ChemblNet&quot;, which was trained to predict drug
activities. Thus, the FCD metric takes into account chemically and biologically
relevant information about molecules, and also measures the diversity of the
set via the distribution of generated molecules. The FCD&apos;s advantage over
previous metrics is that it can detect if generated molecules are a) diverse
and have similar b) chemical and c) biological properties as real molecules. We
further provide an easy-to-use implementation that only requires the SMILES
representation of the generated molecules as input to calculate the FCD.
Implementations are available at: https://www.github.com/bioinf-jku/FCD
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preuer_K/0/1/0/all/0/1&quot;&gt;Kristina Preuer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renz_P/0/1/0/all/0/1&quot;&gt;Philipp Renz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1&quot;&gt;Thomas Unterthiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1&quot;&gt;Sepp Hochreiter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klambauer_G/0/1/0/all/0/1&quot;&gt;G&amp;#xfc;nter Klambauer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09704">
<title>MOrdReD: Memory-based Ordinal Regression Deep Neural Networks for Time Series Forecasting. (arXiv:1803.09704v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09704</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series forecasting is ubiquitous in the modern world. Applications range
from health care to astronomy, include climate modelling, financial trading and
monitoring of critical engineering equipment. To offer value over this range of
activities we must have models that not only provide accurate forecasts but
that also quantify and adjust their uncertainty over time. Furthermore, such
models must allow for multimodal, non-Gaussian behaviour that arises regularly
in applied settings. In this work, we propose a novel, end-to-end deep learning
method for time series forecasting. Crucially, our model allows the principled
assessment of predictive uncertainty as well as providing rich information
regarding multiple modes of future data values. Our approach not only provides
an excellent predictive forecast, shadowing true future values, but also allows
us to infer valuable information, such as the predictive distribution of the
occurrence of critical events of interest, accurately and reliably even over
long time horizons. We find the method outperforms other state-of-the-art
algorithms, such as Gaussian Processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Orozco_B/0/1/0/all/0/1&quot;&gt;Bernardo P&amp;#xe9;rez Orozco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abbati_G/0/1/0/all/0/1&quot;&gt;Gabriele Abbati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09974">
<title>Inferring network connectivity from event timing patterns. (arXiv:1803.09974v1 [q-bio.NC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1803.09974</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing network connectivity from the collective dynamics of a system
typically requires access to its complete continuous-time evolution although
these are often experimentally inaccessible. Here we propose a theory for
revealing physical connectivity of networked systems only from the event time
series their intrinsic collective dynamics generate. Representing the patterns
of event timings in an event space spanned by inter-event and cross-event
intervals, we reveal which other units directly influence the inter-event times
of any given unit. For illustration, we linearize an event space mapping
constructed from the spiking patterns in model neural circuits to reveal the
presence or absence of synapses between any pair of neurons as well as whether
the coupling acts in an inhibiting or activating (excitatory) manner. The
proposed model-independent reconstruction theory is scalable to larger networks
and may thus play an important role in the reconstruction of networks from
biology to social science and engineering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Casadiego_J/0/1/0/all/0/1&quot;&gt;Jose Casadiego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Maoutsa_D/0/1/0/all/0/1&quot;&gt;Dimitra Maoutsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Timme_M/0/1/0/all/0/1&quot;&gt;Marc Timme&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10161">
<title>Stein Points. (arXiv:1803.10161v1 [stat.CO] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1803.10161</link>
<description rdf:parseType="Literal">&lt;p&gt;An important task in computational statistics and machine learning is to
approximate a posterior distribution $p(x)$ with an empirical measure supported
on a set of representative points $\{x_i\}_{i=1}^n$. This paper focuses on
methods where the selection of points is essentially deterministic, with an
emphasis on achieving accurate approximation when $n$ is small. To this end, we
present `Stein Points&apos;. The idea is to exploit either a greedy or a conditional
gradient method to iteratively minimise a kernel Stein discrepancy between the
empirical measure and $p(x)$. Our empirical results demonstrate that Stein
Points enable accurate approximation of the posterior at modest computational
cost. In addition, theoretical results are provided to establish convergence of
the method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wilson Ye Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gorham_J/0/1/0/all/0/1&quot;&gt;Jackson Gorham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois-Xavier Briol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oates_C/0/1/0/all/0/1&quot;&gt;Chris J. Oates&lt;/a&gt;</dc:creator>
</item></rdf:RDF>