<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-07T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02185"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02093"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03087"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06096"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08365"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07712"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10461"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04325"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08484"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02017"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02078"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02129"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02237"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02266"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02334"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02341"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02357"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02435"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08273"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04207"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05096"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06063"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01550"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09737"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.02185">
<title>Round-Table Group Optimization for Sequencing Problems. (arXiv:1808.02185v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.02185</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, a round-table group optimization (RTGO) algorithm is
presented. RTGO is a simple metaheuristic framework using the insights of
research on group creativity. In a cooperative group, the agents work in
iterative sessions to search innovative ideas in a common problem landscape.
Each agent has one base idea stored in its individual memory, and one social
idea fed by a round-table group support mechanism in each session. The idea
combination and improvement processes are respectively realized by using a
recombination search (XS) strategy and a local search (LS) strategy, to build
on the base and social ideas. RTGO is then implemented for solving two
difficult sequencing problems, i.e., the flowshop scheduling problem and the
quadratic assignment problem. The domain-specific LS strategies are adopted
from existing algorithms, whereas a general XS class, called socially biased
combination (SBX), is realized in a modular form. The performance of RTGO is
then evaluated on commonly-used benchmark datasets. Good performance on
different problems can be achieved by RTGO using appropriate SBX operators.
Furthermore, RTGO is able to outperform some existing methods, including
methods using the same LS strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiao-Feng Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02093">
<title>Learning to Share and Hide Intentions using Information Regularization. (arXiv:1808.02093v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.02093</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to cooperate with friends and compete with foes is a key component
of multi-agent reinforcement learning. Typically to do so, one requires access
to either a model of or interaction with the other agent(s). Here we show how
to learn effective strategies for cooperation and competition in an asymmetric
information game with no such model or interaction. Our approach is to
encourage an agent to reveal or hide their intentions using an
information-theoretic regularizer. We consider both the mutual information
between goal and action given state, as well as the mutual information between
goal and state. We show how to stochastically optimize these regularizers in a
way that is easy to integrate with policy gradient reinforcement learning.
Finally, we demonstrate that cooperative (competitive) policies learned with
our approach lead to more (less) reward for a second agent in two simple
asymmetric information games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strouse_D/0/1/0/all/0/1&quot;&gt;DJ Strouse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleiman_Weiner_M/0/1/0/all/0/1&quot;&gt;Max Kleiman-Weiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Josh Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botvinick_M/0/1/0/all/0/1&quot;&gt;Matt Botvinick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1&quot;&gt;David Schwab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03087">
<title>Exploration in NetHack With Secret Discovery. (arXiv:1711.03087v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03087</link>
<description rdf:parseType="Literal">&lt;p&gt;Roguelike games generally feature exploration problems as a critical, yet
often repetitive element of gameplay. Automated approaches, however, face
challenges in terms of optimality, as well as due to incomplete information,
such as from the presence of secret doors. This paper presents an algorithmic
approach to exploration of roguelike dungeon environments. Our design aims to
minimize exploration time, balancing coverage and discovery of secret areas
with resource cost. Our algorithm is based on the concept of occupancy maps
popular in robotics, adapted to encourage efficient discovery of secret access
points. Through extensive experimentation on NetHack maps we show that this
technique is significantly more efficient than simpler greedy approaches and an
existing automated player. We further investigate optimized parameterization
for the algorithm through a comprehensive data analysis. These results point
towards better automation for players as well as heuristics applicable to fully
automated gameplay.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_J/0/1/0/all/0/1&quot;&gt;Jonathan C. Campbell&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbrugge_C/0/1/0/all/0/1&quot;&gt;Clark Verbrugge&lt;/a&gt; (1) ((1) McGill University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06096">
<title>Efficient B-mode Ultrasound Image Reconstruction from Sub-sampled RF Data using Deep Learning. (arXiv:1712.06096v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06096</link>
<description rdf:parseType="Literal">&lt;p&gt;In portable, three dimensional, and ultra-fast ultrasound imaging systems,
there is an increasing demand for the reconstruction of high quality images
from a limited number of radio-frequency (RF) measurements due to receiver (Rx)
or transmit (Xmit) event sub-sampling. However, due to the presence of side
lobe artifacts from RF sub-sampling, the standard beamformer often produces
blurry images with less contrast, which are unsuitable for diagnostic purposes.
Existing compressed sensing approaches often require either hardware changes or
computationally expensive algorithms, but their quality improvements are
limited. To address this problem, here we propose a novel deep learning
approach that directly interpolates the missing RF data by utilizing redundancy
in the Rx-Xmit plane. Our extensive experimental results using sub-sampled RF
data from a multi-line acquisition B-mode system confirm that the proposed
method can effectively reduce the data rate without sacrificing image quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1&quot;&gt;Yeo Hun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Shujaat Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huh_J/0/1/0/all/0/1&quot;&gt;Jaeyoung Huh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08365">
<title>Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising. (arXiv:1802.08365v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08365</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time bidding (RTB) is an important mechanism in online display
advertising, where a proper bid for each page view plays an essential role for
good marketing results. Budget constrained bidding is a typical scenario in RTB
where the advertisers hope to maximize the total value of the winning
impressions under a pre-set budget constraint. However, the optimal bidding
strategy is hard to be derived due to the complexity and volatility of the
auction environment. To address these challenges, in this paper, we formulate
budget constrained bidding as a Markov Decision Process and propose a
model-free reinforcement learning framework to resolve the optimization
problem. Our analysis shows that the immediate reward from environment is
misleading under a critical resource constraint. Therefore, we innovate a
reward function design methodology for the reinforcement learning problems with
constraints. Based on the new reward design, we employ a deep neural network to
learn the appropriate reward so that the optimal policy can be learned
effectively. Different from the prior model-based work, which suffers from the
scalability problem, our framework is easy to be deployed in large-scale
industrial applications. The experimental evaluations demonstrate the
effectiveness of our framework on large-scale real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiujun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1&quot;&gt;Qing Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoxun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Kun Gai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07712">
<title>Causal Inference on Discrete Data via Estimating Distance Correlations. (arXiv:1803.07712v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07712</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we deal with the problem of inferring causal directions when
the data is on discrete domain. By considering the distribution of the cause
$P(X)$ and the conditional distribution mapping cause to effect $P(Y|X)$ as
independent random variables, we propose to infer the causal direction via
comparing the distance correlation between $P(X)$ and $P(Y|X)$ with the
distance correlation between $P(Y)$ and $P(X|Y)$. We infer &quot;$X$ causes $Y$&quot; if
the dependence coefficient between $P(X)$ and $P(Y|X)$ is smaller. Experiments
are performed to show the performance of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Furui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chan_L/0/1/0/all/0/1&quot;&gt;Laiwan Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10461">
<title>From Knowledge Graph Embedding to Ontology Embedding? An Analysis of the Compatibility between Vector Space Representations and Rules. (arXiv:1805.10461v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.10461</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed the successful application of low-dimensional
vector space representations of knowledge graphs to predict missing facts or
find erroneous ones. However, it is not yet well-understood to what extent
ontological knowledge, e.g. given as a set of (existential) rules, can be
embedded in a principled way. To address this shortcoming, in this paper we
introduce a general framework based on a view of relations as regions, which
allows us to study the compatibility between ontological knowledge and
different types of vector space embeddings. Our technical contribution is
two-fold. First, we show that some of the most popular existing embedding
methods are not capable of modelling even very simple types of rules, which in
particular also means that they are not able to learn the type of dependencies
captured by such rules. Second, we study a model in which relations are
modelled as convex regions. We show particular that ontologies which are
expressed using so-called quasi-chained existential rules can be exactly
represented using convex regions, such that any set of facts which is induced
using that vector space embedding is logically consistent and deductively
closed with respect to the input ontology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_Basulto_V/0/1/0/all/0/1&quot;&gt;V&amp;#xed;ctor Guti&amp;#xe9;rrez-Basulto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1&quot;&gt;Steven Schockaert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04325">
<title>Augmenting Stream Constraint Programming with Eventuality Conditions. (arXiv:1806.04325v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04325</link>
<description rdf:parseType="Literal">&lt;p&gt;Stream constraint programming is a recent addition to the family of
constraint programming frameworks, where variable domains are sets of infinite
streams over finite alphabets. Previous works showed promising results for its
applicability to real-world planning and control problems. In this paper,
motivated by the modelling of planning applications, we improve the
expressiveness of the framework by introducing 1) the &quot;until&quot; constraint, a new
construct that is adapted from Linear Temporal Logic and 2) the @ operator on
streams, a syntactic sugar for which we provide a more efficient solving
algorithm over simple desugaring. For both constructs, we propose corresponding
novel solving algorithms and prove their correctness. We present competitive
experimental results on the Missionaries and Cannibals logic puzzle and a
standard path planning application on the grid, by comparing with Apt and
Brand&apos;s method for verifying eventuality conditions using a CP approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jasper C.H. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jimmy H.M. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_A/0/1/0/all/0/1&quot;&gt;Allen Z. Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08484">
<title>AceKG: A Large-scale Knowledge Graph for Academic Data Mining. (arXiv:1807.08484v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1807.08484</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing knowledge graphs (KGs) in academic domains suffer from problems
of insufficient multi-relational information, name ambiguity and improper data
format for large-scale machine processing. In this paper, we present AceKG, a
new large-scale KG in academic domain. AceKG not only provides clean academic
information, but also offers a large-scale benchmark dataset for researchers to
conduct challenging data mining projects including link prediction, community
detection and scholar classification. Specifically, AceKG describes 3.13
billion triples of academic facts based on a consistent ontology, including
necessary properties of papers, authors, fields of study, venues and
institutes, as well as the relations among them. To enrich the proposed
knowledge graph, we also perform entity alignment with existing databases and
rule-based inference. Based on AceKG, we conduct experiments of three typical
academic data mining tasks and evaluate several state-of- the-art knowledge
embedding and network representation learning approaches on the benchmark
datasets built from AceKG. Finally, we discuss several promising research
directions that benefit from AceKG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yuchen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jialu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yuting Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ye Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinbing Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02017">
<title>Withholding aggressive treatments may not accelerate time to death among dying ICU patients. (arXiv:1808.02017v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1808.02017</link>
<description rdf:parseType="Literal">&lt;p&gt;Critically ill patients may die despite aggressive treatment. In this study,
we examine trends in the application of two such treatments over a decade, as
well as the impact of these trends on survival durations in patients who die
within a month of ICU admission. We considered observational data available
from the MIMIC-III open-access ICU database, collected from June 2001 to
October 2012: These data comprise almost 60,000 hospital admissions for a total
of 38,645 unique adults.
&lt;/p&gt;
&lt;p&gt;We explored two hypotheses: (i) administration of aggressive treatment during
the ICU stay immediately preceding end-of-life would decrease over the study
time period and (ii) time-to-death from ICU admission would also decrease due
to the decrease in aggressive treatment administration. Tests for significant
trends were performed and a p-value threshold of 0.05 was used to assess
statistical significance. We found that aggressive treatments in this
population were employed with decreasing frequency over the study period
duration, and also that reducing aggressive treatments for such patients may
not result in shorter times to death. The latter finding has implications for
end of life discussions that involve the possible use or non-use of such
treatments in those patients with very poor prognosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramazzotti_D/0/1/0/all/0/1&quot;&gt;Daniele Ramazzotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clardy_P/0/1/0/all/0/1&quot;&gt;Peter Clardy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1&quot;&gt;Leo Anthony Celi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_D/0/1/0/all/0/1&quot;&gt;David J. Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudin_R/0/1/0/all/0/1&quot;&gt;Robert S. Rudin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02078">
<title>Unbiased Implicit Variational Inference. (arXiv:1808.02078v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.02078</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop unbiased implicit variational inference (UIVI), a method that
expands the applicability of variational inference by defining an expressive
variational family. UIVI considers an implicit variational distribution
obtained in a hierarchical manner using a simple reparameterizable distribution
whose variational parameters are defined by arbitrarily flexible deep neural
networks. Unlike previous works, UIVI directly optimizes the evidence lower
bound (ELBO) rather than an approximation to the ELBO. We demonstrate UIVI on
several models, including Bayesian multinomial logistic regression and
variational autoencoders, and show that UIVI achieves both tighter ELBO and
better predictive performance than existing approaches at a similar
computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Titsias_M/0/1/0/all/0/1&quot;&gt;Michalis K. Titsias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ruiz_F/0/1/0/all/0/1&quot;&gt;Francisco J. R. Ruiz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02129">
<title>Probabilistic Causal Analysis of Social Influence. (arXiv:1808.02129v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1808.02129</link>
<description rdf:parseType="Literal">&lt;p&gt;Mastering the dynamics of social influence requires separating, in a database
of information propagation traces, the genuine causal processes from temporal
correlation, homophily and other spurious causes. However, most of the studies
to characterize social influence and, in general, most data-science analyses
focus on correlations, statistical independence, conditional independence etc.;
only recently, there has been a resurgence of interest in &quot;causal data
science&quot;, e.g., grounded on causality theories. In this paper we adopt a
principled causal approach to the analysis of social influence from
information-propagation data, rooted in probabilistic causal theory.
&lt;/p&gt;
&lt;p&gt;Our approach develops around two phases. In the first step, in order to avoid
the pitfalls of misinterpreting causation when the data spans a mixture of
several subtypes (&quot;Simpson&apos;s paradox&quot;), we partition the set of propagation
traces in groups, in such a way that each group is as less contradictory as
possible in terms of the hierarchical structure of information propagation. For
this goal we borrow from the literature the notion of &quot;agony&quot; and define the
Agony-bounded Partitioning problem, which we prove being hard, and for which we
develop two efficient algorithms with approximation guarantees. In the second
step, for each group from the first phase, we apply a constrained MLE approach
to ultimately learn a minimal causal topology. Experiments on synthetic data
show that our method is able to retrieve the genuine causal arcs w.r.t. a known
ground-truth generative model. Experiments on real data show that, by focusing
only on the extracted causal structures instead of the whole social network, we
can improve the effectiveness of predicting influence spread.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1&quot;&gt;Francesco Bonchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gullo_F/0/1/0/all/0/1&quot;&gt;Francesco Gullo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bud Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramazzotti_D/0/1/0/all/0/1&quot;&gt;Daniele Ramazzotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02237">
<title>Inferring Molecular Pathology and micro-RNA Transcriptome from mRNA Profiles of Cancer Biopsies through Deep Multi-Task Learning. (arXiv:1808.02237v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.02237</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite great advances, molecular cancer pathology is often limited to use a
small number of biomarkers rather than the whole transcriptome, partly due to
the computational challenges. Here, we introduce a novel architecture of DNNs
that is capable of simultaneous inference of various properties of biological
samples, through multi-task and transfer learning. We employed this
architecture on mRNA transcription profiles of 10787 clinical samples from 34
classes (one healthy and 33 different types of cancer) from 27 tissues. Our
system significantly outperforms prior works and classical machine learning
approaches in predicting tissue-of-origin, normal or disease state and cancer
type of each sample. Furthermore, it can predict miRNA transcription profile of
each sample, which enables performing miRNA expression research when only mRNA
transcriptome data are available. We also show this system is very robust
against noise and missing values. Collectively, our results highlight
applications of artificial intelligence in molecular cancer pathology and
oncological research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Azarkhalili_B/0/1/0/all/0/1&quot;&gt;Behrooz Azarkhalili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saberi_A/0/1/0/all/0/1&quot;&gt;Ali Saberi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chitsaz_H/0/1/0/all/0/1&quot;&gt;Hamidreza Chitsaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sharifi_Zarchi_A/0/1/0/all/0/1&quot;&gt;Ali Sharifi-Zarchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02266">
<title>Multi-Output Convolution Spectral Mixture for Gaussian Processes. (arXiv:1808.02266v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.02266</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-output Gaussian processes (MOGPs) are recently extended by using
spectral mixture kernel, which enables expressively pattern extrapolation with
a strong interpretation. In particular, Multi-Output Spectral Mixture kernel
(MOSM) is a recent, powerful state of the art method. However, MOSM cannot
reduce to the ordinary spectral mixture kernel (SM) when using a single
channel. Moreover, when the spectral density of different channels is either
very close or very far from each other in the frequency domain, MOSM generates
unreasonable scale effects on cross weights which produces an incorrect
description of the channel correlation structure. In this paper, we tackle
these drawbacks and introduce a principled multi-output convolution spectral
mixture kernel (MOCSM) framework. In our framework, we model channel
dependencies through convolution of time and phase delayed spectral mixtures
between different channels. Results of extensive experiments on synthetic and
real datasets demontrate the advantages of MOCSM and its state of the art
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groot_P/0/1/0/all/0/1&quot;&gt;Perry Groot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchiori_E/0/1/0/all/0/1&quot;&gt;Elena Marchiori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02316">
<title>Modelling hidden structure of signals in group data analysis with modified (Lr, 1) and block-term decompositions. (arXiv:1808.02316v1 [cs.NA])</title>
<link>http://arxiv.org/abs/1808.02316</link>
<description rdf:parseType="Literal">&lt;p&gt;This work is devoted to elaboration on the idea to use block term
decomposition for group data analysis and to raise the possibility of modelling
group activity with (Lr, 1) and Tucker blocks. A new generalization of block
tensor decomposition was considered in application to group data analysis.
Suggested approach was evaluated on multilabel classification task for a set of
images. This contribution also reports results of investigation on clustering
with proposed tensor models in comparison with known matrix models, namely
common orthogonal basis extraction and group independent component analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kharyuk_P/0/1/0/all/0/1&quot;&gt;Pavel Kharyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1&quot;&gt;Ivan Oseledets&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02334">
<title>A novel topology design approach using an integrated deep learning network architecture. (arXiv:1808.02334v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.02334</link>
<description rdf:parseType="Literal">&lt;p&gt;Topology design optimization offers tremendous opportunity in design and
manufacturing freedoms by designing and producing a part from the ground-up
without a meaningful initial design as required by conventional shape design
optimization approaches. Ideally, with adequate problem statements, to
formulate and solve the topology design problem using a standard topology
optimization process, such as SIMP (Simplified Isotropic Material with
Penalization) is possible. In reality, an estimated over thousands of design
iterations is often required for just a few design variables, the conventional
optimization approach is in general impractical or computationally unachievable
for real world applications significantly diluting the development of the
topology optimization technology. There is, therefore, a need for a different
approach that will be able to optimize the initial design topology effectively
and rapidly. Therefore, this work presents a new topology design procedure to
generate optimal structures using an integrated Generative Adversarial Networks
(GANs) and convolutional neural network architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rawat_S/0/1/0/all/0/1&quot;&gt;Sharad Rawat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_M/0/1/0/all/0/1&quot;&gt;M.H. Herman Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02341">
<title>Optimal stopping via deeply boosted backward regression. (arXiv:1808.02341v1 [math.NA])</title>
<link>http://arxiv.org/abs/1808.02341</link>
<description rdf:parseType="Literal">&lt;p&gt;In this note we propose a new approach towards solving numerically optimal
stopping problems via boosted regression based Monte Carlo algorithms. The main
idea of the method is to boost standard linear regression algorithms in each
backward induction step by adding new basis functions based on previously
estimated continuation values. The proposed methodology is illustrated by
several numerical examples from finance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Belomestny_D/0/1/0/all/0/1&quot;&gt;Denis Belomestny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Schoenmakers_J/0/1/0/all/0/1&quot;&gt;John Schoenmakers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Spokoiny_V/0/1/0/all/0/1&quot;&gt;Vladimir Spokoiny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tavyrikov_Y/0/1/0/all/0/1&quot;&gt;Yuri Tavyrikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02357">
<title>Acoustic Scene Classification: A Competition Review. (arXiv:1808.02357v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1808.02357</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we study the problem of acoustic scene classification, i.e.,
categorization of audio sequences into mutually exclusive classes based on
their spectral content. We describe the methods and results discovered during a
competition organized in the context of a graduate machine learning course;
both by the students and external participants. We identify the most suitable
methods and study the impact of each by performing an ablation study of the
mixture of approaches. We also compare the results with a neural network
baseline, and show the improvement over that. Finally, we discuss the impact of
using a competition as a part of a university course, and justify its
importance in the curriculum based on student feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gharib_S/0/1/0/all/0/1&quot;&gt;Shayan Gharib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Derrar_H/0/1/0/all/0/1&quot;&gt;Honain Derrar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Niizumi_D/0/1/0/all/0/1&quot;&gt;Daisuke Niizumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Senttula_T/0/1/0/all/0/1&quot;&gt;Tuukka Senttula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tommola_J/0/1/0/all/0/1&quot;&gt;Janne Tommola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heittola_T/0/1/0/all/0/1&quot;&gt;Toni Heittola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Virtanen_T/0/1/0/all/0/1&quot;&gt;Tuomas Virtanen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huttunen_H/0/1/0/all/0/1&quot;&gt;Heikki Huttunen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02435">
<title>Mixed Integer Linear Programming for Feature Selection in Support Vector Machine. (arXiv:1808.02435v1 [math.OC])</title>
<link>http://arxiv.org/abs/1808.02435</link>
<description rdf:parseType="Literal">&lt;p&gt;This work focuses on support vector machine (SVM) with feature selection. A
MILP formulation is proposed for the problem. The choice of suitable features
to construct the separating hyperplanes has been modelled in this formulation
by including a budget constraint that sets in advance a limit on the number of
features to be used in the classification process. We propose both an exact and
a heuristic procedure to solve this formulation in an efficient way. Finally,
the validation of the model is done by checking it with some well-known data
sets and comparing it with classical classification methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Labbe_M/0/1/0/all/0/1&quot;&gt;Martine Labb&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Martinez_Merino_L/0/1/0/all/0/1&quot;&gt;Luisa I. Mart&amp;#xed;nez-Merino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Rodriguez_Chia_A/0/1/0/all/0/1&quot;&gt;Antonio M. Rodr&amp;#xed;guez-Ch&amp;#xed;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08273">
<title>Multiple Causal Inference with Latent Confounding. (arXiv:1805.08273v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08273</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal inference from observational data requires assumptions. These
assumptions range from measuring confounders to identifying instruments.
Traditionally, these assumptions have focused on estimation in a single causal
problem. In this work, we develop techniques for causal estimation in causal
problems with multiple treatments. We develop two assumptions based on shared
confounding between treatments and independence of treatments given the
confounder. Together these assumptions lead to a confounder estimator
regularized by mutual information. For this estimator, we develop a tractable
lower bound. To fit the outcome model, we use the residual information in the
treatments given the confounder. We validate on simulations and an example from
clinical medicine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1&quot;&gt;Rajesh Ranganath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perotte_A/0/1/0/all/0/1&quot;&gt;Adler Perotte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04207">
<title>Swarming for Faster Convergence in Stochastic Optimization. (arXiv:1806.04207v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04207</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a distributed framework for stochastic optimization which is
inspired by models of collective motion found in nature (e.g., swarming) with
mild communication requirements. Specifically, we analyze a scheme in which
each one of $N &amp;gt; 1$ independent threads, implements in a distributed and
unsynchronized fashion, a stochastic gradient-descent algorithm which is
perturbed by a swarming potential. Assuming the overhead caused by
synchronization is not negligible, we show the swarming-based approach exhibits
better performance than a centralized algorithm (based upon the average of $N$
observations) in terms of (real-time) convergence speed. We also derive an
error bound that is monotone decreasing in network size and connectivity. We
characterize the scheme&apos;s finite-time performances for both convex and
non-convex objective functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pu_S/0/1/0/all/0/1&quot;&gt;Shi Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Garcia_A/0/1/0/all/0/1&quot;&gt;Alfredo Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05096">
<title>Introducing user-prescribed constraints in Markov chains for nonlinear dimensionality reduction. (arXiv:1806.05096v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05096</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic kernel based dimensionality reduction approaches have become
popular in the last decade. The central component of many of these methods is a
symmetric kernel that quantifies the vicinity between pairs of data points and
a kernel-induced Markov chain on the data. Typically, the Markov chain is fully
specified by the kernel through row normalization. However, in many cases, it
is desirable to impose user-specified stationary-state and dynamical
constraints on the Markov chain. Unfortunately, no systematic framework exists
to impose such user-defined constraints. Here, we introduce a path entropy
maximization based approach to derive the transition probabilities of Markov
chains using a kernel and additional user-specified constraints. We illustrate
the usefulness of these Markov chains with examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dixit_P/0/1/0/all/0/1&quot;&gt;Purushottam D. Dixit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06063">
<title>Probabilistic Trajectory Segmentation by Means of Hierarchical Dirichlet Process Switching Linear Dynamical Systems. (arXiv:1806.06063v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.06063</link>
<description rdf:parseType="Literal">&lt;p&gt;Using movement primitive libraries is an effective means to enable robots to
solve more complex tasks. In order to build these movement libraries, current
algorithms require a prior segmentation of the demonstration trajectories. A
promising approach is to model the trajectory as being generated by a set of
Switching Linear Dynamical Systems and inferring a meaningful segmentation by
inspecting the transition points characterized by the switching dynamics. With
respect to the learning, a nonparametric Bayesian approach is employed
utilizing a Gibbs sampler.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sieb_M/0/1/0/all/0/1&quot;&gt;Maximilian Sieb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schultheis_M/0/1/0/all/0/1&quot;&gt;Matthias Schultheis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Szelag_S/0/1/0/all/0/1&quot;&gt;Sebastian Szelag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01550">
<title>Designing Adaptive Neural Networks for Energy-Constrained Image Classification. (arXiv:1808.01550v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.01550</link>
<description rdf:parseType="Literal">&lt;p&gt;As convolutional neural networks (CNNs) enable state-of-the-art computer
vision applications, their high energy consumption has emerged as a key
impediment to their deployment on embedded and mobile devices. Towards
efficient image classification under hardware constraints, prior work has
proposed adaptive CNNs, i.e., systems of networks with different accuracy and
computation characteristics, where a selection scheme adaptively selects the
network to be evaluated for each input image. While previous efforts have
investigated different network selection schemes, we find that they do not
necessarily result in energy savings when deployed on mobile systems. The key
limitation of existing methods is that they learn only how data should be
processed among the CNNs and not the network architectures, with each network
being treated as a blackbox.
&lt;/p&gt;
&lt;p&gt;To address this limitation, we pursue a more powerful design paradigm where
the architecture settings of the CNNs are treated as hyper-parameters to be
globally optimized. We cast the design of adaptive CNNs as a hyper-parameter
optimization problem with respect to energy, accuracy, and communication
constraints imposed by the mobile device. To efficiently solve this problem, we
adapt Bayesian optimization to the properties of the design space, reaching
near-optimal configurations in few tens of function evaluations. Our method
reduces the energy consumed for image classification on a mobile device by up
to 6x, compared to the best previously published work that uses CNNs as
blackboxes. Finally, we evaluate two image classification practices, i.e.,
classifying all images locally versus over the cloud under energy and
communication constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stamoulis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Stamoulis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1&quot;&gt;Ting-Wu Chin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1&quot;&gt;Anand Krishnan Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Haocheng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajja_S/0/1/0/all/0/1&quot;&gt;Sribhuvan Sajja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bognar_M/0/1/0/all/0/1&quot;&gt;Mitchell Bognar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1&quot;&gt;Diana Marculescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09737">
<title>Convergence Rates of Gaussian ODE Filters. (arXiv:1807.09737v1 [math.NA] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1807.09737</link>
<description rdf:parseType="Literal">&lt;p&gt;A recently-introduced class of probabilistic (uncertainty-aware) solvers for
ordinary differential equations (ODEs) applies Gaussian (Kalman) filtering to
initial value problems. These methods model the true solution $x$ and its first
$q$ derivatives a priori as a Gauss--Markov process $\boldsymbol{X}$, which is
then iteratively conditioned on information about $\dot{x}$. We prove
worst-case local convergence rates of order $h^{q+1}$ for a wide range of
versions of this Gaussian ODE filter, as well as global convergence rates of
order $h^q$ in the case of $q=1$ and an integrated Brownian motion prior, and
analyze how inaccurate information on $\dot{x}$ coming from approximate
evaluations of $f$ affects these rates. Moreover, we present explicit formulas
for the steady states and show that the posterior confidence intervals are well
calibrated in all considered cases that exhibit global convergence---in the
sense that they globally contract at the same rate as the truncation error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kersting_H/0/1/0/all/0/1&quot;&gt;Hans Kersting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sullivan_T/0/1/0/all/0/1&quot;&gt;T. J. Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hennig_P/0/1/0/all/0/1&quot;&gt;Philipp Hennig&lt;/a&gt;</dc:creator>
</item></rdf:RDF>