<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-26T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09617"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09731"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09789"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10115"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00447"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09174"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09605"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09612"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09614"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09748"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09769"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09787"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09842"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09935"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09936"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09997"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10018"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10130"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.10087"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00119"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07616"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07830"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09679"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09692"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09708"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09710"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09712"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09736"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09762"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09764"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09780"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09795"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09823"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09827"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09888"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09905"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09908"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09918"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10069"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10080"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1306.3862"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.06348"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.07094"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.04788"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10277"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06293"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07513"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04640"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06208"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02336"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.09617">
<title>Sounderfeit: Cloning a Physical Model using a Conditional Adversarial Autoencoder. (arXiv:1806.09617v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1806.09617</link>
<description rdf:parseType="Literal">&lt;p&gt;An adversarial autoencoder conditioned on known parameters of a physical
modeling bowed string synthesizer is evaluated for use in parameter estimation
and resynthesis tasks. Latent dimensions are provided to capture variance not
explained by the conditional parameters. Results are compared with and without
the adversarial training, and a system capable of &quot;copying&quot; a given
parameter-signal bidirectional relationship is examined. A real-time synthesis
system built on a generative, conditioned and regularized neural network is
presented, allowing to construct engaging sound synthesizers based purely on
recorded data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinclair_S/0/1/0/all/0/1&quot;&gt;Stephen Sinclair&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09731">
<title>Evotype: Towards the Evolution of Type Stencils. (arXiv:1806.09731v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.09731</link>
<description rdf:parseType="Literal">&lt;p&gt;Typefaces are an essential resource employed by graphic designers. The
increasing demand for innovative type design work increases the need for good
technological means to assist the designer in the creation of a typeface. We
present an evolutionary computation approach for the generation of type
stencils to draw coherent glyphs for different characters. The proposed system
employs a Genetic Algorithm to evolve populations of type stencils. The
evaluation of each candidate stencil uses a hill climbing algorithm to search
the best configurations to draw the target glyphs. We study the interplay
between legibility, coherence and expressiveness, and show how our framework
can be used in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martins_T/0/1/0/all/0/1&quot;&gt;Tiago Martins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Correia_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Correia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_E/0/1/0/all/0/1&quot;&gt;Ernesto Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1&quot;&gt;Penousal Machado&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09789">
<title>On an Immuno-inspired Distributed, Embodied Action-Evolution cum Selection Algorithm. (arXiv:1806.09789v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.09789</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional Evolutionary Robotics (ER) employs evolutionary techniques to
search for a single monolithic controller which can aid a robot to learn a
desired task. These techniques suffer from bootstrap and deception issues when
the tasks are complex for a single controller to learn. Behaviour-decomposition
techniques have been used to divide a task into multiple subtasks and evolve
separate subcontrollers for each subtask. However, these subcontrollers and the
associated subcontroller arbitrator(s) are all evolved off-line. A distributed,
fully embodied and evolutionary version of such approaches will greatly aid
online learning and help reduce the reality gap. In this paper, we propose an
immunology-inspired embodied action-evolution cum selection algorithm that can
cater to distributed ER. This algorithm evolves different subcontrollers for
different portions of the search space in a distributed manner just as
antibodies are evolved and primed for different antigens in the antigenic
space. Experimentation on a collective of real robots embodied with the
algorithm showed that a repertoire of antibody-like subcontrollers was created,
evolved and shared on-the-fly to cope up with different environmental
conditions. In addition, instead of the conventionally used approach of
broadcasting for sharing, we present an Intelligent Packet Migration scheme
that reduces energy consumption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semwal_T/0/1/0/all/0/1&quot;&gt;Tushar Semwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_D/0/1/0/all/0/1&quot;&gt;Divya D Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1&quot;&gt;Shivashankar B. Nair&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10115">
<title>Cardiopulmonary Resuscitation Quality Parameters from Motion Capture Data using Differential Evolution Fitting of Sinusoids. (arXiv:1806.10115v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.10115</link>
<description rdf:parseType="Literal">&lt;p&gt;Cardiopulmonary resuscitation (CPR) is alongside with electrical
defibrillation the most important treatment for sudden cardiac arrest, which
affects thousands of individuals every year. In this paper, we present a robust
sinusoid model that uses skeletal motion data from an RGB-D (Kinect) sensor and
the Differential Evolution (DE) optimization algorithm to dynamically fit
sinusoidal curves to derive frequency and depth parameters for cardiopulmonary
resuscitation training. It is intended to be part of a robust and easy-to-use
feedback system for CPR training, allowing its use for unsupervised training.
The accuracy of this DE-based approach is evaluated in comparison with data
recorded by a state-of-the-art training mannequin. We optimized the DE
algorithm constants and have shown that with these optimized parameters the
frequency of the CPR is recognized with a median error of 2.55 (2.4%)
compressions per minute compared to the reference training mannequin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lins_C/0/1/0/all/0/1&quot;&gt;Christian Lins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eckhoff_D/0/1/0/all/0/1&quot;&gt;Daniel Eckhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klausen_A/0/1/0/all/0/1&quot;&gt;Andreas Klausen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hellmers_S/0/1/0/all/0/1&quot;&gt;Sandra Hellmers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hein_A/0/1/0/all/0/1&quot;&gt;Andreas Hein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fudickar_S/0/1/0/all/0/1&quot;&gt;Sebastian Fudickar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00447">
<title>Optimal localist and distributed coding of spatiotemporal spike patterns through STDP and coincidence detection. (arXiv:1803.00447v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00447</link>
<description rdf:parseType="Literal">&lt;p&gt;Repeating spatiotemporal spike patterns exist and carry information. Here we
investigate how a single neuron can optimally signal the presence of one given
pattern (localist coding), or of either one of several patterns (distributed
coding, i.e. the neuron&apos;s response is ambiguous but the identity of the pattern
could be inferred from the response of multiple neurons). Intuitively, we
should connect the detector neuron to the neurons that fire during (subsections
of) the patterns. Using a threshold-free leaky integrate-and-fire (LIF) neuron
with time constant $\tau$, non-plastic unitary synapses and homogeneous Poisson
inputs, we derived analytically the signal-to-noise ratio (SNR) of the
resulting pattern detector, even in the presence of jitter. In most cases, this
SNR turned out to be optimal for relatively short $\tau$ (at most a few tens of
ms). Thus long patterns are optimally detected by coincidence detectors working
at a shorter timescale, although these ignore most of the patterns.
Surprisingly, when increasing the number of patterns, the SNR decreases slowly,
and remains acceptable for tens of independent patterns.
&lt;/p&gt;
&lt;p&gt;Next, we wondered if spike-timing-dependent plasticity (STDP) could enable a
neuron to reach the theoretical optimum. We simulated a LIF equipped with STDP,
and repeatedly exposed it to multiple input spike patterns. The LIF
progressively became selective to every repeating pattern with no supervision,
even when the patterns were embedded in Poisson activity. Furthermore, using
certain STDP parameters, the resulting pattern detectors were optimal. Tens of
independent patterns could be learned by a single neuron with a low adaptive
threshold, in contrast with previous studies, in which higher thresholds led to
localist coding only.
&lt;/p&gt;
&lt;p&gt;Taken together these results suggest that coincidence detection and STDP are
powerful mechanisms, compatible with distributed coding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1&quot;&gt;Timoth&amp;#xe9;e Masquelier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kheradpisheh_S/0/1/0/all/0/1&quot;&gt;Saeed Reza Kheradpisheh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09174">
<title>Dilated Temporal Fully-Convolutional Network for Semantic Segmentation of Motion Capture Data. (arXiv:1806.09174v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1806.09174</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation of motion capture sequences plays a key part in many
data-driven motion synthesis frameworks. It is a preprocessing step in which
long recordings of motion capture sequences are partitioned into smaller
segments. Afterwards, additional methods like statistical modeling can be
applied to each group of structurally-similar segments to learn an abstract
motion manifold. The segmentation task however often remains a manual task,
which increases the effort and cost of generating large-scale motion databases.
We therefore propose an automatic framework for semantic segmentation of motion
capture data using a dilated temporal fully-convolutional network. Our model
outperforms a state-of-the-art model in action segmentation, as well as three
networks for sequence modeling. We further show our model is robust against
high noisy training labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheema_N/0/1/0/all/0/1&quot;&gt;Noshaba Cheema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1&quot;&gt;Somayeh Hosseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sprenger_J/0/1/0/all/0/1&quot;&gt;Janis Sprenger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrmann_E/0/1/0/all/0/1&quot;&gt;Erik Herrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Han Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_K/0/1/0/all/0/1&quot;&gt;Klaus Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slusallek_P/0/1/0/all/0/1&quot;&gt;Philipp Slusallek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09605">
<title>Many-Goals Reinforcement Learning. (arXiv:1806.09605v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09605</link>
<description rdf:parseType="Literal">&lt;p&gt;All-goals updating exploits the off-policy nature of Q-learning to update all
possible goals an agent could have from each transition in the world, and was
introduced into Reinforcement Learning (RL) by Kaelbling (1993). In prior work
this was mostly explored in small-state RL problems that allowed tabular
representations and where all possible goals could be explicitly enumerated and
learned separately. In this paper we empirically explore 3 different extensions
of the idea of updating many (instead of all) goals in the context of RL with
deep neural networks (or DeepRL for short). First, in a direct adaptation of
Kaelbling&apos;s approach we explore if many-goals updating can be used to achieve
mastery in non-tabular visual-observation domains. Second, we explore whether
many-goals updating can be used to pre-train a network to subsequently learn
faster and better on a single main task of interest. Third, we explore whether
many-goals updating can be used to provide auxiliary task updates in training a
network to learn faster and better on a single main task of interest. We
provide comparisons to baselines for each of the 3 extensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeriah_V/0/1/0/all/0/1&quot;&gt;Vivek Veeriah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Junhyuk Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Satinder Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09612">
<title>Predictive Maintenance for Industrial IoT of Vehicle Fleets using Hierarchical Modified Fuzzy Support Vector Machine. (arXiv:1806.09612v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.09612</link>
<description rdf:parseType="Literal">&lt;p&gt;Connected vehicle fleets are deployed worldwide in several industrial IoT
scenarios. With the gradual increase of machines being controlled and managed
through networked smart devices, the predictive maintenance potential grows
rapidly. Predictive maintenance has the potential of optimizing uptime as well
as performance such that time and labor associated with inspections and
preventive maintenance are reduced. In order to understand the trends of
vehicle faults with respect to important vehicle attributes viz mileage, age,
vehicle type etc this problem is addressed through hierarchical modified fuzzy
support vector machine (HMFSVM). The proposed method is compared with other
commonly used approaches like logistic regression, random forests and support
vector machines. This helps better implementation of telematics data to ensure
preventative management as part of the desired solution. The superiority of the
proposed method is highlighted through several experimental results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1&quot;&gt;Arindam Chaudhuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09614">
<title>Accuracy-based Curriculum Learning in Deep Reinforcement Learning. (arXiv:1806.09614v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09614</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate a new form of automated curriculum learning
based on adaptive selection of accuracy requirements, called accuracy-based
curriculum learning. Using a reinforcement learning agent based on the Deep
Deterministic Policy Gradient algorithm and addressing the Reacher environment,
we first show that an agent trained with various accuracy requirements sampled
randomly learns more efficiently than when asked to be very accurate at all
times. Then we show that adaptive selection of accuracy requirements, based on
a local measure of competence progress, automatically generates a curriculum
where difficulty progressively increases, resulting in a better learning
efficiency than sampling randomly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fournier_P/0/1/0/all/0/1&quot;&gt;Pierre Fournier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigaud_O/0/1/0/all/0/1&quot;&gt;Olivier Sigaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chetouani_M/0/1/0/all/0/1&quot;&gt;Mohamed Chetouani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09748">
<title>Cycle Consistent Adversarial Denoising Network for Multiphase Coronary CT Angiography. (arXiv:1806.09748v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.09748</link>
<description rdf:parseType="Literal">&lt;p&gt;In coronary CT angiography, a series of CT images are taken at different
levels of radiation dose during the examination. Although this reduces the
total radiation dose, the image quality during the low-dose phases is
significantly degraded. To address this problem, here we propose a novel
semi-supervised learning technique that can remove the noises of the CT images
obtained in the low-dose phases by learning from the CT images in the routine
dose phases. Although a supervised learning approach is not possible due to the
differences in the underlying heart structure in two phases, the images in the
two phases are closely related so that we propose a cycle-consistent
adversarial denoising network to learn the non-degenerate mapping between the
low and high dose cardiac phases. Experimental results showed that the proposed
method effectively reduces the noise in the low-dose CT image while the
preserving detailed texture and edge information. Moreover, thanks to the
cyclic consistency and identity loss, the proposed network does not create any
artificial features that are not present in the input images. Visual grading
and quality evaluation also confirm that the proposed method provides
significant improvement in diagnostic quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_E/0/1/0/all/0/1&quot;&gt;Eunhee Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_H/0/1/0/all/0/1&quot;&gt;Hyun Jung Koo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dong Hyun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Joon Bum Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09769">
<title>Plenoptic Monte Carlo Object Localization for Robot Grasping under Layered Translucency. (arXiv:1806.09769v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1806.09769</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to fully function in human environments, robot perception will need
to account for the uncertainty caused by translucent materials. Translucency
poses several open challenges in the form of transparent objects (e.g.,
drinking glasses), refractive media (e.g., water), and diffuse partial
occlusions (e.g., objects behind stained glass panels). This paper presents
Plenoptic Monte Carlo Localization (PMCL) as a method for localizing object
poses in the presence of translucency using plenoptic (light-field)
observations. We propose a new depth descriptor, the Depth Likelihood Volume
(DLV), and its use within a Monte Carlo object localization algorithm. We
present results of localizing and manipulating objects with translucent
materials and objects occluded by layers of translucency. Our PMCL
implementation uses observations from a Lytro first generation light field
camera to allow a Michigan Progress Fetch robot to perform grasping.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zheming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenkins_O/0/1/0/all/0/1&quot;&gt;Odest Chadwicke Jenkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09787">
<title>Causal Inference for Early Detection of Pathogenic Social Media Accounts. (arXiv:1806.09787v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1806.09787</link>
<description rdf:parseType="Literal">&lt;p&gt;Pathogenic social media accounts such as terrorist supporters exploit
communities of supporters for conducting attacks on social media. Early
detection of PSM accounts is crucial as they are likely to be key users in
making a harmful message &quot;viral&quot;. This paper overviews my recent doctoral work
on utilizing causal inference to identify PSM accounts within a short time
frame around their activity. The proposed scheme (1) assigns time-decay
causality scores to users, (2) applies a community detection-based algorithm to
group of users sharing similar causality scores and finally (3) deploys a
classification algorithm to classify accounts. Unlike existing techniques that
require network structure, cascade path, or content, our scheme relies solely
on action log of users.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvari_H/0/1/0/all/0/1&quot;&gt;Hamidreza Alvari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09842">
<title>Quadratic Decomposable Submodular Function Minimization. (arXiv:1806.09842v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09842</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new convex optimization problem, termed quadratic decomposable
submodular function minimization. The problem arises in many learning on graphs
and hypergraphs settings and is closely related to decomposable submodular
function minimization. We approach the problem via a new dual strategy and
describe an objective that may be optimized via random coordinate descent (RCD)
methods and projections onto cones. We also establish the linear convergence
rate of the RCD algorithm and develop efficient projection algorithms with
provable performance guarantees. Numerical experiments in transductive learning
on hypergraphs confirm the efficiency of the proposed algorithm and demonstrate
the significant improvements in prediction accuracy with respect to
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1&quot;&gt;Niao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1&quot;&gt;Olgica Milenkovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09935">
<title>On the performance of multi-objective estimation of distribution algorithms for combinatorial problems. (arXiv:1806.09935v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.09935</link>
<description rdf:parseType="Literal">&lt;p&gt;Fitness landscape analysis investigates features with a high influence on the
performance of optimization algorithms, aiming to take advantage of the
addressed problem characteristics. In this work, a fitness landscape analysis
using problem features is performed for a Multi-objective Bayesian Optimization
Algorithm (mBOA) on instances of MNK-landscape problem for 2, 3, 5 and 8
objectives. We also compare the results of mBOA with those provided by NSGA-III
through the analysis of their estimated runtime necessary to identify an
approximation of the Pareto front. Moreover, in order to scrutinize the
probabilistic graphic model obtained by mBOA, the Pareto front is examined
according to a probabilistic view. The fitness landscape study shows that mBOA
is moderately or loosely influenced by some problem features, according to a
simple and a multiple linear regression model, which is being proposed to
predict the algorithms performance in terms of the estimated runtime. Besides,
we conclude that the analysis of the probabilistic graphic model produced at
the end of evolution can be useful to understand the convergence and diversity
performances of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martins_M/0/1/0/all/0/1&quot;&gt;Marcella S. R. Martins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yafrani_M/0/1/0/all/0/1&quot;&gt;Mohamed El Yafrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santana_R/0/1/0/all/0/1&quot;&gt;Roberto Santana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delgado_M/0/1/0/all/0/1&quot;&gt;Myriam Delgado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luders_R/0/1/0/all/0/1&quot;&gt;Ricardo L&amp;#xfc;ders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahiod_B/0/1/0/all/0/1&quot;&gt;Bela&amp;#xef;d Ahiod&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09936">
<title>Open the Black Box Data-Driven Explanation of Black Box Decision Systems. (arXiv:1806.09936v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.09936</link>
<description rdf:parseType="Literal">&lt;p&gt;Black box systems for automated decision making, often based on machine
learning over (big) data, map a user&apos;s features into a class or a score without
exposing the reasons why. This is problematic not only for lack of
transparency, but also for possible biases hidden in the algorithms, due to
human prejudices and collection artifacts hidden in the training data, which
may lead to unfair or wrong decisions. We introduce the local-to-global
framework for black box explanation, a novel approach with promising early
results, which paves the road for a wide spectrum of future developments along
three dimensions: (i) the language for expressing explanations in terms of
highly expressive logic-based rules, with a statistical and causal
interpretation; (ii) the inference of local explanations aimed at revealing the
logic of the decision adopted for a specific instance by querying and auditing
the black box in the vicinity of the target instance; (iii), the bottom-up
generalization of the many local explanations into simple global ones, with
algorithms that optimize the quality and comprehensibility of explanations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedreschi_D/0/1/0/all/0/1&quot;&gt;Dino Pedreschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannotti_F/0/1/0/all/0/1&quot;&gt;Fosca Giannotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guidotti_R/0/1/0/all/0/1&quot;&gt;Riccardo Guidotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monreale_A/0/1/0/all/0/1&quot;&gt;Anna Monreale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappalardo_L/0/1/0/all/0/1&quot;&gt;Luca Pappalardo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruggieri_S/0/1/0/all/0/1&quot;&gt;Salvatore Ruggieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turini_F/0/1/0/all/0/1&quot;&gt;Franco Turini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09997">
<title>Probabilistic Inference Using Generators - The Statues Algorithm. (arXiv:1806.09997v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.09997</link>
<description rdf:parseType="Literal">&lt;p&gt;We present here a new probabilistic inference algorithm that gives exact
results in the domain of discrete probability distributions. This algorithm,
named the Statues algorithm, calculates the marginal probability distribution
on probabilistic models defined as direct acyclic graphs. These models are made
up of well-defined primitives that allow to express, in particular, joint
probability distributions, Bayesian networks, discrete Markov chains,
conditioning and probabilistic arithmetic. The Statues algorithm relies on a
variable binding mechanism based on the generator construct, a special form of
coroutine; being related to the enumeration algorithm, this new algorithm
brings important improvements in terms of efficiency, which makes it valuable
in regard to other exact marginalization algorithms. After introduction of
several definitions, primitives and compositional rules, we present in details
the Statues algorithm. Then, we briefly discuss the interest of this algorithm
compared to others and we present possible extensions. Finally, we introduce
Lea and MicroLea, two Python libraries implementing the Statues algorithm,
along with several use cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denis_P/0/1/0/all/0/1&quot;&gt;Pierre Denis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10018">
<title>Complexity Results for Preference Aggregation over (m)CP-nets: Pareto and Majority Voting. (arXiv:1806.10018v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.10018</link>
<description rdf:parseType="Literal">&lt;p&gt;Combinatorial preference aggregation has many applications in AI. Given the
exponential nature of these preferences, compact representations are needed and
($m$)CP-nets are among the most studied ones. Sequential and global voting are
two ways to aggregate preferences over CP-nets. In the former, preferences are
aggregated feature-by-feature. Hence, when preferences have specific feature
dependencies, sequential voting may exhibit voting paradoxes, i.e., it might
select sub-optimal outcomes. To avoid paradoxes in sequential voting, one has
often assumed the $\mathcal{O}$-legality restriction, which imposes a shared
topological order among all the CP-nets. On the contrary, in global voting,
CP-nets are considered as a whole during preference aggregation. For this
reason, global voting is immune from paradoxes, and there is no need to impose
restrictions over the CP-nets&apos; topological structure. Sequential voting over
$\mathcal{O}$-legal CP-nets has extensively been investigated. On the other
hand, global voting over non-$\mathcal{O}$-legal CP-nets has not carefully been
analyzed, despite it was stated in the literature that a theoretical comparison
between global and sequential voting was promising and a precise complexity
analysis for global voting has been asked for multiple times. In quite few
works, very partial results on the complexity of global voting over CP-nets
have been given. We start to fill this gap by carrying out a thorough
complexity analysis of Pareto and majority global voting over not necessarily
$\mathcal{O}$-legal acyclic binary polynomially connected (m)CP-nets. We settle
these problems in the polynomial hierarchy, and some of them in PTIME or
LOGSPACE, whereas EXPTIME was the previously known upper bound for most of
them. We show various tight lower bounds and matching upper bounds for problems
that up to date did not have any explicit non-obvious lower bound.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1&quot;&gt;Thomas Lukasiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malizia_E/0/1/0/all/0/1&quot;&gt;Enrico Malizia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10130">
<title>The Art of Drafting: A Team-Oriented Hero Recommendation System for Multiplayer Online Battle Arena Games. (arXiv:1806.10130v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.10130</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiplayer Online Battle Arena (MOBA) games have received increasing
popularity recently. In a match of such games, players compete in two teams of
five, each controlling an in-game avatars, known as heroes, selected from a
roster of more than 100. The selection of heroes, also known as pick or draft,
takes place before the match starts and alternates between the two teams until
each player has selected one hero. Heroes are designed with different strengths
and weaknesses to promote team cooperation in a game. Intuitively, heroes in a
strong team should complement each other&apos;s strengths and suppressing those of
opponents. Hero drafting is therefore a challenging problem due to the complex
hero-to-hero relationships to consider. In this paper, we propose a novel hero
recommendation system that suggests heroes to add to an existing team while
maximizing the team&apos;s prospect for victory. To that end, we model the drafting
between two teams as a combinatorial game and use Monte Carlo Tree Search
(MCTS) for estimating the values of hero combinations. Our empirical evaluation
shows that hero teams drafted by our recommendation algorithm have
significantly higher win rate against teams constructed by other baseline and
state-of-the-art strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhengxing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Truong-Huy D Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amato_C/0/1/0/all/0/1&quot;&gt;Chris Amato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooper_S/0/1/0/all/0/1&quot;&gt;Seth Cooper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yizhou Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Nasr_M/0/1/0/all/0/1&quot;&gt;Magy Seif El-Nasr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.10087">
<title>Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations. (arXiv:1709.10087v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.10087</link>
<description rdf:parseType="Literal">&lt;p&gt;Dexterous multi-fingered hands are extremely versatile and provide a generic
way to perform a multitude of tasks in human-centric environments. However,
effectively controlling them remains challenging due to their high
dimensionality and large number of potential contacts. Deep reinforcement
learning (DRL) provides a model-agnostic approach to control complex dynamical
systems, but has not been shown to scale to high-dimensional dexterous
manipulation. Furthermore, deployment of DRL on physical systems remains
challenging due to sample inefficiency. Consequently, the success of DRL in
robotics has thus far been limited to simpler manipulators and tasks. In this
work, we show that model-free DRL can effectively scale up to complex
manipulation tasks with a high-dimensional 24-DoF hand, and solve them from
scratch in simulated experiments. Furthermore, with the use of a small number
of human demonstrations, the sample complexity can be significantly reduced,
which enables learning with sample sizes equivalent to a few hours of robot
experience. The use of demonstrations result in policies that exhibit very
natural movements and, surprisingly, are also substantially more robust.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1&quot;&gt;Aravind Rajeswaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vikash Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vezzani_G/0/1/0/all/0/1&quot;&gt;Giulia Vezzani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1&quot;&gt;John Schulman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Todorov_E/0/1/0/all/0/1&quot;&gt;Emanuel Todorov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00119">
<title>Integrating Human-Provided Information Into Belief State Representation Using Dynamic Factorization. (arXiv:1803.00119v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00119</link>
<description rdf:parseType="Literal">&lt;p&gt;In partially observed environments, it can be useful for a human to provide
the robot with declarative information that represents probabilistic relational
constraints on properties of objects in the world, augmenting the robot&apos;s
sensory observations. For instance, a robot tasked with a search-and-rescue
mission may be informed by the human that two victims are probably in the same
room. An important question arises: how should we represent the robot&apos;s
internal knowledge so that this information is correctly processed and combined
with raw sensory information? In this paper, we provide an efficient belief
state representation that dynamically selects an appropriate factoring,
combining aspects of the belief when they are correlated through information
and separating them when they are not. This strategy works in open domains, in
which the set of possible objects is not known in advance, and provides
significant improvements in inference time over a static factoring, leading to
more efficient planning for complex partially observed tasks. We validate our
approach experimentally in two open-domain planning problems: a 2D discrete
gridworld task and a 3D continuous cooking task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitnis_R/0/1/0/all/0/1&quot;&gt;Rohan Chitnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie Pack Kaelbling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Lozano-P&amp;#xe9;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07616">
<title>IntPhys: A Framework and Benchmark for Visual Intuitive Physics Reasoning. (arXiv:1803.07616v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07616</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to reach human performance on complex visual tasks, artificial
systems need to incorporate a significant amount of understanding of the world
in terms of macroscopic objects, movements, forces, etc. Inspired by work on
intuitive physics in infants, we propose an evaluation framework which
diagnoses how much a given system understands about physics by testing whether
it can tell apart well matched videos of possible versus impossible events. The
test requires systems to compute a physical plausibility score over an entire
video. It is free of bias and can test a range of specific physical reasoning
skills. We then describe the first release of a benchmark dataset aimed at
learning intuitive physics in an unsupervised way, using videos constructed
with a game engine. We describe two Deep Neural Network baseline systems
trained with a future frame prediction objective and tested on the possible
versus impossible discrimination task. The analysis of their results compared
to human data gives novel insights in the potentials and limitations of next
frame prediction architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riochet_R/0/1/0/all/0/1&quot;&gt;Ronan Riochet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_M/0/1/0/all/0/1&quot;&gt;Mario Ynocente Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernard_M/0/1/0/all/0/1&quot;&gt;Mathieu Bernard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1&quot;&gt;Adam Lerer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1&quot;&gt;Rob Fergus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izard_V/0/1/0/all/0/1&quot;&gt;V&amp;#xe9;ronique Izard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1&quot;&gt;Emmanuel Dupoux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08834">
<title>Measuring and Computing Database Inconsistency via Repairs. (arXiv:1804.08834v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08834</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a generic numerical measure of inconsistency of a database with
respect to integrity constraints that is based on a repair semantics. A
particular measure is investigated, with mechanisms for computing it via
answer-set programs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1&quot;&gt;Leopoldo Bertossi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07830">
<title>Learning to Teach in Cooperative Multiagent Reinforcement Learning. (arXiv:1805.07830v3 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07830</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a framework and algorithm for peer-to-peer teaching in cooperative
multiagent reinforcement learning. Our algorithm, Learning to Coordinate and
Teach Reinforcement (LeCTR), trains advising policies by using students&apos;
learning progress as a teaching reward. Agents using LeCTR learn to assume the
role of a teacher or student at the appropriate moments, exchanging action
advice to accelerate the entire learning process. Our algorithm supports
teaching heterogeneous teammates, advising under communication constraints, and
learns both what and when to advise. LeCTR is demonstrated to outperform the
final performance and rate of learning of prior teaching methods on multiple
benchmark domains. To our knowledge, this is the first approach for learning to
teach in a multiagent setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omidshafiei_S/0/1/0/all/0/1&quot;&gt;Shayegan Omidshafiei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dong-Ki Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Miao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tesauro_G/0/1/0/all/0/1&quot;&gt;Gerald Tesauro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1&quot;&gt;Matthew Riemer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amato_C/0/1/0/all/0/1&quot;&gt;Christopher Amato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1&quot;&gt;Murray Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1&quot;&gt;Jonathan P. How&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09655">
<title>Unsupervised Learning of Sensorimotor Affordances by Stochastic Future Prediction. (arXiv:1806.09655v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09655</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, much progress has been made building systems that can capture
static image properties, but natural environments are intrinsically dynamic.
For an intelligent agent, perception is responsible not only for capturing
features of scene content, but also capturing its \textit{affordances}: how the
state of things can change, especially as the result of the agent&apos;s actions. We
propose an unsupervised method to learn representations of the sensorimotor
affordances of an environment. We do so by learning an embedding for stochastic
future prediction that is (i) sensitive to scene dynamics and minimally
sensitive to static scene content and (ii) compositional in nature, capturing
the fact that changes in the environment can be composed to produce a
cumulative change. We show that these two properties are sufficient to induce
representations that are reusable across visually distinct scenes that share
degrees of freedom. We show the applicability of our method to synthetic
settings and its potential for understanding more complex, realistic visual
settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1&quot;&gt;Oleh Rybkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pertsch_K/0/1/0/all/0/1&quot;&gt;Karl Pertsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1&quot;&gt;Andrew Jaegle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1&quot;&gt;Konstantinos G. Derpanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1&quot;&gt;Kostas Daniilidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09679">
<title>On the Resilience of RTL NN Accelerators: Fault Characterization and Mitigation. (arXiv:1806.09679v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09679</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Learning (ML) is making a strong resurgence in tune with the massive
generation of unstructured data which in turn requires massive computational
resources. Due to the inherently compute- and power-intensive structure of
Neural Networks (NNs), hardware accelerators emerge as a promising solution.
However, with technology node scaling below 10nm, hardware accelerators become
more susceptible to faults, which in turn can impact the NN accuracy. In this
paper, we study the resilience aspects of Register-Transfer Level (RTL) model
of NN accelerators, in particular, fault characterization and mitigation. By
following a High-Level Synthesis (HLS) approach, first, we characterize the
vulnerability of various components of RTL NN. We observed that the severity of
faults depends on both i) application-level specifications, i.e., NN data
(inputs, weights, or intermediate), NN layers, and NN activation functions, and
ii) architectural-level specifications, i.e., data representation model and the
parallelism degree of the underlying accelerator. Second, motivated by
characterization results, we present a low-overhead fault mitigation technique
that can efficiently correct bit flips, by 47.3% better than state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salami_B/0/1/0/all/0/1&quot;&gt;Behzad Salami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unsal_O/0/1/0/all/0/1&quot;&gt;Osman Unsal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cristal_A/0/1/0/all/0/1&quot;&gt;Adrian Cristal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09692">
<title>An Outcome Model Approach to Translating a Randomized Controlled Trial Results to a Target Population. (arXiv:1806.09692v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1806.09692</link>
<description rdf:parseType="Literal">&lt;p&gt;Participants enrolled into randomized controlled trials (RCTs) often do not
reflect real-world populations. Previous research in how best to translate RCT
results to target populations has focused on weighting RCT data to look like
the target data. Simulation work, however, has suggested that an outcome model
approach may be preferable. Here we describe such an approach using source data
from the 2x2 factorial NAVIGATOR trial which evaluated the impact of valsartan
and nateglinide on cardiovascular outcomes and new-onset diabetes in a
pre-diabetic population. Our target data consisted of people with pre-diabetes
serviced at our institution. We used Random Survival Forests to develop
separate outcome models for each of the 4 treatments, estimating the 5-year
risk difference for progression to diabetes and estimated the treatment effect
in our local patient populations, as well as sub-populations, and the results
compared to the traditional weighting approach. Our models suggested that the
treatment effect for valsartan in our patient population was the same as in the
trial, whereas for nateglinide treatment effect was stronger than observed in
the original trial. Our effect estimates were more efficient than the weighting
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goldstein_B/0/1/0/all/0/1&quot;&gt;Benjamin A. Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Phelan_M/0/1/0/all/0/1&quot;&gt;Matthew Phelan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pagidipati_N/0/1/0/all/0/1&quot;&gt;Neha J. Pagidipati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Holman_R/0/1/0/all/0/1&quot;&gt;Rury R. Holman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stuart_M/0/1/0/all/0/1&quot;&gt;Michael J. Pencina Elizabeth A Stuart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09708">
<title>Mimic and Classify : A meta-algorithm for Conditional Independence Testing. (arXiv:1806.09708v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.09708</link>
<description rdf:parseType="Literal">&lt;p&gt;Given independent samples generated from the joint distribution
$p(\mathbf{x},\mathbf{y},\mathbf{z})$, we study the problem of Conditional
Independence (CI-Testing), i.e., whether the joint equals the CI distribution
$p^{CI}(\mathbf{x},\mathbf{y},\mathbf{z})= p(\mathbf{z})
p(\mathbf{y}|\mathbf{z})p(\mathbf{x}|\mathbf{z})$ or not. We cast this problem
under the purview of the proposed, provable meta-algorithm, &quot;Mimic and
Classify&quot;, which is realized in two-steps: (a) Mimic the CI distribution close
enough to recover the support, and (b) Classify to distinguish the joint and
the CI distribution. Thus, as long as we have a good generative model and a
good classifier, we potentially have a sound CI Tester. With this modular
paradigm, CI Testing becomes amiable to be handled by state-of-the-art, both
generative and classification methods from the modern advances in Deep
Learning, which in general can handle issues related to curse of dimensionality
and operation in small sample regime. We show intensive numerical experiments
on synthetic and real datasets where new mimic methods such conditional GANs,
Regression with Neural Nets, outperform the current best CI Testing performance
in the literature. Our theoretical results provide analysis on the estimation
of null distribution as well as allow for general measures, i.e., when either
some of the random variables are discrete and some are continuous or when one
or more of them are discrete-continuous mixtures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sen_R/0/1/0/all/0/1&quot;&gt;Rajat Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shanmugam_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Asnani_H/0/1/0/all/0/1&quot;&gt;Himanshu Asnani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rahimzamani_A/0/1/0/all/0/1&quot;&gt;Arman Rahimzamani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kannan_S/0/1/0/all/0/1&quot;&gt;Sreeram Kannan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09710">
<title>Why Interpretability in Machine Learning? An Answer Using Distributed Detection and Data Fusion Theory. (arXiv:1806.09710v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.09710</link>
<description rdf:parseType="Literal">&lt;p&gt;As artificial intelligence is increasingly affecting all parts of society and
life, there is growing recognition that human interpretability of machine
learning models is important. It is often argued that accuracy or other similar
generalization performance metrics must be sacrificed in order to gain
interpretability. Such arguments, however, fail to acknowledge that the overall
decision-making system is composed of two entities: the learned model and a
human who fuses together model outputs with his or her own information. As
such, the relevant performance criteria should be for the entire system, not
just for the machine learning component. In this work, we characterize the
performance of such two-node tandem data fusion systems using the theory of
distributed detection. In doing so, we work in the population setting and model
interpretable learned models as multi-level quantizers. We prove that under our
abstraction, the overall system of a human with an interpretable classifier
outperforms one with a black box classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Varshney_K/0/1/0/all/0/1&quot;&gt;Kush R. Varshney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khanduri_P/0/1/0/all/0/1&quot;&gt;Prashant Khanduri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sharma_P/0/1/0/all/0/1&quot;&gt;Pranay Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Varshney_P/0/1/0/all/0/1&quot;&gt;Pramod K. Varshney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09712">
<title>On consistent estimation of the missing mass. (arXiv:1806.09712v1 [math.ST])</title>
<link>http://arxiv.org/abs/1806.09712</link>
<description rdf:parseType="Literal">&lt;p&gt;Given $n$ samples from a population of individuals belonging to different
types with unknown proportions, how do we estimate the probability of
discovering a new type at the $(n+1)$-th draw? This is a classical problem in
statistics, commonly referred to as the missing mass estimation problem. Recent
results by Ohannessian and Dahleh \citet{Oha12} and Mossel and Ohannessian
\citet{Mos15} showed: i) the impossibility of estimating (learning) the missing
mass without imposing further structural assumptions on the type proportions;
ii) the consistency of the Good-Turing estimator for the missing mass under the
assumption that the tail of the type proportions decays to zero as a regularly
varying function with parameter $\alpha\in(0,1)$. In this paper we rely on
tools from Bayesian nonparametrics to provide an alternative, and simpler,
proof of the impossibility of a distribution-free estimation of the missing
mass. Up to our knowledge, the use of Bayesian ideas to study large sample
asymptotics for the missing mass is new, and it could be of independent
interest. Still relying on Bayesian nonparametric tools, we then show that
under regularly varying type proportions the convergence rate of the
Good-Turing estimator is the best rate that any estimator can achieve, up to a
slowly varying function, and that minimax rate must be at least
$n^{-\alpha/2}$. We conclude with a discussion of our results, and by
conjecturing that the Good-Turing estimator is an rate optimal minimax
estimator under regularly varying type proportions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ayed_F/0/1/0/all/0/1&quot;&gt;Fadhel Ayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Battiston_M/0/1/0/all/0/1&quot;&gt;Marco Battiston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Camerlenghi_F/0/1/0/all/0/1&quot;&gt;Federico Camerlenghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Favaro_S/0/1/0/all/0/1&quot;&gt;Stefano Favaro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09736">
<title>Computational Analysis of Insurance Complaints: GEICO Case Study. (arXiv:1806.09736v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1806.09736</link>
<description rdf:parseType="Literal">&lt;p&gt;The online environment has provided a great opportunity for insurance
policyholders to share their complaints with respect to different services.
These complaints can reveal valuable information for insurance companies who
seek to improve their services; however, analyzing a huge number of online
complaints is a complicated task for human and must involve computational
methods to create an efficient process. This research proposes a computational
approach to characterize the major topics of a large number of online
complaints. Our approach is based on using the topic modeling approach to
disclose the latent semantic of complaints. The proposed approach deployed on
thousands of GEICO negative reviews. Analyzing 1,371 GEICO complaints indicates
that there are 30 major complains in four categories: (1) customer service, (2)
insurance coverage, paperwork, policy, and reports, (3) legal issues, and (4)
costs, estimates, and payments. This research approach can be used in other
applications to explore a large number of reviews.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karami_A/0/1/0/all/0/1&quot;&gt;Amir Karami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pendergraft_N/0/1/0/all/0/1&quot;&gt;Noelle M. Pendergraft&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09762">
<title>Boulevard: Regularized Stochastic Gradient Boosted Trees and Their Limiting Distribution. (arXiv:1806.09762v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1806.09762</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper examines a novel gradient boosting framework for regression. We
regularize gradient boosted trees by introducing subsampling and employ a
modified shrinkage algorithm so that at every boosting stage the estimate is
given by an average of trees. The resulting algorithm, titled Boulevard, is
shown to converge as the number of trees grows. We also demonstrate a central
limit theorem for this limit, allowing a characterization of uncertainty for
predictions. A simulation study and real world examples provide support for
both the predictive accuracy of the model and its limiting behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yichen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hooker_G/0/1/0/all/0/1&quot;&gt;Giles Hooker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09764">
<title>Deep Generative Models with Learnable Knowledge Constraints. (arXiv:1806.09764v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09764</link>
<description rdf:parseType="Literal">&lt;p&gt;The broad set of deep generative models (DGMs) has achieved remarkable
advances. However, it is often difficult to incorporate rich structured domain
knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a
principled framework to impose structured constraints on probabilistic models,
but has limited applicability to the diverse DGMs that can lack a Bayesian
formulation or even explicit density evaluation. PR also requires constraints
to be fully specified {\it a priori}, which is impractical or suboptimal for
complex knowledge with learnable uncertain parts. In this paper, we establish
mathematical correspondence between PR and reinforcement learning (RL), and,
based on the connection, expand PR to learn constraints as the extrinsic reward
in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is
flexible to adapt arbitrary constraints with the model jointly. Experiments on
human image generation and templated sentence generation show models with
learned knowledge constraints by our algorithm greatly improve over base
generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhiting Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zichao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1&quot;&gt;Lianhui Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Haoye Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09780">
<title>Correlated pseudo-marginal Metropolis-Hastings using quasi-Newton proposals. (arXiv:1806.09780v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1806.09780</link>
<description rdf:parseType="Literal">&lt;p&gt;Pseudo-marginal Metropolis-Hastings (pmMH) is a versatile algorithm for
sampling from target distributions which are not easy to evaluate point-wise.
However, pmMH requires good proposal distributions to sample efficiently from
the target, which can be problematic to construct in practice. This is
especially a problem for high-dimensional targets when the standard random-walk
proposal is inefficient.
&lt;/p&gt;
&lt;p&gt;We extend pmMH to allow for constructing the proposal based on information
from multiple past iterations. As a consequence, quasi-Newton (qN) methods can
be employed to form proposals which utilize gradient information to guide the
Markov chain to areas of high probability and to construct approximations of
the local curvature to scale step sizes. The proposed method is demonstrated on
several problems which indicate that qN proposals can perform better than other
common Hessian-based proposals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dahlin_J/0/1/0/all/0/1&quot;&gt;Johan Dahlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wills_A/0/1/0/all/0/1&quot;&gt;Adrian Wills&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ninness_B/0/1/0/all/0/1&quot;&gt;Brett Ninness&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09795">
<title>Multi-agent Inverse Reinforcement Learning for General-sum Stochastic Games. (arXiv:1806.09795v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09795</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the problem of multi-agent inverse reinforcement
learning (MIRL) in a two-player general-sum stochastic game framework. Five
variants of MIRL are considered: uCS-MIRL, advE-MIRL, cooE-MIRL, uCE-MIRL, and
uNE-MIRL, each distinguished by its solution concept. Problem uCS-MIRL is a
cooperative game in which the agents employ cooperative strategies that aim to
maximize the total game value. In problem uCE-MIRL, agents are assumed to
follow strategies that constitute a correlated equilibrium while maximizing
total game value. Problem uNE-MIRL is similar to uCE-MIRL in total game value
maximization, but it is assumed that the agents are playing a Nash equilibrium.
Problems advE-MIRL and cooE-MIRL assume agents are playing an adversarial
equilibrium and a coordination equilibrium, respectively. We propose novel
approaches to address these five problems under the assumption that the game
observer either knows or is able to accurate estimate the policies and solution
concepts for players. For uCS-MIRL, we first develop a characteristic set of
solutions ensuring that the observed bi-policy is a uCS and then apply a
Bayesian inverse learning method. For uCE-MIRL, we develop a linear programming
problem subject to constraints that define necessary and sufficient conditions
for the observed policies to be correlated equilibria. The objective is to
choose a solution that not only minimizes the total game value difference
between the observed bi-policy and a local uCS, but also maximizes the scale of
the solution. We apply a similar treatment to the problem of uNE-MIRL. The
remaining two problems can be solved efficiently by taking advantage of
solution uniqueness and setting up a convex optimization problem. Results are
validated on various benchmark grid-world games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiaomin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1&quot;&gt;Stephen C. Adams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beling_P/0/1/0/all/0/1&quot;&gt;Peter A. Beling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09823">
<title>Approximate Nearest Neighbor Search in High Dimensions. (arXiv:1806.09823v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1806.09823</link>
<description rdf:parseType="Literal">&lt;p&gt;The nearest neighbor problem is defined as follows: Given a set $P$ of $n$
points in some metric space $(X,D)$, build a data structure that, given any
point $q$, returns a point in $P$ that is closest to $q$ (its &quot;nearest
neighbor&quot; in $P$). The data structure stores additional information about the
set $P$, which is then used to find the nearest neighbor without computing all
distances between $q$ and $P$. The problem has a wide range of applications in
machine learning, computer vision, databases and other fields.
&lt;/p&gt;
&lt;p&gt;To reduce the time needed to find nearest neighbors and the amount of memory
used by the data structure, one can formulate the {\em approximate} nearest
neighbor problem, where the the goal is to return any point $p&apos; \in P$ such
that the distance from $q$ to $p&apos;$ is at most $c \cdot \min_{p \in P} D(q,p)$,
for some $c \geq 1$. Over the last two decades, many efficient solutions to
this problem were developed. In this article we survey these developments, as
well as their connections to questions in geometric functional analysis and
combinatorial geometry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andoni_A/0/1/0/all/0/1&quot;&gt;Alexandr Andoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1&quot;&gt;Piotr Indyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razenshteyn_I/0/1/0/all/0/1&quot;&gt;Ilya Razenshteyn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09827">
<title>Unveiling the semantic structure of text documents using paragraph-aware Topic Models. (arXiv:1806.09827v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1806.09827</link>
<description rdf:parseType="Literal">&lt;p&gt;Classic Topic Models are built under the Bag Of Words assumption, in which
word position is ignored for simplicity. Besides, symmetric priors are
typically used in most applications. In order to easily learn topics with
different properties among the same corpus, we propose a new line of work in
which the paragraph structure is exploited. Our proposal is based on the
following assumption: in many text document corpora there are formal
constraints shared across all the collection, e.g. sections. When this
assumption is satisfied, some paragraphs may be related to general concepts
shared by all documents in the corpus, while others would contain the genuine
description of documents. Assuming each paragraph can be semantically more
general, specific, or hybrid, we look for ways to measure this, transferring
this distinction to topics and being able to learn what we call specific and
general topics. Experiments show that this is a proper methodology to highlight
certain paragraphs in structured documents at the same time we learn
interesting and more diverse topics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roca_Sotelo_S/0/1/0/all/0/1&quot;&gt;Sim&amp;#xf3;n Roca-Sotelo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arenas_Garcia_J/0/1/0/all/0/1&quot;&gt;Jer&amp;#xf3;nimo Arenas-Garc&amp;#xed;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09888">
<title>Towards an understanding of CNNs: analysing the recovery of activation pathways via Deep Convolutional Sparse Coding. (arXiv:1806.09888v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.09888</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Convolutional Sparse Coding (D-CSC) is a framework reminiscent of deep
convolutional neural networks (DCNNs), but by omitting the learning of the
dictionaries one can more transparently analyse the role of the activation
function and its ability to recover activation paths through the layers.
Papyan, Romano, and Elad conducted an analysis of such an architecture,
demonstrated the relationship with DCNNs and proved conditions under which the
D-CSC is guaranteed to recover specific activation paths. A technical
innovation of their work highlights that one can view the efficacy of the ReLU
nonlinear activation function of a DCNN through a new variant of the tensor&apos;s
sparsity, referred to as stripe-sparsity. Using this they proved that
representations with an activation density proportional to the ambient
dimension of the data are recoverable. We extend their uniform guarantees to a
modified model and prove that with high probability the true activation is
typically possible to recover for a greater density of activations per layer.
Our extension follows from incorporating the prior work on one step
thresholding by Schnass and Vandergheynst.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murray_M/0/1/0/all/0/1&quot;&gt;Michael Murray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanner_J/0/1/0/all/0/1&quot;&gt;Jared Tanner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09905">
<title>Conditioning Deep Generative Raw Audio Models for Structured Automatic Music. (arXiv:1806.09905v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1806.09905</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing automatic music generation approaches that feature deep learning can
be broadly classified into two types: raw audio models and symbolic models.
Symbolic models, which train and generate at the note level, are currently the
more prevalent approach; these models can capture long-range dependencies of
melodic structure, but fail to grasp the nuances and richness of raw audio
generations. Raw audio models, such as DeepMind&apos;s WaveNet, train directly on
sampled audio waveforms, allowing them to produce realistic-sounding, albeit
unstructured music. In this paper, we propose an automatic music generation
methodology combining both of these approaches to create structured,
realistic-sounding compositions. We consider a Long Short Term Memory network
to learn the melodic structure of different styles of music, and then use the
unique symbolic generations from this model as a conditioning input to a
WaveNet-based raw audio generator, creating a model for automatic, novel music.
We then evaluate this approach by showcasing results of this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manzelli_R/0/1/0/all/0/1&quot;&gt;Rachel Manzelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakkar_V/0/1/0/all/0/1&quot;&gt;Vijay Thakkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siahkamari_A/0/1/0/all/0/1&quot;&gt;Ali Siahkamari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulis_B/0/1/0/all/0/1&quot;&gt;Brian Kulis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09908">
<title>Manifold Structured Prediction. (arXiv:1806.09908v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.09908</link>
<description rdf:parseType="Literal">&lt;p&gt;Structured prediction provides a general framework to deal with supervised
problems where the outputs have semantically rich structure. While classical
approaches consider finite, albeit potentially huge, output spaces, in this
paper we discuss how structured prediction can be extended to a continuous
scenario. Specifically, we study a structured prediction approach to manifold
valued regression. We characterize a class of problems for which the considered
approach is statistically consistent and study how geometric optimization can
be used to compute the corresponding estimator. Promising experimental results
on both simulated and real data complete our study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1&quot;&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ciliberto_C/0/1/0/all/0/1&quot;&gt;Carlo Ciliberto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marconi_G/0/1/0/all/0/1&quot;&gt;Gian Maria Marconi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1&quot;&gt;Lorenzo Rosasco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09918">
<title>Hierarchical VampPrior Variational Fair Auto-Encoder. (arXiv:1806.09918v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.09918</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision making is a process that is extremely prone to different biases. In
this paper we consider learning fair representation that aim at removing
nuisance (sensitive) information from the decision process. For this purpose,
we propose to use deep generative modeling and adapt a hierarchical Variational
Auto-Encoder to learn fair representations. Moreover, we utilize the mutual
information as a useful regularizer for enforcing fairness of a representation.
In experiments on two benchmark datasets and two scenarios where the sensitive
variables are fully and partially observable, we show that the proposed
approach either outperforms or performs on par with the current best model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Botros_P/0/1/0/all/0/1&quot;&gt;Philip Botros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tomczak_J/0/1/0/all/0/1&quot;&gt;Jakub M. Tomczak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10069">
<title>Deep $k$-Means: Jointly Clustering with $k$-Means and Learning Representations. (arXiv:1806.10069v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.10069</link>
<description rdf:parseType="Literal">&lt;p&gt;We study in this paper the problem of jointly clustering and learning
representations. As several previous studies have shown, learning
representations that are both faithful to the data to be clustered and adapted
to the clustering algorithm can lead to better clustering performance, all the
more so that the two tasks are performed jointly. We propose here such an
approach for $k$-Means clustering based on a continuous reparametrization of
the objective function that leads to a truly joint solution. The behavior of
our approach is illustrated on various datasets showing its efficacy in
learning representations for objects while clustering them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fard_M/0/1/0/all/0/1&quot;&gt;Maziar Moradi Fard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thonet_T/0/1/0/all/0/1&quot;&gt;Thibaut Thonet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaussier_E/0/1/0/all/0/1&quot;&gt;Eric Gaussier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10080">
<title>A Theory of Diagnostic Interpretation in Supervised Classification. (arXiv:1806.10080v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.10080</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretable deep learning is a fundamental building block towards safer AI,
especially when the deployment possibilities of deep learning-based
computer-aided medical diagnostic systems are so eminent. However, without a
computational formulation of black-box interpretation, general interpretability
research rely heavily on subjective bias. Clear decision structure of the
medical diagnostics lets us approximate the decision process of a radiologist
as a model - removed from subjective bias. We define the process of
interpretation as a finite communication between a known model and a black-box
model to optimally map the black box&apos;s decision process in the known model.
Consequently, we define interpretability as maximal information gain over the
initial uncertainty about the black-box&apos;s decision within finite communication.
We relax this definition based on the observation that diagnostic
interpretation is typically achieved by a process of minimal querying. We
derive an algorithm to calculate diagnostic interpretability. The usual
question of accuracy-interpretability tradeoff, i.e. whether a black-box
model&apos;s prediction accuracy is dependent on its ability to be interpreted by a
known source model, does not arise in this theory. With multiple example
simulation experiments of various complexity levels, we demonstrate the working
of such a theoretical model in synthetic supervised classification scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1&quot;&gt;Anirban Mukhopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1306.3862">
<title>Bayesian methods for low-rank matrix estimation: short survey and theoretical study. (arXiv:1306.3862v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1306.3862</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of low-rank matrix estimation recently received a lot of
attention due to challenging applications. A lot of work has been done on
rank-penalized methods and convex relaxation, both on the theoretical and
applied sides. However, only a few papers considered Bayesian estimation. In
this paper, we review the different type of priors considered on matrices to
favour low-rank. We also prove that the obtained Bayesian estimators, under
suitable assumptions, enjoys the same optimality properties as the ones based
on penalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alquier_P/0/1/0/all/0/1&quot;&gt;Pierre Alquier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.06348">
<title>Frank-Wolfe Optimization for Symmetric-NMF under Simplicial Constraint. (arXiv:1706.06348v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.06348</link>
<description rdf:parseType="Literal">&lt;p&gt;Symmetric nonnegative matrix factorization has found abundant applications in
various domains by providing a symmetric low-rank decomposition of nonnegative
matrices. In this paper we propose a Frank-Wolfe (FW) solver to optimize the
symmetric nonnegative matrix factorization problem under a simplicial
constraint, which has recently been proposed for probabilistic clustering.
Compared with existing solutions, this algorithm is simple to implement, and
has no hyperparameters to be tuned. Building on the recent advances of FW
algorithms in nonconvex optimization, we prove an $O(1/\varepsilon^2)$
convergence rate to $\varepsilon$-approximate KKT points, via a tight bound
$\Theta(n^2)$ on the curvature constant, which matches the best known result in
unconstrained nonconvex setting using gradient methods. Numerical results
demonstrate the effectiveness of our algorithm. As a side contribution, we
construct a simple nonsmooth convex problem where the FW algorithm fails to
converge to the optimum. This result raises an interesting question about
necessary conditions of the success of the FW algorithm on convex problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Han Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1&quot;&gt;Geoff Gordon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.07094">
<title>Constrained Bayesian Optimization with Noisy Experiments. (arXiv:1706.07094v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.07094</link>
<description rdf:parseType="Literal">&lt;p&gt;Randomized experiments are the gold standard for evaluating the effects of
changes to real-world systems. Data in these tests may be difficult to collect
and outcomes may have high variance, resulting in potentially large measurement
error. Bayesian optimization is a promising technique for efficiently
optimizing multiple continuous parameters, but existing approaches degrade in
performance when the noise level is high, limiting its applicability to many
randomized experiments. We derive an expression for expected improvement under
greedy batch optimization with noisy observations and noisy constraints, and
develop a quasi-Monte Carlo approximation that allows it to be efficiently
optimized. Simulations with synthetic functions show that optimization
performance on noisy, constrained problems outperforms existing methods. We
further demonstrate the effectiveness of the method with two real-world
experiments conducted at Facebook: optimizing a ranking system, and optimizing
server compiler flags.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Letham_B/0/1/0/all/0/1&quot;&gt;Benjamin Letham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karrer_B/0/1/0/all/0/1&quot;&gt;Brian Karrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ottoni_G/0/1/0/all/0/1&quot;&gt;Guilherme Ottoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bakshy_E/0/1/0/all/0/1&quot;&gt;Eytan Bakshy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.04788">
<title>BitNet: Bit-Regularized Deep Neural Networks. (arXiv:1708.04788v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.04788</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel optimization strategy for training neural networks which
we call &quot;BitNet&quot;. The parameters of neural networks are usually unconstrained
and have a dynamic range dispersed over all real values. Our key idea is to
limit the expressive power of the network by dynamically controlling the range
and set of values that the parameters can take. We formulate this idea using a
novel end-to-end approach that circumvents the discrete parameter space by
optimizing a relaxed continuous and differentiable upper bound of the typical
classification loss function. The approach can be interpreted as a
regularization inspired by the Minimum Description Length (MDL) principle. For
each layer of the network, our approach optimizes real-valued translation and
scaling factors and arbitrary precision integer-valued parameters (weights). We
empirically compare BitNet to an equivalent unregularized model on the MNIST
and CIFAR-10 datasets. We show that BitNet converges faster to a superior
quality solution. Additionally, the resulting model has significant savings in
memory due to the use of integer-valued parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghavan_A/0/1/0/all/0/1&quot;&gt;Aswin Raghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amer_M/0/1/0/all/0/1&quot;&gt;Mohamed Amer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_S/0/1/0/all/0/1&quot;&gt;Sek Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1&quot;&gt;Graham Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10277">
<title>A Stochastic Trust Region Algorithm Based on Careful Step Normalization. (arXiv:1712.10277v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1712.10277</link>
<description rdf:parseType="Literal">&lt;p&gt;An algorithm is proposed for solving stochastic and finite sum minimization
problems. Based on a trust region methodology, the algorithm employs normalized
steps, at least as long as the norms of the stochastic gradient estimates are
within a specified interval. The complete algorithm---which dynamically chooses
whether or not to employ normalized steps---is proved to have convergence
guarantees that are similar to those possessed by a traditional stochastic
gradient approach under various sets of conditions related to the accuracy of
the stochastic gradient estimates and choice of stepsize sequence. The results
of numerical experiments are presented when the method is employed to minimize
convex and nonconvex machine learning test problems. These results illustrate
that the method can outperform a traditional stochastic gradient approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Curtis_F/0/1/0/all/0/1&quot;&gt;Frank E. Curtis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Scheinberg_K/0/1/0/all/0/1&quot;&gt;Katya Scheinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shi_R/0/1/0/all/0/1&quot;&gt;Rui Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06293">
<title>Black-Box Reductions for Parameter-free Online Learning in Banach Spaces. (arXiv:1802.06293v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06293</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce several new black-box reductions that significantly improve the
design of adaptive and parameter-free online learning algorithms by simplifying
analysis, improving regret guarantees, and sometimes even improving runtime. We
reduce parameter-free online learning to online exp-concave optimization, we
reduce optimization in a Banach space to one-dimensional optimization, and we
reduce optimization over a constrained domain to unconstrained optimization.
All of our reductions run as fast as online gradient descent. We use our new
techniques to improve upon the previously best regret bounds for parameter-free
learning, and do so for arbitrary norms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1&quot;&gt;Ashok Cutkosky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orabona_F/0/1/0/all/0/1&quot;&gt;Francesco Orabona&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07513">
<title>Adversarial classification: An adversarial risk analysis approach. (arXiv:1802.07513v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07513</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification problems in security settings are usually contemplated as
confrontations in which one or more adversaries try to fool a classifier to
obtain a benefit. Most approaches to such adversarial classification problems
have focused on game theoretical ideas with strong underlying common knowledge
assumptions, which are actually not realistic in security domains. We provide
an alternative framework to such problem based on adversarial risk analysis,
which we illustrate with several examples. Computational and implementation
issues are discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Naveiro_R/0/1/0/all/0/1&quot;&gt;Roi Naveiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Redondo_A/0/1/0/all/0/1&quot;&gt;Alberto Redondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Insua_D/0/1/0/all/0/1&quot;&gt;David R&amp;#xed;os Insua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ruggeri_F/0/1/0/all/0/1&quot;&gt;Fabrizio Ruggeri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04640">
<title>Fast Counting in Machine Learning Applications. (arXiv:1804.04640v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04640</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose scalable methods to execute counting queries in machine learning
applications. To achieve memory and computational efficiency, we abstract
counting queries and their context such that the counts can be aggregated as a
stream. We demonstrate performance and scalability of the resulting approach on
random queries, and through extensive experimentation using Bayesian networks
learning and association rule mining. Our methods significantly outperform
commonly used ADtrees and hash tables, and are practical alternatives for
processing large-scale data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karan_S/0/1/0/all/0/1&quot;&gt;Subhadeep Karan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eichhorn_M/0/1/0/all/0/1&quot;&gt;Matthew Eichhorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hurlburt_B/0/1/0/all/0/1&quot;&gt;Blake Hurlburt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Iraci_G/0/1/0/all/0/1&quot;&gt;Grant Iraci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zola_J/0/1/0/all/0/1&quot;&gt;Jaroslaw Zola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06208">
<title>CDM: Compound dissimilarity measure and an application to fingerprinting-based positioning. (arXiv:1805.06208v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1805.06208</link>
<description rdf:parseType="Literal">&lt;p&gt;A non-vector-based dissimilarity measure is proposed by combining
vector-based distance metrics and set operations. This proposed compound
dissimilarity measure (CDM) is applicable to quantify similarity of collections
of attribute/feature pairs where not all attributes are present in all
collections. This is a typical challenge in the context of e.g.,
fingerprinting-based positioning (FbP). Compared to vector-based distance
metrics (e.g., Minkowski), the merits of the proposed CDM are i) the data do
not need to be converted to vectors of equal dimension, ii) shared and unshared
attributes can be weighted differently within the assessment, and iii)
additional degrees of freedom within the measure allow to adapt its properties
to application needs in a data-driven way. We indicate the validity of the
proposed CDM by demonstrating the improvements of the positioning performance
of fingerprinting-based WLAN indoor positioning using four different datasets,
three of them publicly available. When processing these datasets using CDM
instead of conventional distance metrics the accuracy of identifying buildings
and floors improves by about 5% on average. The 2d positioning errors in terms
of root mean squared error (RMSE) are reduced by a factor of two, and the
percentage of position solutions with less than 2m error improves by over 10%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Caifa Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wieser_A/0/1/0/all/0/1&quot;&gt;Andreas Wieser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02336">
<title>Spatial Frequency Loss for Learning Convolutional Autoencoders. (arXiv:1806.02336v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1806.02336</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a learning method for convolutional autoencoders (CAEs)
for extracting features from images. CAEs can be obtained by utilizing
convolutional neural networks to learn an approximation to the identity
function in an unsupervised manner. The loss function based on the pixel loss
(PL) that is the mean squared error between the pixel values of original and
reconstructed images is the common choice for learning. However, using the loss
function leads to blurred reconstructed images. A method for learning CAEs
using a loss function computed from features reflecting spatial frequencies is
proposed to mitigate the problem. The blurs in reconstructed images show lack
of high spatial frequency components mainly constituting edges and detailed
textures that are important features for tasks such as object detection and
spatial matching. In order to evaluate the lack of components, a convolutional
layer with a Laplacian filter bank as weights is added to CAEs and the mean
squared error of features in a subband, called the spatial frequency loss
(SFL), is computed from the outputs of each filter. The learning is performed
using a loss function based on the SFL. Empirical evaluation demonstrates that
using the SFL reduces the blurs in reconstructed images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_N/0/1/0/all/0/1&quot;&gt;Naoyuki Ichimura&lt;/a&gt;</dc:creator>
</item></rdf:RDF>