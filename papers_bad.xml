<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-31T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10472"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10492"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.01905"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00727"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10186"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10287"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10291"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10437"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10459"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10495"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10545"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1512.08899"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.04046"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11223"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07632"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10199"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10242"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10395"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10562"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10578"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10579"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1602.04474"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1603.02644"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.10958"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.07657"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10468"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11379"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06446"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09819"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.10472">
<title>Soft Computing Techniques for Dependable Cyber-Physical Systems. (arXiv:1801.10472v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1801.10472</link>
<description rdf:parseType="Literal">&lt;p&gt;Cyber-Physical Systems (CPS) allow us to manipulate objects in the physical
world by providing a communication bridge between computation and actuation
elements. In the current scheme of things, this sought-after control is marred
by limitations inherent in the underlying communication network(s) as well as
by the uncertainty found in the physical world. These limitations hamper
fine-grained control of elements that may be separated by large-scale
distances. In this regard, soft computing is an emerging paradigm that can help
to overcome the vulnerabilities, and unreliability of CPS by using techniques
including fuzzy systems, neural network, evolutionary computation,
probabilistic reasoning and rough sets. In this paper, we present a
comprehensive contemporary review of soft computing techniques for CPS
dependability modeling, analysis, and improvement. This paper provides an
overview of CPS applications, explores the foundations of dependability
engineering, and highlights the potential role of soft computing techniques for
CPS dependability with various case studies, while identifying common pitfalls
and future directions. In addition, this paper provides a comprehensive survey
on the use of various soft computing techniques for making CPS dependable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atif_M/0/1/0/all/0/1&quot;&gt;Muhammad Atif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latif_S/0/1/0/all/0/1&quot;&gt;Siddique Latif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_R/0/1/0/all/0/1&quot;&gt;Rizwan Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiani_A/0/1/0/all/0/1&quot;&gt;Adnan Khalid Kiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qadir_J/0/1/0/all/0/1&quot;&gt;Junaid Qadir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baig_A/0/1/0/all/0/1&quot;&gt;Adeel Baig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishibuchi_H/0/1/0/all/0/1&quot;&gt;Hisao Ishibuchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbas_W/0/1/0/all/0/1&quot;&gt;Waseem Abbas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10492">
<title>Deep Predictive Models in Interactive Music. (arXiv:1801.10492v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1801.10492</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic music generation is a compelling task where much recent progress
has been made with deep learning models. In this paper, we ask how these models
can be integrated into interactive music systems; how can they encourage or
enhance the music making of human users? Musical performance requires
prediction to operate instruments, and perform in groups. We argue that
predictive models could help interactive systems to understand their temporal
context, and ensemble behaviour. Deep learning can allow data-driven models
with a long memory of past states.
&lt;/p&gt;
&lt;p&gt;We advocate for predictive musical interaction, where a predictive model is
embedded in a musical interface, assisting users by predicting unknown states
of musical processes. We propose a framework for incorporating such predictive
models into the sensing, processing, and result architecture that is often used
in musical interface design. We show that our framework accommodates deep
generative models, as well as models for predicting gestural states, or other
high-level musical information. We motivate the framework with two examples
from our recent work, as well as systems from the literature, and suggest
musical use-cases where prediction is a necessary component.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_C/0/1/0/all/0/1&quot;&gt;Charles P. Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellefsen_K/0/1/0/all/0/1&quot;&gt;Kai Olav Ellefsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torresen_J/0/1/0/all/0/1&quot;&gt;Jim Torresen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.01905">
<title>Parameter Space Noise for Exploration. (arXiv:1706.01905v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.01905</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning (RL) methods generally engage in exploratory
behavior through noise injection in the action space. An alternative is to add
noise directly to the agent&apos;s parameters, which can lead to more consistent
exploration and a richer set of behaviors. Methods such as evolutionary
strategies use parameter perturbations, but discard all temporal structure in
the process and require significantly more samples. Combining parameter noise
with traditional RL methods allows to combine the best of both worlds. We
demonstrate that both off- and on-policy methods benefit from this approach
through experimental comparison of DQN, DDPG, and TRPO on high-dimensional
discrete action environments as well as continuous control tasks. Our results
show that RL with parameter noise learns more efficiently than traditional RL
with action space noise and evolutionary strategies individually.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plappert_M/0/1/0/all/0/1&quot;&gt;Matthias Plappert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houthooft_R/0/1/0/all/0/1&quot;&gt;Rein Houthooft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhariwal_P/0/1/0/all/0/1&quot;&gt;Prafulla Dhariwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sidor_S/0/1/0/all/0/1&quot;&gt;Szymon Sidor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Richard Y. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asfour_T/0/1/0/all/0/1&quot;&gt;Tamim Asfour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrychowicz_M/0/1/0/all/0/1&quot;&gt;Marcin Andrychowicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00727">
<title>Performance Evaluation of Channel Decoding With Deep Neural Networks. (arXiv:1711.00727v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00727</link>
<description rdf:parseType="Literal">&lt;p&gt;With the demand of high data rate and low latency in fifth generation (5G),
deep neural network decoder (NND) has become a promising candidate due to its
capability of one-shot decoding and parallel computing. In this paper, three
types of NND, i.e., multi-layer perceptron (MLP), convolution neural network
(CNN) and recurrent neural network (RNN), are proposed with the same parameter
magnitude. The performance of these deep neural networks are evaluated through
extensive simulation. Numerical results show that RNN has the best decoding
performance, yet at the price of the highest computational overhead. Moreover,
we find there exists a saturation length for each type of neural network, which
is caused by their restricted learning abilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyu_W/0/1/0/all/0/1&quot;&gt;Wei Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiao_C/0/1/0/all/0/1&quot;&gt;Chunxu Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qin_K/0/1/0/all/0/1&quot;&gt;Kangjian Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huazi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10186">
<title>A Rational Distributed Process-level Account of Independence Judgment. (arXiv:1801.10186v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.10186</link>
<description rdf:parseType="Literal">&lt;p&gt;It is inconceivable how chaotic the world would look to humans, faced with
innumerable decisions a day to be made under uncertainty, had they been lacking
the capacity to distinguish the relevant from the irrelevant---a capacity which
computationally amounts to handling probabilistic independence relations. The
highly parallel and distributed computational machinery of the brain suggests
that a satisfying process-level account of human independence judgment should
also mimic these features. In this work, we present the first rational,
distributed, message-passing, process-level account of independence judgment,
called $\mathcal{D}^\ast$. Interestingly, $\mathcal{D}^\ast$ shows a curious,
but normatively-justified tendency for quick detection of dependencies,
whenever they hold. Furthermore, $\mathcal{D}^\ast$ outperforms all the
previously proposed algorithms in the AI literature in terms of worst-case
running time, and a salient aspect of it is supported by recent work in
neuroscience investigating possible implementations of Bayes nets at the neural
level. $\mathcal{D}^\ast$ nicely exemplifies how the pursuit of cognitive
plausibility can lead to the discovery of state-of-the-art algorithms with
appealing properties, and its simplicity makes $\mathcal{D}^\ast$ potentially a
good candidate for pedagogical purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nobandegani_A/0/1/0/all/0/1&quot;&gt;Ardavan S. Nobandegani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Psaromiligkos_I/0/1/0/all/0/1&quot;&gt;Ioannis N. Psaromiligkos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10287">
<title>An Incremental Off-policy Search in a Model-free Markov Decision Process Using a Single Sample Path. (arXiv:1801.10287v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.10287</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider a modified version of the control problem in a
model free Markov decision process (MDP) setting with large state and action
spaces. The control problem most commonly addressed in the contemporary
literature is to find an optimal policy which maximizes the value function,
i.e., the long run discounted reward of the MDP. The current settings also
assume access to a generative model of the MDP with the hidden premise that
observations of the system behaviour in the form of sample trajectories can be
obtained with ease from the model. In this paper, we consider a modified
version, where the cost function is the expectation of a non-convex function of
the value function without access to the generative model. Rather, we assume
that a sample trajectory generated using a priori chosen behaviour policy is
made available. In this restricted setting, we solve the modified control
problem in its true sense, i.e., to find the best possible policy given this
limited information. We propose a stochastic approximation algorithm based on
the well-known cross entropy method which is data (sample trajectory)
efficient, stable, robust as well as computationally and storage efficient. We
provide a proof of convergence of our algorithm to a policy which is globally
optimal relative to the behaviour policy. We also present experimental results
to corroborate our claims and we demonstrate the superiority of the solution
produced by our algorithm compared to the state-of-the-art algorithms under
appropriately chosen behaviour policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joseph_A/0/1/0/all/0/1&quot;&gt;Ajin George Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatnagar_S/0/1/0/all/0/1&quot;&gt;Shalabh Bhatnagar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10291">
<title>A Cross Entropy based Optimization Algorithm with Global Convergence Guarantees. (arXiv:1801.10291v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.10291</link>
<description rdf:parseType="Literal">&lt;p&gt;The cross entropy (CE) method is a model based search method to solve
optimization problems where the objective function has minimal structure. The
Monte-Carlo version of the CE method employs the naive sample averaging
technique which is inefficient, both computationally and space wise. We provide
a novel stochastic approximation version of the CE method, where the sample
averaging is replaced with incremental geometric averaging. This approach can
save considerable computational and storage costs. Our algorithm is incremental
in nature and possesses additional attractive features such as accuracy,
stability, robustness and convergence to the global optimum for a particular
class of objective functions. We evaluate the algorithm on a variety of global
optimization benchmark problems and the results obtained corroborate our
theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joseph_A/0/1/0/all/0/1&quot;&gt;Ajin George Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatnagar_S/0/1/0/all/0/1&quot;&gt;Shalabh Bhatnagar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10437">
<title>Deep Learning Works in Practice. But Does it Work in Theory?. (arXiv:1801.10437v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.10437</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning relies on a very specific kind of neural networks: those
superposing several neural layers. In the last few years, deep learning
achieved major breakthroughs in many tasks such as image analysis, speech
recognition, natural language processing, and so on. Yet, there is no
theoretical explanation of this success. In particular, it is not clear why the
deeper the network, the better it actually performs.
&lt;/p&gt;
&lt;p&gt;We argue that the explanation is intimately connected to a key feature of the
data collected from our surrounding universe to feed the machine learning
algorithms: large non-parallelizable logical depth. Roughly speaking, we
conjecture that the shortest computational descriptions of the universe are
algorithms with inherently large computation times, even when a large number of
computers are available for parallelization. Interestingly, this conjecture,
combined with the folklore conjecture in theoretical computer science that $ P
\neq NC$, explains the success of deep learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1&quot;&gt;L&amp;#xea; Nguy&amp;#xea;n Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1&quot;&gt;Rachid Guerraoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10459">
<title>Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With Expert Demonstrations. (arXiv:1801.10459v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.10459</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretraining with expert demonstrations have been found useful in speeding up
the training process of deep reinforcement learning algorithms since less
online simulation data is required. Some people use supervised learning to
speed up the process of feature learning, others pretrain the policies by
imitating expert demonstrations. However, these methods are unstable and not
suitable for actor-critic reinforcement learning algorithms. Also, some
existing methods rely on the global optimum assumption, which is not true in
most scenarios. In this paper, we employ expert demonstrations in a
actor-critic reinforcement learning framework, and meanwhile ensure that the
performance is not affected by the fact that expert demonstrations are not
global optimal. We theoretically derive a method for computing policy gradients
and value estimators with only expert demonstrations. Our method is
theoretically plausible for actor-critic reinforcement learning algorithms that
pretrains both policy and value functions. We apply our method to two of the
typical actor-critic reinforcement learning algorithms, DDPG and ACER, and
demonstrate with experiments that our method not only outperforms the RL
algorithms without pretraining process, but also is more simulation efficient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huimin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10495">
<title>Lifted Filtering via Exchangeable Decomposition. (arXiv:1801.10495v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.10495</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a model for recursive Bayesian filtering based on lifted multiset
states. Combining multisets with lifting makes it possible to simultaneously
exploit multiple strategies for reducing inference complexity when compared to
list-based grounded state representations. The core idea is to borrow the
concept of Maximally Parallel Multiset Rewriting Systems and to enhance it by
concepts from Rao-Blackwellisation and Lifted Inference, giving a
representation of state distributions that enables efficient inference. In
worlds where the random variables that define the system state are exchangeable
- where the identity of entities does not matter - it automatically uses a
representation that abstracts from ordering (achieving an exponential reduction
in complexity) and it automatically adapts when observations or system dynamics
destroy exchangeability by breaking symmetry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ludtke_S/0/1/0/all/0/1&quot;&gt;Stefan L&amp;#xfc;dtke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroder_M/0/1/0/all/0/1&quot;&gt;Max Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bader_S/0/1/0/all/0/1&quot;&gt;Sebastian Bader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirste_T/0/1/0/all/0/1&quot;&gt;Thomas Kirste&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10545">
<title>A family of OWA operators based on Faulhaber&apos;s formulas. (arXiv:1801.10545v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.10545</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we develop a new family of Ordered Weighted Averaging (OWA)
operators. Weight vector is obtained from a desired orness of the operator.
Using Faulhaber&apos;s formulas we obtain direct and simple expressions for the
weight vector without any iteration loop. With the exception of one weight, the
remaining follow a straight line relation. As a result, a fast and robust
algorithm is developed. The resulting weight vector is suboptimal according
with the Maximum Entropy criterion, but it is very close to the optimal.
Comparisons are done with other procedures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duarte_O/0/1/0/all/0/1&quot;&gt;Oscar Duarte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tellez_S/0/1/0/all/0/1&quot;&gt;Sandra T&amp;#xe9;llez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1512.08899">
<title>Modeling Variations of First-Order Horn Abduction in Answer Set Programming. (arXiv:1512.08899v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1512.08899</link>
<description rdf:parseType="Literal">&lt;p&gt;We study abduction in First Order Horn logic theories where all atoms can be
abduced and we are looking for preferred solutions with respect to three
objective functions: cardinality minimality, coherence, and weighted abduction.
We represent this reasoning problem in Answer Set Programming (ASP), in order
to obtain a flexible framework for experimenting with global constraints and
objective functions, and to test the boundaries of what is possible with ASP.
Realizing this problem in ASP is challenging as it requires value invention and
equivalence between certain constants, because the Unique Names Assumption does
not hold in general. To permit reasoning in cyclic theories, we formally
describe fine-grained variations of limiting Skolemization. We identify term
equivalence as a main instantiation bottleneck, and improve the efficiency of
our approach with on-demand constraints that were used to eliminate the same
bottleneck in state-of-the-art solvers. We evaluate our approach experimentally
on the ACCEL benchmark for plan recognition in Natural Language Understanding.
Our encodings are publicly available, modular, and our approach is more
efficient than state-of-the-art solvers on the ACCEL benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuller_P/0/1/0/all/0/1&quot;&gt;Peter Sch&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.04046">
<title>Stable Distribution Alignment Using the Dual of the Adversarial Distance. (arXiv:1707.04046v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1707.04046</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods that align distributions by minimizing an adversarial distance
between them have recently achieved impressive results. However, these
approaches are difficult to optimize with gradient descent and they often do
not converge well without careful hyperparameter tuning and proper
initialization. We investigate whether turning the adversarial min-max problem
into an optimization problem by replacing the maximization part with its dual
improves the quality of the resulting alignment and explore its connections to
Maximum Mean Discrepancy. Our empirical results suggest that using the dual
formulation for the restricted family of linear discriminators results in a
more stable convergence to a desirable solution when compared with the
performance of a primal min-max GAN-like objective and an MMD objective under
the same restrictions. We test our hypothesis on the problem of aligning two
synthetic point clouds on a plane and on a real-image domain adaptation problem
on digits. In both cases, the dual formulation yields an iterative procedure
that gives more stable and monotonic improvement over time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usman_B/0/1/0/all/0/1&quot;&gt;Ben Usman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1&quot;&gt;Kate Saenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulis_B/0/1/0/all/0/1&quot;&gt;Brian Kulis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11223">
<title>Fast and Scalable Learning of Sparse Changes in High-Dimensional Gaussian Graphical Model Structure. (arXiv:1710.11223v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11223</link>
<description rdf:parseType="Literal">&lt;p&gt;We focus on the problem of estimating the change in the dependency structures
of two $p$-dimensional Gaussian Graphical models (GGMs). Previous studies for
sparse change estimation in GGMs involve expensive and difficult non-smooth
optimization. We propose a novel method, DIFFEE for estimating DIFFerential
networks via an Elementary Estimator under a high-dimensional situation. DIFFEE
is solved through a faster and closed form solution that enables it to work in
large-scale settings. We conduct a rigorous statistical analysis showing that
surprisingly DIFFEE achieves the same asymptotic convergence rates as the
state-of-the-art estimators that are much more difficult to compute. Our
experimental results on multiple synthetic datasets and one real-world data
about brain connectivity show strong performance improvements over baselines,
as well as significant computational benefits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Beilun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1&quot;&gt;Arshdeep Sekhon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yanjun Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07632">
<title>Generating Thematic Chinese Poetry using Conditional Variational Autoencoders with Hybrid Decoders. (arXiv:1711.07632v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07632</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer poetry generation is our first step towards computer writing.
Writing must have a theme. The current approaches of using sequence-to-sequence
models with attention often produce non-thematic poems. We present a novel
conditional variational autoencoder with a hybrid decoder adding the
deconvolutional neural networks to the general recurrent neural networks to
fully learn topic information via latent variables. This approach significantly
improves the relevance of the generated poems by representing each line of the
poem not only in a context-sensitive manner but also in a holistic way that is
highly related to the given keyword and the learned topic. A proposed augmented
word2vec model further improves the rhythm and symmetry. Tests show that the
generated poems by our approach are mostly satisfying with regulated rules and
consistent themes, and 73.42% of them receive an Overall score no less than 3
(the highest score is 5).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiaowen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suo_S/0/1/0/all/0/1&quot;&gt;Shunda Suo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09744">
<title>How linguistic descriptions of data can help to the teaching-learning process in higher education, case of study: artificial intelligence. (arXiv:1711.09744v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09744</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence is a central topic in the computer science
curriculum. From the year 2011 a project-based learning methodology based on
computer games has been designed and implemented into the intelligence
artificial course at the University of the Bio-Bio. The project aims to develop
software-controlled agents (bots) which are programmed by using heuristic
algorithms seen during the course. This methodology allows us to obtain good
learning results, however several challenges have been founded during its
implementation.
&lt;/p&gt;
&lt;p&gt;In this paper we show how linguistic descriptions of data can help to provide
students and teachers with technical and personalized feedback about the
learned algorithms. Algorithm behavior profile and a new Turing test for
computer games bots based on linguistic modelling of complex phenomena are also
proposed in order to deal with such challenges.
&lt;/p&gt;
&lt;p&gt;In order to show and explore the possibilities of this new technology, a web
platform has been designed and implemented by one of authors and its
incorporation in the process of assessment allows us to improve the teaching
learning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubio_Manzano_C/0/1/0/all/0/1&quot;&gt;Clemente Rubio-Manzano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senoceain_T/0/1/0/all/0/1&quot;&gt;Tomas Lermanda Senoceain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10193">
<title>DeepDTA: Deep Drug-Target Binding Affinity Prediction. (arXiv:1801.10193v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.10193</link>
<description rdf:parseType="Literal">&lt;p&gt;The identification of novel drug-target (DT) interactions is a substantial
part of the drug discovery process. Most of the computational methods that have
been proposed to predict DT interactions have focused on binary classification,
where the goal is to determine whether a DT pair interacts or not. However,
protein-ligand interactions assume a continuum of binding strength values, also
called binding affinity and predicting this value still remains a challenge.
The increase in the affinity data available in DT knowledge-bases allow the use
of advanced learning techniques such as deep learning architectures in the
prediction of binding affinities. In this study, we propose a deep-learning
based model that uses only sequence information of both targets and drugs to
predict DT interaction binding affinities. The few studies that focus on DT
binding affinity prediction either use 3D structure of protein-ligand complexes
or 2D features of compounds. One novel approach used in this work is the
modeling of protein sequences and compound 1D representations with
convolutional neural networks (CNNs). The results show that the proposed deep
learning based model that uses the 1D representations of targets and drugs is
an effective approach for drug target binding affinity prediction. The model in
which a high-level representation of a drug is constructed via CNNs and
Smith-Waterman similarity is used for proteins achieved the best Concordance
Index (CI) performance, outperforming KronRLS, a state-of-the-art algorithm for
DT binding affinity prediction, with statistical significance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ozturk_H/0/1/0/all/0/1&quot;&gt;Hakime &amp;#xd6;zt&amp;#xfc;rk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ozkirimli_E/0/1/0/all/0/1&quot;&gt;Elif Ozkirimli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ozgur_A/0/1/0/all/0/1&quot;&gt;Arzucan &amp;#xd6;zg&amp;#xfc;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10199">
<title>A novel methodology on distributed representations of proteins using their interacting ligands. (arXiv:1801.10199v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.10199</link>
<description rdf:parseType="Literal">&lt;p&gt;The effective representation of proteins is a crucial task that directly
affects the performance of many bioinformatics problems. Related proteins
usually bind to similar ligands. Chemical characteristics of ligands are known
to capture the functional and mechanistic properties of proteins suggesting
that a ligand based approach can be utilized in protein representation. In this
study, we propose SMILESVec, a SMILES-based method to represent ligands and a
novel method to compute similarity of proteins by describing them based on
their ligands. The proteins are defined utilizing the word-embeddings of the
SMILES strings of their ligands. The performance of the proposed protein
description method is evaluated in protein clustering task using TransClust and
MCL algorithms. Two other protein representation methods that utilize protein
sequence, BLAST and ProtVec, and two compound fingerprint based protein
representation methods are compared. We showed that ligand-based protein
representation, which uses only SMILES strings of the ligands that proteins
bind to, performs as well as protein-sequence based representation methods in
protein clustering. The results suggest that ligand-based protein description
can be an alternative to the traditional sequence or structure based
representation of proteins and this novel approach can be applied to different
bioinformatics problems such as prediction of new protein-ligand interactions
and protein function annotation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ozturk_H/0/1/0/all/0/1&quot;&gt;Hakime &amp;#xd6;zt&amp;#xfc;rk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ozkirimli_E/0/1/0/all/0/1&quot;&gt;Elif Ozkirimli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ozgur_A/0/1/0/all/0/1&quot;&gt;Arzucan &amp;#xd6;zg&amp;#xfc;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10242">
<title>Low-rank Bandit Methods for High-dimensional Dynamic Pricing. (arXiv:1801.10242v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.10242</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider high dimensional dynamic multi-product pricing with an evolving
but low-dimensional linear demand model. Assuming the temporal variation in
cross-elasticities exhibits low-rank structure based on fixed (latent) features
of the products, we show that the revenue maximization problem reduces to an
online bandit convex optimization with side information given by the observed
demands. We design dynamic pricing algorithms whose revenue approaches that of
the best fixed price vector in hindsight, at a rate that only depends on the
intrinsic rank of the demand model and not the number of products. Our approach
applies a bandit convex optimization algorithm in a projected low-dimensional
space spanned by the latent product features, while simultaneously learning
this span via online singular value decomposition of a carefully-crafted matrix
containing the observed demands.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1&quot;&gt;Jonas Mueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taddy_M/0/1/0/all/0/1&quot;&gt;Matt Taddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10395">
<title>Probabilistic Recurrent State-Space Models. (arXiv:1801.10395v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.10395</link>
<description rdf:parseType="Literal">&lt;p&gt;State-space models (SSMs) are a highly expressive model class for learning
patterns in time series data and for system identification. Deterministic
versions of SSMs (e.g., LSTMs) proved extremely successful in modeling complex
time-series data. Fully probabilistic SSMs, however, unfortunately often prove
hard to train, even for smaller problems. To overcome this limitation, we
propose a scalable initialization and training algorithm based on doubly
stochastic variational inference and Gaussian processes. In the variational
approximation we propose in contrast to related approaches to fully capture the
latent state temporal correlations to allow for robust training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Doerr_A/0/1/0/all/0/1&quot;&gt;Andreas Doerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Daniel_C/0/1/0/all/0/1&quot;&gt;Christian Daniel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schiegg_M/0/1/0/all/0/1&quot;&gt;Martin Schiegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nguyen_Tuong_D/0/1/0/all/0/1&quot;&gt;Duy Nguyen-Tuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schaal_S/0/1/0/all/0/1&quot;&gt;Stefan Schaal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Toussaint_M/0/1/0/all/0/1&quot;&gt;Marc Toussaint&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trimpe_S/0/1/0/all/0/1&quot;&gt;Sebastian Trimpe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10562">
<title>Feature Decomposition Based Saliency Detection in Electron Cryo-Tomograms. (arXiv:1801.10562v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1801.10562</link>
<description rdf:parseType="Literal">&lt;p&gt;Electron Cryo-Tomography (ECT) allows 3D visualization of subcellular
structures at the submolecular resolution in close to the native state.
However, due to the high degree of structural complexity and imaging limits,
the automatic segmentation of cellular components from ECT images is very
difficult. To complement and speed up existing segmentation methods, it is
desirable to develop a generic cell component segmentation method that is 1)
not specific to particular types of cellular components, 2) able to segment
unknown cellular components, 3) fully unsupervised and does not rely on the
availability of training data. As an important step towards this goal, in this
paper, we propose a saliency detection method that computes the likelihood that
a subregion in a tomogram stands out from the background. Our method consists
of four steps: supervoxel over-segmentation, feature extraction, feature matrix
decomposition, and computation of saliency. The method produces a distribution
map that represents the regions&apos; saliency in tomograms. Our experiments show
that our method can successfully label most salient regions detected by a human
observer, and able to filter out regions not containing cellular components.
Therefore, our method can remove the majority of the background region, and
significantly speed up the subsequent processing of segmentation and
recognition of cellular components captured by ECT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiangrui Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Min Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10578">
<title>Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach. (arXiv:1801.10578v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.10578</link>
<description rdf:parseType="Literal">&lt;p&gt;The robustness of neural networks to adversarial examples has received great
attention due to security implications. Despite various attack approaches to
crafting visually imperceptible adversarial examples, little has been developed
towards a comprehensive measure of robustness. In this paper, we provide a
theoretical justification for converting robustness analysis into a local
Lipschitz constant estimation problem, and propose to use the Extreme Value
Theory for efficient evaluation. Our analysis yields a novel robustness metric
called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork
Robustness. The proposed CLEVER score is attack-agnostic and computationally
feasible for large neural networks. Experimental results on various networks,
including ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned
with the robustness indication measured by the $\ell_2$ and $\ell_\infty$ norms
of adversarial examples from powerful attacks, and (ii) defended networks using
defensive distillation or bounded ReLU indeed achieve better CLEVER scores. To
the best of our knowledge, CLEVER is the first attack-independent robustness
metric that can be applied to any neural network classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weng_T/0/1/0/all/0/1&quot;&gt;Tsui-Wei Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Su_D/0/1/0/all/0/1&quot;&gt;Dong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yupeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Daniel_L/0/1/0/all/0/1&quot;&gt;Luca Daniel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10579">
<title>Nonparametric Quantile-Based Causal Discovery. (arXiv:1801.10579v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.10579</link>
<description rdf:parseType="Literal">&lt;p&gt;Telling cause from effect using observational data is a challenging problem,
especially in the bivariate case. Contemporary methods often assume an
independence between the cause and the generating mechanism of the effect given
the cause. From this postulate, they derive asymmetries to uncover causal
relationships. In this work, we propose such an approach, based on the link
between Kolmogorov complexity and quantile scoring. We use a nonparametric
conditional quantile estimator based on copulas to implement our procedure,
thus avoiding restrictive assumptions about the joint distribution between
cause and effect. In an extensive study on real and synthetic data, we show
that quantile copula causal discovery (QCCD) compares favorably to
state-of-the-art methods, while at the same time being computationally
efficient and scalable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tagasovska_N/0/1/0/all/0/1&quot;&gt;Natasa Tagasovska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vatter_T/0/1/0/all/0/1&quot;&gt;Thibault Vatter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chavez_Demoulin_V/0/1/0/all/0/1&quot;&gt;Val&amp;#xe9;rie Chavez-Demoulin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1602.04474">
<title>Generalization Properties of Learning with Random Features. (arXiv:1602.04474v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1602.04474</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the generalization properties of ridge regression with random
features in the statistical learning framework. We show for the first time that
$O(1/\sqrt{n})$ learning bounds can be achieved with only $O(\sqrt{n}\log n)$
random features rather than $O({n})$ as suggested by previous results. Further,
we prove faster learning rates and show that they might require more random
features, unless they are sampled according to a possibly problem dependent
distribution. Our results shed light on the statistical computational
trade-offs in large scale kernelized learning, showing the potential
effectiveness of random features in reducing the computational complexity while
keeping optimal generalization properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1&quot;&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1&quot;&gt;Lorenzo Rosasco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1603.02644">
<title>Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling. (arXiv:1603.02644v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1603.02644</link>
<description rdf:parseType="Literal">&lt;p&gt;We study parameter inference in large-scale latent variable models. We first
propose an unified treatment of online inference for latent variable models
from a non-canonical exponential family, and draw explicit links between
several previously proposed frequentist or Bayesian methods. We then propose a
novel inference method for the frequentist estimation of parameters, that
adapts MCMC methods to online inference of latent variable models with the
proper use of local Gibbs sampling. Then, for latent Dirich-let allocation,we
provide an extensive set of experiments and comparisons with existing work,
where our new approach outperforms all previously proposed methods. In
particular, using Gibbs sampling for latent variable inference is superior to
variational inference in terms of test log-likelihoods. Moreover, Bayesian
inference through variational methods perform poorly, sometimes leading to
worse fits with latent variables of higher dimensionality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1&quot;&gt;Christophe Dupuy&lt;/a&gt; (SIERRA), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis Bach&lt;/a&gt; (LIENS, SIERRA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.10958">
<title>FALKON: An Optimal Large Scale Kernel Method. (arXiv:1705.10958v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.10958</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel methods provide a principled way to perform non linear, nonparametric
learning. They rely on solid functional analytic foundations and enjoy optimal
statistical properties. However, at least in their basic form, they have
limited applicability in large scale scenarios because of stringent
computational requirements in terms of time and especially memory. In this
paper, we take a substantial step in scaling up kernel methods, proposing
FALKON, a novel algorithm that allows to efficiently process millions of
points. FALKON is derived combining several algorithmic principles, namely
stochastic subsampling, iterative solvers and preconditioning. Our theoretical
analysis shows that optimal statistical accuracy is achieved requiring
essentially $O(n)$ memory and $O(n\sqrt{n})$ time. An extensive experimental
analysis on large scale datasets shows that, even with a single machine, FALKON
outperforms previous state of the art solutions, which exploit
parallel/distributed architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1&quot;&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carratino_L/0/1/0/all/0/1&quot;&gt;Luigi Carratino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1&quot;&gt;Lorenzo Rosasco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.07657">
<title>Engineering fast multilevel support vector machines. (arXiv:1707.07657v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1707.07657</link>
<description rdf:parseType="Literal">&lt;p&gt;The computational complexity of solving nonlinear support vector machine
(SVM) is prohibitive on large-scale data. In particular, this issue becomes
very sensitive when the data represents additional difficulties such as highly
imbalanced class sizes. Typically, nonlinear kernels produce significantly
higher classification quality to linear kernels but introduce extra kernel and
model parameters which computationally expensive fitting. This increases the
quality but also reduces the performance dramatically. We introduce a
generalized fast multilevel framework for regular and weighted SVM and discuss
several versions of its algorithmic components that lead to a good trade-off
between quality and time. Our framework is implemented using PETSc which allows
an easy integration with scientific computing tasks. The experimental results
demonstrate significant speedup compared to the state-of-the-art nonlinear SVM
libraries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadrfaridpour_E/0/1/0/all/0/1&quot;&gt;E. Sadrfaridpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razzaghi_T/0/1/0/all/0/1&quot;&gt;T. Razzaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safro_I/0/1/0/all/0/1&quot;&gt;I. Safro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10468">
<title>Speaker Diarization with LSTM. (arXiv:1710.10468v5 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10468</link>
<description rdf:parseType="Literal">&lt;p&gt;For many years, i-vector based audio embedding techniques were the dominant
approach for speaker verification and speaker diarization applications.
However, mirroring the rise of deep learning in various domains, neural network
based audio embeddings, also known as d-vectors, have consistently demonstrated
superior speaker verification performance. In this paper, we build on the
success of d-vector based speaker verification systems to develop a new
d-vector based approach to speaker diarization. Specifically, we combine
LSTM-based d-vector audio embeddings with recent work in non-parametric
clustering to obtain a state-of-the-art speaker diarization system. Our system
is evaluated on three standard public datasets, suggesting that d-vector based
diarization systems offer significant advantages over traditional i-vector
based systems. We achieved a 12.0% diarization error rate on NIST SRE 2000
CALLHOME, while our model is trained with out-of-domain data from voice search
logs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Downey_C/0/1/0/all/0/1&quot;&gt;Carlton Downey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wan_L/0/1/0/all/0/1&quot;&gt;Li Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mansfield_P/0/1/0/all/0/1&quot;&gt;Philip Andrew Mansfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1&quot;&gt;Ignacio Lopez Moreno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11379">
<title>Latent Space Oddity: on the Curvature of Deep Generative Models. (arXiv:1710.11379v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11379</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models provide a systematic way to learn nonlinear data
distributions, through a set of latent variables and a nonlinear &quot;generator&quot;
function that maps latent points into the input space. The nonlinearity of the
generator imply that the latent space gives a distorted view of the input
space. Under mild conditions, we show that this distortion can be characterized
by a stochastic Riemannian metric, and demonstrate that distances and
interpolants are significantly improved under this metric. This in turn
improves probability distributions, sampling algorithms and clustering in the
latent space. Our geometric analysis further reveals that current generators
provide poor variance estimates and we propose a new generator architecture
with vastly improved variance estimates. Results are demonstrated on
convolutional and fully connected variational autoencoders, but the formalism
easily generalize to other deep generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arvanitidis_G/0/1/0/all/0/1&quot;&gt;Georgios Arvanitidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hansen_L/0/1/0/all/0/1&quot;&gt;Lars Kai Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hauberg_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06446">
<title>Stochastic Non-convex Ordinal Embedding with Stabilized Barzilai-Borwein Step Size. (arXiv:1711.06446v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06446</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning representation from relative similarity comparisons, often called
ordinal embedding, gains rising attention in recent years. Most of the existing
methods are batch methods designed mainly based on the convex optimization,
say, the projected gradient descent method. However, they are generally
time-consuming due to that the singular value decomposition (SVD) is commonly
adopted during the update, especially when the data size is very large. To
overcome this challenge, we propose a stochastic algorithm called SVRG-SBB,
which has the following features: (a) SVD-free via dropping convexity, with
good scalability by the use of stochastic algorithm, i.e., stochastic variance
reduced gradient (SVRG), and (b) adaptive step size choice via introducing a
new stabilized Barzilai-Borwein (SBB) method as the original version for convex
problems might fail for the considered stochastic \textit{non-convex}
optimization problem. Moreover, we show that the proposed algorithm converges
to a stationary point at a rate $\mathcal{O}(\frac{1}{T})$ in our setting,
where $T$ is the number of total iterations. Numerous simulations and
real-world data experiments are conducted to show the effectiveness of the
proposed algorithm via comparing with the state-of-the-art methods,
particularly, much lower computational cost with good prediction performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Ke Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Jinshan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xiong_J/0/1/0/all/0/1&quot;&gt;Jiechao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qianqian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaochun Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09819">
<title>Transformation Autoregressive Networks. (arXiv:1801.09819v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.09819</link>
<description rdf:parseType="Literal">&lt;p&gt;The fundamental task of general density estimation has been of keen interest
to machine learning. Recent advances in density estimation have either: a)
proposed a flexible model to estimate the conditional factors of the chain
rule, $p(x_{i}\, |\, x_{i-1}, \ldots)$; or b) used flexible, non-linear
transformations of variables of a simple base distribution. Instead, this work
jointly leverages transformations of variables and autoregressive conditional
models, and proposes novel methods for both. We provide a deeper understanding
of our methods, showing a considerable improvement through a comprehensive
study over both real world and synthetic data. Moreover, we illustrate the use
of our models in outlier detection and image modeling tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oliva_J/0/1/0/all/0/1&quot;&gt;Junier B. Oliva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Avinava Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnab&amp;#xe1;s P&amp;#xf3;czos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item></rdf:RDF>