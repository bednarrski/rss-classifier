<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-24T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08711"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08996"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.03768"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08617"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08619"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08667"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08748"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08750"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08798"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08833"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08984"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09021"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09066"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09153"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09160"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1503.01334"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.01006"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.05165"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.02066"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03067"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08615"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08646"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08697"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08796"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08806"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08841"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09028"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09046"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1402.6744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.09794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.02766"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06373"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10110"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08380"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02686"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08233"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08416"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08172"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.08711">
<title>All-Optical Machine Learning Using Diffractive Deep Neural Networks. (arXiv:1804.08711v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.08711</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an all-optical Diffractive Deep Neural Network (D2NN)
architecture that can learn to implement various functions after deep
learning-based design of passive diffractive layers that work collectively. We
experimentally demonstrated the success of this framework by creating
3D-printed D2NNs that learned to implement handwritten digit classification and
the function of an imaging lens at terahertz spectrum. With the existing
plethora of 3D-printing and other lithographic fabrication methods as well as
spatial-light-modulators, this all-optical deep learning framework can perform,
at the speed of light, various complex functions that computer-based neural
networks can implement, and will find applications in all-optical image
analysis, feature detection and object classification, also enabling new camera
designs and optical components that can learn to perform unique tasks using
D2NNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xing Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivenson_Y/0/1/0/all/0/1&quot;&gt;Yair Rivenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yardimci_N/0/1/0/all/0/1&quot;&gt;Nezih T. Yardimci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veli_M/0/1/0/all/0/1&quot;&gt;Muhammed Veli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jarrahi_M/0/1/0/all/0/1&quot;&gt;Mona Jarrahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozcan_A/0/1/0/all/0/1&quot;&gt;Aydogan Ozcan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08996">
<title>Genesis of Basic and Multi-Layer Echo State Network Recurrent Autoencoders for Efficient Data Representations. (arXiv:1804.08996v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.08996</link>
<description rdf:parseType="Literal">&lt;p&gt;It is a widely accepted fact that data representations intervene noticeably
in machine learning tools. The more they are well defined the better the
performance results are. Feature extraction-based methods such as autoencoders
are conceived for finding more accurate data representations from the original
ones. They efficiently perform on a specific task in terms of 1) high accuracy,
2) large short term memory and 3) low execution time. Echo State Network (ESN)
is a recent specific kind of Recurrent Neural Network which presents very rich
dynamics thanks to its reservoir-based hidden layer. It is widely used in
dealing with complex non-linear problems and it has outperformed classical
approaches in a number of tasks including regression, classification, etc. In
this paper, the noticeable dynamism and the large memory provided by ESN and
the strength of Autoencoders in feature extraction are gathered within an ESN
Recurrent Autoencoder (ESN-RAE). In order to bring up sturdier alternative to
conventional reservoir-based networks, not only single layer basic ESN is used
as an autoencoder, but also Multi-Layer ESN (ML-ESN-RAE). The new features,
once extracted from ESN&apos;s hidden layer, are applied to classification tasks.
The classification rates rise considerably compared to those obtained when
applying the original data features. An accuracy-based comparison is performed
between the proposed recurrent AEs and two variants of an ELM feed-forward AEs
(Basic and ML) in both of noise free and noisy environments. The empirical
study reveals the main contribution of recurrent connections in improving the
classification performance results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chouikhi_N/0/1/0/all/0/1&quot;&gt;Naima Chouikhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ammar_B/0/1/0/all/0/1&quot;&gt;Boudour Ammar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alimi_A/0/1/0/all/0/1&quot;&gt;Adel M. Alimi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.03768">
<title>Integer Factorization with a Neuromorphic Sieve. (arXiv:1703.03768v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1703.03768</link>
<description rdf:parseType="Literal">&lt;p&gt;The bound to factor large integers is dominated by the computational effort
to discover numbers that are smooth, typically performed by sieving a
polynomial sequence. On a von Neumann architecture, sieving has log-log
amortized time complexity to check each value for smoothness. This work
presents a neuromorphic sieve that achieves a constant time check for
smoothness by exploiting two characteristic properties of neuromorphic
architectures: constant time synaptic integration and massively parallel
computation. The approach is validated by modifying msieve, one of the fastest
publicly available integer factorization implementations, to use the IBM
Neurosynaptic System (NS1e) as a coprocessor for the sieving stage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monaco_J/0/1/0/all/0/1&quot;&gt;John V. Monaco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vindiola_M/0/1/0/all/0/1&quot;&gt;Manuel M. Vindiola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08617">
<title>Distributed Distributional Deterministic Policy Gradients. (arXiv:1804.08617v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.08617</link>
<description rdf:parseType="Literal">&lt;p&gt;This work adopts the very successful distributional perspective on
reinforcement learning and adapts it to the continuous control setting. We
combine this within a distributed framework for off-policy learning in order to
develop what we call the Distributed Distributional Deep Deterministic Policy
Gradient algorithm, D4PG. We also combine this technique with a number of
additional, simple improvements such as the use of $N$-step returns and
prioritized experience replay. Experimentally we examine the contribution of
each of these individual components, and show how they interact, as well as
their combined contributions. Our results show that across a wide variety of
simple control tasks, difficult manipulation tasks, and a set of hard
obstacle-based locomotion tasks the D4PG algorithm achieves state of the art
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barth_Maron_G/0/1/0/all/0/1&quot;&gt;Gabriel Barth-Maron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffman_M/0/1/0/all/0/1&quot;&gt;Matthew W. Hoffman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budden_D/0/1/0/all/0/1&quot;&gt;David Budden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabney_W/0/1/0/all/0/1&quot;&gt;Will Dabney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horgan_D/0/1/0/all/0/1&quot;&gt;Dan Horgan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+TB_D/0/1/0/all/0/1&quot;&gt;Dhruva TB&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muldal_A/0/1/0/all/0/1&quot;&gt;Alistair Muldal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1&quot;&gt;Nicolas Heess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lillicrap_T/0/1/0/all/0/1&quot;&gt;Timothy Lillicrap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08619">
<title>State Distribution-aware Sampling for Deep Q-learning. (arXiv:1804.08619v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.08619</link>
<description rdf:parseType="Literal">&lt;p&gt;A critical and challenging problem in reinforcement learning is how to learn
the state-action value function from the experience replay buffer and
simultaneously keep sample efficiency and faster convergence to a high quality
solution. In prior works, transitions are uniformly sampled at random from the
replay buffer or sampled based on their priority measured by
temporal-difference (TD) error. However, these approaches do not fully take
into consideration the intrinsic characteristics of transition distribution in
the state space and could result in redundant and unnecessary TD updates,
slowing down the convergence of the learning procedure. To overcome this
problem, we propose a novel state distribution-aware sampling method to balance
the replay times for transitions with skew distribution, which takes into
account both the occurrence frequencies of transitions and the uncertainty of
state-action values. Consequently, our approach could reduce the unnecessary TD
updates and increase the TD updates for state-action value with more
uncertainty, making the experience replay more effective and efficient.
Extensive experiments are conducted on both classic control tasks and Atari
2600 games based on OpenAI gym platform and the experimental results
demonstrate the effectiveness of our approach in comparison with the standard
DQN approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weichao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fuxian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1&quot;&gt;Gang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08667">
<title>Influencing Flock Formation in Low-Density Settings. (arXiv:1804.08667v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1804.08667</link>
<description rdf:parseType="Literal">&lt;p&gt;Flocking is a coordinated collective behavior that results from local sensing
between individual agents that have a tendency to orient towards each other.
Flocking is common among animal groups and might also be useful in robotic
swarms. In the interest of learning how to control flocking behavior, recent
work in the multiagent systems literature has explored the use of influencing
agents for guiding flocking agents to face a target direction. The existing
work in this domain has focused on simulation settings of small areas with
toroidal shapes. In such settings, agent density is high, so interactions are
common, and flock formation occurs easily. In our work, we study new
environments with lower agent density, wherein interactions are more rare. We
study the efficacy of placement strategies and influencing agent behaviors
drawn from the literature, and find that the behaviors that have been shown to
work well in high-density conditions tend to be much less effective in lower
density environments. The source of this ineffectiveness is that the
influencing agents explored in prior work tended to face directions optimized
for maximal influence, but which actually separate the influencing agents from
the flock. We find that in low-density conditions maintaining a connection to
the flock is more important than rushing to orient towards the desired
direction. We use these insights to propose new influencing agent behaviors,
which we dub &quot;follow-then-influence&quot;; agents act like normal members of the
flock to achieve positions that allow for control and then exert their
influence. This strategy overcomes the difficulties posed by low density
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Daniel Y. Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1&quot;&gt;Emily S. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krafft_P/0/1/0/all/0/1&quot;&gt;Peter M. Krafft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosz_B/0/1/0/all/0/1&quot;&gt;Barbara J. Grosz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08748">
<title>Discovery of Driving Patterns by Trajectory Segmentation. (arXiv:1804.08748v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.08748</link>
<description rdf:parseType="Literal">&lt;p&gt;Telematics data is becoming increasingly available due to the ubiquity of
devices that collect data during drives, for different purposes, such as usage
based insurance (UBI), fleet management, navigation of connected vehicles, etc.
Consequently, a variety of data-analytic applications have become feasible that
extract valuable insights from the data. In this paper, we address the
especially challenging problem of discovering behavior-based driving patterns
from only externally observable phenomena (e.g. vehicle&apos;s speed). We present a
trajectory segmentation approach capable of discovering driving patterns as
separate segments, based on the behavior of drivers. This segmentation approach
includes a novel transformation of trajectories along with a dynamic
programming approach for segmentation. We apply the segmentation approach on a
real-word, rich dataset of personal car trajectories provided by a major
insurance company based in Columbus, Ohio. Analysis and preliminary results
show the applicability of approach for finding significant driving patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moosavi_S/0/1/0/all/0/1&quot;&gt;Sobhan Moosavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nandi_A/0/1/0/all/0/1&quot;&gt;Arnab Nandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramnath_R/0/1/0/all/0/1&quot;&gt;Rajiv Ramnath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08750">
<title>A machine learning model for identifying cyclic alternating patterns in the sleeping brain. (arXiv:1804.08750v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.08750</link>
<description rdf:parseType="Literal">&lt;p&gt;Electroencephalography (EEG) is a method to record the electrical signals in
the brain. Recognizing the EEG patterns in the sleeping brain gives insights
into the understanding of sleeping disorders. The dataset under consideration
contains EEG data points associated with various physiological conditions. This
study attempts to generalize the detection of particular patterns associated
with the Non-Rapid Eye Movement (NREM) sleep cycle of the brain using a machine
learning model. The proposed model uses additional feature engineering to
incorporate sequential information for training a classifier to predict the
occurrence of Cyclic Alternating Pattern (CAP) sequences in the sleep cycle,
which are often associated with sleep disorders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chindhade_A/0/1/0/all/0/1&quot;&gt;Aditya Chindhade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alshi_A/0/1/0/all/0/1&quot;&gt;Abhijeet Alshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhatia_A/0/1/0/all/0/1&quot;&gt;Aakash Bhatia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dabhadkar_K/0/1/0/all/0/1&quot;&gt;Kedar Dabhadkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Menon_P/0/1/0/all/0/1&quot;&gt;Pranav Sivadas Menon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08798">
<title>SimpleQuestions Nearly Solved: A New Upperbound and Baseline Approach. (arXiv:1804.08798v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.08798</link>
<description rdf:parseType="Literal">&lt;p&gt;The SimpleQuestions dataset is one of the most commonly used benchmarks for
studying single-relation factoid questions. In this paper, we present new
evidence that this benchmark can be nearly solved by standard methods. First we
show that ambiguity in the data bounds performance on this benchmark at 83.4%;
there are often multiple answers that cannot be disambiguated from the
linguistic signal alone. Second we introduce a baseline that sets a new
state-of-the-art performance level at 78.1% accuracy, despite using standard
methods. Finally, we report an empirical analysis showing that the upperbound
is loose; roughly a third of the remaining errors are also not resolvable from
the linguistic signal. Together, these results suggest that the SimpleQuestions
dataset is nearly solved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrochuk_M/0/1/0/all/0/1&quot;&gt;Michael Petrochuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1&quot;&gt;Luke Zettlemoyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08833">
<title>Learning Manifolds from Non-stationary Streaming Data. (arXiv:1804.08833v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.08833</link>
<description rdf:parseType="Literal">&lt;p&gt;Streaming adaptations of manifold learning based dimensionality reduction
methods, such as Isomap, typically assume that the underlying data distribution
is stationary. Such methods are not equipped to detect or handle sudden changes
or gradual drifts in the distribution generating the stream. We prove that a
Gaussian Process Regression (GPR) model that uses a manifold-specific kernel
function and is trained on an initial batch of sufficient size, can closely
approximate the state-of-art streaming Isomap algorithm. The predictive
variance obtained from the GPR prediction is then shown to be an effective
detector of changes in the underlying data distribution. Results on several
synthetic and real data sets show that the resulting algorithm can effectively
learns lower dimensional representation of high dimensional data in a streaming
setting, while identify shifts in the generative distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mahapatra_S/0/1/0/all/0/1&quot;&gt;Suchismit Mahapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chandola_V/0/1/0/all/0/1&quot;&gt;Varun Chandola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08834">
<title>Measuring and Computing Database Inconsistency via Repairs. (arXiv:1804.08834v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1804.08834</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a generic numerical measure of inconsistency of a database with
respect to integrity constraints that is based on a repair semantics. A
particular measure is investigated, with mechanisms for computing it via
answer-set programs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1&quot;&gt;Leopoldo Bertossi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08924">
<title>Learning-Based Mean-Payoff Optimization in an Unknown MDP under Omega-Regular Constraints. (arXiv:1804.08924v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.08924</link>
<description rdf:parseType="Literal">&lt;p&gt;We formalize the problem of maximizing the mean-payoff value with high
probability while satisfying a parity objective in a Markov decision process
(MDP) with unknown probabilistic transition function and unknown reward
function. Assuming the support of the unknown transition function and a lower
bound on the minimal transition probability are known in advance, we show that
in MDPs consisting of a single end component, two combinations of guarantees on
the parity and mean-payoff objectives can be achieved depending on how much
memory one is willing to use. (i) For all $\epsilon$ and $\gamma$ we can
construct an online-learning finite-memory strategy that almost-surely
satisfies the parity objective and which achieves an $\epsilon$-optimal mean
payoff with probability at least $1 - \gamma$. (ii) Alternatively, for all
$\epsilon$ and $\gamma$ there exists an online-learning infinite-memory
strategy that satisfies the parity objective surely and which achieves an
$\epsilon$-optimal mean payoff with probability at least $1 - \gamma$. We
extend the above results to MDPs consisting of more than one end component in a
natural way. Finally, we show that the aforementioned guarantees are tight,
i.e. there are MDPs for which stronger combinations of the guarantees cannot be
ensured.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kretinsky_J/0/1/0/all/0/1&quot;&gt;Jan Kretinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1&quot;&gt;Guillermo A. Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raskin_J/0/1/0/all/0/1&quot;&gt;Jean-Francois Raskin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08984">
<title>Computational Approaches for Stochastic Shortest Path on Succinct MDPs. (arXiv:1804.08984v1 [cs.PL])</title>
<link>http://arxiv.org/abs/1804.08984</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the stochastic shortest path (SSP) problem for succinct Markov
decision processes (MDPs), where the MDP consists of a set of variables, and a
set of nondeterministic rules that update the variables. First, we show that
several examples from the AI literature can be modeled as succinct MDPs. Then
we present computational approaches for upper and lower bounds for the SSP
problem: (a)~for computing upper bounds, our method is polynomial-time in the
implicit description of the MDP; (b)~for lower bounds, we present a
polynomial-time (in the size of the implicit description) reduction to
quadratic programming. Our approach is applicable even to infinite-state MDPs.
Finally, we present experimental results to demonstrate the effectiveness of
our approach on several classical examples from the AI literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_K/0/1/0/all/0/1&quot;&gt;Krishnendu Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Hongfei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goharshady_A/0/1/0/all/0/1&quot;&gt;Amir Kafshdar Goharshady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okati_N/0/1/0/all/0/1&quot;&gt;Nastaran Okati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09021">
<title>Label-aware Double Transfer Learning for Cross-Specialty Medical Named Entity Recognition. (arXiv:1804.09021v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.09021</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of named entity recognition (NER) from electronic
medical records, which is one of the most fundamental and critical problems for
medical text mining. Medical records which are written by clinicians from
different specialties usually contain quite different terminologies and writing
styles. The difference of specialties and the cost of human annotation makes it
particularly difficult to train a universal medical NER system. In this paper,
we propose a label-aware double transfer learning framework (La-DTL) for
cross-specialty NER, so that a medical NER system designed for one specialty
could be conveniently applied to another one with minimal annotation efforts.
The transferability is guaranteed by 2 components: (i) we propose label-aware
MMD for feature representation transfer, and (ii) we perform parameter transfer
with a theoretical upper bound which is also label aware. We annotate a new
medical NER corpus and conduct extensive experiments on 12 cross-specialty NER
tasks. The experimental results demonstrate that La-DTL provides consistent
accuracy improvement over strong baselines. Besides, the promising experimental
results on non-medical NER scenarios indicate that La-DTL is potential to be
seamlessly adapted to a wide range of NER tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenghui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1&quot;&gt;Yanru Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jian Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaodian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yimei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1&quot;&gt;Gen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ken Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09066">
<title>ECO: Efficient Convolutional Network for Online Video Understanding. (arXiv:1804.09066v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.09066</link>
<description rdf:parseType="Literal">&lt;p&gt;The state of the art in video understanding suffers from two problems: (1)
The major part of reasoning is performed locally in the video, therefore, it
misses important relationships within actions that span several seconds. (2)
While there are local methods with fast per-frame processing, the processing of
the whole video is not efficient and hampers fast video retrieval or online
classification of long-term activities. In this paper, we introduce a network
architecture that takes long-term content into account and enables fast
per-video processing at the same time. The architecture is based on merging
long-term content already in the network rather than in a post-hoc fusion.
Together with a sampling strategy, which exploits that neighboring frames are
largely redundant, this yields high-quality action classification and video
captioning at up to 230 videos per second, where each video can consist of a
few hundred frames. The approach achieves competitive performance across all
datasets while being 10x to 80x faster than state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zolfaghari_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Zolfaghari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Kamaljeet Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1&quot;&gt;Thomas Brox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09153">
<title>An Integrated Framework for AI Assisted Level Design in 2D Platformers. (arXiv:1804.09153v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.09153</link>
<description rdf:parseType="Literal">&lt;p&gt;The design of video game levels is a complex and critical task. Levels need
to elicit fun and challenge while avoiding frustration at all costs. In this
paper, we present a framework to assist designers in the creation of levels for
2D platformers. Our framework provides designers with a toolbox (i) to create
2D platformer levels, (ii) to estimate the difficulty and probability of
success of single jump actions (the main mechanics of platformer games), and
(iii) a set of metrics to evaluate the difficulty and probability of completion
of entire levels. At the end, we present the results of a set of experiments we
carried out with human players to validate the metrics included in our
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aramini_A/0/1/0/all/0/1&quot;&gt;Antonio Umberto Aramini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanzi_P/0/1/0/all/0/1&quot;&gt;Pier Luca Lanzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loiacono_D/0/1/0/all/0/1&quot;&gt;Daniele Loiacono&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09160">
<title>No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling. (arXiv:1804.09160v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.09160</link>
<description rdf:parseType="Literal">&lt;p&gt;Though impressive results have been achieved in visual captioning, the task
of generating abstract stories from photo streams is still a little-tapped
problem. Different from captions, stories have more expressive language styles
and contain many imaginary concepts that do not appear in the images. Thus it
poses challenges to behavioral cloning algorithms. Furthermore, due to the
limitations of automatic metrics on evaluating story quality, reinforcement
learning methods with hand-crafted rewards also face difficulties in gaining an
overall performance boost. Therefore, we propose an Adversarial REward Learning
(AREL) framework to learn an implicit reward function from human
demonstrations, and then optimize policy search with the learned reward
function. Though automatic evaluation indicates slight performance boost over
state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation
shows that our approach achieves significant improvement in generating more
human-like stories than SOTA systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuan-Fang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1503.01334">
<title>Optimal quantum mixing for slowly evolving sequences of Markov chains. (arXiv:1503.01334v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1503.01334</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we consider the problem of preparation of the stationary
distribution of irreducible, time-reversible Markov chains, which is a
fundamental task in algorithmic Markov chain theory. For the classical setting,
this task has a complexity lower bound of $\Omega(\delta^{-1})$, where $\delta$
is the spectral gap of the Markov chain, and other dependencies contribute only
logarithmically. In the quantum case, the conjectured complexity is
$O(\sqrt{\delta^{-1}})$, with other dependencies contributing only
logarithmically. However, this bound has only been achieved for a few special
classes of Markov chains. In this work, we provide a method for the sequential
preparation of stationary distributions for sequences of time-reversible
$N$-state Markov chains, akin to the setting of simulated annealing methods.
The complexity of preparation we achieve is $O(\sqrt{\delta^{-1}}
\sqrt[4]{N})$, neglecting logarithmic factors. While this result falls short of
the conjectured optimal time, it provides a quadratic improvement over
na\&quot;{i}ve approaches. Moreover, for the case when the output distributions are
required to be encoded in pure quantum states we identify the settings where
our algorithm is strictly optimal. The settings of slowly evolving sequences of
Markov chains naturally appear in reinforcement learning, and consequently our
results can be readily applied in quantum machine learning as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Orsucci_D/0/1/0/all/0/1&quot;&gt;Davide Orsucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Briegel_H/0/1/0/all/0/1&quot;&gt;Hans J. Briegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Dunjko_V/0/1/0/all/0/1&quot;&gt;Vedran Dunjko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.01006">
<title>Ontology based Scene Creation for the Development of Automated Vehicles. (arXiv:1704.01006v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1704.01006</link>
<description rdf:parseType="Literal">&lt;p&gt;The introduction of automated vehicles without permanent human supervision
demands a functional system description, including functional system boundaries
and a comprehensive safety analysis. These inputs to the technical development
can be identified and analyzed by a scenario-based approach. Furthermore, to
establish an economical test and release process, a large number of scenarios
must be identified to obtain meaningful test results. Experts are doing well to
identify scenarios that are difficult to handle or unlikely to happen. However,
experts are unlikely to identify all scenarios possible based on the knowledge
they have on hand. Expert knowledge modeled for computer aided processing may
help for the purpose of providing a wide range of scenarios. This contribution
reviews ontologies as knowledge-based systems in the field of automated
vehicles, and proposes a generation of traffic scenes in natural language as a
basis for a scenario creation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagschik_G/0/1/0/all/0/1&quot;&gt;Gerrit Bagschik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menzel_T/0/1/0/all/0/1&quot;&gt;Till Menzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maurer_M/0/1/0/all/0/1&quot;&gt;Markus Maurer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.05165">
<title>A Comprehensive Implementation of Conceptual Spaces. (arXiv:1707.05165v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1707.05165</link>
<description rdf:parseType="Literal">&lt;p&gt;The highly influential framework of conceptual spaces provides a geometric
way of representing knowledge. Instances are represented by points and concepts
are represented by regions in a (potentially) high-dimensional space. Based on
our recent formalization, we present a comprehensive implementation of the
conceptual spaces framework that is not only capable of representing concepts
with inter-domain correlations, but that also offers a variety of operations on
these concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bechberger_L/0/1/0/all/0/1&quot;&gt;Lucas Bechberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhnberger_K/0/1/0/all/0/1&quot;&gt;Kai-Uwe K&amp;#xfc;hnberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.02066">
<title>Formulation of Deep Reinforcement Learning Architecture Toward Autonomous Driving for On-Ramp Merge. (arXiv:1709.02066v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.02066</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiple automakers have in development or in production automated driving
systems (ADS) that offer freeway-pilot functions. This type of ADS is typically
limited to restricted-access freeways only, that is, the transition from manual
to automated modes takes place only after the ramp merging process is completed
manually. One major challenge to extend the automation to ramp merging is that
the automated vehicle needs to incorporate and optimize long-term objectives
(e.g. successful and smooth merge) when near-term actions must be safely
executed. Moreover, the merging process involves interactions with other
vehicles whose behaviors are sometimes hard to predict but may influence the
merging vehicle optimal actions. To tackle such a complicated control problem,
we propose to apply Deep Reinforcement Learning (DRL) techniques for finding an
optimal driving policy by maximizing the long-term reward in an interactive
environment. Specifically, we apply a Long Short-Term Memory (LSTM)
architecture to model the interactive environment, from which an internal state
containing historical driving information is conveyed to a Deep Q-Network
(DQN). The DQN is used to approximate the Q-function, which takes the internal
state as input and generates Q-values as output for action selection. With this
DRL architecture, the historical impact of interactive environment on the
long-term reward can be captured and taken into account for deciding the
optimal control policy. The proposed architecture has the potential to be
extended and applied to other autonomous driving scenarios such as driving
through a complex intersection or changing lanes under varying traffic flow
conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Ching-Yao Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03067">
<title>Compositional Attention Networks for Machine Reasoning. (arXiv:1803.03067v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03067</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the MAC network, a novel fully differentiable neural network
architecture, designed to facilitate explicit and expressive reasoning. MAC
moves away from monolithic black-box neural architectures towards a design that
encourages both transparency and versatility. The model approaches problems by
decomposing them into a series of attention-based reasoning steps, each
performed by a novel recurrent Memory, Attention, and Composition (MAC) cell
that maintains a separation between control and memory. By stringing the cells
together and imposing structural constraints that regulate their interaction,
MAC effectively learns to perform iterative reasoning processes that are
directly inferred from the data in an end-to-end approach. We demonstrate the
model&apos;s strength, robustness and interpretability on the challenging CLEVR
dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy,
halving the error rate of the previous best model. More importantly, we show
that the model is computationally-efficient and data-efficient, in particular
requiring 5x less data than existing models to achieve strong results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hudson_D/0/1/0/all/0/1&quot;&gt;Drew A. Hudson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1&quot;&gt;Christopher D. Manning&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08615">
<title>Descriptor Selection via Self-Paced Learning for Bioactivity of Molecular Structure in QSAR Classification. (arXiv:1804.08615v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.08615</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantitative structure-activity relationship (QSAR) modelling is effective
&apos;bridge&apos; to search the reliable relationship related biological activities to
chemical structure. A QSAR classification model contains a lager number of
redundant, noisy and irrelevant descriptors. To solve this problem, various of
methods have been proposed for descriptor selection. Generally, they can be
grouped into three categories: filters, wrappers, and embedded methods.
Regularization method is an important embedded technology, which can be used
for continuous shrinkage and automatic descriptors selection. In recent years,
the interest of researchers in the application of regularization techniques is
increasing in descriptors selection , such as, logistic regression(LR) with
$L_1$ penalty. In this paper, we proposed a novel descriptor selection method
based on self-paced learning(SPL) with Logsum penalized LR for classifying the
biological activities of molecular structure. SPL inspired by the learning
process of humans and animals that gradually learns from easy samples(smaller
losses) to hard samples(bigger losses) samples into training and Logsum
regularization has capacity to select few meaningful and significant molecular
descriptors, respectively. Experimental results on artificial and three public
QSAR datasets show that our proposed SPL-Logsum method is superior to other
commonly used sparse methods in terms of classification performance and model
interpretation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Liang-Yong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qing-Yong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yong Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08646">
<title>A Theory of Statistical Inference for Ensuring the Robustness of Scientific Results. (arXiv:1804.08646v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.08646</link>
<description rdf:parseType="Literal">&lt;p&gt;Inference is the process of using facts we know to learn about facts we do
not know. A theory of inference gives assumptions necessary to get from the
former to the latter, along with a definition for and summary of the resulting
uncertainty. Any one theory of inference is neither right nor wrong, but merely
an axiom that may or may not be useful. Each of the many diverse theories of
inference can be valuable for certain applications. However, no existing theory
of inference addresses the tendency to choose, from the range of plausible data
analysis specifications consistent with prior evidence, those that
inadvertently favor one&apos;s own hypotheses. Since the biases from these choices
are a growing concern across scientific fields, and in a sense the reason the
scientific community was invented in the first place, we introduce a new theory
of inference designed to address this critical problem. We derive &quot;hacking
intervals,&quot; which are the range of a summary statistic one may obtain given a
class of possible endogenous manipulations of the data. Hacking intervals
require no appeal to hypothetical data sets drawn from imaginary
superpopulations. A scientific result with a small hacking interval is more
robust to researcher manipulation than one with a larger interval, and is often
easier to interpret than a classical confidence interval. Some versions of
hacking intervals turn out to be equivalent to classical confidence intervals,
which means they may also provide a more intuitive and potentially more useful
interpretation of classical confidence intervals
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Coker_B/0/1/0/all/0/1&quot;&gt;Beau Coker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+King_G/0/1/0/all/0/1&quot;&gt;Gary King&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08697">
<title>Simultaneous shot inversion for nonuniform geometries using fast data interpolation. (arXiv:1804.08697v1 [math.OC])</title>
<link>http://arxiv.org/abs/1804.08697</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic optimization is key to efficient inversion in PDE-constrained
optimization. Using &apos;simultaneous shots&apos;, or random superposition of source
terms, works very well in simple acquisition geometries where all sources see
all receivers, but this rarely occurs in practice. We develop an approach that
interpolates data to an ideal acquisition geometry while solving the inverse
problem using simultaneous shots. The approach is formulated as a joint inverse
problem, combining ideas from low-rank interpolation with full-waveform
inversion. Results using synthetic experiments illustrate the flexibility and
efficiency of the approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Michelle Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kumar_R/0/1/0/all/0/1&quot;&gt;Rajiv Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Haber_E/0/1/0/all/0/1&quot;&gt;Eldad Haber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Aravkin_A/0/1/0/all/0/1&quot;&gt;Aleksandr Aravkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08796">
<title>Block-Structure Based Time-Series Models For Graph Sequences. (arXiv:1804.08796v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.08796</link>
<description rdf:parseType="Literal">&lt;p&gt;Although the computational and statistical trade-off for modeling single
graphs, for instance using block models, is relatively well understood,
extending such results to sequences of graphs has proven to be difficult. In
this work, we propose two models for graph sequences that capture: (a) link
persistence between nodes across time, and (b) community persistence of each
node across time. In the first model, we assume that the latent community of
each node does not change over time, and in the second model we relax this
assumption suitably. For both of these proposed models, we provide
computationally efficient inference algorithms, whose unique feature is that
they leverage community detection methods that work on single graphs. We also
provide experimental results validating the suitability of the models and the
performance of our algorithms on synthetic instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Amjadi_M/0/1/0/all/0/1&quot;&gt;Mehrnaz Amjadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tulabandhula_T/0/1/0/all/0/1&quot;&gt;Theja Tulabandhula&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08806">
<title>Structured SUMCOR Multiview Canonical Correlation Analysis for Large-Scale Data. (arXiv:1804.08806v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.08806</link>
<description rdf:parseType="Literal">&lt;p&gt;The sum-of-correlations (SUMCOR) formulation of generalized canonical
correlation analysis (GCCA) seeks highly correlated low-dimensional
representations of different views via maximizing pairwise latent similarity of
the views. SUMCOR is considered arguably the most natural extension of
classical two-view CCA to the multiview case, and thus has numerous
applications in signal processing and data analytics. Recent work has proposed
effective algorithms for handling the SUMCOR problem at very large scale.
However, the existing scalable algorithms cannot incorporate structural
regularization and prior information -- which are critical for good performance
in real-world applications. In this work, we propose a new computational
framework for large-scale SUMCOR GCCA that can easily incorporate a suite of
structural regularizers which are frequently used in data analytics. The
updates of the proposed algorithm are lightweight and the memory complexity is
also low. In addition, the proposed algorithm can be readily implemented in a
parallel fashion. We show that the proposed algorithm converges to a
Karush-Kuhn-Tucker (KKT) point of the regularized SUMCOR problem. Judiciously
designed simulations and real-data experiments are employed to demonstrate the
effectiveness of the proposed algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanatsoulis_C/0/1/0/all/0/1&quot;&gt;Charilaos I. Kanatsoulis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xiao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sidiropoulos_N/0/1/0/all/0/1&quot;&gt;Nicholas D. Sidiropoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1&quot;&gt;Mingyi Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08841">
<title>Between hard and soft thresholding: optimal iterative thresholding algorithms. (arXiv:1804.08841v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1804.08841</link>
<description rdf:parseType="Literal">&lt;p&gt;Iterative thresholding algorithms seek to optimize a differentiable objective
function over a sparsity or rank constraint by alternating between gradient
steps that reduce the objective, and thresholding steps that enforce the
constraint. This work examines the choice of the thresholding operator, and
asks whether it is possible to achieve stronger guarantees than what is
possible with hard thresholding. We develop the notion of relative concavity of
a thresholding operator, a quantity that characterizes the convergence
performance of any thresholding operator on the target optimization problem.
Surprisingly, we find that commonly used thresholding operators, such as hard
thresholding and soft thresholding, are suboptimal in terms of convergence
guarantees. Instead, a general class of thresholding operators, lying between
hard thresholding and soft thresholding, is shown to be optimal with the
strongest possible convergence guarantee among all thresholding operators.
Examples of this general class includes $\ell_q$ thresholding with appropriate
choices of $q$, and a newly defined {\em reciprocal thresholding} operator. As
a byproduct of the improved convergence guarantee, these new thresholding
operators improve on the best known upper bound for prediction error of both
iterative hard thresholding and Lasso in terms of the dependence on condition
number in the setting of sparse linear regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haoyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barber_R/0/1/0/all/0/1&quot;&gt;Rina Foygel Barber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09028">
<title>Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications. (arXiv:1804.09028v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.09028</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing applications include a huge amount of knowledge that is out of reach
for deep neural networks. This paper presents a novel approach for integrating
calls to existing applications into deep learning architectures. Using this
approach, we estimate each application&apos;s functionality with an estimator, which
is implemented as a deep neural network (DNN). The estimator is then embedded
into a base network that we direct into complying with the application&apos;s
interface during an end-to-end optimization process. At inference time, we
replace each estimator with its existing application counterpart and let the
base network solve the task by interacting with the existing application. Using
this &apos;Estimate and Replace&apos; method, we were able to train a DNN end-to-end with
less data and outperformed a matching DNN that did not interact with the
external application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadash_G/0/1/0/all/0/1&quot;&gt;Guy Hadash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kermany_E/0/1/0/all/0/1&quot;&gt;Einat Kermany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmeli_B/0/1/0/all/0/1&quot;&gt;Boaz Carmeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavi_O/0/1/0/all/0/1&quot;&gt;Ofer Lavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kour_G/0/1/0/all/0/1&quot;&gt;George Kour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1&quot;&gt;Alon Jacovi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09046">
<title>Is it possible to retrieve soil-moisture content from measured VNIR hyperspectral data?. (arXiv:1804.09046v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.09046</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the potential of estimating the soil-moisture
content based on VNIR hyperspectral data combined with IR data. Measurements
from a multi-sensor field campaign represent the benchmark dataset which
contains measured hyperspectral, IR, and soil-moisture data. We introduce a
regression framework with three steps consisting of feature selection,
preprocessing, and well-chosen regression models. The latter are mainly
supervised machine learning models. An exception are the self-organizing maps
which are a combination of unsupervised and supervised learning. We analyze the
impact of the distinct preprocessing methods on the regression results. Of all
regression models, the extremely randomized trees model without preprocessing
provides the best estimation performance. Our results reveal the potential of
the respective regression framework combined with the VNIR hyperspectral data
to estimate soil moisture. In conclusion, the results of this paper provide a
basis for further improvements in different research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_S/0/1/0/all/0/1&quot;&gt;Sina Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riese_F/0/1/0/all/0/1&quot;&gt;Felix M. Riese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stotzer_J/0/1/0/all/0/1&quot;&gt;Johanna St&amp;#xf6;tzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_P/0/1/0/all/0/1&quot;&gt;Philipp M. Maier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hinz_S/0/1/0/all/0/1&quot;&gt;Stefan Hinz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1402.6744">
<title>Asymmetric Clusters and Outliers: Mixtures of Multivariate Contaminated Shifted Asymmetric Laplace Distributions. (arXiv:1402.6744v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1402.6744</link>
<description rdf:parseType="Literal">&lt;p&gt;Mixtures of multivariate contaminated shifted asymmetric Laplace
distributions are developed for handling asymmetric clusters in the presence of
outliers (also referred to as bad points herein). In addition to the parameters
of the related non-contaminated mixture, for each (asymmetric) cluster, our
model has one parameter controlling the proportion of outliers and one
specifying the degree of contamination. Crucially, these parameters do not have
to be specified a priori, adding a flexibility to our approach that is absent
from other approaches such as trimming. Moreover, each observation is given a
posterior probability of belonging to a particular cluster, and of being an
outlier or not; advantageously, this allows for the automatic detection of
outliers. An expectation-conditional maximization algorithm is outlined for
parameter estimation and various implementation issues are discussed. The
behaviour of the proposed model is investigated, and compared with
well-established finite mixtures, on artificial and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Morris_K/0/1/0/all/0/1&quot;&gt;Katherine Morris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Punzo_A/0/1/0/all/0/1&quot;&gt;Antonio Punzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McNicholas_P/0/1/0/all/0/1&quot;&gt;Paul D. McNicholas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Browne_R/0/1/0/all/0/1&quot;&gt;Ryan P. Browne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.09794">
<title>Design and Analysis of the NIPS 2016 Review Process. (arXiv:1708.09794v2 [cs.DL] UPDATED)</title>
<link>http://arxiv.org/abs/1708.09794</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Information Processing Systems (NIPS) is a top-tier annual conference
in machine learning. The 2016 edition of the conference comprised more than
2,400 paper submissions, 3,000 reviewers, and 8,000 attendees. This represents
a growth of nearly 40% in terms of submissions, 96% in terms of reviewers, and
over 100% in terms of attendees as compared to the previous year. The massive
scale as well as rapid growth of the conference calls for a thorough quality
assessment of the peer-review process and novel means of improvement. In this
paper, we analyze several aspects of the data collected during the review
process, including an experiment investigating the efficacy of collecting
ordinal rankings from reviewers. Our goal is to check the soundness of the
review process, and provide insights that may be useful in the design of the
review process of subsequent conferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nihar B. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabibian_B/0/1/0/all/0/1&quot;&gt;Behzad Tabibian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muandet_K/0/1/0/all/0/1&quot;&gt;Krikamol Muandet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1&quot;&gt;Isabelle Guyon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luxburg_U/0/1/0/all/0/1&quot;&gt;Ulrike von Luxburg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.02766">
<title>Bayesian Alignments of Warped Multi-Output Gaussian Processes. (arXiv:1710.02766v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.02766</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a Bayesian extension to convolution processes which defines a
representation between multiple functions via an embedding in a shared latent
space. The proposed model allows for both arbitrary alignments of the inputs
and non-parametric output warpings to transform the observations. This gives
rise to multiple deep Gaussian process models connected via latent generating
processes. We derive an efficient variational approximation based on nested
variational compression and show how the model can be used to extract shared
information between dependent time series, recovering an interpretable
functional decomposition of the learning problem. The method is applied to both
an artificial data set and measurements of a pair of wind turbines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kaiser_M/0/1/0/all/0/1&quot;&gt;Markus Kaiser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Otte_C/0/1/0/all/0/1&quot;&gt;Clemens Otte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Runkler_T/0/1/0/all/0/1&quot;&gt;Thomas Runkler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ek_C/0/1/0/all/0/1&quot;&gt;Carl Henrik Ek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06373">
<title>Thoracic Disease Identification and Localization with Limited Supervision. (arXiv:1711.06373v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06373</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate identification and localization of abnormalities from radiology
images play an integral part in clinical diagnosis and treatment planning.
Building a highly accurate prediction model for these tasks usually requires a
large number of images manually annotated with labels and finding sites of
abnormalities. In reality, however, such annotated data are expensive to
acquire, especially the ones with location annotations. We need methods that
can work well with only a small amount of location annotations. To address this
challenge, we present a unified approach that simultaneously performs disease
identification and localization through the same underlying model for all
images. We demonstrate that our approach can effectively leverage both class
information as well as limited location annotation, and significantly
outperforms the comparative reference baseline in both classification and
localization tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1&quot;&gt;Mei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yuan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li-Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10110">
<title>Beyond Keywords and Relevance: A Personalized Ad Retrieval Framework in E-Commerce Sponsored Search. (arXiv:1712.10110v5 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1712.10110</link>
<description rdf:parseType="Literal">&lt;p&gt;On most sponsored search platforms, advertisers bid on some keywords for
their advertisements (ads). Given a search request, ad retrieval module
rewrites the query into bidding keywords, and uses these keywords as keys to
select Top N ads through inverted indexes. In this way, an ad will not be
retrieved even if queries are related when the advertiser does not bid on
corresponding keywords. Moreover, most ad retrieval approaches regard rewriting
and ad-selecting as two separated tasks, and focus on boosting relevance
between search queries and ads. Recently, in e-commerce sponsored search more
and more personalized information has been introduced, such as user profiles,
long-time and real-time clicks. Personalized information makes ad retrieval
able to employ more elements (e.g. real-time clicks) as search signals and
retrieval keys, however it makes ad retrieval more difficult to measure ads
retrieved through different signals. To address these problems, we propose a
novel ad retrieval framework beyond keywords and relevance in e-commerce
sponsored search. Firstly, we employ historical ad click data to initialize a
hierarchical network representing signals, keys and ads, in which personalized
information is introduced. Then we train a model on top of the hierarchical
network by learning the weights of edges. Finally we select the best edges
according to the model, boosting RPM/CTR. Experimental results on our
e-commerce platform demonstrate that our ad retrieval framework achieves good
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Su Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianshu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1&quot;&gt;Daorui Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kaipeng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08380">
<title>On Abruptly-Changing and Slowly-Varying Multiarmed Bandit Problems. (arXiv:1802.08380v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08380</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the non-stationary stochastic multiarmed bandit (MAB) problem and
propose two generic algorithms, namely, the limited memory deterministic
sequencing of exploration and exploitation (LM-DSEE) and the Sliding-Window
Upper Confidence Bound# (SW-UCB#). We rigorously analyze these algorithms in
abruptly-changing and slowly-varying environments and characterize their
performance. We show that the expected cumulative regret for these algorithms
under either of the environments is upper bounded by sublinear functions of
time, i.e., the time average of the regret asymptotically converges to zero. We
complement our analytic results with numerical illustrations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Lai Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srivastava_V/0/1/0/all/0/1&quot;&gt;Vaibhav Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02686">
<title>Complex energy landscapes in spiked-tensor and simple glassy models: ruggedness, arrangements of local minima and phase transitions. (arXiv:1804.02686v2 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02686</link>
<description rdf:parseType="Literal">&lt;p&gt;We study rough high-dimensional landscapes in which an increasingly stronger
preference for a given configuration emerges. Such energy landscapes arise in
glass physics and inference. In particular we focus on random Gaussian
functions, and on the spiked-tensor model and generalizations. We thoroughly
analyze the statistical properties of the corresponding landscapes and
characterize the associated geometrical phase transitions. In order to perform
our study, we develop a framework based on the Kac-Rice method that allows to
compute the complexity of the landscape, i.e. the logarithm of the typical
number of stationary points and their Hessian. This approach generalizes the
one used to compute rigorously the annealed complexity of mean-field glass
models. We discuss its advantages with respect to previous frameworks, in
particular the thermodynamical replica method which is shown to lead to
partially incorrect predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ros_V/0/1/0/all/0/1&quot;&gt;Valentina Ros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Arous_G/0/1/0/all/0/1&quot;&gt;Gerard Ben Arous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Biroli_G/0/1/0/all/0/1&quot;&gt;Giulio Biroli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Cammarota_C/0/1/0/all/0/1&quot;&gt;Chiara Cammarota&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03329">
<title>Representation Tradeoffs for Hyperbolic Embeddings. (arXiv:1804.03329v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.03329</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperbolic embeddings offer excellent quality with few dimensions when
embedding hierarchical data structures like synonym or type hierarchies. Given
a tree, we give a combinatorial construction that embeds the tree in hyperbolic
space with arbitrarily low distortion without using optimization. On WordNet,
our combinatorial embedding obtains a mean-average-precision of 0.989 with only
two dimensions, while Nickel et al.&apos;s recent construction obtains 0.87 using
200 dimensions. We provide upper and lower bounds that allow us to characterize
the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To
embed general metric spaces, we propose a hyperbolic generalization of
multidimensional scaling (h-MDS). We show how to perform exact recovery of
hyperbolic points from distances, provide a perturbation analysis, and give a
recovery result that allows us to reduce dimensionality. The h-MDS approach
offers consistently low distortion even with few dimensions across several
datasets. Finally, we extract lessons from the algorithms and theory above to
design a PyTorch-based implementation that can handle incomplete information
and is scalable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1&quot;&gt;Christopher De Sa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1&quot;&gt;Albert Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1&quot;&gt;Christopher R&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1&quot;&gt;Frederic Sala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08233">
<title>N-fold Superposition: Improving Neural Networks by Reducing the Noise in Feature Maps. (arXiv:1804.08233v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08233</link>
<description rdf:parseType="Literal">&lt;p&gt;Considering the use of Fully Connected (FC) layer limits the performance of
Convolutional Neural Networks (CNNs), this paper develops a method to improve
the coupling between the convolution layer and the FC layer by reducing the
noise in Feature Maps (FMs). Our approach is divided into three steps. Firstly,
we separate all the FMs into n blocks equally. Then, the weighted summation of
FMs at the same position in all blocks constitutes a new block of FMs. Finally,
we replicate this new block into n copies and concatenate them as the input to
the FC layer. This sharing of FMs could reduce the noise in them apparently and
avert the impact by a particular FM on the specific part weight of hidden
layers, hence preventing the network from overfitting to some extent. Using the
Fermat Lemma, we prove that this method could make the global minima value
range of the loss function wider, by which makes it easier for neural networks
to converge and accelerates the convergence process. This method does not
significantly increase the amounts of network parameters (only a few more
coefficients added), and the experiments demonstrate that this method could
increase the convergence speed and improve the classification performance of
neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Q/0/1/0/all/0/1&quot;&gt;Qiang Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chao Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08416">
<title>Learn and Pick Right Nodes to Offload. (arXiv:1804.08416v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08416</link>
<description rdf:parseType="Literal">&lt;p&gt;Task offloading is a promising technology to exploit the benefits of fog
computing. An effective task offloading strategy is needed to utilize the
computational resources efficiently. In this paper, we endeavor to seek an
online task offloading strategy to minimize the long-term latency. In
particular, we formulate a stochastic programming problem, where the
expectations of the system parameters change abruptly at unknown time instants.
Meanwhile, we consider the fact that the queried nodes can only feed back the
processing results after finishing the tasks. We then put forward an effective
algorithm to solve this challenging stochastic programming under the
non-stationary bandit model. We further prove that our proposed algorithm is
asymptotically optimal in a non-stationary fog-enabled network. Numerical
simulations are carried out to corroborate our designs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhaowei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Shengda Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiliang Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08172">
<title>Run-and-Inspect Method for Nonconvex Optimization and Global Optimality Bounds for R-Local Minimizers. (arXiv:1711.08172v1 [math.OC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1711.08172</link>
<description rdf:parseType="Literal">&lt;p&gt;Many optimization algorithms converge to stationary points. When the
underlying problem is nonconvex, they may get trapped at local minimizers and
occasionally stagnate near saddle points. We propose the Run-and-Inspect
Method, which adds an &quot;inspect&quot; phase to existing algorithms that helps escape
from non-global stationary points. The inspection samples a set of points in a
radius $R$ around the current point. When a sample point yields a sufficient
decrease in the objective, we move there and resume an existing algorithm. If
no sufficient decrease is found, the current point is called an approximate
$R$-local minimizer. We show that an $R$-local minimizer is globally optimal,
up to a specific error depending on $R$, if the objective function can be
implicitly decomposed into a smooth convex function plus a restricted function
that is possibly nonconvex, nonsmooth. For high-dimensional problems, we
introduce blockwise inspections to overcome the curse of dimensionality while
still maintaining optimality bounds up to a factor equal to the number of
blocks. Our method performs well on a set of artificial and realistic nonconvex
problems by coupling with gradient descent, coordinate descent, EM, and
prox-linear algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuejiao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wotao Yin&lt;/a&gt;</dc:creator>
</item></rdf:RDF>