<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-30T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10087"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06733"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09780"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09788"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09804"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09808"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09810"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09854"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09955"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10055"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.02000"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.08475"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.10301"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11089"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07163"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09827"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09829"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09870"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10108"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10123"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10130"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1603.02644"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.04599"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.09866"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.06150"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.04340"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.08310"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03189"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03634"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07883"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.10087">
<title>The Benefits of Population Diversity in Evolutionary Algorithms: A Survey of Rigorous Runtime Analyses. (arXiv:1801.10087v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.10087</link>
<description rdf:parseType="Literal">&lt;p&gt;Population diversity is crucial in evolutionary algorithms to enable global
exploration and to avoid poor performance due to premature convergence. This
book chapter reviews runtime analyses that have shown benefits of population
diversity, either through explicit diversity mechanisms or through naturally
emerging diversity. These works show that the benefits of diversity are
manifold: diversity is important for global exploration and the ability to find
several global optima. Diversity enhances crossover and enables crossover to be
more effective than mutation. Diversity can be crucial in dynamic optimization,
when the problem landscape changes over time. And, finally, it facilitates
search for the whole Pareto front in evolutionary multiobjective optimization.
The presented analyses rigorously quantify the performance of evolutionary
algorithms in the light of population diversity, laying the foundation for a
rigorous understanding of how search dynamics are affected by the presence or
absence of population diversity and the introduction of diversity mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sudholt_D/0/1/0/all/0/1&quot;&gt;Dirk Sudholt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04604">
<title>Deep Quaternion Networks. (arXiv:1712.04604v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04604</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of deep learning has seen significant advancement in recent years.
However, much of the existing work has been focused on real-valued numbers.
Recent work has shown that a deep learning system using the complex numbers can
be deeper for a fixed parameter budget compared to its real-valued counterpart.
In this work, we explore the benefits of generalizing one step further into the
hyper-complex numbers, quaternions specifically, and provide the architecture
components needed to build deep quaternion networks. We go over quaternion
convolutions, present a quaternion weight initialization scheme, and present
algorithms for quaternion batch-normalization. These pieces are tested in a
classification model by end-to-end training on the CIFAR-10 and CIFAR-100 data
sets and a segmentation model by end-to-end training on the KITTI Road
Segmentation data set. The quaternion networks show improved convergence
compared to real-valued and complex-valued networks, especially on the
segmentation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaudet_C/0/1/0/all/0/1&quot;&gt;Chase Gaudet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maida_A/0/1/0/all/0/1&quot;&gt;Anthony Maida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06733">
<title>Probabilistic Tools for the Analysis of Randomized Optimization Heuristics. (arXiv:1801.06733v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06733</link>
<description rdf:parseType="Literal">&lt;p&gt;This chapter collects several probabilistic tools that proved to be useful in
the analysis of randomized search heuristics. This includes classic material
like Markov, Chebyshev and Chernoff inequalities, but also lesser known topics
like stochastic domination and coupling or Chernoff bounds for geometrically
distributed random variables and for negatively correlated random variables.
Almost all of the results presented here have appeared previously, some,
however, only in recent conference publications. While the focus is on
collecting tools for the analysis of randomized search heuristics, many of
these may be useful as well in the analysis of classic randomized algorithms or
discrete random structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09780">
<title>Bounded Policy Synthesis for POMDPs with Safe-Reachability Objectives. (arXiv:1801.09780v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1801.09780</link>
<description rdf:parseType="Literal">&lt;p&gt;Planning robust executions under uncertainty is a fundamental challenge for
building autonomous robots. Partially Observable Markov Decision Processes
(POMDPs) provide a standard framework for modeling uncertainty in many robot
applications. A key algorithmic problem for POMDPs is policy synthesis. While
this problem has traditionally been posed w.r.t. optimality objectives, many
robot applications are better modeled by POMDPs where the objective is a
boolean requirement. In this paper, we study the latter problem in a setting
where the requirement is a safe-reachability property, which states that with a
probability above a certain threshold, it is possible to eventually reach a
goal state while satisfying a safety requirement. The central challenge in our
problem is that it requires reasoning over a vast space of probability
distributions. What&apos;s more, it has been shown that policy synthesis of POMDPs
with reachability objectives is undecidable in general. To address these
challenges, we introduce the notion of a goal-constrained belief space, which
only contains beliefs (probability distributions over states) reachable from
the initial belief under desired executions. This constrained space is
generally much smaller than the original belief space. Our approach compactly
represents this space over a bounded horizon using symbolic constraints, and
employs an incremental Satisfiability Modulo Theories (SMT) solver to
efficiently search for a valid policy over it. We evaluate our method using a
case study involving a partially observable robotics domain with uncertain
obstacles. Our results suggest that it is possible to synthesize policies over
large belief spaces with a small number of SMT solver calls by focusing on
goal-constrained belief space, and our method o ers a stronger guarantee of
both safety and reachability than alternative unconstrained/constrained POMDP
formulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1&quot;&gt;Swarat Chaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kavraki_L/0/1/0/all/0/1&quot;&gt;Lydia E. Kavraki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09788">
<title>Evaluating approaches for supervised semantic labeling. (arXiv:1801.09788v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.09788</link>
<description rdf:parseType="Literal">&lt;p&gt;Relational data sources are still one of the most popular ways to store
enterprise or Web data, however, the issue with relational schema is the lack
of a well-defined semantic description. A common ontology provides a way to
represent the meaning of a relational schema and can facilitate the integration
of heterogeneous data sources within a domain. Semantic labeling is achieved by
mapping attributes from the data sources to the classes and properties in the
ontology. We formulate this problem as a multi-class classification problem
where previously labeled data sources are used to learn rules for labeling new
data sources. The majority of existing approaches for semantic labeling have
focused on data integration challenges such as naming conflicts and semantic
heterogeneity. In addition, machine learning approaches typically have issues
around class imbalance, lack of labeled instances and relative importance of
attributes. To address these issues, we develop a new machine learning model
with engineered features as well as two deep learning models which do not
require extensive feature engineering. We evaluate our new approaches with the
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruemmele_N/0/1/0/all/0/1&quot;&gt;Natalia Ruemmele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyshetskiy_Y/0/1/0/all/0/1&quot;&gt;Yuriy Tyshetskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_A/0/1/0/all/0/1&quot;&gt;Alex Collins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09804">
<title>Predicting Rapid Fire Growth (Flashover) Using Conditional Generative Adversarial Networks. (arXiv:1801.09804v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.09804</link>
<description rdf:parseType="Literal">&lt;p&gt;A flashover occurs when a fire spreads very rapidly through crevices due to
intense heat. Flashovers present one of the most frightening and challenging
fire phenomena to those who regularly encounter them: firefighters.
Firefighters&apos; safety and lives often depend on their ability to predict
flashovers before they occur. Typical pre-flashover fire characteristics
include dark smoke, high heat, and rollover (&quot;angel fingers&quot;) and can be
quantified by color, size, and shape. Using a color video stream from a
firefighter&apos;s body camera, we applied generative adversarial neural networks
for image enhancement. The neural networks were trained to enhance very dark
fire and smoke patterns in videos and monitor dynamic changes in smoke and fire
areas. Preliminary tests with limited flashover training videos showed that we
predicted a flashover as early as 55 seconds before it occurred.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_K/0/1/0/all/0/1&quot;&gt;Kyongsik Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bustos_J/0/1/0/all/0/1&quot;&gt;Jessi Bustos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Thomas Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09808">
<title>The Intriguing Properties of Model Explanations. (arXiv:1801.09808v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.09808</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear approximations to the decision boundary of a complex model have become
one of the most popular tools for interpreting predictions. In this paper, we
study such linear explanations produced either post-hoc by a few recent methods
or generated along with predictions with contextual explanation networks
(CENs). We focus on two questions: (i) whether linear explanations are always
consistent or can be misleading, and (ii) when integrated into the prediction
process, whether and how explanations affect the performance of the model. Our
analysis sheds more light on certain properties of explanations produced by
different methods and suggests that learning models that explain and predict
jointly is often advantageous.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1&quot;&gt;Maruan Al-Shedivat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Avinava Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09810">
<title>Personalized Survival Prediction with Contextual Explanation Networks. (arXiv:1801.09810v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.09810</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and transparent prediction of cancer survival times on the level of
individual patients can inform and improve patient care and treatment
practices. In this paper, we design a model that concurrently learns to
accurately predict patient-specific survival distributions and to explain its
predictions in terms of patient attributes such as clinical tests or
assessments. Our model is flexible and based on a recurrent network, can handle
various modalities of data including temporal measurements, and yet constructs
and uses simple explanations in the form of patient- and time-specific linear
regression. For analysis, we use two publicly available datasets and show that
our networks outperform a number of baselines in prediction while providing a
way to inspect the reasons behind each prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1&quot;&gt;Maruan Al-Shedivat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Avinava Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09854">
<title>Algorithms for the Greater Good! On Mental Modeling and Acceptable Symbiosis in Human-AI Collaboration. (arXiv:1801.09854v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.09854</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective collaboration between humans and AI-based systems requires
effective modeling of the human in the loop, both in terms of the mental state
as well as the physical capabilities of the latter. However, these models can
also open up pathways for manipulating and exploiting the human in the hopes of
achieving some greater good, especially when the intent or values of the AI and
the human are not aligned or when they have an asymmetrical relationship with
respect to knowledge or computation power. In fact, such behavior does not
necessarily require any malicious intent but can rather be borne out of
cooperative scenarios. It is also beyond simple misinterpretation of intents,
as in the case of value alignment problems, and thus can be effectively
engineered if desired. Such techniques already exist and pose several
unresolved ethical and moral questions with regards to the design of autonomy.
In this paper, we illustrate some of these issues in a teaming scenario and
investigate how they are perceived by participants in a thought experiment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborti_T/0/1/0/all/0/1&quot;&gt;Tathagata Chakraborti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1&quot;&gt;Subbarao Kambhampati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09955">
<title>COBRA: A Fast and Simple Method for Active Clustering with Pairwise Constraints. (arXiv:1801.09955v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.09955</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering is inherently ill-posed: there often exist multiple valid
clusterings of a single dataset, and without any additional information a
clustering system has no way of knowing which clustering it should produce.
This motivates the use of constraints in clustering, as they allow users to
communicate their interests to the clustering system. Active constraint-based
clustering algorithms select the most useful constraints to query, aiming to
produce a good clustering using as few constraints as possible. We propose
COBRA, an active method that first over-clusters the data by running K-means
with a $K$ that is intended to be too large, and subsequently merges the
resulting small clusters into larger ones based on pairwise constraints. In its
merging step, COBRA is able to keep the number of pairwise queries low by
maximally exploiting constraint transitivity and entailment. We experimentally
show that COBRA outperforms the state of the art in terms of clustering quality
and runtime, without requiring the number of clusters in advance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Craenendonck_T/0/1/0/all/0/1&quot;&gt;Toon Van Craenendonck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumancic_S/0/1/0/all/0/1&quot;&gt;Sebastijan Dumancic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blockeel_H/0/1/0/all/0/1&quot;&gt;Hendrik Blockeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10055">
<title>Features, Projections, and Representation Change for Generalized Planning. (arXiv:1801.10055v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.10055</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized planning is concerned with the characterization and computation
of plans that solve many instances at once. In the standard formulation, a
generalized plan is a mapping from feature or observation histories into
actions, assuming that the instances share a common pool of features and
actions. This assumption, however, excludes the standard relational planning
domains where actions and objects change across instances. In this work, we
extend the formulation of generalized planning to such domains. This is
achieved by projecting the actions over the features, resulting in a common set
of abstract actions which can be tested for soundness and completeness, and
which can be used for generating general policies such as &quot;if the gripper is
empty, pick the clear block above x and place it on the table&quot; that achieve the
goal clear(x) in any Blocksworld instance. In this policy, &quot;pick the clear
block above x&quot; is an abstract action that may represent the action Unstack(a,
b) in one situation and the action Unstack(b, c) in another. Transformations
are also introduced for computing such policies by means of fully observable
non-deterministic (FOND) planners. The value of generalized representations for
learning general policies is also discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonet_B/0/1/0/all/0/1&quot;&gt;Blai Bonet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geffner_H/0/1/0/all/0/1&quot;&gt;Hector Geffner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.02000">
<title>Activation Maximization Generative Adversarial Nets. (arXiv:1703.02000v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.02000</link>
<description rdf:parseType="Literal">&lt;p&gt;Class labels have been empirically shown useful in improving the sample
quality of generative adversarial nets (GANs). In this paper, we mathematically
study the properties of the current variants of GANs that make use of class
label information. With class aware gradient and cross-entropy decomposition,
we reveal how class labels and associated losses influence GAN&apos;s training.
Based on that, we propose Activation Maximization Generative Adversarial
Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been
conducted to validate our analysis and evaluate the effectiveness of our
solution, where AM-GAN outperforms other strong baselines and achieves
state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we
demonstrate that, with the Inception ImageNet classifier, Inception Score
mainly tracks the diversity of the generator, and there is, however, no
reliable evidence that it can reflect the true sample quality. We thus propose
a new metric, called AM Score, to provide more accurate estimation on the
sample quality. Our proposed model also outperforms the baseline methods in the
new metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhiming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Han Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_S/0/1/0/all/0/1&quot;&gt;Shu Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.08475">
<title>Overcoming Catastrophic Forgetting by Incremental Moment Matching. (arXiv:1703.08475v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.08475</link>
<description rdf:parseType="Literal">&lt;p&gt;Catastrophic forgetting is a problem of neural networks that loses the
information of the first task after training the second task. Here, we propose
a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM
incrementally matches the moment of the posterior distribution of the neural
network which is trained on the first and the second task, respectively. To
make the search space of posterior parameter smooth, the IMM procedure is
complemented by various transfer learning techniques including weight transfer,
L2-norm of the old and the new parameter, and a variant of dropout with the old
parameter. We analyze our approach on a variety of datasets including the
MNIST, CIFAR-10, Caltech-UCSD-Birds, and Lifelog datasets. The experimental
results show that IMM achieves state-of-the-art performance by balancing the
information between an old and a new network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sang-Woo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jin-Hwa Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jun_J/0/1/0/all/0/1&quot;&gt;Jaehyun Jun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1&quot;&gt;Jung-Woo Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Byoung-Tak Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.10301">
<title>Contextual Explanation Networks. (arXiv:1705.10301v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.10301</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce contextual explanation networks (CENs)---a class of models that
learn to predict by generating and leveraging intermediate explanations. CENs
are deep networks that generate parameters for context-specific probabilistic
graphical models which are further used for prediction and play the role of
explanations. Contrary to the existing post-hoc model-explanation tools, CENs
learn to predict and to explain jointly. Our approach offers two major
advantages: (i) for each prediction, valid instance-specific explanations are
generated with no computational overhead and (ii) prediction via explanation
acts as a regularization and boosts performance in low-resource settings. We
prove that local approximations to the decision boundary of our networks are
consistent with the generated explanations. Our results on image and text
classification and survival analysis tasks demonstrate that CENs are
competitive with the state-of-the-art while offering additional insights behind
each prediction, valuable for decision support.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1&quot;&gt;Maruan Al-Shedivat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Avinava Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11089">
<title>Eigenoption Discovery through the Deep Successor Representation. (arXiv:1710.11089v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11089</link>
<description rdf:parseType="Literal">&lt;p&gt;Options in reinforcement learning allow agents to hierarchically decompose a
task into subtasks, having the potential to speed up learning and planning.
However, autonomously learning effective sets of options is still a major
challenge in the field. In this paper we focus on the recently introduced idea
of using representation learning methods to guide the option discovery process.
Specifically, we look at eigenoptions, options obtained from representations
that encode diffusive information flow in the environment. We extend the
existing algorithms for eigenoption discovery to settings with stochastic
transitions and in which handcrafted features are not available. We propose an
algorithm that discovers eigenoptions while learning non-linear state
representations from raw pixels. It exploits recent successes in the deep
reinforcement learning literature and the equivalence between proto-value
functions and the successor representation. We use traditional tabular domains
to provide intuition about our approach and Atari 2600 games to demonstrate its
potential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1&quot;&gt;Marlos C. Machado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenbaum_C/0/1/0/all/0/1&quot;&gt;Clemens Rosenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Miao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tesauro_G/0/1/0/all/0/1&quot;&gt;Gerald Tesauro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1&quot;&gt;Murray Campbell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07163">
<title>Dynamic Neural Program Embedding for Program Repair. (arXiv:1711.07163v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07163</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural program embeddings have shown much promise recently for a variety of
program analysis tasks, including program synthesis, program repair, fault
localization, etc. However, most existing program embeddings are based on
syntactic features of programs, such as raw token sequences or abstract syntax
trees. Unlike images and text, a program has an unambiguous semantic meaning
that can be difficult to capture by only considering its syntax (i.e.
syntactically similar pro- grams can exhibit vastly different run-time
behavior), which makes syntax-based program embeddings fundamentally limited.
This paper proposes a novel semantic program embedding that is learned from
program execution traces. Our key insight is that program states expressed as
sequential tuples of live variable values not only captures program semantics
more precisely, but also offer a more natural fit for Recurrent Neural Networks
to model. We evaluate different syntactic and semantic program embeddings on
predicting the types of errors that students make in their submissions to an
introductory programming class and two exercises on the CodeHunt education
platform. Evaluation results show that our new semantic program embedding
significantly outperforms the syntactic program embeddings based on token
sequences and abstract syntax trees. In addition, we augment a search-based
program repair system with the predictions obtained from our se- mantic
embedding, and show that search efficiency is also significantly improved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Ke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rishabh Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1&quot;&gt;Zhendong Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09819">
<title>Transformation Autoregressive Networks. (arXiv:1801.09819v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.09819</link>
<description rdf:parseType="Literal">&lt;p&gt;The fundamental task of general density estimation has been of keen interest
to machine learning. Recent advances in density estimation have either: a)
proposed a flexible model to estimate the conditional factors of the chain
rule, $p(x_{i}\, |\, x_{i-1}, \ldots)$; or b) used flexible, non-linear
transformations of variables of a simple base distribution. Instead, this work
jointly leverages transformations of variables and autoregressive conditional
models, and proposes novel methods for both. We provide a deeper understanding
of our methods, showing a considerable improvement through a comprehensive
study over both real world and synthetic data. Moreover, we illustrate the use
of our models in outlier detection and image modeling tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oliva_J/0/1/0/all/0/1&quot;&gt;Junier B. Oliva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Avinava Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnab&amp;#xe1;s P&amp;#xf3;czos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09827">
<title>Robustness of classification ability of spiking neural networks. (arXiv:1801.09827v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.09827</link>
<description rdf:parseType="Literal">&lt;p&gt;It is well-known that the robustness of artificial neural networks (ANNs) is
important for their wide ranges of applications. In this paper, we focus on the
robustness of the classification ability of a spiking neural network which
receives perturbed inputs. Actually, the perturbation is allowed to be
arbitrary styles. However, Gaussian perturbation and other regular ones have
been rarely investigated. For classification problems, the closer to the
desired point, the more perturbed points there are in the input space. In
addition, the perturbation may be periodic. Based on these facts, we only
consider sinusoidal and Gaussian perturbations in this paper. With the
SpikeProp algorithm, we perform extensive experiments on the classical XOR
problem and other three benchmark datasets. The numerical results show that
there is not significant reduction in the classification ability of the network
if the input signals are subject to sinusoidal and Gaussian perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pingping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09829">
<title>Weighted Community Detection and Data Clustering Using Message Passing. (arXiv:1801.09829v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/1801.09829</link>
<description rdf:parseType="Literal">&lt;p&gt;Grouping objects into clusters based on similarities or weights between them
is one of the most important problems in science and engineering. In this work,
by extending message passing algorithms and spectral algorithms proposed for
unweighted community detection problem, we develop a non-parametric method
based on statistical physics, by mapping the problem to Potts model at the
critical temperature of spin glass transition and applying belief propagation
to solve the marginals corresponding to the Boltzmann distribution. Our
algorithm is robust to over-fitting and gives a principled way to determine
whether there are significant clusters in the data and how many clusters there
are. We apply our method to different clustering tasks and use extensive
numerical experiments to illustrate the advantage of our method over existing
algorithms. In the community detection problem in weighted and directed
networks, we show that our algorithm significantly outperforms existing
algorithms. In the clustering problem when the data was generated by mixture
models in the sparse regime we show that our method works to the theoretical
limit of detectability and gives accuracy very close to that of the optimal
Bayesian inference. In the semi-supervised clustering problem, our method only
needs several labels to work perfectly in classic datasets. Finally, we further
develop Thouless-Anderson-Palmer equations which reduce heavily the computation
complexity in dense-networks but gives almost the same performance as belief
propagation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Cheng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09834">
<title>Mixture Proportion Estimation for Positive--Unlabeled Learning via Classifier Dimension Reduction. (arXiv:1801.09834v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1801.09834</link>
<description rdf:parseType="Literal">&lt;p&gt;Positive--unlabeled (PU) learning considers two samples, a positive set $P$
with observations from only one class and an unlabeled set $U$ with
observations from two classes. The goal is to classify observations in $U$.
Class mixture proportion estimation (MPE) in $U$ is a key step in PU learning.
In this paper, we show that PU learning is a generalization of local False
Discovery Rate estimation. Further we show that PU learning MPE can be reduced
to a one--dimensional problem via construction of a classifier trained on the
$P$ and $U$ data sets. These observations enable application of methodology
from the multiple testing literature to the PU learning problem. In particular
we adapt ideas from Storey [2002] and Patra and Sen [2015] to address parameter
identifiability and MPE. We prove consistency of two mixture proportion
estimators using bounds from empirical process theory, develop tuning parameter
free implementations, and demonstrate that they have competitive performance on
simulated waveform data and a protein signaling problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhenfeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Long_J/0/1/0/all/0/1&quot;&gt;James P. Long&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09870">
<title>Fast Power system security analysis with Guided Dropout. (arXiv:1801.09870v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.09870</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new method to efficiently compute load-flows (the steady-state
of the power-grid for given productions, consumptions and grid topology),
substituting conventional simulators based on differential equation solvers. We
use a deep feed-forward neural network trained with load-flows precomputed by
simulation. Our architecture permits to train a network on so-called &quot;n-1&quot;
problems, in which load flows are evaluated for every possible line
disconnection, then generalize to &quot;n-2&quot; problems without retraining (a clear
advantage because of the combinatorial nature of the problem). To that end, we
developed a technique bearing similarity with &quot;dropout&quot;, which we named &quot;guided
dropout&quot;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Donnot_B/0/1/0/all/0/1&quot;&gt;Benjamin Donnot&lt;/a&gt; (1, 2), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guyon_I/0/1/0/all/0/1&quot;&gt;Isabelle Guyon&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schoenauer_M/0/1/0/all/0/1&quot;&gt;Marc Schoenauer&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marot_A/0/1/0/all/0/1&quot;&gt;Antoine Marot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Panciatici_P/0/1/0/all/0/1&quot;&gt;Patrick Panciatici&lt;/a&gt; ((1) TAU, (2) LRI)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10108">
<title>Error estimates for spectral convergence of the graph Laplacian on random geometric graphs towards the Laplace--Beltrami operator. (arXiv:1801.10108v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.10108</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the convergence of the graph Laplacian of a random geometric graph
generated by an i.i.d. sample from a $m$-dimensional submanifold $M$ in $R^d$
as the sample size $n$ increases and the neighborhood size $h$ tends to zero.
We show that eigenvalues and eigenvectors of the graph Laplacian converge with
a rate of $O\Big(\big(\frac{\log n}{n}\big)^\frac{1}{2m}\Big)$ to the
eigenvalues and eigenfunctions of the weighted Laplace-Beltrami operator of
$M$.
&lt;/p&gt;
&lt;p&gt;No information on the submanifold $M$ is needed in the construction of the
graph or the &quot;out-of-sample extension&quot; of the eigenvectors. Of independent
interest is a generalization of the rate of convergence of empirical measures
on submanifolds in $R^d$ in infinity transportation distance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trillos_N/0/1/0/all/0/1&quot;&gt;Nicolas Garcia Trillos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gerlach_M/0/1/0/all/0/1&quot;&gt;Moritz Gerlach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hein_M/0/1/0/all/0/1&quot;&gt;Matthias Hein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Slepcev_D/0/1/0/all/0/1&quot;&gt;Dejan Slepcev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10123">
<title>Links: A High-Dimensional Online Clustering Method. (arXiv:1801.10123v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.10123</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel algorithm, called Links, designed to perform online
clustering on unit vectors in a high-dimensional Euclidean space. The algorithm
is appropriate when it is necessary to cluster data efficiently as it streams
in, and is to be contrasted with traditional batch clustering algorithms that
have access to all data at once. For example, Links has been successfully
applied to embedding vectors generated from face images or voice recordings for
the purpose of recognizing people, thereby providing real-time identification
during video or audio capture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mansfield_P/0/1/0/all/0/1&quot;&gt;Philip Andrew Mansfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Downey_C/0/1/0/all/0/1&quot;&gt;Carlton Downey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wan_L/0/1/0/all/0/1&quot;&gt;Li Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moreno_I/0/1/0/all/0/1&quot;&gt;Ignacio Lopez Moreno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10130">
<title>Spherical CNNs. (arXiv:1801.10130v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.10130</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) have become the method of choice for
learning problems involving 2D planar images. However, a number of problems of
recent interest have created a demand for models that can analyze spherical
images. Examples include omnidirectional vision for drones, robots, and
autonomous cars, molecular regression problems, and global weather and climate
modelling. A naive application of convolutional networks to a planar projection
of the spherical signal is destined to fail, because the space-varying
distortions introduced by such a projection will make translational weight
sharing ineffective.
&lt;/p&gt;
&lt;p&gt;In this paper we introduce the building blocks for constructing spherical
CNNs. We propose a definition for the spherical cross-correlation that is both
expressive and rotation-equivariant. The spherical correlation satisfies a
generalized Fourier theorem, which allows us to compute it efficiently using a
generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We
demonstrate the computational efficiency, numerical accuracy, and effectiveness
of spherical CNNs applied to 3D model recognition and atomization energy
regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1&quot;&gt;Taco S. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1&quot;&gt;Mario Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koehler_J/0/1/0/all/0/1&quot;&gt;Jonas Koehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1603.02644">
<title>Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling. (arXiv:1603.02644v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1603.02644</link>
<description rdf:parseType="Literal">&lt;p&gt;We study parameter inference in large-scale latent variable models. We first
propose an unified treatment of online inference for latent variable models
from a non-canonical exponential family, and draw explicit links between
several previously proposed frequentist or Bayesian methods. We then propose a
novel inference method for the frequentist estimation of parameters, that
adapts MCMC methods to online inference of latent variable models with the
proper use of local Gibbs sampling. Then, for latent Dirich-let allocation,we
provide an extensive set of experiments and comparisons with existing work,
where our new approach outperforms all previously proposed methods. In
particular, using Gibbs sampling for latent variable inference is superior to
variational inference in terms of test log-likelihoods. Moreover, Bayesian
inference through variational methods perform poorly, sometimes leading to
worse fits with latent variables of higher dimensionality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1&quot;&gt;Christophe Dupuy&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis Bach&lt;/a&gt; (2, 1) ((1) SIERRA, (2) LIENS)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.04599">
<title>Generalized Self-Concordant Functions: A Recipe for Newton-Type Methods. (arXiv:1703.04599v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1703.04599</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the smooth structure of convex functions by generalizing a powerful
concept so-called self-concordance introduced by Nesterov and Nemirovskii in
the early 1990s to a broader class of convex functions, which we call
generalized self-concordant functions. This notion allows us to develop a
unified framework for designing Newton-type methods to solve convex optimiza-
tion problems. The proposed theory provides a mathematical tool to analyze both
local and global convergence of Newton-type methods without imposing
unverifiable assumptions as long as the un- derlying functionals fall into our
generalized self-concordant function class. First, we introduce the class of
generalized self-concordant functions, which covers standard self-concordant
functions as a special case. Next, we establish several properties and key
estimates of this function class, which can be used to design numerical
methods. Then, we apply this theory to develop several Newton-type methods for
solving a class of smooth convex optimization problems involving the
generalized self- concordant functions. We provide an explicit step-size for
the damped-step Newton-type scheme which can guarantee a global convergence
without performing any globalization strategy. We also prove a local quadratic
convergence of this method and its full-step variant without requiring the
Lipschitz continuity of the objective Hessian. Then, we extend our result to
develop proximal Newton-type methods for a class of composite convex
minimization problems involving generalized self-concordant functions. We also
achieve both global and local convergence without additional assumption.
Finally, we verify our theoretical results via several numerical examples, and
compare them with existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tianxiao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tran_Dinh_Q/0/1/0/all/0/1&quot;&gt;Quoc Tran-Dinh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.09866">
<title>Machine learning for graph-based representations of three-dimensional discrete fracture networks. (arXiv:1705.09866v4 [physics.geo-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1705.09866</link>
<description rdf:parseType="Literal">&lt;p&gt;Structural and topological information play a key role in modeling flow and
transport through fractured rock in the subsurface. Discrete fracture network
(DFN) computational suites such as dfnWorks are designed to simulate flow and
transport in such porous media. Flow and transport calculations reveal that a
small backbone of fractures exists, where most flow and transport occurs.
Restricting the flowing fracture network to this backbone provides a
significant reduction in the network&apos;s effective size. However, the particle
tracking simulations needed to determine the reduction are computationally
intensive. Such methods may be impractical for large systems or for robust
uncertainty quantification of fracture networks, where thousands of forward
simulations are needed to bound system behavior.
&lt;/p&gt;
&lt;p&gt;In this paper, we develop an alternative network reduction approach to
characterizing transport in DFNs, by combining graph theoretical and machine
learning methods. We consider a graph representation where nodes signify
fractures and edges denote their intersections. Using random forest and support
vector machines, we rapidly identify a subnetwork that captures the flow
patterns of the full DFN, based primarily on node centrality features in the
graph. Our supervised learning techniques train on particle-tracking backbone
paths found by dfnWorks, but run in negligible time compared to those
simulations. We find that our predictions can reduce the network to
approximately 20% of its original size, while still generating breakthrough
curves consistent with those of the original network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Valera_M/0/1/0/all/0/1&quot;&gt;Manuel Valera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kelly_P/0/1/0/all/0/1&quot;&gt;Priscilla Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Matz_S/0/1/0/all/0/1&quot;&gt;Sean Matz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cantu_V/0/1/0/all/0/1&quot;&gt;Vito Adrian Cantu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Percus_A/0/1/0/all/0/1&quot;&gt;Allon G. Percus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hyman_J/0/1/0/all/0/1&quot;&gt;Jeffrey D. Hyman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Srinivasan_G/0/1/0/all/0/1&quot;&gt;Gowri Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Viswanathan_H/0/1/0/all/0/1&quot;&gt;Hari S. Viswanathan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.06150">
<title>A Comparison of Resampling and Recursive Partitioning Methods in Random Forest for Estimating the Asymptotic Variance Using the Infinitesimal Jackknife. (arXiv:1706.06150v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.06150</link>
<description rdf:parseType="Literal">&lt;p&gt;The infinitesimal jackknife (IJ) has recently been applied to the random
forest to estimate its prediction variance. These theorems were verified under
a traditional random forest framework which uses classification and regression
trees (CART) and bootstrap resampling. However, random forests using
conditional inference (CI) trees and subsampling have been found to be not
prone to variable selection bias. Here, we conduct simulation experiments using
a novel approach to explore the applicability of the IJ to random forests using
variations on the resampling method and base learner. Test data points were
simulated and each trained using random forest on one hundred simulated
training data sets using different combinations of resampling and base
learners. Using CI trees instead of traditional CART trees as well as using
subsampling instead of bootstrap sampling resulted in a much more accurate
estimation of prediction variance when using the IJ. The random forest
variations here have been incorporated into an open source software package for
the R programming language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brokamp_C/0/1/0/all/0/1&quot;&gt;Cole Brokamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rao_M/0/1/0/all/0/1&quot;&gt;MB Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ryan_P/0/1/0/all/0/1&quot;&gt;Patrick Ryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jandarov_R/0/1/0/all/0/1&quot;&gt;Roman Jandarov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.04340">
<title>Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition. (arXiv:1710.04340v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.04340</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral decomposition of the Koopman operator is attracting attention as a
tool for the analysis of nonlinear dynamical systems. Dynamic mode
decomposition is a popular numerical algorithm for Koopman spectral analysis;
however, we often need to prepare nonlinear observables manually according to
the underlying dynamics, which is not always possible since we may not have any
a priori knowledge about them. In this paper, we propose a fully data-driven
method for Koopman spectral analysis based on the principle of learning Koopman
invariant subspaces from observed data. To this end, we propose minimization of
the residual sum of squares of linear least-squares regression to estimate a
set of functions that transforms data into a form in which the linear
regression fits well. We introduce an implementation with neural networks and
evaluate performance empirically using nonlinear dynamical systems and
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeishi_N/0/1/0/all/0/1&quot;&gt;Naoya Takeishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawahara_Y/0/1/0/all/0/1&quot;&gt;Yoshinobu Kawahara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yairi_T/0/1/0/all/0/1&quot;&gt;Takehisa Yairi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.08310">
<title>AutoEncoder Inspired Unsupervised Feature Selection. (arXiv:1710.08310v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.08310</link>
<description rdf:parseType="Literal">&lt;p&gt;High-dimensional data in many areas such as computer vision and machine
learning tasks brings in computational and analytical difficulty. Feature
selection which selects a subset from observed features is a widely used
approach for improving performance and effectiveness of machine learning models
with high-dimensional data. In this paper, we propose a novel AutoEncoder
Feature Selector (AEFS) for unsupervised feature selection which combines
autoencoder regression and group lasso tasks. Compared to traditional feature
selection methods, AEFS can select the most important features by excavating
both linear and nonlinear information among features, which is more flexible
than the conventional self-representation method for unsupervised feature
selection with only linear assumptions. Experimental results on benchmark
dataset show that the proposed method is superior to the state-of-the-art
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kai Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03189">
<title>Deep Hyperspherical Learning. (arXiv:1711.03189v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03189</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolution as inner product has been the founding basis of convolutional
neural networks (CNNs) and the key to end-to-end visual representation
learning. Benefiting from deeper architectures, recent CNNs have demonstrated
increasingly strong representation abilities. Despite such improvement, the
increased depth and larger parameter space have also led to challenges in
properly training a network. In light of such challenges, we propose
hyperspherical convolution (SphereConv), a novel learning framework that gives
angular representations on hyperspheres. We introduce SphereNet, deep
hyperspherical convolution networks that are distinct from conventional inner
product based convolutional networks. In particular, SphereNet adopts
SphereConv as its basic convolution operator and is supervised by generalized
angular softmax loss - a natural loss formulation under SphereConv. We show
that SphereNet can effectively encode discriminative representation and
alleviate training difficulty, leading to easier optimization, faster
convergence and comparable (even better) classification accuracy over
convolutional counterparts. We also provide some theoretical insights for the
advantages of learning on hyperspheres. In addition, we introduce the learnable
SphereConv, i.e., a natural improvement over prefixed SphereConv, and
SphereNorm, i.e., hyperspherical learning as a normalization method.
Experiments have verified our conclusions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan-Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiding Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Le Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03634">
<title>Alternating minimization for dictionary learning with random initialization. (arXiv:1711.03634v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03634</link>
<description rdf:parseType="Literal">&lt;p&gt;We present theoretical guarantees for an alternating minimization algorithm
for the dictionary learning/sparse coding problem. The dictionary learning
problem is to factorize vector samples $y^{1},y^{2},\ldots, y^{n}$ into an
appropriate basis (dictionary) $A^*$ and sparse vectors $x^{1*},\ldots,x^{n*}$.
Our algorithm is a simple alternating minimization procedure that switches
between $\ell_1$ minimization and gradient descent in alternate steps.
Dictionary learning and specifically alternating minimization algorithms for
dictionary learning are well studied both theoretically and empirically.
However, in contrast to previous theoretical analyses for this problem, we
replace the condition on the operator norm (that is, the largest magnitude
singular value) of the true underlying dictionary $A^*$ with a condition on the
matrix infinity norm (that is, the largest magnitude term). This not only
allows us to get convergence rates for the error of the estimated dictionary
measured in the matrix infinity norm, but also ensures that a random
initialization will provably converge to the global optimum. Our guarantees are
under a reasonable generative model that allows for dictionaries with growing
operator norms, and can handle an arbitrary level of overcompleteness, while
having sparsity that is information theoretically optimal. We also establish
upper bounds on the sample complexity of our algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1&quot;&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1&quot;&gt;Peter L. Bartlett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07883">
<title>Deep Learning for Sentiment Analysis : A Survey. (arXiv:1801.07883v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07883</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has emerged as a powerful machine learning technique that
learns multiple layers of representations or features of the data and produces
state-of-the-art prediction results. Along with the success of deep learning in
many other application domains, deep learning is also popularly used in
sentiment analysis in recent years. This paper first gives an overview of deep
learning and then provides a comprehensive survey of its current applications
in sentiment analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bing Liu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>