<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00768"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00815"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03357"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00732"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04403"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00681"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00684"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00779"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00792"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00795"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00864"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00891"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00920"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00921"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00982"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01016"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01050"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01071"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1501.02102"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07006"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.09285"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10210"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08421"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06246"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07200"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11157"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11262"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.00768">
<title>A Bi-population Particle Swarm Optimizer for Learning Automata based Slow Intelligent System. (arXiv:1804.00768v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.00768</link>
<description rdf:parseType="Literal">&lt;p&gt;Particle Swarm Optimization (PSO) is an Evolutionary Algorithm (EA) that
utilizes a swarm of particles to solve an optimization problem. Slow
Intelligence System (SIS) is a learning framework which slowly learns the
solution to a problem performing a series of operations. Moreover, Learning
Automata (LA) are minuscule but effective decision making entities which are
best suited to act as a controller component. In this paper, we combine two
isolate populations of PSO to forge the Adaptive Intelligence Optimizer (AIO)
which harnesses the advantages of a bi-population PSO to escape from the local
minimum and avoid premature convergence. Furthermore, using the rich framework
of SIS and the nifty control theory that LA derived from, we find the perfect
matching between SIS and LA where acting slowly is the pillar of both of them.
Both SIS and LA need time to converge to the optimal decision where this
enables AIO to outperform standard PSO having an incomparable performance on
evolutionary optimization benchmark functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mofrad_M/0/1/0/all/0/1&quot;&gt;Mohammad Hasanzadeh Mofrad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;S. K. Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00815">
<title>Convolutional Neural Networks Regularized by Correlated Noise. (arXiv:1804.00815v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00815</link>
<description rdf:parseType="Literal">&lt;p&gt;Neurons in the visual cortex are correlated in their variability. The
presence of correlation impacts cortical processing because noise cannot be
averaged out over many neurons. In an effort to understand the functional
purpose of correlated variability, we implement and evaluate correlated noise
models in deep convolutional neural networks. Inspired by the cortex,
correlation is defined as a function of the distance between neurons and their
selectivity. We show how to sample from high-dimensional correlated
distributions while keeping the procedure differentiable, so that
back-propagation can proceed as usual. The impact of correlated variability is
evaluated on the classification of occluded and non-occluded images with and
without the presence of other regularization techniques, such as dropout. More
work is needed to understand the effects of correlations in various conditions,
however in 10/12 of the cases we studied, the best performance on occluded
images was obtained from a model with correlated noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Shamak Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tripp_B/0/1/0/all/0/1&quot;&gt;Bryan Tripp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1&quot;&gt;Graham Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03357">
<title>Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz. (arXiv:1711.03357v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03357</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper demonstrates a method for tensorizing neural networks based upon
an efficient way of approximating scale invariant quantum states, the
Multi-scale Entanglement Renormalization Ansatz (MERA). We employ MERA as a
replacement for the fully connected layers in a convolutional neural network
and test this implementation on the CIFAR-10 and CIFAR-100 datasets. The
proposed method outperforms factorization using tensor trains, providing
greater compression for the same level of accuracy and greater accuracy for the
same level of compression. We demonstrate MERA layers with 14000 times fewer
parameters and a reduction in accuracy of less than 1% compared to the
equivalent fully connected layers, scaling like O(N).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallam_A/0/1/0/all/0/1&quot;&gt;Andrew Hallam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1&quot;&gt;Edward Grant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stojevic_V/0/1/0/all/0/1&quot;&gt;Vid Stojevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Severini_S/0/1/0/all/0/1&quot;&gt;Simone Severini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Green_A/0/1/0/all/0/1&quot;&gt;Andrew G. Green&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00732">
<title>Speaker-Invariant Training via Adversarial Learning. (arXiv:1804.00732v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1804.00732</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel adversarial multi-task learning scheme, aiming at actively
curtailing the inter-talker feature variability while maximizing its senone
discriminability so as to enhance the performance of a deep neural network
(DNN) based ASR system. We call the scheme speaker-invariant training (SIT). In
SIT, a DNN acoustic model and a speaker classifier network are jointly
optimized to minimize the senone (tied triphone state) classification loss, and
simultaneously mini-maximize the speaker classification loss. A
speaker-invariant and senone-discriminative deep feature is learned through
this adversarial multi-task learning. With SIT, a canonical DNN acoustic model
with significantly reduced variance in its output probabilities is learned with
no explicit speaker-independent (SI) transformations or speaker-specific
representations used in training or testing. Evaluated on the CHiME-3 dataset,
the SIT achieves 4.99% relative word error rate (WER) improvement over the
conventional SI acoustic model. With additional unsupervised speaker
adaptation, the speaker-adapted (SA) SIT model achieves 4.86% relative WER gain
over the SA SI acoustic model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Zhong Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mazalov_V/0/1/0/all/0/1&quot;&gt;Vadim Mazalov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yifan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Biing-Hwang/0/1/0/all/0/1&quot;&gt;Biing-Hwang&lt;/a&gt; (Fred) &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Juang/0/1/0/all/0/1&quot;&gt;Juang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10386">
<title>Dual Skipping Networks. (arXiv:1710.10386v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10386</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the recent neuroscience studies on the left-right asymmetry of
the human brain in processing low and high spatial frequency information, this
paper introduces a dual skipping network which carries out coarse-to-fine
object categorization. Such a network has two branches to simultaneously deal
with both coarse and fine-grained classification tasks. Specifically, we
propose a layer-skipping mechanism that learns a gating network to predict
which layers to skip in the testing stage. This layer-skipping mechanism endows
the network with good flexibility and capability in practice. Evaluations are
conducted on several widely used coarse-to-fine object categorization
benchmarks, and promising results are achieved by our proposed network model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Changmao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yanwei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wenlian Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jianfeng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiangyang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04403">
<title>TVAE: Triplet-Based Variational Autoencoder using Metric Learning. (arXiv:1802.04403v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04403</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep metric learning has been demonstrated to be highly effective in learning
semantic representation and encoding information that can be used to measure
data similarity, by relying on the embedding learned from metric learning. At
the same time, variational autoencoder (VAE) has widely been used to
approximate inference and proved to have a good performance for directed
probabilistic models. However, for traditional VAE, the data label or feature
information are intractable. Similarly, traditional representation learning
approaches fail to represent many salient aspects of the data. In this project,
we propose a novel integrated framework to learn latent embedding in VAE by
incorporating deep metric learning. The features are learned by optimizing a
triplet loss on the mean vectors of VAE in conjunction with standard evidence
lower bound (ELBO) of VAE. This approach, which we call Triplet based
Variational Autoencoder (TVAE), allows us to capture more fine-grained
information in the latent embedding. Our model is tested on MNIST data set and
achieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma &amp;amp;
Welling, 2013) achieves triplet accuracy of 75.08%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ishfaq_H/0/1/0/all/0/1&quot;&gt;Haque Ishfaq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hoogi_A/0/1/0/all/0/1&quot;&gt;Assaf Hoogi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rubin_D/0/1/0/all/0/1&quot;&gt;Daniel Rubin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00681">
<title>Stochastic EM for Shuffled Linear Regression. (arXiv:1804.00681v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.00681</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of inference in a linear regression model in which
the relative ordering of the input features and output labels is not known.
Such datasets naturally arise from experiments in which the samples are
shuffled or permuted during the protocol. In this work, we propose a framework
that treats the unknown permutation as a latent variable. We maximize the
likelihood of observations using a stochastic expectation-maximization (EM)
approach. We compare this to the dominant approach in the literature, which
corresponds to hard EM in our framework. We show on synthetic data that the
stochastic EM algorithm we develop has several advantages, including lower
parameter error, less sensitivity to the choice of initialization, and
significantly better performance on datasets that are only partially shuffled.
We conclude by performing two experiments on real datasets that have been
partially shuffled, in which we show that the stochastic EM algorithm can
recover the weights with modest error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abid_A/0/1/0/all/0/1&quot;&gt;Abubakar Abid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00684">
<title>Graph-Based Deep Modeling and Real Time Forecasting of Sparse Spatio-Temporal Data. (arXiv:1804.00684v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00684</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a generic framework for spatio-temporal (ST) data modeling,
analysis, and forecasting, with a special focus on data that is sparse in both
space and time. Our multi-scaled framework is a seamless coupling of two major
components: a self-exciting point process that models the macroscale
statistical behaviors of the ST data and a graph structured recurrent neural
network (GSRNN) to discover the microscale patterns of the ST data on the
inferred graph. This novel deep neural network (DNN) incorporates the real time
interactions of the graph nodes to enable more accurate real time forecasting.
The effectiveness of our method is demonstrated on both crime and traffic
forecasting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiyang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fangbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1&quot;&gt;Baichuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertozzi_A/0/1/0/all/0/1&quot;&gt;Andrea L. Bertozzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brantingham_P/0/1/0/all/0/1&quot;&gt;P. Jeffrey Brantingham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00779">
<title>Neural Autoregressive Flows. (arXiv:1804.00779v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00779</link>
<description rdf:parseType="Literal">&lt;p&gt;Normalizing flows and autoregressive models have been successfully combined
to produce state-of-the-art results in density estimation, via Masked
Autoregressive Flows (MAF), and to accelerate state-of-the-art WaveNet-based
speech synthesis to 20x faster than real-time, via Inverse Autoregressive Flows
(IAF). We unify and generalize these approaches, replacing the (conditionally)
affine univariate transformations of MAF/IAF with a more general class of
invertible univariate transformations expressed as monotonic neural networks.
We demonstrate that the proposed neural autoregressive flows (NAF) are
universal approximators for continuous probability distributions, and their
greater expressivity allows them to better capture multimodal target
distributions. Experimentally, NAF yields state-of-the-art performance on a
suite of density estimation tasks and outperforms IAF in variational
autoencoders trained on binarized MNIST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chin-Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1&quot;&gt;David Krueger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacoste_A/0/1/0/all/0/1&quot;&gt;Alexandre Lacoste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00792">
<title>Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks. (arXiv:1804.00792v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00792</link>
<description rdf:parseType="Literal">&lt;p&gt;Data poisoning is a type of adversarial attack on machine learning models
wherein the attacker adds examples to the training set to manipulate the
behavior of the model at test time. This paper explores a broad class of
poisoning attacks on neural nets. The proposed attacks use &quot;clean-labels&quot;; they
don&apos;t require the attacker to have any control over the labeling of training
data. They are also targeted; they control the behavior of the classifier on a
specific test instance without noticeably degrading classifier performance on
other instances.
&lt;/p&gt;
&lt;p&gt;For example, an attacker could add a seemingly innocuous image (that is
properly labeled) to a training set for a face recognition engine, and control
the identity of a chosen person at test time. Because the attacker does not
need to control the labeling function, poisons could be entered into the
training set simply by putting them online and waiting for them to be scraped
by a data collection bot.
&lt;/p&gt;
&lt;p&gt;We present an optimization-based method for crafting poisons, and show that
just one single poison image can control classifier behavior when transfer
learning is used. For full end-to-end training, we present a &quot;watermarking&quot;
strategy that makes poisoning reliable using multiple (~50) poisoned training
instances. We demonstrate our method by generating poisoned frog images from
the CIFAR dataset and using them to manipulate image classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafahi_A/0/1/0/all/0/1&quot;&gt;Ali Shafahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;W. Ronny Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najibi_M/0/1/0/all/0/1&quot;&gt;Mahyar Najibi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suciu_O/0/1/0/all/0/1&quot;&gt;Octavian Suciu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Studer_C/0/1/0/all/0/1&quot;&gt;Christoph Studer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumitras_T/0/1/0/all/0/1&quot;&gt;Tudor Dumitras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00795">
<title>Estimation of Markov Chain via Rank-constrained Likelihood. (arXiv:1804.00795v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.00795</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the recovery and state compression of low-rank Markov
chains from empirical trajectories. We propose a non-convex estimator based on
rank-constrained likelihood maximization. Statistical upper bounds are provided
for the Kullback-Leiber divergence and the $\ell_2$ risk between the estimator
and the true transition matrix. The estimator reveals a compressed state space
of the Markov chain. We also develop a novel DC (difference of convex function)
programming algorithm to tackle the rank-constrained non-smooth optimization
problem. Convergence results are established. Experiments with taxi trip data
show that the estimator is able to identify the zoning of Manhattan city.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xudong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anru Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00864">
<title>Adaptive distributed methods under communication constraints. (arXiv:1804.00864v1 [math.ST])</title>
<link>http://arxiv.org/abs/1804.00864</link>
<description rdf:parseType="Literal">&lt;p&gt;We study distributed estimation methods under communication constraints in a
distributed version of the nonparametric signal-in-white-noise model. We derive
minimax lower bounds and exhibit methods that attain those bounds. Moreover, we
show that adaptive estimation is possible in this setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Szabo_B/0/1/0/all/0/1&quot;&gt;Botond Szabo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zanten_H/0/1/0/all/0/1&quot;&gt;Harry van Zanten&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00891">
<title>Hyperspherical Variational Auto-Encoders. (arXiv:1804.00891v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.00891</link>
<description rdf:parseType="Literal">&lt;p&gt;The Variational Auto-Encoder (VAE) is one of the most used unsupervised
machine learning models. But although the default choice of a Gaussian
distribution for both the prior and posterior represents a mathematically
convenient distribution often leading to competitive results, we show that this
parameterization fails to model data with a latent hyperspherical structure. To
address this issue we propose using a von Mises-Fisher (vMF) distribution
instead, leading to a hyperspherical latent space. Through a series of
experiments we show how such a hyperspherical VAE, or $\mathcal{S}$-VAE, is
more suitable for capturing data with a hyperspherical latent structure, while
outperforming a normal, $\mathcal{N}$-VAE, in low dimensions on other data
types.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Davidson_T/0/1/0/all/0/1&quot;&gt;Tim R. Davidson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Falorsi_L/0/1/0/all/0/1&quot;&gt;Luca Falorsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cao_N/0/1/0/all/0/1&quot;&gt;Nicola De Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kipf_T/0/1/0/all/0/1&quot;&gt;Thomas Kipf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tomczak_J/0/1/0/all/0/1&quot;&gt;Jakub M. Tomczak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00920">
<title>Speech waveform synthesis from MFCC sequences with generative adversarial networks. (arXiv:1804.00920v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1804.00920</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a method for generating speech from filterbank mel
frequency cepstral coefficients (MFCC), which are widely used in speech
applications, such as ASR, but are generally considered unusable for speech
synthesis. First, we predict fundamental frequency and voicing information from
MFCCs with an autoregressive recurrent neural net. Second, the spectral
envelope information contained in MFCCs is converted to all-pole filters, and a
pitch-synchronous excitation model matched to these filters is trained.
Finally, we introduce a generative adversarial network -based noise model to
add a realistic high-frequency stochastic component to the modeled excitation
signal. The results show that high quality speech reconstruction can be
obtained, given only MFCC information at test time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Juvela_L/0/1/0/all/0/1&quot;&gt;Lauri Juvela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bollepalli_B/0/1/0/all/0/1&quot;&gt;Bajibabu Bollepalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kameoka_H/0/1/0/all/0/1&quot;&gt;Hirokazu Kameoka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Airaksinen_M/0/1/0/all/0/1&quot;&gt;Manu Airaksinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alku_P/0/1/0/all/0/1&quot;&gt;Paavo Alku&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00921">
<title>DeSIGN: Design Inspiration from Generative Networks. (arXiv:1804.00921v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00921</link>
<description rdf:parseType="Literal">&lt;p&gt;Can an algorithm create original and compelling fashion designs to serve as
an inspirational assistant? To help answer this question, we design and
investigate different image generation models associated with different loss
functions to boost creativity in fashion generation. The dimensions of our
explorations include: (i) different Generative Adversarial Networks
architectures that start from noise vectors to generate fashion items, (ii) a
new loss function that encourages creativity, and (iii) a generation process
following the key elements of fashion design (disentangling shape and texture
makers). A key challenge of this study is the evaluation of generated designs
and the retrieval of best ones, hence we put together an evaluation protocol
associating automatic metrics and human experimental studies that we hope will
help ease future research. We show that our proposed creativity loss yields
better overall appreciation than the one employed in Creative Adversarial
Networks. In the end, about 61% of our images are thought to be created by
human designers rather than by a computer while also being considered original
per our human subject experiments, and our proposed loss scores the highest
compared to existing losses in both novelty and likability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sbai_O/0/1/0/all/0/1&quot;&gt;Othman Sbai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1&quot;&gt;Mohamed Elhoseiny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bordes_A/0/1/0/all/0/1&quot;&gt;Antoine Bordes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1&quot;&gt;Yann LeCun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couprie_C/0/1/0/all/0/1&quot;&gt;Camille Couprie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00982">
<title>360{\deg} Stance Detection. (arXiv:1804.00982v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.00982</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of fake news and filter bubbles makes it increasingly
difficult to form an unbiased, balanced opinion towards a topic. To ameliorate
this, we propose 360{\deg} Stance Detection, a tool that aggregates news with
multiple perspectives on a topic. It presents them on a spectrum ranging from
support to opposition, enabling the user to base their opinion on multiple
pieces of diverse evidence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1&quot;&gt;Sebastian Ruder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glover_J/0/1/0/all/0/1&quot;&gt;John Glover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehrabani_A/0/1/0/all/0/1&quot;&gt;Afshin Mehrabani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghaffari_P/0/1/0/all/0/1&quot;&gt;Parsa Ghaffari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01016">
<title>Large-Scale Cox Process Inference using Variational Fourier Features. (arXiv:1804.01016v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.01016</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian process modulated Poisson processes provide a flexible framework for
modelling spatiotemporal point patterns. So far this had been restricted to one
dimension, binning to a pre-determined grid, or small data sets of up to a few
thousand data points. Here we introduce Cox process inference based on Fourier
features. This sparse representation induces global rather than local
constraints on the function space and is computationally efficient. This allows
us to formulate a grid-free approximation that scales well with the number of
data points and the size of the domain. We demonstrate that this allows MCMC
approximations to the non-Gaussian posterior. We also find that, in practice,
Fourier features have more consistent optimization behavior than previous
approaches. Our approximate Bayesian method can fit over 100,000 events with
complex spatiotemporal patterns in three dimensions on a single GPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+John_S/0/1/0/all/0/1&quot;&gt;S.T. John&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hensman_J/0/1/0/all/0/1&quot;&gt;James Hensman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01050">
<title>Training VAEs Under Structured Residuals. (arXiv:1804.01050v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.01050</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational auto-encoders (VAEs) are a popular and powerful deep generative
model. Previous works on VAEs have assumed a factorised likelihood model,
whereby the output uncertainty of each pixel is assumed to be independent. This
approximation is clearly limited as demonstrated by observing a residual image
from a VAE reconstruction, which often possess a high level of structure. This
paper demonstrates a novel scheme to incorporate a structured Gaussian
likelihood prediction network within the VAE that allows the residual
correlations to be modelled. Our novel architecture, with minimal increase in
complexity, incorporates the covariance matrix prediction within the VAE. We
also propose a new mechanism for allowing structured uncertainty on color
images. Furthermore, we provide a scheme for effectively training this model,
and include some suggestions for improving performance in terms of efficiency
or modelling longer range correlations. The advantage of our approach is
illustrated on the CelebA face data and the LSUN outdoor churches dataset, with
substantial improvements in terms of samples over traditional VAE and better
reconstructions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dorta_G/0/1/0/all/0/1&quot;&gt;Garoe Dorta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vicente_S/0/1/0/all/0/1&quot;&gt;Sara Vicente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Agapito_L/0/1/0/all/0/1&quot;&gt;Lourdes Agapito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Campbell_N/0/1/0/all/0/1&quot;&gt;Neill D.F. Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Simpson_I/0/1/0/all/0/1&quot;&gt;Ivor Simpson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01071">
<title>Average performance analysis of the stochastic gradient method for online PCA. (arXiv:1804.01071v1 [math.ST])</title>
<link>http://arxiv.org/abs/1804.01071</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the complexity of the stochastic gradient algorithm for
PCA when the data are observed in a streaming setting. We also propose an
online approach for selecting the learning rate. Simulation experiments confirm
the practical relevance of the plain stochastic gradient approach and that
drastic improvements can be achieved by learning the learning rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chretien_S/0/1/0/all/0/1&quot;&gt;Stephane Chretien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Guyeux_C/0/1/0/all/0/1&quot;&gt;Christophe Guyeux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+HO_Z/0/1/0/all/0/1&quot;&gt;Zhen-Wai Olivier HO&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1501.02102">
<title>Equitability of Dependence Measure. (arXiv:1501.02102v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1501.02102</link>
<description rdf:parseType="Literal">&lt;p&gt;A measure of dependence is said to be equitable if it gives similar scores to
equally noisy relationship of different types. In practice, we do not know what
kind of functional relationship is underlying two given observations, Hence the
equitability of dependence measure is critical in analysis and by scoring
relationships according to an equitable measure one hopes to find important
patterns of any type of further examination. In this paper, we introduce our
definition of equitability of a dependence measure, which is naturally from
this initial description, and Further more power-equitable(weak-equitable) is
introduced which is of the most practical meaning in evaluating the equitablity
of a dependence measure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hangjin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yiming Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07006">
<title>Bayesian Nonparametric Poisson-Process Allocation for Time-Sequence Modeling. (arXiv:1705.07006v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07006</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing the underlying structure of multiple time-sequences provides
insights into the understanding of social networks and human activities. In
this work, we present the \emph{Bayesian nonparametric Poisson process
allocation} (BaNPPA), a latent-function model for time-sequences, which
automatically infers the number of latent functions. We model the intensity of
each sequence as an infinite mixture of latent functions, each of which is
obtained using a function drawn from a Gaussian process. We show that a
technical challenge for the inference of such mixture models is the
unidentifiability of the weights of the latent functions. We propose to cope
with the issue by regulating the volume of each latent function within a
variational inference algorithm. Our algorithm is computationally efficient and
scales well to large data sets. We demonstrate the usefulness of our proposed
model through experiments on both synthetic and real-world data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hongyi Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sato_I/0/1/0/all/0/1&quot;&gt;Issei Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.09285">
<title>Simplified Energy Landscape for Modularity Using Total Variation. (arXiv:1707.09285v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1707.09285</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks capture pairwise interactions between entities and are frequently
used in applications such as social networks, food networks, and protein
interaction networks, to name a few. Communities, cohesive groups of nodes,
often form in these applications, and identifying them gives insight into the
overall organization of the network. One common quality function used to
identify community structure is modularity. In Hu et al. [SIAM J. App. Math.,
73(6), 2013], it was shown that modularity optimization is equivalent to
minimizing a particular nonconvex total variation (TV) based functional over a
discrete domain. They solve this problem, assuming the number of communities is
known, using a Merriman, Bence, Osher (MBO) scheme.
&lt;/p&gt;
&lt;p&gt;We show that modularity optimization is equivalent to minimizing a convex
TV-based functional over a discrete domain, again, assuming the number of
communities is known. Furthermore, we show that modularity has no convex
relaxation satisfying certain natural conditions. We therefore, find a
manageable non-convex approximation using a Ginzburg Landau functional, which
provably converges to the correct energy in the limit of a certain parameter.
We then derive an MBO algorithm with fewer hand-tuned parameters than in Hu et
al. and which is 7 times faster at solving the associated diffusion equation
due to the fact that the underlying discretization is unconditionally stable.
Our numerical tests include a hyperspectral video whose associated graph has
2.9x10^7 edges, which is roughly 37 times larger than was handled in the paper
of Hu et al.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Boyd_Z/0/1/0/all/0/1&quot;&gt;Zachary Boyd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bae_E/0/1/0/all/0/1&quot;&gt;Egil Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tai_X/0/1/0/all/0/1&quot;&gt;Xue-Cheng Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bertozzi_A/0/1/0/all/0/1&quot;&gt;Andrea L. Bertozzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10210">
<title>On denoising modulo 1 samples of a function. (arXiv:1710.10210v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10210</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider an unknown smooth function $f: [0,1] \rightarrow \mathbb{R}$, and
say we are given $n$ noisy$\mod 1$ samples of $f$, i.e., $y_i = (f(x_i) +
\eta_i)\mod 1$ for $x_i \in [0,1]$, where $\eta_i$ denotes noise. Given the
samples $(x_i,y_i)_{i=1}^{n}$ our goal is to recover smooth, robust estimates
of the clean samples $f(x_i) \bmod 1$. We formulate a natural approach for
solving this problem which works with representations of mod 1 values over the
unit circle. This amounts to solving a quadratically constrained quadratic
program (QCQP) with non-convex constraints involving points lying on the unit
circle. Our proposed approach is based on solving its relaxation which is a
trust-region sub-problem, and hence solvable efficiently. We demonstrate its
robustness to noise % of our approach via extensive simulations on several
synthetic examples, and provide a detailed theoretical analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cucuringu_M/0/1/0/all/0/1&quot;&gt;Mihai Cucuringu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tyagi_H/0/1/0/all/0/1&quot;&gt;Hemant Tyagi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08421">
<title>Relief-Based Feature Selection: Introduction and Review. (arXiv:1711.08421v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08421</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature selection plays a critical role in biomedical data mining, driven by
increasing feature dimensionality in target problems and growing interest in
advanced but computationally expensive methodologies able to model complex
associations. Specifically, there is a need for feature selection methods that
are computationally efficient, yet sensitive to complex patterns of
association, e.g. interactions, so that informative features are not mistakenly
eliminated prior to downstream modeling. This paper focuses on Relief-based
algorithms (RBAs), a unique family of filter-style feature selection algorithms
that have gained appeal by striking an effective balance between these
objectives while flexibly adapting to various data characteristics, e.g.
classification vs. regression. First, this work broadly examines types of
feature selection and defines RBAs within that context. Next, we introduce the
original Relief algorithm and associated concepts, emphasizing the intuition
behind how it works, how feature weights generated by the algorithm can be
interpreted, and why it is sensitive to feature interactions without evaluating
combinations of features. Lastly, we include an expansive review of RBA
methodological research beyond Relief and its popular descendant, ReliefF. In
particular, we characterize branches of RBA research, and provide comparative
summaries of RBA algorithms including contributions, strategies, functionality,
time complexity, adaptation to key data characteristics, and software
availability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urbanowicz_R/0/1/0/all/0/1&quot;&gt;Ryan J. Urbanowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meeker_M/0/1/0/all/0/1&quot;&gt;Melissa Meeker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LaCava_W/0/1/0/all/0/1&quot;&gt;William LaCava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olson_R/0/1/0/all/0/1&quot;&gt;Randal S. Olson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1&quot;&gt;Jason H. Moore&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06246">
<title>A Survey on Multi-View Clustering. (arXiv:1712.06246v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06246</link>
<description rdf:parseType="Literal">&lt;p&gt;With advances in information acquisition technologies, multi-view data become
ubiquitous. Multi-view learning has thus become more and more popular in
machine learning and data mining fields. Multi-view unsupervised or
semi-supervised learning, such as co-training, co-regularization has gained
considerable attention. Although recently, multi-view clustering (MVC) methods
have been developed rapidly, there has not been a survey to summarize and
analyze the current progress. Therefore, this paper reviews the common
strategies for combining multiple views of data and based on this summary we
propose a novel taxonomy of the MVC approaches. We further discuss the
relationships between MVC and multi-view representation, ensemble clustering,
multi-task clustering, multi-view supervised and semi-supervised learning.
Several representative real-world applications are elaborated. To promote
future development of MVC, we envision several open problems that may require
further investigation and thorough examination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_G/0/1/0/all/0/1&quot;&gt;Guoqing Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shiliang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1&quot;&gt;Jinbo Bi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07200">
<title>Training Recurrent Neural Networks as a Constraint Satisfaction Problem. (arXiv:1803.07200v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07200</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new approach for training artificial neural networks
using techniques for solving the constraint satisfaction problem (CSP). The
quotient gradient system (QGS) is a trajectory based method for solving the
CSP. This study converts the training set of a neural network into a CSP and
uses the QGS to find its solutions. The QGS finds the global minimum of the
optimization problem by tracking trajectories of a nonlinear dynamical system
and does not stop at a local minimum of the optimization problem. Lyapunov
theory is used to prove the asymptotic stability of the solutions with and
without the presence of measurement errors. Numerical examples illustrate the
effectiveness of the proposed methodology and compare it to a genetic algorithm
and error backpropagation
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khodabandehlou_H/0/1/0/all/0/1&quot;&gt;Hamid Khodabandehlou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadali_M/0/1/0/all/0/1&quot;&gt;Mohammad Sami Fadali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11157">
<title>Security Consideration For Deep Learning-Based Image Forensics. (arXiv:1803.11157v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1803.11157</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, image forensics community has paied attention to the research on
the design of effective algorithms based on deep learning technology and facts
proved that combining the domain knowledge of image forensics and deep learning
would achieve more robust and better performance than the traditional schemes.
Instead of improving it, in this paper, the safety of deep learning based
methods in the field of image forensics is taken into account. To the best of
our knowledge, this is a first work focusing on this topic. Specifically, we
experimentally find that the method using deep learning would fail when adding
the slight noise into the images (adversarial images). Furthermore, two kinds
of strategys are proposed to enforce security of deep learning-based method.
Firstly, an extra penalty term to the loss function is added, which is referred
to the 2-norm of the gradient of the loss with respect to the input images, and
then an novel training method are adopt to train the model by fusing the normal
and adversarial images. Experimental results show that the proposed algorithm
can achieve good performance even in the case of adversarial images and provide
a safety consideration for deep learning-based image forensics
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1&quot;&gt;Pengpeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1&quot;&gt;Rongrong Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haorui Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11262">
<title>Efficient First-Order Algorithms for Adaptive Signal Denoising. (arXiv:1803.11262v1 [math.ST] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1803.11262</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of discrete-time signal denoising, focusing on a
specific family of non-linear convolution-type estimators. Each such estimator
is associated with a time-invariant filter which is obtained adaptively, by
solving a certain convex optimization problem. Adaptive convolution-type
estimators were demonstrated to have favorable statistical properties. However,
the question of their computational complexity remains largely unexplored, and
in fact we are not aware of any publicly available implementation of these
estimators. Our first contribution is an efficient implementation of these
estimators via some known first-order proximal algorithms. Our second
contribution is a computational complexity analysis of the proposed procedures,
which takes into account their statistical nature and the related notion of
statistical accuracy. The proposed procedures and their analysis are
illustrated on a simulated data benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ostrovskii_D/0/1/0/all/0/1&quot;&gt;Dmitrii Ostrovskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Harchaoui_Z/0/1/0/all/0/1&quot;&gt;Zaid Harchaoui&lt;/a&gt;</dc:creator>
</item></rdf:RDF>