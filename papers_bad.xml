<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-29T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09555"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.06456"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.09902"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09061"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09271"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09303"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09317"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09321"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09346"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09354"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09373"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09466"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09496"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09522"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.07758"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07903"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06620"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09049"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09055"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09065"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09125"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09144"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09150"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09185"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09197"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09319"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09326"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09367"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09390"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09667"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1501.04318"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.09805"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.03077"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.09055"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.02403"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.09866"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.09887"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.03663"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04043"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10277"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06816"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.09555">
<title>DeepLung: Deep 3D Dual Path Nets for Automated Pulmonary Nodule Detection and Classification. (arXiv:1801.09555v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.09555</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a fully automated lung computed tomography (CT)
cancer diagnosis system, DeepLung. DeepLung consists of two components, nodule
detection (identifying the locations of candidate nodules) and classification
(classifying candidate nodules into benign or malignant). Considering the 3D
nature of lung CT data and the compactness of dual path networks (DPN), two
deep 3D DPN are designed for nodule detection and classification respectively.
Specifically, a 3D Faster Regions with Convolutional Neural Net (R-CNN) is
designed for nodule detection with 3D dual path blocks and a U-net-like
encoder-decoder structure to effectively learn nodule features. For nodule
classification, gradient boosting machine (GBM) with 3D dual path network
features is proposed. The nodule classification subnetwork was validated on a
public dataset from LIDC-IDRI, on which it achieved better performance than
state-of-the-art approaches and surpassed the performance of experienced
doctors based on image modality. Within the DeepLung system, candidate nodules
are detected first by the nodule detection subnetwork, and nodule diagnosis is
conducted by the classification subnetwork. Extensive experimental results
demonstrate that DeepLung has performance comparable to experienced doctors
both for the nodule-level and patient-level diagnosis on the LIDC-IDRI
dataset.\footnote{https://github.com/uci-cbcl/DeepLung.git}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chaochun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.06456">
<title>Online Representation Learning with Single and Multi-layer Hebbian Networks for Image Classification. (arXiv:1702.06456v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1702.06456</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised learning permits the development of algorithms that are able to
adapt to a variety of different data sets using the same underlying rules
thanks to the autonomous discovery of discriminating features during training.
Recently, a new class of Hebbian-like and local unsupervised learning rules for
neural networks have been developed that minimise a similarity matching
cost-function. These have been shown to perform sparse representation learning.
This study tests the effectiveness of one such learning rule for learning
features from images. The rule implemented is derived from a nonnegative
classical multidimensional scaling cost-function, and is applied to both single
and multi-layer architectures. The features learned by the algorithm are then
used as input to an SVM to test their effectiveness in classification on the
established CIFAR-10 image dataset. The algorithm performs well in comparison
to other unsupervised learning algorithms and multi-layer networks, thus
suggesting its validity in the design of a new class of compact, online
learning networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahroun_Y/0/1/0/all/0/1&quot;&gt;Yanis Bahroun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltoggio_A/0/1/0/all/0/1&quot;&gt;Andrea Soltoggio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.09902">
<title>Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation. (arXiv:1703.09902v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1703.09902</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper surveys the current state of the art in Natural Language
Generation (NLG), defined as the task of generating text or speech from
non-linguistic input. A survey of NLG is timely in view of the changes that the
field has undergone over the past decade or so, especially in relation to new
(usually data-driven) methods, as well as new applications of NLG technology.
This survey therefore aims to (a) give an up-to-date synthesis of research on
the core tasks in NLG and the architectures adopted in which such tasks are
organised; (b) highlight a number of relatively recent research topics that
have arisen partly as a result of growing synergies between NLG and other areas
of artificial intelligence; (c) draw attention to the challenges in NLG
evaluation, relating them to similar challenges faced in other areas of Natural
Language Processing, with an emphasis on different evaluation methods and the
relationships between them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1&quot;&gt;Albert Gatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krahmer_E/0/1/0/all/0/1&quot;&gt;Emiel Krahmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09061">
<title>SWRL2SPIN: A tool for transforming SWRL rule bases in OWL ontologies to object-oriented SPIN rules. (arXiv:1801.09061v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.09061</link>
<description rdf:parseType="Literal">&lt;p&gt;SWRL is a semantic web rule language that combines OWL ontologies with Horn
Logic rules of the RuleML family of rule languages, extending the set of OWL
axioms to include Horn-like rules. Being supported by the Prot\&apos;eg\&apos;e ontology
editor as well as by popular rule engines and ontology reasoners, such as Jess,
Drools and Pellet, SWRL has become a very popular choice for developing
rule-based applications on top of ontologies. However, SWRL being around for
more than 10 years now, it is most probable that it will never become a W3C
standard; therefore, its scope is difficult to reach out to the industrial
world. On the other hand, SPIN has become a de-facto industry standard to
represent SPARQL rules and constraints on Semantic Web models, building on the
widespread acceptance of the SPARQL query language for querying and processing
Linked Open Data. In this paper, we argue that the life of existing SWRL
rule-based ontology applications can be prolonged by being transformed into
SPIN. To this end, we have developed a prototype tool using SWI-Prolog that
takes as in-put an OWL ontology with a SWRL rule base and transforms SWRL rules
into SPIN rules in the same ontology, taking into consideration the
object-oriented scent of SPIN, i.e. linking rules to the appropriate ontology
classes as derived by analyzing the rule conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassiliades_N/0/1/0/all/0/1&quot;&gt;Nick Bassiliades&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09271">
<title>Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical Registry Data. (arXiv:1801.09271v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.09271</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the first deep reinforcement learning (DRL) framework to
estimate the optimal Dynamic Treatment Regimes from observational medical data.
This framework is more flexible and adaptive for high dimensional action and
state spaces than existing reinforcement learning methods to model real-life
complexity in heterogeneous disease progression and treatment choices, with the
goal of providing doctor and patients the data-driven personalized decision
recommendations. The proposed DRL framework comprises (i) a supervised learning
step to predict the most possible expert actions, and (ii) a deep reinforcement
learning step to estimate the long-term value function of Dynamic Treatment
Regimes. Both steps depend on deep neural networks.
&lt;/p&gt;
&lt;p&gt;As a key motivational example, we have implemented the proposed framework on
a data set from the Center for International Bone Marrow Transplant Research
(CIBMTR) registry database, focusing on the sequence of prevention and
treatments for acute and chronic graft versus host disease after
transplantation. In the experimental results, we have demonstrated promising
accuracy in predicting human experts&apos; decisions, as well as the high expected
reward function in the DRL-based dynamic treatment regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Logan_B/0/1/0/all/0/1&quot;&gt;Brent Logan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09303">
<title>HONE: Higher-Order Network Embeddings. (arXiv:1801.09303v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.09303</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes a general framework for learning Higher-Order Network
Embeddings (HONE) from graph data based on network motifs. The HONE framework
is highly expressive and flexible with many interchangeable components. The
experimental results demonstrate the effectiveness of learning higher-order
network representations. In all cases, HONE outperforms recent embedding
methods that are unable to capture higher-order structures with a mean relative
gain in AUC of $19\%$ (and up to $75\%$ gain) across a wide variety of networks
and embedding methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rossi_R/0/1/0/all/0/1&quot;&gt;Ryan A. Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nesreen K. Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Koh_E/0/1/0/all/0/1&quot;&gt;Eunyee Koh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09317">
<title>A Cyber Science Based Ontology for Artificial General Intelligence Containment. (arXiv:1801.09317v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.09317</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of artificial general intelligence is considered by many to
be inevitable. What such intelligence does after becoming aware is not so
certain. To that end, research suggests that the likelihood of artificial
general intelligence becoming hostile to humans is significant enough to
warrant inquiry into methods to limit such potential. Thus, containment of
artificial general intelligence is a timely and meaningful research topic.
While there is limited research exploring possible containment strategies, such
work is bounded by the underlying field the strategies draw upon. Accordingly,
we set out to construct an ontology to describe necessary elements in any
future containment technology. Using existing academic literature, we developed
a single domain ontology containing five levels, 32 codes, and 32 associated
descriptors. Further, we constructed ontology diagrams to demonstrate intended
relationships. We then identified humans, AGI, and the cyber world as novel
agent objects necessary for future containment activities. Collectively, the
work addresses three critical gaps: (a) identifying and arranging fundamental
constructs; (b) situating AGI containment within cyber science; and (c)
developing scientific rigor within the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pittman_J/0/1/0/all/0/1&quot;&gt;Jason M. Pittman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soboleski_C/0/1/0/all/0/1&quot;&gt;Courtney E. Soboleski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09321">
<title>Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks. (arXiv:1801.09321v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.09321</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, a region-based Deep Convolutional Neural Network framework is
proposed for document structure learning. The contribution of this work
involves efficient training of region based classifiers and effective
ensembling for document image classification. A primary level of `inter-domain&apos;
transfer learning is used by exporting weights from a pre-trained VGG16
architecture on the ImageNet dataset to train a document classifier on whole
document images. Exploiting the nature of region based influence modelling, a
secondary level of `intra-domain&apos; transfer learning is used for rapid training
of deep learning models for image segments. Finally, stacked generalization
based ensembling is utilized for combining the predictions of the base deep
neural network models. The proposed method achieves state-of-the-art accuracy
of 92.2% on the popular RVL-CDIP document image dataset, exceeding benchmarks
set by existing algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arindam Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Saikat Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1&quot;&gt;Ujjwal Bhattacharya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09346">
<title>Representing the Insincere: Strategically Robust Proportional Representation. (arXiv:1801.09346v1 [cs.GT])</title>
<link>http://arxiv.org/abs/1801.09346</link>
<description rdf:parseType="Literal">&lt;p&gt;Proportional representation (PR) is a fundamental principle of many
democracies world-wide which employ PR-based voting rules to elect their
representatives. The normative properties of these voting rules however, are
often only understood in the context of sincere voting.
&lt;/p&gt;
&lt;p&gt;In this paper we consider PR in the presence of strategic voters. We
construct a voting rule such that for every preference profile there exists at
least one costly voting equilibrium satisfying PR with respect to voters&apos;
private and unrevealed preferences - such a voting rule is said to be
strategically robust. In contrast, a commonly applied voting rule is shown not
be strategically robust. Furthermore, we prove a limit on `how strategically
robust&apos; a PR-based voting rule can be; we show that there is no PR-based voting
rule which ensures that every equilibrium satisfies PR. Collectively, our
results highlight the possibility and limit of achieving PR in the presence of
strategic voters and a positive role for mechanisms, such as pre-election
polls, which coordinate voter behaviour towards equilibria which satisfy PR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Barton E. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09354">
<title>On the Inter-relationships among Drift rate, Forgetting rate, Bias/variance profile and Error. (arXiv:1801.09354v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.09354</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose two general and falsifiable hypotheses about expectations on
generalization error when learning in the context of concept drift. One posits
that as drift rate increases, the forgetting rate that minimizes generalization
error will also increase and vice versa. The other posits that as a learner&apos;s
forgetting rate increases, the bias/variance profile that minimizes
generalization error will have lower variance and vice versa. These hypotheses
lead to the concept of the sweet path, a path through the 3-d space of
alternative drift rates, forgetting rates and bias/variance profiles on which
generalization error will be minimized, such that slow drift is coupled with
low forgetting and low bias, while rapid drift is coupled with fast forgetting
and low variance. We present experiments that support the existence of such a
sweet path. We also demonstrate that simple learners that select appropriate
forgetting rates and bias/variance profiles are highly competitive with the
state-of-the-art in incremental learners for concept drift on real-world drift
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaidi_N/0/1/0/all/0/1&quot;&gt;Nayyar A. Zaidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1&quot;&gt;Geoffrey I. Webb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petitjean_F/0/1/0/all/0/1&quot;&gt;Francois Petitjean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forestier_G/0/1/0/all/0/1&quot;&gt;Germain Forestier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09373">
<title>Search Based Code Generation for Machine Learning Programs. (arXiv:1801.09373v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1801.09373</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Learning (ML) has revamped every domain of life as it provides
powerful tools to build complex systems that learn and improve from experience
and data. Our key insight is that to solve a machine learning problem, data
scientists do not invent a new algorithm each time, but evaluate a range of
existing models with different configurations and select the best one. This
task is laborious, error-prone, and drains a large chunk of project budget and
time. In this paper we present a novel framework inspired by programming by
Sketching and Partial Evaluation to minimize human intervention in developing
ML solutions. We templatize machine learning algorithms to expose configuration
choices as holes to be searched. We share code and computation between
different algorithms, and only partially evaluate configuration space of
algorithms based on information gained from initial algorithm evaluations. We
also employ hierarchical and heuristic based pruning to reduce the search
space. Our initial findings indicate that our approach can generate highly
accurate ML models. Interviews with data scientists show that they feel our
framework can eliminate sources of common errors and significantly reduce
development time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_M/0/1/0/all/0/1&quot;&gt;Muhammad Zubair Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nawaz_M/0/1/0/all/0/1&quot;&gt;Muhammad Nawaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_N/0/1/0/all/0/1&quot;&gt;Nimrah Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddiqui_J/0/1/0/all/0/1&quot;&gt;Junaid Haroon Siddiqui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09466">
<title>Using deep Q-learning to understand the tax evasion behavior of risk-averse firms. (arXiv:1801.09466v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.09466</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing tax policies that are effective in curbing tax evasion and maximize
state revenues requires a rigorous understanding of taxpayer behavior. This
work explores the problem of determining the strategy a self-interested,
risk-averse tax entity is expected to follow, as it &quot;navigates&quot; - in the
context of a Markov Decision Process - a government-controlled tax environment
that includes random audits, penalties and occasional tax amnesties. Although
simplified versions of this problem have been previously explored, the mere
assumption of risk-aversion (as opposed to risk-neutrality) raises the
complexity of finding the optimal policy well beyond the reach of analytical
techniques. Here, we obtain approximate solutions via a combination of
Q-learning and recent advances in Deep Reinforcement Learning. By doing so, we
i) determine the tax evasion behavior expected of the taxpayer entity, ii)
calculate the degree of risk aversion of the &quot;average&quot; entity given empirical
estimates of tax evasion, and iii) evaluate sample tax policies, in terms of
expected revenues. Our model can be useful as a testbed for &quot;in-vitro&quot; testing
of tax policies, while our results lead to various policy recommendations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goumagias_N/0/1/0/all/0/1&quot;&gt;Nikolaos D. Goumagias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hristu_Varsakelis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Hristu-Varsakelis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assael_Y/0/1/0/all/0/1&quot;&gt;Yannis M. Assael&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09496">
<title>Improving Active Learning in Systematic Reviews. (arXiv:1801.09496v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1801.09496</link>
<description rdf:parseType="Literal">&lt;p&gt;Systematic reviews are essential to summarizing the results of different
clinical and social science studies. The first step in a systematic review task
is to identify all the studies relevant to the review. The task of identifying
relevant studies for a given systematic review is usually performed manually,
and as a result, involves substantial amounts of expensive human resource.
Lately, there have been some attempts to reduce this manual effort using active
learning. In this work, we build upon some such existing techniques, and
validate by experimenting on a larger and comprehensive dataset than has been
attempted until now. Our experiments provide insights on the use of different
feature extraction models for different disciplines. More importantly, we
identify that a naive active learning based screening process is biased in
favour of selecting similar documents. We aimed to improve the performance of
the screening process using a novel active learning algorithm with success.
Additionally, we propose a mechanism to choose the best feature extraction
method for a given review.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1&quot;&gt;Gaurav Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_J/0/1/0/all/0/1&quot;&gt;James Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shawe_Taylor_J/0/1/0/all/0/1&quot;&gt;John Shawe-Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09522">
<title>Multichannel Sound Event Detection Using 3D Convolutional Neural Networks for Learning Inter-channel Features. (arXiv:1801.09522v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1801.09522</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a stacked convolutional and recurrent neural
network (CRNN) with a 3D convolutional neural network (CNN) in the first layer
for the multichannel sound event detection (SED) task. The 3D CNN enables the
network to simultaneously learn the inter- and intra-channel features from the
input multichannel audio. In order to evaluate the proposed method,
multichannel audio datasets with different number of overlapping sound sources
are synthesized. Each of this dataset has a four-channel first-order Ambisonic,
binaural, and single-channel versions, on which the performance of SED using
the proposed method are compared to study the potential of SED using
multichannel audio. A similar study is also done with the binaural and
single-channel versions of the real-life recording TUT-SED 2017 development
dataset. The proposed method learns to recognize overlapping sound events from
multichannel features faster and performs better SED with a fewer number of
training epochs. The results show that on using multichannel Ambisonic audio in
place of single-channel audio we improve the overall F-score by 7.5%, overall
error rate by 10% and recognize 15.6% more sound events in time frames with
four overlapping sound sources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adavanne_S/0/1/0/all/0/1&quot;&gt;Sharath Adavanne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Politis_A/0/1/0/all/0/1&quot;&gt;Archontis Politis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Virtanen_T/0/1/0/all/0/1&quot;&gt;Tuomas Virtanen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.07758">
<title>Sample and Computationally Efficient Learning Algorithms under S-Concave Distributions. (arXiv:1703.07758v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.07758</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide new results for noise-tolerant and sample-efficient learning
algorithms under $s$-concave distributions. The new class of $s$-concave
distributions is a broad and natural generalization of log-concavity, and
includes many important additional distributions, e.g., the Pareto distribution
and $t$-distribution. This class has been studied in the context of efficient
sampling, integration, and optimization, but much remains unknown about the
geometry of this class of distributions and their applications in the context
of learning. The challenge is that unlike the commonly used distributions in
learning (uniform or more generally log-concave distributions), this broader
class is not closed under the marginalization operator and many such
distributions are fat-tailed. In this work, we introduce new convex geometry
tools to study the properties of $s$-concave distributions and use these
properties to provide bounds on quantities of interest to learning including
the probability of disagreement between two halfspaces, disagreement outside a
band, and the disagreement coefficient. We use these results to significantly
generalize prior results for margin-based active learning, disagreement-based
active learning, and passive learning of intersections of halfspaces. Our
analysis of geometric properties of $s$-concave distributions might be of
independent interest to optimization more broadly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balcan_M/0/1/0/all/0/1&quot;&gt;Maria-Florina Balcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07903">
<title>The Complexity of Graph-Based Reductions for Reachability in Markov Decision Processes. (arXiv:1710.07903v3 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07903</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the never-worse relation (NWR) for Markov decision processes with an
infinite-horizon reachability objective. A state q is never worse than a state
p if the maximal probability of reaching the target set of states from p is at
most the same value from q, regard- less of the probabilities labelling the
transitions. Extremal-probability states, end components, and essential states
are all special cases of the equivalence relation induced by the NWR. Using the
NWR, states in the same equivalence class can be collapsed. Then, actions
leading to sub- optimal states can be removed. We show the natural decision
problem associated to computing the NWR is coNP-complete. Finally, we ex- tend
a previously known incomplete polynomial-time iterative algorithm to
under-approximate the NWR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roux_S/0/1/0/all/0/1&quot;&gt;Stephane Le Roux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1&quot;&gt;Guillermo A. Perez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06620">
<title>A high-performance analog Max-SAT solver and its application to Ramsey numbers. (arXiv:1801.06620v2 [cs.CC] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06620</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a continuous-time analog solver for MaxSAT, a quintessential
class of NP-hard discrete optimization problems, where the task is to find a
truth assignment for a set of Boolean variables satisfying the maximum number
of given logical constraints. We show that the scaling of an invariant of the
solver&apos;s dynamics, the escape rate, as function of the number of unsatisfied
clauses can predict the global optimum value, often well before reaching the
corresponding state. We demonstrate the performance of the solver on hard
MaxSAT competition problems. We then consider the two-color Ramsey number
$R(m,m)$ problem, translate it to SAT, and apply our algorithm to the still
unknown $R(5,5)$. We find edge colorings without monochromatic 5-cliques for
complete graphs up to 42 vertices, while on 43 vertices we find colorings with
only two monochromatic 5-cliques, the best coloring found so far, supporting
the conjecture that $R(5,5) = 43$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molnar_B/0/1/0/all/0/1&quot;&gt;Botond Moln&amp;#xe1;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varga_M/0/1/0/all/0/1&quot;&gt;Melinda Varga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toroczkai_Z/0/1/0/all/0/1&quot;&gt;Zoltan Toroczkai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ercsey_Ravasz_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;ria Ercsey-Ravasz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09049">
<title>Covariance-based Dissimilarity Measures Applied to Clustering Wide-sense Stationary Ergodic Processes. (arXiv:1801.09049v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.09049</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new unsupervised learning problem: clustering wide-sense
stationary ergodic stochastic processes. A covariance-based dissimilarity
measure and consistent algorithms are designed for clustering offline and
online data settings, respectively. We also suggest a formal criterion on the
efficiency of dissimilarity measures, and discuss of some approach to improve
the efficiency of clustering algorithms, when they are applied to cluster
particular type of processes, such as self-similar processes with wide-sense
stationary ergodic increments. Clustering synthetic data sampled from
fractional Brownian motions is provided as an example of application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peng_Q/0/1/0/all/0/1&quot;&gt;Qidi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rao_N/0/1/0/all/0/1&quot;&gt;Nan Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ran Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09055">
<title>Solving for multi-class using orthogonal coding matrices. (arXiv:1801.09055v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.09055</link>
<description rdf:parseType="Literal">&lt;p&gt;Probability estimates are desirable in statistical classification both for
gauging the accuracy of a classification result and for calibration. Here we
describe a method of solving for the conditional probabilities in multi-class
classification using orthogonal error correcting codes. The method is tested on
six different datasets using support vector machines and compares favorably
with an existing technique based on the one-versus-one multi-class method.
Probabilities are validated based on the cumulative sum of a boolean evaluation
of the correctness of the class label divided by the estimated probability.
Probability estimation using orthogonal coding is simple and efficient and has
the potential for faster classification results than the one-versus-one method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mills_P/0/1/0/all/0/1&quot;&gt;Peter Mills&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09065">
<title>A Review of Multiple Try MCMC algorithms for Signal Processing. (arXiv:1801.09065v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1801.09065</link>
<description rdf:parseType="Literal">&lt;p&gt;Many applications in signal processing require the estimation of some
parameters of interest given a set of observed data. More specifically,
Bayesian inference needs the computation of {\it a-posteriori} estimators which
are often expressed as complicated multi-dimensional integrals. Unfortunately,
analytical expressions for these estimators cannot be found in most real-world
applications, and Monte Carlo methods are the only feasible approach. A very
powerful class of Monte Carlo techniques is formed by the Markov Chain Monte
Carlo (MCMC) algorithms. They generate a Markov chain such that its stationary
distribution coincides with the target posterior density. In this work, we
perform a thorough review of MCMC methods using multiple candidates in order to
select the next state of the chain, at each iteration. With respect to the
classical Metropolis-Hastings method, the use of multiple try techniques foster
the exploration of the sample space. We present different Multiple Try
Metropolis schemes, Ensemble MCMC methods, Particle Metropolis-Hastings
algorithms and the Delayed Rejection Metropolis technique. We highlight
limitations, benefits, connections and differences among the different methods,
and compare them by numerical simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Martino_L/0/1/0/all/0/1&quot;&gt;Luca Martino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09125">
<title>Scalable Mutual Information Estimation using Dependence Graphs. (arXiv:1801.09125v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1801.09125</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a unified method for empirical non-parametric estimation of
general Mutual Information (MI) function between the random vectors in
$\mathbb{R}^d$ based on $N$ i.i.d. samples. The proposed low complexity
estimator is based on a bipartite graph, referred to as dependence graph. The
data points are mapped to the vertices of this graph using randomized Locality
Sensitive Hashing (LSH). The vertex and edge weights are defined in terms of
marginal and joint hash collisions. For a given set of hash parameters
$\epsilon(1), \ldots, \epsilon(k)$, a base estimator is defined as a weighted
average of the transformed edge weights. The proposed estimator, called the
ensemble dependency graph estimator (EDGE), is obtained as a weighted average
of the base estimators, where the weights are computed offline as the solution
of a linear programming problem. EDGE achieves optimal computational complexity
$O(N)$, and can achieve the optimal parametric MSE rate of $O(1/N)$ if the
density is $d$ times differentiable. To the best of our knowledge EDGE is the
first non-parametric MI estimator that can achieve parametric MSE rates with
linear time complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noshad_M/0/1/0/all/0/1&quot;&gt;Morteza Noshad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hero_A/0/1/0/all/0/1&quot;&gt;Alfred O. Hero III&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09144">
<title>Adaptive Scan Gibbs Sampler for Large Scale Inference Problems. (arXiv:1801.09144v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.09144</link>
<description rdf:parseType="Literal">&lt;p&gt;For large scale on-line inference problems the update strategy is critical
for performance. We derive an adaptive scan Gibbs sampler that optimizes the
update frequency by selecting an optimum mini-batch size. We demonstrate
performance of our adaptive batch-size Gibbs sampler by comparing it against
the collapsed Gibbs sampler for Bayesian Lasso, Dirichlet Process Mixture
Models (DPMM) and Latent Dirichlet Allocation (LDA) graphical models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smolyakov_V/0/1/0/all/0/1&quot;&gt;Vadim Smolyakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fisher_J/0/1/0/all/0/1&quot;&gt;John W. Fisher III&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09150">
<title>Bayesian Nonparametric Modeling of Driver Behavior using HDP Split-Merge Sampling Algorithm. (arXiv:1801.09150v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.09150</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern vehicles are equipped with increasingly complex sensors. These sensors
generate large volumes of data that provide opportunities for modeling and
analysis. Here, we are interested in exploiting this data to learn aspects of
behaviors and the road network associated with individual drivers. Our dataset
is collected on a standard vehicle used to commute to work and for personal
trips. A Hidden Markov Model (HMM) trained on the GPS position and orientation
data is utilized to compress the large amount of position information into a
small amount of road segment states. Each state has a set of observations, i.e.
car signals, associated with it that are quantized and modeled as draws from a
Hierarchical Dirichlet Process (HDP). The inference for the topic distributions
is carried out using HDP split-merge sampling algorithm. The topic
distributions over joint quantized car signals characterize the driving
situation in the respective road state. In a novel manner, we demonstrate how
the sparsity of the personal road network of a driver in conjunction with a
hierarchical topic model allows data driven predictions about destinations as
well as likely road conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smolyakov_V/0/1/0/all/0/1&quot;&gt;Vadim Smolyakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Straub_J/0/1/0/all/0/1&quot;&gt;Julian Straub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sue Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fisher_J/0/1/0/all/0/1&quot;&gt;John W. Fisher III&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09185">
<title>Marketing Analytics: Methods, Practice, Implementation, and Links to Other Fields. (arXiv:1801.09185v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.09185</link>
<description rdf:parseType="Literal">&lt;p&gt;Marketing analytics is a diverse field, with both academic researchers and
practitioners coming from a range of backgrounds including marketing,
operations research, statistics, and computer science. This paper provides an
integrative review at the boundary of these three areas. The topics of
visualization, segmentation, and class prediction are featured. Links between
the disciplines are emphasized. For each of these topics, a historical overview
is given, starting with initial work in the 1960s and carrying through to the
present day. Recent innovations for modern large and complex &quot;big data&quot; sets
are described. Practical implementation advice is given, along with a directory
of open source R routines for implementing marketing analytics techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+France_S/0/1/0/all/0/1&quot;&gt;Stephen L. France&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghose_S/0/1/0/all/0/1&quot;&gt;Sanjoy Ghose&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09197">
<title>Algorithmic Linearly Constrained Gaussian Processes. (arXiv:1801.09197v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.09197</link>
<description rdf:parseType="Literal">&lt;p&gt;We algorithmically construct multi-output Gaussian process priors which
satisfy linear differential equations. Our approach attempts to parametrize all
solutions of the equations using Gr\&quot;obner bases. If successful, a push forward
Gaussian process along the paramerization is the desired prior. We consider
several examples, among them the full inhomogeneous system of Maxwell&apos;s
equations. By bringing together stochastic learning and computeralgebra in a
novel way, we combine noisy observations with precise algebraic computations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lange_Hegermann_M/0/1/0/all/0/1&quot;&gt;Markus Lange-Hegermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09319">
<title>Less is more: sampling chemical space with active learning. (arXiv:1801.09319v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/1801.09319</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of accurate and transferable machine learning (ML) potentials
for predicting molecular energetics is a challenging task. The process of data
generation to train such ML potentials is a task neither well understood nor
researched in detail. In this work, we present a fully automated approach for
the generation of datasets with the intent of training universal ML potentials.
It is based on the concept of active learning (AL) via Query by Committee
(QBC), which uses the disagreement between an ensemble of ML potentials to
infer the reliability of the ensemble&apos;s prediction. QBC allows our AL algorithm
to automatically sample regions of chemical space where the machine learned
potential fails to accurately predict the potential energy. AL improves the
overall fitness of ANAKIN-ME (ANI) deep learning potentials in rigorous test
cases by mitigating human biases in deciding what new training data to use. AL
also reduces the training set size to a fraction of the data required when
using naive random sampling techniques. To provide validation of our AL
approach we develop the COMP6 benchmark (publicly available on GitHub), which
contains a diverse set of organic molecules. We show the use of our proposed AL
technique develops a universal ANI potential (ANI-1x), which provides very
accurate energy and force predictions on the entire COMP6 benchmark. This
universal potential achieves a level of accuracy on par with the best ML
potentials for single molecule or materials while remaining applicable to the
general class of organic molecules comprised of the elements CHNO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Smith_J/0/1/0/all/0/1&quot;&gt;Justin S. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nebgen_B/0/1/0/all/0/1&quot;&gt;Ben Nebgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lubbers_N/0/1/0/all/0/1&quot;&gt;Nicholas Lubbers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Isayev_O/0/1/0/all/0/1&quot;&gt;Olexandr Isayev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Roitberg_A/0/1/0/all/0/1&quot;&gt;Adrian E. Roitberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09326">
<title>Sparse and Low-rank Tensor Estimation via Cubic Sketchings. (arXiv:1801.09326v1 [math.ST])</title>
<link>http://arxiv.org/abs/1801.09326</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a general framework for sparse and low-rank tensor
estimation from cubic sketchings. A two-stage non-convex implementation is
developed based on sparse tensor decomposition and thresholded gradient
descent, which ensures exact recovery in the noiseless case and stable recovery
in the noisy case with high probability. The non-asymptotic analysis sheds
light on an interplay between optimization error and statistical error. The
proposed procedure is shown to be rate-optimal under certain conditions. As a
technical by-product, novel high-order concentration inequalities are derived
for studying high-moment sub-Gaussian tensors. An interesting tensor
formulation illustrates the potential application to high-order interaction
pursuit in high-dimensional linear regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hao_B/0/1/0/all/0/1&quot;&gt;Botao Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cheng_G/0/1/0/all/0/1&quot;&gt;Guang Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09367">
<title>Approximate Vanishing Ideal via Data Knotting. (arXiv:1801.09367v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.09367</link>
<description rdf:parseType="Literal">&lt;p&gt;The vanishing ideal is a set of polynomials that takes zero value on the
given data points. Originally proposed in computer algebra, the vanishing ideal
has been recently exploited for extracting the nonlinear structures of data in
many applications. To avoid overfitting to noisy data, the polynomials are
often designed to approximately rather than exactly equal zero on the
designated data. Although such approximations empirically demonstrate high
performance, the sound algebraic structure of the vanishing ideal is lost. The
present paper proposes a vanishing ideal that is tolerant to noisy data and
also pursued to have a better algebraic structure. As a new problem, we
simultaneously find a set of polynomials and data points for which the
polynomials approximately vanish on the input data points, and almost exactly
vanish on the discovered data points. In experimental classification tests, our
method discovered much fewer and lower-degree polynomials than an existing
state-of-the-art method. Consequently, our method accelerated the runtime of
the classification tasks without degrading the classification accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kera_H/0/1/0/all/0/1&quot;&gt;Hiroshi Kera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hasegawa_Y/0/1/0/all/0/1&quot;&gt;Yoshihiko Hasegawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09386">
<title>Tournament Leave-pair-out Cross-validation for Receiver Operating Characteristic (ROC) Analysis. (arXiv:1801.09386v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.09386</link>
<description rdf:parseType="Literal">&lt;p&gt;Receiver operating characteristic (ROC) analysis is widely used for
evaluating diagnostic systems. Recent studies have shown that estimating an
area under ROC curve (AUC) with standard cross-validation methods suffers from
a large bias. The leave-pair-out (LPO) cross-validation has been shown to
correct this bias. However, while LPO produces an almost unbiased estimate of
AUC, it does not provide a ranking of the data needed for plotting and
analyzing the ROC curve. In this study, we propose a new method called
tournament leave-pair-out (TLPO) cross-validation. This method extends LPO by
creating a tournament from pair comparisons to produce a ranking for the data.
TLPO preserves the advantage of LPO for estimating AUC, while it also allows
performing ROC analysis. We have shown using both synthetic and real world data
that TLPO is as reliable as LPO for AUC estimation and confirmed the bias in
leave-one-out cross-validation on low-dimensional data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perez_I/0/1/0/all/0/1&quot;&gt;Ileana Montoya Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Airola_A/0/1/0/all/0/1&quot;&gt;Antti Airola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bostrom_P/0/1/0/all/0/1&quot;&gt;Peter J. Bostr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jambor_I/0/1/0/all/0/1&quot;&gt;Ivan Jambor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pahikkala_T/0/1/0/all/0/1&quot;&gt;Tapio Pahikkala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09390">
<title>Nonlinear Dimensionality Reduction on Graphs. (arXiv:1801.09390v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.09390</link>
<description rdf:parseType="Literal">&lt;p&gt;In this era of data deluge, many signal processing and machine learning tasks
are faced with high-dimensional datasets, including images, videos, as well as
time series generated from social, commercial and brain network interactions.
Their efficient processing calls for dimensionality reduction techniques
capable of properly compressing the data while preserving task-related
characteristics, going beyond pairwise data correlations. The present paper
puts forth a nonlinear dimensionality reduction framework that accounts for
data lying on known graphs. The novel framework turns out to encompass most of
the existing dimensionality reduction methods as special cases, and it is
capable of capturing and preserving possibly nonlinear correlations that are
ignored by linear methods, as well as taking into account information from
multiple graphs. An efficient algorithm admitting closed-form solution is
developed and tested on synthetic datasets to corroborate its effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yanning Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traganitis_P/0/1/0/all/0/1&quot;&gt;Panagiotis A. Traganitis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09667">
<title>Information Directed Sampling and Bandits with Heteroscedastic Noise. (arXiv:1801.09667v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.09667</link>
<description rdf:parseType="Literal">&lt;p&gt;In the stochastic bandit problem, the goal is to maximize an unknown function
via a sequence of noisy function evaluations. Typically, the observation noise
is assumed to be independent of the evaluation point and satisfies a tail bound
taken uniformly on the domain. In this work, we consider the setting of
heteroscedastic noise, that is, we explicitly allow the noise distribution to
depend on the evaluation point. We show that this leads to new trade-offs for
information and regret, which are not taken into account by existing approaches
like upper confidence bound algorithms (UCB) or Thompson Sampling. To address
these shortcomings, we introduce a frequentist regret framework, that is
similar to the Bayesian analysis of Russo and Van Roy (2014). We prove a new
high-probability regret bound for general, possibly randomized policies,
depending on a quantity we call the regret-information ratio. From this bound,
we define a frequentist version of Information Directed Sampling (IDS) to
minimize a surrogate of the regret-information ratio over all possible action
sampling distributions. In order to construct the surrogate function, we
generalize known concentration inequalities for least squares regression in
separable Hilbert spaces to the case of heteroscedastic noise. This allows us
to formulate several variants of IDS for linear and reproducing kernel Hilbert
space response functions, yielding a family of novel algorithms for Bayesian
optimization. We also provide frequentist regret bounds, which in the
homoscedastic case are comparable to existing bounds for UCB, but can be much
better when the noise is heteroscedastic. Finally, we empirically demonstrate
in a linear setting, that some of our methods can outperform UCB and Thompson
Sampling, even when the noise is homoscedastic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kirschner_J/0/1/0/all/0/1&quot;&gt;Johannes Kirschner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1501.04318">
<title>Clustering based on the In-tree Graph Structure and Affinity Propagation. (arXiv:1501.04318v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1501.04318</link>
<description rdf:parseType="Literal">&lt;p&gt;A recently proposed clustering method, called the Nearest Descent (ND), can
organize the whole dataset into a sparsely connected graph, called the In-tree.
This ND-based Intree structure proves able to reveal the clustering structure
underlying the dataset, except one imperfect place, that is, there are some
undesired edges in this In-tree which require to be removed. Here, we propose
an effective way to automatically remove the undesired edges in In-tree via an
effective combination of the In-tree structure with affinity propagation (AP).
The key for the combination is to add edges between the reachable nodes in
In-tree before using AP to remove the undesired edges. The experiments on both
synthetic and real datasets demonstrate the effectiveness of the proposed
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_T/0/1/0/all/0/1&quot;&gt;Teng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongjie Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.09805">
<title>A new primal-dual algorithm for minimizing the sum of three functions with a linear operator. (arXiv:1611.09805v4 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1611.09805</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a new primal-dual algorithm for minimizing $f(x) +
g(x) + h(Ax)$, where $f$, $g$, and $h$ are proper lower semi-continuous convex
functions, $f$ is differentiable with a Lipschitz continuous gradient, and $A$
is a bounded linear operator. The proposed algorithm has some famous
primal-dual algorithms for minimizing the sum of two functions as special
cases. E.g., it reduces to the Chambolle-Pock algorithm when $f = 0$ and the
proximal alternating predictor-corrector when $g = 0$. For the general convex
case, we prove the convergence of this new algorithm in terms of the distance
to a fixed point by showing that the iteration is a nonexpansive operator. In
addition, we prove the $O(1/k)$ ergodic convergence rate in the primal-dual
gap. With additional assumptions, we derive the linear convergence rate in
terms of the distance to the fixed point. Comparing to other primal-dual
algorithms for solving the same problem, this algorithm extends the range of
acceptable parameters to ensure its convergence and has a smaller per-iteration
cost. The numerical experiments show the efficiency of this algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Ming Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.03077">
<title>A More General Robust Loss Function. (arXiv:1701.03077v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1701.03077</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a two-parameter loss function which can be viewed as a
generalization of many popular loss functions used in robust statistics: the
Cauchy/Lorentzian, Geman-McClure, Welsch, and generalized Charbonnier loss
functions(and by transitivity the L2, L1, L1-L2, and pseudo-Huber/Charbonnier
loss functions). We describe and visualize this penalty, and document several
of its useful properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1&quot;&gt;Jonathan T. Barron&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.09055">
<title>A Gaussian Process Regression Model for Distribution Inputs. (arXiv:1701.09055v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1701.09055</link>
<description rdf:parseType="Literal">&lt;p&gt;Monge-Kantorovich distances, otherwise known as Wasserstein distances, have
received a growing attention in statistics and machine learning as a powerful
discrepancy measure for probability distributions. In this paper, we focus on
forecasting a Gaussian process indexed by probability distributions. For this,
we provide a family of positive definite kernels built using transportation
based distances. We provide a probabilistic understanding of these kernels and
characterize the corresponding stochastic processes. We prove that the Gaussian
processes indexed by distributions corresponding to these kernels can be
efficiently forecast, opening new perspectives in Gaussian process modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bachoc_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Bachoc&lt;/a&gt; (GdR MASCOT-NUM, IMT), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gamboa_F/0/1/0/all/0/1&quot;&gt;Fabrice Gamboa&lt;/a&gt; (IMT), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loubes_J/0/1/0/all/0/1&quot;&gt;Jean-Michel Loubes&lt;/a&gt; (IMT), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Venet_N/0/1/0/all/0/1&quot;&gt;Nil Venet&lt;/a&gt; (CEA, IMT)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.02403">
<title>On Structured Prediction Theory with Calibrated Convex Surrogate Losses. (arXiv:1703.02403v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.02403</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide novel theoretical insights on structured prediction in the context
of efficient convex surrogate loss minimization with consistency guarantees.
For any task loss, we construct a convex surrogate that can be optimized via
stochastic gradient descent and we prove tight bounds on the so-called
&quot;calibration function&quot; relating the excess surrogate risk to the actual risk.
In contrast to prior related work, we carefully monitor the effect of the
exponential number of classes in the learning guarantees as well as on the
optimization complexity. As an interesting consequence, we formalize the
intuition that some task losses make learning harder than others, and that the
classical 0-1 loss is ill-suited for general structured prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osokin_A/0/1/0/all/0/1&quot;&gt;Anton Osokin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis Bach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1&quot;&gt;Simon Lacoste-Julien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.09866">
<title>Machine learning for graph-based representations of three-dimensional discrete fracture networks. (arXiv:1705.09866v3 [physics.geo-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1705.09866</link>
<description rdf:parseType="Literal">&lt;p&gt;Structural and topological information play a key role in modeling flow and
transport through fractured rock in the subsurface. Discrete fracture network
(DFN) computational suites such as dfnWorks are designed to simulate flow and
transport in such porous media. Flow and transport calculations reveal that a
small backbone of fractures exists, where most flow and transport occurs.
Restricting the flowing fracture network to this backbone provides a
significant reduction in the network&apos;s effective size. However, the particle
tracking simulations needed to determine the reduction are computationally
intensive. Such methods may be impractical for large systems or for robust
uncertainty quantification of fracture networks, where thousands of forward
simulations are needed to bound system behavior.
&lt;/p&gt;
&lt;p&gt;In this paper, we develop an alternative network reduction approach to
characterizing transport in DFNs, by combining graph theoretical and machine
learning methods. We consider a graph representation where nodes signify
fractures and edges denote their intersections. Using random forest and support
vector machines, we rapidly identify a subnetwork that captures the flow
patterns of the full DFN, based primarily on node centrality features in the
graph. Our supervised learning techniques train on particle-tracking backbone
paths found by dfnWorks, but run in negligible time compared to those
simulations. We find that our predictions can reduce the network to
approximately 20% of its original size, while still generating breakthrough
curves consistent with those of the original network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Valera_M/0/1/0/all/0/1&quot;&gt;Manuel Valera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kelly_P/0/1/0/all/0/1&quot;&gt;Priscilla Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Matz_S/0/1/0/all/0/1&quot;&gt;Sean Matz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cantu_A/0/1/0/all/0/1&quot;&gt;Adrian Cantu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Percus_A/0/1/0/all/0/1&quot;&gt;Allon G. Percus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hyman_J/0/1/0/all/0/1&quot;&gt;Jeffrey D. Hyman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Srinivasan_G/0/1/0/all/0/1&quot;&gt;Gowri Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Viswanathan_H/0/1/0/all/0/1&quot;&gt;Hari S. Viswanathan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.09887">
<title>Vocabulary-informed Extreme Value Learning. (arXiv:1705.09887v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1705.09887</link>
<description rdf:parseType="Literal">&lt;p&gt;The novel unseen classes can be formulated as the extreme values of known
classes. This inspired the recent works on open-set recognition
\cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no
way of naming the novel unseen classes. To solve this problem, we propose the
Extreme Value Learning (EVL) formulation to learn the mapping from visual
feature to semantic space. To model the margin and coverage distributions of
each class, the Vocabulary-informed Learning (ViL) is adopted by using vast
open vocabulary in the semantic space. Essentially, by incorporating the EVL
and ViL, we for the first time propose a novel semantic embedding paradigm --
Vocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual
features into semantic space in a probabilistic way. The learned embedding can
be directly used to solve supervised learning, zero-shot and open set
recognition simultaneously. Experiments on two benchmark datasets demonstrate
the effectiveness of proposed frameworks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yanwei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;HanZe Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yu-feng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhengjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiangyang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.03663">
<title>Underdamped Langevin MCMC: A non-asymptotic analysis. (arXiv:1707.03663v7 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.03663</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the underdamped Langevin diffusion when the log of the target
distribution is smooth and strongly concave. We present a MCMC algorithm based
on its discretization and show that it achieves $\varepsilon$ error (in
2-Wasserstein distance) in $\mathcal{O}(\sqrt{d}/\varepsilon)$ steps. This is a
significant improvement over the best known rate for overdamped Langevin MCMC,
which is $\mathcal{O}(d/\varepsilon^2)$ steps under the same
smoothness/concavity assumptions.
&lt;/p&gt;
&lt;p&gt;The underdamped Langevin MCMC scheme can be viewed as a version of
Hamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped
Langevin MCMC methods in a number of application areas. We provide quantitative
rates that support this empirical wisdom.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1&quot;&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1&quot;&gt;Peter L. Bartlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04043">
<title>Few-Shot Learning with Graph Neural Networks. (arXiv:1711.04043v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04043</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to study the problem of few-shot learning with the prism of
inference on a partially observed graphical model, constructed from a
collection of input images whose label can be either observed or not. By
assimilating generic message-passing inference algorithms with their
neural-network counterparts, we define a graph neural network architecture that
generalizes several of the recently proposed few-shot learning models. Besides
providing improved numerical performance, our framework is easily extended to
variants of few-shot learning, such as semi-supervised or active learning,
demonstrating the ability of graph-based models to operate well on &apos;relational&apos;
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garcia_V/0/1/0/all/0/1&quot;&gt;Victor Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bruna_J/0/1/0/all/0/1&quot;&gt;Joan Bruna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10277">
<title>A Stochastic Trust Region Algorithm. (arXiv:1712.10277v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1712.10277</link>
<description rdf:parseType="Literal">&lt;p&gt;An algorithm is proposed for solving stochastic and finite sum minimization
problems. Based on a trust region methodology, the algorithm employs normalized
steps, at least as long as the norms of the stochastic gradient estimates are
within a user-defined interval. The complete algorithm---which dynamically
chooses whether or not to employ normalized steps---is proved to have
convergence guarantees that are similar to those possessed by a traditional
stochastic gradient approach under various sets of conditions related to the
accuracy of the stochastic gradient estimates and choice of stepsize sequence.
The results of numerical experiments are presented when the method is employed
to minimize convex and nonconvex machine learning test problems, illustrating
that the method can outperform a traditional stochastic gradient approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Curtis_F/0/1/0/all/0/1&quot;&gt;Frank E. Curtis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Scheinberg_K/0/1/0/all/0/1&quot;&gt;Katya Scheinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shi_R/0/1/0/all/0/1&quot;&gt;Rui Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06816">
<title>Preferential Attachment Graphs with Planted Communities. (arXiv:1801.06816v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06816</link>
<description rdf:parseType="Literal">&lt;p&gt;A variation of the preferential attachment random graph model of Barab\&apos;asi
and Albert is defined that incorporates planted communities. The graph is built
progressively, with new vertices attaching to the existing ones one-by-one. At
every step, the incoming vertex is randomly assigned a label, which represents
a community it belongs to. This vertex then chooses certain vertices as its
neighbors, with the choice of each vertex being proportional to the degree of
the vertex multiplied by an affinity depending on the labels of the new vertex
and a potential neighbor. It is shown that the fraction of half-edges attached
to vertices with a given label converges almost surely for some classes of
affinity matrices. In addition, the empirical degree distribution for the set
of vertices with a given label converges to a heavy tailed distribution, such
that the tail decay parameter can be different for different communities. Our
proof method may be of independent interest, both for the classical Barab\&apos;asi
-Albert model and for other possible extensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hajek_B/0/1/0/all/0/1&quot;&gt;Bruce Hajek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sankagiri_S/0/1/0/all/0/1&quot;&gt;Suryanarayana Sankagiri&lt;/a&gt;</dc:creator>
</item></rdf:RDF>