<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-15T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05373"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05854"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.00157"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03891"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05408"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05445"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05456"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05518"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05537"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05714"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05769"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05859"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.10494"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10055"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02884"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05324"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05383"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05396"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05409"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05452"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05491"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05606"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05703"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05751"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05809"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05814"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.04335"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.06499"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08856"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03134"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03628"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05757"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07519"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04051"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04784"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.05373">
<title>DeepEM: Deep 3D ConvNets With EM For Weakly Supervised Pulmonary Nodule Detection. (arXiv:1805.05373v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.05373</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently deep learning has been witnessing widespread adoption in various
medical image applications. However, training complex deep neural nets requires
large-scale datasets labeled with ground truth, which are often unavailable in
many medical image domains. For instance, to train a deep neural net to detect
pulmonary nodules in lung computed tomography (CT) images, current practice is
to manually label nodule locations and sizes in many CT images to construct a
sufficiently large training dataset, which is costly and difficult to scale. On
the other hand, electronic medical records (EMR) contain plenty of partial
information on the content of each medical image. In this work, we explore how
to tap this vast, but currently unexplored data source to improve pulmonary
nodule detection. We propose DeepEM, a novel deep 3D ConvNet framework
augmented with expectation-maximization (EM), to mine weakly supervised labels
in EMRs for pulmonary nodule detection. Experimental results show that DeepEM
can lead to 1.5\% and 3.9\% average improvement in free-response receiver
operating characteristic (FROC) scores on LUNA16 and Tianchi datasets,
respectively, demonstrating the utility of incomplete information in EMRs for
improving deep learning
algorithms.\footnote{https://github.com/uci-cbcl/DeepEM-for-Weakly-Supervised-Detection.git}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vang_Y/0/1/0/all/0/1&quot;&gt;Yeeleng S. Vang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yufang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05854">
<title>New directional bat algorithm for continuous optimization problems. (arXiv:1805.05854v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.05854</link>
<description rdf:parseType="Literal">&lt;p&gt;Bat algorithm (BA) is a recent optimization algorithm based on swarm
intelligence and inspiration from the echolocation behavior of bats. One of the
issues in the standard bat algorithm is the premature convergence that can
occur due to the low exploration ability of the algorithm under some
conditions. To overcome this deficiency, directional echolocation is introduced
to the standard bat algorithm to enhance its exploration and exploitation
capabilities. In addition to such directional echolocation, three other
improvements have been embedded into the standard bat algorithm to enhance its
performance. The new proposed approach, namely the directional Bat Algorithm
(dBA), has been then tested using several standard and non-standard benchmarks
from the CEC2005 benchmark suite. The performance of dBA has been compared with
ten other algorithms and BA variants using non-parametric statistical tests.
The statistical test results show the superiority of the directional bat
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakri_A/0/1/0/all/0/1&quot;&gt;Asma Chakri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khelif_R/0/1/0/all/0/1&quot;&gt;Rabia Khelif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benouaret_M/0/1/0/all/0/1&quot;&gt;Mohamed Benouaret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin-She Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.00157">
<title>CaMKII activation supports reward-based neural network optimization through Hamiltonian sampling. (arXiv:1606.00157v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1606.00157</link>
<description rdf:parseType="Literal">&lt;p&gt;Synaptic plasticity is implemented and controlled through over thousand
different types of molecules in the postsynaptic density and presynaptic
boutons that assume a staggering array of different states through
phosporylation and other mechanisms. One of the most prominent molecule in the
postsynaptic density is CaMKII, that is described in molecular biology as a
&quot;memory molecule&quot; that can integrate through auto-phosporylation Ca-influx
signals on a relatively large time scale of dozens of seconds. The functional
impact of this memory mechanism is largely unknown. We show that the
experimental data on the specific role of CaMKII activation in dopamine-gated
spine consolidation suggest a general functional role in speeding up
reward-guided search for network configurations that maximize reward
expectation. Our theoretical analysis shows that stochastic search could in
principle even attain optimal network configurations by emulating one of the
most well-known nonlinear optimization methods, simulated annealing. But this
optimization is usually impeded by slowness of stochastic search at a given
temperature. We propose that CaMKII contributes a momentum term that
substantially speeds up this search. In particular, it allows the network to
overcome saddle points of the fitness function. The resulting improved
stochastic policy search can be understood on a more abstract level as
Hamiltonian sampling, which is known to be one of the most efficient stochastic
search methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kappel_D/0/1/0/all/0/1&quot;&gt;David Kappel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legenstein_R/0/1/0/all/0/1&quot;&gt;Robert Legenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1&quot;&gt;Wolfgang Maass&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03891">
<title>Multifunctionality in embodied agents: Three levels of neural reuse. (arXiv:1802.03891v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03891</link>
<description rdf:parseType="Literal">&lt;p&gt;The brain in conjunction with the body is able to adapt to new environments
and perform multiple behaviors through reuse of neural resources and transfer
of existing behavioral traits. Although mechanisms that underlie this ability
are not well understood, they are largely attributed to neuromodulation. In
this work, we demonstrate that an agent can be multifunctional using the same
sensory and motor systems across behaviors, in the absence of modulatory
mechanisms. Further, we lay out the different levels at which neural reuse can
occur through a dynamical filtering of the brain-body-environment system&apos;s
operation: structural network, autonomous dynamics, and transient dynamics.
Notably, transient dynamics reuse could only be explained by studying the
brain-body-environment system as a whole and not just the brain. The
multifunctional agent we present here demonstrates neural reuse at all three
levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Candadai_M/0/1/0/all/0/1&quot;&gt;Madhavun Candadai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izquierdo_E/0/1/0/all/0/1&quot;&gt;Eduardo Izquierdo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05408">
<title>The Concept of the Deep Learning-Based System &quot;Artificial Dispatcher&quot; to Power System Control and Dispatch. (arXiv:1805.05408v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1805.05408</link>
<description rdf:parseType="Literal">&lt;p&gt;Year by year control of normal and emergency conditions of up-to-date power
systems becomes an increasingly complicated problem. With the increasing
complexity the existing control system of power system conditions which
includes operative actions of the dispatcher and work of special automatic
devices proves to be insufficiently effective more and more frequently, which
raises risks of dangerous and emergency conditions in power systems. The paper
is aimed at compensating for the shortcomings of man (a cognitive barrier,
exposure to stresses and so on) and automatic devices by combining their strong
points, i.e. the dispatcher&apos;s intelligence and the speed of automatic devices
by virtue of development of the intelligent system &quot;Artificial dispatcher&quot; on
the basis of deep machine learning technology. For realization of the system
&quot;Artificial dispatcher&quot; in addition to deep learning it is planned to attract
the game theory approaches to formalize work of the up-to-date power system as
a game problem. The &quot;gain&quot; for &quot;Artificial dispatcher&quot; will consist in bringing
in a power system in the normal steady-state or post-emergency conditions by
means of the required control actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomin_N/0/1/0/all/0/1&quot;&gt;Nikita Tomin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurbatsky_V/0/1/0/all/0/1&quot;&gt;Victor Kurbatsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negnevitsky_M/0/1/0/all/0/1&quot;&gt;Michael Negnevitsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05445">
<title>Exploiting Treewidth for Projected Model Counting and its Limits. (arXiv:1805.05445v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.05445</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel algorithm to solve projected model
counting (PMC). PMC asks to count solutions of a Boolean formula with respect
to a given set of projected variables, where multiple solutions that are
identical when restricted to the projected variables count as only one
solution. Our algorithm exploits small treewidth of the primal graph of the
input instance. It runs in time $O({2^{2^{k+4}} n^2})$ where k is the treewidth
and n is the input size of the instance. In other words, we obtain that the
problem PMC is fixed-parameter tractable when parameterized by treewidth.
Further, we take the exponential time hypothesis (ETH) into consideration and
establish lower bounds of bounded treewidth algorithms for PMC, yielding
asymptotically tight runtime bounds of our algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fichte_J/0/1/0/all/0/1&quot;&gt;Johannes K. Fichte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morak_M/0/1/0/all/0/1&quot;&gt;Michael Morak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hecher_M/0/1/0/all/0/1&quot;&gt;Markus Hecher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woltran_S/0/1/0/all/0/1&quot;&gt;Stefan Woltran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05456">
<title>Wearable Audio and IMU Based Shot Detection in Racquet Sports. (arXiv:1805.05456v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.05456</link>
<description rdf:parseType="Literal">&lt;p&gt;Wearables like smartwatches which are embedded with sensors and powerful
processors, provide a strong platform for development of analytics solutions in
sports domain. To analyze players&apos; games, while motion sensor based shot
detection has been extensively studied in sports like Tennis, Golf, Baseball;
Table Tennis and Badminton are relatively less explored due to possible less
intense hand motion during shots. In our paper, we propose a novel,
computationally inexpensive and real-time system for shot detection in table
tennis, based on fusion of Inertial Measurement Unit (IMU) and audio sensor
data embedded in a wrist-worn wearable. The system builds upon our presented
methodology for synchronizing IMU and audio sensor input in time using detected
shots and achieves 95.6% accuracy. To our knowledge, it is the first
fusion-based solution for sports analysis in wearables. Shot detectors for
other racquet sports as well as further analytics to provide features like shot
classification, rally analysis and recommendations, can easily be built over
our proposed solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1&quot;&gt;Manish Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1&quot;&gt;Akash Anand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_R/0/1/0/all/0/1&quot;&gt;Rupika Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaligounder_L/0/1/0/all/0/1&quot;&gt;Lakshmi Kaligounder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05518">
<title>Formal Modelling of Ontologies : An Event-B based Approach Using the Rodin Platform. (arXiv:1805.05518v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1805.05518</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper reports on the results of the French ANR IMPEX research project
dealing with making explicit domain knowledge in design models. Ontologies are
formalised as theories with sets, axioms, theorems and reasoning rules. They
are integrated to design models through an annotation mechanism. Event-B has
been chosen as the ground formal modelling technique for all our developments.
In this paper, we particularly describe how ontologies are formalised as
Event-B theories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ameur_Y/0/1/0/all/0/1&quot;&gt;Yamine Ait Ameur&lt;/a&gt; (IRIT/INPT-ENSEEIHT, Toulouse, France), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadoune_I/0/1/0/all/0/1&quot;&gt;Idir Ait Sadoune&lt;/a&gt; (LRI/CentraleSupelec/Paris-Saclay University, Plateau de Saclay, France), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hacid_K/0/1/0/all/0/1&quot;&gt;Kahina Hacid&lt;/a&gt; (IRIT/INPT-ENSEEIHT, Toulouse, France), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oussaid_L/0/1/0/all/0/1&quot;&gt;Linda Mohand Oussaid&lt;/a&gt; (LRI/CentraleSupelec/Paris-Saclay University, Plateau de Saclay, France)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05537">
<title>A Dynamic Neural Network Approach to Generating Robot&apos;s Novel Actions: A Simulation Experiment. (arXiv:1805.05537v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1805.05537</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we investigate how a robot can generate novel and creative
actions from its own experience of learning basic actions. Inspired by a
machine learning approach to computational creativity, we propose a dynamic
neural network model that can learn and generate robot&apos;s actions. We conducted
a set of simulation experiments with a humanoid robot. The results showed that
the proposed model was able to learn the basic actions and also to generate
novel actions by modulating and combining those learned actions. The analysis
on the neural activities illustrated that the ability to generate creative
actions emerged from the model&apos;s nonlinear memory structure self-organized
during training. The results also showed that the different way of learning the
basic actions induced the self-organization of the memory structure with the
different characteristics, resulting in the generation of different levels of
creative actions. Our approach can be utilized in human-robot interaction in
which a user can interactively explore the robot&apos;s memory to control its
behavior and also discover other novel actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jungsik Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tani_J/0/1/0/all/0/1&quot;&gt;Jun Tani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05714">
<title>Intrinsic dimension and its application to association rules. (arXiv:1805.05714v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.05714</link>
<description rdf:parseType="Literal">&lt;p&gt;The curse of dimensionality in the realm of association rules is twofold.
Firstly, we have the well known exponential increase in computational
complexity with increasing item set size. Secondly, there is a \emph{related
curse} concerned with the distribution of (spare) data itself in high
dimension. The former problem is often coped with by projection, i.e., feature
selection, whereas the best known strategy for the latter is avoidance. This
work summarizes the first attempt to provide a computationally feasible method
for measuring the extent of dimension curse present in a data set with respect
to a particular class machine of learning procedures. This recent development
enables the application of various other methods from geometric analysis to be
investigated and applied in machine learning procedures in the presence of high
dimension.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanika_T/0/1/0/all/0/1&quot;&gt;Tom Hanika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_F/0/1/0/all/0/1&quot;&gt;Friedrich Martin Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stumme_G/0/1/0/all/0/1&quot;&gt;Gerd Stumme&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05769">
<title>Leveraging human knowledge in tabular reinforcement learning: A study of human subjects. (arXiv:1805.05769v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.05769</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning (RL) can be extremely effective in solving complex,
real-world problems. However, injecting human knowledge into an RL agent may
require extensive effort and expertise on the human designer&apos;s part. To date,
human factors are generally not considered in the development and evaluation of
possible RL approaches. In this article, we set out to investigate how
different methods for injecting human knowledge are applied, in practice, by
human designers of varying levels of knowledge and skill. We perform the first
empirical evaluation of several methods, including a newly proposed method
named SASS which is based on the notion of similarities in the agent&apos;s
state-action space. Through this human study, consisting of 51 human
participants, we shed new light on the human factors that play a key role in
RL. We find that the classical reward shaping technique seems to be the most
natural method for most designers, both expert and non-expert, to speed up RL.
However, we further find that our proposed method SASS can be effectively and
efficiently combined with reward shaping, and provides a beneficial alternative
to using only a single speedup method with minimal human designer effort
overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenfeld_A/0/1/0/all/0/1&quot;&gt;Ariel Rosenfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_M/0/1/0/all/0/1&quot;&gt;Moshe Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1&quot;&gt;Matthew E. Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraus_S/0/1/0/all/0/1&quot;&gt;Sarit Kraus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05859">
<title>Causal Reasoning for Algorithmic Fairness. (arXiv:1805.05859v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.05859</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we argue for the importance of causal reasoning in creating
fair algorithms for decision making. We give a review of existing approaches to
fairness, describe work in causality necessary for the understanding of causal
approaches, argue why causality is necessary for any approach that wishes to be
fair, and give a detailed analysis of the many recent approaches to
causality-based fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loftus_J/0/1/0/all/0/1&quot;&gt;Joshua R. Loftus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1&quot;&gt;Chris Russell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1&quot;&gt;Matt J. Kusner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1&quot;&gt;Ricardo Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.10494">
<title>Discovery and recognition of motion primitives in human activities. (arXiv:1709.10494v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1709.10494</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel framework for the automatic discovery and recognition of
motion primitives in videos of human activities. Given the 3D pose of a human
in a video, human motion primitives are discovered by optimizing the `motion
flux&apos;, a quantity which captures the motion variation of a group of skeletal
joints. A normalization of the primitives is proposed in order to make them
invariant with respect to a subject anatomical variations and data sampling
rate. The discovered primitives are unknown and unlabeled and are
unsupervisedly collected into classes via a hierarchical non-parametric Bayes
mixture model. Once classes are determined and labeled they are further
analyzed for establishing models for recognizing discovered primitives. Each
primitive model is defined by a set of learned parameters.
&lt;/p&gt;
&lt;p&gt;Given new video data and given the estimated pose of the subject appearing on
the video, the motion is segmented into primitives, which are recognized with a
probability given according to the parameters of the learned models.
&lt;/p&gt;
&lt;p&gt;Using our framework we build a publicly available dataset of human motion
primitives, using sequences taken from well-known motion capture datasets. We
expect that our framework, by providing an objective way for discovering and
categorizing human motion, will be a useful tool in numerous research fields
including video analysis, human inspired motion generation, learning by
demonstration, intuitive human-robot interaction, and human behavior analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanzari_M/0/1/0/all/0/1&quot;&gt;Marta Sanzari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ntouskos_V/0/1/0/all/0/1&quot;&gt;Valsamis Ntouskos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pirri_F/0/1/0/all/0/1&quot;&gt;Fiora Pirri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10055">
<title>Features, Projections, and Representation Change for Generalized Planning. (arXiv:1801.10055v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.10055</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized planning is concerned with the characterization and computation
of plans that solve many instances at once. In the standard formulation, a
generalized plan is a mapping from feature or observation histories into
actions, assuming that the instances share a common pool of features and
actions. This assumption, however, excludes the standard relational planning
domains where actions and objects change across instances. In this work, we
extend the standard formulation of generalized planning to such domains. This
is achieved by projecting the actions over the features, resulting in a common
set of abstract actions which can be tested for soundness and completeness, and
which can be used for generating general policies such as &quot;if the gripper is
empty, pick the clear block above x and place it on the table&quot; that achieve the
goal clear(x) in any Blocksworld instance. In this policy, &quot;pick the clear
block above x&quot; is an abstract action that may represent the action Unstack(a,
b) in one situation and the action Unstack(b, c) in another. Transformations
are also introduced for computing such policies by means of fully observable
non-deterministic (FOND) planners. The value of generalized representations for
learning general policies is also discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonet_B/0/1/0/all/0/1&quot;&gt;Blai Bonet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geffner_H/0/1/0/all/0/1&quot;&gt;Hector Geffner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02884">
<title>Synthesis in pMDPs: A Tale of 1001 Parameters. (arXiv:1803.02884v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02884</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers parametric Markov decision processes (pMDPs) whose
transitions are equipped with affine functions over a finite set of parameters.
The synthesis problem is to find a parameter valuation such that the
instantiated pMDP satisfies a specification under all strategies. We show that
this problem can be formulated as a quadratically-constrained quadratic program
(QCQP) and is non-convex in general. To deal with the NP-hardness of such
problems, we exploit a convex-concave procedure (CCP) to iteratively obtain
local optima. An appropriate interplay between CCP solvers and probabilistic
model checkers creates a procedure --- realized in the open-source tool
PROPhESY --- that solves the synthesis problem for models with thousands of
parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cubuktepe_M/0/1/0/all/0/1&quot;&gt;Murat Cubuktepe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jansen_N/0/1/0/all/0/1&quot;&gt;Nils Jansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junges_S/0/1/0/all/0/1&quot;&gt;Sebastian Junges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katoen_J/0/1/0/all/0/1&quot;&gt;Joost-Pieter Katoen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1&quot;&gt;Ufuk Topcu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05324">
<title>Extended pipeline for content-based feature engineering in music genre recognition. (arXiv:1805.05324v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1805.05324</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a feature engineering pipeline for the construction of musical
signal characteristics, to be used for the design of a supervised model for
musical genre identification. The key idea is to extend the traditional
two-step process of extraction and classification with additive stand-alone
phases which are no longer organized in a waterfall scheme. The whole system is
realized by traversing backtrack arrows and cycles between various stages. In
order to give a compact and effective representation of the features, the
standard early temporal integration is combined with other selection and
extraction phases: on the one hand, the selection of the most meaningful
characteristics based on information gain, and on the other hand, the inclusion
of the nonlinear correlation between this subset of features, determined by an
autoencoder. The results of the experiments conducted on GTZAN dataset reveal a
noticeable contribution of this methodology towards the model&apos;s performance in
classification task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raissi_T/0/1/0/all/0/1&quot;&gt;Tina Raissi&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tibo_A/0/1/0/all/0/1&quot;&gt;Alessandro Tibo&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bientinesi_P/0/1/0/all/0/1&quot;&gt;Paolo Bientinesi&lt;/a&gt; (1), ((1) RWTH Aachen University, (2) University of Florence)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05383">
<title>Spatio-temporal Bayesian On-line Changepoint Detection with Model Selection. (arXiv:1805.05383v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.05383</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian On-line Changepoint Detection is extended to on-line model selection
and non-stationary spatio-temporal processes. We propose spatially structured
Vector Autoregressions (VARs) for modelling the process between changepoints
(CPs) and give an upper bound on the approximation error of such models. The
resulting algorithm performs prediction, model selection and CP detection
on-line. Its time complexity is linear and its space complexity constant, and
thus it is two orders of magnitudes faster than its closest competitor. In
addition, it outperforms the state of the art for multivariate data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Knoblauch_J/0/1/0/all/0/1&quot;&gt;Jeremias Knoblauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Damoulas_T/0/1/0/all/0/1&quot;&gt;Theodoros Damoulas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05396">
<title>Confidence Scoring Using Whitebox Meta-models with Linear Classifier Probes. (arXiv:1805.05396v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.05396</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a confidence scoring mechanism for multi-layer neural networks
based on a paradigm of a base model and a meta-model. The confidence score is
learned by the meta-model using features derived from the base model -- a deep
multi-layer neural network -- considered a whitebox. As features, we
investigate linear classifier probes inserted between the various layers of the
base model and trained using each layer&apos;s intermediate activations. Experiments
show that this approach outperforms various baselines in a filtering task,
i.e., task of rejecting samples with low confidence. Experimental results are
presented using CIFAR-10 and CIFAR-100 dataset with and without added noise
exploring various aspects of the method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tongfei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navratil_J/0/1/0/all/0/1&quot;&gt;Ji&amp;#x159;&amp;#xed; Navr&amp;#xe1;til&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyengar_V/0/1/0/all/0/1&quot;&gt;Vijay Iyengar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Shanmugam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05409">
<title>Machine Learning for Public Administration Research, with Application to Organizational Reputation. (arXiv:1805.05409v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1805.05409</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning methods have gained a great deal of popularity in recent
years among public administration scholars and practitioners. These techniques
open the door to the analysis of text, image and other types of data that allow
us to test foundational theories of public administration and to develop new
theories. Despite the excitement surrounding machine learning methods, clarity
regarding their proper use and potential pitfalls is lacking. This paper
attempts to fill this gap in the literature through providing a machine
learning &quot;guide to practice&quot; for public administration scholars and
practitioners. Here, we take a foundational view of machine learning and
describe how these methods can enrich public administration research and
practice through their ability develop new measures, tap into new sources of
data and conduct statistical inference and causal inference in a principled
manner. We then turn our attention to the pitfalls of using these methods such
as unvalidated measures and lack of interpretability. Finally, we demonstrate
how machine learning techniques can help us learn about organizational
reputation in federal agencies through an illustrated example using tweets from
13 executive federal agencies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anastasopoulos_L/0/1/0/all/0/1&quot;&gt;L. Jason Anastasopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitford_A/0/1/0/all/0/1&quot;&gt;Andrew B. Whitford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05431">
<title>Comparison of Classical and Nonlinear Models for Short-Term Electricity Price Prediction. (arXiv:1805.05431v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1805.05431</link>
<description rdf:parseType="Literal">&lt;p&gt;Electricity is bought and sold in wholesale markets at prices that fluctuate
significantly. Short-term forecasting of electricity prices is an important
endeavor because it helps electric utilities control risk and because it
influences competitive strategy for generators. As the &quot;smart grid&quot; grows,
short-term price forecasts are becoming an important input to bidding and
control algorithms for battery operators and demand response aggregators. While
the statistics and machine learning literature offers many proposed methods for
electricity price prediction, there is no consensus supporting a single best
approach. We test two contrasting machine learning approaches for predicting
electricity prices, regression decision trees and recurrent neural networks
(RNNs), and compare them to a more traditional ARIMA implementation. We conduct
the analysis on a challenging dataset of electricity prices from ERCOT, in
Texas, where price fluctuation is especially high. We find that regression
decision trees in particular achieves high performance compared to the other
methods, suggesting that regression trees should be more carefully considered
for electricity price forecasting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fata_E/0/1/0/all/0/1&quot;&gt;Elaheh Fata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadota_I/0/1/0/all/0/1&quot;&gt;Igor Kadota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_I/0/1/0/all/0/1&quot;&gt;Ian Schneider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05452">
<title>Improved Predictive Models for Acute Kidney Injury with IDEAs: Intraoperative Data Embedded Analytics. (arXiv:1805.05452v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1805.05452</link>
<description rdf:parseType="Literal">&lt;p&gt;Acute kidney injury (AKI) is a common and serious complication after a
surgery which is associated with morbidity and mortality. The majority of
existing perioperative AKI risk score prediction models are limited in their
generalizability and do not fully utilize the physiological intraoperative
time-series data. Thus, there is a need for intelligent, accurate, and robust
systems, able to leverage information from large-scale data to predict
patient&apos;s risk of developing postoperative AKI. A retrospective single-center
cohort of 2,911 adult patients who underwent surgery at the University of
Florida Health has been used for this study. We used machine learning and
statistical analysis techniques to develop perioperative models to predict the
risk of AKI (risk during the first 3 days, 7 days, and until the discharge day)
before and after the surgery. In particular, we examined the improvement in
risk prediction by incorporating three intraoperative physiologic time series
data, i.e., mean arterial blood pressure, minimum alveolar concentration, and
heart rate. For an individual patient, the preoperative model produces a
probabilistic AKI risk score, which will be enriched by integrating
intraoperative statistical features through a machine learning stacking
approach inside a random forest classifier. We compared the performance of our
model based on the area under the receiver operating characteristics curve
(AUROC), accuracy and net reclassification improvement (NRI). The predictive
performance of the proposed model is better than the preoperative data only
model. For AKI-7day outcome: The AUC was 0.86 (accuracy was 0.78) in the
proposed model, while the preoperative AUC was 0.84 (accuracy 0.76).
Furthermore, with the integration of intraoperative features, we were able to
classify patients who were misclassified in the preoperative model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adhikari_L/0/1/0/all/0/1&quot;&gt;Lasith Adhikari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozrazgat_Baslanti_T/0/1/0/all/0/1&quot;&gt;Tezcan Ozrazgat-Baslanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thottakkara_P/0/1/0/all/0/1&quot;&gt;Paul Thottakkara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebadi_A/0/1/0/all/0/1&quot;&gt;Ashkan Ebadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motaei_A/0/1/0/all/0/1&quot;&gt;Amir Motaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashidi_P/0/1/0/all/0/1&quot;&gt;Parisa Rashidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaolin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bihorac_A/0/1/0/all/0/1&quot;&gt;Azra Bihorac&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05491">
<title>Crowdbreaks: Tracking Health Trends using Public Social Media Data and Crowdsourcing. (arXiv:1805.05491v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1805.05491</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past decade, tracking health trends using social media data has shown
great promise, due to a powerful combination of massive adoption of social
media around the world, and increasingly potent hardware and software that
enables us to work with these new big data streams. At the same time, many
challenging problems have been identified. First, there is often a mismatch
between how rapidly online data can change, and how rapidly algorithms are
updated, which means that there is limited reusability for algorithms trained
on past data as their performance decreases over time. Second, much of the work
is focusing on specific issues during a specific past period in time, even
though public health institutions would need flexible tools to assess multiple
evolving situations in real time. Third, most tools providing such capabilities
are proprietary systems with little algorithmic or data transparency, and thus
little buy-in from the global public health and research community. Here, we
introduce Crowdbreaks, an open platform which allows tracking of health trends
by making use of continuous crowdsourced labelling of public social media
content. The system is built in a way which automatizes the typical workflow
from data collection, filtering, labelling and training of machine learning
classifiers and therefore can greatly accelerate the research process in the
public health domain. This work introduces the technical aspects of the
platform and explores its future use cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueller_M/0/1/0/all/0/1&quot;&gt;Martin Mueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salathe_M/0/1/0/all/0/1&quot;&gt;Marcel Salath&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05606">
<title>Nonparametric Bayesian volatility learning under microstructure noise. (arXiv:1805.05606v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1805.05606</link>
<description rdf:parseType="Literal">&lt;p&gt;Aiming at financial applications, we study the problem of learning the
volatility under market microstructure noise. Specifically, we consider noisy
discrete time observations from a stochastic differential equation and develop
a novel computational method to learn the diffusion coefficient of the
equation. We take a nonparametric Bayesian approach, where we model the
volatility function a priori as piecewise constant. Its prior is specified via
the inverse Gamma Markov chain. Sampling from the posterior is accomplished by
incorporating the Forward Filtering Backward Simulation algorithm in the Gibbs
sampler. Good performance of the method is demonstrated on two representative
synthetic data examples. Finally, we apply the method on the EUR/USD exchange
rate dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gugushvili_S/0/1/0/all/0/1&quot;&gt;Shota Gugushvili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meulen_F/0/1/0/all/0/1&quot;&gt;Frank van der Meulen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schauer_M/0/1/0/all/0/1&quot;&gt;Moritz Schauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spreij_P/0/1/0/all/0/1&quot;&gt;Peter Spreij&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05703">
<title>The Hierarchical Adaptive Forgetting Variational Filter. (arXiv:1805.05703v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.05703</link>
<description rdf:parseType="Literal">&lt;p&gt;A common problem in Machine Learning and statistics consists in detecting
whether the current sample in a stream of data belongs to the same distribution
as previous ones, is an isolated outlier or inaugurates a new distribution of
data. We present a hierarchical Bayesian algorithm that aims at learning a
time-specific approximate posterior distribution of the parameters describing
the distribution of the data observed. We derive the update equations of the
variational parameters of the approximate posterior at each time step for
models from the exponential family, and show that these updates find
interesting correspondents in Reinforcement Learning (RL). In this perspective,
our model can be seen as a hierarchical RL algorithm that learns a posterior
distribution according to a certain stability confidence that is, in turn,
learned according to its own stability confidence. Finally, we show some
applications of our generic model, first in a RL context, next with an adaptive
Bayesian Autoregressive model, and finally in the context of Stochastic
Gradient Descent optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moens_V/0/1/0/all/0/1&quot;&gt;Vincent Moens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05751">
<title>Local Saddle Point Optimization: A Curvature Exploitation Approach. (arXiv:1805.05751v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.05751</link>
<description rdf:parseType="Literal">&lt;p&gt;Gradient-based optimization methods are the most popular choice for finding
local optima for classical minimization and saddle point problems. Here, we
highlight a systemic issue of gradient dynamics that arise for saddle point
problems, namely the presence of undesired stable stationary points that are no
local optima. We propose a novel optimization approach that exploits curvature
information in order to escape from these undesired stationary points. We prove
that different optimization methods, including gradient method and adagrad,
equipped with curvature exploitation can escape non-optimal stationary points.
We also provide empirical results on common saddle point problems which confirm
the advantage of using curvature exploitation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1&quot;&gt;Leonard Adolphs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daneshmand_H/0/1/0/all/0/1&quot;&gt;Hadi Daneshmand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1&quot;&gt;Aurelien Lucchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1&quot;&gt;Thomas Hofmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05809">
<title>Efficient end-to-end learning for quantizable representations. (arXiv:1805.05809v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.05809</link>
<description rdf:parseType="Literal">&lt;p&gt;Embedding representation learning via neural networks is at the core
foundation of modern similarity based search. While much effort has been put in
developing algorithms for learning binary hamming code representations for
search efficiency, this still requires a linear scan of the entire dataset per
each query and trades off the search accuracy through binarization. To this
end, we consider the problem of directly learning a quantizable embedding
representation and the sparse binary hash code end-to-end which can be used to
construct an efficient hash table not only providing significant search
reduction in the number of data but also achieving the state of the art search
accuracy outperforming previous state of the art deep metric learning methods.
We also show that finding the optimal sparse binary hash code in a mini-batch
can be computed exactly in polynomial time by solving a minimum cost flow
problem. Our results on Cifar-100 and on ImageNet datasets show the state of
the art search accuracy in precision@k and NMI metrics while providing up to
98X and 478X search speedup respectively over exhaustive linear search.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1&quot;&gt;Yeonwoo Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hyun Oh Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05814">
<title>SHADE: Information-Based Regularization for Deep Learning. (arXiv:1805.05814v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.05814</link>
<description rdf:parseType="Literal">&lt;p&gt;Regularization is a big issue for training deep neural networks. In this
paper, we propose a new information-theory-based regularization scheme named
SHADE for SHAnnon DEcay. The originality of the approach is to define a prior
based on conditional entropy, which explicitly decouples the learning of
invariant representations in the regularizer and the learning of correlations
between inputs and labels in the data fitting term. Our second contribution is
to derive a stochastic version of the regularizer compatible with deep
learning, resulting in a tractable training scheme. We empirically validate the
efficiency of our approach to improve classification performances compared to
standard regularization schemes on several standard architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blot_M/0/1/0/all/0/1&quot;&gt;Michael Blot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Robert_T/0/1/0/all/0/1&quot;&gt;Thomas Robert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thome_N/0/1/0/all/0/1&quot;&gt;Nicolas Thome&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cord_M/0/1/0/all/0/1&quot;&gt;Matthieu Cord&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.04335">
<title>Practical Bayesian Optimization for Variable Cost Objectives. (arXiv:1703.04335v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.04335</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel Bayesian Optimization approach for black-box functions
with an environmental variable whose value determines the tradeoff between
evaluation cost and the fidelity of the evaluations. Further, we use a novel
approach to sampling support points, allowing faster construction of the
acquisition function. This allows us to achieve optimization with lower
overheads than previous approaches and is implemented for a more general class
of problem. We show this approach to be effective on synthetic and real world
benchmark problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McLeod_M/0/1/0/all/0/1&quot;&gt;Mark McLeod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osborne_M/0/1/0/all/0/1&quot;&gt;Michael A. Osborne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen J. Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.06499">
<title>A Non-monotone Alternating Updating Method for A Class of Matrix Factorization Problems. (arXiv:1705.06499v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1705.06499</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we consider a general matrix factorization model which covers a
large class of existing models with many applications in areas such as machine
learning and imaging sciences. To solve this possibly nonconvex, nonsmooth and
non-Lipschitz problem, we develop a non-monotone alternating updating method
based on a potential function. Our method essentially updates two blocks of
variables in turn by inexactly minimizing this potential function, and updates
another auxiliary block of variables using an explicit formula. The special
structure of our potential function allows us to take advantage of efficient
computational strategies for non-negative matrix factorization to perform the
alternating minimization over the two blocks of variables. A suitable line
search criterion is also incorporated to improve the numerical performance.
Under some mild conditions, we show that the line search criterion is well
defined, and establish that the sequence generated is bounded and any cluster
point of the sequence is a stationary point. Finally, we conduct some numerical
experiments using real datasets to compare our method with some existing
efficient methods for non-negative matrix factorization and matrix completion.
The numerical results show that our method can outperform these methods for
these specific applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pong_T/0/1/0/all/0/1&quot;&gt;Ting Kei Pong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaojun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08856">
<title>Critical Learning Periods in Deep Neural Networks. (arXiv:1711.08856v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08856</link>
<description rdf:parseType="Literal">&lt;p&gt;Critical periods are phases in the early development of humans and animals
during which experience can irreversibly affect the architecture of neuronal
networks. In this work, we study the effects of visual stimulus deficits on the
training of artificial neural networks (ANNs). Introducing well-characterized
visual deficits, such as cataract-like blurring, in the early training phase of
a standard deep neural network causes a permanent performance loss that closely
mimics critical period behavior in humans and animal models. Deficits that do
not affect low-level image statistics, such as vertical flipping of the images,
have no lasting effect on the ANNs&apos; performance and can be rapidly overcome
with further training. In addition, the deeper the ANN is, the more pronounced
the critical period. To better understand this phenomenon, we use Fisher
Information as a measure of the strength of the network&apos;s connections during
the training. Our information-theoretic analysis suggests that the first few
epochs are critical for the creation of strong connections across different
layers, optimal for processing the input data distribution. Once such strong
connections are created, they do not appear to change during additional
training. These findings suggest that the initial rapid learning phase of ANN
training, under-scrutinized compared to its asymptotic behavior, plays a key
role in defining the final performance of networks. Our results also show how
critical periods are not restricted to biological systems, but can emerge
naturally in learning systems, whether biological or artificial, due to
fundamental constrains arising from learning dynamics and information
processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1&quot;&gt;Alessandro Achille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rovere_M/0/1/0/all/0/1&quot;&gt;Matteo Rovere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03134">
<title>On Adaptive Estimation for Dynamic Bernoulli Bandits. (arXiv:1712.03134v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.03134</link>
<description rdf:parseType="Literal">&lt;p&gt;The multi-armed bandit (MAB) problem is a classic example of the
exploration-exploitation dilemma. It is concerned with maximising the total
rewards for a gambler by sequentially pulling an arm from a multi-armed slot
machine where each arm is associated with a reward distribution. In static
MABs, the reward distributions do not change over time, while in dynamic MABs,
each arm&apos;s reward distribution can change, and the optimal arm can switch over
time. Motivated by many real applications where rewards are binary, we focus on
dynamic Bernoulli bandits. Standard methods like $\epsilon$-Greedy and Upper
Confidence Bound (UCB), which rely on the sample mean estimator, often fail to
track changes in the underlying reward for dynamic problems. In this paper, we
overcome the shortcoming of slow response to change by deploying adaptive
estimation in the standard methods and propose a new family of algorithms,
which are adaptive versions of $\epsilon$-Greedy, UCB, and Thompson sampling.
These new methods are simple and easy to implement. Moreover, they do not
require any prior knowledge about the dynamic reward process, which is
important for real applications. We examine the new algorithms numerically in
different scenarios and the results show solid improvements of our algorithms
in dynamic environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xue Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Adams_N/0/1/0/all/0/1&quot;&gt;Niall Adams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kantas_N/0/1/0/all/0/1&quot;&gt;Nikolas Kantas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03628">
<title>Learning Correlation Space for Time Series. (arXiv:1802.03628v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03628</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an approximation algorithm for efficient correlation search in
time series data. In our method, we use Fourier transform and neural network to
embed time series into a low-dimensional Euclidean space. The given space is
learned such that time series correlation can be effectively approximated from
Euclidean distance between corresponding embedded vectors. Therefore, search
for correlated time series can be done using an index in the embedding space
for efficient nearest neighbor search. Our theoretical analysis illustrates
that our method&apos;s accuracy can be guaranteed under certain regularity
conditions. We further conduct experiments on real-world datasets and the
results show that our method indeed outperforms the baseline solution. In
particular, for approximation of correlation, our method reduces the
approximation loss by a half in most test cases compared to the baseline
solution. For top-$k$ highest correlation search, our method improves the
precision from 5\% to 20\% while the query time is similar to the baseline
approach query time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Han Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_H/0/1/0/all/0/1&quot;&gt;Hoang Thanh Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1&quot;&gt;Francesco Fusco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinn_M/0/1/0/all/0/1&quot;&gt;Mathieu Sinn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05757">
<title>Stochastic Wasserstein Barycenters. (arXiv:1802.05757v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05757</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a stochastic algorithm to compute the barycenter of a set of
probability distributions under the Wasserstein metric from optimal transport.
Unlike previous approaches, our method extends to continuous input
distributions and allows the support of the barycenter to be adjusted in each
iteration. We tackle the problem without regularization, allowing us to recover
a sharp output whose support is contained within the support of the true
barycenter. We give examples where our algorithm recovers a more meaningful
barycenter than previous work. Our method is versatile and can be extended to
applications such as generating super samples from a given distribution and
recovering blue noise approximations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Claici_S/0/1/0/all/0/1&quot;&gt;Sebastian Claici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1&quot;&gt;Edward Chien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1&quot;&gt;Justin Solomon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07519">
<title>DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems. (arXiv:1803.07519v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07519</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning defines a new data-driven programming paradigm that constructs
the internal system logic of a crafted neuron network through a set of training
data. Deep learning (DL) has been widely adopted in many safety-critical
scenarios. However, a plethora of studies have shown that the state-of-the-art
DL systems suffer from various vulnerabilities which can lead to severe
consequences when applied to real-world applications. Currently, the robustness
of a DL system against adversarial attacks is usually measured by the accuracy
of test data. Considering the limitation of accessible high quality test data,
good accuracy performance on test data can hardly guarantee the robustness and
generality of DL systems. Different from traditional software systems which
have clear and controllable logic and functionality, a DL system is trained
with data and lacks thorough understanding. This makes it difficult for system
analysis and defect detection, which could potentially hinder its real-world
deployment without safety guarantees. In this paper, we propose DeepGauge,
comprehensive and multi-granularity testing criteria for DL systems, which aims
to render a complete and multi-faceted portrayal of the testbed. The in-depth
evaluation of our proposed testing criteria is demonstrated on two well-known
datasets, five DL systems, with four state-of-the-art attack techniques against
DL. The effectiveness of DeepGauge sheds light on the construction of robust DL
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1&quot;&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiyuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1&quot;&gt;Minhui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chunyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_T/0/1/0/all/0/1&quot;&gt;Ting Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jianjun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yadong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04051">
<title>Classification of Household Materials via Spectroscopy. (arXiv:1805.04051v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1805.04051</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognizing an object&apos;s material can inform a robot on how hard it may grasp
the object during manipulation, or if the object may be safely heated up. To
estimate an object&apos;s material during manipulation, many prior works have
explored the use of haptic sensing. In this paper, we explore a technique for
robots to estimate the materials of objects using spectroscopy. We demonstrate
that spectrometers provide several benefits for material recognition, including
fast sensing times and accurate measurements with low noise. Furthermore,
spectrometers do not require direct contact with an object. To illustrate this,
we collected a dataset of spectral measurements from two commercially available
spectrometers during which a robotic platform interacted with 50 distinct
objects, and we show that a residual neural network can accurately analyze
these measurements. Due to the low variance in consecutive spectral
measurements, our model achieved a material classification accuracy of 97.7%
when given only one spectral sample per object. Similar to prior works with
haptic sensors, we found that generalizing material recognition to new objects
posed a greater challenge, for which we achieved an accuracy of 81.4% via
leave-one-object-out cross-validation. From this work, we find that
spectroscopy poses a promising approach for further research in material
classification during robotic manipulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erickson_Z/0/1/0/all/0/1&quot;&gt;Zackory Erickson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luskey_N/0/1/0/all/0/1&quot;&gt;Nathan Luskey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1&quot;&gt;Sonia Chernova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1&quot;&gt;Charles C. Kemp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04784">
<title>Nonlinear Metric Learning through Geodesic Polylinear Interpolation (ML-GPI). (arXiv:1805.04784v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.04784</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a nonlinear distance metric learning scheme based
on the fusion of component linear metrics. Instead of merging displacements at
each data point, our model calculates the velocities induced by the component
transformations, via a geodesic interpolation on a Lie transfor- mation group.
Such velocities are later summed up to produce a global transformation that is
guaranteed to be diffeomorphic. Consequently, pair-wise distances computed this
way conform to a smooth and spatially varying metric, which can greatly benefit
k-NN classification. Experiments on synthetic and real datasets demonstrate the
effectiveness of our model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhewei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bibo Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_C/0/1/0/all/0/1&quot;&gt;Charles D. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jundong Liu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>