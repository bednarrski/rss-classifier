<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-24T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08319"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08521"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05772"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08164"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08266"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08443"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08588"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.06215"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.02792"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04271"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07745"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08211"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08235"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08240"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08243"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08259"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08289"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08363"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08449"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08493"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08577"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.07904"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.09847"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.02538"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00443"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.08319">
<title>Virtual Sensor Modelling using Neural Networks with Coefficient-based Adaptive Weights and Biases Search Algorithm for Diesel Engines. (arXiv:1712.08319v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.08319</link>
<description rdf:parseType="Literal">&lt;p&gt;With the explosion in the field of Big Data and introduction of more
stringent emission norms every three to five years, automotive companies must
not only continue to enhance the fuel economy ratings of their products, but
also provide valued services to their customers such as delivering engine
performance and health reports at regular intervals. A reasonable solution to
both issues is installing a variety of sensors on the engine. Sensor data can
be used to develop fuel economy features and will directly indicate engine
performance. However, mounting a plethora of sensors is impractical in a very
cost-sensitive industry. Thus, virtual sensors can replace physical sensors by
reducing cost while capturing essential engine data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastogi_K/0/1/0/all/0/1&quot;&gt;Kushagra Rastogi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saini_N/0/1/0/all/0/1&quot;&gt;Navreet Saini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08521">
<title>An Incremental Self-Organizing Architecture for Sensorimotor Learning and Prediction. (arXiv:1712.08521v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.08521</link>
<description rdf:parseType="Literal">&lt;p&gt;During visuomotor tasks, robots have to compensate for the temporal delays
inherent in their sensorimotor processing systems. This capability becomes
crucial in a dynamic environment where the visual input is constantly changing,
e.g. when interacting with humans. For this purpose, the robot should be
equipped with a prediction mechanism able to use the acquired perceptual
experience in order to estimate possible future motor commands. In this paper,
we present a novel neural network architecture that learns prototypical
visuomotor representations and provides reliable predictions to compensate for
the delayed robot behavior in an online manner. We investigate the performance
of our method in the context of a synchronization task, where a humanoid robot
has to generate visually perceived arm motion trajectories in synchrony with a
human demonstrator. We evaluate the prediction accuracy in terms of mean
prediction error and analyze the response of the network to novel movement
demonstrations. Additionally, we provide experiments with the system receiving
incomplete data sequences, showing the robustness of the proposed architecture
in the case of a noisy and faulty visual sensor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mici_L/0/1/0/all/0/1&quot;&gt;Luiza Mici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parisi_G/0/1/0/all/0/1&quot;&gt;German I. Parisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05772">
<title>Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models. (arXiv:1711.05772v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05772</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative neural networks have proven effective at both conditional and
unconditional modeling of complex data distributions. Conditional generation
enables interactive control, but creating new controls often requires expensive
retraining. In this paper, we develop a method to condition generation without
retraining the model. By post-hoc learning latent constraints, value functions
that identify regions in latent space that generate outputs with desired
attributes, we can conditionally sample from these regions with gradient-based
optimization or amortized actor functions. Combining attribute constraints with
a universal &quot;realism&quot; constraint, which enforces similarity to the data
distribution, we generate realistic conditional images from an unconditional
variational autoencoder. Further, using gradient-based optimization, we
demonstrate identity-preserving transformations that make the minimal
adjustment in latent space to modify the attributes of an image. Finally, with
discrete sequences of musical notes, we demonstrate zero-shot conditional
generation, learning latent constraints in the absence of labeled data or a
differentiable reward function. Code with dedicated cloud instance has been
made publicly available (https://goo.gl/STGMGx).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engel_J/0/1/0/all/0/1&quot;&gt;Jesse Engel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffman_M/0/1/0/all/0/1&quot;&gt;Matthew Hoffman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1&quot;&gt;Adam Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08164">
<title>Multi-task learning of time series and its application to the travel demand. (arXiv:1712.08164v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.08164</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of modeling and prediction of a set of temporal events
in the context of intelligent transportation systems. To leverage the
information shared by different events, we propose a multi-task learning
framework. We develop a support vector regression model for joint learning of
mutually dependent time series. It is the regularization-based multi-task
learning previously developed for the classification case and extended to time
series. We discuss the relatedness of observed time series and first deploy the
dynamic time warping distance measure to identify groups of similar series.
Then we take into account both time and scale warping and propose to align
multiple time series by inferring their common latent representation. We test
the proposed models on the problem of travel demand prediction in Nancy
(France) public transport system and analyze the benefits of multi-task
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chidlovskii_B/0/1/0/all/0/1&quot;&gt;Boris Chidlovskii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08266">
<title>Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning. (arXiv:1712.08266v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.08266</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a framework combining hierarchical and multi-agent deep
reinforcement learning approaches to solve coordination problems among a
multitude of agents using a semi-decentralized model. The framework extends the
multi-agent learning setup by introducing a meta-controller that guides the
communication between agent pairs, enabling agents to focus on communicating
with only one other agent at any step. This hierarchical decomposition of the
task allows for efficient exploration to learn policies that identify globally
optimal solutions even as the number of collaborating agents increases. We show
promising initial experimental results on a simulated distributed scheduling
problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Saurabh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1&quot;&gt;Pararth Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1&quot;&gt;Dilek Hakkani-Tur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heck_L/0/1/0/all/0/1&quot;&gt;Larry Heck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08443">
<title>Inverse Classification for Comparison-based Interpretability in Machine Learning. (arXiv:1712.08443v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.08443</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of post-hoc interpretability, this paper addresses the task of
explaining the prediction of a classifier, considering the case where no
information is available, neither on the classifier itself, nor on the
processed data (neither the training nor the test data). It proposes an
instance-based approach whose principle consists in determining the minimal
changes needed to alter a prediction: given a data point whose classification
must be explained, the proposed method consists in identifying a close
neighbour classified differently, where the closeness definition integrates a
sparsity constraint. This principle is implemented using observation generation
in the Growing Spheres algorithm. Experimental results on two datasets
illustrate the relevance of the proposed approach that can be used to gain
knowledge about the classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laugel_T/0/1/0/all/0/1&quot;&gt;Thibault Laugel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lesot_M/0/1/0/all/0/1&quot;&gt;Marie-Jeanne Lesot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marsala_C/0/1/0/all/0/1&quot;&gt;Christophe Marsala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Renard_X/0/1/0/all/0/1&quot;&gt;Xavier Renard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Detyniecki_M/0/1/0/all/0/1&quot;&gt;Marcin Detyniecki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08588">
<title>Rank Pruning for Dominance Queries in CP-Nets. (arXiv:1712.08588v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.08588</link>
<description rdf:parseType="Literal">&lt;p&gt;Conditional preference networks (CP-nets) are a graphical representation of a
person&apos;s (conditional) preferences over a set of discrete variables. In this
paper, we introduce a novel method of quantifying preference for any given
outcome based on a CP-net representation of a user&apos;s preferences. We
demonstrate that these values are useful for reasoning about user preferences.
In particular, they allow us to order (any subset of) the possible outcomes in
accordance with the user&apos;s preferences. Further, these values can be used to
improve the efficiency of outcome dominance testing. That is, given a pair of
outcomes, we can determine which the user prefers more efficiently. We show
that these results also hold for CP-nets that express indifference between
variable values.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laing_K/0/1/0/all/0/1&quot;&gt;Kathryn Laing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thwaites_P/0/1/0/all/0/1&quot;&gt;Peter Adam Thwaites&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gosling_J/0/1/0/all/0/1&quot;&gt;John Paul Gosling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.06215">
<title>On Singleton Arc Consistency for CSPs Defined by Monotone Patterns. (arXiv:1704.06215v3 [cs.CC] UPDATED)</title>
<link>http://arxiv.org/abs/1704.06215</link>
<description rdf:parseType="Literal">&lt;p&gt;Singleton arc consistency is an important type of local consistency which has
been recently shown to solve all constraint satisfaction problems (CSPs) over
constraint languages of bounded width. We aim to characterise all classes of
CSPs defined by a forbidden pattern that are solved by singleton arc
consistency and closed under removing constraints. We identify five new
patterns whose absence ensures solvability by singleton arc consistency, four
of which are provably maximal and three of which generalise 2-SAT. Combined
with simple counter-examples for other patterns, we make significant progress
towards a complete classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbonnel_C/0/1/0/all/0/1&quot;&gt;Clement Carbonnel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1&quot;&gt;David A. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooper_M/0/1/0/all/0/1&quot;&gt;Martin C. Cooper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zivny_S/0/1/0/all/0/1&quot;&gt;Stanislav Zivny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.02792">
<title>The FastMap Algorithm for Shortest Path Computations. (arXiv:1706.02792v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1706.02792</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new preprocessing algorithm for embedding the nodes of a given
edge-weighted undirected graph into a Euclidean space. The Euclidean distance
between any two nodes in this space approximates the length of the shortest
path between them in the given graph. Later, at runtime, a shortest path
between any two nodes can be computed with A* search using the Euclidean
distances as heuristic. Our preprocessing algorithm, called FastMap, is
inspired by the data mining algorithm of the same name and runs in near-linear
time. Hence, FastMap is orders of magnitude faster than competing approaches
that produce a Euclidean embedding using Semidefinite Programming. FastMap also
produces admissible and consistent heuristics and therefore guarantees the
generation of shortest paths. Moreover, FastMap applies to general undirected
graphs for which many traditional heuristics, such as the Manhattan Distance
heuristic, are not well defined. Empirically, we demonstrate that A* search
using the FastMap heuristic is competitive with A* search using other
state-of-the-art heuristics, such as the Differential heuristic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_L/0/1/0/all/0/1&quot;&gt;Liron Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uras_T/0/1/0/all/0/1&quot;&gt;Tansel Uras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jahangiri_S/0/1/0/all/0/1&quot;&gt;Shiva Jahangiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arunasalam_A/0/1/0/all/0/1&quot;&gt;Aliyah Arunasalam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koenig_S/0/1/0/all/0/1&quot;&gt;Sven Koenig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_T/0/1/0/all/0/1&quot;&gt;T.K. Satish Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04271">
<title>Action Schema Networks: Generalised Policies with Deep Learning. (arXiv:1709.04271v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04271</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce the Action Schema Network (ASNet): a neural
network architecture for learning generalised policies for probabilistic
planning problems. By mimicking the relational structure of planning problems,
ASNets are able to adopt a weight-sharing scheme which allows the network to be
applied to any problem from a given planning domain. This allows the cost of
training the network to be amortised over all problems in that domain. Further,
we propose a training method which balances exploration and supervised training
on small problems to produce a policy which remains robust when evaluated on
larger problems. In experiments, we show that ASNet&apos;s learning capability
allows it to significantly outperform traditional non-learning planners in
several challenging domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toyer_S/0/1/0/all/0/1&quot;&gt;Sam Toyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trevizan_F/0/1/0/all/0/1&quot;&gt;Felipe Trevizan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiebaux_S/0/1/0/all/0/1&quot;&gt;Sylvie Thi&amp;#xe9;baux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lexing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07745">
<title>Context-aware Path Ranking for Knowledge Base Completion. (arXiv:1712.07745v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.07745</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge base (KB) completion aims to infer missing facts from existing ones
in a KB. Among various approaches, path ranking (PR) algorithms have received
increasing attention in recent years. PR algorithms enumerate paths between
entity pairs in a KB and use those paths as features to train a model for
missing fact prediction. Due to their good performances and high model
interpretability, several methods have been proposed. However, most existing
methods suffer from scalability (high RAM consumption) and feature explosion
(trains on an exponentially large number of features) problems. This paper
proposes a Context-aware Path Ranking (C-PR) algorithm to solve these problems
by introducing a selective path exploration strategy. C-PR learns global
semantics of entities in the KB using word embedding and leverages the
knowledge of entity semantics to enumerate contextually relevant paths using
bidirectional random walk. Experimental results on three large KBs show that
the path features (fewer in number) discovered by C-PR not only improve
predictive performance but also are more interpretable than existing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumder_S/0/1/0/all/0/1&quot;&gt;Sahisnu Mazumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08211">
<title>Robust Detection of Covariate-Treatment Interactions in Clinical Trials. (arXiv:1712.08211v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1712.08211</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection of interactions between treatment effects and patient descriptors
in clinical trials is critical for optimizing the drug development process. The
increasing volume of data accumulated in clinical trials provides a unique
opportunity to discover new biomarkers and further the goal of personalized
medicine, but it also requires innovative robust biomarker detection methods
capable of detecting non-linear, and sometimes weak, signals. We propose a set
of novel univariate statistical tests, based on the theory of random walks,
which are able to capture non-linear and non-monotonic covariate-treatment
interactions. We also propose a novel combined test, which leverages the power
of all of our proposed univariate tests into a single general-case tool. We
present results for both synthetic trials as well as real-world clinical
trials, where we compare our method with state-of-the-art techniques and
demonstrate the utility and robustness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goujaud_B/0/1/0/all/0/1&quot;&gt;Baptiste Goujaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tramel_E/0/1/0/all/0/1&quot;&gt;Eric W. Tramel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Courtiol_P/0/1/0/all/0/1&quot;&gt;Pierre Courtiol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zaslavskiy_M/0/1/0/all/0/1&quot;&gt;Mikhail Zaslavskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wainrib_G/0/1/0/all/0/1&quot;&gt;Gilles Wainrib&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08235">
<title>Estimating activity cycles with probabilistic methods I. Bayesian Generalised Lomb-Scargle Periodogram with Trend. (arXiv:1712.08235v1 [astro-ph.SR])</title>
<link>http://arxiv.org/abs/1712.08235</link>
<description rdf:parseType="Literal">&lt;p&gt;Period estimation is one of the central topics in astronomical time series
analysis, where data is often unevenly sampled. Especially challenging are
studies of stellar magnetic cycles, as there the periods looked for are of the
order of the same length than the datasets themselves. The datasets often
contain trends, the origin of which is either a real long-term cycle or an
instrumental effect, but these effects cannot be reliably separated, while they
can lead to erroneous period determinations if not properly handled. In this
study we aim at developing a method that can handle the trends properly, and by
performing extensive set of testing, we show that this is the optimal procedure
when contrasted with methods that do not include the trend directly to the
model. The effect of the noise model on the results is also investigated. We
introduce a Bayesian Generalised Lomb-Scargle Periodogram with Trend (BGLST),
which is a probabilistic linear regression model using Gaussian priors for the
coefficients and uniform prior for the frequency parameter. We show, using
synthetic data, that when there is no prior information on whether and to what
extent the true model of the data contains a linear trend, the introduced BGLST
method is preferable to the methods which either detrend the data or leave the
data untrended before fitting the periodic model. Whether to use different from
constant noise model depends on the density of the data sampling as well as on
the true noise model of the process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Olspert_N/0/1/0/all/0/1&quot;&gt;N. Olspert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Pelt_J/0/1/0/all/0/1&quot;&gt;J. Pelt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kapyla_M/0/1/0/all/0/1&quot;&gt;M. J. K&amp;#xe4;pyl&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Lehtinen_J/0/1/0/all/0/1&quot;&gt;J. J. Lehtinen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08240">
<title>Estimating activity cycles with probabilistic methods II. The Mount Wilson Ca H&amp;K data. (arXiv:1712.08240v1 [astro-ph.SR])</title>
<link>http://arxiv.org/abs/1712.08240</link>
<description rdf:parseType="Literal">&lt;p&gt;Debate over the existence versus nonexistence of trends in the stellar
activity-rotation diagrams continues. Application of modern time series
analysis tools to study the mean cycle periods in chromospheric activity index
is lacking. We develop such models, based on Gaussian processes, for
one-dimensional time series and apply it to the extended Mount Wilson Ca H&amp;amp;K
sample. Our main aim is to study how the previously commonly used assumption of
strict harmonicity of the stellar cycles affects the results. We introduce
three methods of different complexity, starting with the simple harmonic model
and followed by Gaussian Process models with periodic and quasi-periodic
covariance functions. We confirm the existence of two populations in the
activity-period diagram. We find only one significant trend in the inactive
population, namely that the cycle periods get shorter with increasing rotation.
This is in contrast with earlier studies, that postulate the existence of
trends in both of the populations. In terms of rotation to cycle period ratio,
our data is consistent with only two activity branches such that the active
branch merges together with the transitional one. The retrieved stellar cycles
are uniformly distributed over the R&apos;HK activity index, indicating that the
operation of stellar large-scale dynamos carries smoothly over the
Vaughan-Preston gap. At around the solar activity index, however, indications
of a disruption in the cyclic dynamo action are seen. Our study shows that
stellar cycle estimates depend significantly on the model applied. Such
model-dependent aspects include the improper treatment of linear trends and too
simple assumptions of the noise variance model. Assumption of strict
harmonicity can result in the appearance of double cyclicities that seem more
likely to be explained by the quasi-periodicity of the cycles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Olspert_N/0/1/0/all/0/1&quot;&gt;N. Olspert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Lehtinen_J/0/1/0/all/0/1&quot;&gt;J. J. Lehtinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kapyla_M/0/1/0/all/0/1&quot;&gt;M. J. K&amp;#xe4;pyl&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Pelt_J/0/1/0/all/0/1&quot;&gt;J. Pelt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Grigorievskiy_A/0/1/0/all/0/1&quot;&gt;A. Grigorievskiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08243">
<title>ConvSCCS: convolutional self-controlled case series model for lagged adverse event detection. (arXiv:1712.08243v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1712.08243</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increased availability of large databases of electronic health
records (EHRs) comes the chance of enhancing health risks screening. Most
post-marketing detections of adverse drug reaction (ADR) rely on physicians&apos;
spontaneous reports, leading to under reporting. To take up this challenge, we
develop a scalable model to estimate the effect of multiple longitudinal
features (drug exposures) on a rare longitudinal outcome. Our procedure is
based on a conditional Poisson model also known as self-controlled case series
(SCCS). We model the intensity of outcomes using a convolution between
exposures and step functions, that are penalized using a combination of
group-Lasso and total-variation. This approach does not require the
specification of precise risk periods, and allows to study in the same model
several exposures at the same time. We illustrate the fact that this approach
improves the state-of-the-art for the estimation of the relative risks both on
simulations and on a cohort of diabetic patients, extracted from the large
French national health insurance database (SNIIRAM).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Morel_M/0/1/0/all/0/1&quot;&gt;Maryan Morel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bacry_E/0/1/0/all/0/1&quot;&gt;Emmanuel Bacry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gaiffas_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Ga&amp;#xef;ffas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guilloux_A/0/1/0/all/0/1&quot;&gt;Agathe Guilloux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Leroy_F/0/1/0/all/0/1&quot;&gt;Fanny Leroy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08259">
<title>Linear centralization classifier. (arXiv:1712.08259v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.08259</link>
<description rdf:parseType="Literal">&lt;p&gt;A classification algorithm, called the Linear Centralization Classifier
(LCC), is introduced. The algorithm seeks to find a transformation that best
maps instances from the feature space to a space where they concentrate towards
the center of their own classes, while maximimizing the distance between class
centers. We formulate the classifier as a quadratic program with quadratic
constraints. We then simplify this formulation to a linear program that can be
solved effectively using a linear programming solver (e.g., simplex-dual). We
extend the formulation for LCC to enable the use of kernel functions for
non-linear classification applications. We compare our method with two standard
classification methods (support vector machine and linear discriminant
analysis) and four state-of-the-art classification methods when they are
applied to eight standard classification datasets. Our experimental results
show that LCC is able to classify instances more accurately (based on the area
under the receiver operating characteristic) in comparison to other tested
methods on the chosen datasets. We also report the results for LCC with a
particular kernel to solve for synthetic non-linear classification problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonyadi_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Bonyadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vegh_V/0/1/0/all/0/1&quot;&gt;Viktor Vegh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reutens_D/0/1/0/all/0/1&quot;&gt;David C. Reutens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08289">
<title>Joint IDs Embedding and its Applications in E-commerce. (arXiv:1712.08289v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.08289</link>
<description rdf:parseType="Literal">&lt;p&gt;E-commerce has become an important part of our daily lives and there are
great challenges due to its dynamic and complex business environment. Many
machine intelligence techniques are developed to overcome these challenges. One
of the essential elements in those techniques is the representation of data,
especially for ID-type data, e.g. item ID, product ID, store ID, brand ID,
category ID etc. The classical one-hot encoding suffers sparsity problems due
to its high dimension. Moreover, it cannot reflect the relationships among IDs,
either homogeneous or heterogeneous ones. In this paper, we propose a novel
hierarchical embedding model to jointly learn low-dimensional representations
for different types of IDs from the implicit feedback of users. Our approach
incorporates the structural information among IDs and embeds all types of IDs
into a semantic space. The low-dimensional representations can be effectively
extended to many applications including recommendation and forecast etc. We
evaluate our approach in several scenarios of &quot;Hema App&quot; and the experimental
results validate the effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuechuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shuai_Z/0/1/0/all/0/1&quot;&gt;Zhaoqian Shuai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08363">
<title>On Using Backpropagation for Speech Texture Generation and Voice Conversion. (arXiv:1712.08363v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1712.08363</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by recent work on neural network image generation which rely on
backpropagation towards the network inputs, we present a proof-of-concept
system for speech texture synthesis and voice conversion based on two
mechanisms: approximate inversion of the representation learned by a speech
recognition neural network, and on matching statistics of neuron activations
between different source and target utterances. Similar to image texture
synthesis and neural style transfer, the system works by optimizing a cost
function with respect to the input waveform samples. To this end we use a
differentiable mel-filterbank feature extraction pipeline and train a
convolutional CTC speech recognition network. Our system is able to extract
speaker characteristics from very limited amounts of target speaker data, as
little as a few seconds, and can be used to generate realistic speech babble or
reconstruct an utterance in a different voice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1&quot;&gt;Jan Chorowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiss_R/0/1/0/all/0/1&quot;&gt;Ron J. Weiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saurous_R/0/1/0/all/0/1&quot;&gt;Rif A. Saurous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1&quot;&gt;Samy Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08449">
<title>True Asymptotic Natural Gradient Optimization. (arXiv:1712.08449v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.08449</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a simple algorithm, True Asymptotic Natural Gradient
Optimization (TANGO), that converges to a true natural gradient descent in the
limit of small learning rates, without explicit Fisher matrix estimation.
&lt;/p&gt;
&lt;p&gt;For quadratic models the algorithm is also an instance of averaged stochastic
gradient, where the parameter is a moving average of a &quot;fast&quot;, constant-rate
gradient descent. TANGO appears as a particular de-linearization of averaged
SGD, and is sometimes quite different on non-quadratic models. This further
connects averaged SGD and natural gradient, both of which are arguably optimal
asymptotically.
&lt;/p&gt;
&lt;p&gt;In large dimension, small learning rates will be required to approximate the
natural gradient well. Still, this shows it is possible to get arbitrarily
close to exact natural gradient descent with a lightweight algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ollivier_Y/0/1/0/all/0/1&quot;&gt;Yann Ollivier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08493">
<title>Diversifying Support Vector Machines for Boosting using Kernel Perturbation: Applications to Class Imbalance and Small Disjuncts. (arXiv:1712.08493v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.08493</link>
<description rdf:parseType="Literal">&lt;p&gt;The diversification (generating slightly varying separating discriminators)
of Support Vector Machines (SVMs) for boosting has proven to be a challenge due
to the strong learning nature of SVMs. Based on the insight that perturbing the
SVM kernel may help in diversifying SVMs, we propose two kernel perturbation
based boosting schemes where the kernel is modified in each round so as to
increase the resolution of the kernel-induced Reimannian metric in the vicinity
of the datapoints misclassified in the previous round. We propose a method for
identifying the disjuncts in a dataset, dispelling the dependence on rule-based
learning methods for identifying the disjuncts. We also present a new
performance measure called Geometric Small Disjunct Index (GSDI) to quantify
the performance on small disjuncts for balanced as well as class imbalanced
datasets. Experimental comparison with a variety of state-of-the-art algorithms
is carried out using the best classifiers of each type selected by a new
approach inspired by multi-criteria decision making. The proposed method is
found to outperform the contending state-of-the-art methods on different
datasets (ranging from mildly imbalanced to highly imbalanced and characterized
by varying number of disjuncts) in terms of three different performance indices
(including the proposed GSDI).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1&quot;&gt;Shounak Datta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1&quot;&gt;Sayak Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullick_S/0/1/0/all/0/1&quot;&gt;Sankha Subhra Mullick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Swagatam Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08577">
<title>Adaptive Stochastic Dual Coordinate Ascent for Conditional Random Fields. (arXiv:1712.08577v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.08577</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates training Conditional Random Fields (CRF) by Stochastic
Dual Coordinate Ascent (SDCA). SDCA enjoys a linear convergence rate and a
strong empirical performance for independent classification problems. However,
it has never been used to train CRF. Yet it benefits from an exact line search
with a single marginalization oracle call, unlike previous approaches. In this
paper, we adapt SDCA to train CRF and we enhance it with an adaptive
non-uniform sampling strategy. Our preliminary experiments suggest that this
method matches state-of-the-art CRF optimization techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Priol_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Le Priol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Touati_A/0/1/0/all/0/1&quot;&gt;Ahmed Touati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lacoste_Julien_S/0/1/0/all/0/1&quot;&gt;Simon Lacoste-Julien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.07904">
<title>Cross-Validation with Confidence. (arXiv:1703.07904v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1703.07904</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-validation is one of the most popular model selection methods in
statistics and machine learning. Despite its wide applicability, traditional
cross validation methods tend to select overfitting models, due to the
ignorance of the uncertainty in the testing sample. We develop a new,
statistically principled inference tool based on cross-validation that takes
into account the uncertainty in the testing sample. This new method outputs a
set of highly competitive candidate models containing the best one with
guaranteed probability. As a consequence, our method can achieve consistent
variable selection in a classical linear regression setting, for which existing
cross-validation methods require unconventional split ratios. When used for
regularizing tuning parameter selection, the method can provide a further
trade-off between prediction accuracy and model interpretability. We
demonstrate the performance of the proposed method in several simulated and
real data examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jing Lei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.09847">
<title>Runaway Feedback Loops in Predictive Policing. (arXiv:1706.09847v3 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1706.09847</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive policing systems are increasingly used to determine how to
allocate police across a city in order to best prevent crime. Discovered crime
data (e.g., arrest counts) are used to help update the model, and the process
is repeated. Such systems have been empirically shown to be susceptible to
runaway feedback loops, where police are repeatedly sent back to the same
neighborhoods regardless of the true crime rate.
&lt;/p&gt;
&lt;p&gt;In response, we develop a mathematical model of predictive policing that
proves why this feedback loop occurs, show empirically that this model exhibits
such problems, and demonstrate how to change the inputs to a predictive
policing system (in a black-box manner) so the runaway feedback loop does not
occur, allowing the true crime rate to be learned. Our results are
quantitative: we can establish a link (in our model) between the degree to
which runaway feedback causes problems and the disparity in crime rates between
areas. Moreover, we can also demonstrate the way in which \emph{reported}
incidents of crime (those reported by residents) and \emph{discovered}
incidents of crime (i.e. those directly observed by police officers dispatched
as a result of the predictive policing algorithm) interact: in brief, while
reported incidents can attenuate the degree of runaway feedback, they cannot
entirely remove it without the interventions we suggest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ensign_D/0/1/0/all/0/1&quot;&gt;Danielle Ensign&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedler_S/0/1/0/all/0/1&quot;&gt;Sorelle A. Friedler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neville_S/0/1/0/all/0/1&quot;&gt;Scott Neville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheidegger_C/0/1/0/all/0/1&quot;&gt;Carlos Scheidegger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatasubramanian_S/0/1/0/all/0/1&quot;&gt;Suresh Venkatasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.02538">
<title>CuRTAIL: ChaRacterizing and Thwarting AdversarIal deep Learning. (arXiv:1709.02538v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1709.02538</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes CuRTAIL, an end-to-end computing framework for
characterizing and thwarting adversarial space in the context of Deep Learning
(DL). The framework protects deep neural networks against adversarial samples,
which are perturbed inputs carefully crafted by malicious entities to mislead
the underlying DL model. The precursor for the proposed methodology is a set of
new quantitative metrics to assess the vulnerability of various deep learning
architectures to adversarial samples. CuRTAIL formalizes the goal of preventing
adversarial samples as a minimization of the space unexplored by the pertinent
DL model that is characterized in CuRTAIL vulnerability analysis step. To
thwart the adversarial machine learning attack, CuRTAIL introduces the concept
of Modular Robust Redundancy (MRR) as a viable solution to achieve the
formalized minimization objective. The MRR methodology explicitly characterizes
the geometry of the input data and the DL model parameters. It then learns a
set of complementary but disjoint models which maximally cover the unexplored
subspaces of the target DL model, thus reducing the risk of integrity attacks.
We extensively evaluate CuRTAIL performance against the state-of-the-art attack
models including fast-sign-gradient, Jacobian Saliency Map Attack, Deepfool,
and Carlini&amp;amp;WagnerL2. Proof-of-concept implementations for analyzing various
data collections including MNIST, CIFAR10, and ImageNet corroborate CuRTAIL
effectiveness to detect adversarial samples in different settings. The
computations in each MRR module can be performed independently. As such,
CuRTAIL detection algorithm can be completely parallelized among multiple
hardware settings to achieve maximum throughput. We further provide an
accompanying API to facilitate the adoption of the proposed framework for
various applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rouhani_B/0/1/0/all/0/1&quot;&gt;Bita Darvish Rouhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samragh_M/0/1/0/all/0/1&quot;&gt;Mohammad Samragh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1&quot;&gt;Tara Javidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1&quot;&gt;Farinaz Koushanfar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00443">
<title>Deep Neural Network Architectures for Modulation Classification. (arXiv:1712.00443v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00443</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we investigate the value of employing deep learning for the
task of wireless signal modulation recognition. Recently in [1], a framework
has been introduced by generating a dataset using GNU radio that mimics the
imperfections in a real wireless channel, and uses 10 different modulation
types. Further, a convolutional neural network (CNN) architecture was developed
and shown to deliver performance that exceeds that of expert-based approaches.
Here, we follow the framework of [1] and find deep neural network architectures
that deliver higher accuracy than the state of the art. We tested the
architecture of [1] and found it to achieve an accuracy of approximately 75% of
correctly recognizing the modulation type. We first tune the CNN architecture
of [1] and find a design with four convolutional layers and two dense layers
that gives an accuracy of approximately 83.8% at high SNR. We then develop
architectures based on the recently introduced ideas of Residual Networks
(ResNet [2]) and Densely Connected Networks (DenseNet [3]) to achieve high SNR
accuracies of approximately 83.5% and 86.6%, respectively. Finally, we
introduce a Convolutional Long Short-term Deep Neural Network (CLDNN [4]) to
achieve an accuracy of approximately 88.5% at high SNR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Diyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gamal_A/0/1/0/all/0/1&quot;&gt;Aly El Gamal&lt;/a&gt;</dc:creator>
</item></rdf:RDF>