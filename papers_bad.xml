<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-28T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10307"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10365"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10498"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10547"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10606"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10644"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10692"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10795"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10796"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11014"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02195"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05373"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08594"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10364"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10461"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10582"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10662"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10672"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10723"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10777"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10817"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10820"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10884"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10900"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11074"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.02526"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.10726"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.03184"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09564"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06288"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07710"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09928"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00506"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10309"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10318"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10341"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10348"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10367"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10377"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10378"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10384"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10406"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10469"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10477"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10522"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10531"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10572"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10611"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10615"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10616"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10638"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10665"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10694"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10724"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10755"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10759"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10767"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10833"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10887"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10915"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10917"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10927"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10939"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10940"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10958"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10965"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10970"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10982"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10988"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11022"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11028"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11051"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11073"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11085"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1601.05285"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.02869"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.02122"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.08948"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.05778"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11511"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03558"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03832"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04920"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05315"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07581"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00651"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01370"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01526"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03836"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06218"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06481"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04874"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05857"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08206"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08336"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09654"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.05543"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03248"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.10307">
<title>Forecasting the successful execution of horizontal strategy in a diversified corporation via a DEMATEL-supported artificial neural network - A case study. (arXiv:1805.10307v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.10307</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, competition is getting tougher as market shrinks because of
financial crisis of the late 2000s. Organizations are tensely forced to
leverage their core competencies to survive through attracting more customers
and gaining more efficacious operations. In such a situation, diversified
corporations which run multiple businesses have opportunities to get
competitive advantage and differentiate themselves by executing horizontal
strategy. Since this strategy completely engages a number of business units of
a diversified corporation through resource sharing among them, any effort to
implement it will fail if being not supported by enough information. However,
for successful execution of horizontal strategy, managers should have reliable
information concerning its success probability in advance. To provide such a
precious information, a three-step framework has been developed. In the first
step, major influencers on successful execution of horizontal strategy have
been captured through literature study and interviewing subject matter experts.
In the second step through the decision making trial and evaluation laboratory
(DEMATEL) methodology, critical success factors (CSFs) have been extracted from
major influencers and a success probability assessment index system (SPAIS) has
been formed. In the third step, due to the statistical nature (multivariate and
distribution free) of SPAIS, an artificial neural network has been designed for
enabling organizational managers to forecast the success probability of
horizontal strategy execution in a multi-business corporation far better than
other classical models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabzian_H/0/1/0/all/0/1&quot;&gt;Hossein Sabzian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gharib_H/0/1/0/all/0/1&quot;&gt;Hossein Gharib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noori_J/0/1/0/all/0/1&quot;&gt;Javad Noori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafia_M/0/1/0/all/0/1&quot;&gt;Mohammad Ali Shafia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheikh_M/0/1/0/all/0/1&quot;&gt;Mohammad Javad Sheikh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10365">
<title>Analysing Symbolic Regression Benchmarks under a Meta-Learning Approach. (arXiv:1805.10365v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.10365</link>
<description rdf:parseType="Literal">&lt;p&gt;The definition of a concise and effective testbed for Genetic Programming
(GP) is a recurrent matter in the research community. This paper takes a new
step in this direction, proposing a different approach to measure the quality
of the symbolic regression benchmarks quantitatively. The proposed approach is
based on meta-learning and uses a set of dataset meta-features---such as the
number of examples or output skewness---to describe the datasets. Our idea is
to correlate these meta-features with the errors obtained by a GP method. These
meta-features define a space of benchmarks that should, ideally, have datasets
(points) covering different regions of the space. An initial analysis of 63
datasets showed that current benchmarks are concentrated in a small region of
this benchmark space. We also found out that number of instances and output
skewness are the most relevant meta-features to GP output error. Both
conclusions can help define which datasets should compose an effective testbed
for symbolic regression methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1&quot;&gt;Luiz Otavio Vilas Boas Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martins_J/0/1/0/all/0/1&quot;&gt;Joao Francisco Barreto da Silva Martins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miranda_L/0/1/0/all/0/1&quot;&gt;Luis Fernando Miranda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappa_G/0/1/0/all/0/1&quot;&gt;Gisele Lobo Pappa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10498">
<title>Automatic context window composition for distant speech recognition. (arXiv:1805.10498v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1805.10498</link>
<description rdf:parseType="Literal">&lt;p&gt;Distant speech recognition is being revolutionized by deep learning, that has
contributed to significantly outperform previous HMM-GMM systems. A key aspect
behind the rapid rise and success of DNNs is their ability to better manage
large time contexts. With this regard, asymmetric context windows that embed
more past than future frames have been recently used with feed-forward neural
networks. This context configuration turns out to be useful not only to address
low-latency speech recognition, but also to boost the recognition performance
under reverberant conditions. This paper investigates on the mechanisms
occurring inside DNNs, which lead to an effective application of asymmetric
contexts.In particular, we propose a novel method for automatic context window
composition based on a gradient analysis. The experiments, performed with
different acoustic environments, features, DNN architectures, microphone
settings, and recognition tasks show that our simple and efficient strategy
leads to a less redundant frame configuration, which makes DNN training more
effective in reverberant scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ravanelli_M/0/1/0/all/0/1&quot;&gt;Mirco Ravanelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Omologo_M/0/1/0/all/0/1&quot;&gt;Maurizio Omologo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10547">
<title>Using Syntax to Ground Referring Expressions in Natural Images. (arXiv:1805.10547v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.10547</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce GroundNet, a neural network for referring expression recognition
-- the task of localizing (or grounding) in an image the object referred to by
a natural language expression. Our approach to this task is the first to rely
on a syntactic analysis of the input referring expression in order to inform
the structure of the computation graph. Given a parse tree for an input
expression, we explicitly map the syntactic constituents and relationships
present in the tree to a composed graph of neural modules that defines our
architecture for performing localization. This syntax-based approach aids
localization of \textit{both} the target object and auxiliary supporting
objects mentioned in the expression. As a result, GroundNet is more
interpretable than previous methods: we can (1) determine which phrase of the
referring expression points to which object in the image and (2) track how the
localization of the target object is determined by the network. We study this
property empirically by introducing a new set of annotations on the GoogleRef
dataset to evaluate localization of supporting objects. Our experiments show
that GroundNet achieves state-of-the-art accuracy in identifying supporting
objects, while maintaining comparable performance in the localization of target
objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cirik_V/0/1/0/all/0/1&quot;&gt;Volkan Cirik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1&quot;&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10606">
<title>Reduction of the Pareto Set in Bicriteria Asymmetric Traveling Salesman Problem. (arXiv:1805.10606v1 [cs.DM])</title>
<link>http://arxiv.org/abs/1805.10606</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the bicriteria asymmetric traveling salesman problem (bi-ATSP).
Optimal solution to a multicriteria problem is usually supposed to be the
Pareto set, which is rather wide in real-world problems. We apply to the
bi-ATSP the axiomatic approach of the Pareto set reduction proposed by V.
Noghin. We identify series of &quot;quanta of information&quot; that guarantee the
reduction of the Pareto set for particular cases of the bi-ATSP. An
approximation of the Pareto set to the bi-ATSP is constructed by a new
multi-objective genetic algorithm. The experimental evaluation carried out in
this paper shows the degree of reduction of the Pareto set approximation for
various &quot;quanta of information&quot; and various structures of the bi-ATSP instances
generated randomly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakharov_A/0/1/0/all/0/1&quot;&gt;Aleksey O. Zakharov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovalenko_Y/0/1/0/all/0/1&quot;&gt;Yulia V. Kovalenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10644">
<title>Comparison of VCA and GAEE algorithms for Endmember Extraction. (arXiv:1805.10644v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.10644</link>
<description rdf:parseType="Literal">&lt;p&gt;Endmember Extraction is a critical step in hyperspectral image analysis and
classification. It is an useful method to decompose a mixed spectrum into a
collection of spectra and their corresponding proportions. In this paper, we
solve a linear endmember extraction problem as an evolutionary optimization
task, maximizing the Simplex Volume in the endmember space. We propose a
standard genetic algorithm and a variation with In Vitro Fertilization module
(IVFm) to find the best solutions and compare the results with the state-of-art
Vertex Component Analysis (VCA) method and the traditional algorithms Pixel
Purity Index (PPI) and N-FINDR. The experimental results on real and synthetic
hyperspectral data confirms the overcome in performance and accuracy of the
proposed approaches over the mentioned algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S%2E_D/0/1/0/all/0/1&quot;&gt;Douglas Winston. R. S.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laureano_G/0/1/0/all/0/1&quot;&gt;Gustavo T. Laureano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camilo_C/0/1/0/all/0/1&quot;&gt;Celso G. Camilo Jr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10692">
<title>Compact and Computationally Efficient Representation of Deep Neural Networks. (arXiv:1805.10692v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10692</link>
<description rdf:parseType="Literal">&lt;p&gt;Dot product operations between matrices are at the heart of almost any field
in science and technology. In many cases, they are the component that requires
the highest computational resources during execution. For instance, deep neural
networks such as VGG-16 require up to 15 giga-operations in order to perform
the dot products present in a single forward pass, which results in significant
energy consumption and thus limits their use in resource-limited environments,
e.g., on embedded devices or smartphones. One common approach to reduce the
complexity of the inference is to prune and quantize the weight matrices of the
neural network and to efficiently represent them using sparse matrix data
structures. However, since there is no guarantee that the weight matrices
exhibit significant sparsity after quantization, the sparse format may be
suboptimal. In this paper we present new efficient data structures for
representing matrices with low entropy statistics and show that these formats
are especially suitable for representing neural networks. Alike sparse matrix
data structures, these formats exploit the statistical properties of the data
in order to reduce the size and execution complexity. Moreover, we show that
the proposed data structures can not only be regarded as a generalization of
sparse formats, but are also more energy and time efficient under practically
relevant assumptions. Finally, we test the storage requirements and execution
performance of the proposed formats on compressed neural networks and compare
them to dense and sparse representations. We experimentally show that we are
able to attain up to x15 compression ratios, x1.7 speed ups and x20 energy
savings when we lossless convert state-of-the-art networks such as AlexNet,
VGG-16, ResNet152 and DenseNet into the new data structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiedemann_S/0/1/0/all/0/1&quot;&gt;Simon Wiedemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1&quot;&gt;Wojciech Samek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10795">
<title>Deep Discriminative Latent Space for Clustering. (arXiv:1805.10795v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.10795</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering is one of the most fundamental tasks in data analysis and machine
learning. It is central to many data-driven applications that aim to separate
the data into groups with similar patterns. Moreover, clustering is a complex
procedure that is affected significantly by the choice of the data
representation method. Recent research has demonstrated encouraging clustering
results by learning effectively these representations. In most of these works a
deep auto-encoder is initially pre-trained to minimize a reconstruction loss,
and then jointly optimized with clustering centroids in order to improve the
clustering objective. Those works focus mainly on the clustering phase of the
procedure, while not utilizing the potential benefit out of the initial phase.
In this paper we propose to optimize an auto-encoder with respect to a
discriminative pairwise loss function during the auto-encoder pre-training
phase. We demonstrate the high accuracy obtained by the proposed method as well
as its rapid convergence (e.g. reaching above 92% accuracy on MNIST during the
pre-training phase, in less than 50 epochs), even with small networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzoreff_E/0/1/0/all/0/1&quot;&gt;Elad Tzoreff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kogan_O/0/1/0/all/0/1&quot;&gt;Olga Kogan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choukroun_Y/0/1/0/all/0/1&quot;&gt;Yoni Choukroun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10796">
<title>Convolutional neural network compression for natural language processing. (arXiv:1805.10796v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.10796</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks are modern models that are very efficient in
many classification tasks. They were originally created for image processing
purposes. Then some trials were performed to use them in different domains like
natural language processing. The artificial intelligence systems (like humanoid
robots) are very often based on embedded systems with constraints on memory,
power consumption etc. Therefore convolutional neural network because of its
memory capacity should be reduced to be mapped to given hardware. In this
paper, results are presented of compressing the efficient convolutional neural
networks for sentiment analysis. The main steps are quantization and pruning
processes. The method responsible for mapping compressed network to FPGA and
results of this implementation are presented. The described simulations showed
that 5-bit width is enough to have no drop in accuracy from floating point
version of the network. Additionally, significant memory footprint reduction
was achieved (from 85% up to 93%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wrobel_K/0/1/0/all/0/1&quot;&gt;Krzysztof Wr&amp;#xf3;bel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pietron_M/0/1/0/all/0/1&quot;&gt;Marcin Pietro&amp;#x144;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wielgosz_M/0/1/0/all/0/1&quot;&gt;Maciej Wielgosz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karwatowski_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Karwatowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiatr_K/0/1/0/all/0/1&quot;&gt;Kazimierz Wiatr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11014">
<title>Evolutionary Algorithms. (arXiv:1805.11014v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.11014</link>
<description rdf:parseType="Literal">&lt;p&gt;Evolutionary algorithms (EAs) are population-based metaheuristics, originally
inspired by aspects of natural evolution. Modern varieties incorporate a broad
mixture of search mechanisms, and tend to blend inspiration from nature with
pragmatic engineering concerns; however, all EAs essentially operate by
maintaining a population of potential solutions and in some way artificially
&apos;evolving&apos; that population over time. Particularly well-known categories of EAs
include genetic algorithms (GAs), Genetic Programming (GP), and Evolution
Strategies (ES). EAs have proven very successful in practical applications,
particularly those requiring solutions to combinatorial problems. EAs are
highly flexible and can be configured to address any optimization task, without
the requirements for reformulation and/or simplification that would be needed
for other techniques. However, this flexibility goes hand in hand with a cost:
the tailoring of an EA&apos;s configuration and parameters, so as to provide robust
performance for a given class of tasks, is often a complex and time-consuming
process. This tailoring process is one of the many ongoing research areas
associated with EAs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corne_D/0/1/0/all/0/1&quot;&gt;David W. Corne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lones_M/0/1/0/all/0/1&quot;&gt;Michael A. Lones&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02195">
<title>Granger-causal Attentive Mixtures of Experts: Learning Important Features with Neural Networks. (arXiv:1802.02195v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02195</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge of the importance of input features towards decisions made by
machine-learning models is essential to increase our understanding of both the
models and the underlying data. Here, we present a new approach to estimating
feature importance with neural networks based on the idea of distributing the
features of interest among experts in an attentive mixture of experts (AME).
AMEs couple attentive gating networks with a Granger-causal objective to
jointly produce accurate predictions as well as estimates of feature
importance. Our experiments on an established benchmark and two real-world
datasets show (i) that the feature importance estimates provided by AMEs
compare favourably to those provided by state-of-the-art methods, (ii) that
AMEs are significantly faster than existing methods, and (iii) that the
associations discovered by AMEs are consistent with those reported by domain
experts. In addition, we analyse the trade-off between predictive performance
and estimation accuracy, the degree to which importance estimates of existing
methods conform to predictive value, and whether a lower Granger-causal error
on held-out data indicates a better feature importance estimation accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwab_P/0/1/0/all/0/1&quot;&gt;Patrick Schwab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miladinovic_D/0/1/0/all/0/1&quot;&gt;Djordje Miladinovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlen_W/0/1/0/all/0/1&quot;&gt;Walter Karlen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05373">
<title>DeepEM: Deep 3D ConvNets With EM For Weakly Supervised Pulmonary Nodule Detection. (arXiv:1805.05373v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05373</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently deep learning has been witnessing widespread adoption in various
medical image applications. However, training complex deep neural nets requires
large-scale datasets labeled with ground truth, which are often unavailable in
many medical image domains. For instance, to train a deep neural net to detect
pulmonary nodules in lung computed tomography (CT) images, current practice is
to manually label nodule locations and sizes in many CT images to construct a
sufficiently large training dataset, which is costly and difficult to scale. On
the other hand, electronic medical records (EMR) contain plenty of partial
information on the content of each medical image. In this work, we explore how
to tap this vast, but currently unexplored data source to improve pulmonary
nodule detection. We propose DeepEM, a novel deep 3D ConvNet framework
augmented with expectation-maximization (EM), to mine weakly supervised labels
in EMRs for pulmonary nodule detection. Experimental results show that DeepEM
can lead to 1.5\% and 3.9\% average improvement in free-response receiver
operating characteristic (FROC) scores on LUNA16 and Tianchi datasets,
respectively, demonstrating the utility of incomplete information in EMRs for
improving deep learning
algorithms.\footnote{https://github.com/uci-cbcl/DeepEM-for-Weakly-Supervised-Detection.git}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vang_Y/0/1/0/all/0/1&quot;&gt;Yeeleng S. Vang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yufang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08594">
<title>Neural Generative Models for Global Optimization with Gradients. (arXiv:1805.08594v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08594</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of global optimization is to find the global optimum of arbitrary
classes of functions, possibly highly multimodal ones. In this paper we focus
on the subproblem of global optimization for differentiable functions and we
propose an Evolutionary Search-inspired solution where we model point search
distributions via Generative Neural Networks. This approach enables us to model
diverse and complex search distributions based on which we can efficiently
explore complicated objective landscapes. In our experiments we show the
practical superiority of our algorithm versus classical Evolutionary Search and
gradient-based solutions on a benchmark set of multimodal functions, and
demonstrate how it can be used to accelerate Bayesian Optimization with
Gaussian Processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faury_L/0/1/0/all/0/1&quot;&gt;Louis Faury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasile_F/0/1/0/all/0/1&quot;&gt;Flavian Vasile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calauzenes_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Calauz&amp;#xe8;nes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fercoq_O/0/1/0/all/0/1&quot;&gt;Olivier Fercoq&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10364">
<title>Detecting Deceptive Reviews using Generative Adversarial Networks. (arXiv:1805.10364v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1805.10364</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past few years, consumer review sites have become the main target of
deceptive opinion spam, where fictitious opinions or reviews are deliberately
written to sound authentic. Most of the existing work to detect the deceptive
reviews focus on building supervised classifiers based on syntactic and lexical
patterns of an opinion. With the successful use of Neural Networks on various
classification applications, in this paper, we propose FakeGAN a system that
for the first time augments and adopts Generative Adversarial Networks (GANs)
for a text classification task, in particular, detecting deceptive reviews.
Unlike standard GAN models which have a single Generator and Discriminator
model, FakeGAN uses two discriminator models and one generative model. The
generator is modeled as a stochastic policy agent in reinforcement learning
(RL), and the discriminators use Monte Carlo search algorithm to estimate and
pass the intermediate action-value as the RL reward to the generator. Providing
the generator model with two discriminator models avoids the mod collapse issue
by learning from both distributions of truthful and deceptive reviews. Indeed,
our experiments show that using two discriminators provides FakeGAN high
stability, which is a known issue for GAN architectures. While FakeGAN is built
upon a semi-supervised classifier, known for less accuracy, our evaluation
results on a dataset of TripAdvisor hotel reviews show the same performance in
terms of accuracy as of the state-of-the-art approaches that apply supervised
machine learning. These results indicate that GANs can be effective for text
classification tasks. Specifically, FakeGAN is effective at detecting deceptive
reviews.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghakhani_H/0/1/0/all/0/1&quot;&gt;Hojjat Aghakhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machiry_A/0/1/0/all/0/1&quot;&gt;Aravind Machiry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nilizadeh_S/0/1/0/all/0/1&quot;&gt;Shirin Nilizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruegel_C/0/1/0/all/0/1&quot;&gt;Christopher Kruegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vigna_G/0/1/0/all/0/1&quot;&gt;Giovanni Vigna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10461">
<title>From Knowledge Graph Embedding to Ontology Embedding: Region Based Representations of Relational Structures. (arXiv:1805.10461v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.10461</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed the enormous success of low-dimensional vector
space representations of knowledge graphs to predict missing facts or find
erroneous ones. Currently, however, it is not yet well-understood how
ontological knowledge, e.g. given as a set of (existential) rules, can be
embedded in a principled way. To address this shortcoming, in this paper we
introduce a framework based on convex regions, which can faithfully incorporate
ontological knowledge into the vector space embedding. Our technical
contribution is two-fold. First, we show that some of the most popular existing
embedding approaches are not capable of modelling even very simple types of
rules. Second, we show that our framework can represent ontologies that are
expressed using so-called quasi-chained existential rules in an exact way, such
that any set of facts which is induced using that vector space embedding is
logically consistent and deductively closed with respect to the input ontology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_Basulto_V/0/1/0/all/0/1&quot;&gt;V&amp;#xed;ctor Guti&amp;#xe9;rrez-Basulto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1&quot;&gt;Steven Schockaert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10582">
<title>Metric-Optimized Example Weights. (arXiv:1805.10582v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10582</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world machine learning applications often have complex test metrics, and
may have training and test data that follow different distributions. We propose
addressing these issues by using a weighted loss function with a standard
convex loss, but with weights on the training examples that are learned to
optimize the test metric of interest on the validation set. These
metric-optimized example weights can be learned for any test metric, including
black box losses and customized metrics for specific applications. We
illustrate the performance of our proposal with public benchmark datasets and
real-world applications with domain shift and custom loss functions that
balance multiple objectives, impose fairness policies, and are non-convex and
non-decomposable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Sen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fard_M/0/1/0/all/0/1&quot;&gt;Mahdi Milani Fard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Maya Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10662">
<title>Contextual Policy Optimisation. (arXiv:1805.10662v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10662</link>
<description rdf:parseType="Literal">&lt;p&gt;Policy gradient methods have been successfully applied to a variety of
reinforcement learning tasks. However, while learning in a simulator, these
methods do not utilise the opportunity to improve learning by adjusting certain
environment variables: unobservable state features that are randomly determined
by the environment in a physical setting, but that are controllable in a
simulator. This can lead to slow learning, or convergence to highly suboptimal
policies. In this paper, we present contextual policy optimisation (CPO). The
central idea is to use Bayesian optimisation to actively select the
distribution of the environment variable that maximises the improvement
generated by each iteration of the policy gradient method. To make this
Bayesian optimisation practical, we contribute two easy-to-compute
low-dimensional fingerprints of the current policy. We apply CPO to a number of
continuous control tasks of varying difficulty and show that CPO can
efficiently learn policies that are robust to significant rare events, which
are unlikely to be observable under random sampling but are key to learning
good policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1&quot;&gt;Supratik Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osborne_M/0/1/0/all/0/1&quot;&gt;Michael A. Osborne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10672">
<title>A note on belief structures and S-approximation spaces. (arXiv:1805.10672v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.10672</link>
<description rdf:parseType="Literal">&lt;p&gt;We study relations between evidence theory and S-approximation spaces. Both
theories have their roots in the analysis of Dempster&apos;s multivalued mappings
and lower and upper probabilities and have close relations to rough sets. We
show that an S-approximation space, satisfying a monotonicity condition, can
induce a natural belief structure which is a fundamental block in evidence
theory. We also demonstrate that one can induce a natural belief structure on
one set, given a belief structure on another set if those sets are related by a
partial monotone S-approximation space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakiba_A/0/1/0/all/0/1&quot;&gt;Ali Shakiba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goharshady_A/0/1/0/all/0/1&quot;&gt;Amir Kafshdar Goharshady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooshmandasl_M/0/1/0/all/0/1&quot;&gt;MohammadReza Hooshmandasl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meybodi_M/0/1/0/all/0/1&quot;&gt;Mohsen Alambardar Meybodi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10723">
<title>Designing for Democratization: Introducing Novices to Artificial Intelligence Via Maker Kits. (arXiv:1805.10723v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1805.10723</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing research highlight the myriad of benefits realized when technology
is sufficiently democratized and made accessible to non-technical or novice
users. However, democratizing complex technologies such as artificial
intelligence (AI) remains hard. In this work, we draw on theoretical
underpinnings from the democratization of innovation, in exploring the design
of maker kits that help introduce novice users to complex technologies. We
report on our work designing TJBot: an open source cardboard robot that can be
programmed using pre-built AI services. We highlight principles we adopted in
this process (approachable design, simplicity, extensibility and
accessibility), insights we learned from showing the kit at workshops (66
participants) and how users interacted with the project on GitHub over a
12-month period (Nov 2016 - Nov 2017). We find that the project succeeds in
attracting novice users (40\% of users who forked the project are new to
GitHub) and a variety of demographics are interested in prototyping use cases
such as home automation, task delegation, teaching and learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dibia_V/0/1/0/all/0/1&quot;&gt;Victor Dibia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashoori_M/0/1/0/all/0/1&quot;&gt;Maryam Ashoori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cox_A/0/1/0/all/0/1&quot;&gt;Aaron Cox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weisz_J/0/1/0/all/0/1&quot;&gt;Justin Weisz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10777">
<title>Object-Level Representation Learning for Few-Shot Image Classification. (arXiv:1805.10777v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.10777</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot learning that trains image classifiers over few labeled examples per
category is a challenging task. In this paper, we propose to exploit an
additional big dataset with different categories to improve the accuracy of
few-shot learning over our target dataset. Our approach is based on the
observation that images can be decomposed into objects, which may appear in
images from both the additional dataset and our target dataset. We use the
object-level relation learned from the additional dataset to infer the
similarity of images in our target dataset with unseen categories. Nearest
neighbor search is applied to do image classification, which is a
non-parametric model and thus does not need fine-tuning. We evaluate our
algorithm on two popular datasets, namely Omniglot and MiniImagenet. We obtain
8.5\% and 2.7\% absolute improvements for 5-way 1-shot and 5-way 5-shot
experiments on MiniImagenet, respectively. Source code will be published upon
acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_L/0/1/0/all/0/1&quot;&gt;Liangqu Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Jun Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Meihui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qian Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ooi_B/0/1/0/all/0/1&quot;&gt;Beng Chin Ooi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10817">
<title>Linear tSNE optimization for the Web. (arXiv:1805.10817v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10817</link>
<description rdf:parseType="Literal">&lt;p&gt;The t-distributed Stochastic Neighbor Embedding (tSNE) algorithm has become
in recent years one of the most used and insightful techniques for the
exploratory data analysis of high-dimensional data. tSNE reveals clusters of
high-dimensional data points at different scales while it requires only minimal
tuning of its parameters. Despite these advantages, the computational
complexity of the algorithm limits its application to relatively small
datasets. To address this problem, several evolutions of tSNE have been
developed in recent years, mainly focusing on the scalability of the similarity
computations between data points. However, these contributions are insufficient
to achieve interactive rates when visualizing the evolution of the tSNE
embedding for large datasets. In this work, we present a novel approach to the
minimization of the tSNE objective function that heavily relies on modern
graphics hardware and has linear computational complexity. Our technique does
not only beat the state of the art, but can even be executed on the client side
in a browser. We propose to approximate the repulsion forces between data
points using adaptive-resolution textures that are drawn at every iteration
with WebGL. This approximation allows us to reformulate the tSNE minimization
problem as a series of tensor operation that are computed with TensorFlow.js, a
JavaScript library for scalable tensor computations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pezzotti_N/0/1/0/all/0/1&quot;&gt;Nicola Pezzotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mordvintsev_A/0/1/0/all/0/1&quot;&gt;Alexander Mordvintsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hollt_T/0/1/0/all/0/1&quot;&gt;Thomas Hollt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lelieveldt_B/0/1/0/all/0/1&quot;&gt;Boudewijn P.F. Lelieveldt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisemann_E/0/1/0/all/0/1&quot;&gt;Elmar Eisemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilanova_A/0/1/0/all/0/1&quot;&gt;Anna Vilanova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10820">
<title>Local Rule-Based Explanations of Black Box Decision Systems. (arXiv:1805.10820v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.10820</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent years have witnessed the rise of accurate but obscure decision
systems which hide the logic of their internal decision processes to the users.
The lack of explanations for the decisions of black box systems is a key
ethical issue, and a limitation to the adoption of machine learning components
in socially sensitive and safety-critical contexts. %Therefore, we need
explanations that reveals the reasons why a predictor takes a certain decision.
In this paper we focus on the problem of black box outcome explanation, i.e.,
explaining the reasons of the decision taken on a specific instance. We propose
LORE, an agnostic method able to provide interpretable and faithful
explanations. LORE first leans a local interpretable predictor on a synthetic
neighborhood generated by a genetic algorithm. Then it derives from the logic
of the local interpretable predictor a meaningful explanation consisting of: a
decision rule, which explains the reasons of the decision; and a set of
counterfactual rules, suggesting the changes in the instance&apos;s features that
lead to a different outcome. Wide experiments show that LORE outperforms
existing methods and baselines both in the quality of explanations and in the
accuracy in mimicking the black box.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guidotti_R/0/1/0/all/0/1&quot;&gt;Riccardo Guidotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monreale_A/0/1/0/all/0/1&quot;&gt;Anna Monreale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruggieri_S/0/1/0/all/0/1&quot;&gt;Salvatore Ruggieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedreschi_D/0/1/0/all/0/1&quot;&gt;Dino Pedreschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turini_F/0/1/0/all/0/1&quot;&gt;Franco Turini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannotti_F/0/1/0/all/0/1&quot;&gt;Fosca Giannotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10884">
<title>Training Medical Image Analysis Systems like Radiologists. (arXiv:1805.10884v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.10884</link>
<description rdf:parseType="Literal">&lt;p&gt;The training of medical image analysis systems using machine learning
approaches follows a common script: collect and annotate a large dataset, train
the classifier on the training set, and test it on a hold-out test set. This
process bears no direct resemblance with radiologist training, which is based
on solving a series of tasks of increasing difficulty, where each task involves
the use of significantly smaller datasets than those used in machine learning.
In this paper, we propose a novel training approach inspired by how
radiologists are trained. In particular, we explore the use of meta-training
that models a classifier based on a series of tasks. Tasks are selected using
teacher-student curriculum learning, where each task consists of simple
classification problems containing small training sets. We hypothesize that our
proposed meta-training approach can be used to pre-train medical image analysis
models. This hypothesis is tested on the automatic breast screening
classification from DCE-MRI trained with weakly labeled datasets. The
classification performance achieved by our approach is shown to be the best in
the field for that application, compared to state of art baseline approaches:
DenseNet, multiple instance learning and multi-task learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maicas_G/0/1/0/all/0/1&quot;&gt;Gabriel Maicas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1&quot;&gt;Andrew P. Bradley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nascimento_J/0/1/0/all/0/1&quot;&gt;Jacinto C. Nascimento&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1&quot;&gt;Ian Reid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1&quot;&gt;Gustavo Carneiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10900">
<title>Hierarchical clustering with deep Q-learning. (arXiv:1805.10900v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.10900</link>
<description rdf:parseType="Literal">&lt;p&gt;The reconstruction and analyzation of high energy particle physics data is
just as important as the analyzation of the structure in real world networks.
In a previous study it was explored how hierarchical clustering algorithms can
be combined with kt cluster algorithms to provide a more generic clusterization
method. Building on that, this paper explores the possibilities to involve deep
learning in the process of cluster computation, by applying reinforcement
learning techniques. The result is a model, that by learning on a modest
dataset of 10; 000 nodes during 70 epochs can reach 83; 77% precision in
predicting the appropriate clusters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forster_R/0/1/0/all/0/1&quot;&gt;Richard Forster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fulop_A/0/1/0/all/0/1&quot;&gt;Agnes Fulop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11074">
<title>Reward Constrained Policy Optimization. (arXiv:1805.11074v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.11074</link>
<description rdf:parseType="Literal">&lt;p&gt;Teaching agents to perform tasks using Reinforcement Learning is no easy
feat. As the goal of reinforcement learning agents is to maximize the
accumulated reward, they often find loopholes and misspecifications in the
reward signal which lead to unwanted behavior. To overcome this, often,
regularization is employed through the technique of reward shaping - the agent
is provided an additional weighted reward signal, meant to lead it towards a
desired behavior. The weight is considered as a hyper-parameter and is selected
through trial and error, a time consuming and computationally intensive task.
In this work, we present a novel multi-timescale approach for constrained
policy optimization, called, &apos;Reward Constrained Policy Optimization&apos; (RCPO),
which enables policy regularization without the use of reward shaping. We prove
the convergence of our approach and provide empirical evidence of its ability
to train constraint satisfying policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tessler_C/0/1/0/all/0/1&quot;&gt;Chen Tessler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mankowitz_D/0/1/0/all/0/1&quot;&gt;Daniel J. Mankowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1&quot;&gt;Shie Mannor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.02526">
<title>Prediction with a Short Memory. (arXiv:1612.02526v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1612.02526</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of predicting the next observation given a sequence
of past observations, and consider the extent to which accurate prediction
requires complex algorithms that explicitly leverage long-range dependencies.
Perhaps surprisingly, our positive results show that for a broad class of
sequences, there is an algorithm that predicts well on average, and bases its
predictions only on the most recent few observation together with a set of
simple summary statistics of the past observations. Specifically, we show that
for any distribution over observations, if the mutual information between past
observations and future observations is upper bounded by $I$, then a simple
Markov model over the most recent $I/\epsilon$ observations obtains expected KL
error $\epsilon$---and hence $\ell_1$ error $\sqrt{\epsilon}$---with respect to
the optimal predictor that has access to the entire past and knows the data
generating distribution. For a Hidden Markov Model with $n$ hidden states, $I$
is bounded by $\log n$, a quantity that does not depend on the mixing time, and
we show that the trivial prediction algorithm based on the empirical
frequencies of length $O(\log n/\epsilon)$ windows of observations achieves
this error, provided the length of the sequence is $d^{\Omega(\log
n/\epsilon)}$, where $d$ is the size of the observation alphabet.
&lt;/p&gt;
&lt;p&gt;We also establish that this result cannot be improved upon, even for the
class of HMMs, in the following two senses: First, for HMMs with $n$ hidden
states, a window length of $\log n/\epsilon$ is information-theoretically
necessary to achieve expected $\ell_1$ error $\sqrt{\epsilon}$. Second, the
$d^{\Theta(\log n/\epsilon)}$ samples required to estimate the Markov model for
an observation alphabet of size $d$ is necessary for any computationally
tractable learning algorithm, assuming the hardness of strongly refuting a
certain class of CSPs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharan_V/0/1/0/all/0/1&quot;&gt;Vatsal Sharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1&quot;&gt;Sham Kakade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1&quot;&gt;Gregory Valiant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.10726">
<title>Strength Factors: An Uncertainty System for a Quantified Modal Logic. (arXiv:1705.10726v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1705.10726</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new system S for handling uncertainty in a quantified modal
logic (first-order modal logic). The system is based on both probability theory
and proof theory. The system is derived from Chisholm&apos;s epistemology. We
concretize Chisholm&apos;s system by grounding his undefined and primitive (i.e.
foundational) concept of reasonablenes in probability and proof theory. S can
be useful in systems that have to interact with humans and provide
justifications for their uncertainty. As a demonstration of the system, we
apply the system to provide a solution to the lottery paradox. Another
advantage of the system is that it can be used to provide uncertainty values
for counterfactual statements. Counterfactuals are statements that an agent
knows for sure are false. Among other cases, counterfactuals are useful when
systems have to explain their actions to users. Uncertainties for
counterfactuals fall out naturally from our system.
&lt;/p&gt;
&lt;p&gt;Efficient reasoning in just simple first-order logic is a hard problem.
Resolution-based first-order reasoning systems have made significant progress
over the last several decades in building systems that have solved non-trivial
tasks (even unsolved conjectures in mathematics). We present a sketch of a
novel algorithm for reasoning that extends first-order resolution.
&lt;/p&gt;
&lt;p&gt;Finally, while there have been many systems of uncertainty for propositional
logics, first-order logics and propositional modal logics, there has been very
little work in building systems of uncertainty for first-order modal logics.
The work described below is in progress; and once finished will address this
lack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govindarajulu_N/0/1/0/all/0/1&quot;&gt;Naveen Sundar Govindarajulu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bringsjord_S/0/1/0/all/0/1&quot;&gt;Selmer Bringsjord&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.03184">
<title>On Formalizing Fairness in Prediction with Machine Learning. (arXiv:1710.03184v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.03184</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning algorithms for prediction are increasingly being used in
critical decisions affecting human lives. Various fairness formalizations, with
no firm consensus yet, are employed to prevent such algorithms from
systematically discriminating against people based on certain attributes
protected by law. The aim of this article is to survey how fairness is
formalized in the machine learning literature for the task of prediction and
present these formalizations with their corresponding notions of distributive
justice from the social sciences literature. We provide theoretical as well as
empirical critiques of these notions from the social sciences literature and
explain how these critiques limit the suitability of the corresponding fairness
formalizations to certain domains. We also suggest two notions of distributive
justice which address some of these critiques and discuss avenues for
prospective fairness formalizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gajane_P/0/1/0/all/0/1&quot;&gt;Pratik Gajane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1&quot;&gt;Mykola Pechenizkiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10386">
<title>Dual Skipping Networks. (arXiv:1710.10386v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10386</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the recent neuroscience studies on the left-right asymmetry of
the human brain in processing low and high spatial frequency information, this
paper introduces a dual skipping network which carries out coarse-to-fine
object categorization. Such a network has two branches to simultaneously deal
with both coarse and fine-grained classification tasks. Specifically, we
propose a layer-skipping mechanism that learns a gating network to predict
which layers to skip in the testing stage. This layer-skipping mechanism endows
the network with good flexibility and capability in practice. Evaluations are
conducted on several widely used coarse-to-fine object categorization
benchmarks, and promising results are achieved by our proposed network model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Changmao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yanwei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wenlian Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jianfeng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiangyang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09564">
<title>Reinforcement and Imitation Learning for Diverse Visuomotor Skills. (arXiv:1802.09564v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09564</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a model-free deep reinforcement learning method that leverages a
small amount of demonstration data to assist a reinforcement learning agent. We
apply this approach to robotic manipulation tasks and train end-to-end
visuomotor policies that map directly from RGB camera inputs to joint
velocities. We demonstrate that our approach can solve a wide variety of
visuomotor tasks, for which engineering a scripted controller would be
laborious. In experiments, our reinforcement and imitation agent achieves
significantly better performances than agents trained with reinforcement
learning or imitation learning alone. We also illustrate that these policies,
trained with large visual and dynamics variations, can achieve preliminary
successes in zero-shot sim2real transfer. A brief visual description of this
work can be viewed in https://youtu.be/EDl8SQUNjj0
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merel_J/0/1/0/all/0/1&quot;&gt;Josh Merel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rusu_A/0/1/0/all/0/1&quot;&gt;Andrei Rusu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erez_T/0/1/0/all/0/1&quot;&gt;Tom Erez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1&quot;&gt;Serkan Cabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tunyasuvunakool_S/0/1/0/all/0/1&quot;&gt;Saran Tunyasuvunakool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kramar_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe1;nos Kram&amp;#xe1;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadsell_R/0/1/0/all/0/1&quot;&gt;Raia Hadsell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freitas_N/0/1/0/all/0/1&quot;&gt;Nando de Freitas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1&quot;&gt;Nicolas Heess&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06288">
<title>ORGaNICs: A Theory of Working Memory in Brains and Machines. (arXiv:1803.06288v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06288</link>
<description rdf:parseType="Literal">&lt;p&gt;Working memory is a cognitive process that is responsible for temporarily
holding and manipulating information. Most of the empirical neuroscience
research on working memory has focused on measuring sustained activity in
prefrontal cortex (PFC) and/or parietal cortex during simple delayed-response
tasks, and most of the models of working memory have been based on neural
integrators. But working memory means much more than just holding a piece of
information online. We describe a new theory of working memory, based on a
recurrent neural circuit that we call ORGaNICs (Oscillatory Recurrent GAted
Neural Integrator Circuits). ORGaNICs are a variety of Long Short Term Memory
units (LSTMs), imported from machine learning and artificial intelligence.
ORGaNICs can be used to explain the complex dynamics of delay-period activity
in prefrontal cortex (PFC) during a working memory task. The theory is
analytically tractable so that we can characterize the dynamics, and the theory
provides a means for reading out information from the dynamically varying
responses at any point in time, in spite of the complex dynamics. ORGaNICs can
be implemented with a biophysical (electrical circuit) model of pyramidal
cells, combined with shunting inhibition via a thalamocortical loop. Although
introduced as a computational theory of working memory, ORGaNICs are also
applicable to models of sensory processing, motor preparation and motor
control. ORGaNICs offer computational advantages compared to other varieties of
LSTMs that are commonly used in AI applications. Consequently, ORGaNICs are a
framework for canonical computation in brains and machines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heeger_D/0/1/0/all/0/1&quot;&gt;David J. Heeger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mackey_W/0/1/0/all/0/1&quot;&gt;Wayne E. Mackey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07710">
<title>Inference in Probabilistic Graphical Models by Graph Neural Networks. (arXiv:1803.07710v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07710</link>
<description rdf:parseType="Literal">&lt;p&gt;A fundamental computation for statistical inference and accurate
decision-making is to compute the marginal probabilities or most probable
states of task-relevant variables. Probabilistic graphical models can
efficiently represent the structure of such complex data, but performing these
inferences is generally difficult. Message-passing algorithms, such as belief
propagation, are a natural way to disseminate evidence amongst correlated
variables while exploiting the graph structure, but these algorithms can
struggle when the conditional dependency graphs contain loops. Here we use
Graph Neural Networks (GNNs) to learn a message-passing algorithm that solves
these inference tasks. We first show that the architecture of GNNs is
well-matched to inference tasks. We then demonstrate the efficacy of this
inference approach by training GNNs on a collection of graphical models and
showing that they substantially outperform belief propagation on loopy graphs.
Our message-passing algorithms generalize out of the training set to larger
graphs and graphs with different structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;KiJung Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1&quot;&gt;Renjie Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lisa Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1&quot;&gt;Ethan Fetaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pitkow_X/0/1/0/all/0/1&quot;&gt;Xaq Pitkow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09928">
<title>Entropy Controlled Non-Stationarity for Improving Performance of Independent Learners in Anonymous MARL Settings. (arXiv:1803.09928v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09928</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advent of sequential matching (of supply and demand) systems (uber,
Lyft, Grab for taxis; ubereats, deliveroo, etc for food; amazon prime, lazada
etc. for groceries) across many online and offline services, individuals (taxi
drivers, delivery boys, delivery van drivers, etc.) earn more by being at the
&quot;right&quot; place at the &quot;right&quot; time. We focus on learning techniques for
providing guidance (on right locations to be at right times) to individuals in
the presence of other &quot;learning&quot; individuals. Interactions between indivduals
are anonymous, i.e, the outcome of an interaction (competing for demand) is
independent of the identity of the agents and therefore we refer to these as
Anonymous MARL settings.
&lt;/p&gt;
&lt;p&gt;Existing research of relevance is on independent learning using Reinforcement
Learning (RL) or on Multi-Agent Reinforcement Learning (MARL). The number of
individuals in aggregation systems is extremely large and individuals have
their own selfish interest (of maximising revenue). Therefore, traditional MARL
approaches are either not scalable or assumptions of common objective or action
coordination are not viable. In this paper, we focus on improving performance
of independent reinforcement learners, specifically the popular Deep Q-Networks
(DQN) and Advantage Actor Critic (A2C) approaches by exploiting anonymity.
Specifically, we control non-stationarity introduced by other agents using
entropy of agent density distribution. We demonstrate a significant improvement
in revenue for individuals and for all agents together with our learners on a
generic experimental set up for aggregation systems and a real world taxi
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_T/0/1/0/all/0/1&quot;&gt;Tanvi Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varakantham_P/0/1/0/all/0/1&quot;&gt;Pradeep Varakantham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_H/0/1/0/all/0/1&quot;&gt;Hoong Chuin Lau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00506">
<title>Towards Explanation of DNN-based Prediction with Guided Feature Inversion. (arXiv:1804.00506v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00506</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep neural networks (DNN) have become an effective computational tool,
the prediction results are often criticized by the lack of interpretability,
which is essential in many real-world applications such as health informatics.
Existing attempts based on local interpretations aim to identify relevant
features contributing the most to the prediction of DNN by monitoring the
neighborhood of a given input. They usually simply ignore the intermediate
layers of the DNN that might contain rich information for interpretation. To
bridge the gap, in this paper, we propose to investigate a guided feature
inversion framework for taking advantage of the deep architectures towards
effective interpretation. The proposed framework not only determines the
contribution of each feature in the input but also provides insights into the
decision-making process of DNN models. By further interacting with the neuron
of the target category at the output layer of the DNN, we enforce the
interpretation result to be class-discriminative. We apply the proposed
interpretation model to different CNN architectures to provide explanations for
image data and conduct extensive experiments on ImageNet and PASCAL VOC07
datasets. The interpretation results demonstrate the effectiveness of our
proposed framework in providing class-discriminative interpretation for
DNN-based prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1&quot;&gt;Qingquan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xia Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10309">
<title>Learning Self-Imitating Diverse Policies. (arXiv:1805.10309v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10309</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning algorithms, including policy gradient methods and
Q-learning, have been widely applied to a variety of decision-making problems.
Their success has relied heavily on having very well designed dense reward
signals, and therefore, they often perform badly on the sparse or episodic
reward settings. Trajectory-based policy optimization methods, such as
cross-entropy method and evolution strategies, do not take into consideration
the temporal nature of the problem and often suffer from high sample
complexity. Scaling up the efficiency of RL algorithms to real-world problems
with sparse or episodic rewards is therefore a pressing need. In this work, we
present a new perspective of policy optimization and introduce a self-imitation
learning algorithm that exploits and explores well in the sparse and episodic
reward settings. First, we view each policy as a state-action visitation
distribution and formulate policy optimization as a divergence minimization
problem. Then, we show that, with Jensen-Shannon divergence, this divergence
minimization problem can be reduced into a policy-gradient algorithm with dense
reward learned from experience replays. Experimental results indicate that our
algorithm works comparable to existing algorithms in the dense reward setting,
and significantly better in the sparse and episodic settings. To encourage
exploration, we further apply the Stein variational policy gradient descent
with the Jensen-Shannon kernel to learn multiple diverse policies and
demonstrate its effectiveness on a number of challenging tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gangwani_T/0/1/0/all/0/1&quot;&gt;Tanmay Gangwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jian Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10318">
<title>Enhancing the Accuracy and Fairness of Human Decision Making. (arXiv:1805.10318v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10318</link>
<description rdf:parseType="Literal">&lt;p&gt;Societies often rely on human experts to take a wide variety of decisions
affecting their members, from jail-or-release decisions taken by judges and
stop-and-frisk decisions taken by police officers to accept-or-reject decisions
taken by academics. In this context, each decision is taken by an expert who is
typically chosen uniformly at random from a pool of experts. However, these
decisions may be imperfect due to limited experience, implicit biases, or
faulty probabilistic reasoning. Can we improve the accuracy and fairness of the
overall decision making process by optimizing the assignment between experts
and decisions?
&lt;/p&gt;
&lt;p&gt;In this paper, we address the above problem from the perspective of
sequential decision making and show that, for different fairness notions from
the literature, it reduces to a sequence of (constrained) weighted bipartite
matchings, which can be solved efficiently using algorithms with approximation
guarantees. Moreover, these algorithms also benefit from posterior sampling to
actively trade off exploitation---selecting expert assignments which lead to
accurate and fair decisions---and exploration---selecting expert assignments to
learn about the experts&apos; preferences and biases. We demonstrate the
effectiveness of our algorithms on both synthetic and real-world data and show
that they can significantly improve both the accuracy and fairness of the
decisions taken by pools of experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Valera_I/0/1/0/all/0/1&quot;&gt;Isabel Valera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Singla_A/0/1/0/all/0/1&quot;&gt;Adish Singla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rodriguez_M/0/1/0/all/0/1&quot;&gt;Manuel Gomez Rodriguez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10341">
<title>An end-to-end Differentially Private Latent Dirichlet Allocation Using a Spectral Algorithm. (arXiv:1805.10341v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10341</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent Dirichlet Allocation (LDA) is a powerful probabilistic model used to
cluster documents based on thematic structure. We provide end-to-end analysis
of {\em differentially private\/} LDA learning models, based on a spectral
algorithm with established theoretically guaranteed utility. The spectral
algorithm involves a complex data flow, with multiple options for noise
injection. We analyze the sensitivity and utility of different configurations
of noise injection to characterize configurations that achieve least
performance degradation under different operating regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Esmaeili_S/0/1/0/all/0/1&quot;&gt;Seyed A. Esmaeili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10348">
<title>Guaranteed Simultaneous Asymmetric Tensor Decomposition via Orthogonalized Alternating Least Squares. (arXiv:1805.10348v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10348</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the asymmetric orthogonal tensor decomposition problem, and
present an orthogonalized alternating least square algorithm that converges to
rank-$r$ of the true tensor factors simultaneously in
$O(\log(\log(\frac{1}{\epsilon})))$ steps under our proposed Trace Based
Initialization procedure. Trace Based Initialization requires $O(1/{\log
(\frac{\lambda_{r}}{\lambda_{r+1}})})$ number of matrix subspace iterations to
guarantee a &quot;good&quot; initialization for the simultaneous orthogonalized ALS
method, where $\lambda_r$ is the $r$-th largest singular value of the tensor.
We are the first to give a theoretical guarantee on orthogonal asymmetric
tensor decomposition using Trace Based Initialization procedure and the
orthogonalized alternating least squares. Our Trace Based Initialization also
improves convergence for symmetric orthogonal tensor decomposition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jialin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10367">
<title>Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization. (arXiv:1805.10367v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10367</link>
<description rdf:parseType="Literal">&lt;p&gt;As application demands for zeroth-order (gradient-free) optimization
accelerate, the need for variance reduced and faster converging approaches is
also intensifying. This paper addresses these challenges by presenting: a) a
comprehensive theoretical analysis of variance reduced zeroth-order (ZO)
optimization, b) a novel variance reduced ZO algorithm, called ZO-SVRG, and c)
an experimental evaluation of our approach in the context of two compelling
applications, black-box chemical material classification and generation of
adversarial examples from black-box deep neural network models. Our theoretical
analysis uncovers an essential difficulty in the analysis of ZO-SVRG: the
unbiased assumption on gradient estimates no longer holds. We prove that
compared to its first-order counterpart, ZO-SVRG with a two-point random
gradient estimator suffers an additional error of order $O(1/b)$, where $b$ is
the mini-batch size. To mitigate this error, we propose two accelerated
versions of ZO-SVRG utilizing variance reduced gradient estimators, which
achieve the best rate known for ZO stochastic optimization (in terms of
iterations). Our extensive experimental results show that our approaches
outperform other state-of-the-art ZO algorithms, and strike a balance between
the convergence rate and the function query complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1&quot;&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ting_P/0/1/0/all/0/1&quot;&gt;Paishun Ting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shiyu Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amini_L/0/1/0/all/0/1&quot;&gt;Lisa Amini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10377">
<title>Variational Measure Preserving Flows. (arXiv:1805.10377v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10377</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic modelling is a general and elegant framework to capture the
uncertainty, ambiguity and diversity of hidden structures in data.
Probabilistic inference is the key operation on probabilistic models to obtain
the distribution over the latent representations given data. Unfortunately, the
computation of inference on complex models is extremely challenging. In spite
of the success of existing inference methods, like Markov chain Monte
Carlo(MCMC) and variational inference(VI), many powerful models are not
available for large scale problems because inference is simply computationally
intractable. The recent advances in using neural networks for probabilistic
inference have shown promising results on this challenge. In this work, we
propose a novel general inference framework that has the strength from both
MCMC and VI. The proposed method is not only computationally scalable and
efficient, but also has its root from the ergodicity theorem, that provides the
guarantee of better performance with more computational power. Our experiment
results suggest that our method can outperform state-of-the-art methods on
generative models and Bayesian neural networks on some popular benchmark
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1&quot;&gt;Jose Miguel Hernandez-Lobato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghahramani_Z/0/1/0/all/0/1&quot;&gt;Zoubin Ghahramani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10378">
<title>Gradient Coding via the Stochastic Block Model. (arXiv:1805.10378v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10378</link>
<description rdf:parseType="Literal">&lt;p&gt;Gradient descent and its many variants, including mini-batch stochastic
gradient descent, form the algorithmic foundation of modern large-scale machine
learning. Due to the size and scale of modern data, gradient computations are
often distributed across multiple compute nodes. Unfortunately, such
distributed implementations can face significant delays caused by straggler
nodes, i.e., nodes that are much slower than average. Gradient coding is a new
technique for mitigating the effect of stragglers via algorithmic redundancy.
While effective, previously proposed gradient codes can be computationally
expensive to construct, inaccurate, or susceptible to adversarial stragglers.
In this work, we present the stochastic block code (SBC), a gradient code based
on the stochastic block model. We show that SBCs are efficient, accurate, and
that under certain settings, adversarial straggler selection becomes as hard as
detecting a community structure in the multiple community, block stochastic
graph model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Charles_Z/0/1/0/all/0/1&quot;&gt;Zachary Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papailiopoulos_D/0/1/0/all/0/1&quot;&gt;Dimitris Papailiopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10384">
<title>Large-scale Distance Metric Learning with Uncertainty. (arXiv:1805.10384v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10384</link>
<description rdf:parseType="Literal">&lt;p&gt;Distance metric learning (DML) has been studied extensively in the past
decades for its superior performance with distance-based algorithms. Most of
the existing methods propose to learn a distance metric with pairwise or
triplet constraints. However, the number of constraints is quadratic or even
cubic in the number of the original examples, which makes it challenging for
DML to handle the large-scale data set. Besides, the real-world data may
contain various uncertainty, especially for the image data. The uncertainty can
mislead the learning procedure and cause the performance degradation. By
investigating the image data, we find that the original data can be observed
from a small set of clean latent examples with different distortions. In this
work, we propose the margin preserving metric learning framework to learn the
distance metric and latent examples simultaneously. By leveraging the ideal
properties of latent examples, the training efficiency can be improved
significantly while the learned metric also becomes robust to the uncertainty
in the original data. Furthermore, we can show that the metric is learned from
latent examples only, but it can preserve the large margin property even for
the original data. The empirical study on the benchmark image data sets
demonstrates the efficacy and efficiency of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1&quot;&gt;Qi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiasheng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shenghuo Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Rong Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10406">
<title>Robust Nonparametric Regression under Huber&apos;s $\epsilon$-contamination Model. (arXiv:1805.10406v1 [math.ST])</title>
<link>http://arxiv.org/abs/1805.10406</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the non-parametric regression problem under Huber&apos;s
$\epsilon$-contamination model, in which an $\epsilon$ fraction of observations
are subject to arbitrary adversarial noise. We first show that a simple local
binning median step can effectively remove the adversary noise and this median
estimator is minimax optimal up to absolute constants over the H\&quot;{o}lder
function class with smoothness parameters smaller than or equal to 1.
Furthermore, when the underlying function has higher smoothness, we show that
using local binning median as pre-preprocessing step to remove the adversarial
noise, then we can apply any non-parametric estimator on top of the medians. In
particular we show local median binning followed by kernel smoothing and local
polynomial regression achieve minimaxity over H\&quot;{o}lder and Sobolev classes
with arbitrary smoothness parameters. Our main proof technique is a decoupled
analysis of adversary noise and stochastic noise, which can be potentially
applied to other robust estimation problems. We also provide numerical results
to verify the effectiveness of our proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon S. Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yining Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Balakrishnan_S/0/1/0/all/0/1&quot;&gt;Sivaraman Balakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aarti Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10469">
<title>Revisiting Reweighted Wake-Sleep. (arXiv:1805.10469v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10469</link>
<description rdf:parseType="Literal">&lt;p&gt;Discrete latent-variable models, while applicable in a variety of settings,
can often be difficult to learn. Sampling discrete latent variables can result
in high-variance gradient estimators for two primary reasons: 1. branching on
the samples within the model, and 2. the lack of a pathwise derivative for the
samples. While current state-of-the-art methods employ control-variate schemes
for the former and continuous-relaxation methods for the latter, their utility
is limited by the complexities of implementing and training effective
control-variate schemes and the necessity of evaluating (potentially
exponentially) many branch paths in the model. Here, we revisit the reweighted
wake-sleep (RWS) (Bornschein and Bengio, 2015) algorithm, and through extensive
evaluations, show that it circumvents both these issues, outperforming current
state-of-the-art methods in learning discrete latent-variable models. Moreover,
we observe that, unlike the importance weighted autoencoder, RWS learns better
models and inference networks with increasing numbers of particles, and that
its benefits extend to continuous latent-variable models as well. Our results
suggest that RWS is a competitive, often preferable, alternative for learning
deep generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Tuan Anh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kosiorek_A/0/1/0/all/0/1&quot;&gt;Adam R. Kosiorek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siddharth_N/0/1/0/all/0/1&quot;&gt;N. Siddharth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1&quot;&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wood_F/0/1/0/all/0/1&quot;&gt;Frank Wood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10477">
<title>Nonlinear Inductive Matrix Completion based on One-layer Neural Networks. (arXiv:1805.10477v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10477</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of a recommendation system is to predict the interest of a user in a
given item by exploiting the existing set of ratings as well as certain
user/item features. A standard approach to modeling this problem is Inductive
Matrix Completion where the predicted rating is modeled as an inner product of
the user and the item features projected onto a latent space. In order to learn
the parameters effectively from a small number of observed ratings, the latent
space is constrained to be low-dimensional which implies that the parameter
matrix is constrained to be low-rank. However, such bilinear modeling of the
ratings can be limiting in practice and non-linear prediction functions can
lead to significant improvements. A natural approach to introducing
non-linearity in the prediction function is to apply a non-linear activation
function on top of the projected user/item features. Imposition of
non-linearities further complicates an already challenging problem that has two
sources of non-convexity: a) low-rank structure of the parameter matrix, and b)
non-linear activation function. We show that one can still solve the non-linear
Inductive Matrix Completion problem using gradient descent type methods as long
as the solution is initialized well. That is, close to the optima, the
optimization function is strongly convex and hence admits standard optimization
techniques, at least for certain activation functions, such as Sigmoid and
tanh. We also highlight the importance of the activation function and show how
ReLU can behave significantly differently than say a sigmoid function. Finally,
we apply our proposed technique to recommendation systems and semi-supervised
clustering, and show that our method can lead to much better performance than
standard linear Inductive Matrix Completion methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1&quot;&gt;Kai Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1&quot;&gt;Prateek Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1&quot;&gt;Inderjit S. Dhillon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10522">
<title>Calibrating Deep Convolutional Gaussian Processes. (arXiv:1805.10522v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10522</link>
<description rdf:parseType="Literal">&lt;p&gt;The wide adoption of Convolutional Neural Networks (CNNs) in applications
where decision-making under uncertainty is fundamental, has brought a great
deal of attention to the ability of these models to accurately quantify the
uncertainty in their predictions. Previous work on combining CNNs with Gaussian
processes (GPs) has been developed under the assumption that the predictive
probabilities of these models are well-calibrated. In this paper we show that,
in fact, current combinations of CNNs and GPs are miscalibrated. We proposes a
novel combination that considerably outperforms previous approaches on this
aspect, while achieving state-of-the-art performance on image classification
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tran_G/0/1/0/all/0/1&quot;&gt;Gia-Lac Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bonilla_E/0/1/0/all/0/1&quot;&gt;Edwin V. Bonilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cunningham_J/0/1/0/all/0/1&quot;&gt;John P. Cunningham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Michiardi_P/0/1/0/all/0/1&quot;&gt;Pietro Michiardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Filippone_M/0/1/0/all/0/1&quot;&gt;Maurizio Filippone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10531">
<title>Unsupervised Learning with Stein&apos;s Unbiased Risk Estimator. (arXiv:1805.10531v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10531</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from unlabeled and noisy data is one of the grand challenges of
machine learning. As such, it has seen a flurry of research with new ideas
proposed continuously. In this work, we revisit a classical idea: Stein&apos;s
Unbiased Risk Estimator (SURE). We show that, in the context of image recovery,
SURE and its generalizations can be used to train convolutional neural networks
(CNNs) for a range of image denoising and recovery problems {\em without any
ground truth data.}
&lt;/p&gt;
&lt;p&gt;Specifically, our goal is to reconstruct an image $x$ from a {\em noisy}
linear transformation (measurement) of the image. We consider two scenarios:
one where no additional data is available and one where we have measurements of
other images that are drawn from the same noisy distribution as $x$, but have
no access to the clean images. Such is the case, for instance, in the context
of medical imaging, microscopy, and astronomy, where noise-less ground truth
data is rarely available.
&lt;/p&gt;
&lt;p&gt;We show that in this situation, SURE can be used to estimate the
mean-squared-error loss associated with an estimate of $x$. Using this estimate
of the loss, we train networks to perform denoising and compressed sensing
recovery. In addition, we also use the SURE framework to partially explain and
improve upon an intriguing results presented by Ulyanov et al. in &quot;Deep Image
Prior&quot;: that a network initialized with random weights and fit to a single
noisy image can effectively denoise that image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Metzler_C/0/1/0/all/0/1&quot;&gt;Christopher A. Metzler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mousavi_A/0/1/0/all/0/1&quot;&gt;Ali Mousavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Heckel_R/0/1/0/all/0/1&quot;&gt;Reinhard Heckel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baraniuk_R/0/1/0/all/0/1&quot;&gt;Richard G. Baraniuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10572">
<title>BRITS: Bidirectional Recurrent Imputation for Time Series. (arXiv:1805.10572v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10572</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series are widely used as signals in many classification/regression
tasks. It is ubiquitous that time series contains many missing values. Given
multiple correlated time series data, how to fill in missing values and to
predict their class labels? Existing imputation methods often impose strong
assumptions of the underlying data generating process, such as linear dynamics
in the state space. In this paper, we propose BRITS, a novel method based on
recurrent neural networks for missing value imputation in time series data. Our
proposed method directly learns the missing values in a bidirectional recurrent
dynamical system, without any specific assumption. The imputed values are
treated as variables of RNN graph and can be effectively updated during the
backpropagation.BRITS has three advantages: (a) it can handle multiple
correlated missing values in time series; (b) it generalizes to time series
with nonlinear dynamics underlying; (c) it provides a data-driven imputation
procedure and applies to general settings with missing data.We evaluate our
model on three real-world datasets, including an air quality dataset, a
health-care data, and a localization data for human activity. Experiments show
that our model outperforms the state-of-the-art methods in both imputation and
classification/regression accuracies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1&quot;&gt;Wei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yitan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10611">
<title>Robust Hypothesis Testing Using Wasserstein Uncertainty Sets. (arXiv:1805.10611v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10611</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a novel computationally efficient and general framework for robust
hypothesis testing. The new framework features a new way to construct
uncertainty sets under the null and the alternative distributions, which are
sets centered around the empirical distribution defined via Wasserstein metric,
thus our approach is data-driven and free of distributional assumptions. We
develop a convex safe approximation of the minimax formulation and show that
such approximation renders a nearly-optimal detector among the family of all
possible tests. By exploiting the structure of the least favorable
distribution, we also develop a tractable reformulation of such approximation,
with complexity independent of the dimension of observation space and can be
nearly sample-size-independent in general. Real-data example using human
activity data demonstrated the excellent performance of the new robust
detector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Rui Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Liyan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10615">
<title>A Local Information Criterion for Dynamical Systems. (arXiv:1805.10615v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10615</link>
<description rdf:parseType="Literal">&lt;p&gt;Encoding a sequence of observations is an essential task with many
applications. The encoding can become highly efficient when the observations
are generated by a dynamical system. A dynamical system imposes regularities on
the observations that can be leveraged to achieve a more efficient code. We
propose a method to encode a given or learned dynamical system. Apart from its
application for encoding a sequence of observations, we propose to use the
compression achieved by this encoding as a criterion for model selection. Given
a dataset, different learning algorithms result in different models. But not
all learned models are equally good. We show that the proposed encoding
approach can be used to choose the learned model which is closer to the true
underlying dynamics. We provide experiments for both encoding and model
selection, and theoretical results that shed light on why the approach works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mehrjou_A/0/1/0/all/0/1&quot;&gt;Arash Mehrjou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Solowjow_F/0/1/0/all/0/1&quot;&gt;Friedrich Solowjow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trimpe_S/0/1/0/all/0/1&quot;&gt;Sebastian Trimpe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10616">
<title>Dynamic Network Model from Partial Observations. (arXiv:1805.10616v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10616</link>
<description rdf:parseType="Literal">&lt;p&gt;Can evolving networks be inferred and modeled without directly observing
their nodes and edges? In many applications, the edges of a dynamic network
might not be observed, but one can observe the dynamics of stochastic cascading
processes (e.g., information diffusion, virus propagation) occurring over the
unobserved network. While there have been efforts to infer networks based on
such data, providing a generative probabilistic model that is able to identify
the underlying time-varying network remains an open question. Here we consider
the problem of inferring generative dynamic network models based on network
cascade diffusion data. We propose a novel framework for providing a
non-parametric dynamic network model--based on a mixture of coupled
hierarchical Dirichlet processes-- based on data capturing cascade node
infection times. Our approach allows us to infer the evolving community
structure in networks and to obtain an explicit predictive distribution over
the edges of the underlying network--including those that were not involved in
transmission of any cascade, or are likely to appear in the future. We show the
effectiveness of our approach using extensive experiments on synthetic as well
as real-world networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghalebi_E/0/1/0/all/0/1&quot;&gt;Elahe Ghalebi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirzasoleiman_B/0/1/0/all/0/1&quot;&gt;Baharan Mirzasoleiman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1&quot;&gt;Radu Grosu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10638">
<title>Fast K-Means Clustering with Anderson Acceleration. (arXiv:1805.10638v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10638</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel method to accelerate Lloyd&apos;s algorithm for K-Means
clustering. Unlike previous acceleration approaches that reduce computational
cost per iterations or improve initialization, our approach is focused on
reducing the number of iterations required for convergence. This is achieved by
treating the assignment step and the update step of Lloyd&apos;s algorithm as a
fixed-point iteration, and applying Anderson acceleration, a well-established
technique for accelerating fixed-point solvers. Classical Anderson acceleration
utilizes m previous iterates to find an accelerated iterate, and its
performance on K-Means clustering can be sensitive to choice of m and the
distribution of samples. We propose a new strategy to dynamically adjust the
value of m, which achieves robust and consistent speedups across different
problem instances. Our method complements existing acceleration techniques, and
can be combined with them to achieve state-of-the-art performance. We perform
extensive experiments to evaluate the performance of the proposed method, where
it outperforms other algorithms in 106 out of 120 test cases, and the mean
decrease ratio of computational time is more than 33%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Juyong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuxin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yue Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1&quot;&gt;Bailin Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10665">
<title>Adversarial Deformation Regularization for Training Image Registration Neural Networks. (arXiv:1805.10665v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10665</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe an adversarial learning approach to constrain convolutional
neural network training for image registration, replacing heuristic smoothness
measures of displacement fields often used in these tasks. Using
minimally-invasive prostate cancer intervention as an example application, we
demonstrate the feasibility of utilizing biomechanical simulations to
regularize a weakly-supervised anatomical-label-driven registration network for
aligning pre-procedural magnetic resonance (MR) and 3D intra-procedural
transrectal ultrasound (TRUS) images. A discriminator network is optimized to
distinguish the registration-predicted displacement fields from the motion data
simulated by finite element analysis. During training, the registration network
simultaneously aims to maximize similarity between anatomical labels that
drives image alignment and to minimize an adversarial generator loss that
measures divergence between the predicted- and simulated deformation. The
end-to-end trained network enables efficient and fully-automated registration
that only requires an MR and TRUS image pair as input, without anatomical
labels or simulated data during inference. 108 pairs of labelled MR and TRUS
images from 76 prostate cancer patients and 71,500 nonlinear finite-element
simulations from 143 different patients were used for this study. We show that,
with only gland segmentation as training labels, the proposed method can help
predict physically plausible deformation without any other smoothness penalty.
Based on cross-validation experiments using 834 pairs of independent validation
landmarks, the proposed adversarial-regularized registration achieved a target
registration error of 6.3 mm that is significantly lower than those from
several other regularization methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yipeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gibson_E/0/1/0/all/0/1&quot;&gt;Eli Gibson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghavami_N/0/1/0/all/0/1&quot;&gt;Nooshin Ghavami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonmati_E/0/1/0/all/0/1&quot;&gt;Ester Bonmati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_C/0/1/0/all/0/1&quot;&gt;Caroline M. Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emberton_M/0/1/0/all/0/1&quot;&gt;Mark Emberton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1&quot;&gt;Tom Vercauteren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1&quot;&gt;J. Alison Noble&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1&quot;&gt;Dean C. Barratt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10694">
<title>Towards a Theoretical Understanding of Batch Normalization. (arXiv:1805.10694v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10694</link>
<description rdf:parseType="Literal">&lt;p&gt;Normalization techniques such as Batch Normalization have been applied very
successfully for training deep neural networks. Yet, despite its apparent
empirical benefits, the reasons behind the success of Batch Normalization are
mostly hypothetical. We thus aim to provide a more thorough theoretical
understanding from an optimization perspective. Our main contribution towards
this goal is the identification of various problem instances in the realm of
machine learning where, under certain assumptions, Batch Normalization can
provably accelerate optimization with gradient-based methods. We thereby turn
Batch Normalization from an effective practical heuristic into a provably
converging algorithm for these settings. Furthermore, we substantiate our
analysis with empirical evidence that suggests the validity of our theoretical
results in a broader context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohler_J/0/1/0/all/0/1&quot;&gt;Jonas Kohler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Daneshmand_H/0/1/0/all/0/1&quot;&gt;Hadi Daneshmand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lucchi_A/0/1/0/all/0/1&quot;&gt;Aurelien Lucchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Ming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neymeyr_K/0/1/0/all/0/1&quot;&gt;Klaus Neymeyr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hofmann_T/0/1/0/all/0/1&quot;&gt;Thomas Hofmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10724">
<title>RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records. (arXiv:1805.10724v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10724</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past decade, we have seen many successful applications of recurrent
neural networks (RNNs) on electronic medical records (EMRs), which contain
histories of patients&apos; diagnoses, medications, and other various events, in
order to predict the current and future states of patients. Despite the strong
performance of RNNs, it is often very challenging for users to understand why
the model makes a particular prediction. Such black box nature of RNNs can
impede its wide adoption in clinical practice. Furthermore, we have no
established method to interactively leverage users&apos; domain expertise and prior
knowledge as inputs for steering the model. Therefore, our design study aims to
provide a visual analytics solution to increase interpretability and
interactivity of RNNs via a joint effort of medical experts, artificial
intelligence scientists, and visual analytics researchers. Following the
iterative design process between the experts, we design, implement, and
evaluate a visual analytics tool called RetainVis, which couples a recently
proposed, interpretable RNN-based model called RETAIN and visualizations for
users&apos; exploration of EMR data in the context of prediction tasks. Our study
shows the effective use of RetainVis for gaining insights into how RNN models
EMR data, using real medical records of patients with heart failure, cataract,
or dermatological symptoms. Our study also demonstrates how we made substantial
changes to the state-of-the-art RNN model called RETAIN in order to make use of
temporal information and increase interactivity. This study will provide a
useful guideline for researchers who aim to design more interpretable and
interactive visual analytics tool for RNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_B/0/1/0/all/0/1&quot;&gt;Bum Chul Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Min-Je Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Joanne Taery Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Edward Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Young Bin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1&quot;&gt;Soonwook Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_a/0/1/0/all/0/1&quot;&gt;and Jaegul Choo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10755">
<title>Dual Policy Iteration. (arXiv:1805.10755v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10755</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a novel class of Approximate Policy Iteration (API) algorithms have
demonstrated impressive practical performance (e.g., ExIt from [2],
AlphaGo-Zero from [27]). This new family of algorithms maintains, and
alternately optimizes, two policies: a fast, reactive policy (e.g., a deep
neural network) deployed at test time, and a slow, non-reactive policy (e.g.,
Tree Search), that can plan multiple steps ahead. The reactive policy is
updated under supervision from the non-reactive policy, while the non-reactive
policy is improved with guidance from the reactive policy. In this work we
study this Dual Policy Iteration (DPI) strategy in an alternating optimization
framework and provide a convergence analysis that extends existing API theory.
We also develop a special instance of this framework which reduces the update
of non-reactive policies to model-based optimal control using learned local
models, and provides a theoretically sound way of unifying model-free and
model-based RL approaches with unknown dynamics. We demonstrate the efficacy of
our approach on various continuous control Markov Decision Processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1&quot;&gt;Geoffrey J. Gordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1&quot;&gt;Byron Boots&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagnell_J/0/1/0/all/0/1&quot;&gt;J. Andrew Bagnell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10759">
<title>Clustering by latent dimensions. (arXiv:1805.10759v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10759</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new clustering technique, called {\em dimensional
clustering}, which clusters each data point by its latent {\em pointwise
dimension}, which is a measure of the dimensionality of the data set local to
that point. Pointwise dimension is invariant under a broad class of
transformations. As a result, dimensional clustering can be usefully applied to
a wide range of datasets. Concretely, we present a statistical model which
estimates the pointwise dimension of a dataset around the points in that
dataset using the distance of each point from its $n^{\text{th}}$ nearest
neighbor. We demonstrate the applicability of our technique to the analysis of
dynamical systems, images, and complex human movements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hidaka_S/0/1/0/all/0/1&quot;&gt;Shohei Hidaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kashyap_N/0/1/0/all/0/1&quot;&gt;Neeraj Kashyap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10767">
<title>Understanding Generalization and Optimization Performance of Deep CNNs. (arXiv:1805.10767v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10767</link>
<description rdf:parseType="Literal">&lt;p&gt;This work aims to provide understandings on the remarkable success of deep
convolutional neural networks (CNNs) by theoretically analyzing their
generalization performance and establishing optimization guarantees for
gradient descent based training algorithms. Specifically, for a CNN model
consisting of $l$ convolutional layers and one fully connected layer, we prove
that its generalization error is bounded by
$\mathcal{O}(\sqrt{\dt\widetilde{\varrho}/n})$ where $\theta$ denotes freedom
degree of the network parameters and
$\widetilde{\varrho}=\mathcal{O}(\log(\prod_{i=1}^{l}\rwi{i}
(\ki{i}-\si{i}+1)/p)+\log(\rf))$ encapsulates architecture parameters including
the kernel size $\ki{i}$, stride $\si{i}$, pooling size $p$ and parameter
magnitude $\rwi{i}$. To our best knowledge, this is the first generalization
bound that only depends on $\mathcal{O}(\log(\prod_{i=1}^{l+1}\rwi{i}))$,
tighter than existing ones that all involve an exponential term like
$\mathcal{O}(\prod_{i=1}^{l+1}\rwi{i})$. Besides, we prove that for an
arbitrary gradient descent algorithm, the computed approximate stationary point
by minimizing empirical risk is also an approximate stationary point to the
population risk. This well explains why gradient descent training algorithms
usually perform sufficiently well in practice. Furthermore, we prove the
one-to-one correspondence and convergence guarantees for the non-degenerate
stationary points between the empirical and population risks. It implies that
the computed local minimum for the empirical risk is also close to a local
minimum for the population risk, thus ensuring the good generalization
performance of CNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10833">
<title>Bayesian Learning with Wasserstein Barycenters. (arXiv:1805.10833v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10833</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we introduce a novel paradigm for Bayesian learning based on
optimal transport theory. Namely, we propose to use the Wasserstein barycenter
of the posterior law on models, as an alternative to the maximum a posteriori
estimator (MAP) and Bayes predictive distributions. We exhibit conditions
granting the existence and consistency of this estimator, discuss some of its
basic and specific properties, and propose a numerical approximation relying on
standard posterior sampling in general finite-dimensional parameter spaces. We
thus also contribute to the recent blooming of applications of optimal
transport theory in machine learning, beyond the discrete and semidiscrete
settings so far considered. Advantages of the proposed estimator are discussed
and illustrated with numerical simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rios_G/0/1/0/all/0/1&quot;&gt;Gonzalo Rios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Backhoff_Veraguas_J/0/1/0/all/0/1&quot;&gt;Julio Backhoff-Veraguas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fontbona_J/0/1/0/all/0/1&quot;&gt;Joaquin Fontbona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tobar_F/0/1/0/all/0/1&quot;&gt;Felipe Tobar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10887">
<title>Block-optimized Variable Bit Rate Neural Image Compression. (arXiv:1805.10887v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10887</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose an end-to-end block-based auto-encoder system for
image compression. We introduce novel contributions to neural-network based
image compression, mainly in achieving binarization simulation, variable bit
rates with multiple networks, entropy-friendly representations, inference-stage
code optimization and performance-improving normalization layers in the
auto-encoder. We evaluate and show the incremental performance increase of each
of our contributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aytekin_C/0/1/0/all/0/1&quot;&gt;Caglar Aytekin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_X/0/1/0/all/0/1&quot;&gt;Xingyang Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cricri_F/0/1/0/all/0/1&quot;&gt;Francesco Cricri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lainema_J/0/1/0/all/0/1&quot;&gt;Jani Lainema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aksu_E/0/1/0/all/0/1&quot;&gt;Emre Aksu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hannuksela_M/0/1/0/all/0/1&quot;&gt;Miska Hannuksela&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10915">
<title>Dirichlet-based Gaussian Processes for Large-scale Calibrated Classification. (arXiv:1805.10915v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10915</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of deriving fast and accurate
classification algorithms with uncertainty quantification. Gaussian process
classification provides a principled approach, but the corresponding
computational burden is hardly sustainable in large-scale problems and devising
efficient alternatives is a challenge. In this work, we investigate if and how
Gaussian process regression directly applied to the classification labels can
be used to tackle this question. While in this case training time is remarkably
faster, predictions need be calibrated for classification and uncertainty
estimation. To this aim, we propose a novel approach based on interpreting the
labels as the output of a Dirichlet distribution. Extensive experimental
results show that the proposed approach provides essentially the same accuracy
and uncertainty quantification of Gaussian process classification while
requiring only a fraction of computational resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milios_D/0/1/0/all/0/1&quot;&gt;Dimitrios Milios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camoriano_R/0/1/0/all/0/1&quot;&gt;Raffaello Camoriano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michiardi_P/0/1/0/all/0/1&quot;&gt;Pietro Michiardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosasco_L/0/1/0/all/0/1&quot;&gt;Lorenzo Rosasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filippone_M/0/1/0/all/0/1&quot;&gt;Maurizio Filippone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10917">
<title>Deep Anomaly Detection Using Geometric Transformations. (arXiv:1805.10917v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10917</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of anomaly detection in images, and present a new
detection technique. Given a sample of images, all known to belong to a
&quot;normal&quot; class (e.g., dogs), we show how to train a deep neural model that can
detect out-of-distribution images (i.e., non-dog objects). The main idea behind
our scheme is to train a multi-class model to discriminate between dozens of
geometric transformations applied on all the given images. The auxiliary
expertise learned by the model generates feature detectors that effectively
identify, at test time, anomalous images based on the softmax activation
statistics of the model when applied on transformed images. We present
extensive experiments using the proposed detector, which indicate that our
algorithm improves state-of-the-art methods by a wide margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golan_I/0/1/0/all/0/1&quot;&gt;Izhak Golan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Yaniv_R/0/1/0/all/0/1&quot;&gt;Ran El-Yaniv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10927">
<title>Randomized Robust Matrix Completion for the Community Detection Problem. (arXiv:1805.10927v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1805.10927</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on the unsupervised clustering of large partially observed
graphs. We propose a provable randomized framework in which a clustering
algorithm is applied to a graphs adjacency matrix generated from a stochastic
block model. A sub-matrix is constructed using random sampling, and the low
rank component is found using a convex-optimization based matrix completion
algorithm. The clusters are then identified based on this low rank component
using a correlation based retrieval step. Additionally, a new random node
sampling algorithm is presented which significantly improves upon the
performance of the clustering algorithm with unbalanced data. Given a partially
observed graph with adjacency matrix A \in R^{N \times N} , the proposed
approach can reduce the computational complexity from O(N^2) to O(N).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmani_M/0/1/0/all/0/1&quot;&gt;Mostafa Rahmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karimian_A/0/1/0/all/0/1&quot;&gt;Adel Karimian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beckus_A/0/1/0/all/0/1&quot;&gt;Andre Beckus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atia_G/0/1/0/all/0/1&quot;&gt;George Atia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10939">
<title>Implicit ridge regularization provided by the minimum-norm least squares estimator when $n\ll p$. (arXiv:1805.10939v1 [math.ST])</title>
<link>http://arxiv.org/abs/1805.10939</link>
<description rdf:parseType="Literal">&lt;p&gt;A conventional wisdom in statistical learning is that large models require
strong regularization to prevent overfitting. This rule has been recently
challenged by deep neural networks: despite being expressive enough to fit any
training set perfectly, they still generalize well. Here we show that the same
is true for linear regression in the under-determined $n\ll p$ situation,
provided that one uses the minimum-norm estimator. The case of linear model
with least squares loss allows full and exact mathematical analysis. We prove
that augmenting a model with many random covariates with small constant
variance and using minimum-norm estimator is asymptotically equivalent to
adding the ridge penalty. Using toy example simulations as well as real-life
high-dimensional data sets, we demonstrate that explicit ridge penalty often
fails to provide any improvement over this implicit ridge regularization. In
this regime, minimum-norm estimator achieves zero training error but
nevertheless has low expected error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kobak_D/0/1/0/all/0/1&quot;&gt;Dmitry Kobak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lomond_J/0/1/0/all/0/1&quot;&gt;Jonathan Lomond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sanchez_B/0/1/0/all/0/1&quot;&gt;Benoit Sanchez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10940">
<title>Personalized Influence Estimation Technique. (arXiv:1805.10940v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10940</link>
<description rdf:parseType="Literal">&lt;p&gt;Customer Satisfaction is the most important factors in the industry
irrespective of domain. Key Driver Analysis is a common practice in data
science to help the business to evaluate the same. Understanding key features,
which influence the outcome or dependent feature, is highly important in
statistical model building. This helps to eliminate not so important factors
from the model to minimize noise coming from the features, which does not
contribute significantly enough to explain the behavior of the dependent
feature, which we want to predict. Personalized Influence Estimation is a
technique introduced in this paper, which can estimate key factor influence for
individual observations, which contribute most for each observations behavior
pattern based on the dependent class or estimate. Observations can come from
multiple business problem i.e. customers related to satisfaction study,
customer related to Fraud Detection, network devices for Fault detection etc.
It is highly important to understand the cause of issue at each observation
level to take appropriate Individualized action at customer level or device
level etc. This technique is based on joint behavior of the feature dimension
for the specific observation, and relative importance of the feature to
estimate impact. The technique mentioned in this paper is aimed to help
organizations to understand each respondents or observations individual key
contributing factor of Influence. Result of the experiment is really
encouraging and able to justify key reasons for churn for majority of the
sample appropriately
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pathak_K/0/1/0/all/0/1&quot;&gt;Kumarjit Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kapila_J/0/1/0/all/0/1&quot;&gt;Jitin Kapila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barvey_A/0/1/0/all/0/1&quot;&gt;Aasheesh Barvey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10958">
<title>Discrete flow posteriors for variational inference in discrete dynamical systems. (arXiv:1805.10958v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10958</link>
<description rdf:parseType="Literal">&lt;p&gt;Each training step for a variational autoencoder (VAE) requires us to sample
from the approximate posterior, so we usually choose simple (e.g. factorised)
approximate posteriors in which sampling is an efficient computation that fully
exploits GPU parallelism. However, such simple approximate posteriors are often
insufficient, as they eliminate statistical dependencies in the posterior.
While it is possible to use normalizing flow approximate posteriors for
continuous latents, some problems have discrete latents and strong statistical
dependencies. The most natural approach to model these dependencies is an
autoregressive distribution, but sampling from such distributions is inherently
sequential and thus slow. We develop a fast, parallel sampling procedure for
autoregressive distributions based on fixed-point iterations which enables
efficient and accurate variational inference in discrete state-space latent
variable dynamical systems. To optimize the variational bound, we considered
two ways to evaluate probabilities: inserting the relaxed samples directly into
the pmf for the discrete distribution, or converting to continuous logistic
latent variables and interpreting the K-step fixed-point iterations as a
normalizing flow. We found that converting to continuous latent variables gave
considerable additional scope for mismatch between the true and approximate
posteriors, which resulted in biased inferences, we thus used the former
approach. Using our fast sampling procedure, we were able to realize the
benefits of correlated posteriors, including accurate uncertainty estimates for
one cell, and accurate connectivity estimates for multiple cells, in an order
of magnitude less time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1&quot;&gt;Laurence Aitchison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Adam_V/0/1/0/all/0/1&quot;&gt;Vincent Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Turaga_S/0/1/0/all/0/1&quot;&gt;Srinivas C. Turaga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10965">
<title>Lipschitz regularity of deep neural networks: analysis and efficient estimation. (arXiv:1805.10965v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.10965</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are notorious for being sensitive to small well-chosen
perturbations, and estimating the regularity of such architectures is of utmost
importance for safe and robust practical applications. In this paper, we
investigate one of the key characteristics to assess the regularity of such
methods: the Lipschitz constant of deep learning architectures. First, we show
that, even for two layer neural networks, the exact computation of this
quantity is NP-hard and state-of-art methods may significantly overestimate it.
Then, we both extend and improve previous estimation methods by providing
AutoLip, the first generic algorithm for upper bounding the Lipschitz constant
of any automatically differentiable function. We provide a power method
algorithm working with automatic differentiation, allowing efficient
computations even on large convolutions. Second, for sequential neural
networks, we propose an improved algorithm named SeqLip that takes advantage of
the linear computation graph to split the computation per pair of consecutive
layers. Third we propose heuristics on SeqLip in order to tackle very large
networks. Our experiments show that SeqLip can significantly improve on the
existing upper bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scaman_K/0/1/0/all/0/1&quot;&gt;Kevin Scaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Virmaux_A/0/1/0/all/0/1&quot;&gt;Aladin Virmaux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10970">
<title>Predicting Electron Paths. (arXiv:1805.10970v1 [physics.chem-ph])</title>
<link>http://arxiv.org/abs/1805.10970</link>
<description rdf:parseType="Literal">&lt;p&gt;Chemical reactions can be described as the stepwise redistribution of
electrons in molecules. As such, reactions are often depicted using
&quot;arrow-pushing&quot; diagrams which show this movement as a sequence of arrows. We
propose an electron path prediction model (ELECTRO) to learn these sequences
directly from raw reaction data. Instead of predicting product molecules
directly from reactant molecules in one shot, learning a model of electron
movement has the benefits of (a) being easy for chemists to interpret, (b)
incorporating constraints of chemistry, such as balanced atom counts before and
after the reaction, and (c) naturally encoding the sparsity of chemical
reactions, which usually involve changes in only a small number of atoms in the
reactants. We design a method to extract approximate reaction paths from any
dataset of atom-mapped reaction SMILES strings. Our model achieves
state-of-the-art results on a subset of the UPSTO reaction dataset.
Furthermore, we show that our model recovers a basic knowledge of chemistry
without being explicitly trained to do so.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bradshaw_J/0/1/0/all/0/1&quot;&gt;John Bradshaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kusner_M/0/1/0/all/0/1&quot;&gt;Matt J. Kusner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Paige_B/0/1/0/all/0/1&quot;&gt;Brooks Paige&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Segler_M/0/1/0/all/0/1&quot;&gt;Marwin H. S. Segler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Miguel Hern&amp;#xe1;ndez-Lobato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10982">
<title>Sacrificing Accuracy for Reduced Computation: Cascaded Inference Based on Softmax Confidence. (arXiv:1805.10982v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10982</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the tradeoff between computational effort and accuracy in a cascade
of deep neural networks. During inference, early termination in the cascade is
controlled by confidence levels derived directly from the softmax outputs of
intermediate classifiers. The advantage of early termination is that
classification is performed using less computation, thus adjusting the
computational effort to the complexity of the input. Moreover, dynamic
modification of confidence thresholds allow one to trade accuracy for
computational effort without requiring retraining. Basing of early termination
on softmax classifier outputs is justified by experimentation that demonstrates
an almost linear relation between confidence levels in intermediate classifiers
and accuracy. Our experimentation with architectures based on ResNet obtained
the following results. (i) A speedup of 1.5 that sacrifices 1.4% accuracy with
respect to the CIFAR-10 test set. (ii) A speedup of 1.19 that sacrifices 0.7%
accuracy with respect to the CIFAR-100 test set. (iii) A speedup of 2.16 that
sacrifices 1.4% accuracy with respect to the SVHN test set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berestizshevsky_K/0/1/0/all/0/1&quot;&gt;Konstantin Berestizshevsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Even_G/0/1/0/all/0/1&quot;&gt;Guy Even&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10988">
<title>Deeply learning molecular structure-property relationships using graph attention neural network. (arXiv:1805.10988v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.10988</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecular structure-property relationships are the key to molecular
engineering for materials and drug discovery. The rise of deep learning offers
a new viable solution to elucidate the structure-property relationships
directly from chemical data. Here we show that graph attention networks can
greatly improve performance of the deep learning for chemistry. The attention
mechanism enables to distinguish atoms in different environments and thus to
extract important structural features determining target properties. We
demonstrated that our model can detect appropriate features for molecular
polarity, solubility, and energy. Interestingly, it identified two distinct
parts of molecules as essential structural features for high photovoltaic
efficiency, each of which coincided with the area of donor and acceptor
orbitals in charge-transfer excitations, respectively. As a result, it could
accurately predict molecular properties. Moreover, the resultant latent space
was well-organized such that molecules with similar properties were closely
located, which is critical for successful molecular engineering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_S/0/1/0/all/0/1&quot;&gt;Seongok Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1&quot;&gt;Jaechang Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Woo Youn Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11022">
<title>Online Influence Maximization with Local Observations. (arXiv:1805.11022v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.11022</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider an online influence maximization problem in which a decision
maker selects a node among a large number of possibilities and places a piece
of information at the node. The node transmits the information to some others
that are in the same connected component in a random graph. The goal of the
decision maker is to reach as many nodes as possible, with the added
complication that feedback is only available about the degree of the selected
node. Our main result shows that such local observations can be sufficient for
maximizing global influence in two broadly studied families of random graph
models: stochastic block models and Chung--Lu models. With this insight, we
propose a bandit algorithm that aims at maximizing local (and thus global)
influence, and provide its theoretical analysis in both the subcritical and
supercritical regimes of both considered models. Notably, our performance
guarantees show no explicit dependence on the total number of nodes in the
network, making our approach well-suited for large-scale applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olkhovskaya_J/0/1/0/all/0/1&quot;&gt;Julia Olkhovskaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neu_G/0/1/0/all/0/1&quot;&gt;Gergely Neu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lugosi_G/0/1/0/all/0/1&quot;&gt;G&amp;#xe1;bor Lugosi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11028">
<title>Autoencoding any Data through Kernel Autoencoders. (arXiv:1805.11028v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11028</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates a novel algorithmic approach to data representation
based on kernel methods. Assuming the observations lie in a Hilbert space X,
this work introduces a new formulation of Representation Learning, stated as a
regularized empirical risk minimization problem over a class of composite
functions. These functions are obtained by composing elementary mappings from
vector-valued Reproducing Kernel Hilbert Spaces (vv-RKHSs), and the risk is
measured by the expected distortion rate in the input space X. The proposed
algorithms crucially rely on the form taken by the minimizers, revealed by a
dedicated Representer Theorem. Beyond a first extension of the autoencoding
scheme to possibly infinite dimensional Hilbert spaces, an important
application of the introduced Kernel Autoencoders (KAEs) arises when X is
assumed itself to be a RKHS: this makes it possible to extract finite
dimensional representations from any kind of data. Numerical experiments on
simulated data as well as real labeled graphs (molecules) provide empirical
evidences of the performance attained by KAEs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laforgue_P/0/1/0/all/0/1&quot;&gt;Pierre Laforgue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Clemencon_S/0/1/0/all/0/1&quot;&gt;Stephan Clemencon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+dAlche_Buc_F/0/1/0/all/0/1&quot;&gt;Florence d&amp;#x27;Alche-Buc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11048">
<title>Scalable Spectral Clustering Using Random Binning Features. (arXiv:1805.11048v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.11048</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral clustering is one of the most effective clustering approaches that
capture hidden cluster structures in the data. However, it does not scale well
to large-scale problems due to its quadratic complexity in constructing
similarity graphs and computing subsequent eigendecomposition. Although a
number of methods have been proposed to accelerate spectral clustering, most of
them compromise considerable information loss in the original data for reducing
computational bottlenecks. In this paper, we present a novel scalable spectral
clustering method using Random Binning features (RB) to simultaneously
accelerate both similarity graph construction and the eigendecomposition.
Specifically, we implicitly approximate the graph similarity (kernel) matrix by
the inner product of a large sparse feature matrix generated by RB. Then we
introduce a state-of-the-art SVD solver to effectively compute eigenvectors of
this large matrix for spectral clustering. Using these two building blocks, we
reduce the computational cost from quadratic to linear in the number of data
points while achieving similar accuracy. Our theoretical analysis shows that
spectral clustering via RB converges faster to the exact spectral clustering
than the standard Random Feature approximation. Extensive experiments on 8
benchmarks show that the proposed method either outperforms or matches the
state-of-the-art methods in both accuracy and runtime. Moreover, our method
exhibits linear scalability in both the number of data samples and the number
of RB features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lingfei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yen_I/0/1/0/all/0/1&quot;&gt;Ian En-Hsu Yen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1&quot;&gt;Fangli Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yinglong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1&quot;&gt;Charu Aggarwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11051">
<title>Flexible and accurate inference and learning for deep generative models. (arXiv:1805.11051v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.11051</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new approach to learning in hierarchical latent-variable
generative models called the &quot;distributed distributional code Helmholtz
machine&quot;, which emphasises flexibility and accuracy in the inferential process.
In common with the original Helmholtz machine and later variational autoencoder
algorithms (but unlike adverserial methods) our approach learns an explicit
inference or &quot;recognition&quot; model to approximate the posterior distribution over
the latent variables. Unlike in these earlier methods, the posterior
representation is not limited to a narrow tractable parameterised form (nor is
it represented by samples). To train the generative and recognition models we
develop an extended wake-sleep algorithm inspired by the original Helmholtz
Machine. This makes it possible to learn hierarchical latent models with both
discrete and continuous variables, where an accurate posterior representation
is essential. We demonstrate that the new algorithm outperforms current
state-of-the-art methods on synthetic, natural image patch and the MNIST data
sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vertes_E/0/1/0/all/0/1&quot;&gt;Eszter Vertes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sahani_M/0/1/0/all/0/1&quot;&gt;Maneesh Sahani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11073">
<title>Non-bifurcating phylogenetic tree inference via the adaptive LASSO. (arXiv:1805.11073v1 [q-bio.PE])</title>
<link>http://arxiv.org/abs/1805.11073</link>
<description rdf:parseType="Literal">&lt;p&gt;Phylogenetic tree inference using deep DNA sequencing is reshaping our
understanding of rapidly evolving systems, such as the within-host battle
between viruses and the immune system. Densely sampled phylogenetic trees can
contain special features, including &quot;sampled ancestors&quot; in which we sequence a
genotype along with its direct descendants, and &quot;polytomies&quot; in which multiple
descendants arise simultaneously. These features are apparent after identifying
zero-length branches in the tree. However, current maximum-likelihood based
approaches are not capable of revealing such zero-length branches. In this
paper, we find these zero-length branches by introducing adaptive-LASSO-type
regularization estimators to phylogenetics, deriving their properties, and
showing regularization to be a practically useful approach for phylogenetics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dinh_V/0/1/0/all/0/1&quot;&gt;Vu Dinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Matsen_F/0/1/0/all/0/1&quot;&gt;Frederick A. Matsen IV&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11085">
<title>More Than a Feeling: Learning to Grasp and Regrasp using Vision and Touch. (arXiv:1805.11085v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1805.11085</link>
<description rdf:parseType="Literal">&lt;p&gt;For humans, the process of grasping an object relies heavily on rich tactile
feedback. Recent robotic grasping work, however, has been largely based only on
visual input, and thus cannot easily benefit from feedback after initiating
contact. In this paper, we investigate if a robot can learn to use tactile
information to iteratively and efficiently adjust its grasp. To this end, we
propose an end-to-end action-conditional model that learns greedy regrasping
policies from raw visuo-tactile data. This model - a deep, multimodal
convolutional network - predicts the outcome of a candidate grasp adjustment,
and then executes a grasp by iteratively selecting the most promising actions.
Our approach requires neither calibration of the tactile sensors, nor any
analytical modeling of contact forces, thus reducing the engineering effort
required to obtain efficient grasping policies. We train our model with data
from over 6,450 grasping trials on a two-finger gripper equipped with GelSight
high-resolution tactile sensors on each finger. Across extensive experiments,
our approach outperforms a variety of baselines at (i) estimating grasp
adjustment outcomes, (ii) selecting efficient grasp adjustments for quick
grasping, and (iii) reducing the amount of force applied at the fingers, while
maintaining competitive performance. Finally, we study the choices made by our
model and show that it has successfully acquired useful and interpretable
grasping behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1&quot;&gt;Roberto Calandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owens_A/0/1/0/all/0/1&quot;&gt;Andrew Owens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1&quot;&gt;Dinesh Jayaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Justin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wenzhen Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adelson_E/0/1/0/all/0/1&quot;&gt;Edward H. Adelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1601.05285">
<title>Nonlinear variable selection with continuous outcome: a nonparametric incremental forward stagewise approach. (arXiv:1601.05285v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1601.05285</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method of variable selection for the sparse generalized additive
model. The method doesn&apos;t assume any specific functional form, and can select
from a large number of candidates. It takes the form of incremental forward
stagewise regression. Given no functional form is assumed, we devised an
approach termed roughening to adjust the residuals in the iterations. In
simulations, we show the new method is competitive against popular machine
learning approaches. We also demonstrate its performance using some real
datasets. The method is available as a part of the nlnet package on CRAN
https://cran.r-project.org/package=nlnet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tianwei Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.02869">
<title>Network Modeling of Short Over-Dispersed Spike-Counts: A Hierarchical Parametric Empirical Bayes Framework. (arXiv:1605.02869v3 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/1605.02869</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate statistical models of neural spike responses can characterize the
information carried by neural populations. Yet, challenges in recording at the
level of individual neurons commonly results in relatively limited samples of
spike counts, which can lead to model overfitting. Moreover, current models
assume spike counts to be Poisson-distributed, which ignores the fact that many
neurons demonstrate over-dispersed spiking behavior. The Negative Binomial
Generalized Linear Model (NB-GLM) provides a powerful tool for modeling
over-dispersed spike counts. However, maximum likelihood based standard NB-GLM
leads to unstable and inaccurate parameter estimations. Thus, we propose a
hierarchical parametric empirical Bayes method for estimating the parameters of
the NB-GLM. Our method integrates Generalized Linear Models (GLMs) and
empirical Bayes theory to: (1) effectively capture over-dispersion nature of
spike counts from retinal ganglion neural responses; (2) significantly reduce
mean square error of parameter estimations when compared to maximum likelihood
based method for NB-GLMs; (3) provide an efficient alternative to fully
Bayesian inference with low computational cost for hierarchical models; and (4)
give insightful findings on both neural interactions and spiking behaviors of
real retina cells. We apply our approach to study both simulated data and
experimental neural data from the retina. The simulation results indicate the
new framework can efficiently and accurately retrieve the weights of functional
connections among neural populations and predict mean spike counts. The results
from the retinal datasets demonstrate the proposed method outperforms both
standard Poisson and Negative Binomial GLMs in terms of the predictive
log-likelihood of held-out data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+She_Q/0/1/0/all/0/1&quot;&gt;Qi She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jelfs_B/0/1/0/all/0/1&quot;&gt;Beth Jelfs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Charles_A/0/1/0/all/0/1&quot;&gt;Adam S. Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chan_R/0/1/0/all/0/1&quot;&gt;Rosa H.M.Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.02122">
<title>Significance testing in non-sparse high-dimensional linear models. (arXiv:1610.02122v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1610.02122</link>
<description rdf:parseType="Literal">&lt;p&gt;In high-dimensional linear models, the sparsity assumption is typically made,
stating that most of the parameters are equal to zero. Under the sparsity
assumption, estimation and, recently, inference have been well studied.
However, in practice, sparsity assumption is not checkable and more importantly
is often violated; a large number of covariates might be expected to be
associated with the response, indicating that possibly all, rather than just a
few, parameters are non-zero. A natural example is a genome-wide gene
expression profiling, where all genes are believed to affect a common disease
marker. We show that existing inferential methods are sensitive to the sparsity
assumption, and may, in turn, result in the severe lack of control of Type-I
error. In this article, we propose a new inferential method, named CorrT, which
is robust to model misspecification such as heteroscedasticity and lack of
sparsity. CorrT is shown to have Type I error approaching the nominal level for
\textit{any} models and Type II error approaching zero for sparse and many
dense models.
&lt;/p&gt;
&lt;p&gt;In fact, CorrT is also shown to be optimal in a variety of frameworks:
sparse, non-sparse and hybrid models where sparse and dense signals are mixed.
Numerical experiments show a favorable performance of the CorrT test compared
to the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinchu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bradic_J/0/1/0/all/0/1&quot;&gt;Jelena Bradic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.08948">
<title>Provable Dynamic Robust PCA or Robust Subspace Tracking. (arXiv:1705.08948v3 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1705.08948</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic robust PCA refers to the dynamic (time-varying) extension of robust
PCA (RPCA). It assumes that the true (uncorrupted) data lies in a
low-dimensional subspace that can change with time, albeit slowly. The goal is
to track this changing subspace over time in the presence of sparse outliers.
We develop and study a novel algorithm, that we call simple-ReProCS, based on
the recently introduced Recursive Projected Compressive Sensing (ReProCS)
framework. Our work provides the first guarantee for dynamic RPCA that holds
under weakened versions of standard RPCA assumptions, slow subspace change and
a lower bound assumption on most outlier magnitudes. Our result is significant
because (i) it removes the strong assumptions needed by the two previous
complete guarantees for ReProCS-based algorithms; (ii) it shows that it is
possible to achieve significantly improved outlier tolerance, compared with all
existing RPCA or dynamic RPCA solutions by exploiting the above two simple
extra assumptions; and (iii) it proves that simple-ReProCS is online (after
initialization), fast, and, has near-optimal memory complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanamurthy_P/0/1/0/all/0/1&quot;&gt;Praneeth Narayanamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaswani_N/0/1/0/all/0/1&quot;&gt;Namrata Vaswani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.05778">
<title>A successive difference-of-convex approximation method for a class of nonconvex nonsmooth optimization problems. (arXiv:1710.05778v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1710.05778</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a class of nonconvex nonsmooth optimization problems whose
objective is the sum of a smooth function and a finite number of nonnegative
proper closed possibly nonsmooth functions (whose proximal mappings are easy to
compute), some of which are further composed with linear maps. This kind of
problems arises naturally in various applications when different regularizers
are introduced for inducing simultaneous structures in the solutions. Solving
these problems, however, can be challenging because of the coupled nonsmooth
functions: the corresponding proximal mapping can be hard to compute so that
standard first-order methods such as the proximal gradient algorithm cannot be
applied efficiently. In this paper, we propose a successive
difference-of-convex approximation method for solving this kind of problems. In
this algorithm, we approximate the nonsmooth functions by their Moreau
envelopes in each iteration. Making use of the simple observation that Moreau
envelopes of nonnegative proper closed functions are continuous {\em
difference-of-convex} functions, we can then approximately minimize the
approximation function by first-order methods with suitable majorization
techniques. These first-order methods can be implemented efficiently thanks to
the fact that the proximal mapping of {\em each} nonsmooth function is easy to
compute. Under suitable assumptions, we prove that the sequence generated by
our method is bounded and any accumulation point is a stationary point of the
objective. We also discuss how our method can be applied to concrete
applications such as nonconvex fused regularized optimization problems and
simultaneously structured matrix optimization problems, and illustrate the
performance numerically for these two specific applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianxiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pong_T/0/1/0/all/0/1&quot;&gt;Ting Kei Pong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Takeda_A/0/1/0/all/0/1&quot;&gt;Akiko Takeda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06114">
<title>Robust Unsupervised Domain Adaptation for Neural Networks via Moment Alignment. (arXiv:1711.06114v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06114</link>
<description rdf:parseType="Literal">&lt;p&gt;A novel approach for unsupervised domain adaptation for neural networks is
proposed that relies on metric-based regularization of the learning process.
The metric-based regularization aims at domain-invariant latent feature
representations by means of maximizing the similarity between domain-specific
activation distributions. The proposed metric results from modifying an
integral probability metric such that it becomes translation-invariant on a
polynomial function space. The metric has an intuitive interpretation in the
dual space as the sum of differences of higher order central moments of the
corresponding activation distributions. Error minimization guarantees are
proven for the continuous case. As demonstrated by an analysis of standard
benchmark experiments for sentiment analysis, object recognition and digit
recognition, the outlined approach is robust regarding parameter changes and
achieves higher classification accuracies than comparable approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zellinger_W/0/1/0/all/0/1&quot;&gt;Werner Zellinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moser_B/0/1/0/all/0/1&quot;&gt;Bernhard A. Moser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grubinger_T/0/1/0/all/0/1&quot;&gt;Thomas Grubinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lughofer_E/0/1/0/all/0/1&quot;&gt;Edwin Lughofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Natschlager_T/0/1/0/all/0/1&quot;&gt;Thomas Natschl&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saminger_Platz_S/0/1/0/all/0/1&quot;&gt;Susanne Saminger-Platz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11511">
<title>Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for Bayesian learning. (arXiv:1711.11511v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11511</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel sampling method, the thermostat-assisted
continuously-tempered Hamiltonian Monte Carlo, for the purpose of multimodal
Bayesian learning. It simulates a noisy dynamical system by incorporating both
a continuously-varying tempering variable and the Nos\&apos;e-Hoover thermostats. A
significant benefit is that it is not only able to efficiently generate i.i.d.
samples when the underlying posterior distributions are multimodal, but also
capable of adaptively neutralising the noise arising from the use of
mini-batches. While the properties of the approach have been studied using
synthetic datasets, our experiments on three real datasets have also shown its
performance gains over several strong baselines for Bayesian learning with
various types of neural networks plunged in.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Rui Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhanxing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03558">
<title>Inference Suboptimality in Variational Autoencoders. (arXiv:1801.03558v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03558</link>
<description rdf:parseType="Literal">&lt;p&gt;Amortized inference allows latent-variable models trained via variational
learning to scale to large datasets. The quality of approximate inference is
determined by two factors: a) the capacity of the variational distribution to
match the true posterior and b) the ability of the recognition network to
produce good variational parameters for each datapoint. We examine approximate
inference in variational autoencoders in terms of these factors. We find that
divergence from the true posterior is often due to imperfect recognition
networks, rather than the limited complexity of the approximating distribution.
We show that this is due partly to the generator learning to accommodate the
choice of approximation. Furthermore, we show that the parameters used to
increase the expressiveness of the approximation play a role in generalizing
inference rather than simply improving the complexity of the approximation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremer_C/0/1/0/all/0/1&quot;&gt;Chris Cremer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuechen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1&quot;&gt;David Duvenaud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03832">
<title>Quadrature-based features for kernel approximation. (arXiv:1802.03832v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03832</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of improving kernel approximation via randomized
feature maps. These maps arise as Monte Carlo approximation to integral
representations of kernel functions and scale up kernel methods for larger
datasets. Based on an efficient numerical integration technique, we propose a
unifying approach that reinterprets the previous random features methods and
extends to better estimates of the kernel approximation. We derive the
convergence behaviour and conduct an extensive empirical study that supports
our hypothesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munkhoeva_M/0/1/0/all/0/1&quot;&gt;Marina Munkhoeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapushev_Y/0/1/0/all/0/1&quot;&gt;Yermek Kapushev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1&quot;&gt;Evgeny Burnaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1&quot;&gt;Ivan Oseledets&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04920">
<title>DVAE++: Discrete Variational Autoencoders with Overlapping Transformations. (arXiv:1802.04920v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04920</link>
<description rdf:parseType="Literal">&lt;p&gt;Training of discrete latent variable models remains challenging because
passing gradient information through discrete units is difficult. We propose a
new class of smoothing transformations based on a mixture of two overlapping
distributions, and show that the proposed transformation can be used for
training binary latent models with either directed or undirected priors. We
derive a new variational bound to efficiently train with Boltzmann machine
priors. Using this bound, we develop DVAE++, a generative model with a global
discrete prior and a hierarchy of convolutional continuous variables.
Experiments on several benchmarks show that overlapping transformations
outperform other recent continuous relaxations of discrete latent variables
including Gumbel-Softmax (Maddison et al., 2016; Jang et al., 2016), and
discrete variational autoencoders (Rolfe 2016).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1&quot;&gt;Arash Vahdat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macready_W/0/1/0/all/0/1&quot;&gt;William G. Macready&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1&quot;&gt;Zhengbing Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoshaman_A/0/1/0/all/0/1&quot;&gt;Amir Khoshaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andriyash_E/0/1/0/all/0/1&quot;&gt;Evgeny Andriyash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05315">
<title>Online Learning for Non-Stationary A/B Tests. (arXiv:1802.05315v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05315</link>
<description rdf:parseType="Literal">&lt;p&gt;The rollout of new versions of a feature in modern applications is a manual
multi-stage process, as the feature is released to ever larger groups of users,
while its performance is carefully monitored. This kind of A/B testing is
ubiquitous, but suboptimal, as the monitoring requires heavy human
intervention, is not guaranteed to capture consistent, but short-term
fluctuations in performance, and is inefficient, as better versions take a long
time to reach the full population.
&lt;/p&gt;
&lt;p&gt;In this work we formulate this question as that of expert learning, and give
a new algorithm Follow-The-Best-Interval, FTBI, that works in dynamic,
non-stationary environments. Our approach is practical, simple, and efficient,
and has rigorous guarantees on its performance. Finally, we perform a thorough
evaluation on synthetic and real world datasets and show that our approach
outperforms current state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Medina_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Mu&amp;#xf1;oz Medina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vassilvitskii_S/0/1/0/all/0/1&quot;&gt;Sergei Vassilvitskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dong Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07581">
<title>Universal Hypothesis Testing with Kernels: Asymptotically Optimal Tests for Goodness of Fit. (arXiv:1802.07581v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07581</link>
<description rdf:parseType="Literal">&lt;p&gt;We characterize the asymptotic performance of nonparametric goodness of fit
testing, otherwise known as universal hypothesis testing in information theory
and statistics. The exponential decay rate of the type-II error probability is
used as the asymptotic performance metric, and an optimal test achieves the
maximum rate subject to a constant level constraint on the type-I error
probability. We show that two classes of Maximum Mean Discrepancy (MMD) based
tests attain this optimality on $\mathbb R^d$, while the quadratic-time Kernel
Stein Discrepancy (KSD) based tests achieve the same exponential decay rate
under an asymptotic level constraint. With bootstrap thresholds, these kernel
based tests have similar statistical performance in our experiments of finite
samples. Key to our approach are Sanov&apos;s theorem~in large deviation theory and
the weak convergence properties of the MMD and KSD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shengyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Biao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_P/0/1/0/all/0/1&quot;&gt;Pengfei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhitang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00651">
<title>Static and Dynamic Robust PCA and Matrix Completion: A Review. (arXiv:1803.00651v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00651</link>
<description rdf:parseType="Literal">&lt;p&gt;Principal Components Analysis (PCA) is one of the most widely used dimension
reduction techniques. Robust PCA (RPCA) refers to the problem of PCA when the
data may be corrupted by outliers. Recent work by Cand{\`e}s, Wright, Li, and
Ma defined RPCA as a problem of decomposing a given data matrix into the sum of
a low-rank matrix (true data) and a sparse matrix (outliers). The column space
of the low-rank matrix then gives the PCA solution. This simple definition has
lead to a large amount of interesting new work on provably correct, fast, and
practical solutions to RPCA. More recently, the dynamic (time-varying) version
of the RPCA problem has been studied and a series of provably correct, fast,
and memory efficient tracking solutions have been proposed. Dynamic RPCA (or
robust subspace tracking) is the problem of tracking data lying in a (slowly)
changing subspace while being robust to sparse outliers. This article provides
an exhaustive review of the last decade of literature on RPCA and its dynamic
counterpart (robust subspace tracking), along with describing their theoretical
guarantees, discussing the pros and cons of various approaches, and providing
empirical comparisons of performance and speed.
&lt;/p&gt;
&lt;p&gt;A brief overview of the (low-rank) matrix completion literature is also
provided (the focus is on works not discussed in other recent reviews). This
refers to the problem of completing a low-rank matrix when only a subset of its
entries are observed. It can be interpreted as a simpler special case of RPCA
in which the indices of the outlier corrupted entries are known.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaswani_N/0/1/0/all/0/1&quot;&gt;Namrata Vaswani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanamurthy_P/0/1/0/all/0/1&quot;&gt;Praneeth Narayanamurthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01370">
<title>A Distributed Quasi-Newton Algorithm for Empirical Risk Minimization with Nonsmooth Regularization. (arXiv:1803.01370v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01370</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a communication- and computation-efficient distributed
optimization algorithm using second-order information for solving ERM problems
with a nonsmooth regularization term. Current second-order and quasi-Newton
methods for this problem either do not work well in the distributed setting or
work only for specific regularizers. Our algorithm uses successive quadratic
approximations, and we describe how to maintain an approximation of the Hessian
and solve subproblems efficiently in a distributed manner. The proposed method
enjoys global linear convergence for a broad range of non-strongly convex
problems that includes the most commonly used ERMs, thus requiring lower
communication complexity. It also converges on non-convex problems, so has the
potential to be used on applications such as deep learning. Initial
computational results on convex problems demonstrate that our method
significantly improves on communication cost and running time over the current
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Ching-pei Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lim_C/0/1/0/all/0/1&quot;&gt;Cong Han Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wright_S/0/1/0/all/0/1&quot;&gt;Stephen J. Wright&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10342">
<title>Classification of crystallization outcomes using deep convolutional neural networks. (arXiv:1803.10342v2 [q-bio.BM] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10342</link>
<description rdf:parseType="Literal">&lt;p&gt;The Machine Recognition of Crystallization Outcomes (MARCO) initiative has
assembled roughly half a million annotated images of macromolecular
crystallization experiments from various sources and setups. Here,
state-of-the-art machine learning algorithms are trained and tested on
different parts of this data set. We find that more than 94% of the test images
can be correctly labeled, irrespective of their experimental origin. Because
crystal recognition is key to high-density screening and the systematic
analysis of crystallization experiments, this approach opens the door to both
industrial and fundamental research applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bruno_A/0/1/0/all/0/1&quot;&gt;Andrew E. Bruno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Charbonneau_P/0/1/0/all/0/1&quot;&gt;Patrick Charbonneau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Newman_J/0/1/0/all/0/1&quot;&gt;Janet Newman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Snell_E/0/1/0/all/0/1&quot;&gt;Edward H. Snell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+So_D/0/1/0/all/0/1&quot;&gt;David R. So&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Vanhoucke_V/0/1/0/all/0/1&quot;&gt;Vincent Vanhoucke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Watkins_C/0/1/0/all/0/1&quot;&gt;Christopher J. Watkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Williams_S/0/1/0/all/0/1&quot;&gt;Shawn Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wilson_J/0/1/0/all/0/1&quot;&gt;Julie Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01526">
<title>Training DNNs with Hybrid Block Floating Point. (arXiv:1804.01526v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01526</link>
<description rdf:parseType="Literal">&lt;p&gt;The wide adoption of DNNs has given birth to unrelenting computing
requirements, forcing datacenter operators to adopt domain-specific
accelerators to train them. These accelerators typically employ densely packed
full precision floating-point arithmetic to maximize performance per area.
Ongoing research efforts seek to further increase that performance density by
replacing floating-point with fixed-point arithmetic. However, a significant
roadblock for these attempts has been fixed point&apos;s narrow dynamic range, which
is insufficient for DNN training convergence. We identify block floating
point~(BFP) as a promising alternative representation since it exhibits wide
dynamic range and enables the majority of DNN operations to be performed with
fixed-point logic. Unfortunately, BFP alone introduces several limitations that
preclude its direct applicability. In this work, we introduce HBFP, a hybrid
BFP-FP approach, which performs all dot products in BFP and other operations in
floating point. HBFP delivers the best of both worlds: the high accuracy of
floating point at the superior hardware density of fixed point. For a wide
variety of models, we show that HBFP matches floating point&apos;s accuracy while
enabling hardware implementations that deliver up to 8.5x higher throughput.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drumond_M/0/1/0/all/0/1&quot;&gt;Mario Drumond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falsafi_B/0/1/0/all/0/1&quot;&gt;Babak Falsafi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02744">
<title>Unsupervised Learning of Mixture Models with a Uniform Background Component. (arXiv:1804.02744v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02744</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian Mixture Models are one of the most studied and mature models in
unsupervised learning. However, outliers are often present in the data and
could influence the cluster estimation. In this paper, we study a new model
that assumes that data comes from a mixture of a number of Gaussians as well as
a uniform &quot;background&quot; component assumed to contain outliers and other
non-interesting observations. We develop a novel method based on robust loss
minimization that performs well in clustering such GMM with a uniform
background. We give theoretical guarantees for our clustering algorithm to
obtain best clustering results with high probability. Besides, we show that the
result of our algorithm does not depend on initialization or local optima, and
the parameter tuning is an easy task. By numeric simulations, we demonstrate
that our algorithm enjoys high accuracy and achieves the best clustering
results given a large enough sample size. Finally, experimental comparisons
with typical clustering methods on real datasets witness the potential of our
algorithm in real applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sida Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barbu_A/0/1/0/all/0/1&quot;&gt;Adrian Barbu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03836">
<title>E-commerce Anomaly Detection: A Bayesian Semi-Supervised Tensor Decomposition Approach using Natural Gradients. (arXiv:1804.03836v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.03836</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly Detection has several important applications. In this paper, our
focus is on detecting anomalies in seller-reviewer data using tensor
decomposition. While tensor-decomposition is mostly unsupervised, we formulate
Bayesian semi-supervised tensor decomposition to take advantage of sparse
labeled data. In addition, we use Polya-Gamma data augmentation for the
semi-supervised Bayesian tensor decomposition. Finally, we show that the
P\&apos;olya-Gamma formulation simplifies calculation of the Fisher information
matrix for partial natural gradient learning. Our experimental results show
that our semi-supervised approach outperforms state of the art unsupervised
baselines. And that the partial natural gradient learning outperforms
stochastic gradient learning and Online-EM with sufficient statistics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yelundur_A/0/1/0/all/0/1&quot;&gt;Anil R. Yelundur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengamedu_S/0/1/0/all/0/1&quot;&gt;Srinivasan H. Sengamedu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bamdev Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06218">
<title>Hierarchical correlation reconstruction with missing data, for example for biology-inspired neuron. (arXiv:1804.06218v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06218</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning often needs to model density from a multidimensional data
sample, including correlations between coordinates. Additionally, we often have
missing data case: that data points can miss values for some of coordinates.
This article adapts rapid parametric density estimation approach for this
purpose: modelling density as a linear combination of orthonormal functions,
for which $L^2$ optimization says that (independently) estimated coefficient
for a given function is just average over the sample of value of this function.
Hierarchical correlation reconstruction first models probability density for
each separate coordinate using all its appearances in data sample, then adds
corrections from independently modelled pairwise correlations using all samples
having both coordinates, and so on independently adding correlations for
growing numbers of variables using often decreasing evidence in data sample. A
basic application of such modelled multidimensional density can be imputation
of missing coordinates: by inserting known coordinates to the density, and
taking expected values for the missing coordinates, or even their entire joint
probability distribution. Presented method can be compared with cascade
correlations approach, offering several advantages in flexibility and accuracy.
It can be also used as artificial neuron: maximizing prediction capabilities
for only local behavior - modelling and predicting local connections.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duda_J/0/1/0/all/0/1&quot;&gt;Jarek Duda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06481">
<title>Unlearn What You Have Learned: Adaptive Crowd Teaching with Exponentially Decayed Memory Learners. (arXiv:1804.06481v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06481</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing demand for large amount of labeled data, crowdsourcing
has been used in many large-scale data mining applications. However, most
existing works in crowdsourcing mainly focus on label inference and incentive
design. In this paper, we address a different problem of adaptive crowd
teaching, which is a sub-area of machine teaching in the context of
crowdsourcing. Compared with machines, human beings are extremely good at
learning a specific target concept (e.g., classifying the images into given
categories) and they can also easily transfer the learned concepts into similar
learning tasks. Therefore, a more effective way of utilizing crowdsourcing is
by supervising the crowd to label in the form of teaching. In order to perform
the teaching and expertise estimation simultaneously, we propose an adaptive
teaching framework named JEDI to construct the personalized optimal teaching
set for the crowdsourcing workers. In JEDI teaching, the teacher assumes that
each learner has an exponentially decayed memory. Furthermore, it ensures
comprehensiveness in the learning process by carefully balancing teaching
diversity and learner&apos;s accurate learning in terms of teaching usefulness.
Finally, we validate the effectiveness and efficacy of JEDI teaching in
comparison with the state-of-the-art techniques on multiple data sets with both
synthetic learners and real crowdsourcing workers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nelakurthi_A/0/1/0/all/0/1&quot;&gt;Arun Reddy Nelakurthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jingrui He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04874">
<title>GAN Q-learning. (arXiv:1805.04874v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.04874</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributional reinforcement learning (distributional RL) has seen empirical
success in complex Markov Decision Processes (MDPs) in the setting of nonlinear
function approximation. However there are many different ways in which one can
leverage the distributional approach to reinforcement learning. In this paper,
we propose GAN Q-learning, a novel distributional RL method based on generative
adversarial networks (GANs) and analyze its performance in simple tabular
environments, as well as OpenAI Gym. We empirically show that our algorithm
leverages the flexibility and blackbox approach of deep learning models while
providing a viable alternative to traditional methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Doan_T/0/1/0/all/0/1&quot;&gt;Thang Doan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mazoure_B/0/1/0/all/0/1&quot;&gt;Bogdan Mazoure&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lyle_C/0/1/0/all/0/1&quot;&gt;Clare Lyle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05857">
<title>On the glassy nature of the hard phase in inference problems. (arXiv:1805.05857v3 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05857</link>
<description rdf:parseType="Literal">&lt;p&gt;An algorithmically hard phase was described in a range of inference problems:
even if the signal can be reconstructed with a small error from an information
theoretic point of view, known algorithms fail unless the noise-to-signal ratio
is sufficiently small. This hard phase is typically understood as a metastable
branch of the dynamical evolution of message passing algorithms. In this work
we study the metastable branch for a prototypical inference problem, the
low-rank matrix factorization, that presents a hard phase. We show that for
noise-to-signal ratios that are below the information theoretic threshold, the
posterior measure is composed of an exponential number of metastable glassy
states and we compute their entropy, called the complexity. We show that this
glassiness extends even slightly below the algorithmic threshold below which
the well-known approximate message passing (AMP) algorithm is able to closely
reconstruct the signal. Counter-intuitively, we find that the performance of
the AMP algorithm is not improved by taking into account the glassy nature of
the hard phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Antenucci_F/0/1/0/all/0/1&quot;&gt;Fabrizio Antenucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Franz_S/0/1/0/all/0/1&quot;&gt;Silvio Franz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Urbani_P/0/1/0/all/0/1&quot;&gt;Pierfrancesco Urbani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zdeborova_L/0/1/0/all/0/1&quot;&gt;Lenka Zdeborov&amp;#xe1;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08206">
<title>Boosting Uncertainty Estimation for Deep Neural Classifiers. (arXiv:1805.08206v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08206</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of uncertainty estimation in the context of
(non-Bayesian) deep neural classification. All current methods are based on
extracting uncertainty signals from a trained network optimized to solve the
classification problem at hand. We demonstrate that such techniques tend to
misestimate instances whose predictions are supposed to be highly confident.
This deficiency is an artifact of the training process with SGD-like
optimizers. Based on this observation, we develop an uncertainty estimation
algorithm that &quot;peels away&quot; highly confident points sequentially and estimates
their confidence using earlier snapshots of the trained model, before their
uncertainty estimates are jittered. We present extensive experiments indicating
that the proposed algorithm provides uncertainty estimates that are
consistently better than the best known methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geifman_Y/0/1/0/all/0/1&quot;&gt;Yonatan Geifman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uziel_G/0/1/0/all/0/1&quot;&gt;Guy Uziel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Yaniv_R/0/1/0/all/0/1&quot;&gt;Ran El-Yaniv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08336">
<title>Maximum Causal Tsallis Entropy Imitation Learning. (arXiv:1805.08336v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08336</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel maximum causal Tsallis entropy (MCTE)
framework for imitation learning which can efficiently learn a sparse
multi-modal policy distribution from demonstrations. We provide the full
mathematical analysis of the proposed framework. First, the optimal solution of
an MCTE problem is shown to be a sparsemax distribution, whose supporting set
can be adjusted. The proposed method has advantages over a softmax distribution
in that it can exclude unnecessary actions by assigning zero probability.
Second, we prove that an MCTE problem is equivalent to robust Bayes estimation
in the sense of the Brier score. Third, we propose a maximum causal Tsallis
entropy imitation learning (MCTEIL) algorithm with a sparse mixture density
network (sparse MDN) by modeling mixture weights using a sparsemax
distribution. In particular, we show that the causal Tsallis entropy of an MDN
encourages exploration and efficient mixture utilization while Boltzmann Gibbs
entropy is less effective. We validate the proposed method in two simulation
studies and MCTEIL outperforms existing imitation learning methods in terms of
average returns and learning multi-modal policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyungjae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Sungjoon Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Songhwai Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09654">
<title>Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals. (arXiv:1805.09654v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09654</link>
<description rdf:parseType="Literal">&lt;p&gt;Frequency-specific patterns of neural activity are traditionally interpreted
as sustained rhythmic oscillations, and related to cognitive mechanisms such as
attention, high level visual processing or motor control. While alpha waves
(8-12 Hz) are known to closely resemble short sinusoids, and thus are revealed
by Fourier analysis or wavelet transforms, there is an evolving debate that
electromagnetic neural signals are composed of more complex waveforms that
cannot be analyzed by linear filters and traditional signal representations. In
this paper, we propose to learn dedicated representations of such recordings
using a multivariate convolutional sparse coding (CSC) algorithm. Applied to
electroencephalography (EEG) or magnetoencephalography (MEG) data, this method
is able to learn not only prototypical temporal waveforms, but also associated
spatial patterns so their origin can be localized in the brain. Our algorithm
is based on alternated minimization and a greedy coordinate descent solver that
leads to state-of-the-art running time on long time series. To demonstrate the
implications of this method, we apply it to MEG data and show that it is able
to recover biological artifacts. More remarkably, our approach also reveals the
presence of non-sinusoidal mu-shaped patterns, along with their topographic
maps related to the somatosensory cortex.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tour_T/0/1/0/all/0/1&quot;&gt;Tom Dupr&amp;#xe9; La Tour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moreau_T/0/1/0/all/0/1&quot;&gt;Thomas Moreau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jas_M/0/1/0/all/0/1&quot;&gt;Mainak Jas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gramfort_A/0/1/0/all/0/1&quot;&gt;Alexandre Gramfort&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.05543">
<title>In Defense of the Indefensible: A Very Naive Approach to High-Dimensional Inference. (arXiv:1705.05543v2 [stat.ME] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1705.05543</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, a great deal of interest has focused on conducting inference
on the parameters in a linear model in the high-dimensional setting. In this
paper, we consider a simple and very na\&quot;{i}ve two-step procedure for this
task, in which we (i) fit a lasso model in order to obtain a subset of the
variables; and (ii) fit a least squares model on the lasso-selected set.
Conventional statistical wisdom tells us that we cannot make use of the
standard statistical inference tools for the resulting least squares model
(such as confidence intervals and $p$-values), since we peeked at the data
twice: once in running the lasso, and again in fitting the least squares model.
However, in this paper, we show that under a certain set of assumptions, with
high probability, the set of variables selected by the lasso is deterministic.
Consequently, the na\&quot;{i}ve two-step approach can yield confidence intervals
that have asymptotically correct coverage, as well as p-values with proper
Type-I error control. Furthermore, this two-step approach unifies two existing
camps of work on high-dimensional inference: one camp has focused on inference
based on a sub-model selected by the lasso, and the other has focused on
inference using a debiased version of the lasso estimator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Sen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shojaie_A/0/1/0/all/0/1&quot;&gt;Ali Shojaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Witten_D/0/1/0/all/0/1&quot;&gt;Daniela Witten&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03248">
<title>Piecewise Flat Embedding for Image Segmentation. (arXiv:1802.03248v5 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1802.03248</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new multi-dimensional nonlinear embedding -- Piecewise Flat
Embedding (PFE) -- for image segmentation. Based on the theory of sparse signal
recovery, piecewise flat embedding with diverse channels attempts to recover a
piecewise constant image representation with sparse region boundaries and
sparse cluster value scattering. The resultant piecewise flat embedding
exhibits interesting properties such as suppressing slowly varying signals, and
offers an image representation with higher region identifiability which is
desirable for image segmentation or high-level semantic analysis tasks. We
formulate our embedding as a variant of the Laplacian Eigenmap embedding with
an $L_{1,p} (0&amp;lt;p\leq1)$ regularization term to promote sparse solutions. First,
we devise a two-stage numerical algorithm based on Bregman iterations to
compute $L_{1,1}$-regularized piecewise flat embeddings. We further generalize
this algorithm through iterative reweighting to solve the general
$L_{1,p}$-regularized problem. To demonstrate its efficacy, we integrate PFE
into two existing image segmentation frameworks, segmentation based on
clustering and hierarchical segmentation based on contour detection.
Experiments on four major benchmark datasets, BSDS500, MSRC, Stanford
Background Dataset, and PASCAL Context, show that segmentation algorithms
incorporating our embedding achieve significantly improved results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Chaowei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yizhou Yu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>