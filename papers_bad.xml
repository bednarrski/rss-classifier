<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-01T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00092"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00272"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00090"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00121"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00145"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00184"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00185"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00307"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00314"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00326"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00361"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00460"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07745"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10899"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00013"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00020"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00063"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00089"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00169"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00216"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00356"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00452"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10571"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02037"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04126"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00636"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.00092">
<title>New Methods of Studying Valley Fitness Landscapes. (arXiv:1805.00092v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.00092</link>
<description rdf:parseType="Literal">&lt;p&gt;The word &quot;valley&quot; is a popular term used in intuitively describing fitness
landscapes. What is a valley on a fitness landscape? How to identify the
direction and location of a valley if it exists? However, such questions are
seldom rigorously studied in evolutionary optimization especially when the
search space is a high dimensional continuous space. This paper presents two
methods of studying valleys on a fitness landscape. The first method is based
on the topological homeomorphism. It establishes a rigorous definition of a
valley. A valley is regarded as a one-dimensional manifold. The second method
takes a different viewpoint from statistics. It provides an algorithm of
identifying the valley direction and location using principle component
analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jun He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00272">
<title>Multiobjective Optimization Differential Evolution Enhanced with Principle Component Analysis for Constrained Optimization. (arXiv:1805.00272v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.00272</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiobjective optimization evolutionary algorithms have been successfully
applied to solving constrained optimization problems. This paper proposes a new
multiobjective optimization differential evolution algorithm for constrained
optimization. Through a study of fitness landscapes using principle component
analysis, we discover a statistic method of identifying the valley direction in
a valley landscape. Based on this discovery, a new search operator called
PCA-projection is constructed which projects an individual to a position along
the valley direction. Then multiobjective optimization differential evolution
using this projection operator is designed for constrained optimization. A
comparative experiment has been implemented between the proposed algorithm and
a state-of-the-art multiobjective differential evolution algorithm on a
standard set of 24 benchmarks. Experimental results show that the new algorithm
makes a significant improvement in terms of solution accuracy. The proposed
algorithm is also competitive with ten evolutionary algorithms participated in
an IEEE CEC 2006 competition and is ranked third in terms of the final rank.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jun He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kangshun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00342">
<title>A Feedback Neural Network for Small Target Motion Detection in Cluttered Backgrounds. (arXiv:1805.00342v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.00342</link>
<description rdf:parseType="Literal">&lt;p&gt;Small target motion detection is critical for insects to search for and track
mates or prey which always appear as small dim speckles in the visual field. A
class of specific neurons, called small target motion detectors (STMDs), has
been characterized by exquisite sensitivity for small target motion.
Understanding and analyzing visual pathway of STMD neurons are beneficial to
design artificial visual systems for small target motion detection. Feedback
loops have been widely identified in visual neural circuits and play an
important role in target detection. However, if there exists a feedback loop in
the STMD visual pathway or if a feedback loop could significantly improve the
detection performance of STMD neurons, is unclear. In this paper, we propose a
feedback neural network for small target motion detection against naturally
cluttered backgrounds. In order to form a feedback loop, model output is
temporally delayed and relayed to previous neural layer as feedback signal.
Extensive experiments showed that the significant improvement of the proposed
feedback neural network over the existing STMD-based models for small target
motion detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jigen Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1&quot;&gt;Shigang Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00090">
<title>Using Multi Expression Programming in Software Effort Estimation. (arXiv:1805.00090v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.00090</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating the effort of software systems is an essential topic in software
engineering, carrying out an estimation process reliably and accurately for a
software forms a vital part of the software development phases. Many
researchers have utilized different methods and techniques hopping to find
solutions to this issue, such techniques include COCOMO, SEER-SEM,SLIM and
others. Recently, Artificial Intelligent techniques are being utilized to solve
such problems; different studies have been issued focusing on techniques such
as Neural Networks NN, Genetic Algorithms GA, and Genetic Programming GP. This
work uses one of the linear variations of GP, namely: Multi Expression
Programming (MEP) aiming to find the equation that best estimates the effort of
software. Benchmark datasets (based on previous projects) are used learning and
testing. Results are compared with those obtained by GP using different fitness
functions. Results show that MEP is far better in discovering effective
functions for the estimation of about 6 datasets each comprising several
projects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akram_N/0/1/0/all/0/1&quot;&gt;Najla Akram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AL-Saati/0/1/0/all/0/1&quot;&gt;AL-Saati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alreffaee_T/0/1/0/all/0/1&quot;&gt;Taghreed Riyadh Alreffaee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00121">
<title>A Missing Information Loss function for implicit feedback datasets. (arXiv:1805.00121v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.00121</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent factor models with implicit feedback typically treat unobserved
user-item interactions (i.e. missing information) as negative feedback. This is
frequently done either through negative sampling (point-wise loss) or with a
ranking loss function (pair- or list-wise estimation). Since a zero preference
recommendation is a valid solution for most common objective functions,
regarding unknown values as actual zeros results in users having a zero
preference recommendation for most of the available items.
&lt;/p&gt;
&lt;p&gt;In this paper we propose a novel objective function, the Missing Information
Loss (MIL) function, that explicitly forbids treating unobserved user-item
interactions as positive or negative feedback. We apply this loss to a
user--based Denoising Autoencoder and compare it with other known objective
functions such as cross-entropy (both point-- and pair--wise) or the recently
proposed multinomial log-likelihood. The MIL function achieves best results in
ranking-aware metrics when applied to the Movielens-20M and Netflix datasets,
slightly above those obtained with cross-entropy in point-wise estimation.
Furthermore, such a competitive performance is obtained while recommending
popular items less frequently, a valuable feature for Recommender Systems with
a large catalogue of products.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arevalo_J/0/1/0/all/0/1&quot;&gt;Juan Ar&amp;#xe9;valo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duque_J/0/1/0/all/0/1&quot;&gt;Juan Ram&amp;#xf3;n Duque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Creatura_M/0/1/0/all/0/1&quot;&gt;Marco Creatura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00145">
<title>Dialog-based Interactive Image Retrieval. (arXiv:1805.00145v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.00145</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods for interactive image retrieval have demonstrated the merit
of integrating user feedback, improving retrieval results. However, most
current systems rely on restricted forms of user feedback, such as binary
relevance responses, or feedback based on a fixed set of relative attributes,
which limits their impact. In this paper, we introduce a new approach to
interactive image search that enables users to provide feedback via natural
language, allowing for more natural and effective interaction. We formulate the
task of dialog-based interactive image retrieval as a reinforcement learning
problem, and reward the dialog system for improving the rank of the target
image during each dialog turn. To avoid the cumbersome and costly process of
collecting human-machine conversations as the dialog system learns, we train
our system with a user simulator, which is itself trained to describe the
differences between target and candidate images. The efficacy of our approach
is demonstrated in a footwear retrieval application. Extensive experiments on
both simulated and real-world data show that 1) our proposed learning framework
achieves better accuracy than other supervised and reinforcement learning
baselines and 2) user feedback based on natural language rather than
pre-specified attributes leads to more effective retrieval results, and a more
natural and expressive communication interface.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rennie_S/0/1/0/all/0/1&quot;&gt;Steven Rennie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1&quot;&gt;Rogerio Schmidt Feris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00184">
<title>Compact Factorization of Matrices Using Generalized Round-Rank. (arXiv:1805.00184v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.00184</link>
<description rdf:parseType="Literal">&lt;p&gt;Matrix factorization is a well-studied task in machine learning for compactly
representing large, noisy data. In our approach, instead of using the
traditional concept of matrix rank, we define a new notion of link-rank based
on a non-linear link function used within factorization. In particular, by
applying the round function on a factorization to obtain ordinal-valued
matrices, we introduce generalized round-rank (GRR). We show that not only are
there many full-rank matrices that are low GRR, but further, that these
matrices cannot be approximated well by low-rank linear factorization. We
provide uniqueness conditions of this formulation and provide gradient
descent-based algorithms. Finally, we present experiments on real-world
datasets to demonstrate that the GRR-based factorization is significantly more
accurate than linear factorization, while converging faster and using lower
rank representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pezeshkpour_P/0/1/0/all/0/1&quot;&gt;Pouya Pezeshkpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guestrin_C/0/1/0/all/0/1&quot;&gt;Carlos Guestrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sameer Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00185">
<title>Phylotastic: An Experiment in Creating, Manipulating, and Evolving Phylogenetic Biology Workflows Using Logic Programming. (arXiv:1805.00185v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.00185</link>
<description rdf:parseType="Literal">&lt;p&gt;Evolutionary Biologists have long struggled with the challenge of developing
analysis workflows in a flexible manner, thus facilitating the reuse of
phylogenetic knowledge. An evolutionary biology workflow can be viewed as a
plan which composes web services that can retrieve, manipulate, and produce
phylogenetic trees. The Phylotastic project was launched two years ago as a
collaboration between evolutionary biologists and computer scientists, with the
goal of developing an open architecture to facilitate the creation of such
analysis workflows. While composition of web services is a problem that has
been extensively explored in the literature, including within the logic
programming domain, the incarnation of the problem in Phylotastic provides a
number of additional challenges. Along with the need to integrate preferences
and formal ontologies in the description of the desired workflow, evolutionary
biologists tend to construct workflows in an incremental manner, by
successively refining the workflow, by indicating desired changes (e.g.,
exclusion of certain services, modifications of the desired output). This leads
to the need of successive iterations of incremental replanning, to develop a
new workflow that integrates the requested changes while minimizing the changes
to the original workflow. This paper illustrates how Phylotastic has addressed
the challenges of creating and refining phylogenetic analysis workflows using
logic programming technology and how such solutions have been used within the
general framework of the Phylotastic project. Under consideration in Theory and
Practice of Logic Programming (TPLP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thanh Hai Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pontelli_E/0/1/0/all/0/1&quot;&gt;Enrico Pontelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Son_T/0/1/0/all/0/1&quot;&gt;Tran Cao Son&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00307">
<title>Tourist Navigation in Android Smartphone by using Emotion Generating Calculations and Mental State Transition Networks. (arXiv:1805.00307v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1805.00307</link>
<description rdf:parseType="Literal">&lt;p&gt;Mental State Transition Network which consists of mental states connected to
each other is a basic concept of approximating to human psychological and
mental responses. It can represent transition from an emotional state to other
one with stimulus by calculating Emotion Generating Calculations method. A
computer agent can transit a mental state in MSTN based on analysis of emotion
by EGC method. In this paper, the Andorid EGC which the agent works in Android
smartphone can evaluate the feelings in the conversation. The tourist
navigation system with the proposed technique in this paper will be expected to
be an emotional oriented interface in Android smartphone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanabe_K/0/1/0/all/0/1&quot;&gt;Kosuke Tanabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tachibana_I/0/1/0/all/0/1&quot;&gt;Issei Tachibana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00314">
<title>Object Counts! Bringing Explicit Detections Back into Image Captioning. (arXiv:1805.00314v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.00314</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of explicit object detectors as an intermediate step to image
captioning - which used to constitute an essential stage in early work - is
often bypassed in the currently dominant end-to-end approaches, where the
language model is conditioned directly on a mid-level image embedding. We argue
that explicit detections provide rich semantic information, and can thus be
used as an interpretable representation to better understand why end-to-end
image captioning systems work well. We provide an in-depth analysis of
end-to-end image captioning by exploring a variety of cues that can be derived
from such object detections. Our study reveals that end-to-end image captioning
systems rely on matching image representations to generate captions, and that
encoding the frequency, size and position of objects are complementary and all
play a role in forming a good image representation. It also reveals that
different object categories contribute in different ways towards image
captioning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Josiah Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1&quot;&gt;Pranava Madhyastha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1&quot;&gt;Lucia Specia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00326">
<title>I Know How You Feel: Emotion Recognition with Facial Landmarks. (arXiv:1805.00326v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.00326</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification of human emotions remains an important and challenging task
for many computer vision algorithms, especially in the era of humanoid robots
which coexist with humans in their everyday life. Currently proposed methods
for emotion recognition solve this task using multi-layered convolutional
networks that do not explicitly infer any facial features in the classification
phase. In this work, we postulate a fundamentally different approach to solve
emotion recognition task that relies on incorporating facial landmarks as a
part of the classification loss function. To that end, we extend a recently
proposed Deep Alignment Network (DAN), that achieves state-of-the-art results
in the recent facial landmark recognition challenge, with a term related to
facial features. Thanks to this simple modification, our model called
EmotionalDAN is able to outperform state-of-the-art emotion classification
methods on two challenging benchmark dataset by up to 5%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tautkute_I/0/1/0/all/0/1&quot;&gt;Ivona Tautkute&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1&quot;&gt;Tomasz Trzcinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bielski_A/0/1/0/all/0/1&quot;&gt;Adam Bielski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00361">
<title>Ultra Power-Efficient CNN Domain Specific Accelerator with 9.3TOPS/Watt for Mobile and Embedded Applications. (arXiv:1805.00361v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.00361</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer vision performances have been significantly improved in recent years
by Convolutional Neural Networks(CNN). Currently, applications using CNN
algorithms are deployed mainly on general purpose hardwares, such as CPUs, GPUs
or FPGAs. However, power consumption, speed, accuracy, memory footprint, and
die size should all be taken into consideration for mobile and embedded
applications. Domain Specific Architecture (DSA) for CNN is the efficient and
practical solution for CNN deployment and implementation. We designed and
produced a 28nm Two-Dimensional CNN-DSA accelerator with an ultra
power-efficient performance of 9.3TOPS/Watt and with all processing done in the
internal memory instead of outside DRAM. It classifies 224x224 RGB image inputs
at more than 140fps with peak power consumption at less than 300mW and an
accuracy comparable to the VGG benchmark. The CNN-DSA accelerator is
reconfigurable to support CNN model coefficients of various layer sizes and
layer types, including convolution, depth-wise convolution, short-cut
connections, max pooling, and ReLU. Furthermore, in order to better support
real-world deployment for various application scenarios, especially with
low-end mobile and embedded platforms and MCUs (Microcontroller Units), we also
designed algorithms to fully utilize the CNN-DSA accelerator efficiently by
reducing the dependency on external accelerator computation resources,
including implementation of Fully-Connected (FC) layers within the accelerator
and compression of extracted features from the CNN-DSA accelerator. Live demos
with our CNN-DSA accelerator on mobile and embedded systems show its
capabilities to be widely and practically applied in the real world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Baohua Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1&quot;&gt;Patrick Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenhan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jason Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_C/0/1/0/all/0/1&quot;&gt;Charles Young&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00460">
<title>Customized Image Narrative Generation via Interactive Visual Question Generation and Answering. (arXiv:1805.00460v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.00460</link>
<description rdf:parseType="Literal">&lt;p&gt;Image description task has been invariably examined in a static manner with
qualitative presumptions held to be universally applicable, regardless of the
scope or target of the description. In practice, however, different viewers may
pay attention to different aspects of the image, and yield different
descriptions or interpretations under various contexts. Such diversity in
perspectives is difficult to derive with conventional image description
techniques. In this paper, we propose a customized image narrative generation
task, in which the users are interactively engaged in the generation process by
providing answers to the questions. We further attempt to learn the user&apos;s
interest via repeating such interactive stages, and to automatically reflect
the interest in descriptions for new images. Experimental results demonstrate
that our model can generate a variety of descriptions from single image that
cover a wider range of topics than conventional models, while being
customizable to the target user of interaction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_A/0/1/0/all/0/1&quot;&gt;Andrew Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1&quot;&gt;Yoshitaka Ushiku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07745">
<title>Optimal Transport on Discrete Domains. (arXiv:1801.07745v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07745</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the matching of supply to demand in logistical problems, the
optimal transport (or Monge--Kantorovich) problem involves the matching of
probability distributions defined over a geometric domain such as a surface or
manifold. In its most obvious discretization, optimal transport becomes a
large-scale linear program, which typically is infeasible to solve efficiently
on triangle meshes, graphs, point clouds, and other domains encountered in
graphics and machine learning. Recent breakthroughs in numerical optimal
transport, however, enable scalability to orders-of-magnitude larger problems,
solvable in a fraction of a second. Here, we discuss advances in numerical
optimal transport that leverage understanding of both discrete and smooth
aspects of the problem. State-of-the-art techniques in discrete optimal
transport combine insight from partial differential equations (PDE) with convex
analysis to reformulate, discretize, and optimize transportation problems. The
end result is a set of theoretically-justified models suitable for domains with
thousands or millions of vertices. Since numerical optimal transport is a
relatively new discipline, special emphasis is placed on identifying and
explaining open problems in need of mathematical insight and additional
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Solomon_J/0/1/0/all/0/1&quot;&gt;Justin Solomon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10899">
<title>Scalable Angular Discriminative Deep Metric Learning for Face Recognition. (arXiv:1804.10899v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10899</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of deep learning, Deep Metric Learning (DML) has
achieved great improvements in face recognition. Specifically, the widely used
softmax loss in the training process often bring large intra-class variations,
and feature normalization is only exploited in the testing process to compute
the pair similarities. To bridge the gap, we impose the intra-class cosine
similarity between the features and weight vectors in softmax loss larger than
a margin in the training step, and extend it from four aspects. First, we
explore the effect of a hard sample mining strategy. To alleviate the human
labor of adjusting the margin hyper-parameter, a self-adaptive margin updating
strategy is proposed. Then, a normalized version is given to take full
advantage of the cosine similarity constraint. Furthermore, we enhance the
former constraint to force the intra-class cosine similarity larger than the
mean inter-class cosine similarity with a margin in the exponential feature
projection space. Extensive experiments on Labeled Face in the Wild (LFW),
Youtube Faces (YTF) and IARPA Janus Benchmark A (IJB-A) datasets demonstrate
that the proposed methods outperform the mainstream DML methods and approach
the state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bowen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Huaming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Monica M.Y. Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00013">
<title>Constraining Effective Field Theories with Machine Learning. (arXiv:1805.00013v1 [hep-ph])</title>
<link>http://arxiv.org/abs/1805.00013</link>
<description rdf:parseType="Literal">&lt;p&gt;We present powerful new analysis techniques to constrain effective field
theories at the LHC. By leveraging the structure of particle physics processes,
we extract extra information from Monte-Carlo simulations, which can be used to
train neural network models that estimate the likelihood ratio. These methods
scale well to processes with many observables and theory parameters, do not
require any approximations of the parton shower or detector response, and can
be evaluated in microseconds. We show that they allow us to put significantly
stronger bounds on dimension-six operators than existing methods, demonstrating
their potential to improve the precision of the LHC legacy constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Brehmer_J/0/1/0/all/0/1&quot;&gt;Johann Brehmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Cranmer_K/0/1/0/all/0/1&quot;&gt;Kyle Cranmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Louppe_G/0/1/0/all/0/1&quot;&gt;Gilles Louppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Pavez_J/0/1/0/all/0/1&quot;&gt;Juan Pavez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00020">
<title>A Guide to Constraining Effective Field Theories with Machine Learning. (arXiv:1805.00020v1 [hep-ph])</title>
<link>http://arxiv.org/abs/1805.00020</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop, discuss, and compare several inference techniques to constrain
theory parameters in collider experiments. By harnessing the latent-space
structure of particle physics processes, we extract extra information from the
simulator. This augmented data can be used to train neural networks that
precisely estimate the likelihood ratio. The new methods scale well to many
observables and high-dimensional parameter spaces, do not require any
approximations of the parton shower and detector response, and can be evaluated
in microseconds. Using weak-boson-fusion Higgs production as an example
process, we compare the performance of several techniques. The best results are
found for likelihood ratio estimators trained with extra information about the
score, the gradient of the log likelihood function with respect to the theory
parameters. The score also provides sufficient statistics that contain all the
information needed for inference in the neighborhood of the Standard Model.
These methods enable us to put significantly stronger bounds on effective
dimension-six operators than the traditional approach based on histograms. They
also outperform generic machine learning methods that do not make use of the
particle physics structure, demonstrating their potential to substantially
improve the new physics reach of the LHC legacy results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Brehmer_J/0/1/0/all/0/1&quot;&gt;Johann Brehmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Cranmer_K/0/1/0/all/0/1&quot;&gt;Kyle Cranmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Louppe_G/0/1/0/all/0/1&quot;&gt;Gilles Louppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Pavez_J/0/1/0/all/0/1&quot;&gt;Juan Pavez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00063">
<title>Improved Image Captioning with Adversarial Semantic Alignment. (arXiv:1805.00063v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.00063</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose a new conditional GAN for image captioning that
enforces semantic alignment between images and captions through a co-attentive
discriminator and a context-aware LSTM sequence generator. In order to train
these sequence GANs, we empirically study two algorithms: Self-critical
Sequence Training (SCST) and Gumbel Straight-Through. Both techniques are
confirmed to be viable for training sequence GANs. However, SCST displays
better gradient behavior despite not directly leveraging gradients from the
discriminator. This ensures a stronger stability of sequence GANs training and
ultimately produces models with improved results under human evaluation.
Automatic evaluation of GAN trained captioning models is an open question. To
remedy this, we introduce a new semantic score with strong correlation to human
judgement. As a paradigm for evaluation, we suggest that the generalization
ability of the captioner to Out of Context (OOC) scenes is an important
criterion to assess generalization and composition. To this end, we propose an
OOC dataset which, combined with our automatic metric of semantic score, is a
new benchmark for the captioning community to measure the generalization
ability of automatic image captioning. Under this new OOC benchmark, and on the
traditional MSCOCO dataset, our models trained with SCST have strong
performance in both semantic score and human evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melnyk_I/0/1/0/all/0/1&quot;&gt;Igor Melnyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sercu_T/0/1/0/all/0/1&quot;&gt;Tom Sercu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dognin_P/0/1/0/all/0/1&quot;&gt;Pierre L. Dognin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1&quot;&gt;Jarret Ross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1&quot;&gt;Youssef Mroueh&lt;/a&gt; (IBM Research, USA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00089">
<title>Concolic Testing for Deep Neural Networks. (arXiv:1805.00089v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.00089</link>
<description rdf:parseType="Literal">&lt;p&gt;Concolic testing alternates between CONCrete program execution and symbOLIC
analysis to explore the execution paths of a software program and to increase
code coverage. In this paper, we develop the first concolic testing approach
for Deep Neural Networks (DNNs). More specifically, we utilise quantified
linear arithmetic over rationals to express test requirements that have been
studied in the literature, and then develop a coherent method to perform
concolic testing with the aim of better coverage. Our experimental results show
the effectiveness of the concolic testing approach in both achieving high
coverage and finding adversarial examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Youcheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1&quot;&gt;Marta Kwiatkowska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kroening_D/0/1/0/all/0/1&quot;&gt;Daniel Kroening&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00169">
<title>Multi-Step Knowledge-Aided Iterative ESPRIT for Direction Finding. (arXiv:1805.00169v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1805.00169</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a subspace-based algorithm for DOA estimation which
iteratively reduces the disturbance factors of the estimated data covariance
matrix and incorporates prior knowledge which is gradually obtained on line. An
analysis of the MSE of the reshaped data covariance matrix is carried out along
with comparisons between computational complexities of the proposed and
existing algorithms. Simulations focusing on closely-spaced sources, where they
are uncorrelated and correlated, illustrate the improvements achieved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pinto_S/0/1/0/all/0/1&quot;&gt;S. F. B. Pinto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lamare_R/0/1/0/all/0/1&quot;&gt;R. C. de Lamare&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00216">
<title>Privately Learning High-Dimensional Distributions. (arXiv:1805.00216v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1805.00216</link>
<description rdf:parseType="Literal">&lt;p&gt;We design nearly optimal differentially private algorithms for learning two
fundamental families of high-dimensional distributions in total variation
distance: multivariate Gaussians in $\mathbb{R}^{d}$ and product distributions
on the hypercube. The sample complexity of both our algorithms approaches the
sample complexity of non-private learners up to a small multiplicative factor
and an additional additive term that is lower order for a wide range of
parameters, showing that privacy comes essentially for free for these problems.
Our algorithms use a novel technical approach to reducing the sensitivity of
the estimation procedure that we call recursive private preconditioning and may
find additional applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamath_G/0/1/0/all/0/1&quot;&gt;Gautam Kamath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jerry Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_V/0/1/0/all/0/1&quot;&gt;Vikrant Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullman_J/0/1/0/all/0/1&quot;&gt;Jonathan Ullman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00356">
<title>Deep Factorization Machines for Knowledge Tracing. (arXiv:1805.00356v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1805.00356</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces our solution to the 2018 Duolingo Shared Task on Second
Language Acquisition Modeling (SLAM). We used deep factorization machines, a
wide and deep learning model of pairwise relationships between users, items,
skills, and other entities considered. Our solution (AUC 0.815) hopefully
managed to beat the logistic regression baseline (AUC 0.774) but not the top
performing model (AUC 0.861) and reveals interesting strategies to build upon
item response theory models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vie_J/0/1/0/all/0/1&quot;&gt;Jill-J&amp;#xea;nn Vie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00452">
<title>Coupling and Convergence for Hamiltonian Monte Carlo. (arXiv:1805.00452v1 [math.PR])</title>
<link>http://arxiv.org/abs/1805.00452</link>
<description rdf:parseType="Literal">&lt;p&gt;Based on a new coupling approach, we prove that the transition step of the
Hamiltonian Monte Carlo algorithm is contractive w.r.t. a carefully designed
Kantorovich (L1 Wasserstein) distance. The lower bound for the contraction rate
is explicit. Global convexity of the potential is not required, and thus
multimodal target distributions are included. Explicit quantitative bounds for
the number of steps required to approximate the stationary distribution up to a
given error are a direct consequence of contractivity. These bounds show that
HMC can overcome diffusive behaviour if the duration of the Hamiltonian
dynamics is adjusted appropriately.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bou_Rabee_N/0/1/0/all/0/1&quot;&gt;Nawaf Bou-Rabee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Eberle_A/0/1/0/all/0/1&quot;&gt;Andreas Eberle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zimmer_R/0/1/0/all/0/1&quot;&gt;Raphael Zimmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10571">
<title>Certifying Some Distributional Robustness with Principled Adversarial Training. (arXiv:1710.10571v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10571</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are vulnerable to adversarial examples and researchers have
proposed many heuristic attack and defense mechanisms. We address this problem
through the principled lens of distributionally robust optimization, which
guarantees performance under adversarial input perturbations. By considering a
Lagrangian penalty formulation of perturbing the underlying data distribution
in a Wasserstein ball, we provide a training procedure that augments model
parameter updates with worst-case perturbations of training data. For smooth
losses, our procedure provably achieves moderate levels of robustness with
little computational or statistical cost relative to empirical risk
minimization. Furthermore, our statistical guarantees allow us to efficiently
certify robustness for the population loss. For imperceptible perturbations,
our method matches or outperforms heuristic approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sinha_A/0/1/0/all/0/1&quot;&gt;Aman Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Namkoong_H/0/1/0/all/0/1&quot;&gt;Hongseok Namkoong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duchi_J/0/1/0/all/0/1&quot;&gt;John Duchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.02037">
<title>Randomized Nonnegative Matrix Factorization. (arXiv:1711.02037v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.02037</link>
<description rdf:parseType="Literal">&lt;p&gt;Nonnegative matrix factorization (NMF) is a powerful tool for data mining.
However, the emergence of `big data&apos; has severely challenged our ability to
compute this fundamental decomposition using deterministic algorithms. This
paper presents a randomized hierarchical alternating least squares (HALS)
algorithm to compute the NMF. By deriving a smaller matrix from the nonnegative
input data, a more efficient nonnegative decomposition can be computed. Our
algorithm scales to big data applications while attaining a near-optimal
factorization. The proposed algorithm is evaluated using synthetic and real
world data and shows substantial speedups compared to deterministic HALS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Erichson_N/0/1/0/all/0/1&quot;&gt;N. Benjamin Erichson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mendible_A/0/1/0/all/0/1&quot;&gt;Ariana Mendible&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wihlborn_S/0/1/0/all/0/1&quot;&gt;Sophie Wihlborn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kutz_J/0/1/0/all/0/1&quot;&gt;J. Nathan Kutz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04126">
<title>Adversarial Training for Disease Prediction from Electronic Health Records with Missing Data. (arXiv:1711.04126v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04126</link>
<description rdf:parseType="Literal">&lt;p&gt;Electronic health records (EHRs) have contributed to the computerization of
patient records and can thus be used not only for efficient and systematic
medical services, but also for research on biomedical data science. However,
there are many missing values in EHRs when provided in matrix form, which is an
important issue in many biomedical EHR applications. In this paper, we propose
a two-stage framework that includes missing data imputation and disease
prediction to address the missing data problem in EHRs. We compared the disease
prediction performance of generative adversarial networks (GANs) and
conventional learning algorithms in combination with missing data prediction
methods. As a result, we obtained a level of accuracy of 0.9777, sensitivity of
0.9521, specificity of 0.9925, area under the receiver operating characteristic
curve (AUC-ROC) of 0.9889, and F-score of 0.9688 with a stacked autoencoder as
the missing data prediction method and an auxiliary classifier GAN (AC-GAN) as
the disease prediction method. The comparison results show that a combination
of a stacked autoencoder and an AC-GAN significantly outperforms other existing
approaches. Our results suggest that the proposed framework is more robust for
disease prediction from EHRs with missing data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_U/0/1/0/all/0/1&quot;&gt;Uiwon Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Sungwoon Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Han-Byoel Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sungroh Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00636">
<title>Recursive Optimization of Convex Risk Measures: Mean-Semideviation Models. (arXiv:1804.00636v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00636</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop and analyze stochastic subgradient methods for optimizing a new,
versatile, application-friendly and tractable class of convex risk measures,
termed here as mean-semideviations. Their construction relies on on the concept
of a risk regularizer, a one-dimensional nonlinear map with certain properties,
essentially generalizing the positive part weighting function in the
mean-upper-semideviation risk measure. After we formally introduce
mean-semideviations, we study their basic properties, and we present a
fundamental constructive characterization result, demonstrating their
generality.
&lt;/p&gt;
&lt;p&gt;We then introduce and rigorously analyze the MESSAGEp algorithm, an efficient
stochastic subgradient procedure for iteratively solving convex
mean-semideviation risk-averse problems to optimality. The MESSAGEp algorithm
may be derived as an application of the T-SCGD algorithm of (Yang et al.,
2018). However, the generic theoretical framework of (Yang et al., 2018) is too
narrow and structurally restrictive, as far as optimization of
mean-semideviations is concerned, including the classical
mean-upper-semideviation risk measure. By exploiting problem structure, we
propose a substantially weaker theoretical framework, under which we establish
pathwise convergence of the MESSAGEp algorithm, under the same strong sense as
in (Yang et al., 2018). The new framework reveals a fundamental trade-off
between the smoothness of the random position function and that of the
particular mean-semideviation risk measure under consideration. Further, we
explicitly show that the class of mean-semideviation problems supported under
our framework is strictly larger than the respective class of problems
supported in (Yang et al., 2018). Thus, applicability of compositional
stochastic optimization is established for a strictly wider spectrum of
mean-semideviation problems, justifying the purpose of our work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kalogerias_D/0/1/0/all/0/1&quot;&gt;Dionysios S. Kalogerias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Powell_W/0/1/0/all/0/1&quot;&gt;Warren B. Powell&lt;/a&gt;</dc:creator>
</item></rdf:RDF>