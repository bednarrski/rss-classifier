<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-02T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00438"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00456"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00480"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00578"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01224"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01883"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00124"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00196"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00275"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00298"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00381"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00401"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00403"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00442"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00468"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00553"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00589"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00643"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00755"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00771"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.06564"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06562"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01412"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07540"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00617"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10829"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11324"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11704"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11391"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00028"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00046"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00068"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00084"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00122"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00123"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00126"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00130"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00243"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00263"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00311"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00325"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00349"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00373"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00448"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00451"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00546"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00558"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00583"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00595"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00636"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00692"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00787"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00801"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1502.00727"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.03425"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.05230"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.06131"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.00476"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.08705"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05707"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.03560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03840"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00212"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02781"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09539"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03121"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04823"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10547"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.00438">
<title>Dynamic Swarm Dispersion in Particle Swarm Optimization for Mining Unsearched Area in Solution Space (DSDPSO). (arXiv:1807.00438v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.00438</link>
<description rdf:parseType="Literal">&lt;p&gt;Premature convergence in particle swarm optimization (PSO) algorithm usually
leads to gaining local optimum and preventing from surveying those regions of
solution space which have optimal points in. In this paper, by applying special
mechanisms, suitable regions were detected and then swarm was guided to them by
dispersing part of particles on proper times. This process is called dynamic
swarm dispersion in PSO (DSDPSO) algorithm. In order to specify the proper
times and to rein the evolutionary process alternating between exploring and
exploiting behaviors, we used a diversity measuring approach and implemented
the dispersion mechanism. To promote the performance of DSDPSO algorithm, three
different policies including particle relocation, velocity settings of
dispersed particles and parameters setting were applied. We compared the
promoted algorithm with similar new approaches and according to the numerical
results, the proposed algorithm outperformed the basic GPSO, LPSO, DMS-PSO,
CLPSO and APSO in most of the 12 standard benchmark problems with different
properties taken in this study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahrampour_A/0/1/0/all/0/1&quot;&gt;Anvar Bahrampour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nezami_O/0/1/0/all/0/1&quot;&gt;Omid Mohamad Nezami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00456">
<title>Evenly Cascaded Convolutional Networks. (arXiv:1807.00456v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.00456</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we demonstrate that state-of-the-art convolutional neural
networks can be constructed using a cascade algorithm for deep networks,
inspired by the cascade algorithm in wavelet analysis.
&lt;/p&gt;
&lt;p&gt;For each network layer the cascade algorithm creates two streams of features
from the previous layer: one stream modulates the existing features producing
low-level features, the other stream produces new features of a higher level.
We evenly structure our network by resizing feature map dimensions by a
consistent ratio.
&lt;/p&gt;
&lt;p&gt;Our network produces humanly interpretable features maps, a result whose
intuition can be understood in the context of scale-space theory. We
demonstrate that our cascaded design facilitates the training process through
providing easily trainable shortcuts. We report new state-of-the-art results
for small networks - a consequence of our architecture&apos;s simple structure and
direct training, without the need for additional treatment such as pruning or
compression. Our 6-cascading-layer design with under 500k parameters achieves
95.24% and 78.99% accuracy on CIFAR-10 and CIFAR-100 datasets, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1&quot;&gt;Chengxi Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devaraj_C/0/1/0/all/0/1&quot;&gt;Chinmaya Devaraj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maynord_M/0/1/0/all/0/1&quot;&gt;Michael Maynord&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fermuller_C/0/1/0/all/0/1&quot;&gt;Cornelia Ferm&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aloimonos_Y/0/1/0/all/0/1&quot;&gt;Yiannis Aloimonos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00480">
<title>FATE: Fast and Accurate Timing Error Prediction Framework for Low Power DNN Accelerator Design. (arXiv:1807.00480v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00480</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNN) are increasingly being accelerated on
application-specific hardware such as the Google TPU designed especially for
deep learning. Timing speculation is a promising approach to further increase
the energy efficiency of DNN accelerators. Architectural exploration for timing
speculation requires detailed gate-level timing simulations that can be
time-consuming for large DNNs that execute millions of multiply-and-accumulate
(MAC) operations. In this paper we propose FATE, a new methodology for fast and
accurate timing simulations of DNN accelerators like the Google TPU. FATE
proposes two novel ideas: (i) DelayNet, a DNN based timing model for MAC units;
and (ii) a statistical sampling methodology that reduces the number of MAC
operations for which timing simulations are performed. We show that FATE
results in between 8 times-58 times speed-up in timing simulations, while
introducing less than 2% error in classification accuracy estimates. We
demonstrate the use of FATE by comparing to conventional DNN accelerator that
uses 2&apos;s complement (2C) arithmetic with an alternative implementation that
uses signed magnitude representations (SMR). We show that that the SMR
implementation provides 18% more energy savings for the same classification
accuracy than 2C, a result that might be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jeff Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Siddharth Garg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00578">
<title>Classifying neuromorphic data using a deep learning framework for image classification. (arXiv:1807.00578v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.00578</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of artificial intelligence, neuromorphic computing has been
around for several decades. Deep learning has however made much recent progress
such that it consistently outperforms neuromorphic learning algorithms in
classification tasks in terms of accuracy. Specifically in the field of image
classification, neuromorphic computing has been traditionally using either the
temporal or rate code for encoding static images in datasets into spike trains.
It is only till recently, that neuromorphic vision sensors are widely used by
the neuromorphic research community, and provides an alternative to such
encoding methods. Since then, several neuromorphic datasets as obtained by
applying such sensors on image datasets (e.g. the neuromorphic CALTECH 101)
have been introduced. These data are encoded in spike trains and hence seem
ideal for benchmarking of neuromorphic learning algorithms. Specifically, we
train a deep learning framework used for image classification on the CALTECH
101 and a collapsed version of the neuromorphic CALTECH 101 datasets. We
obtained an accuracy of 91.66% and 78.01% for the CALTECH 101 and neuromorphic
CALTECH 101 datasets respectively. For CALTECH 101, our accuracy is close to
the best reported accuracy, while for neuromorphic CALTECH 101, it outperforms
the last best reported accuracy by over 10%. This raises the question of the
suitability of such datasets as benchmarks for neuromorphic learning
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_R/0/1/0/all/0/1&quot;&gt;Roshan Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_Y/0/1/0/all/0/1&quot;&gt;Yansong Chua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyer_L/0/1/0/all/0/1&quot;&gt;Laxmi R Iyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01224">
<title>Challenges in High-dimensional Reinforcement Learning with Evolution Strategies. (arXiv:1806.01224v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01224</link>
<description rdf:parseType="Literal">&lt;p&gt;Evolution Strategies (ESs) have recently become popular for training deep
neural networks, in particular on reinforcement learning tasks, a special form
of controller design. Compared to classic problems in continuous direct search,
deep networks pose extremely high-dimensional optimization problems, with many
thousands or even millions of variables. In addition, many control problems
give rise to a stochastic fitness function. Considering the relevance of the
application, we study the suitability of evolution strategies for
high-dimensional, stochastic problems. Our results give insights into which
algorithmic mechanisms of modern ES are of value for the class of problems at
hand, and they reveal principled limitations of the approach. They are in line
with our theoretical understanding of ESs. We show that combining ESs that
offer reduced internal algorithm cost with uncertainty handling techniques
yields promising methods for this class of problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_N/0/1/0/all/0/1&quot;&gt;Nils M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glasmachers_T/0/1/0/all/0/1&quot;&gt;Tobias Glasmachers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01883">
<title>Routes to Open-Endedness in Evolutionary Systems. (arXiv:1806.01883v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01883</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a high-level conceptual framework to help orient the
discussion and implementation of open-endedness in evolutionary systems.
Drawing upon earlier work by Banzhaf et al., three different kinds of
open-endedness are identified: exploratory, expansive, and transformational.
These are characterised in terms of their relationship to the search space of
phenotypic behaviours. A formalism is introduced to describe three key
processes required for an evolutionary process: the generation of a phenotype
from a genetic description, the evaluation of that phenotype, and the
reproduction with variation of individuals according to their evaluation. The
formalism makes explicit various influences in each of these processes that can
easily be overlooked. The distinction is made between intrinsic and extrinsic
implementations of these processes. A discussion then investigates how various
interactions between these processes, and their modes of implementation, can
lead to open-endedness. However, it is demonstrated that these considerations
relate to exploratory open-endedness only. Conditions for the implementation of
the more interesting kinds of open-endedness - expansive and transformational -
are also discussed, emphasizing factors such as multiple domains of behaviour,
transdomain bridges, and non-additive compositional systems. In contrast to a
traditional &quot;neo-Darwinian&quot; analysis, these factors relate not to the generic
evolutionary properties of individuals, but rather to the nature of the
building blocks out of which individual organisms are constructed, and the laws
and properties of the environment in which they exist. The paper ends with
suggestions of how the framework can be used to categorise and compare the
open-ended evolutionary potential of different systems, and how it might guide
the design of systems with greater capacity for open-ended evolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_T/0/1/0/all/0/1&quot;&gt;Tim Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00124">
<title>Modeling Mistrust in End-of-Life Care. (arXiv:1807.00124v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00124</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we characterize the doctor-patient relationship using a machine
learning-derived trust score. We show that this score has statistically
significant racial associations, and that by modeling trust directly we find
stronger disparities in care than by stratifying on race. We further
demonstrate that mistrust is indicative of worse outcomes, but is only weakly
associated with physiologically-created severity scores. Finally, we describe
sentiment analysis experiments indicating patients with higher levels of
mistrust have worse experiences and interactions with their caregivers. This
work is a step towards measuring fairer machine learning in the healthcare
domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boag_W/0/1/0/all/0/1&quot;&gt;Willie Boag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suresh_H/0/1/0/all/0/1&quot;&gt;Harini Suresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1&quot;&gt;Leo Anthony Celi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szolovits_P/0/1/0/all/0/1&quot;&gt;Peter Szolovits&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1&quot;&gt;Marzyeh Ghassemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00196">
<title>Modeling Friends and Foes. (arXiv:1807.00196v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00196</link>
<description rdf:parseType="Literal">&lt;p&gt;How can one detect friendly and adversarial behavior from raw data? Detecting
whether an environment is a friend, a foe, or anything in between, remains a
poorly understood yet desirable ability for safe and robust agents. This paper
proposes a definition of these environmental &quot;attitudes&quot; based on an
characterization of the environment&apos;s ability to react to the agent&apos;s private
strategy. We define an objective function for a one-shot game that allows
deriving the environment&apos;s probability distribution under friendly and
adversarial assumptions alongside the agent&apos;s optimal strategy. Furthermore, we
present an algorithm to compute these equilibrium strategies, and show
experimentally that both friendly and adversarial environments possess
non-trivial optimal strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_P/0/1/0/all/0/1&quot;&gt;Pedro A. Ortega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legg_S/0/1/0/all/0/1&quot;&gt;Shane Legg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00275">
<title>Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera. (arXiv:1807.00275v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.00275</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth completion, the technique of estimating a dense depth image from sparse
depth measurements, has a variety of applications in robotics and autonomous
driving. However, depth completion faces 3 main challenges: the irregularly
spaced pattern in the sparse depth input, the difficulty in handling multiple
sensor modalities (when color images are available), as well as the lack of
dense, pixel-level ground truth depth labels. In this work, we address all
these challenges. Specifically, we develop a deep regression model to learn a
direct mapping from sparse depth (and color images) to dense depth. We also
propose a self-supervised training framework that requires only sequences of
color and sparse depth images, without the need for dense depth labels. Our
experiments demonstrate that our network, when trained with semi-dense
annotations, attains state-of-the- art accuracy and is the winning approach on
the KITTI depth completion benchmark at the time of submission. Furthermore,
the self-supervised framework outperforms a number of existing solutions
trained with semi- dense annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1&quot;&gt;Fangchang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavalheiro_G/0/1/0/all/0/1&quot;&gt;Guilherme Venturelli Cavalheiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1&quot;&gt;Sertac Karaman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00298">
<title>Multi-Task Generative Adversarial Nets with Shared Memory for Cross-Domain Coordination Control. (arXiv:1807.00298v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00298</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating sequential decision process from huge amounts of measured process
data is a future research direction for collaborative factory automation,
making full use of those online or offline process data to directly design
flexible make decisions policy, and evaluate performance. The key challenges
for the sequential decision process is to online generate sequential
decision-making policy directly, and transferring knowledge across tasks
domain. Most multi-task policy generating algorithms often suffer from
insufficient generating cross-task sharing structure at discrete-time nonlinear
systems with applications. This paper proposes the multi-task generative
adversarial nets with shared memory for cross-domain coordination control,
which can generate sequential decision policy directly from raw sensory input
of all of tasks, and online evaluate performance of system actions in
discrete-time nonlinear systems. Experiments have been undertaken using a
professional flexible manufacturing testbed deployed within a smart factory of
Weichai Power in China. Results on three groups of discrete-time nonlinear
control tasks show that our proposed model can availably improve the
performance of task with the help of other related tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;JunPing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;WenSheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_I/0/1/0/all/0/1&quot;&gt;Ian Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1&quot;&gt;ShiHui Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;YouKang Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00381">
<title>Model-based Exception Mining for Object-Relational Data. (arXiv:1807.00381v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00381</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper is based on a previous publication [29]. Our work extends
exception mining and outlier detection to the case of object-relational data.
Object-relational data represent a complex heterogeneous network [12], which
comprises objects of different types, links among these objects, also of
different types, and attributes of these links. This special structure
prohibits a direct vectorial data representation. We follow the
well-established Exceptional Model Mining framework, which leverages machine
learning models for exception mining: A object is exceptional to the extent
that a model learned for the object data differs from a model learned for the
general population. Exceptional objects can be viewed as outliers. We apply
state of-the-art probabilistic modelling techniques for object-relational data
that construct a graphical model (Bayesian network), which compactly represents
probabilistic associations in the data. A new metric, derived from the learned
object-relational model, quantifies the extent to which the individual
association pattern of a potential outlier deviates from that of the whole
population. The metric is based on the likelihood ratio of two parameter
vectors: One that represents the population associations, and another that
represents the individual associations. Our method is validated on synthetic
datasets and on real-world data sets about soccer matches and movies. Compared
to baseline methods, our novel transformed likelihood ratio achieved the best
detection accuracy on all datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riahi_F/0/1/0/all/0/1&quot;&gt;Fatemeh Riahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulte_O/0/1/0/all/0/1&quot;&gt;Oliver Schulte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00401">
<title>Machine learning 2.0 : Engineering Data Driven AI Products. (arXiv:1807.00401v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00401</link>
<description rdf:parseType="Literal">&lt;p&gt;ML 2.0: In this paper, we propose a paradigm shift from the current practice
of creating machine learning models - which requires months-long discovery,
exploration and &quot;feasibility report&quot; generation, followed by re-engineering for
deployment - in favor of a rapid, 8-week process of development, understanding,
validation and deployment that can executed by developers or subject matter
experts (non-ML experts) using reusable APIs. This accomplishes what we call a
&quot;minimum viable data-driven model,&quot; delivering a ready-to-use machine learning
model for problems that haven&apos;t been solved before using machine learning. We
provide provisions for the refinement and adaptation of the &quot;model,&quot; with
strict enforcement and adherence to both the scaffolding/abstractions and the
process. We imagine that this will bring forth the second phase in machine
learning, in which discovery is subsumed by more targeted goals of delivery and
impact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanter_J/0/1/0/all/0/1&quot;&gt;James Max Kanter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schreck_B/0/1/0/all/0/1&quot;&gt;Benjamin Schreck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1&quot;&gt;Kalyan Veeramachaneni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00403">
<title>Towards Mixed Optimization for Reinforcement Learning with Program Synthesis. (arXiv:1807.00403v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00403</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning has led to several recent breakthroughs, though
the learned policies are often based on black-box neural networks. This makes
them difficult to interpret and to impose desired specification constraints
during learning. We present an iterative framework, MORL, for improving the
learned policies using program synthesis. Concretely, we propose to use
synthesis techniques to obtain a symbolic representation of the learned policy,
which can then be debugged manually or automatically using program repair.
After the repair step, we use behavior cloning to obtain the policy
corresponding to the repaired program, which is then further improved using
gradient descent. This process continues until the learned policy satisfies
desired constraints. We instantiate MORL for the simple CartPole problem and
show that the programmatic representation allows for high-level modifications
that in turn lead to improved learning of the policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhupatiraju_S/0/1/0/all/0/1&quot;&gt;Surya Bhupatiraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_K/0/1/0/all/0/1&quot;&gt;Kumar Krishna Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rishabh Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00442">
<title>Policy Optimization With Penalized Point Probability Distance: An Alternative To Proximal Policy Optimization. (arXiv:1807.00442v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00442</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a first order gradient reinforcement learning algorithm,
which can be seen as a variant for Trust Region Policy Optimization(TRPO). This
method, which we call policy optimization with penalized point probability
distance (POP3D), keeps almost all positive spheres of proximal policy
optimization (PPO) such as easy implementation, fast learning and high score
capability. As PPO, we also use a single surrogate objective without
constraints, where a penalized item based on point probability distance is
included to prevent update step from growing too large. Experiments verify that
POP3D is state-of-the-art within 40 million frame steps on 49 Atari games based
on two common metrics, which can be a competitive alternative to PPO. Moreover,
comparison experiments regarding PPO based on Mujoco environment verify that
POP3D is also competitive in continuous domain. In addition, we release the
code on github https://github.com/cxxgtxy/POP3D.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Chu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00468">
<title>Automated Directed Fairness Testing. (arXiv:1807.00468v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00468</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness is a critical trait in decision making. As machine-learning models
are increasingly being used in sensitive application domains (e.g. education
and employment) for decision making, it is crucial that the decisions computed
by such models are free of unintended bias. But how can we automatically
validate the fairness of arbitrary machine-learning models? For a given
machine-learning model and a set of sensitive input parameters, our AEQUITAS
approach automatically discovers discriminatory inputs that highlight fairness
violation. At the core of AEQUITAS are three novel strategies to employ
probabilistic search over the input space with the objective of uncovering
fairness violation. Our AEQUITAS approach leverages inherent robustness
property in common machine-learning models to design and implement scalable
test generation methodologies. An appealing feature of our generated test
inputs is that they can be systematically added to the training set of the
underlying model and improve its fairness. To this end, we design a fully
automated module that guarantees to improve the fairness of the underlying
model.
&lt;/p&gt;
&lt;p&gt;We implemented AEQUITAS and we have evaluated it on six state-of-the-art
classifiers, including a classifier that was designed with fairness
constraints. We show that AEQUITAS effectively generates inputs to uncover
fairness violation in all the subject classifiers and systematically improves
the fairness of the respective models using the generated test inputs. In our
evaluation, AEQUITAS generates up to 70% discriminatory inputs (w.r.t. the
total number of inputs generated) and leverages these inputs to improve the
fairness up to 94%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udeshi_S/0/1/0/all/0/1&quot;&gt;Sakshi Udeshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1&quot;&gt;Pryanshu Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1&quot;&gt;Sudipta Chattopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00553">
<title>A Broader View on Bias in Automated Decision-Making: Reflecting on Epistemology and Dynamics. (arXiv:1807.00553v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00553</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) is increasingly deployed in real world contexts,
supplying actionable insights and forming the basis of automated
decision-making systems. While issues resulting from biases pre-existing in
training data have been at the center of the fairness debate, these systems are
also affected by technical and emergent biases, which often arise as
context-specific artifacts of implementation. This position paper interprets
technical bias as an epistemological problem and emergent bias as a dynamical
feedback phenomenon. In order to stimulate debate on how to change machine
learning practice to effectively address these issues, we explore this broader
view on bias, stress the need to reflect on epistemology, and point to
value-sensitive design methodologies to revisit the design and implementation
process of automated decision-making systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobbe_R/0/1/0/all/0/1&quot;&gt;Roel Dobbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dean_S/0/1/0/all/0/1&quot;&gt;Sarah Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_T/0/1/0/all/0/1&quot;&gt;Thomas Gilbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohli_N/0/1/0/all/0/1&quot;&gt;Nitin Kohli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00589">
<title>Lifted Marginal MAP Inference. (arXiv:1807.00589v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00589</link>
<description rdf:parseType="Literal">&lt;p&gt;Lifted inference reduces the complexity of inference in relational
probabilistic models by identifying groups of constants (or atoms) which behave
symmetric to each other. A number of techniques have been proposed in the
literature for lifting marginal as well MAP inference. We present the first
application of lifting rules for marginal-MAP (MMAP), an important inference
problem in models having latent (random) variables. Our main contribution is
two fold: (1) we define a new equivalence class of (logical) variables, called
Single Occurrence for MAX (SOM), and show that solution lies at extreme with
respect to the SOM variables, i.e., predicate groundings differing only in the
instantiation of the SOM variables take the same truth value (2) we define a
sub-class {\em SOM-R} (SOM Reduce) and exploit properties of extreme
assignments to show that MMAP inference can be performed by reducing the domain
of SOM-R variables to a single constant.We refer to our lifting technique as
the {\em SOM-R} rule for lifted MMAP. Combined with existing rules such as
decomposer and binomial, this results in a powerful framework for lifted MMAP.
Experiments on three benchmark domains show significant gains in both time and
memory compared to ground inference as well as lifted approaches not using
SOM-R.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vishal Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheikh_N/0/1/0/all/0/1&quot;&gt;Noman Ahmed Sheikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_H/0/1/0/all/0/1&quot;&gt;Happy Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gogate_V/0/1/0/all/0/1&quot;&gt;Vibhav Gogate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1&quot;&gt;Parag Singla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00643">
<title>Block-Value Symmetries in Probabilistic Graphical Models. (arXiv:1807.00643v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00643</link>
<description rdf:parseType="Literal">&lt;p&gt;Several lifted inference algorithms for probabilistic graphical models first
merge symmetric states into a single cluster (orbit) and then use these for
downstream inference, via variations of orbital MCMC [Niepert, 2012]. These
orbits are represented compactly using permutations over variables, and
variable-value (VV) pairs, but these can miss several state symmetries in a
domain.
&lt;/p&gt;
&lt;p&gt;We define the notion of permutations over block-value (BV) pairs, where a
block is a set of variables. BV strictly generalizes VV symmetries, and can
compute many more symmetries for increasing block sizes. To operationalize use
of BV permutations in lifted inference, we describe 1) an algorithm to compute
BV permutations given a block partition of the variables, 2) BV-MCMC, an
extension of orbital MCMC that can sample from BV orbits, and 3) a heuristic to
suggest good block partitions. Our experiments show that BV-MCMC can mix much
faster compared to vanilla MCMC and orbital MCMC over VV permutations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madan_G/0/1/0/all/0/1&quot;&gt;Gagan Madan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1&quot;&gt;Ankit Anand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1&quot;&gt;Mausam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1&quot;&gt;Parag Singla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00755">
<title>LeapsAndBounds: A Method for Approximately Optimal Algorithm Configuration. (arXiv:1807.00755v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00755</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of configuring general-purpose solvers to run
efficiently on problem instances drawn from an unknown distribution. The goal
of the configurator is to find a configuration that runs fast on average on
most instances, and do so with the least amount of total work. It can run a
chosen solver on a random instance until the solver finishes or a timeout is
reached. We propose LeapsAndBounds, an algorithm that tests configurations on
randomly selected problem instances for longer and longer time. We prove that
the capped expected runtime of the configuration returned by LeapsAndBounds is
close to the optimal expected runtime, while our algorithm&apos;s running time is
near-optimal. Our results show that LeapsAndBounds is more efficient than the
recent algorithm of Kleinberg et al. (2017), which, to our knowledge, is the
only other algorithm configuration method with non-trivial theoretical
guarantees. Experimental results on configuring a public SAT solver on a new
benchmark dataset also stand witness to the superiority of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weisz_G/0/1/0/all/0/1&quot;&gt;Gell&amp;#xe9;rt Weisz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe1;s Gy&amp;#xf6;rgy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1&quot;&gt;Csaba Szepesv&amp;#xe1;ri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00771">
<title>Path Finding for the Coalition of Co-operative Agents Acting in the Environment with Destructible Obstacles. (arXiv:1807.00771v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.00771</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of planning a set of paths for the coalition of robots (agents)
with different capabilities is considered in the paper. Some agents can modify
the environment by destructing the obstacles thus allowing the other ones to
shorten their paths to the goal. As a result the mutual solution of lower cost,
e.g. time to completion, may be acquired. We suggest an original procedure to
identify the obstacles for further removal that can be embedded into almost any
heuristic search planner (we use Theta*) and evaluate it empirically. Results
of the evaluation show that time-to-complete the mission can be decreased up to
9-12 % by utilizing the proposed technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreychuk_A/0/1/0/all/0/1&quot;&gt;Anton Andreychuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yakovlev_K/0/1/0/all/0/1&quot;&gt;Konstantin Yakovlev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.06564">
<title>The Continuous Hint Factory - Providing Hints in Vast and Sparsely Populated Edit Distance Spaces. (arXiv:1708.06564v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1708.06564</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent tutoring systems can support students in solving multi-step tasks
by providing hints regarding what to do next. However, engineering such
next-step hints manually or via an expert model becomes infeasible if the space
of possible states is too large. Therefore, several approaches have emerged to
infer next-step hints automatically, relying on past students&apos; data. In
particular, the Hint Factory (Barnes &amp;amp; Stamper, 2008) recommends edits that are
most likely to guide students from their current state towards a correct
solution, based on what successful students in the past have done in the same
situation. Still, the Hint Factory relies on student data being available for
any state a student might visit while solving the task, which is not the case
for some learning tasks, such as open-ended programming tasks. In this
contribution we provide a mathematical framework for edit-based hint policies
and, based on this theory, propose a novel hint policy to provide edit hints in
vast and sparsely populated state spaces. In particular, we extend the Hint
Factory by considering data of past students in all states which are similar to
the student&apos;s current state and creating hints approximating the weighted
average of all these reference states. Because the space of possible weighted
averages is continuous, we call this approach the Continuous Hint Factory. In
our experimental evaluation, we demonstrate that the Continuous Hint Factory
can predict more accurately what capable students would do compared to existing
prediction schemes on two learning tasks, especially in an open-ended
programming task, and that the Continuous Hint Factory is comparable to
existing hint policies at reproducing tutor hints on a simple UML diagram task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paassen_B/0/1/0/all/0/1&quot;&gt;Benjamin Paa&amp;#xdf;en&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1&quot;&gt;Barbara Hammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Price_T/0/1/0/all/0/1&quot;&gt;Thomas William Price&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_T/0/1/0/all/0/1&quot;&gt;Tiffany Barnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_S/0/1/0/all/0/1&quot;&gt;Sebastian Gross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinkwart_N/0/1/0/all/0/1&quot;&gt;Niels Pinkwart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06562">
<title>An Iterative Closest Points Approach to Neural Generative Models. (arXiv:1711.06562v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06562</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a simple way to learn a transformation that maps samples of one
distribution to the samples of another distribution. Our algorithm comprises an
iteration of 1) drawing samples from some simple distribution and transforming
them using a neural network, 2) determining pairwise correspondences between
the transformed samples and training data (or a minibatch), and 3) optimizing
the weights of the neural network being trained to minimize the distances
between the corresponding vectors. This can be considered as a variant of the
Iterative Closest Points (ICP) algorithm, common in geometric computer vision,
although ICP typically operates on sensor point clouds and linear transforms
instead of random sample sets and neural nonlinear transforms. We demonstrate
the algorithm on simple synthetic data and MNIST data. We furthermore
demonstrate that the algorithm is capable of handling distributions with both
continuous and discrete variables.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajamaki_J/0/1/0/all/0/1&quot;&gt;Joose Rajam&amp;#xe4;ki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamalainen_P/0/1/0/all/0/1&quot;&gt;Perttu H&amp;#xe4;m&amp;#xe4;l&amp;#xe4;inen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01412">
<title>A real-time decision support system for bridge management based on the rules generalized by CART decision tree and SMO algorithms. (arXiv:1803.01412v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01412</link>
<description rdf:parseType="Literal">&lt;p&gt;Under dynamic conditions on bridges, we need a real-time management. To this
end, this paper presents a rule-based decision support system in which the
necessary rules are extracted from simulation results made by Aimsun traffic
micro-simulation software. Then, these rules are generalized by the aid of
fuzzy rule generation algorithms. Then, they are trained by a set of supervised
and the unsupervised learning algorithms to get an ability to make decision in
real cases. As a pilot case study, Nasr Bridge in Tehran is simulated in Aimsun
and WEKA data mining software is used to execute the learning algorithms. Based
on this experiment, the accuracy of the supervised algorithms to generalize the
rules is greater than 80%. In addition, CART decision tree and sequential
minimal optimization (SMO) provides 100% accuracy for normal data and these
algorithms are so reliable for crisis management on bridge. This means that, it
is possible to use such machine learning methods to manage bridges in the
real-time conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abpeykar_S/0/1/0/all/0/1&quot;&gt;Shadi Abpeykar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghatee_M/0/1/0/all/0/1&quot;&gt;Mehdi Ghatee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07540">
<title>Enslaving the Algorithm: From a &quot;Right to an Explanation&quot; to a &quot;Right to Better Decisions&quot;?. (arXiv:1803.07540v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07540</link>
<description rdf:parseType="Literal">&lt;p&gt;As concerns about unfairness and discrimination in &quot;black box&quot; machine
learning systems rise, a legal &quot;right to an explanation&quot; has emerged as a
compellingly attractive approach for challenge and redress. We outline recent
debates on the limited provisions in European data protection law, and
introduce and analyze newer explanation rights in French administrative law and
the draft modernized Council of Europe Convention 108. While individual rights
can be useful, in privacy law they have historically unreasonably burdened the
average data subject. &quot;Meaningful information&quot; about algorithmic logics is more
technically possible than commonly thought, but this exacerbates a new
&quot;transparency fallacy&quot;---an illusion of remedy rather than anything
substantively helpful. While rights-based approaches deserve a firm place in
the toolbox, other forms of governance, such as impact assessments, &quot;soft law,&quot;
judicial review, and model repositories deserve more attention, alongside
catalyzing agencies acting for users to control algorithmic system design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwards_L/0/1/0/all/0/1&quot;&gt;Lilian Edwards&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veale_M/0/1/0/all/0/1&quot;&gt;Michael Veale&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00617">
<title>Specification-Driven Multi-Perspective Predictive Business Process Monitoring (Extended Version). (arXiv:1804.00617v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00617</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive analysis in business process monitoring aims at forecasting the
future information of a running business process. The prediction is typically
made based on the model extracted from historical process execution logs (event
logs). In practice, different business domains might require different kinds of
predictions. Hence, it is important to have a means for properly specifying the
desired prediction tasks, and a mechanism to deal with these various prediction
tasks. Although there have been many studies in this area, they mostly focus on
a specific prediction task. This work introduces a language for specifying the
desired prediction tasks, and this language allows us to express various kinds
of prediction tasks. This work also presents a mechanism for automatically
creating the corresponding prediction model based on the given specification.
Thus, different from previous studies, our approach enables us to deal with
various kinds of prediction tasks based on the given specification. A prototype
implementing our approach has been developed and experiments using a real-life
event log have been conducted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santoso_A/0/1/0/all/0/1&quot;&gt;Ario Santoso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10829">
<title>Formal Security Analysis of Neural Networks using Symbolic Intervals. (arXiv:1804.10829v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10829</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the increasing deployment of Deep Neural Networks (DNNs) in real-world
security-critical domains including autonomous vehicles and collision avoidance
systems, formally checking security properties of DNNs, especially under
different attacker capabilities, is becoming crucial. Most existing security
testing techniques for DNNs try to find adversarial examples without providing
any formal security guarantees about the non-existence of such adversarial
examples. Recently, several projects have used different types of
Satisfiability Modulo Theory (SMT) solvers to formally check security
properties of DNNs. However, all of these approaches are limited by the high
overhead caused by the solver.
&lt;/p&gt;
&lt;p&gt;In this paper, we present a new direction for formally checking security
properties of DNNs without using SMT solvers. Instead, we leverage interval
arithmetic to compute rigorous bounds on the DNN outputs. Our approach, unlike
existing solver-based approaches, is easily parallelizable. We further present
symbolic interval analysis along with several other optimizations to minimize
overestimations of output bounds.
&lt;/p&gt;
&lt;p&gt;We design, implement, and evaluate our approach as part of ReluVal, a system
for formally checking security properties of Relu-based DNNs. Our extensive
empirical results show that ReluVal outperforms Reluplex, a state-of-the-art
solver-based system, by 200 times on average. On a single 8-core machine
without GPUs, within 4 hours, ReluVal is able to verify a security property
that Reluplex deemed inconclusive due to timeout after running for more than 5
days. Our experiments demonstrate that symbolic interval analysis is a
promising new direction towards rigorously analyzing different security
properties of DNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_K/0/1/0/all/0/1&quot;&gt;Kexin Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitehouse_J/0/1/0/all/0/1&quot;&gt;Justin Whitehouse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Junfeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1&quot;&gt;Suman Jana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11324">
<title>Bayesian Inference with Anchored Ensembles of Neural Networks, and Application to Exploration in Reinforcement Learning. (arXiv:1805.11324v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11324</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of ensembles of neural networks (NNs) for the quantification of
predictive uncertainty is widespread. However, the current justification is
intuitive rather than analytical. This work proposes one minor modification to
the normal ensembling methodology, which we prove allows the ensemble to
perform Bayesian inference, hence converging to the corresponding Gaussian
Process as both the total number of NNs, and the size of each, tend to
infinity. This working paper provides early-stage results in a reinforcement
learning setting, analysing the practicality of the technique for an ensemble
of small, finite number. Using the uncertainty estimates produced by anchored
ensembles to govern the exploration-exploitation process results in steadier,
more stable learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pearce_T/0/1/0/all/0/1&quot;&gt;Tim Pearce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Anastassacos_N/0/1/0/all/0/1&quot;&gt;Nicolas Anastassacos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zaki_M/0/1/0/all/0/1&quot;&gt;Mohamed Zaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neely_A/0/1/0/all/0/1&quot;&gt;Andy Neely&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11704">
<title>Deep Semantic Architecture with discriminative feature visualization for neuroimage analysis. (arXiv:1805.11704v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11704</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuroimaging data analysis often involves \emph{a-priori} selection of data
features to study the underlying neural activity. Since this could lead to
sub-optimal feature selection and thereby prevent the detection of subtle
patterns in neural activity, data-driven methods have recently gained
popularity for optimizing neuroimaging data analysis pipelines and thereby,
improving our understanding of neural mechanisms. In this context, we developed
a deep convolutional architecture that can identify discriminating patterns in
neuroimaging data and applied it to electroencephalography (EEG) recordings
collected from 25 subjects performing a hand motor task before and after a rest
period or a bout of exercise. The deep network was trained to classify subjects
into exercise and control groups based on differences in their EEG signals.
Subsequently, we developed a novel method termed the cue-combination for Class
Activation Map (ccCAM), which enabled us to identify discriminating
spatio-temporal features within definite frequency bands (23--33 Hz) and assess
the effects of exercise on the brain. Additionally, the proposed architecture
allowed the visualization of the differences in the propagation of underlying
neural activity across the cortex between the two groups, for the first time in
our knowledge. Our results demonstrate the feasibility of using deep network
architectures for neuroimaging analysis in different contexts such as, for the
identification of robust brain biomarkers to better characterize and
potentially treat neurological disorders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Arna Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Maso_F/0/1/0/all/0/1&quot;&gt;Fabien dal Maso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Roig_M/0/1/0/all/0/1&quot;&gt;Marc Roig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mitsis_G/0/1/0/all/0/1&quot;&gt;Georgios D Mitsis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Boudrias_M/0/1/0/all/0/1&quot;&gt;Marie-H&amp;#xe9;l&amp;#xe8;ne Boudrias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11391">
<title>On embeddings as an alternative paradigm for relational learning. (arXiv:1806.11391v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.11391</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world domains can be expressed as graphs and, more generally, as
multi-relational knowledge graphs. Though reasoning and learning with knowledge
graphs has traditionally been addressed by symbolic approaches, recent methods
in (deep) representation learning has shown promising results for specialized
tasks such as knowledge base completion. These approaches abandon the
traditional symbolic paradigm by replacing symbols with vectors in Euclidean
space. With few exceptions, symbolic and distributional approaches are explored
in different communities and little is known about their respective strengths
and weaknesses. In this work, we compare representation learning and relational
learning on various relational classification and clustering tasks and analyse
the complexity of the rules used implicitly by these approaches. Preliminary
results reveal possible indicators that could help in choosing one approach
over the other for particular knowledge graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumancic_S/0/1/0/all/0/1&quot;&gt;Sebastijan Dumancic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Duran_A/0/1/0/all/0/1&quot;&gt;Alberto Garcia-Duran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00028">
<title>Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints. (arXiv:1807.00028v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00028</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifiers can be trained with data-dependent constraints to satisfy
fairness goals, reduce churn, achieve a targeted false positive rate, or other
policy goals. We study the generalization performance for such constrained
optimization problems, in terms of how well the constraints are satisfied at
evaluation time, given that they are satisfied at training time. To improve
generalization performance, we frame the problem as a two-player game where one
player optimizes the model parameters on a training dataset, and the other
player enforces the constraints on an independent validation dataset. We build
on recent work in two-player constrained optimization to show that if one uses
this two-dataset approach, then constraint generalization can be significantly
improved. As we illustrate experimentally, this approach works not only in
theory, but also in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotter_A/0/1/0/all/0/1&quot;&gt;Andrew Cotter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Maya Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Heinrich Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1&quot;&gt;Nathan Srebro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1&quot;&gt;Karthik Sridharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Serena Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1&quot;&gt;Blake Woodworth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Seungil You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00046">
<title>It All Matters: Reporting Accuracy, Inference Time and Power Consumption for Face Emotion Recognition on Embedded Systems. (arXiv:1807.00046v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.00046</link>
<description rdf:parseType="Literal">&lt;p&gt;While several approaches to face emotion recognition task are proposed in
literature, none of them reports on power consumption nor inference time
required to run the system in an embedded environment. Without adequate
knowledge about these factors it is not clear whether we are actually able to
provide accurate face emotion recognition in the embedded environment or not,
and if not, how far we are from making it feasible and what are the biggest
bottlenecks we face.
&lt;/p&gt;
&lt;p&gt;The main goal of this paper is to answer these questions and to convey the
message that instead of reporting only detection accuracy also power
consumption and inference time should be reported as real usability of the
proposed systems and their adoption in human computer interaction strongly
depends on it. In this paper, we identify the state-of-the art face emotion
recognition methods that are potentially suitable for embedded environment and
the most frequently used datasets for this task. Our study shows that most of
the performed experiments use datasets with posed expressions or in a
particular experimental setup with special conditions for image collection.
Since our goal is to evaluate the performance of the identified promising
methods in the realistic scenario, we collect a new dataset with
non-exaggerated emotions and we use it, in addition to the publicly available
datasets, for the evaluation of detection accuracy, power consumption and
inference time on three frequently used embedded devices with different
computational capabilities. Our results show that gray images are still more
suitable for embedded environment than color ones and that for most of the
analyzed systems either inference time or energy consumption or both are
limiting factor for their adoption in real-life embedded applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milosevic_J/0/1/0/all/0/1&quot;&gt;Jelena Milosevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pena_D/0/1/0/all/0/1&quot;&gt;Dexmont Pena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forembsky_A/0/1/0/all/0/1&quot;&gt;Andrew Forembsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moloney_D/0/1/0/all/0/1&quot;&gt;David Moloney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malek_M/0/1/0/all/0/1&quot;&gt;Miroslaw Malek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00068">
<title>Fully Nonparametric Bayesian Additive Regression Trees. (arXiv:1807.00068v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.00068</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian Additive Regression Trees (BART) is fully Bayesian approach to
modeling with ensembles of trees. BART can uncover complex regression functions
with high dimensional regressors in a fairly automatic way and provide Bayesian
quantification of the uncertainty through posterior. However, BART assumes IID
normal errors. This strong parametric assumption can lead to misleading
inference and predictive uncertainty. In this paper we use the classic
Dirichlet process mixture (DPM) mechanism to nonparametrically model the error
distribution. A key strength of BART is that default priors work reasonable
well in a variety of problems. The challenge in extending BART is to choose
hyperparameters of the DPM so that the strengths of standard BART approach is
not lost when the errors are close to normal, but the DPM has the ability to
adapt to non-normal errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+George_E/0/1/0/all/0/1&quot;&gt;Edward George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laud_P/0/1/0/all/0/1&quot;&gt;Prakash Laud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Logan_B/0/1/0/all/0/1&quot;&gt;Brent Logan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McCulloch_R/0/1/0/all/0/1&quot;&gt;Robert McCulloch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sparapani_R/0/1/0/all/0/1&quot;&gt;Rodney Sparapani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00084">
<title>A Learning Theory in Linear Systems under Compositional Models. (arXiv:1807.00084v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.00084</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a learning theory for the training of a linear system operator
having an input compositional variable and propose a Bayesian inversion method
for inferring the unknown variable from an output of a noisy linear system. We
assume that we have partial or even no knowledge of the operator but have
training data of input and ouput. A compositional variable satisfies the
constraints that the elements of the variable are all non-negative and sum to
unity. We quantified the uncertainty in the trained operator and present the
convergence rates of training in explicit forms for several interesting cases
under stochastic compositional models. The trained linear operator with the
covariance matrix, estimated from the training set of pairs of ground-truth
input and noisy output data, is further used in evaluation of posterior
uncertainty of the solution. This posterior uncertainty clearly demonstrates
uncertainty propagation from noisy training data and addresses possible
mismatch between the true operator and the estimated one in the final solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Se Un Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00122">
<title>A Constrained Coupled Matrix-Tensor Factorization for Learning Time-evolving and Emerging Topics. (arXiv:1807.00122v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.00122</link>
<description rdf:parseType="Literal">&lt;p&gt;Topic discovery has witnessed a significant growth as a field of data mining
at large. In particular, time-evolving topic discovery, where the evolution of
a topic is taken into account has been instrumental in understanding the
historical context of an emerging topic in a dynamic corpus. Traditionally,
time-evolving topic discovery has focused on this notion of time. However,
especially in settings where content is contributed by a community or a crowd,
an orthogonal notion of time is the one that pertains to the level of expertise
of the content creator: the more experienced the creator, the more advanced the
topic. In this paper, we propose a novel time-evolving topic discovery method
which, in addition to the extracted topics, is able to identify the evolution
of that topic over time, as well as the level of difficulty of that topic, as
it is inferred by the level of expertise of its main contributors. Our method
is based on a novel formulation of Constrained Coupled Matrix-Tensor
Factorization, which adopts constraints well-motivated for, and, as we
demonstrate, are essential for high-quality topic discovery. We qualitatively
evaluate our approach using real data from the Physics and also Programming
Stack Exchange forum, and we were able to identify topics of varying levels of
difficulty which can be linked to external events, such as the announcement of
gravitational waves by the LIGO lab in Physics forum. We provide a quantitative
evaluation of our method by conducting a user study where experts were asked to
judge the coherence and quality of the extracted topics. Finally, our proposed
method has implications for automatic curriculum design using the extracted
topics, where the notion of the level of difficulty is necessary for the proper
modeling of prerequisites and advanced concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahargam_S/0/1/0/all/0/1&quot;&gt;Sanaz Bahargam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1&quot;&gt;Evangelos E. Papalexakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00123">
<title>Machine Learning for Integrating Data in Biology and Medicine: Principles, Practice, and Opportunities. (arXiv:1807.00123v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1807.00123</link>
<description rdf:parseType="Literal">&lt;p&gt;New technologies have enabled the investigation of biology and human health
at an unprecedented scale and in multiple dimensions. These dimensions include
myriad properties describing genome, epigenome, transcriptome, microbiome,
phenotype, and lifestyle. No single data type, however, can capture the
complexity of all the factors relevant to understanding a phenomenon such as a
disease. Integrative methods that combine data from multiple technologies have
thus emerged as critical statistical and computational approaches. The key
challenge in developing such approaches is the identification of effective
models to provide a comprehensive and relevant systems view. An ideal method
can answer a biological or medical question, identifying important features and
predicting outcomes, by harnessing heterogeneous data across several dimensions
of biological variation. In this Review, we describe the principles of data
integration and discuss current methods and available implementations. We
provide examples of successful data integration in biology and medicine.
Finally, we discuss current challenges in biomedical integrative methods and
our perspective on the future development of the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zitnik_M/0/1/0/all/0/1&quot;&gt;Marinka Zitnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nguyen_F/0/1/0/all/0/1&quot;&gt;Francis Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Goldenberg_A/0/1/0/all/0/1&quot;&gt;Anna Goldenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hoffman_M/0/1/0/all/0/1&quot;&gt;Michael M. Hoffman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00126">
<title>A New Benchmark and Progress Toward Improved Weakly Supervised Learning. (arXiv:1807.00126v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00126</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Matters: Importance of Prior Information for Optimization [7], by
Gulcehre et. al., sought to establish the limits of current black-box, deep
learning techniques by posing problems which are difficult to learn without
engineering knowledge into the model or training procedure. In our work, we
completely solve the previous Knowledge Matters problem using a generic model,
pose a more difficult and scalable problem, All-Pairs, and advance this new
problem by introducing a new learned, spatially-varying histogram model called
TypeNet which outperforms conventional models on the problem. We present
results on All-Pairs where our model achieves 100% test accuracy while the best
ResNet models achieve 79% accuracy. In addition, our model is more than an
order of magnitude smaller than Resnet-34. The challenge of solving
larger-scale All-Pairs problems with high accuracy is presented to the
community for investigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1&quot;&gt;Jason Ramapuram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_R/0/1/0/all/0/1&quot;&gt;Russ Webb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00130">
<title>Game-Theoretic Interpretability for Temporal Modeling. (arXiv:1807.00130v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00130</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretability has arisen as a key desideratum of machine learning models
alongside performance. Approaches so far have been primarily concerned with
fixed dimensional inputs emphasizing feature relevance or selection. In
contrast, we focus on temporal modeling and the problem of tailoring the
predictor, functionally, towards an interpretable family. To this end, we
propose a co-operative game between the predictor and an explainer without any
a priori restrictions on the functional class of the predictor. The goal of the
explainer is to highlight, locally, how well the predictor conforms to the
chosen interpretable family of temporal models. Our co-operative game is setup
asymmetrically in terms of information sets for efficiency reasons. We develop
and illustrate the framework in the context of temporal sequence models with
examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Guang-He Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_Melis_D/0/1/0/all/0/1&quot;&gt;David Alvarez-Melis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1&quot;&gt;Tommi S. Jaakkola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00243">
<title>chemmodlab: A Cheminformatics Modeling Laboratory for Fitting and Assessing Machine Learning Models. (arXiv:1807.00243v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.00243</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of chemmodlab is to streamline the fitting and assessment pipeline
for many machine learning models in R, making it easy for researchers to
compare the utility of new models. While focused on implementing methods for
model fitting and assessment that have been accepted by experts in the
cheminformatics field, all of the methods in chemmodlab have broad utility for
the machine learning community. chemmodlab contains several assessment
utilities including a plotting function that constructs accumulation curves and
a function that computes many performance measures. The most novel feature of
chemmodlab is the ease with which statistically significant performance
differences for many machine learning models is presented by means of the
multiple comparisons similarity plot. Differences are assessed using repeated
k-fold cross validation where blocking increases precision and multiplicity
adjustments are applied.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hughes_Oliver_J/0/1/0/all/0/1&quot;&gt;Jeremy R. Ash Jacqueline M. Hughes-Oliver&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00263">
<title>Accurate Uncertainties for Deep Learning Using Calibrated Regression. (arXiv:1807.00263v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00263</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods for reasoning under uncertainty are a key building block of accurate
and reliable machine learning systems. Bayesian methods provide a general
framework to quantify uncertainty. However, because of model misspecification
and the use of approximate inference, Bayesian uncertainty estimates are often
inaccurate -- for example, a 90% credible interval may not contain the true
outcome 90% of the time. Here, we propose a simple procedure for calibrating
any regression algorithm; when applied to Bayesian and probabilistic models, it
is guaranteed to produce calibrated uncertainty estimates given enough data.
Our procedure is inspired by Platt scaling and extends previous work on
classification. We evaluate this approach on Bayesian linear regression,
feedforward, and recurrent neural networks, and find that it consistently
outputs well-calibrated credible intervals while improving performance on time
series forecasting and model-based reinforcement learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuleshov_V/0/1/0/all/0/1&quot;&gt;Volodymyr Kuleshov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fenner_N/0/1/0/all/0/1&quot;&gt;Nathan Fenner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00311">
<title>Product-based Neural Networks for User Response Prediction over Multi-field Categorical Data. (arXiv:1807.00311v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.00311</link>
<description rdf:parseType="Literal">&lt;p&gt;User response prediction is a crucial component for personalized information
retrieval and filtering scenarios, such as recommender system and web search.
The data in user response prediction is mostly in a multi-field categorical
format and transformed into sparse representations via one-hot encoding. Due to
the sparsity problems in representation and optimization, most research focuses
on feature engineering and shallow modeling. Recently, deep neural networks
have attracted research attention on such a problem for their high capacity and
end-to-end training scheme. In this paper, we study user response prediction in
the scenario of click prediction. We first analyze a coupled gradient issue in
latent vector-based models and propose kernel product to learn field-aware
feature interactions. Then we discuss an insensitive gradient issue in
DNN-based models and propose Product-based Neural Network (PNN) which adopts a
feature extractor to explore feature interactions. Generalizing the kernel
product to a net-in-net architecture, we further propose Product-network In
Network (PIN) which can generalize previous models. Extensive experiments on 4
industrial datasets and 1 contest dataset demonstrate that our models
consistently outperform 8 baselines on both AUC and log loss. Besides, PIN
makes great CTR improvement (relatively 34.67%) in online A/B test.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1&quot;&gt;Yanru Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1&quot;&gt;Bohui Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1&quot;&gt;Ruiming Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1&quot;&gt;Minzhe Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Huifeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiuqiang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00325">
<title>Data-driven satisficing measure and ranking. (arXiv:1807.00325v1 [math.OC])</title>
<link>http://arxiv.org/abs/1807.00325</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an computational framework for real-time risk assessment and
prioritizing for random outcomes without prior information on probability
distributions. The basic model is built based on satisficing measure (SM) which
yields a single index for risk comparison. Since SM is a dual representation
for a family of risk measures, we consider problems constrained by general
convex risk measures and specifically by Conditional value-at-risk. Starting
from offline optimization, we apply sample average approximation technique and
argue the convergence rate and validation of optimal solutions. In online
stochastic optimization case, we develop primal-dual stochastic approximation
algorithms respectively for general risk constrained problems, and derive their
regret bounds. For both offline and online cases, we illustrate the
relationship between risk ranking accuracy with sample size (or iterations).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenjie Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00349">
<title>Heuristic Framework for Multi-Scale Testing of the Multi-Manifold Hypothesis. (arXiv:1807.00349v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.00349</link>
<description rdf:parseType="Literal">&lt;p&gt;When analyzing empirical data, we often find that global linear models
overestimate the number of parameters required. In such cases, we may ask
whether the data lies on or near a manifold or a set of manifolds (a so-called
multi-manifold) of lower dimension than the ambient space. This question can be
phrased as a (multi-) manifold hypothesis. The identification of such intrinsic
multiscale features is a cornerstone of data analysis and representation and
has given rise to a large body of work on manifold learning. In this work, we
review key results on multi-scale data analysis and intrinsic dimension
followed by the introduction of a heuristic, multiscale framework for testing
the multi-manifold hypothesis. Our method implements a hypothesis test on a set
of spline-interpolated manifolds constructed from variance-based intrinsic
dimensions. The workflow is suitable for empirical data analysis as we
demonstrate on two use cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Medina_F/0/1/0/all/0/1&quot;&gt;F. Patricia Medina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ness_L/0/1/0/all/0/1&quot;&gt;Linda Ness&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weber_M/0/1/0/all/0/1&quot;&gt;Melanie Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Djima_K/0/1/0/all/0/1&quot;&gt;Karamatou Yacoubou Djima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00373">
<title>New Heuristics for Parallel and Scalable Bayesian Optimization. (arXiv:1807.00373v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1807.00373</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization has emerged as a strong candidate tool for global
optimization of functions with expensive evaluation costs. However, due to the
dynamic nature of research in Bayesian approaches, and the evolution of
computing technology, using Bayesian optimization in a parallel computing
environment remains a challenge for the non-expert. In this report, I review
the state-of-the-art in parallel and scalable Bayesian optimization methods. In
addition, I propose practical ways to avoid a few of the pitfalls of Bayesian
optimization, such as oversampling of edge parameters and over-exploitation of
high performance parameters. Finally, I provide relatively simple, heuristic
algorithms, along with their open source software implementations, that can be
immediately and easily deployed in any computing environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rubin_R/0/1/0/all/0/1&quot;&gt;Ran Rubin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00374">
<title>Augmented Cyclic Adversarial Learning for Domain Adaptation. (arXiv:1807.00374v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00374</link>
<description rdf:parseType="Literal">&lt;p&gt;Training a model to perform a task typically requires a large amount of data
from the domains in which the task will be applied. However, it is often the
case that data are abundant in some domains but scarce in others. Domain
adaptation deals with the challenge of adapting a model trained from a
data-rich source domain to perform well in a data-poor target domain. In
general, this requires learning plausible mappings between domains. CycleGAN is
a powerful framework that efficiently learns to map inputs from one domain to
another using adversarial training and a cycle-consistency constraint. However,
the conventional approach of enforcing cycle-consistency via reconstruction may
be overly restrictive in cases where one or more domains have limited training
data. In this paper, we propose an augmented cyclic adversarial learning model
that enforces the cycle-consistency constraint through an external task
specific model, which encourages the preservation of task-relevant
\textit{content} as opposed to exact reconstruction. This task specific model
both relaxes the cycle-consistency constraint and complements the role of the
discriminator during training, serving as an augmented information source for
learning the mapping. In the experiment, we adopt a speech recognition model
from each domain as the task specific model. Our approach improves absolute
performance of speech recognition by 2\% for female speakers in the TIMIT
dataset, where the majority of training samples are from male voices. We also
explore digit classification with MNIST and SVHN in a low-resource setting and
show that our approach improves absolute performance by 14\% and 4\% when
adapting SVHN to MNIST and vice versa, respectively. Our approach also
outperforms unsupervised domain adaptation methods, which require high-resource
unlabeled target domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_Asl_E/0/1/0/all/0/1&quot;&gt;Ehsan Hosseini-Asl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingbo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00431">
<title>Confounding variables can degrade generalization performance of radiological deep learning models. (arXiv:1807.00431v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.00431</link>
<description rdf:parseType="Literal">&lt;p&gt;Early results in using convolutional neural networks (CNNs) on x-rays to
diagnose disease have been promising, but it has not yet been shown that models
trained on x-rays from one hospital or one group of hospitals will work equally
well at different hospitals. Before these tools are used for computer-aided
diagnosis in real-world clinical settings, we must verify their ability to
generalize across a variety of hospital systems. A cross-sectional design was
used to train and evaluate pneumonia screening CNNs on 158,323 chest x-rays
from NIH (n=112,120 from 30,805 patients), Mount Sinai (42,396 from 12,904
patients), and Indiana (n=3,807 from 3,683 patients). In 3 / 5 natural
comparisons, performance on chest x-rays from outside hospitals was
significantly lower than on held-out x-rays from the original hospital systems.
CNNs were able to detect where an x-ray was acquired (hospital system, hospital
department) with extremely high accuracy and calibrate predictions accordingly.
The performance of CNNs in diagnosing diseases on x-rays may reflect not only
their ability to identify disease-specific imaging findings on x-rays, but also
their ability to exploit confounding information. Estimates of CNN performance
based on test data from hospital systems used for model training may overstate
their likely real-world performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zech_J/0/1/0/all/0/1&quot;&gt;John R. Zech&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badgeley_M/0/1/0/all/0/1&quot;&gt;Marcus A. Badgeley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Manway Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1&quot;&gt;Anthony B. Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Titano_J/0/1/0/all/0/1&quot;&gt;Joseph J. Titano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oermann_E/0/1/0/all/0/1&quot;&gt;Eric K. Oermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00448">
<title>Speeding up the Metabolism in E-commerce by Reinforcement Mechanism Design. (arXiv:1807.00448v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00448</link>
<description rdf:parseType="Literal">&lt;p&gt;In a large E-commerce platform, all the participants compete for impressions
under the allocation mechanism of the platform. Existing methods mainly focus
on the short-term return based on the current observations instead of the
long-term return. In this paper, we formally establish the lifecycle model for
products, by defining the introduction, growth, maturity and decline stages and
their transitions throughout the whole life period. Based on such model, we
further propose a reinforcement learning based mechanism design framework for
impression allocation, which incorporates the first principal component based
permutation and the novel experiences generation method, to maximize short-term
as well as long-term return of the platform. With the power of trial-and-error,
it is possible to optimize impression allocation strategies globally which is
contribute to the healthy development of participants and the platform itself.
We evaluate our algorithm on a simulated environment built based on one of the
largest E-commerce platforms, and a significant improvement has been achieved
in comparison with the baseline solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hua-Lin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Chun-Xiang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1&quot;&gt;Qing Da&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;An-Xiang Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00451">
<title>Multi-distance Support Matrix Machines. (arXiv:1807.00451v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.00451</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world data such as digital images, MRI scans and electroencephalography
signals are naturally represented as matrices with structural information. Most
existing classifiers aim to capture these structures by regularizing the
regression matrix to be low-rank or sparse. Some other methodologies introduce
factorization technique to explore nonlinear relationships of matrix data in
kernel space. In this paper, we propose a multi-distance support matrix machine
(MDSMM), which provides a principled way of solving matrix classification
problems. The multi-distance is introduced to capture the correlation within
matrix data, by means of intrinsic information in rows and columns of input
data. A complex hyperplane is established upon these values to separate
distinct classes. We further study the generalization bounds for i.i.d.
processes and non i.i.d. process based on both SVM and SMM classifiers. For
typical hypothesis classes where matrix norms are constrained, MDSMM achieves a
faster learning rate than traditional classifiers. We also provide a more
general approach for samples without prior knowledge. We demonstrate the merits
of the proposed method by conducting exhaustive experiments on both simulation
study and a number of real-word datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yunfei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Dong Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00546">
<title>Clustering with Temporal Constraints on Spatio-Temporal Data of Human Mobility. (arXiv:1807.00546v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00546</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting significant places or places of interest (POIs) using individuals&apos;
spatio-temporal data is of fundamental importance for human mobility analysis.
Classical clustering methods have been used in prior work for detecting POIs,
but without considering temporal constraints. Usually, the involved parameters
for clustering are difficult to determine, e.g., the optimal cluster number in
hierarchical clustering. Currently, researchers either choose heuristic values
or use spatial distance-based optimization to determine an appropriate
parameter set. We argue that existing research does not optimally address
temporal information and thus leaves much room for improvement. Considering
temporal constraints in human mobility, we introduce an effective clustering
approach - namely POI clustering with temporal constraints (PC-TC) - to extract
POIs from spatio-temporal data of human mobility. Following human mobility
nature in modern society, our approach aims to extract both global POIs (e.g.,
workplace or university) and local POIs (e.g., library, lab, and canteen).
Based on two publicly available datasets including 193 individuals, our
evaluation results show that PC-TC has much potential for next place prediction
in terms of granularity (i.e., the number of extracted POIs) and
predictability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunlong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommer_B/0/1/0/all/0/1&quot;&gt;Bjoern Sommer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schreiber_F/0/1/0/all/0/1&quot;&gt;Falk Schreiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reiterer_H/0/1/0/all/0/1&quot;&gt;Harald Reiterer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00558">
<title>Relational Constraints for Metric Learning on Relational Data. (arXiv:1807.00558v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00558</link>
<description rdf:parseType="Literal">&lt;p&gt;Most of metric learning approaches are dedicated to be applied on data
described by feature vectors, with some notable exceptions such as times
series, trees or graphs. The objective of this paper is to propose a metric
learning algorithm that specifically considers relational data. The proposed
approach can take benefit from both the topological structure of the data and
supervised labels. For selecting relative constraints representing the
relational information, we introduce a link-strength function that measures the
strength of relationship links between entities by the side-information of
their common parents. We show the performance of the proposed method with two
different classical metric learning algorithms, which are ITML (Information
Theoretic Metric Learning) and LSML (Least Squares Metric Learning), and test
on several real-world datasets. Experimental results show that using relational
information improves the quality of the learned metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiajun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Capitaine_H/0/1/0/all/0/1&quot;&gt;Hoel Le Capitaine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leray_P/0/1/0/all/0/1&quot;&gt;Philippe Leray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00583">
<title>Sample Efficient Semantic Segmentation using Rotation Equivariant Convolutional Networks. (arXiv:1807.00583v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.00583</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a semantic segmentation model that exploits rotation and
reflection symmetries. We demonstrate significant gains in sample efficiency
due to increased weight sharing, as well as improvements in robustness to
symmetry transformations. The group equivariant CNN framework is extended for
segmentation by introducing a new equivariant (G-&amp;gt;Z2)-convolution that
transforms feature maps on a group to planar feature maps. Also, equivariant
transposed convolution is formulated for up-sampling in an encoder-decoder
network. To demonstrate improvements in sample efficiency we evaluate on
multiple data regimes of a rotation-equivariant segmentation task: cancer
metastases detection in histopathology images. We further show the
effectiveness of exploiting more symmetries by varying the size of the group.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linmans_J/0/1/0/all/0/1&quot;&gt;Jasper Linmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winkens_J/0/1/0/all/0/1&quot;&gt;Jim Winkens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeling_B/0/1/0/all/0/1&quot;&gt;Bastiaan S. Veeling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1&quot;&gt;Taco S. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00595">
<title>Logical Explanations for Deep Relational Machines Using Relevance Information. (arXiv:1807.00595v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00595</link>
<description rdf:parseType="Literal">&lt;p&gt;Our interest in this paper is in the construction of symbolic explanations
for predictions made by a deep neural network. We will focus attention on deep
relational machines (DRMs, first proposed by H. Lodhi). A DRM is a deep network
in which the input layer consists of Boolean-valued functions (features) that
are defined in terms of relations provided as domain, or background, knowledge.
Our DRMs differ from those proposed by Lodhi, which use an Inductive Logic
Programming (ILP) engine to first select features (we use random selections
from a space of features that satisfies some approximate constraints on logical
relevance and non-redundancy). But why do the DRMs predict what they do? One
way of answering this is the LIME setting, in which readable proxies for a
black-box predictor. The proxies are intended only to model the predictions of
the black-box in local regions of the instance-space. But readability alone may
not enough: to be understandable, the local models must use relevant concepts
in an meaningful manner. We investigate the use of a Bayes-like approach to
identify logical proxies for local predictions of a DRM. We show: (a) DRM&apos;s
with our randomised propositionalization method achieve state-of-the-art
predictive performance; (b) Models in first-order logic can approximate the
DRM&apos;s prediction closely in a small local region; and (c) Expert-provided
relevance information can play the role of a prior to distinguish between
logical explanations that perform equivalently on prediction alone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1&quot;&gt;Ashwin Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vig_L/0/1/0/all/0/1&quot;&gt;Lovekesh Vig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bain_M/0/1/0/all/0/1&quot;&gt;Michael Bain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00636">
<title>Adaptation to Easy Data in Prediction with Limited Advice. (arXiv:1807.00636v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00636</link>
<description rdf:parseType="Literal">&lt;p&gt;We derive an online learning algorithm with improved regret guarantees for
&quot;easy&quot; loss sequences. We consider two types of &quot;easiness&quot;: (a) stochastic loss
sequences and (b) adversarial loss sequences with small effective range of the
losses. While a number of algorithms have been proposed for exploiting small
effective range in the full information setting, Gerchinovitz and Lattimore
[2016] have shown the impossibility of regret scaling with the effective range
of the losses in the bandit setting. We show that just one additional
observation per round is sufficient to bypass the impossibility result. The
proposed Second Order Difference Adjustments (SODA) algorithm requires no prior
knowledge of the effective range of the losses, $\varepsilon$, and achieves an
$O(\varepsilon \sqrt{KT \ln K}) + \tilde{O}(\varepsilon K \sqrt[4]{T})$
expected regret guarantee, where $T$ is the time horizon and $K$ is the number
of actions. The scaling with the effective loss range is achieved under
significantly weaker assumptions than those made by Cesa-Bianchi and Shamir
[2018] in an earlier attempt to bypass the impossibility result. We also
provide regret lower bound of $\Omega(\varepsilon\sqrt{T K})$, which almost
matches the upper bound. In addition, we show that in the stochastic setting
SODA achieves an $O\left(\sum_{a:\Delta_a&amp;gt;0}
\frac{K\varepsilon^2}{\Delta_a}\right)$ pseudo-regret bound that holds
simultaneously with the adversarial regret guarantee. In other words, SODA is
safe against an unrestricted oblivious adversary and provides improved regret
guarantees for at least two different types of &quot;easiness&quot; simultaneously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thune_T/0/1/0/all/0/1&quot;&gt;Tobias Sommer Thune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seldin_Y/0/1/0/all/0/1&quot;&gt;Yevgeny Seldin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00692">
<title>Grapevine: A Wine Prediction Algorithm Using Multi-dimensional Clustering Methods. (arXiv:1807.00692v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.00692</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method for a wine recommendation system that employs
multidimensional clustering and unsupervised learning methods. Our algorithm
first performs clustering on a large corpus of wine reviews. It then uses the
resulting wine clusters as an approximation of the most common flavor palates,
recommending a user a wine by optimizing over a price-quality ratio within
clusters that they demonstrated a preference for.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_R/0/1/0/all/0/1&quot;&gt;Richard Diehl Martinez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angus_G/0/1/0/all/0/1&quot;&gt;Geoffrey Angus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdavian_R/0/1/0/all/0/1&quot;&gt;Rooz Mahdavian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00787">
<title>A Unified Approach to Quantifying Algorithmic Unfairness: Measuring Individual &amp; Group Unfairness via Inequality Indices. (arXiv:1807.00787v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00787</link>
<description rdf:parseType="Literal">&lt;p&gt;Discrimination via algorithmic decision making has received considerable
attention. Prior work largely focuses on defining conditions for fairness, but
does not define satisfactory measures of algorithmic unfairness. In this paper,
we focus on the following question: Given two unfair algorithms, how should we
determine which of the two is more unfair? Our core idea is to use existing
inequality indices from economics to measure how unequally the outcomes of an
algorithm benefit different individuals or groups in a population. Our work
offers a justified and general framework to compare and contrast the
(un)fairness of algorithmic predictors. This unifying approach enables us to
quantify unfairness both at the individual and the group level. Further, our
work reveals overlooked tradeoffs between different fairness notions: using our
proposed measures, the overall individual-level unfairness of an algorithm can
be decomposed into a between-group and a within-group component. Earlier
methods are typically designed to tackle only between-group unfairness, which
may be justified for legal or other reasons. However, we demonstrate that
minimizing exclusively the between-group component may, in fact, increase the
within-group, and hence the overall unfairness. We characterize and illustrate
the tradeoffs between our measures of (un)fairness and the prediction accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Speicher_T/0/1/0/all/0/1&quot;&gt;Till Speicher&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1&quot;&gt;Hoda Heidari&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grgic_Hlaca_N/0/1/0/all/0/1&quot;&gt;Nina Grgic-Hlaca&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gummadi_K/0/1/0/all/0/1&quot;&gt;Krishna P. Gummadi&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1&quot;&gt;Adish Singla&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt; (3 and 4), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1&quot;&gt;Muhammad Bilal Zafar&lt;/a&gt; (1) ((1) MPI-SWS, (2) ETH Zurich, (3) University of Cambridge, (4) The Alan Turing Institute)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00801">
<title>Deepcode: Feedback Codes via Deep Learning. (arXiv:1807.00801v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.00801</link>
<description rdf:parseType="Literal">&lt;p&gt;The design of codes for communicating reliably over a statistically well
defined channel is an important endeavor involving deep mathematical research
and wide-ranging practical applications. In this work, we present the first
family of codes obtained via deep learning, which significantly beats
state-of-the-art codes designed over several decades of research. The
communication channel under consideration is the Gaussian noise channel with
feedback, whose study was initiated by Shannon; feedback is known theoretically
to improve reliability of communication, but no practical codes that do so have
ever been successfully constructed.
&lt;/p&gt;
&lt;p&gt;We break this logjam by integrating information theoretic insights
harmoniously with recurrent-neural-network based encoders and decoders to
create novel codes that outperform known codes by 3 orders of magnitude in
reliability. We also demonstrate several desirable properties of the codes: (a)
generalization to larger block lengths, (b) composability with known codes, (c)
adaptation to practical constraints. This result also has broader ramifications
for coding theory: even when the channel has a clear mathematical model, deep
learning methodologies, when combined with channel-specific
information-theoretic insights, can potentially beat state-of-the-art codes
constructed over decades of mathematical research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyeji Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yihan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_S/0/1/0/all/0/1&quot;&gt;Sreeram Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Sewoong Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viswanath_P/0/1/0/all/0/1&quot;&gt;Pramod Viswanath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1502.00727">
<title>Laplacian Mixture Modeling for Network Analysis and Unsupervised Learning on Graphs. (arXiv:1502.00727v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1502.00727</link>
<description rdf:parseType="Literal">&lt;p&gt;Laplacian mixture models identify overlapping regions of influence in
unlabeled graph and network data in a scalable and computationally efficient
way, yielding useful low-dimensional representations. By combining Laplacian
eigenspace and finite mixture modeling methods, they provide probabilistic or
fuzzy dimensionality reductions or domain decompositions for a variety of input
data types, including mixture distributions, feature vectors, and graphs or
networks. Provable optimal recovery using the algorithm is analytically shown
for a nontrivial class of noisy cluster graphs. Heuristic approximations for
scalable high-performance implementations are described and empirically tested.
Connections to PageRank and community detection in network analysis demonstrate
the wide applicability of this approach. The origins of fuzzy spectral methods,
beginning with generalized heat or diffusion equations in physics, are reviewed
and summarized. Comparisons to other dimensionality reduction and clustering
methods for challenging unsupervised machine learning problems are also
discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Korenblum_D/0/1/0/all/0/1&quot;&gt;Daniel Korenblum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.03425">
<title>Statistics of Robust Optimization: A Generalized Empirical Likelihood Approach. (arXiv:1610.03425v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.03425</link>
<description rdf:parseType="Literal">&lt;p&gt;We study statistical inference and distributionally robust solution methods
for stochastic optimization problems, focusing on confidence intervals for
optimal values and solutions that achieve exact coverage asymptotically. We
develop a generalized empirical likelihood framework---based on distributional
uncertainty sets constructed from nonparametric $f$-divergence balls---for
Hadamard differentiable functionals, and in particular, stochastic optimization
problems. As consequences of this theory, we provide a principled method for
choosing the size of distributional uncertainty regions to provide one- and
two-sided confidence intervals that achieve exact coverage. We also give an
asymptotic expansion for our distributionally robust formulation, showing how
robustification regularizes problems by their variance. Finally, we show that
optimizers of the distributionally robust formulations we study enjoy
(essentially) the same consistency properties as those in classical sample
average approximations. Our general approach applies to quickly mixing
stationary sequences, including geometrically ergodic Harris recurrent Markov
chains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duchi_J/0/1/0/all/0/1&quot;&gt;John Duchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Glynn_P/0/1/0/all/0/1&quot;&gt;Peter Glynn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Namkoong_H/0/1/0/all/0/1&quot;&gt;Hongseok Namkoong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.05230">
<title>Surrogate Aided Unsupervised Recovery of Sparse Signals in Single Index Models for Binary Outcomes. (arXiv:1701.05230v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1701.05230</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the recovery of regression coefficients, denoted by
$\boldsymbol{\beta}_0$, for a single index model (SIM) relating a binary
outcome $Y$ to a set of possibly high dimensional covariates $\boldsymbol{X}$,
based on a large but &apos;unlabeled&apos; dataset $\mathcal{U}$, with $Y$ never
observed. On $\mathcal{U}$, we fully observe $\boldsymbol{X}$ and additionally,
a surrogate $S$ which, while not being strongly predictive of $Y$ throughout
the entirety of its support, can forecast it with high accuracy when it assumes
extreme values. Such datasets arise naturally in modern studies involving large
databases such as electronic medical records (EMR) where $Y$, unlike
$(\boldsymbol{X}, S)$, is difficult and/or expensive to obtain. In EMR studies,
an example of $Y$ and $S$ would be the true disease phenotype and the count of
the associated diagnostic codes respectively. Assuming another SIM for $S$
given $\boldsymbol{X}$, we show that under sparsity assumptions, we can recover
$\boldsymbol{\beta}_0$ proportionally by simply fitting a least squares LASSO
estimator to the subset of the observed data on $(\boldsymbol{X}, S)$
restricted to the extreme sets of $S$, with $Y$ imputed using the surrogacy of
$S$. We obtain sharp finite sample performance bounds for our estimator,
including deterministic deviation bounds and probabilistic guarantees. We
demonstrate the effectiveness of our approach through multiple simulation
studies, as well as by application to real data from an EMR study conducted at
the Partners HealthCare Systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chakrabortty_A/0/1/0/all/0/1&quot;&gt;Abhishek Chakrabortty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neykov_M/0/1/0/all/0/1&quot;&gt;Matey Neykov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carroll_R/0/1/0/all/0/1&quot;&gt;Raymond Carroll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1&quot;&gt;Tianxi Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.06131">
<title>Inference via low-dimensional couplings. (arXiv:1703.06131v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1703.06131</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the low-dimensional structure of deterministic transformations
between random variables, i.e., transport maps between probability measures. In
the context of statistics and machine learning, these transformations can be
used to couple a tractable &quot;reference&quot; measure (e.g., a standard Gaussian) with
a target measure of interest. Direct simulation from the desired measure can
then be achieved by pushing forward reference samples through the map. Yet
characterizing such a map---e.g., representing and evaluating it---grows
challenging in high dimensions. The central contribution of this paper is to
establish a link between the Markov properties of the target measure and the
existence of low-dimensional couplings, induced by transport maps that are
sparse and/or decomposable. Our analysis not only facilitates the construction
of transformations in high-dimensional settings, but also suggests new
inference methodologies for continuous non-Gaussian graphical models. For
instance, in the context of nonlinear state-space models, we describe new
variational algorithms for filtering, smoothing, and sequential parameter
inference. These algorithms can be understood as the natural
generalization---to the non-Gaussian case---of the square-root
Rauch-Tung-Striebel Gaussian smoother.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spantini_A/0/1/0/all/0/1&quot;&gt;Alessio Spantini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bigoni_D/0/1/0/all/0/1&quot;&gt;Daniele Bigoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marzouk_Y/0/1/0/all/0/1&quot;&gt;Youssef Marzouk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.00476">
<title>The Mixing method: low-rank coordinate descent for semidefinite programming with diagonal constraints. (arXiv:1706.00476v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1706.00476</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a low-rank coordinate descent approach to
structured semidefinite programming with diagonal constraints. The approach,
which we call the Mixing method, is extremely simple to implement, has no free
parameters, and typically attains an order of magnitude or better improvement
in optimization performance over the current state of the art. We show that the
method is strictly decreasing, will converge to a critical point, and further
that for sufficient rank all non-optimal critical points are unstable.
Moreover, we prove that with a step size, the Mixing method converges to the
global optimum of the semidefinite program almost surely in a locally linear
rate under random initialization. This is the first low-rank semidefinite
programming method that has been shown to achieve a global optimum on the
spherical manifold without assumption. We apply our algorithm to two related
domains: solving the maximum cut semidefinite relaxation, and solving a maximum
satisfiability relaxation (we also briefly consider additional applications
such as learning word embeddings). In all settings, we demonstrate substantial
improvement over the existing state of the art along various dimensions, and in
total, this work expands the scope and scale of problems that can be solved
using semidefinite programming methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Po-Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chang_W/0/1/0/all/0/1&quot;&gt;Wei-Cheng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.08705">
<title>Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary Learning. (arXiv:1708.08705v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1708.08705</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently proposed Multi-Layer Convolutional Sparse Coding (ML-CSC) model,
consisting of a cascade of convolutional sparse layers, provides a new
interpretation of Convolutional Neural Networks (CNNs). Under this framework,
the computation of the forward pass in a CNN is equivalent to a pursuit
algorithm aiming to estimate the nested sparse representation vectors -- or
feature maps -- from a given input signal. Despite having served as a pivotal
connection between CNNs and sparse modeling, a deeper understanding of the
ML-CSC is still lacking: there are no pursuit algorithms that can serve this
model exactly, nor are there conditions to guarantee a non-empty model. While
one can easily obtain signals that approximately satisfy the ML-CSC
constraints, it remains unclear how to simply sample from the model and, more
importantly, how one can train the convolutional filters from real data.
&lt;/p&gt;
&lt;p&gt;In this work, we propose a sound pursuit algorithm for the ML-CSC model by
adopting a projection approach. We provide new and improved bounds on the
stability of the solution of such pursuit and we analyze different practical
alternatives to implement this in practice. We show that the training of the
filters is essential to allow for non-trivial signals in the model, and we
derive an online algorithm to learn the dictionaries from real data,
effectively resulting in cascaded sparse convolutional layers. Last, but not
least, we demonstrate the applicability of the ML-CSC model for several
applications in an unsupervised setting, providing competitive results. Our
work represents a bridge between matrix factorization, sparse dictionary
learning and sparse auto-encoders, and we analyze these connections in detail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sulam_J/0/1/0/all/0/1&quot;&gt;Jeremias Sulam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papyan_V/0/1/0/all/0/1&quot;&gt;Vardan Papyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romano_Y/0/1/0/all/0/1&quot;&gt;Yaniv Romano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1&quot;&gt;Michael Elad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05707">
<title>Nonparametric Shape-restricted Regression. (arXiv:1709.05707v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05707</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of nonparametric regression under shape constraints.
The main examples include isotonic regression (with respect to any partial
order), unimodal/convex regression, additive shape-restricted regression, and
constrained single index model. We review some of the theoretical properties of
the least squares estimator (LSE) in these problems, emphasizing on the
adaptive nature of the LSE. In particular, we study the behavior of the risk of
the LSE, and its pointwise limiting distribution theory, with special emphasis
to isotonic regression. We survey various methods for constructing pointwise
confidence intervals around these shape-restricted functions. We also briefly
discuss the computation of the LSE and indicate some open research problems and
future directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Guntuboyina_A/0/1/0/all/0/1&quot;&gt;Adityanand Guntuboyina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sen_B/0/1/0/all/0/1&quot;&gt;Bodhisattva Sen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.03560">
<title>SHOPPER: A Probabilistic Model of Consumer Choice with Substitutes and Complements. (arXiv:1711.03560v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.03560</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop SHOPPER, a sequential probabilistic model of shopping data.
SHOPPER uses interpretable components to model the forces that drive how a
customer chooses products; in particular, we designed SHOPPER to capture how
items interact with other items. We develop an efficient posterior inference
algorithm to estimate these forces from large-scale data, and we analyze a
large dataset from a major chain grocery store. We are interested in answering
counterfactual queries about changes in prices. We found that SHOPPER provides
accurate predictions even under price interventions, and that it helps identify
complementary and substitutable pairs of products.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ruiz_F/0/1/0/all/0/1&quot;&gt;Francisco J. R. Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Athey_S/0/1/0/all/0/1&quot;&gt;Susan Athey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03840">
<title>Uncharted Forest a Technique for Exploratory Data Analysis. (arXiv:1802.03840v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03840</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploratory data analysis is crucial for developing and understanding
classification models from high-dimensional datasets. We explore the utility of
a new unsupervised tree ensemble called uncharted forest for visualizing class
associations, sample-sample associations, class heterogeneity, and
uninformative classes for provenance studies. The uncharted forest algorithm
can be used to partition data using random selections of variables and metrics
based on statistical spread. After each tree is grown, a tally of the samples
that arrive at every terminal node is maintained. Those tallies are stored in
single sample association matrix and a likelihood measure for each sample being
partitioned with one another can be made. That matrix may be readily viewed as
a heat map, and the probabilities can be quantified via new metrics that
account for class or cluster membership. We display the advantages and
limitations of using this technique by applying it to two classification
datasets and three provenance study datasets. Two of the metrics presented in
this paper are also compared with widely used metrics from two algorithms that
have variance-based clustering mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kneale_C/0/1/0/all/0/1&quot;&gt;Casey Kneale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brown_S/0/1/0/all/0/1&quot;&gt;Steven D. Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00212">
<title>prDeep: Robust Phase Retrieval with a Flexible Deep Network. (arXiv:1803.00212v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00212</link>
<description rdf:parseType="Literal">&lt;p&gt;Phase retrieval algorithms have become an important component in many modern
computational imaging systems. For instance, in the context of ptychography and
speckle correlation imaging, they enable imaging past the diffraction limit and
through scattering media, respectively. Unfortunately, traditional phase
retrieval algorithms struggle in the presence of noise. Progress has been made
recently on more robust algorithms using signal priors, but at the expense of
limiting the range of supported measurement models (e.g., to Gaussian or coded
diffraction patterns). In this work we leverage the regularization-by-denoising
framework and a convolutional neural network denoiser to create prDeep, a new
phase retrieval algorithm that is both robust and broadly applicable. We test
and validate prDeep in simulation to demonstrate that it is robust to noise and
can handle a variety of system models.
&lt;/p&gt;
&lt;p&gt;A MatConvNet implementation of prDeep is available at
https://github.com/ricedsp/prDeep.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Metzler_C/0/1/0/all/0/1&quot;&gt;Christopher A. Metzler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schniter_P/0/1/0/all/0/1&quot;&gt;Philip Schniter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Veeraraghavan_A/0/1/0/all/0/1&quot;&gt;Ashok Veeraraghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baraniuk_R/0/1/0/all/0/1&quot;&gt;Richard G. Baraniuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02781">
<title>Fast Dawid-Skene: A Fast Vote Aggregation Scheme for Sentiment Classification. (arXiv:1803.02781v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02781</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real world problems can now be effectively solved using supervised
machine learning. A major roadblock is often the lack of an adequate quantity
of labeled data for training. A possible solution is to assign the task of
labeling data to a crowd, and then infer the true label using aggregation
methods. A well-known approach for aggregation is the Dawid-Skene (DS)
algorithm, which is based on the principle of Expectation-Maximization (EM). We
propose a new simple, yet effective, EM-based algorithm, which can be
interpreted as a `hard&apos; version of DS, that allows much faster convergence
while maintaining similar accuracy in aggregation. We show the use of this
algorithm as a quick and effective technique for online, real-time sentiment
annotation. We also prove that our algorithm converges to the estimated labels
at a linear rate. Our experiments on standard datasets show a significant
speedup in time taken for aggregation - upto $\sim$8x over Dawid-Skene and
$\sim$6x over other fast EM methods, at competitive accuracy performance. The
code for the implementation of the algorithms can be found at
https://github.com/GoodDeeds/Fast-Dawid-Skene.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sinha_V/0/1/0/all/0/1&quot;&gt;Vaibhav B Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rao_S/0/1/0/all/0/1&quot;&gt;Sukrut Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balasubramanian_V/0/1/0/all/0/1&quot;&gt;Vineeth N Balasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09539">
<title>On Matching Pursuit and Coordinate Descent. (arXiv:1803.09539v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09539</link>
<description rdf:parseType="Literal">&lt;p&gt;Two popular examples of first-order optimization methods over linear spaces
are coordinate descent and matching pursuit algorithms, with their randomized
variants. While the former targets the optimization by moving along
coordinates, the latter considers a generalized notion of directions.
Exploiting the connection between the two algorithms, we present a unified
analysis of both, providing affine invariant sublinear $\mathcal{O}(1/t)$ rates
on smooth objectives and linear convergence on strongly convex objectives. As a
byproduct of our affine invariant analysis of matching pursuit, our rates for
steepest coordinate descent are the tightest known. Furthermore, we show the
first accelerated convergence rate $\mathcal{O}(1/t^2)$ for matching pursuit
and steepest coordinate descent on convex objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Anant Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karimireddy_S/0/1/0/all/0/1&quot;&gt;Sai Praneeth Karimireddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1&quot;&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stich_S/0/1/0/all/0/1&quot;&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03121">
<title>Machine Learning CICY Threefolds. (arXiv:1806.03121v2 [hep-th] UPDATED)</title>
<link>http://arxiv.org/abs/1806.03121</link>
<description rdf:parseType="Literal">&lt;p&gt;The latest techniques from Neural Networks and Support Vector Machines (SVM)
are used to investigate geometric properties of Complete Intersection
Calabi-Yau (CICY) threefolds, a class of manifolds that facilitate string model
building. An advanced neural network classifier and SVM are employed to (1)
learn Hodge numbers and report a remarkable improvement over previous efforts,
(2) query for favourability, and (3) predict discrete symmetries, a highly
imbalanced problem to which both Synthetic Minority Oversampling Technique
(SMOTE) and permutations of the CICY matrix are used to decrease the class
imbalance and improve performance. In each case study, we employ a genetic
algorithm to optimise the hyperparameters of the neural network. We demonstrate
that our approach provides quick diagnostic tools capable of shortlisting
quasi-realistic string models based on compactification over smooth CICYs and
further supports the paradigm that classes of problems in algebraic geometry
can be machine learned.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Bull_K/0/1/0/all/0/1&quot;&gt;Kieran Bull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yang-Hui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Jejjala_V/0/1/0/all/0/1&quot;&gt;Vishnu Jejjala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-th/1/au:+Mishra_C/0/1/0/all/0/1&quot;&gt;Challenger Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04823">
<title>Plug-in Regularized Estimation of High-Dimensional Parameters in Nonlinear Semiparametric Models. (arXiv:1806.04823v3 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04823</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a theory for estimation of a high-dimensional sparse parameter
$\theta$ defined as a minimizer of a population loss function $L_D(\theta,g_0)$
which, in addition to $\theta$, depends on a, potentially infinite dimensional,
nuisance parameter $g_0$. Our approach is based on estimating $\theta$ via an
$\ell_1$-regularized minimization of a sample analog of $L_S(\theta, \hat{g})$,
plugging in a first-stage estimate $\hat{g}$, computed on a hold-out sample. We
define a population loss to be (Neyman) orthogonal if the gradient of the loss
with respect to $\theta$, has pathwise derivative with respect to $g$ equal to
zero, when evaluated at the true parameter and nuisance component. We show that
orthogonality implies a second-order impact of the first stage nuisance error
on the second stage target parameter estimate. Our approach applies to both
convex and non-convex losses, albeit the latter case requires a small
adaptation of our method with a preliminary estimation step of the target
parameter. Our result enables oracle convergence rates for $\theta$ under
assumptions on the first stage rates, typically of the order of $n^{-1/4}$.
&lt;/p&gt;
&lt;p&gt;We show how such an orthogonal loss can be constructed via a novel
orthogonalization process for a general model defined by conditional moment
restrictions. We apply our theory to high-dimensional versions of standard
estimation problems in statistics and econometrics, such as: estimation of
conditional moment models with missing data, estimation of structural utilities
in games of incomplete information and estimation of treatment effects in
regression models with non-linear link functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chernozhukov_V/0/1/0/all/0/1&quot;&gt;Victor Chernozhukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nekipelov_D/0/1/0/all/0/1&quot;&gt;Denis Nekipelov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Semenova_V/0/1/0/all/0/1&quot;&gt;Vira Semenova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10547">
<title>Online optimal task offloading with one-bit feedback. (arXiv:1806.10547v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.10547</link>
<description rdf:parseType="Literal">&lt;p&gt;Task offloading is an emerging technology in fog-enabled networks. It allows
users to transmit tasks to neighbor fog nodes so as to utilize the computing
resources of the networks. In this paper, we investigate a stochastic task
offloading model and propose a multi-armed bandit framework to formulate this
model. We consider the fact that different helper nodes prefer different kinds
of tasks. Further, we assume each helper node just feeds back one-bit
information to the task node to indicate the level of happiness. The key
challenge of this problem lies in the exploration-exploitation tradeoff. We
thus implement a UCB-type algorithm to maximize the long-term happiness metric.
Numerical simulations are given in the end of the paper to corroborate our
strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shangshu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhaowei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fuqian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiliang Luo&lt;/a&gt;</dc:creator>
</item></rdf:RDF>