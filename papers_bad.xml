<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-27T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09074"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09258"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09587"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09715"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09760"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09766"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09807"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09877"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.04167"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.09832"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03737"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08971"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08986"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08999"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09001"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09010"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09203"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09425"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09473"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09578"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09689"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09785"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09840"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09844"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09866"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09887"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09928"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09956"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09967"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09992"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10081"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10133"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10136"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10150"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.09233"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03526"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04590"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08186"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02172"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06382"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07833"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09640"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09904"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04585"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07347"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08784"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08979"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08993"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09018"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09082"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09093"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09119"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09133"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09153"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09159"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09160"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09162"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09163"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09177"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09191"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09202"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09237"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09318"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09319"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09349"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09353"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09357"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09460"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09468"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09518"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09533"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09539"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09546"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09621"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09638"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09704"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09730"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09737"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09791"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09862"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09868"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09946"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09984"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10045"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10049"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10172"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1310.1562"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1506.01110"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1603.08232"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.03948"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.03950"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.03536"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.02542"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.03530"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10210"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10733"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06373"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00559"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03553"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06061"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08708"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02950"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02550"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03839"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04220"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02312"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03607"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04926"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07954"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.09074">
<title>Multi-range Reasoning for Machine Comprehension. (arXiv:1803.09074v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1803.09074</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose MRU (Multi-Range Reasoning Units), a new fast compositional
encoder for machine comprehension (MC). Our proposed MRU encoders are
characterized by multi-ranged gating, executing a series of parameterized
contract-and-expand layers for learning gating vectors that benefit from long
and short-term dependencies. The aims of our approach are as follows: (1)
learning representations that are concurrently aware of long and short-term
context, (2) modeling relationships between intra-document blocks and (3) fast
and efficient sequence encoding. We show that our proposed encoder demonstrates
promising results both as a standalone encoder and as well as a complementary
building block. We conduct extensive experiments on three challenging MC
datasets, namely RACE, SearchQA and NarrativeQA, achieving highly competitive
performance on all. On the RACE benchmark, our model outperforms DFN (Dynamic
Fusion Networks) by 1.5%-6% without using any recurrent or convolution layers.
Similarly, we achieve competitive performance relative to AMANDA on the
SearchQA benchmark and BiDAF on the NarrativeQA benchmark without using any
LSTM/GRU layers. Finally, incorporating MRU encoders with standard BiLSTM
architectures further improves performance, achieving state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1&quot;&gt;Yi Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1&quot;&gt;Luu Anh Tuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1&quot;&gt;Siu Cheung Hui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09258">
<title>Evolutionary n-level Hypergraph Partitioning with Adaptive Coarsening. (arXiv:1803.09258v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.09258</link>
<description rdf:parseType="Literal">&lt;p&gt;Hypergraph partitioning is an NP-hard problem that occurs in many computer
science applications where it is necessary to reduce large problems into a
number of smaller, computationally tractable sub-problems, with the consequent
desire that these should be as independent as possible to reduce the inevitable
side-effects of not taking a global approach. Current techniques use a
multilevel approach that first coarsens the hypergraph into a smaller set of
representative super nodes, partitions these, prior to uncoarsening to achieve
a final set of partitions for the full hypergraph. We develop evolutionary
approaches for the initial (high-level) partitioning problem, and show that
meta-heuristic global search outperforms existing state-of-the-art frameworks
that use a portfolio of simpler local search algorithms. We explore the
coarsening spectrum of possible initial hypergraphs to identify the optimum
landscape in which to achieve the lowest final cut-sizes and introduce an
adaptive coarsening scheme using the characteristics of the hypergraph as it is
coarsened to identify initial hypergraphs which maximise compression and
information content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preen_R/0/1/0/all/0/1&quot;&gt;Richard J. Preen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1&quot;&gt;Jim Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09587">
<title>Evaluation of Session-based Recommendation Algorithms. (arXiv:1803.09587v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1803.09587</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems help users find relevant items of interest, for example
on e-commerce or media streaming sites. Most academic research is concerned
with approaches that personalize the recommendations according to long-term
user profiles. In many real-world applications, however, such long-term
profiles often do not exist and recommendations therefore have to be made
solely based on the observed behavior of a user during an ongoing session.
Given the high practical relevance of the problem, an increased interest in
this problem can be observed in recent years, leading to a number of proposals
for session-based recommendation algorithms that typically aim to predict the
user&apos;s immediate next actions. In this work, we present the results of an
in-depth performance comparison of a number of such algorithms, using a variety
of datasets and evaluation measures. Our comparison includes the most recent
approaches based on recurrent neural networks like GRU4REC, factorized Markov
model approaches such as FISM or Fossil, as well as more simple methods based,
e.g., on nearest neighbor schemes. Our experiments reveal that algorithms of
this latter class, despite their sometimes almost trivial nature, often perform
equally well or significantly better than today&apos;s more complex approaches based
on deep neural networks. Our results therefore suggest that there is
substantial room for improvement regarding the development of more
sophisticated session-based recommendation algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ludewig_M/0/1/0/all/0/1&quot;&gt;Malte Ludewig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jannach_D/0/1/0/all/0/1&quot;&gt;Dietmar Jannach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09715">
<title>On the Runtime Analysis of the Clearing Diversity-Preserving Mechanism. (arXiv:1803.09715v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.09715</link>
<description rdf:parseType="Literal">&lt;p&gt;Clearing is a niching method inspired by the principle of assigning the
available resources among a niche to a single individual. The clearing
procedure supplies these resources only to the best individual of each niche:
the winner. So far, its analysis has been focused on experimental approaches
that have shown that clearing is a powerful diversity-preserving mechanism.
Using rigorous runtime analysis to explain how and why it is a powerful method,
we prove that a mutation-based evolutionary algorithm with a large enough
population size, and a phenotypic distance function always succeeds in
optimising all functions of unitation for small niches in polynomial time,
while a genotypic distance function requires exponential time. Finally, we
prove that with phenotypic and genotypic distances clearing is able to find
both optima for Twomax and several general classes of bimodal functions in
polynomial expected time. We use empirical analysis to highlight some of the
characteristics that makes it a useful mechanism and to support the theoretical
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osuna_E/0/1/0/all/0/1&quot;&gt;Edgar Covantes Osuna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sudholt_D/0/1/0/all/0/1&quot;&gt;Dirk Sudholt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09760">
<title>Predicting the Future with Transformational States. (arXiv:1803.09760v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.09760</link>
<description rdf:parseType="Literal">&lt;p&gt;An intelligent observer looks at the world and sees not only what is, but
what is moving and what can be moved. In other words, the observer sees how the
present state of the world can transform in the future. We propose a model that
predicts future images by learning to represent the present state and its
transformation given only a sequence of images. To do so, we introduce an
architecture with a latent state composed of two components designed to capture
(i) the present image state and (ii) the transformation between present and
future states, respectively. We couple this latent state with a recurrent
neural network (RNN) core that predicts future frames by transforming past
states into future states by applying the accumulated state transformation with
a learned operator. We describe how this model can be integrated into an
encoder-decoder convolutional neural network (CNN) architecture that uses
weighted residual connections to integrate representations of the past with
representations of the future. Qualitatively, our approach generates image
sequences that are stable and capture realistic motion over multiple predicted
frames, without requiring adversarial training. Quantitatively, our method
achieves prediction results comparable to state-of-the-art results on standard
image prediction benchmarks (Moving MNIST, KTH, and UCF101).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1&quot;&gt;Andrew Jaegle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1&quot;&gt;Oleh Rybkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1&quot;&gt;Konstantinos G. Derpanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1&quot;&gt;Kostas Daniilidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09766">
<title>Runtime Analysis of Probabilistic Crowding and Restricted Tournament Selection for Bimodal Optimisation. (arXiv:1803.09766v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.09766</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real optimisation problems lead to multimodal domains and so require the
identification of multiple optima. Niching methods have been developed to
maintain the population diversity, to investigate many peaks in parallel and to
reduce the effect of genetic drift. Using rigorous runtime analysis, we analyse
for the first time two well known niching methods: probabilistic crowding and
restricted tournament selection (RTS). We incorporate both methods into a
$(\mu+1)~EA$ on the bimodal function Twomax where the goal is to find two
optima at opposite ends of the search space. In probabilistic crowding, the
offspring compete with their parents and the survivor is chosen proportionally
to its fitness. On Twomax probabilistic crowding fails to find any reasonable
solution quality even in exponential time. In RTS the offspring compete against
the closest individual amongst $w$ (window size) individuals. We prove that RTS
fails if $w$ is too small, leading to exponential times with high probability.
However, if w is chosen large enough, it finds both optima for Twomax in time
$O(\mu n \log{n})$ with high probability. Our theoretical results are
accompanied by experimental studies that match the theoretical results and also
shed light on parameters not covered by the theoretical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osuna_E/0/1/0/all/0/1&quot;&gt;Edgar Covantes Osuna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sudholt_D/0/1/0/all/0/1&quot;&gt;Dirk Sudholt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09807">
<title>Deep learning as a tool for neural data analysis: speech classification and cross-frequency coupling in human sensorimotor cortex. (arXiv:1803.09807v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.09807</link>
<description rdf:parseType="Literal">&lt;p&gt;A fundamental challenge in neuroscience is to understand what structure in
the world is represented in spatially distributed patterns of neural activity
from multiple single-trial measurements. This is often accomplished by learning
a simple, linear transformations between neural features and features of the
sensory stimuli or motor task. While successful in some early sensory
processing areas, linear mappings are unlikely to be ideal tools for
elucidating nonlinear, hierarchical representations of higher-order brain areas
during complex tasks, such as the production of speech by humans. Here, we
apply deep networks to predict produced speech syllables from cortical surface
electric potentials recorded from human sensorimotor cortex. We found that deep
networks had higher decoding prediction accuracy compared to baseline models,
and also exhibited greater improvements in accuracy with increasing dataset
size. We further demonstrate that deep network&apos;s confusions revealed
hierarchical latent structure in the neural data, which recapitulated the
underlying articulatory nature of speech motor control. Finally, we used deep
networks to compare task-relevant information in different neural frequency
bands, and found that the high-gamma band contains the vast majority of
information relevant for the speech prediction task, with little-to-no
additional contribution from lower-frequencies. Together, these results
demonstrate the utility of deep networks as a data analysis tool for
neuroscience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Livezey_J/0/1/0/all/0/1&quot;&gt;Jesse A. Livezey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1&quot;&gt;Kristofer E. Bouchard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1&quot;&gt;Edward F. Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09877">
<title>DRACO: Robust Distributed Training via Redundant Gradients. (arXiv:1803.09877v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09877</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed model training is vulnerable to worst-case system failures and
adversarial compute nodes, i.e., nodes that use malicious updates to corrupt
the global model stored at a parameter server (PS). To tolerate node failures
and adversarial attacks, recent work suggests using variants of the geometric
median to aggregate distributed updates at the PS, in place of bulk averaging.
Although median-based update rules are robust to adversarial nodes, their
computational cost can be prohibitive in large-scale settings and their
convergence guarantees often require relatively strong assumptions.
&lt;/p&gt;
&lt;p&gt;In this work, we present DRACO, a scalable framework for robust distributed
training that uses ideas from coding theory. In DRACO, each compute node
evaluates redundant gradients that are then used by the parameter server to
eliminate the effects of adversarial updates. We present problem-independent
robustness guarantees for DRACO and show that the model it produces is
identical to the one trained in the adversary-free setup. We provide extensive
experiments on real datasets and distributed setups across a variety of
large-scale models, where we show that DRACO is several times to orders of
magnitude faster than median-based approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lingjiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+HongyiWang/0/1/0/all/0/1&quot;&gt;HongyiWang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Charles_Z/0/1/0/all/0/1&quot;&gt;Zachary Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papailiopoulos_D/0/1/0/all/0/1&quot;&gt;Dimitris Papailiopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.04167">
<title>Tensorial Mixture Models. (arXiv:1610.04167v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1610.04167</link>
<description rdf:parseType="Literal">&lt;p&gt;Casting neural networks in generative frameworks is a highly sought-after
endeavor these days. Contemporary methods, such as Generative Adversarial
Networks, capture some of the generative capabilities, but not all. In
particular, they lack the ability of tractable marginalization, and thus are
not suitable for many tasks. Other methods, based on arithmetic circuits and
sum-product networks, do allow tractable marginalization, but their performance
is challenged by the need to learn the structure of a circuit. Building on the
tractability of arithmetic circuits, we leverage concepts from tensor analysis,
and derive a family of generative models we call Tensorial Mixture Models
(TMMs). TMMs assume a simple convolutional network structure, and in addition,
lend themselves to theoretical analyses that allow comprehensive understanding
of the relation between their structure and their expressive properties. We
thus obtain a generative model that is tractable on one hand, and on the other
hand, allows effective representation of rich distributions in an easily
controlled manner. These two capabilities are brought together in the task of
classification under missing data, where TMMs deliver state of the art
accuracies with seamless implementation and design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharir_O/0/1/0/all/0/1&quot;&gt;Or Sharir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamari_R/0/1/0/all/0/1&quot;&gt;Ronen Tamari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1&quot;&gt;Nadav Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1&quot;&gt;Amnon Shashua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.09832">
<title>Model based learning for accelerated, limited-view 3D photoacoustic tomography. (arXiv:1708.09832v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1708.09832</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep learning for tomographic reconstructions have shown
great potential to create accurate and high quality images with a considerable
speed-up. In this work we present a deep neural network that is specifically
designed to provide high resolution 3D images from restricted photoacoustic
measurements. The network is designed to represent an iterative scheme and
incorporates gradient information of the data fit to compensate for limited
view artefacts. Due to the high complexity of the photoacoustic forward
operator, we separate training and computation of the gradient information. A
suitable prior for the desired image structures is learned as part of the
training. The resulting network is trained and tested on a set of segmented
vessels from lung CT scans and then applied to in-vivo photoacoustic
measurement data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1&quot;&gt;Andreas Hauptmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucka_F/0/1/0/all/0/1&quot;&gt;Felix Lucka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Betcke_M/0/1/0/all/0/1&quot;&gt;Marta Betcke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1&quot;&gt;Nam Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adler_J/0/1/0/all/0/1&quot;&gt;Jonas Adler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cox_B/0/1/0/all/0/1&quot;&gt;Ben Cox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beard_P/0/1/0/all/0/1&quot;&gt;Paul Beard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arridge_S/0/1/0/all/0/1&quot;&gt;Simon Arridge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03737">
<title>Enhancing Evolutionary Optimization in Uncertain Environments by Allocating Evaluations via Multi-armed Bandit Algorithms. (arXiv:1803.03737v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03737</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimization problems with uncertain fitness functions are common in the real
world, and present unique challenges for evolutionary optimization approaches.
Existing issues include excessively expensive evaluation, lack of solution
reliability, and incapability in maintaining high overall fitness during
optimization. Using conversion rate optimization as an example, this paper
proposes a series of new techniques for addressing these issues. The main
innovation is to augment evolutionary algorithms by allocating evaluation
budget through multi-armed bandit algorithms. Experimental results demonstrate
that multi-armed bandit algorithms can be used to allocate evaluations
efficiently, select the winning solution reliably and increase overall fitness
during exploration. The proposed methods can be generalized to any optimization
problems with noisy fitness functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08971">
<title>Computational Power and the Social Impact of Artificial Intelligence. (arXiv:1803.08971v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.08971</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning is a computational process. To that end, it is inextricably
tied to computational power - the tangible material of chips and semiconductors
that the algorithms of machine intelligence operate on. Most obviously,
computational power and computing architectures shape the speed of training and
inference in machine learning, and therefore influence the rate of progress in
the technology. But, these relationships are more nuanced than that: hardware
shapes the methods used by researchers and engineers in the design and
development of machine learning models. Characteristics such as the power
consumption of chips also define where and how machine learning can be used in
the real world.
&lt;/p&gt;
&lt;p&gt;Despite this, many analyses of the social impact of the current wave of
progress in AI have not substantively brought the dimension of hardware into
their accounts. While a common trope in both the popular press and scholarly
literature is to highlight the massive increase in computational power that has
enabled the recent breakthroughs in machine learning, the analysis frequently
goes no further than this observation around magnitude. This paper aims to dig
more deeply into the relationship between computational power and the
development of machine learning. Specifically, it examines how changes in
computing architectures, machine learning methodologies, and supply chains
might influence the future of AI. In doing so, it seeks to trace a set of
specific relationships between this underlying hardware layer and the broader
social impacts and risks around AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_T/0/1/0/all/0/1&quot;&gt;Tim Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08986">
<title>DeepMood: Modeling Mobile Phone Typing Dynamics for Mood Detection. (arXiv:1803.08986v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1803.08986</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing use of electronic forms of communication presents new
opportunities in the study of mental health, including the ability to
investigate the manifestations of psychiatric diseases unobtrusively and in the
setting of patients&apos; daily lives. A pilot study to explore the possible
connections between bipolar affective disorder and mobile phone usage was
conducted. In this study, participants were provided a mobile phone to use as
their primary phone. This phone was loaded with a custom keyboard that
collected metadata consisting of keypress entry time and accelerometer
movement. Individual character data with the exceptions of the backspace key
and space bar were not collected due to privacy concerns. We propose an
end-to-end deep architecture based on late fusion, named DeepMood, to model the
multi-view metadata for the prediction of mood scores. Experimental results
show that 90.31% prediction accuracy on the depression score can be achieved
based on session-level mobile phone typing dynamics which is typically less
than one minute. It demonstrates the feasibility of using mobile phone metadata
to infer mood disturbance and severity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1&quot;&gt;Bokai Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piscitello_A/0/1/0/all/0/1&quot;&gt;Andrea Piscitello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zulueta_J/0/1/0/all/0/1&quot;&gt;John Zulueta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajilore_O/0/1/0/all/0/1&quot;&gt;Olu Ajilore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryan_K/0/1/0/all/0/1&quot;&gt;Kelly Ryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leow_A/0/1/0/all/0/1&quot;&gt;Alex D. Leow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08999">
<title>LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image. (arXiv:1803.08999v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.08999</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an algorithm to predict room layout from a single image that
generalizes across panoramas and perspective images, cuboid layouts and more
general layouts (e.g. L-shape room). Our method operates directly on the
panoramic image, rather than decomposing into perspective images as do recent
works. Our network architecture is similar to that of RoomNet, but we show
improvements due to aligning the image based on vanishing points, predicting
multiple layout elements (corners, boundaries, size and translation), and
fitting a constrained Manhattan layout to the resulting predictions. Our method
compares well in speed and accuracy to other existing work on panoramas,
achieves among the best accuracy for perspective images, and can handle both
cuboid-shaped and more general Manhattan layouts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_C/0/1/0/all/0/1&quot;&gt;Chuhang Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colburn_A/0/1/0/all/0/1&quot;&gt;Alex Colburn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1&quot;&gt;Qi Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1&quot;&gt;Derek Hoiem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09001">
<title>Accelerating Learning in Constructive Predictive Frameworks with the Successor Representation. (arXiv:1803.09001v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09001</link>
<description rdf:parseType="Literal">&lt;p&gt;Here we propose using the successor representation (SR) to accelerate
learning in a constructive knowledge system based on general value functions
(GVFs). In real-world settings like robotics for unstructured and dynamic
environments, it is infeasible to model all meaningful aspects of a system and
its environment by hand due to both complexity and size. Instead, robots must
be capable of learning and adapting to changes in their environment and task,
incrementally constructing models from their own experience. GVFs, taken from
the field of reinforcement learning (RL), are a way of modeling the world as
predictive questions. One approach to such models proposes a massive network of
interconnected and interdependent GVFs, which are incrementally added over
time. It is reasonable to expect that new, incrementally added predictions can
be learned more swiftly if the learning process leverages knowledge gained from
past experience. The SR provides such a means of separating the dynamics of the
world from the prediction targets and thus capturing regularities that can be
reused across multiple GVFs. As a primary contribution of this work, we show
that using SR-based predictions can improve sample efficiency and learning
speed in a continual learning setting where new predictions are incrementally
added and learned over time. We analyze our approach in a grid-world and then
demonstrate its potential on data from a physical robot arm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sherstan_C/0/1/0/all/0/1&quot;&gt;Craig Sherstan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1&quot;&gt;Marlos C. Machado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilarski_P/0/1/0/all/0/1&quot;&gt;Patrick M. Pilarski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09010">
<title>Datasheets for Datasets. (arXiv:1803.09010v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1803.09010</link>
<description rdf:parseType="Literal">&lt;p&gt;Currently there is no standard way to identify how a dataset was created, and
what characteristics, motivations, and potential skews it represents. To begin
to address this issue, we propose the concept of a datasheet for datasets, a
short document to accompany public datasets, commercial APIs, and pretrained
models. The goal of this proposal is to enable better communication between
dataset creators and users, and help the AI community move toward greater
transparency and accountability. By analogy, in computer hardware, it has
become industry standard to accompany everything from the simplest components
(e.g., resistors), to the most complex microprocessor chips, with datasheets
detailing standard operating characteristics, test results, recommended usage,
and other information. We outline some of the questions a datasheet for
datasets should answer. These questions focus on when, where, and how the
training data was gathered, its recommended use cases, and, in the case of
human-centric datasets, information regarding the subjects&apos; demographics and
consent as applicable. We develop prototypes of datasheets for two well-known
datasets: Labeled Faces in The Wild~\cite{lfw} and the Pang \&amp;amp; Lee Polarity
Dataset~\cite{polarity}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gebru_T/0/1/0/all/0/1&quot;&gt;Timnit Gebru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgenstern_J/0/1/0/all/0/1&quot;&gt;Jamie Morgenstern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vecchione_B/0/1/0/all/0/1&quot;&gt;Briana Vecchione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaughan_J/0/1/0/all/0/1&quot;&gt;Jennifer Wortman Vaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallach_H/0/1/0/all/0/1&quot;&gt;Hanna Wallach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daumee_H/0/1/0/all/0/1&quot;&gt;Hal Daume&amp;#xe9; III&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crawford_K/0/1/0/all/0/1&quot;&gt;Kate Crawford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09203">
<title>Autonomous Ramp Merge Maneuver Based on Reinforcement Learning with Continuous Action Space. (arXiv:1803.09203v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.09203</link>
<description rdf:parseType="Literal">&lt;p&gt;Ramp merging is a critical maneuver for road safety and traffic efficiency.
Most of the current automated driving systems developed by multiple automobile
manufacturers and suppliers are typically limited to restricted access freeways
only. Extending the automated mode to ramp merging zones presents substantial
challenges. One is that the automated vehicle needs to incorporate a future
objective (e.g. a successful and smooth merge) and optimize a long-term reward
that is impacted by subsequent actions when executing the current action.
Furthermore, the merging process involves interaction between the merging
vehicle and its surrounding vehicles whose behavior may be cooperative or
adversarial, leading to distinct merging countermeasures that are crucial to
successfully complete the merge. In place of the conventional rule-based
approaches, we propose to apply reinforcement learning algorithm on the
automated vehicle agent to find an optimal driving policy by maximizing the
long-term reward in an interactive driving environment. Most importantly, in
contrast to most reinforcement learning applications in which the action space
is resolved as discrete, our approach treats the action space as well as the
state space as continuous without incurring additional computational costs. Our
unique contribution is the design of the Q-function approximation whose format
is structured as a quadratic function, by which simple but effective neural
networks are used to estimate its coefficients. The results obtained through
the implementation of our training platform demonstrate that the vehicle agent
is able to learn a safe, smooth and timely merging policy, indicating the
effectiveness and practicality of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Ching-Yao Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09425">
<title>Scalable photonic reinforcement learning by time-division multiplexing of laser chaos. (arXiv:1803.09425v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1803.09425</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning involves decision making in dynamic and uncertain
environments and constitutes a crucial element of artificial intelligence. In
our previous work, we experimentally demonstrated that the ultrafast chaotic
oscillatory dynamics of lasers can be used to solve the two-armed bandit
problem efficiently, which requires decision making concerning a class of
difficult trade-offs called the exploration-exploitation dilemma. However, only
two selections were employed in that research; thus, the scalability of the
laser-chaos-based reinforcement learning should be clarified. In this study, we
demonstrated a scalable, pipelined principle of resolving the multi-armed
bandit problem by introducing time-division multiplexing of chaotically
oscillated ultrafast time-series. The experimental demonstrations in which
bandit problems with up to 64 arms were successfully solved are presented in
this report. Detailed analyses are also provided that include performance
comparisons among laser chaos signals generated in different physical
conditions, which coincide with the diffusivity inherent in the time series.
This study paves the way for ultrafast reinforcement learning by taking
advantage of the ultrahigh bandwidths of light wave and practical enabling
technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naruse_M/0/1/0/all/0/1&quot;&gt;Makoto Naruse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihana_T/0/1/0/all/0/1&quot;&gt;Takatomo Mihana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hori_H/0/1/0/all/0/1&quot;&gt;Hirokazu Hori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saigo_H/0/1/0/all/0/1&quot;&gt;Hayato Saigo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okamura_K/0/1/0/all/0/1&quot;&gt;Kazuya Okamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasegawa_M/0/1/0/all/0/1&quot;&gt;Mikio Hasegawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uchida_A/0/1/0/all/0/1&quot;&gt;Atsushi Uchida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09473">
<title>code2vec: Learning Distributed Representations of Code. (arXiv:1803.09473v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09473</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a neural model for representing snippets of code as continuous
distributed vectors. The main idea is to represent code as a collection of
paths in its abstract syntax tree, and aggregate these paths, in a smart and
scalable way, into a single fixed-length \emph{code vector}, which can be used
to predict semantic properties of the snippet.
&lt;/p&gt;
&lt;p&gt;We demonstrate the effectiveness of our approach by using it to predict a
method&apos;s name from the vector representation of its body. We evaluate our
approach by training a model on a dataset of $14$M methods. We show that code
vectors trained on this dataset can predict method names from files that were
completely unobserved during training. Furthermore, we show that our model
learns useful method name vectors that capture semantic similarities,
combinations, and analogies.
&lt;/p&gt;
&lt;p&gt;Comparing previous techniques over the same data set, our approach obtains a
relative improvement of over $75\%$, being the first to successfully predict
method names based on a large, cross-project, corpus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1&quot;&gt;Uri Alon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zilberstein_M/0/1/0/all/0/1&quot;&gt;Meital Zilberstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1&quot;&gt;Omer Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahav_E/0/1/0/all/0/1&quot;&gt;Eran Yahav&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09578">
<title>Why Comparing Single Performance Scores Does Not Allow to Draw Conclusions About Machine Learning Approaches. (arXiv:1803.09578v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09578</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing state-of-the-art approaches for specific tasks is a major driving
force in our research community. Depending on the prestige of the task,
publishing it can come along with a lot of visibility. The question arises how
reliable are our evaluation methodologies to compare approaches?
&lt;/p&gt;
&lt;p&gt;One common methodology to identify the state-of-the-art is to partition data
into a train, a development and a test set. Researchers can train and tune
their approach on some part of the dataset and then select the model that
worked best on the development set for a final evaluation on unseen test data.
Test scores from different approaches are compared, and performance differences
are tested for statistical significance.
&lt;/p&gt;
&lt;p&gt;In this publication, we show that there is a high risk that a statistical
significance in this type of evaluation is not due to a superior learning
approach. Instead, there is a high risk that the difference is due to chance.
For example for the CoNLL 2003 NER dataset we observed in up to 26% of the
cases type I errors (false positives) with a threshold of p &amp;lt; 0.05, i.e.,
falsely concluding a statistically significant difference between two identical
approaches.
&lt;/p&gt;
&lt;p&gt;We prove that this evaluation setup is unsuitable to compare learning
approaches. We formalize alternative evaluation setups based on score
distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1&quot;&gt;Nils Reimers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1&quot;&gt;Iryna Gurevych&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09689">
<title>Flow From Motion: A Deep Learning Approach. (arXiv:1803.09689v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09689</link>
<description rdf:parseType="Literal">&lt;p&gt;Wearable devices have the potential to enhance sports performance, yet they
are not fulfilling this promise. Our previous studies with 6 professional
tennis coaches and 20 players indicate that this could be due the lack of
psychological or mental state feedback, which the coaches claim to provide.
Towards this end, we propose to detect the flow state, mental state of optimal
performance, using wearables data to be later used in training. We performed a
study with a professional tennis coach and two players. The coach provided
labels about the players&apos; flow state while each player had a wearable device on
their racket holding wrist. We trained multiple models using the wearables data
and the coach labels. Our deep neural network models achieved around 98%
testing accuracy for a variety of conditions. This suggests that the flow state
or what coaches recognize as flow, can be detected using wearables data in
tennis which is a novel result. The implication for the HCI community is that
having access to such information would allow for design of novel hardware and
interaction paradigms that would be helpful in professional athlete training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eteke_C/0/1/0/all/0/1&quot;&gt;Cem Eteke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havlucu_H/0/1/0/all/0/1&quot;&gt;Hayati Havlucu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirbac_N/0/1/0/all/0/1&quot;&gt;Nisa &amp;#x130;rem K&amp;#x131;rba&amp;#xe7;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onbasli_M/0/1/0/all/0/1&quot;&gt;Mehmet Cengiz Onba&amp;#x15f;l&amp;#x131;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coskun_A/0/1/0/all/0/1&quot;&gt;Aykut Co&amp;#x15f;kun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eskenazi_T/0/1/0/all/0/1&quot;&gt;Terry Eskenazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozcan_O/0/1/0/all/0/1&quot;&gt;O&amp;#x11f;uzhan &amp;#xd6;zcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akgun_B/0/1/0/all/0/1&quot;&gt;Bar&amp;#x131;&amp;#x15f; Akg&amp;#xfc;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09785">
<title>Algorithm Configuration: Learning policies for the quick termination of poor performers. (arXiv:1803.09785v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.09785</link>
<description rdf:parseType="Literal">&lt;p&gt;One way to speed up the algorithm configuration task is to use short runs
instead of long runs as much as possible, but without discarding the
configurations that eventually do well on the long runs. We consider the
problem of selecting the top performing configurations of the Conditional
Markov Chain Search (CMCS), a general algorithm schema that includes, for
examples, VNS. We investigate how the structure of performance on short tests
links with those on long tests, showing that significant differences arise
between test domains. We propose a &quot;performance envelope&quot; method to exploit the
links; that learns when runs should be terminated, but that automatically
adapts to the domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karapetyan_D/0/1/0/all/0/1&quot;&gt;Daniel Karapetyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parkes_A/0/1/0/all/0/1&quot;&gt;Andrew J. Parkes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stutzle_T/0/1/0/all/0/1&quot;&gt;Thomas St&amp;#xfc;tzle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09840">
<title>Empirical Analysis of Foundational Distinctions in the Web of Data. (arXiv:1803.09840v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.09840</link>
<description rdf:parseType="Literal">&lt;p&gt;A main difference between pre-Web artificial intelligence and the current one
is that the Web and its Semantic extension (i.e. Web of Data) contain open
global-scale knowledge and make it available to potentially intelligent
machines that may want to benefit from it. Nevertheless, most of the Web of
Data lacks ontological distinctions and has a sparse distribution of
axiomatisations. For example, foundational distinctions such as whether an
entity is inherently a class or an individual, or whether it is a physical
object or not, are hardly expressed in the data, although they have been
largely studied and formalised by foundational ontologies (e.g. DOLCE, SUMO).
There is a gap between these ontologies, that often formalise or are inspired
by preexisting philosophical theories and are developed with a top-down
approach, and the Web of Data that is mostly derived from existing databases or
from crowd-based effort (e.g. DBpedia, Wikidata, Freebase). We investigate
whether the Web provides an empirical foundation for characterising entities of
the Web of Data according to foundational distinctions. We want to answer
questions such as &quot;is the DBpedia entity for dog a class or an instance?&quot; We
report on a set of experiments based on machine learning and crowdsourcing that
show promising results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asprino_L/0/1/0/all/0/1&quot;&gt;Luigi Asprino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basile_V/0/1/0/all/0/1&quot;&gt;Valerio Basile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ciancarini_P/0/1/0/all/0/1&quot;&gt;Paolo Ciancarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Presutti_V/0/1/0/all/0/1&quot;&gt;Valentina Presutti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09844">
<title>A Conversational Interface to Improve Medication Adherence: Towards AI Support in Patient&apos;s Treatment. (arXiv:1803.09844v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1803.09844</link>
<description rdf:parseType="Literal">&lt;p&gt;Medication adherence is of utmost importance for many chronic conditions,
regardless of the disease type. Engaging patients in self-tracking their
medication is a big challenge. One way to potentially reduce this burden is to
use reminders to promote wellness throughout all stages of life and improve
medication adherence. Chatbots have proven effectiveness in triggering users to
engage in certain activity, such as medication adherence. In this paper, we
discuss &quot;Roborto&quot;, a chatbot to create an engaging interactive and intelligent
environment for patients and assist in positive lifestyle modification. We
introduce a way for healthcare providers to track patients adherence and
intervene whenever necessary. We describe the health, technical and behavioural
approaches to the problem of medication non-adherence and propose a diagnostic
and decision support tool. The proposed study will be implemented and validated
through a pilot experiment with users to measure the efficacy of the proposed
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadhil_A/0/1/0/all/0/1&quot;&gt;Ahmed Fadhil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09866">
<title>Accelerating Empowerment Computation with UCT Tree Search. (arXiv:1803.09866v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.09866</link>
<description rdf:parseType="Literal">&lt;p&gt;Models of intrinsic motivation present an important means to produce sensible
behaviour in the absence of extrinsic rewards. Applications in video games are
varied, and range from intrinsically motivated general game-playing agents to
non-player characters such as companions and enemies. The information-theoretic
quantity of Empowerment is a particularly promising candidate motivation to
produce believable, generic and robust behaviour. However, while it can be used
in the absence of external reward functions that would need to be crafted and
learned, empowerment is computationally expensive. In this paper, we propose a
modified UCT tree search method to mitigate empowerment&apos;s computational
complexity in discrete and deterministic scenarios. We demonstrate how to
modify a Monte-Carlo Search Tree with UCT to realise empowerment maximisation,
and discuss three additional modifications that facilitate better sampling. We
evaluate the approach both quantitatively, by analysing how close our approach
gets to the baseline of exhaustive empowerment computation with varying amounts
of computational resources, and qualitatively, by analysing the resulting
behaviour in a Minecraft-like scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salge_C/0/1/0/all/0/1&quot;&gt;Christoph Salge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guckelsberger_C/0/1/0/all/0/1&quot;&gt;Christian Guckelsberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canaan_R/0/1/0/all/0/1&quot;&gt;Rodrigo Canaan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahlmann_T/0/1/0/all/0/1&quot;&gt;Tobias Mahlmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09887">
<title>MLE-induced Likelihood for Markov Random Fields. (arXiv:1803.09887v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09887</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the intractable partition function, the exact likelihood function for
a Markov random field (MRF), in many situations, can only be approximated.
Major approximation approaches include pseudolikelihood and Laplace
approximation. In this paper, we propose a novel way of approximating the
likelihood function through first approximating the marginal likelihood
functions of individual parameters and then reconstructing the joint likelihood
function from these marginal likelihood functions. For approximating the
marginal likelihood functions, we derive a particular likelihood function from
a modified scenario of coin tossing which is useful for capturing how one
parameter interacts with the remaining parameters in the likelihood function.
For reconstructing the joint likelihood function, we use an appropriate copula
to link up these marginal likelihood functions. Numerical investigation
suggests the superior performance of our approach. Especially as the size of
the MRF increases, both the numerical performance and the computational cost of
our approach remain consistently satisfactory, whereas Laplace approximation
deteriorates and pseudolikelihood becomes computationally unbearable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hao Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09928">
<title>Entropy Controlled Non-Stationarity for Improving Performance of Independent Learners in Anonymous MARL Settings. (arXiv:1803.09928v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09928</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advent of sequential matching (of supply and demand) systems (uber,
Lyft, Grab for taxis; ubereats, deliveroo, etc for food; amazon prime, lazada
etc. for groceries) across many online and offline services, individuals (taxi
drivers, delivery boys, delivery van drivers, etc.) earn more by being at the
&quot;right&quot; place at the &quot;right&quot; time. We focus on learning techniques for
providing guidance (on right locations to be at right times) to individuals in
the presence of other &quot;learning&quot; individuals. Interactions between indivduals
are anonymous, i.e, the outcome of an interaction (competing for demand) is
independent of the identity of the agents and therefore we refer to these as
Anonymous MARL settings.
&lt;/p&gt;
&lt;p&gt;Existing research of relevance is on independent learning using Reinforcement
Learning (RL) or on Multi-Agent Reinforcement Learning (MARL). The number of
individuals in aggregation systems is extremely large and individuals have
their own selfish interest (of maximising revenue). Therefore, traditional MARL
approaches are either not scalable or assumptions of common objective or action
coordination are not viable. In this paper, we focus on improving performance
of independent reinforcement learners, specifically the popular Deep Q-Networks
(DQN) and Advantage Actor Critic (A2C) approaches by exploiting anonymity.
Specifically, we control non-stationarity introduced by other agents using
entropy of agent density distribution. We demonstrate a significant improvement
in revenue for individuals and for all agents together with our learners on a
generic experimental set up for aggregation systems and a real world taxi
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_T/0/1/0/all/0/1&quot;&gt;Tanvi Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varakantham_P/0/1/0/all/0/1&quot;&gt;Pradeep Varakantham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_H/0/1/0/all/0/1&quot;&gt;Hoong Chuin Lau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09956">
<title>Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning. (arXiv:1803.09956v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1803.09956</link>
<description rdf:parseType="Literal">&lt;p&gt;Skilled robotic manipulation benefits from complex synergies between
non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing
can help rearrange cluttered objects to make space for arms and fingers;
likewise, grasping can help displace objects to make pushing movements more
precise and collision-free. In this work, we demonstrate that it is possible to
discover and learn these synergies from scratch through model-free deep
reinforcement learning. Our method involves training two fully convolutional
networks that map from visual observations to actions: one infers the utility
of pushes for a dense pixel-wise sampling of end effector orientations and
locations, while the other does the same for grasping. Both networks are
trained jointly in a Q-learning framework and are entirely self-supervised by
trial and error, where rewards are provided from successful grasps. In this
way, our policy learns pushing motions that enable future grasps, while
learning grasps that can leverage past pushes. During picking experiments in
both simulation and real-world scenarios, we find that our system quickly
learns complex behaviors amid challenging cases of clutter, and achieves better
grasping success rates and picking efficiencies than baseline alternatives
after only a few hours of training. We further demonstrate that our method is
capable of generalizing to novel objects. Qualitative results (videos), code,
pre-trained models, and simulation environments are available at
&lt;a href=&quot;http://vpg.cs.princeton.edu&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Andy Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuran Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welker_S/0/1/0/all/0/1&quot;&gt;Stefan Welker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Johnny Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Alberto Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1&quot;&gt;Thomas Funkhouser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09967">
<title>Reinforcement Learning for Fair Dynamic Pricing. (arXiv:1803.09967v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09967</link>
<description rdf:parseType="Literal">&lt;p&gt;Unfair pricing policies have been shown to be one of the most negative
perceptions customers can have concerning pricing, and may result in long-term
losses for a company. Despite the fact that dynamic pricing models help
companies maximize revenue, fairness and equality should be taken into account
in order to avoid unfair price differences between groups of customers. This
paper shows how to solve dynamic pricing by using Reinforcement Learning (RL)
techniques so that prices are maximized while keeping a balance between revenue
and fairness. We demonstrate that RL provides two main features to support
fairness in dynamic pricing: on the one hand, RL is able to learn from recent
experience, adapting the pricing policy to complex market environments; on the
other hand, it provides a trade-off between short and long-term objectives,
hence integrating fairness into the model&apos;s core. Considering these two
features, we propose the application of RL for revenue optimization, with the
additional integration of fairness as part of the learning procedure by using
Jain&apos;s index as a metric. Results in a simulated environment show a significant
improvement in fairness while at the same time maintaining optimisation of
revenue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maestre_R/0/1/0/all/0/1&quot;&gt;Roberto Maestre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duque_J/0/1/0/all/0/1&quot;&gt;Juan Duque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubio_A/0/1/0/all/0/1&quot;&gt;Alberto Rubio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arevalo_J/0/1/0/all/0/1&quot;&gt;Juan Ar&amp;#xe9;valo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09992">
<title>Applications of Artificial Intelligence to Network Security. (arXiv:1803.09992v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1803.09992</link>
<description rdf:parseType="Literal">&lt;p&gt;Attacks to networks are becoming more complex and sophisticated every day.
Beyond the so-called script-kiddies and hacking newbies, there is a myriad of
professional attackers seeking to make serious profits infiltrating in
corporate networks. Either hostile governments, big corporations or mafias are
constantly increasing their resources and skills in cybercrime in order to spy,
steal or cause damage more effectively. traditional approaches to Network
Security seem to start hitting their limits and it is being recognized the need
for a smarter approach to threat detections. This paper provides an
introduction on the need for evolution of Cyber Security techniques and how
Artificial Intelligence could be of application to help solving some of the
problems. It provides also, a high-level overview of some state of the art AI
Network Security techniques, to finish analysing what is the foreseeable future
of the application of AI to Network Security.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veiga_A/0/1/0/all/0/1&quot;&gt;Alberto Perez Veiga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10081">
<title>DeepJDOT: Deep Joint distribution optimal transport for unsupervised domain adaptation. (arXiv:1803.10081v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.10081</link>
<description rdf:parseType="Literal">&lt;p&gt;In computer vision, one is often confronted with problems of domain shifts,
which occur when one applies a classifier trained on a source dataset to target
data sharing similar characteristics (e.g. same classes), but also different
latent data structures (e.g. different acquisition conditions). In such a
situation, the model will perform poorly on the new data, since the classifier
is specialized to recognize visual cues specific to the source domain. In this
work we explore a solution, named DeepJDOT, to tackle this problem: through a
measure of discrepancy on joint deep representations/labels based on optimal
transport, we not only learn new data representations aligned between the
source and target domain, but also simultaneously preserve the discriminative
information used by the classifier. We applied DeepJDOT to a series of visual
recognition tasks, where it compares favorably against state-of-the-art deep
domain adaptation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1&quot;&gt;Bharath Bhushan Damodaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kellenberger_B/0/1/0/all/0/1&quot;&gt;Benjamin Kellenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Flamary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1&quot;&gt;Devis Tuia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1&quot;&gt;Nicolas Courty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10133">
<title>You are your Metadata: Identification and Obfuscation of Social Media Users using Metadata Information. (arXiv:1803.10133v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1803.10133</link>
<description rdf:parseType="Literal">&lt;p&gt;Metadata are associated to most of the information we produce in our daily
interactions and communication in the digital world. Yet, surprisingly,
metadata are often still catergorized as non-sensitive. Indeed, in the past,
researchers and practitioners have mainly focused on the problem of the
identification of a user from the content of a message.
&lt;/p&gt;
&lt;p&gt;In this paper, we use Twitter as a case study to quantify the uniqueness of
the association between metadata and user identity and to understand the
effectiveness of potential obfuscation strategies. More specifically, we
analyze atomic fields in the metadata and systematically combine them in an
effort to classify new tweets as belonging to an account using different
machine learning algorithms of increasing complexity. We demonstrate that
through the application of a supervised learning algorithm, we are able to
identify any user in a group of 10,000 with approximately 96.7% accuracy.
Moreover, if we broaden the scope of our search and consider the 10 most likely
candidates we increase the accuracy of the model to 99.22%. We also found that
data obfuscation is hard and ineffective for this type of data: even after
perturbing 60% of the training data, it is still possible to classify users
with an accuracy higher than 95%. These results have strong implications in
terms of the design of metadata obfuscation strategies, for example for data
set release, not only for Twitter, but, more generally, for most social media
platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_B/0/1/0/all/0/1&quot;&gt;Beatrice Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1&quot;&gt;Mirco Musolesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stringhini_G/0/1/0/all/0/1&quot;&gt;Gianluca Stringhini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10136">
<title>Comprehending Real Numbers: Development of Bengali Real Number Speech Corpus. (arXiv:1803.10136v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1803.10136</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech recognition has received a less attention in Bengali literature due to
the lack of a comprehensive dataset. In this paper, we describe the development
process of the first comprehensive Bengali speech dataset on real numbers. It
comprehends all the possible words that may arise in uttering any Bengali real
number. The corpus has ten speakers from the different regions of Bengali
native people. It comprises of more than two thousands of speech samples in a
total duration of closed to four hours. We also provide a deep analysis of our
corpus, highlight some of the notable features of it, and finally evaluate the
performances of two of the notable Bengali speech recognizers on it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nahid_M/0/1/0/all/0/1&quot;&gt;Md Mahadi Hasan Nahid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md. Ashraful Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Purkaystha_B/0/1/0/all/0/1&quot;&gt;Bishwajit Purkaystha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md Saiful Islam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10150">
<title>Learning to Branch. (arXiv:1803.10150v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.10150</link>
<description rdf:parseType="Literal">&lt;p&gt;Tree search algorithms, such as branch-and-bound, are the most widely used
tools for solving combinatorial and nonconvex problems. For example, they are
the foremost method for solving (mixed) integer programs and constraint
satisfaction problems. Tree search algorithms recursively partition the search
space to find an optimal solution. In order to keep the tree size small, it is
crucial to carefully decide, when expanding a tree node, which question
(typically variable) to branch on at that node in order to partition the
remaining space. Numerous partitioning techniques (e.g., variable selection)
have been proposed, but there is no theory describing which technique is
optimal. We show how to use machine learning to determine an optimal weighting
of any set of partitioning procedures for the instance distribution at hand
using samples from the distribution. We provide the first sample complexity
guarantees for tree search algorithm configuration. These guarantees bound the
number of samples sufficient to ensure that the empirical performance of an
algorithm over the samples nearly matches its expected performance on the
unknown instance distribution. This thorough theoretical investigation
naturally gives rise to our learning algorithm. Via experiments, we show that
learning an optimal weighting of partitioning procedures can dramatically
reduce tree size, and we prove that this reduction can even be exponential.
Through theory and experiments, we show that learning to branch is both
practical and hugely beneficial.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1&quot;&gt;Maria-Florina Balcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dick_T/0/1/0/all/0/1&quot;&gt;Travis Dick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandholm_T/0/1/0/all/0/1&quot;&gt;Tuomas Sandholm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vitercik_E/0/1/0/all/0/1&quot;&gt;Ellen Vitercik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.09233">
<title>Dose Prediction with U-net: A Feasibility Study for Predicting Dose Distributions from Contours using Deep Learning on Prostate IMRT Patients. (arXiv:1709.09233v2 [physics.med-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1709.09233</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advancement of treatment modalities in radiation therapy for cancer
patients, outcomes have improved, but at the cost of increased treatment plan
complexity and planning time. The accurate prediction of dose distributions
would alleviate this issue by guiding clinical plan optimization to save time
and maintain high quality plans. We have modified a convolutional deep network
model, U-net (originally designed for segmentation purposes), for predicting
dose from patient image contours. We show that, as an example, we are able to
accurately predict the dose of intensity-modulated radiation therapy (IMRT) for
prostate cancer patients, where the average dice similarity coefficient is 0.91
when comparing the predicted vs. true isodose volumes between 0% and 100% of
the prescription dose. The average value of the absolute differences in [max,
mean] dose is found to be under 5% of the prescription dose, specifically for
each structure is [1.80%, 1.03%](PTV), [1.94%, 4.22%](Bladder), [1.80%,
0.48%](Body), [3.87%, 1.79%](L Femoral Head), [5.07%, 2.55%](R Femoral Head),
and [1.26%, 1.62%](Rectum) of the prescription dose. We thus managed to map a
desired radiation dose distribution from a patient&apos;s PTV and OAR contours. As
an additional advantage, relatively little data was used in the techniques and
models described in this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Dan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Long_T/0/1/0/all/0/1&quot;&gt;Troy Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xun Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Weiguo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xuejun Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Iqbal_Z/0/1/0/all/0/1&quot;&gt;Zohaib Iqbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Steve Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03526">
<title>Neural Program Synthesis with Priority Queue Training. (arXiv:1801.03526v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03526</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the task of program synthesis in the presence of a reward
function over the output of programs, where the goal is to find programs with
maximal rewards. We employ an iterative optimization scheme, where we train an
RNN on a dataset of K best programs from a priority queue of the generated
programs so far. Then, we synthesize new programs and add them to the priority
queue by sampling from the RNN. We benchmark our algorithm, called priority
queue training (or PQT), against genetic algorithm and reinforcement learning
baselines on a simple but expressive Turing complete programming language
called BF. Our experimental results show that our simple PQT algorithm
significantly outperforms the baselines. By adding a program length penalty to
the reward function, we are able to synthesize short, human readable programs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abolafia_D/0/1/0/all/0/1&quot;&gt;Daniel A. Abolafia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1&quot;&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jonathan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc V. Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04590">
<title>Frame-Recurrent Video Super-Resolution. (arXiv:1801.04590v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04590</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in video super-resolution have shown that convolutional
neural networks combined with motion compensation are able to merge information
from multiple low-resolution (LR) frames to generate high-quality images.
Current state-of-the-art methods process a batch of LR frames to generate a
single high-resolution (HR) frame and run this scheme in a sliding window
fashion over the entire video, effectively treating the problem as a large
number of separate multi-frame super-resolution tasks. This approach has two
main weaknesses: 1) Each input frame is processed and warped multiple times,
increasing the computational cost, and 2) each output frame is estimated
independently conditioned on the input frames, limiting the system&apos;s ability to
produce temporally consistent results.
&lt;/p&gt;
&lt;p&gt;In this work, we propose an end-to-end trainable frame-recurrent video
super-resolution framework that uses the previously inferred HR estimate to
super-resolve the subsequent frame. This naturally encourages temporally
consistent results and reduces the computational cost by warping only one image
in each step. Furthermore, due to its recurrent nature, the proposed method has
the ability to assimilate a large number of previous frames without increased
computational demands. Extensive evaluations and comparisons with previous
methods validate the strengths of our approach and demonstrate that the
proposed framework is able to significantly outperform the current state of the
art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajjadi_M/0/1/0/all/0/1&quot;&gt;Mehdi S. M. Sajjadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1&quot;&gt;Raviteja Vemulapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1&quot;&gt;Matthew Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08186">
<title>MAttNet: Modular Attention Network for Referring Expression Comprehension. (arXiv:1801.08186v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.08186</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address referring expression comprehension: localizing an
image region described by a natural language expression. While most recent work
treats expressions as a single unit, we propose to decompose them into three
modular components related to subject appearance, location, and relationship to
other objects. This allows us to flexibly adapt to expressions containing
different types of information in an end-to-end framework. In our model, which
we call the Modular Attention Network (MAttNet), two types of attention are
utilized: language-based attention that learns the module weights as well as
the word/phrase attention that each module should focus on; and visual
attention that allows the subject and relationship modules to focus on relevant
image components. Module weights combine scores from all three modules
dynamically to output an overall score. Experiments show that MAttNet
outperforms previous state-of-art methods by a large margin on both
bounding-box-level and pixel-level comprehension tasks. Demo and code are
provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Licheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhe Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaohui Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jimei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1&quot;&gt;Tamara L.Berg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02172">
<title>Augmented Artificial Intelligence: a Conceptual Framework. (arXiv:1802.02172v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02172</link>
<description rdf:parseType="Literal">&lt;p&gt;All artificial Intelligence (AI) systems make errors. These errors are
unexpected, and differ often from the typical human mistakes (&quot;non-human&quot;
errors). The AI errors should be corrected without damage of existing skills
and, hopefully, avoiding direct human expertise. This paper presents an initial
summary report of project taking new and systematic approach to improving the
intellectual effectiveness of the individual AI by communities of AIs. We
combine some ideas of learning in heterogeneous multiagent systems with new and
original mathematical approaches for non-iterative corrections of errors of
legacy AI systems. The mathematical foundations of AI non-destructive
correction are presented and a series of new stochastic separation theorems is
proven. These theorems provide a new instrument for the development, analysis,
and assessment of machine learning methods and algorithms in high dimension.
They demonstrate that in high dimensions and even for exponentially large
samples, linear classifiers in their classical Fisher&apos;s form are powerful
enough to separate errors from correct responses with high probability and to
provide efficient solution to the non-destructive corrector problem. In
particular, we prove some hypotheses formulated in our paper `Stochastic
Separation Theorems&apos; (Neural Networks, 94, 255--259, 2017), and answer one
general problem published by Donoho and Tanner in 2009.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1&quot;&gt;Alexander N. Gorban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grechuk_B/0/1/0/all/0/1&quot;&gt;Bogdan Grechuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1&quot;&gt;Ivan Y. Tyukin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05875">
<title>Detecting truth, just on parts. (arXiv:1802.05875v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05875</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce and discuss, through a computational algebraic geometry
approach, the automatic reasoning handling of propositions that are
simultaneously true and false over some relevant collections of instances. A
rigorous, algorithmic criterion is presented for detecting such cases, and its
performance is exemplified through the implementation of this test on the
dynamic geometry program GeoGebra.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovacs_Z/0/1/0/all/0/1&quot;&gt;Zolt&amp;#xe1;n Kov&amp;#xe1;cs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Recio_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Recio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velez_M/0/1/0/all/0/1&quot;&gt;M. Pilar V&amp;#xe9;lez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06382">
<title>Scalable Alignment Kernels via Space-Efficient Feature Maps. (arXiv:1802.06382v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06382</link>
<description rdf:parseType="Literal">&lt;p&gt;String kernels are attractive data analysis tools for analyzing string data.
Among them, alignment kernels are known for their high prediction accuracies in
string classifications when tested in combination with SVMs in various
applications. However, alignment kernels have a crucial drawback in that they
scale poorly due to their quadratic computation complexity in the number of
input strings, which limits large-scale applications in practice. We present
the first approximation named ESP+SFM for alignment kernels by leveraging a
metric embedding named edit-sensitive parsing (ESP) and space-efficient feature
maps (SFM) for random Fourier features (RFF) for large-scale string analyses.
Input strings are projected into vectors of RFF by leveraging ESP and SFM.
Then, SVMs are trained on the projected vectors, which enables to significantly
improve the scalability of alignment kernels while preserving their prediction
accuracies. We experimentally test ESP+ SFM on its ability to learn SVMs for
large-scale string classifications with various massive string data, and we
demonstrate the superior performance of ESP+SFM with respect to prediction
accuracy, scalability and computation efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabei_Y/0/1/0/all/0/1&quot;&gt;Yasuo Tabei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamanishi_Y/0/1/0/all/0/1&quot;&gt;Yoshihiro Yamanishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pagh_R/0/1/0/all/0/1&quot;&gt;Rasmus Pagh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07833">
<title>Variational Inference for Policy Gradient. (arXiv:1802.07833v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07833</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the seminal work on Stein Variational Inference and Stein
Variational Policy Gradient, we derived a method to generate samples from the
posterior variational parameter distribution by \textit{explicitly} minimizing
the KL divergence to match the target distribution in an amortize fashion.
Consequently, we applied this varational inference technique into vanilla
policy gradient, TRPO and PPO with Bayesian Neural Network parameterizations
for reinforcement learning problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianbing Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09640">
<title>Modeling Others using Oneself in Multi-Agent Reinforcement Learning. (arXiv:1802.09640v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09640</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the multi-agent reinforcement learning setting with imperfect
information in which each agent is trying to maximize its own utility. The
reward function depends on the hidden state (or goal) of both agents, so the
agents must infer the other players&apos; hidden goals from their observed behavior
in order to solve the tasks. We propose a new approach for learning in these
domains: Self Other-Modeling (SOM), in which an agent uses its own policy to
predict the other agent&apos;s actions and update its belief of their hidden state
in an online manner. We evaluate this approach on three different tasks and
show that the agents are able to learn better policies using their estimate of
the other players&apos; hidden states, in both cooperative and adversarial settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1&quot;&gt;Roberta Raileanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denton_E/0/1/0/all/0/1&quot;&gt;Emily Denton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1&quot;&gt;Arthur Szlam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1&quot;&gt;Rob Fergus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09904">
<title>Ab initio Algorithmic Causal Deconvolution of Intertwined Programs and Networks by Generative Mechanism. (arXiv:1802.09904v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09904</link>
<description rdf:parseType="Literal">&lt;p&gt;To extract and learn representations leading to generative mechanisms from
data, especially without making arbitrary decisions and biased assumptions, is
a central challenge in most areas of scientific research particularly in
connection to current major limitations of influential topics and methods of
machine and deep learning as they have often lost sight of the model component.
Complex data is usually produced by interacting sources with different
mechanisms. Here we introduce a parameter-free model-based approach, based upon
the seminal concept of Algorithmic Probability, that decomposes an observation
and signal into its most likely algorithmic generative mechanisms. Our methods
use a causal calculus to infer model representations. We demonstrate the method
ability to distinguish interacting mechanisms and deconvolve them, regardless
of whether the objects produce strings, space-time evolution diagrams, images
or networks. We numerically test and evaluate our method and find that it can
disentangle observations from discrete dynamic systems, random and complex
networks. We think that these causal inference techniques can contribute as key
pieces of information for estimations of probability distributions
complementing other more statistical-oriented techniques that otherwise lack
model inference capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenil_H/0/1/0/all/0/1&quot;&gt;Hector Zenil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiani_N/0/1/0/all/0/1&quot;&gt;Narsis A. Kiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1&quot;&gt;Jesper Tegn&amp;#xe9;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04585">
<title>Categorizing Variants of Goodhart&apos;s Law. (arXiv:1803.04585v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04585</link>
<description rdf:parseType="Literal">&lt;p&gt;There are several distinct failure modes for overoptimization of systems on
the basis of metrics. This occurs when a metric which can be used to improve a
system is used to an extent that further optimization is ineffective or
harmful, and is sometimes termed Goodhart&apos;s Law. This class of failure is often
poorly understood, partly because terminology for discussing them is ambiguous,
and partly because discussion using this ambiguous terminology ignores
distinctions between different failure modes of this general type. This paper
expands on an earlier discussion by Garrabrant, which notes there are &quot;(at
least) four different mechanisms&quot; that relate to Goodhart&apos;s Law. This paper is
intended to explore these mechanisms further, and specify more clearly how they
occur. This discussion should be helpful in better understanding these types of
failures in economic regulation, in public policy, in machine learning, and in
Artificial Intelligence alignment. The importance of Goodhart effects depends
on the amount of power directed towards optimizing the proxy, and so the
increased optimization power offered by artificial intelligence makes it
especially critical for that field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manheim_D/0/1/0/all/0/1&quot;&gt;David Manheim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garrabrant_S/0/1/0/all/0/1&quot;&gt;Scott Garrabrant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07347">
<title>Optimizing Sponsored Search Ranking Strategy by Deep Reinforcement Learning. (arXiv:1803.07347v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07347</link>
<description rdf:parseType="Literal">&lt;p&gt;Sponsored search is an indispensable business model and a major revenue
contributor of almost all the search engines. From the advertisers&apos; side,
participating in ranking the search results by paying for the sponsored search
advertisement to attract more awareness and purchase facilitates their
commercial goal. From the users&apos; side, presenting personalized advertisement
reflecting their propensity would make their online search experience more
satisfactory. Sponsored search platforms rank the advertisements by a ranking
function to determine the list of advertisements to show and the charging price
for the advertisers. Hence, it is crucial to find a good ranking function which
can simultaneously satisfy the platform, the users and the advertisers.
Moreover, advertisements showing positions under different queries from
different users may associate with advertisement candidates of different bid
price distributions and click probability distributions, which requires the
ranking functions to be optimized adaptively to the traffic characteristics. In
this work, we proposed a generic framework to optimize the ranking functions by
deep reinforcement learning methods. The framework is composed of two parts: an
offline learning part which initializes the ranking functions by learning from
a simulated advertising environment, allowing adequate exploration of the
ranking function parameter space without hurting the performance of the
commercial platform. An online learning part which further optimizes the
ranking functions by adapting to the online data distribution. Experimental
results on a large-scale sponsored search platform confirm the effectiveness of
the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Li He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kaipeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08784">
<title>From Random Differential Equations to Structural Causal Models: the stochastic case. (arXiv:1803.08784v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08784</link>
<description rdf:parseType="Literal">&lt;p&gt;Random Differential Equations provide a natural extension of Ordinary
Differential Equations to the stochastic setting. We show how, and under which
conditions, every equilibrium state of a Random Differential Equation (RDE) can
be described by a Structural Causal Model (SCM), while pertaining the causal
semantics. This provides an SCM that captures the stochastic and causal
behavior of the RDE, which can model both cycles and confounders. This enables
the study of the equilibrium states of the RDE by applying the theory and
statistical tools available for SCMs, for example, marginalizations and Markov
properties, as we illustrate by means of an example. Our work thus provides a
direct connection between two fields that so far have been developing in
isolation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bongers_S/0/1/0/all/0/1&quot;&gt;Stephan Bongers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mooij_J/0/1/0/all/0/1&quot;&gt;Joris M. Mooij&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08979">
<title>From Shannon&apos;s Channel to Semantic Channel via New Bayes&apos; Formulas for Machine Learning. (arXiv:1803.08979v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.08979</link>
<description rdf:parseType="Literal">&lt;p&gt;A group of transition probability functions form a Shannon&apos;s channel whereas
a group of truth functions form a semantic channel. By the third kind of Bayes&apos;
theorem, we can directly convert a Shannon&apos;s channel into an optimized semantic
channel. When a sample is not big enough, we can use a truth function with
parameters to produce the likelihood function, then train the truth function by
the conditional sampling distribution. The third kind of Bayes&apos; theorem is
proved. A semantic information theory is simply introduced. The semantic
information measure reflects Popper&apos;s hypothesis-testing thought. The Semantic
Information Method (SIM) adheres to maximum semantic information criterion
which is compatible with maximum likelihood criterion and Regularized Least
Squares criterion. It supports Wittgenstein&apos;s view: the meaning of a word lies
in its use. Letting the two channels mutually match, we obtain the Channels&apos;
Matching (CM) algorithm for machine learning. The CM algorithm is used to
explain the evolution of the semantic meaning of natural language, such as &quot;Old
age&quot;. The semantic channel for medical tests and the confirmation measures of
test-positive and test-negative are discussed. The applications of the CM
algorithm to semi-supervised learning and non-supervised learning are simply
introduced. As a predictive model, the semantic channel fits variable sources
and hence can overcome class-imbalance problem. The SIM strictly distinguishes
statistical probability and logical probability and uses both at the same time.
This method is compatible with the thoughts of Bayes, Fisher, Shannon, Zadeh,
Tarski, Davidson, Wittgenstein, and Popper.It is a competitive alternative to
Bayesian inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chenguang Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08993">
<title>Deep Learning Phase Segregation. (arXiv:1803.08993v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.08993</link>
<description rdf:parseType="Literal">&lt;p&gt;Phase segregation, the process by which the components of a binary mixture
spontaneously separate, is a key process in the evolution and design of many
chemical, mechanical, and biological systems. In this work, we present a
data-driven approach for the learning, modeling, and prediction of phase
segregation. A direct mapping between an initially dispersed, immiscible binary
fluid and the equilibrium concentration field is learned by conditional
generative convolutional neural networks. Concentration field predictions by
the deep learning model conserve phase fraction, correctly predict phase
transition, and reproduce area, perimeter, and total free energy distributions
up to 98% accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farimani_A/0/1/0/all/0/1&quot;&gt;Amir Barati Farimani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomes_J/0/1/0/all/0/1&quot;&gt;Joseph Gomes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;Rishi Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_F/0/1/0/all/0/1&quot;&gt;Franklin L. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pande_V/0/1/0/all/0/1&quot;&gt;Vijay S. Pande&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09018">
<title>The Importance of Constraint Smoothness for Parameter Estimation in Computational Cognitive Modeling. (arXiv:1803.09018v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1803.09018</link>
<description rdf:parseType="Literal">&lt;p&gt;Psychiatric neuroscience is increasingly aware of the need to define
psychopathology in terms of abnormal neural computation. The central tool in
this endeavour is the fitting of computational models to behavioural data. The
most prominent example of this procedure is fitting reinforcement learning (RL)
models to decision-making data collected from mentally ill and healthy subject
populations. These models are generative models of the decision-making data
themselves, and the parameters we seek to infer can be psychologically and
neurobiologically meaningful. Currently, the gold standard approach to this
inference procedure involves Monte-Carlo sampling, which is robust but
computationally intensive---rendering additional procedures, such as
cross-validation, impractical. Searching for point estimates of model
parameters using optimization procedures remains a popular and interesting
option. On a novel testbed simulating parameter estimation from a common RL
task, we investigated the effects of smooth vs. boundary constraints on
parameter estimation using interior point and deterministic direct search
algorithms for optimization. Ultimately, we show that the use of boundary
constraints can lead to substantial truncation effects. Our results discourage
the use of boundary constraints for these applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nunes_A/0/1/0/all/0/1&quot;&gt;Abraham Nunes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rudiuk_A/0/1/0/all/0/1&quot;&gt;Alexander Rudiuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09082">
<title>A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training. (arXiv:1803.09082v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09082</link>
<description rdf:parseType="Literal">&lt;p&gt;Training deep neural networks (DNNs) efficiently is a challenge due to the
associated highly nonconvex optimization. The backpropagation (backprop)
algorithm has long been the most widely used algorithm for gradient computation
of parameters of DNNs and is used along with gradient descent-type algorithms
for this optimization task. Recent work have shown the efficiency of block
coordinate descent (BCD) type methods empirically for training DNNs. In view of
this, we propose a novel algorithm based on the BCD method for training DNNs
and provide its global convergence results built upon the powerful framework of
the Kurdyka-Lojasiewicz (KL) property. Numerical experiments on standard
datasets demonstrate its competitive efficiency against standard optimizers
with backprop.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lau_T/0/1/0/all/0/1&quot;&gt;Tim Tsz-Kit Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Jinshan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09093">
<title>Comparing Generative Adversarial Network Techniques for Image Creation and Modification. (arXiv:1803.09093v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09093</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks (GANs) have demonstrated to be successful at
generating realistic real-world images. In this paper we compare various GAN
techniques, both supervised and unsupervised. The effects on training stability
of different objective functions are compared. We add an encoder to the
network, making it possible to encode images to the latent space of the GAN.
The generator, discriminator and encoder are parameterized by deep
convolutional neural networks. For the discriminator network we experimented
with using the novel Capsule Network, a state-of-the-art technique for
detecting global features in images. Experiments are performed using a digit
and face dataset, with various visualizations illustrating the results. The
results show that using the encoder network it is possible to reconstruct
images. With the conditional GAN we can alter visual attributes of generated or
encoded images. The experiments with the Capsule Network as discriminator
result in generated images of a lower quality, compared to a standard
convolutional neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pieters_M/0/1/0/all/0/1&quot;&gt;Mathijs Pieters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiering_M/0/1/0/all/0/1&quot;&gt;Marco Wiering&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09119">
<title>Gradient descent in Gaussian random fields as a toy model for high-dimensional optimisation in deep learning. (arXiv:1803.09119v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09119</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we model the loss function of high-dimensional optimization
problems by a Gaussian random field, or equivalently a Gaussian process. Our
aim is to study gradient descent in such loss functions or energy landscapes
and compare it to results obtained from real high-dimensional optimization
problems such as encountered in deep learning. In particular, we analyze the
distribution of the improved loss function after a step of gradient descent,
provide analytic expressions for the moments as well as prove asymptotic
normality as the dimension of the parameter space becomes large. Moreover, we
compare this with the expectation of the global minimum of the landscape
obtained by means of the Euler characteristic of excursion sets. Besides
complementing our analytical findings with numerical results from simulated
Gaussian random fields, we also compare it to loss functions obtained from
optimisation problems on synthetic and real data sets by proposing a &quot;black
box&quot; random field toy-model for a deep neural network loss function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chouza_M/0/1/0/all/0/1&quot;&gt;Mariano Chouza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zohren_S/0/1/0/all/0/1&quot;&gt;Stefan Zohren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09133">
<title>Social Media Analysis For Organizations: Us Northeastern Public And State Libraries Case Study. (arXiv:1803.09133v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1803.09133</link>
<description rdf:parseType="Literal">&lt;p&gt;Social networking sites such as Twitter have provided a great opportunity for
organizations such as public libraries to disseminate information for public
relations purposes. However, there is a need to analyze vast amounts of social
media data. This study presents a computational approach to explore the content
of tweets posted by nine public libraries in the northeastern United States of
America. In December 2017, this study extracted more than 19,000 tweets from
the Twitter accounts of seven state libraries and two urban public libraries.
Computational methods were applied to collect the tweets and discover
meaningful themes. This paper shows how the libraries have used Twitter to
represent their services and provides a starting point for different
organizations to evaluate the themes of their public tweets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1&quot;&gt;Matthew Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1&quot;&gt;Amir Karami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09153">
<title>Fast variational Bayes for heavy-tailed PLDA applied to i-vectors and x-vectors. (arXiv:1803.09153v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09153</link>
<description rdf:parseType="Literal">&lt;p&gt;The standard state-of-the-art backend for text-independent speaker
recognizers that use i-vectors or x-vectors, is Gaussian PLDA (G-PLDA),
assisted by a Gaussianization step involving length normalization. G-PLDA can
be trained with both generative or discriminative methods. It has long been
known that heavy-tailed PLDA (HT-PLDA), applied without length normalization,
gives similar accuracy, but at considerable extra computational cost. We have
recently introduced a fast scoring algorithm for a discriminatively trained
HT-PLDA backend. This paper extends that work by introducing a fast,
variational Bayes, generative training algorithm. We compare old and new
backends, with and without length-normalization, with i-vectors and x-vectors,
on SRE&apos;10, SRE&apos;16 and SITW.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Silnova_A/0/1/0/all/0/1&quot;&gt;Anna Silnova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brummer_N/0/1/0/all/0/1&quot;&gt;Niko Brummer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garcia_Romero_D/0/1/0/all/0/1&quot;&gt;Daniel Garcia-Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Snyder_D/0/1/0/all/0/1&quot;&gt;David Snyder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Burget_L/0/1/0/all/0/1&quot;&gt;Lukas Burget&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09159">
<title>Efficient Discovery of Heterogeneous Treatment Effects in Randomized Experiments via Anomalous Pattern Detection. (arXiv:1803.09159v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1803.09159</link>
<description rdf:parseType="Literal">&lt;p&gt;The randomized experiment is an important tool for inferring the causal
impact of an intervention. The recent literature on statistical learning
methods for heterogeneous treatment effects demonstrates the utility of
estimating the marginal conditional average treatment effect (MCATE), i.e., the
average treatment effect for a subpopulation of respondents who share a
particular subset of covariates. However, each proposed method makes its own
set of restrictive assumptions about the intervention&apos;s effects, the underlying
data generating processes, and which subpopulations (MCATEs) to explicitly
estimate. Moreover, the majority of the literature provides no mechanism to
identify which subpopulations are the most affected--beyond manual
inspection--and provides little guarantee on the correctness of the identified
subpopulations. Therefore, we propose Treatment Effect Subset Scan (TESS), a
new method for discovering which subpopulation in a randomized experiment is
most significantly affected by a treatment. We frame this challenge as a
pattern detection problem where we maximize a nonparametric scan statistic
(measurement of distributional divergence) over subpopulations, while being
parsimonious in which specific subpopulations to evaluate. Furthermore, we
identify the subpopulation which experiences the largest distributional change
as a result of the intervention, while making minimal assumptions about the
intervention&apos;s effects or the underlying data generating process. In addition
to the algorithm, we demonstrate that the asymptotic Type I and II error can be
controlled, and provide sufficient conditions for detection consistency---i.e.,
exact identification of the affected subpopulation. Finally, we validate the
efficacy of the method by discovering heterogeneous treatment effects in
simulations and in real-world data from a well-known program evaluation study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McFowland_E/0/1/0/all/0/1&quot;&gt;Edward McFowland III&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Somanchi_S/0/1/0/all/0/1&quot;&gt;Sriram Somanchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neill_D/0/1/0/all/0/1&quot;&gt;Daniel B. Neill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09160">
<title>Handling Adversarial Concept Drift in Streaming Data. (arXiv:1803.09160v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09160</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifiers operating in a dynamic, real world environment, are vulnerable to
adversarial activity, which causes the data distribution to change over time.
These changes are traditionally referred to as concept drift, and several
approaches have been developed in literature to deal with the problem of drift
handling and detection. However, most concept drift handling techniques,
approach it as a domain independent task, to make them applicable to a wide
gamut of reactive systems. These techniques were developed from an adversarial
agnostic perspective, where they are naive and assume that drift is a benign
change, which can be fixed by updating the model. However, this is not the case
when an active adversary is trying to evade the deployed classification system.
In such an environment, the properties of concept drift are unique, as the
drift is intended to degrade the system and at the same time designed to avoid
detection by traditional concept drift detection techniques. This special
category of drift is termed as adversarial drift, and this paper analyzes its
characteristics and impact, in a streaming environment. A novel framework for
dealing with adversarial concept drift is proposed, called the Predict-Detect
streaming framework. Experimental evaluation of the framework, on generated
adversarial drifting data streams, demonstrates that this framework is able to
provide reliable unsupervised indication of drift, and is able to recover from
drifts swiftly. While traditional partially labeled concept drift detection
methodologies fail to detect adversarial drifts, the proposed framework is able
to detect such drifts and operates with &amp;lt;6% labeled data, on average. Also, the
framework provides benefits for active learning over imbalanced data streams,
by innately providing for feature space honeypots, where minority class
adversarial samples may be captured.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1&quot;&gt;Tegjyot Singh Sethi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantardzic_M/0/1/0/all/0/1&quot;&gt;Mehmed Kantardzic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09162">
<title>A Dynamic-Adversarial Mining Approach to the Security of Machine Learning. (arXiv:1803.09162v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09162</link>
<description rdf:parseType="Literal">&lt;p&gt;Operating in a dynamic real world environment requires a forward thinking and
adversarial aware design for classifiers, beyond fitting the model to the
training data. In such scenarios, it is necessary to make classifiers - a)
harder to evade, b) easier to detect changes in the data distribution over
time, and c) be able to retrain and recover from model degradation. While most
works in the security of machine learning has concentrated on the evasion
resistance (a) problem, there is little work in the areas of reacting to
attacks (b and c). Additionally, while streaming data research concentrates on
the ability to react to changes to the data distribution, they often take an
adversarial agnostic view of the security problem. This makes them vulnerable
to adversarial activity, which is aimed towards evading the concept drift
detection mechanism itself. In this paper, we analyze the security of machine
learning, from a dynamic and adversarial aware perspective. The existing
techniques of Restrictive one class classifier models, Complex learning models
and Randomization based ensembles, are shown to be myopic as they approach
security as a static task. These methodologies are ill suited for a dynamic
environment, as they leak excessive information to an adversary, who can
subsequently launch attacks which are indistinguishable from the benign data.
Based on empirical vulnerability analysis against a sophisticated adversary, a
novel feature importance hiding approach for classifier design, is proposed.
The proposed design ensures that future attacks on classifiers can be detected
and recovered from. The proposed work presents motivation, by serving as a
blueprint, for future work in the area of Dynamic-Adversarial mining, which
combines lessons learned from Streaming data mining, Adversarial learning and
Cybersecurity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1&quot;&gt;Tegjyot Singh Sethi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantardzic_M/0/1/0/all/0/1&quot;&gt;Mehmed Kantardzic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyua_L/0/1/0/all/0/1&quot;&gt;Lingyu Lyua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiashun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09163">
<title>Security Theater: On the Vulnerability of Classifiers to Exploratory Attacks. (arXiv:1803.09163v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09163</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing scale and sophistication of cyberattacks has led to the
adoption of machine learning based classification techniques, at the core of
cybersecurity systems. These techniques promise scale and accuracy, which
traditional rule or signature based methods cannot. However, classifiers
operating in adversarial domains are vulnerable to evasion attacks by an
adversary, who is capable of learning the behavior of the system by employing
intelligently crafted probes. Classification accuracy in such domains provides
a false sense of security, as detection can easily be evaded by carefully
perturbing the input samples. In this paper, a generic data driven framework is
presented, to analyze the vulnerability of classification systems to black box
probing based attacks. The framework uses an exploration exploitation based
strategy, to understand an adversary&apos;s point of view of the attack defense
cycle. The adversary assumes a black box model of the defender&apos;s classifier and
can launch indiscriminate attacks on it, without information of the defender&apos;s
model type, training data or the domain of application. Experimental evaluation
on 10 real world datasets demonstrates that even models having high perceived
accuracy (&amp;gt;90%), by a defender, can be effectively circumvented with a high
evasion rate (&amp;gt;95%, on average). The detailed attack algorithms, adversarial
model and empirical evaluation, serve.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1&quot;&gt;Tegjyot Singh Sethi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantardzic_M/0/1/0/all/0/1&quot;&gt;Mehmed Kantardzic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1&quot;&gt;Joung Woo Ryu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09177">
<title>Balanced Random Survival Forests for Extremely Unbalanced, Right Censored Data. (arXiv:1803.09177v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09177</link>
<description rdf:parseType="Literal">&lt;p&gt;Accuracies of survival models for life expectancy prediction as well as
lifesaving critical-care applications are significantly compromised due to the
sparsity of samples and extreme imbalance between the survival and mortality
classes in addition to the invalidity of the popular proportional hazard
assumption. An imbalance in data results in an underestimation (overestimation)
of the hazard of the mortality (survival) classes. Balanced random survival
forests (BRSF) model, based on training random survival forests with balanced
data generated from a synthetic minority sampling scheme is presented to
address this gap. Theoretical findings on the improvement of survival
prediction after balancing are corroborated using extensive empirical
evaluations. Benchmarking studies consider five data sets of different levels
of class imbalance from public repositories and an imbalanced survival data set
of 267 ST-elevated myocardial infarction (STEMI) patients collected over a
period of one year at Heart, Artery, and Vein Center of Fresno, CA.
Investigations suggest BRSF provides a better discriminatory strength between
the censored and the mortality classes and improves survival prediction of the
minority. BRSF outperformed both optimized Cox (without and with balancing) and
RSF with a 55% reduction (averaged over all 6 data sets) in prediction error
over the next best alternative.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Afrin_K/0/1/0/all/0/1&quot;&gt;Kahkashan Afrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Illangovan_G/0/1/0/all/0/1&quot;&gt;Gurudev Illangovan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srivatsa_S/0/1/0/all/0/1&quot;&gt;Sanjay S. Srivatsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bukkapatnam_S/0/1/0/all/0/1&quot;&gt;Satish T. S. Bukkapatnam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09191">
<title>Network archaeology: phase transition in the recoverability of network history. (arXiv:1803.09191v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/1803.09191</link>
<description rdf:parseType="Literal">&lt;p&gt;Network growth processes can be understood as generative models of the
structure and history of complex networks. This point of view naturally leads
to the problem of network archaeology: Reconstructing all the past states of a
network from its structure---a difficult permutation inference problem. In this
paper, we introduce a Bayesian formulation of network archaeology, with a
generalization of preferential attachment as our generative mechanism. We
develop a sequential importance sampling algorithm to evaluate the posterior
averages of this model, as well as an efficient heuristic that uncovers the
history of a network in linear time. We use these methods to identify and
characterize a phase transition in the quality of the reconstructed history,
when they are applied to artificial networks generated by the model itself.
Despite the existence of a no-recovery phase, we find that non-trivial
inference is possible in a large portion of the parameter space as well as on
empirical data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Young_J/0/1/0/all/0/1&quot;&gt;Jean-Gabriel Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hebert_Dufresne_L/0/1/0/all/0/1&quot;&gt;Laurent H&amp;#xe9;bert-Dufresne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Laurence_E/0/1/0/all/0/1&quot;&gt;Edward Laurence&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Murphy_C/0/1/0/all/0/1&quot;&gt;Charles Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+St_Onge_G/0/1/0/all/0/1&quot;&gt;Guillaume St-Onge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Desrosiers_P/0/1/0/all/0/1&quot;&gt;Patrick Desrosiers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09202">
<title>Unsupervised Depth Estimation, 3D Face Rotation and Replacement. (arXiv:1803.09202v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.09202</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an unsupervised approach for learning to estimate three
dimensional (3D) facial structure from a single image while also predicting 3D
viewpoint transformations that match a desired pose and facial geometry. We
achieve this by inferring the depth of facial key-points in an input image in
an unsupervised way. We show how it is possible to use these depths as
intermediate computations within a new backpropable loss to predict the
parameters of a 3D affine transformation matrix that maps inferred 3D
key-points of an input face to corresponding 2D key-points on a desired target
facial geometry or pose. Our resulting approach can therefore be used to infer
plausible 3D transformations from one face pose to another, allowing faces to
be frontalized, trans- formed into 3D models or even warped to another pose and
facial geometry. Lastly, we identify certain shortcomings with our formulation,
and explore adversarial image translation techniques as a post-processing step.
Correspondingly, we explore several adversarial image transformation methods
which allow us to re-synthesize complete head shots for faces re-targeted to
different poses as well as repair images resulting from face replacements
across identities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1&quot;&gt;Joel Ruben Antony Moniz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beckham_C/0/1/0/all/0/1&quot;&gt;Christopher Beckham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajotte_S/0/1/0/all/0/1&quot;&gt;Simon Rajotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honari_S/0/1/0/all/0/1&quot;&gt;Sina Honari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1&quot;&gt;Christopher Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09237">
<title>Goldbach&apos;s Function Approximation Using Deep Learning. (arXiv:1803.09237v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09237</link>
<description rdf:parseType="Literal">&lt;p&gt;Goldbach conjecture is one of the most famous open mathematical problems. It
states that every even number, bigger than two, can be presented as a sum of 2
prime numbers. % In this work we present a deep learning based model that
predicts the number of Goldbach partitions for a given even number.
Surprisingly, our model outperforms all state-of-the-art analytically derived
estimations for the number of couples, while not requiring prime factorization
of the given number. We believe that building a model that can accurately
predict the number of couples brings us one step closer to solving one of the
world most famous open problems. To the best of our knowledge, this is the
first attempt to consider machine learning based data-driven methods to
approximate open mathematical problems in the field of number theory, and hope
that this work will encourage such attempts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stekel_A/0/1/0/all/0/1&quot;&gt;Avigail Stekel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chkroun_M/0/1/0/all/0/1&quot;&gt;Merav Chkroun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1&quot;&gt;Amos Azaria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09318">
<title>Data-driven Discovery of Closure Models. (arXiv:1803.09318v1 [math.DS])</title>
<link>http://arxiv.org/abs/1803.09318</link>
<description rdf:parseType="Literal">&lt;p&gt;Derivation of reduced order representations of dynamical systems requires the
modeling of the truncated dynamics on the retained dynamics. In its most
general form, this so-called closure model has to account for memory effects.
In this work, we present a framework of operator inference to extract the
governing dynamics of closure from data in a compact, non-Markovian form. We
employ sparse polynomial regression and artificial neural networks to extract
the underlying operator. For a special class of non-linear systems,
observability of the closure in terms of the resolved dynamics is analyzed and
theoretical results are presented on the compactness of the memory. The
proposed framework is evaluated on examples consisting of linear to nonlinear
systems with and without chaotic dynamics, with an emphasis on predictive
performance on unseen data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shaowu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Duraisamy_K/0/1/0/all/0/1&quot;&gt;Karthik Duraisamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09319">
<title>SUNLayer: Stable denoising with generative networks. (arXiv:1803.09319v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09319</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been experimentally established that deep neural networks can be used
to produce good generative models for real world data. It has also been
established that such generative models can be exploited to solve classical
inverse problems like compressed sensing and super resolution. In this work we
focus on the classical signal processing problem of image denoising. We propose
a theoretical setting that uses spherical harmonics to identify what
mathematical properties of the activation functions will allow signal denoising
with local methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mixon_D/0/1/0/all/0/1&quot;&gt;Dustin G. Mixon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villar_S/0/1/0/all/0/1&quot;&gt;Soledad Villar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09349">
<title>Logistic Regression: The Importance of Being Improper. (arXiv:1803.09349v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09349</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning linear predictors with the logistic loss---both in stochastic and
online settings---is a fundamental task in learning and statistics, with direct
connections to classification and boosting. Existing &quot;fast rates&quot; for this
setting exhibit exponential dependence on the predictor norm, and Hazan et al.
(2014) showed that this is unfortunately unimprovable. Starting with the simple
observation that the logistic loss is 1-mixable, we design a new efficient
improper learning algorithm for online logistic regression that circumvents the
aforementioned lower bound with a regret bound exhibiting a doubly-exponential
improvement in dependence on the predictor norm. This provides a positive
resolution to a variant of the COLT 2012 open problem of McMahan and Streeter
(2012) when improper learning is allowed. This improvement is obtained both in
the online setting and, with some extra work, in the batch statistical setting
with high probability. We also show that the improved dependency on predictor
norm is also near-optimal.
&lt;/p&gt;
&lt;p&gt;Leveraging this improved dependency on the predictor norm yields the
following applications: (a) we give algorithms for online bandit multiclass
learning with the logistic loss with an $\tilde{O}(\sqrt{n})$ relative mistake
bound across essentially all parameter ranges, thus providing a solution to the
COLT 2009 open problem of Abernethy and Rakhlin (2009), and (b) we give an
adaptive algorithm for online multiclass boosting with optimal sample
complexity, thus partially resolving an open problem of Beygelzimer et al.
(2015) and Jung et al. (2017). Finally, we give information-theoretic bounds on
the optimal rates for improper logistic regression with general function
classes, thereby characterizing the extent to which our improvement for linear
classes extends to other parameteric and even nonparametric settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1&quot;&gt;Dylan J. Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1&quot;&gt;Satyen Kale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Haipeng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1&quot;&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1&quot;&gt;Karthik Sridharan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09353">
<title>Stochastic bandits robust to adversarial corruptions. (arXiv:1803.09353v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09353</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new model of stochastic bandits with adversarial corruptions
which aims to capture settings where most of the input follows a stochastic
pattern but some fraction of it can be adversarially changed to trick the
algorithm, e.g., click fraud, fake reviews and email spam. The goal of this
model is to encourage the design of bandit algorithms that (i) work well in
mixed adversarial and stochastic models, and (ii) whose performance
deteriorates gracefully as we move from fully stochastic to fully adversarial
models.
&lt;/p&gt;
&lt;p&gt;In our model, the rewards for all arms are initially drawn from a
distribution and are then altered by an adaptive adversary. We provide a simple
algorithm whose performance gracefully degrades with the total corruption the
adversary injected in the data, measured by the sum across rounds of the
biggest alteration the adversary made in the data in that round; this total
corruption is denoted by $C$. Our algorithm provides a guarantee that retains
the optimal guarantee (up to a logarithmic term) if the input is stochastic and
whose performance degrades linearly to the amount of corruption $C$, while
crucially being agnostic to it. We also provide a lower bound showing that this
linear degradation is necessary if the algorithm achieves optimal performance
in the stochastic setting (the lower bound works even for a known amount of
corruption, a special case in which our algorithm achieves optimal performance
without the extra logarithm).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lykouris_T/0/1/0/all/0/1&quot;&gt;Thodoris Lykouris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1&quot;&gt;Vahab Mirrokni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leme_R/0/1/0/all/0/1&quot;&gt;Renato Paes Leme&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09357">
<title>Minimizing Nonconvex Population Risk from Rough Empirical Risk. (arXiv:1803.09357v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09357</link>
<description rdf:parseType="Literal">&lt;p&gt;Population risk---the expectation of the loss over the sampling
mechanism---is always of primary interest in machine learning. However,
learning algorithms only have access to empirical risk, which is the average
loss over training examples. Although the two risks are typically guaranteed to
be pointwise close, for applications with nonconvex nonsmooth losses (such as
modern deep networks), the effects of sampling can transform a well-behaved
population risk into an empirical risk with a landscape that is problematic for
optimization. The empirical risk can be nonsmooth, and it may have many
additional local minima.
&lt;/p&gt;
&lt;p&gt;This paper considers a general optimization framework which aims to find
approximate local minima of a smooth nonconvex function $F$ (population risk)
given only access to the function value of another function $f$ (empirical
risk), which is pointwise close to $F$ (i.e., $\|F-f\|_{\infty} \le \nu$). We
propose a simple algorithm based on stochastic gradient descent (SGD) on a
smoothed version of $f$ which is guaranteed to find an $\epsilon$-second-order
stationary point if $\nu \le O(\epsilon^{1.5}/d)$, thus escaping all saddle
points of $F$ and all the additional local minima introduced by $f$. We also
provide an almost matching lower bound showing that our SGD-based approach
achieves the optimal trade-off between $\nu$ and $\epsilon$, as well as the
optimal dependence on problem dimension $d$, among all algorithms making a
polynomial number of queries. As a concrete example, we show that our results
can be directly used to give sample complexities for learning a ReLU unit,
whose empirical risk is nonsmooth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lydia T. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1&quot;&gt;Rong Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09374">
<title>Generalized Hadamard-Product Fusion Operators for Visual Question Answering. (arXiv:1803.09374v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09374</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a generalized class of multimodal fusion operators for the task of
visual question answering (VQA). We identify generalizations of existing
multimodal fusion operators based on the Hadamard product, and show that
specific non-trivial instantiations of this generalized fusion operator exhibit
superior performance in terms of OpenEnded accuracy on the VQA task. In
particular, we introduce Nonlinearity Ensembling, Feature Gating, and
post-fusion neural network layers as fusion operator components, culminating in
an absolute percentage point improvement of $1.9\%$ on the VQA 2.0 test-dev set
over baseline fusion operators, which use the same features as input. We use
our findings as evidence that our generalized class of fusion operators could
lead to the discovery of even superior task-specific operators when used as a
search space in an architecture search over fusion operators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duke_B/0/1/0/all/0/1&quot;&gt;Brendan Duke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1&quot;&gt;Graham W. Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09460">
<title>Scalable inference for crossed random effects models. (arXiv:1803.09460v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1803.09460</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze the complexity of Gibbs samplers for inference in crossed random
effect models used in modern analysis of variance. We demonstrate that for
certain designs the plain vanilla Gibbs sampler is not scalable, in the sense
that its complexity is worse than proportional to the number of parameters and
data. We thus propose a simple modification leading to a collapsed Gibbs
sampler that is provably scalable. Although our theory requires some
balancedness assumptions on the data designs, we demonstrate in simulated and
real datasets that the rates it predicts match remarkably the correct rates in
cases where the assumptions are violated. We also show that the collapsed Gibbs
sampler, extended to sample further unknown hyperparameters, outperforms
significantly alternative state of the art algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papaspiliopoulos_O/0/1/0/all/0/1&quot;&gt;Omiros Papaspiliopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_G/0/1/0/all/0/1&quot;&gt;Gareth O. Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zanella_G/0/1/0/all/0/1&quot;&gt;Giacomo Zanella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09468">
<title>Clipping free attacks against artificial neural networks. (arXiv:1803.09468v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09468</link>
<description rdf:parseType="Literal">&lt;p&gt;During the last years, a remarkable breakthrough has been made in AI domain
thanks to artificial deep neural networks that achieved a great success in many
machine learning tasks in computer vision, natural language processing, speech
recognition, malware detection and so on. However, they are highly vulnerable
to easily crafted adversarial examples. Many investigations have pointed out
this fact and different approaches have been proposed to generate attacks while
adding a limited perturbation to the original data. The most robust known
method so far is the so called C&amp;amp;W attack [1]. Nonetheless, a countermeasure
known as feature squeezing coupled with ensemble defense showed that most of
these attacks can be destroyed [6]. In this paper, we present a new method we
call Centered Initial Attack (CIA) whose advantage is twofold : first, it
insures by construction the maximum perturbation to be smaller than a threshold
fixed beforehand, without the clipping process that degrades the quality of
attacks. Second, it is robust against recently introduced defenses such as
feature squeezing, JPEG encoding and even against a voting ensemble of
defenses. While its application is not limited to images, we illustrate this
using five of the current best classifiers on ImageNet dataset among which two
are adversarialy retrained on purpose to be robust against attacks. With a
fixed maximum perturbation of only 1.5% on any pixel, around 80% of attacks
(targeted) fool the voting ensemble defense and nearly 100% when the
perturbation is only 6%. While this shows how it is difficult to defend against
CIA attacks, the last section of the paper gives some guidelines to limit their
impact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Addad_B/0/1/0/all/0/1&quot;&gt;Boussad Addad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kodjabashian_J/0/1/0/all/0/1&quot;&gt;Jerome Kodjabashian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1&quot;&gt;Christophe Meyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09518">
<title>Fr\&apos;echet ChemblNet Distance: A metric for generative models for molecules. (arXiv:1803.09518v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09518</link>
<description rdf:parseType="Literal">&lt;p&gt;The new wave of successful generative models in machine learning has
increased the interest in deep learning driven de novo drug design. However,
assessing the performance of such generative models is notoriously difficult.
Metrics that are typically used to assess the performance of such generative
models are the percentage of chemically valid molecules or the similarity to
real molecules in terms of particular descriptors, such as the partition
coefficient (logP) or druglikeness. However, method comparison is difficult
because of the inconsistent use of evaluation metrics, the necessity for
multiple metrics, and the fact that some of these measures can easily be
tricked by simple rule-based systems. We propose a novel distance measure
between two sets of molecules, called Fr\&apos;echet ChemblNet distance (FCD), that
can be used as an evaluation metric for generative models. The FCD is similar
to a recently established performance metric for comparing image generation
methods, the Fr\&apos;echet Inception Distance (FID). Whereas the FID uses one of
the hidden layers of InceptionNet, the FCD utilizes the penultimate layer of a
deep neural network called &quot;ChemblNet&quot;, which was trained to predict drug
activities. Thus, the FCD metric takes into account chemically and biologically
relevant information about molecules, and also measures the diversity of the
set via the distribution of generated molecules. The FCD&apos;s advantage over
previous metrics is that it can detect if generated molecules are a) diverse
and have similar b) chemical and c) biological properties as real molecules. We
further provide an easy-to-use implementation that only requires the SMILES
representation of the generated molecules as input to calculate the FCD.
Implementations are available at: https://www.github.com/bioinf-jku/FCD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preuer_K/0/1/0/all/0/1&quot;&gt;Kristina Preuer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renz_P/0/1/0/all/0/1&quot;&gt;Philipp Renz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1&quot;&gt;Thomas Unterthiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1&quot;&gt;Sepp Hochreiter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klambauer_G/0/1/0/all/0/1&quot;&gt;G&amp;#xfc;nter Klambauer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09533">
<title>Deep Representation for Patient Visits from Electronic Health Records. (arXiv:1803.09533v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1803.09533</link>
<description rdf:parseType="Literal">&lt;p&gt;We show how to learn low-dimensional representations (embeddings) of patient
visits from the corresponding electronic health record (EHR) where
International Classification of Diseases (ICD) diagnosis codes are removed. We
expect that these embeddings will be useful for the construction of predictive
statistical models anticipated to drive personalized medicine and improve
healthcare quality. These embeddings are learned using a deep neural network
trained to predict ICD diagnosis categories. We show that our embeddings
capture relevant clinical informations and can be used directly as input to
standard machine learning algorithms like multi-output classifiers for ICD code
prediction. We also show that important medical informations correspond to
particular directions in our embedding space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escudie_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Escudi&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saade_A/0/1/0/all/0/1&quot;&gt;Alaa Saade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coucke_A/0/1/0/all/0/1&quot;&gt;Alice Coucke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lelarge_M/0/1/0/all/0/1&quot;&gt;Marc Lelarge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09539">
<title>Revisiting First-Order Convex Optimization Over Linear Spaces. (arXiv:1803.09539v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09539</link>
<description rdf:parseType="Literal">&lt;p&gt;Two popular examples of first-order optimization methods over linear spaces
are coordinate descent and matching pursuit algorithms, with their randomized
variants. While the former targets the optimization by moving along
coordinates, the latter considers a generalized notion of directions.
Exploiting the connection between the two algorithms, we present a unified
analysis of both, providing affine invariant sublinear $\mathcal{O}(1/t)$ rates
on smooth objectives and linear convergence on strongly convex objectives. As a
byproduct of our affine invariant analysis of matching pursuit, our rates for
steepest coordinate descent are the tightest known. Furthermore, we show the
first accelerated convergence rate $\mathcal{O}(1/t^2)$ for matching pursuit on
convex objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Anant Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Reddy_S/0/1/0/all/0/1&quot;&gt;Sai Praneeth Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1&quot;&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stich_S/0/1/0/all/0/1&quot;&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09546">
<title>Calibrated Prediction Intervals for Neural Network Regressors. (arXiv:1803.09546v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09546</link>
<description rdf:parseType="Literal">&lt;p&gt;Ongoing developments in neural network models are continually advancing the
state-of-the-art in terms of system accuracy. However, the predicted labels
should not be regarded as the only core output; also important is a well
calibrated estimate of the prediction uncertainty. Such estimates and their
calibration is critical in relation to robust handling of out of distribution
events not observed in training data. Despite their obvious aforementioned
advantage in relation to accuracy, contemporary neural networks can, generally,
be regarded as poorly calibrated and as such do not produce reliable output
probability estimates. Further, while post-processing calibration solutions can
be found in the relevant literature, these tend to be for systems performing
classification. In this regard, we herein present a method for acquiring
calibrated predictions intervals for neural network regressors by posing the
regression task as a multi-class classification problem and applying one of
three proposed calibration methods on the classifiers&apos; output. Testing our
method on two exemplar tasks - speaker age prediction and signal-to-noise ratio
estimation - indicates both the suitability of the classification-based
regression models and that post-processing by our proposed empirical
calibration or temperature scaling methods yields well calibrated prediction
intervals. The code for computing calibrated predicted intervals is publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Keren_G/0/1/0/all/0/1&quot;&gt;Gil Keren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cummins_N/0/1/0/all/0/1&quot;&gt;Nicholas Cummins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schuller_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Schuller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09621">
<title>Learning the Multiple Traveling Salesmen Problem with Permutation Invariant Pooling Networks. (arXiv:1803.09621v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09621</link>
<description rdf:parseType="Literal">&lt;p&gt;While there are optimal TSP solvers as well as recent learning-based
approaches, the generalization of the TSP to the multiple Traveling Salesmen
Problem is much less studied. Here, we design a neural network solution that
treats the salesmen, the cities, and the depot as three different sets of
varying cardinalities. Coupled with a normalization algorithm, a dedicated
loss, and a search method, our solution is shown in two benchmarks to
outperform all the meta-heuristics of the leading solver in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaempfer_Y/0/1/0/all/0/1&quot;&gt;Yoav Kaempfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1&quot;&gt;Lior Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09638">
<title>On the Limitation of Local Intrinsic Dimensionality for Characterizing the Subspaces of Adversarial Examples. (arXiv:1803.09638v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09638</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding and characterizing the subspaces of adversarial examples aid in
studying the robustness of deep neural networks (DNNs) to adversarial
perturbations. Very recently, Ma et al. (ICLR 2018) proposed to use local
intrinsic dimensionality (LID) in layer-wise hidden representations of DNNs to
study adversarial subspaces. It was demonstrated that LID can be used to
characterize the adversarial subspaces associated with different attack
methods, e.g., the Carlini and Wagner&apos;s (C&amp;amp;W) attack and the fast gradient sign
attack.
&lt;/p&gt;
&lt;p&gt;In this paper, we use MNIST and CIFAR-10 to conduct two new sets of
experiments that are absent in existing LID analysis and report the limitation
of LID in characterizing the corresponding adversarial subspaces, which are (i)
oblivious attacks and LID analysis using adversarial examples with different
confidence levels; and (ii) black-box transfer attacks. For (i), we find that
the performance of LID is very sensitive to the confidence parameter deployed
by an attack, and the LID learned from ensembles of adversarial examples with
varying confidence levels surprisingly gives poor performance. For (ii), we
find that when adversarial examples are crafted from another DNN model, LID is
ineffective in characterizing their adversarial subspaces. These two findings
together suggest the limited capability of LID in characterizing the subspaces
of adversarial examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pei-Hsuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chia-Mu Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09655">
<title>BAGAN: Data Augmentation with Balancing GAN. (arXiv:1803.09655v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.09655</link>
<description rdf:parseType="Literal">&lt;p&gt;Image classification datasets are often imbalanced, characteristic that
negatively affects the accuracy of deeplearning classifiers. In this work we
propose balancing GANs (BAGANs) as an augmentation tool to restore balance in
imbalanced datasets. This is challenging because the few minority-class images
may not be enough to train a GAN. We overcome this issue by including during
training all available images of majority and minority classes. The generative
model learns useful features from majority classes and uses these to generate
images for minority classes. We apply class-conditioning in the latent space to
drive the generation process towards a target class. Additionally, we couple
GANs with autoencoding techniques to reduce the risk of collapsing toward the
generation of few foolish examples. We compare the proposed methodology with
state-of-the-art GANs and demonstrate that BAGAN generates images of superior
quality when trained with an imbalanced dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mariani_G/0/1/0/all/0/1&quot;&gt;Giovanni Mariani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheidegger_F/0/1/0/all/0/1&quot;&gt;Florian Scheidegger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Istrate_R/0/1/0/all/0/1&quot;&gt;Roxana Istrate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bekas_C/0/1/0/all/0/1&quot;&gt;Costas Bekas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malossi_C/0/1/0/all/0/1&quot;&gt;Cristiano Malossi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09704">
<title>MOrdReD: Memory-based Ordinal Regression Deep Neural Networks for Time Series Forecasting. (arXiv:1803.09704v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09704</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series forecasting is ubiquitous in the modern world. Applications range
from health care to astronomy, include climate modelling, financial trading and
monitoring of critical engineering equipment. To offer value over this range of
activities we must have models that not only provide accurate forecasts but
that also quantify and adjust their uncertainty over time. Furthermore, such
models must allow for multimodal, non-Gaussian behaviour that arises regularly
in applied settings. In this work, we propose a novel, end-to-end deep learning
method for time series forecasting. Crucially, our model allows the principled
assessment of predictive uncertainty as well as providing rich information
regarding multiple modes of future data values. Our approach not only provides
an excellent predictive forecast, shadowing true future values, but also allows
us to infer valuable information, such as the predictive distribution of the
occurrence of critical events of interest, accurately and reliably even over
long time horizons. We find the method outperforms other state-of-the-art
algorithms, such as Gaussian Processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Orozco_B/0/1/0/all/0/1&quot;&gt;Bernardo P&amp;#xe9;rez Orozco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abbati_G/0/1/0/all/0/1&quot;&gt;Gabriele Abbati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09730">
<title>Resilient Active Information Gathering with Mobile Robots. (arXiv:1803.09730v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1803.09730</link>
<description rdf:parseType="Literal">&lt;p&gt;Applications in robotics, such as multi-robot target tracking, involve the
execution of information acquisition tasks by teams of mobile robots. However,
in failure-prone or adversarial environments, robots get attacked, their
communication channels get jammed, and their sensors fail, resulting in the
withdrawal of robots from the collective task, and, subsequently, the inability
of the remaining active robots to coordinate with each other. As a result,
traditional design paradigms become insufficient and, in contrast, resilient
designs against system-wide failures and attacks become important. In general,
resilient design problems are hard, and even though they often involve
objective functions that are monotone and (possibly) submodular, scalable
approximation algorithms for their solution have been hitherto unknown. In this
paper, we provide the first algorithm, enabling the following capabilities:
minimal communication, i.e., the algorithm is executed by the robots based only
on minimal communication between them, system-wide resiliency, i.e., the
algorithm is valid for any number of denial-of-service attacks and failures,
and provable approximation performance, i.e., the algorithm ensures for all
monotone and (possibly) submodular objective functions a solution that is
finitely close to the optimal. We support our theoretical analyses with
simulated and real-world experiments, by considering an active information
acquisition application scenario, namely, multi-robot target tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlotfeldt_B/0/1/0/all/0/1&quot;&gt;Brent Schlotfeldt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzoumas_V/0/1/0/all/0/1&quot;&gt;Vasileios Tzoumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakur_D/0/1/0/all/0/1&quot;&gt;Dinesh Thakur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1&quot;&gt;George J. Pappas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09737">
<title>DJAM: distributed Jacobi asynchronous method for learning personal models. (arXiv:1803.09737v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09737</link>
<description rdf:parseType="Literal">&lt;p&gt;Processing data collected by a network of agents often boils down to solving
an optimization problem. The distributed nature of these problems calls for
methods that are, themselves, distributed. While most collaborative learning
problems require agents to reach a common (or consensus) model, there are
situations in which the consensus solution may not be optimal. For instance,
agents may want to reach a compromise between agreeing with their neighbors and
minimizing a personal loss function. We present DJAM, a Jacobi-like distributed
algorithm for learning personalized models. This method is
implementation-friendly: it has no hyperparameters that need tuning, it is
asynchronous, and its updates only require single-neighbor interactions. We
prove that DJAM converges with probability one to the solution, provided that
the personal loss functions are strongly convex and have Lipschitz gradient. We
then give evidence that DJAM is on par with state-of-the-art methods: our
method reaches a solution with error similar to the error of a carefully tuned
ADMM in about the same number of single-neighbor interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_I/0/1/0/all/0/1&quot;&gt;In&amp;#xea;s Almeida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xavier_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Xavier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09791">
<title>A Common Framework for Natural Gradient and Taylor based Optimisation using Manifold Theory. (arXiv:1803.09791v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09791</link>
<description rdf:parseType="Literal">&lt;p&gt;This technical report constructs a theoretical framework to relate standard
Taylor approximation based optimisation methods with Natural Gradient (NG), a
method which is Fisher efficient with probabilistic models. Such a framework
will be shown to also provide mathematical justification to combine higher
order methods with the method of NG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haider_A/0/1/0/all/0/1&quot;&gt;Adnan Haider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09862">
<title>A Decision Tree Approach to Predicting Recidivism in Domestic Violence. (arXiv:1803.09862v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09862</link>
<description rdf:parseType="Literal">&lt;p&gt;Domestic violence (DV) is a global social and public health issue that is
highly gendered. Being able to accurately predict DV recidivism, i.e.,
re-offending of a previously convicted offender, can speed up and improve risk
assessment procedures for police and front-line agencies, better protect
victims of DV, and potentially prevent future re-occurrences of DV. Previous
work in DV recidivism has employed different classification techniques,
including decision tree (DT) induction and logistic regression, where the main
focus was on achieving high prediction accuracy. As a result, even the diagrams
of trained DTs were often too difficult to interpret due to their size and
complexity, making decision-making challenging. Given there is often a
trade-off between model accuracy and interpretability, in this work our aim is
to employ DT induction to obtain both interpretable trees as well as high
prediction accuracy. Specifically, we implement and evaluate different
approaches to deal with class imbalance as well as feature selection. Compared
to previous work in DV recidivism prediction that employed logistic regression,
our approach can achieve comparable area under the ROC curve results by using
only 3 of 11 available features and generating understandable decision trees
that contain only 4 leaf nodes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijenayake_S/0/1/0/all/0/1&quot;&gt;Senuri Wijenayake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graham_T/0/1/0/all/0/1&quot;&gt;Timothy Graham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christen_P/0/1/0/all/0/1&quot;&gt;Peter Christen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09868">
<title>Bypassing Feature Squeezing by Increasing Adversary Strength. (arXiv:1803.09868v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.09868</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature Squeezing is a recently proposed defense method which reduces the
search space available to an adversary by coalescing samples that correspond to
many different feature vectors in the original space into a single sample. It
has been shown that feature squeezing defenses can be combined in a joint
detection framework to achieve high detection rates against state-of-the-art
attacks. However, we demonstrate on the MNIST and CIFAR-10 datasets that by
increasing the adversary strength of said state-of-the-art attacks, one can
bypass the detection framework with adversarial examples of minimal visual
distortion. These results suggest for proposed defenses to validate against
stronger attack configurations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sharma_Y/0/1/0/all/0/1&quot;&gt;Yash Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09946">
<title>Complex-Valued Restricted Boltzmann Machine for Direct Speech Parameterization from Complex Spectra. (arXiv:1803.09946v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1803.09946</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes a novel energy-based probabilistic distribution that
represents complex-valued data and explains how to apply it to direct feature
extraction from complex-valued spectra. The proposed model, the complex-valued
restricted Boltzmann machine (CRBM), is designed to deal with complex-valued
visible units as an extension of the well-known restricted Boltzmann machine
(RBM). Like the RBM, the CRBM learns the relationships between visible and
hidden units without having connections between units in the same layer, which
dramatically improves training efficiency by using Gibbs sampling or
contrastive divergence (CD). Another important characteristic is that the CRBM
also has connections between real and imaginary parts of each of the
complex-valued visible units that help represent the data distribution in the
complex domain. In speech signal processing, classification and generation
features are often based on amplitude spectra (e.g., MFCC, cepstra, and
mel-cepstra) even if they are calculated from complex spectra, and they ignore
phase information. In contrast, the proposed feature extractor using the CRBM
directly encodes the complex spectra (or another complex-valued representation
of the complex spectra) into binary-valued latent features (hidden units).
Since the visible-hidden connections are undirected, we can also recover
(decode) the complex spectra from the latent features directly. Our speech
coding experiments demonstrated that the CRBM outperformed other speech coding
methods, such as methods using the conventional RBM, the mel-log spectrum
approximate (MLSA) decoder, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nakashika_T/0/1/0/all/0/1&quot;&gt;Toru Nakashika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Takaki_S/0/1/0/all/0/1&quot;&gt;Shinji Takaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09984">
<title>Hiding in the Crowd: A Massively Distributed Algorithm for Private Averaging with Malicious Adversaries. (arXiv:1803.09984v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.09984</link>
<description rdf:parseType="Literal">&lt;p&gt;The amount of personal data collected in our everyday interactions with
connected devices offers great opportunities for innovative services fueled by
machine learning, as well as raises serious concerns for the privacy of
individuals. In this paper, we propose a massively distributed protocol for a
large set of users to privately compute averages over their joint data, which
can then be used to learn predictive models. Our protocol can find a solution
of arbitrary accuracy, does not rely on a third party and preserves the privacy
of users throughout the execution in both the honest-but-curious and malicious
adversary models. Specifically, we prove that the information observed by the
adversary (the set of maliciours users) does not significantly reduce the
uncertainty in its prediction of private values compared to its prior belief.
The level of privacy protection depends on a quantity related to the Laplacian
matrix of the network graph and generally improves with the size of the graph.
Furthermore, we design a verification procedure which offers protection against
malicious users joining the service with the goal of manipulating the outcome
of the algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dellenbach_P/0/1/0/all/0/1&quot;&gt;Pierre Dellenbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellet_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Bellet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramon_J/0/1/0/all/0/1&quot;&gt;Jan Ramon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10045">
<title>Kinetic Compressive Sensing. (arXiv:1803.10045v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/1803.10045</link>
<description rdf:parseType="Literal">&lt;p&gt;Parametric images provide insight into the spatial distribution of
physiological parameters, but they are often extremely noisy, due to low SNR of
tomographic data. Direct estimation from projections allows accurate noise
modeling, improving the results of post-reconstruction fitting. We propose a
method, which we name kinetic compressive sensing (KCS), based on a
hierarchical Bayesian model and on a novel reconstruction algorithm, that
encodes sparsity of kinetic parameters. Parametric maps are reconstructed by
maximizing the joint probability, with an Iterated Conditional Modes (ICM)
approach, alternating the optimization of activity time series (OS-MAP-OSL),
and kinetic parameters (MAP-LM). We evaluated the proposed algorithm on a
simulated dynamic phantom: a bias/variance study confirmed how direct estimates
can improve the quality of parametric maps over a post-reconstruction fitting,
and showed how the novel sparsity prior can further reduce their variance,
without affecting bias. Real FDG PET human brain data (Siemens mMR, 40min)
images were also processed. Results enforced how the proposed KCS-regularized
direct method can produce spatially coherent images and parametric maps, with
lower spatial noise and better tissue contrast. A GPU-based open source
implementation of the algorithm is provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Scipioni_M/0/1/0/all/0/1&quot;&gt;Michele Scipioni&lt;/a&gt; (1 and 3), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Santarelli_M/0/1/0/all/0/1&quot;&gt;Maria F. Santarelli&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Landini_L/0/1/0/all/0/1&quot;&gt;Luigi Landini&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Catana_C/0/1/0/all/0/1&quot;&gt;Ciprian Catana&lt;/a&gt; (3 and 4), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Greve_D/0/1/0/all/0/1&quot;&gt;Douglas N. Greve&lt;/a&gt; (3 and 4), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Price_J/0/1/0/all/0/1&quot;&gt;Julie C. Price&lt;/a&gt; (3 and 4), &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pedemonte_S/0/1/0/all/0/1&quot;&gt;Stefano Pedemonte&lt;/a&gt; (3, 4 and 5) ((1) DII, University of Pisa, (2) IFC-CNR, Pisa, (3) Martinos Center for Biomedical Imaging, Boston, (4) Harvard Medical School, Boston, (5) MGH Center for Clinical Data Science, Boston)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10049">
<title>Fast Parametric Learning with Activation Memorization. (arXiv:1803.10049v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.10049</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks trained with backpropagation often struggle to identify
classes that have been observed a small number of times. In applications where
most class labels are rare, such as language modelling, this can become a
performance bottleneck. One potential remedy is to augment the network with a
fast-learning non-parametric model which stores recent activations and class
labels into an external memory. We explore a simplified architecture where we
treat a subset of the model parameters as fast memory stores. This can help
retain information over longer time intervals than a traditional memory, and
does not require additional space or compute. In the case of image
classification, we display faster binding of novel classes on an Omniglot image
curriculum task. We also show improved performance for word-based language
models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia
articles (WikiText-103) --- the latter achieving a state-of-the-art perplexity
of 29.2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rae_J/0/1/0/all/0/1&quot;&gt;Jack W Rae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1&quot;&gt;Chris Dyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayan_P/0/1/0/all/0/1&quot;&gt;Peter Dayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lillicrap_T/0/1/0/all/0/1&quot;&gt;Timothy P Lillicrap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10172">
<title>Distributed Adaptive Sampling for Kernel Matrix Approximation. (arXiv:1803.10172v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.10172</link>
<description rdf:parseType="Literal">&lt;p&gt;Most kernel-based methods, such as kernel or Gaussian process regression,
kernel PCA, ICA, or $k$-means clustering, do not scale to large datasets,
because constructing and storing the kernel matrix $\mathbf{K}_n$ requires at
least $\mathcal{O}(n^2)$ time and space for $n$ samples. Recent works show that
sampling points with replacement according to their ridge leverage scores (RLS)
generates small dictionaries of relevant points with strong spectral
approximation guarantees for $\mathbf{K}_n$. The drawback of RLS-based methods
is that computing exact RLS requires constructing and storing the whole kernel
matrix. In this paper, we introduce SQUEAK, a new algorithm for kernel
approximation based on RLS sampling that sequentially processes the dataset,
storing a dictionary which creates accurate kernel matrix approximations with a
number of points that only depends on the effective dimension $d_{eff}(\gamma)$
of the dataset. Moreover since all the RLS estimations are efficiently
performed using only the small dictionary, SQUEAK is the first RLS sampling
algorithm that never constructs the whole matrix $\mathbf{K}_n$, runs in linear
time $\widetilde{\mathcal{O}}(nd_{eff}(\gamma)^3)$ w.r.t. $n$, and requires
only a single pass over the dataset. We also propose a parallel and distributed
version of SQUEAK that linearly scales across multiple machines, achieving
similar accuracy in as little as
$\widetilde{\mathcal{O}}(\log(n)d_{eff}(\gamma)^3)$ time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Calandriello_D/0/1/0/all/0/1&quot;&gt;Daniele Calandriello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lazaric_A/0/1/0/all/0/1&quot;&gt;Alessandro Lazaric&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Valko_M/0/1/0/all/0/1&quot;&gt;Michal Valko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1310.1562">
<title>Dependence Measure for non-additive model. (arXiv:1310.1562v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1310.1562</link>
<description rdf:parseType="Literal">&lt;p&gt;We proposed a new statistical dependency measure called Copula Dependency
Coefficient(CDC) for two sets of variables based on copula. It is robust to
outliers, easy to implement, powerful and appropriate to high-dimensional
variables. These properties are important in many applications. Experimental
results show that CDC can detect the dependence between variables in both
additive and non-additive models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hangjin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yiming Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1506.01110">
<title>Multi-View Factorization Machines. (arXiv:1506.01110v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1506.01110</link>
<description rdf:parseType="Literal">&lt;p&gt;For a learning task, data can usually be collected from different sources or
be represented from multiple views. For example, laboratory results from
different medical examinations are available for disease diagnosis, and each of
them can only reflect the health state of a person from a particular
aspect/view. Therefore, different views provide complementary information for
learning tasks. An effective integration of the multi-view information is
expected to facilitate the learning performance. In this paper, we propose a
general predictor, named multi-view machines (MVMs), that can effectively
include all the possible interactions between features from multiple views. A
joint factorization is embedded for the full-order interaction parameters which
allows parameter estimation under sparsity. Moreover, MVMs can work in
conjunction with different loss functions for a variety of machine learning
tasks. A stochastic gradient descent method is presented to learn the MVM
model. We further illustrate the advantages of MVMs through comparison with
other methods for multi-view classification, including support vector machines
(SVMs), support tensor machines (STMs) and factorization machines (FMs).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1&quot;&gt;Bokai Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hucheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1603.08232">
<title>The block-Poisson estimator for exact subsampling MCMC. (arXiv:1603.08232v4 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1603.08232</link>
<description rdf:parseType="Literal">&lt;p&gt;Speeding up Markov Chain Monte Carlo (MCMC) for datasets with many
observations by data subsampling has recently received considerable attention
in the literature. The currently available methods are either approximate,
highly inefficient or limited to small dimensional models. We propose a
pseudo-marginal MCMC method that estimates the likelihood by data subsampling
using a block-Poisson estimator. The estimator is a product of Poisson
estimators, each based on an independent subset of the observations. The
construction allows us to update a subset of the blocks in each MCMC iteration,
thereby inducing a controllable correlation between the estimates at the
current and proposed draw in the Metropolis-Hastings ratio. This makes it
possible to use highly variable likelihood estimators without adversely
affecting the sampling efficiency. Poisson estimators are unbiased but not
necessarily positive. We therefore follow Lyne et al. (2015) and run the MCMC
on the absolute value of the estimator and use an importance sampling
correction for occasionally negative likelihood estimates to estimate
expectations of any function of the parameters. We provide analytically derived
guidelines to select the algorithm&apos;s optimal tuning parameters by minimizing
the variance of the importance sampling corrected estimator per unit of
computing time. The guidelines are derived under idealized conditions, but are
demonstrated to be quite accurate in empirical experiments. The guidelines
apply to any pseudo-marginal algorithm if the likelihood is estimated by the
block-Poisson estimator, including the class of doubly intractable problems in
Lyne et al. (2015). We illustrate the method in a logistic regression example
and find dramatic improvements compared to regular MCMC without subsampling and
a popular exact subsampling approach recently proposed in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Quiroz_M/0/1/0/all/0/1&quot;&gt;Matias Quiroz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Minh-Ngoc Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Villani_M/0/1/0/all/0/1&quot;&gt;Mattias Villani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohn_R/0/1/0/all/0/1&quot;&gt;Robert Kohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dang_K/0/1/0/all/0/1&quot;&gt;Khue-Dung Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.03948">
<title>Identification of release sources in advection-diffusion system by machine learning combined with Green function inverse method. (arXiv:1612.03948v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1612.03948</link>
<description rdf:parseType="Literal">&lt;p&gt;The identification of sources of advection-diffusion transport is based
usually on solving complex ill-posed inverse models against the available
state- variable data records. However, if there are several sources with
different locations and strengths, the data records represent mixtures rather
than the separate influences of the original sources. Importantly, the number
of these original release sources is typically unknown, which hinders
reliability of the classical inverse-model analyses. To address this challenge,
we present here a novel hybrid method for identification of the unknown number
of release sources. Our hybrid method, called HNMF, couples unsupervised
learning based on Nonnegative Matrix Factorization (NMF) and inverse-analysis
Green functions method. HNMF synergistically performs decomposition of the
recorded mixtures, finds the number of the unknown sources and uses the Green
function of advection-diffusion equation to identify their characteristics. In
the paper, we introduce the method and demonstrate that it is capable of
identifying the advection velocity and dispersivity of the medium as well as
the unknown number, locations, and properties of various sets of synthetic
release sources with different space and time dependencies, based only on the
recorded data. HNMF can be applied directly to any problem controlled by a
partial-differential parabolic equation where mixtures of an unknown number of
sources are measured at multiple locations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanev_V/0/1/0/all/0/1&quot;&gt;Valentin G. Stanev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iliev_F/0/1/0/all/0/1&quot;&gt;Filip L. Iliev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansen_S/0/1/0/all/0/1&quot;&gt;Scott Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vesselinov_V/0/1/0/all/0/1&quot;&gt;Velimir V. Vesselinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexandrov_B/0/1/0/all/0/1&quot;&gt;Boian S. Alexandrov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.03950">
<title>Nonnegative Matrix Factorization for identification of unknown number of sources emitting delayed signals. (arXiv:1612.03950v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1612.03950</link>
<description rdf:parseType="Literal">&lt;p&gt;Factor analysis is broadly used as a powerful unsupervised machine learning
tool for reconstruction of hidden features in recorded mixtures of signals. In
the case of a linear approximation, the mixtures can be decomposed by a variety
of model-free Blind Source Separation (BSS) algorithms. Most of the available
BSS algorithms consider an instantaneous mixing of signals, while the case when
the mixtures are linear combinations of signals with delays is less explored.
Especially difficult is the case when the number of sources of the signals with
delays is unknown and has to be determined from the data as well. To address
this problem, in this paper, we present a new method based on Nonnegative
Matrix Factorization (NMF) that is capable of identifying: (a) the unknown
number of the sources, (b) the delays and speed of propagation of the signals,
and (c) the locations of the sources. Our method can be used to decompose
records of mixtures of signals with delays emitted by an unknown number of
sources in a nondispersive medium, based only on recorded data. This is the
case, for example, when electromagnetic signals from multiple antennas are
received asynchronously; or mixtures of acoustic or seismic signals recorded by
sensors located at different positions; or when a shift in frequency is induced
by the Doppler effect. By applying our method to synthetic datasets, we
demonstrate its ability to identify the unknown number of sources as well as
the waveforms, the delays, and the strengths of the signals. Using Bayesian
analysis, we also evaluate estimation uncertainties and identify the region of
likelihood where the positions of the sources can be found.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iliev_F/0/1/0/all/0/1&quot;&gt;Filip L. Iliev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanev_V/0/1/0/all/0/1&quot;&gt;Valentin G. Stanev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vesselinov_V/0/1/0/all/0/1&quot;&gt;Velimir V. Vesselinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexandrov_B/0/1/0/all/0/1&quot;&gt;Boian S. Alexandrov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.03536">
<title>SILVar: Single Index Latent Variable Models. (arXiv:1705.03536v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.03536</link>
<description rdf:parseType="Literal">&lt;p&gt;A semi-parametric, non-linear regression model in the presence of latent
variables is introduced. These latent variables can correspond to unmodeled
phenomena or unmeasured agents in a complex networked system. This new
formulation allows joint estimation of certain non-linearities in the system,
the direct interactions between measured variables, and the effects of
unmodeled elements on the observed system. The particular form of the model
adopted is justified, and learning is posed as a regularized empirical risk
minimization. This leads to classes of structured convex optimization problems
with a &quot;sparse plus low-rank&quot; flavor. Relations between the proposed model and
several common model paradigms, such as those of Robust Principal Component
Analysis (PCA) and Vector Autoregression (VAR), are established. Particularly
in the VAR setting, the low-rank contributions can come from broad trends
exhibited in the time series. Details of the algorithm for learning the model
are presented. Experiments demonstrate the performance of the model and the
estimation algorithm on simulated and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jonathan Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moura_J/0/1/0/all/0/1&quot;&gt;Jose&amp;#x27; M.F. Moura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.02542">
<title>A Deformable Interface for Human Touch Recognition using Stretchable Carbon Nanotube Dielectric Elastomer Sensors and Deep Neural Networks. (arXiv:1706.02542v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/1706.02542</link>
<description rdf:parseType="Literal">&lt;p&gt;User interfaces provide an interactive window between physical and virtual
environments. A new concept in the field of human-computer interaction is a
soft user interface; a compliant surface that facilitates touch interaction
through deformation. Despite the potential of these interfaces, they currently
lack a signal processing framework that can efficiently extract information
from their deformation. Here we present OrbTouch, a device that uses
statistical learning algorithms, based on convolutional neural networks, to map
deformations from human touch to categorical labels (i.e., gestures) and touch
location using stretchable capacitor signals as inputs. We demonstrate this
approach by using the device to control the popular game Tetris. OrbTouch
provides a modular, robust framework to interpret deformation in soft media,
laying a foundation for new modes of human computer interaction through shape
changing solids.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larson_C/0/1/0/all/0/1&quot;&gt;Chris Larson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spjut_J/0/1/0/all/0/1&quot;&gt;Josef Spjut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knepper_R/0/1/0/all/0/1&quot;&gt;Ross Knepper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shepherd_R/0/1/0/all/0/1&quot;&gt;Robert Shepherd&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.03530">
<title>A Cluster Elastic Net for Multivariate Regression. (arXiv:1707.03530v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.03530</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method for estimating coefficients in multivariate regression
when there is a clustering structure to the response variables. The proposed
method includes a fusion penalty, to shrink the difference in fitted values
from responses in the same cluster, and an L1 penalty for simultaneous variable
selection and estimation. The method can be used when the grouping structure of
the response variables is known or unknown. When the clustering structure is
unknown the method will simultaneously estimate the clusters of the response
and the regression coefficients. Theoretical results are presented for the
penalized least squares case, including asymptotic results allowing for p &amp;gt;&amp;gt; n.
We extend our method to the setting where the responses are binomial variables.
We propose a coordinate descent algorithm for both the normal and binomial
likelihood, which can easily be extended to other generalized linear model
(GLM) settings. Simulations and data examples from business operations and
genomics are presented to show the merits of both the least squares and
binomial methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Price_B/0/1/0/all/0/1&quot;&gt;Bradley S. Price&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sherwood_B/0/1/0/all/0/1&quot;&gt;Ben Sherwood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06034">
<title>Stochastic Variance Reduction for Policy Gradient Estimation. (arXiv:1710.06034v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06034</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in policy gradient methods and deep learning have
demonstrated their applicability for complex reinforcement learning problems.
However, the variance of the performance gradient estimates obtained from the
simulation is often excessive, leading to poor sample efficiency. In this
paper, we apply the stochastic variance reduced gradient descent (SVRG) to
model-free policy gradient to significantly improve the sample-efficiency. The
SVRG estimation is incorporated into a trust-region Newton conjugate gradient
framework for the policy optimization. On several Mujoco tasks, our method
achieves significantly better performance compared to the state-of-the-art
model-free policy gradient methods in robotic continuous control such as trust
region policy optimization (TRPO)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianbing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jian Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10210">
<title>On denoising modulo 1 samples of a function. (arXiv:1710.10210v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10210</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider an unknown smooth function $f: [0,1] \rightarrow \mathbb{R}$, and
say we are given $n$ noisy mod 1 samples of $f$, i.e., $y_i = (f(x_i) +
\eta_i)\mod 1$ for $x_i \in [0,1]$, where $\eta_i$ denotes noise. Given the
samples $(x_i,y_i)_{i=1}^{n}$, our goal is to recover smooth, robust estimates
of the clean samples $f(x_i) \bmod 1$. We formulate a natural approach for
solving this problem which works with angular embeddings of the noisy mod 1
samples over the unit complex circle, inspired by the angular synchronization
framework. Our approach amounts to solving a quadratically constrained
quadratic program (QCQP) which is NP-hard in its basic form, and therefore we
consider its relaxation which is a trust region sub-problem and hence solvable
efficiently. We demonstrate its robustness to noise via extensive numerical
simulations on several synthetic examples, along with a detailed theoretical
analysis. To the best of our knowledge, we provide the first algorithm for
denoising mod 1 samples of a smooth function, which comes with robustness
guarantees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cucuringu_M/0/1/0/all/0/1&quot;&gt;Mihai Cucuringu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tyagi_H/0/1/0/all/0/1&quot;&gt;Hemant Tyagi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10733">
<title>Attacking the Madry Defense Model with $L_1$-based Adversarial Examples. (arXiv:1710.10733v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10733</link>
<description rdf:parseType="Literal">&lt;p&gt;The Madry Lab recently hosted a competition designed to test the robustness
of their adversarially trained MNIST model. Attacks were constrained to perturb
each pixel of the input image by a scaled maximal $L_\infty$ distortion
$\epsilon$ = 0.3. This discourages the use of attacks which are not optimized
on the $L_\infty$ distortion metric. Our experimental results demonstrate that
by relaxing the $L_\infty$ constraint of the competition, the elastic-net
attack to deep neural networks (EAD) can generate transferable adversarial
examples which, despite their high average $L_\infty$ distortion, have minimal
visual distortion. These results call into question the use of $L_\infty$ as a
sole measure for visual distortion, and further demonstrate the power of EAD at
generating robust adversarial examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sharma_Y/0/1/0/all/0/1&quot;&gt;Yash Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06373">
<title>Thoracic Disease Identification and Localization with Limited Supervision. (arXiv:1711.06373v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06373</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate identification and localization of abnormalities from radiology
images play an integral part in clinical diagnosis and treatment planning.
Building a highly accurate prediction model for these tasks usually requires a
large number of images manually annotated with labels and finding sites of
abnormalities. In reality, however, such annotated data are expensive to
acquire, especially the ones with location annotations. We need methods that
can work well with only a small amount of location annotations. To address this
challenge, we present a unified approach that simultaneously performs disease
identification and localization through the same underlying model for all
images. We demonstrate that our approach can effectively leverage both class
information as well as limited location annotation, and significantly
outperforms the comparative reference baseline in both classification and
localization tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1&quot;&gt;Mei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yuan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li-Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00559">
<title>Progressive Neural Architecture Search. (arXiv:1712.00559v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00559</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new method for learning the structure of convolutional neural
networks (CNNs) that is more efficient than recent state-of-the-art methods
based on reinforcement learning and evolutionary algorithms. Our approach uses
a sequential model-based optimization (SMBO) strategy, in which we search for
structures in order of increasing complexity, while simultaneously learning a
surrogate model to guide the search through structure space. Direct comparison
under the same search space shows that our method is up to 5 times more
efficient than the RL method of Zoph et al. (2018) in terms of number of models
evaluated, and 8 times faster in terms of total compute. The structures we
discover in this way achieve state of the art classification accuracies on
CIFAR-10 and ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenxi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1&quot;&gt;Barret Zoph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_M/0/1/0/all/0/1&quot;&gt;Maxim Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1&quot;&gt;Jonathon Shlens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wei Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li-Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jonathan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1&quot;&gt;Kevin Murphy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03553">
<title>Counterfactual time-series prediction with encoder-decoder networks. (arXiv:1712.03553v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.03553</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes an alternative to the synthetic control method (SCM) for
estimating the effect of a policy intervention on an outcome over time.
Encoder-decoder recurrent neural networks (RNNs) are used to predict
counterfactual time-series of treated unit outcomes using only the outcomes of
control units as inputs. Unlike SCM, the proposed method does not rely on
pre-intervention covariates, allows for nonconvex combinations of control
units, and can handle multiple treated units. In empirical and simulated data
applications, RNN-based models outperform SCM in terms of predictive accuracy
while using much less information to produce counterfactual predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poulos_J/0/1/0/all/0/1&quot;&gt;Jason Poulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06061">
<title>Nearly Optimal Robust Subspace Tracking. (arXiv:1712.06061v3 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06061</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we study the robust subspace tracking (RST) problem and obtain
one of the first two provable guarantees for it. The goal of RST is to track
sequentially arriving data vectors that lie in a slowly changing
low-dimensional subspace, while being robust to corruption by additive sparse
outliers. It can also be interpreted as a dynamic (time-varying) extension of
robust PCA (RPCA), with the minor difference that RST also requires a short
tracking delay. We develop a recursive projected compressive sensing algorithm
that we call Nearly Optimal RST via ReProCS (ReProCS-NORST) because its
tracking delay is nearly optimal. We prove that NORST solves both the RST and
the dynamic RPCA problems under weakened standard RPCA assumptions, two simple
extra assumptions (slow subspace change and most outlier magnitudes lower
bounded), and a few minor assumptions.
&lt;/p&gt;
&lt;p&gt;Our guarantee shows that NORST enjoys a near optimal tracking delay of $O(r
\log n \log(1/\epsilon))$. Its required delay between subspace change times is
the same, and its memory complexity is $n$ times this value. Thus both these
are also nearly optimal. Here $n$ is the ambient space dimension, $r$ is the
subspaces&apos; dimension, and $\epsilon$ is the tracking accuracy. NORST also has
the best outlier tolerance compared with all previous RPCA or RST methods, both
theoretically and empirically (including for real videos), without requiring
any model on how the outlier support is generated. This is possible because of
the extra assumptions it uses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanamurthy_P/0/1/0/all/0/1&quot;&gt;Praneeth Narayanamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaswani_N/0/1/0/all/0/1&quot;&gt;Namrata Vaswani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08708">
<title>Variational Autoencoders for Learning Latent Representations of Speech Emotion: A Preliminary Study. (arXiv:1712.08708v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08708</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning the latent representation of data in unsupervised fashion is a very
interesting process that provides relevant features for enhancing the
performance of a classifier. For speech emotion recognition tasks, generating
effective features is crucial. Currently, handcrafted features are mostly used
for speech emotion recognition, however, features learned automatically using
deep learning have shown strong success in many problems, especially in image
processing. In particular, deep generative models such as Variational
Autoencoders (VAEs) have gained enormous success for generating features for
natural images. Inspired by this, we propose VAEs for deriving the latent
representation of speech signals and use this representation to classify
emotions. To the best of our knowledge, we are the first to propose VAEs for
speech emotion classification. Evaluations on the IEMOCAP dataset demonstrate
that features learned by VAEs can produce state-of-the-art results for speech
emotion classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latif_S/0/1/0/all/0/1&quot;&gt;Siddique Latif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rana_R/0/1/0/all/0/1&quot;&gt;Rajib Rana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qadir_J/0/1/0/all/0/1&quot;&gt;Junaid Qadir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epps_J/0/1/0/all/0/1&quot;&gt;Julien Epps&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02950">
<title>Adversarial Deep Learning for Robust Detection of Binary Encoded Malware. (arXiv:1801.02950v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02950</link>
<description rdf:parseType="Literal">&lt;p&gt;Malware is constantly adapting in order to avoid detection. Model based
malware detectors, such as SVM and neural networks, are vulnerable to so-called
adversarial examples which are modest changes to detectable malware that allows
the resulting malware to evade detection. Continuous-valued methods that are
robust to adversarial examples of images have been developed using saddle-point
optimization formulations. We are inspired by them to develop similar methods
for the discrete, e.g. binary, domain which characterizes the features of
malware. A specific extra challenge of malware is that the adversarial examples
must be generated in a way that preserves their malicious functionality. We
introduce methods capable of generating functionally preserved adversarial
malware examples in the binary domain. Using the saddle-point formulation, we
incorporate the adversarial examples into the training of models that are
robust to them. We evaluate the effectiveness of the methods and others in the
literature on a set of Portable Execution~(PE) files. Comparison prompts our
introduction of an online measure computed during training to assess general
expectation of robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Dujaili_A/0/1/0/all/0/1&quot;&gt;Abdullah Al-Dujaili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1&quot;&gt;Alex Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemberg_E/0/1/0/all/0/1&quot;&gt;Erik Hemberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OReilly_U/0/1/0/all/0/1&quot;&gt;Una-May O&amp;#x27;Reilly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02550">
<title>Semi-Amortized Variational Autoencoders. (arXiv:1802.02550v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02550</link>
<description rdf:parseType="Literal">&lt;p&gt;Amortized variational inference (AVI) replaces instance-specific local
inference with a global inference network. While AVI has enabled efficient
training of deep generative models such as variational autoencoders (VAE),
recent empirical work suggests that inference networks can produce suboptimal
variational parameters. We propose a hybrid approach, to use AVI to initialize
the variational parameters and run stochastic variational inference (SVI) to
refine them. Crucially, the local SVI procedure is itself differentiable, so
the inference network and generative model can be trained end-to-end with
gradient-based optimization. This semi-amortized approach enables the use of
rich generative models without experiencing the posterior-collapse phenomenon
common in training VAEs for problems like text generation. Experiments show
this approach outperforms strong autoregressive and variational baselines on
standard text and image datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wiseman_S/0/1/0/all/0/1&quot;&gt;Sam Wiseman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Miller_A/0/1/0/all/0/1&quot;&gt;Andrew C. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sontag_D/0/1/0/all/0/1&quot;&gt;David Sontag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rush_A/0/1/0/all/0/1&quot;&gt;Alexander M. Rush&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03839">
<title>Band Target Entropy Minimization and Target Partial Least Squares for Spectral Recovery and Calibration. (arXiv:1802.03839v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03839</link>
<description rdf:parseType="Literal">&lt;p&gt;The resolution and calibration of pure spectra of minority components in
measurements of chemical mixtures without prior knowledge of the mixture is a
challenging problem. In this work, a combination of band target entropy
minimization (BTEM) and target partial least squares (T-PLS) was used to obtain
estimates for single pure component spectra and to calibrate those estimates in
a true, one-at-a-time fashion. This approach allows for minor components to be
targeted and their relative amounts estimated in the presence of other varying
components in spectral data. The use of T-PLS estimation is an improvement to
the BTEM method because it overcomes the need to identify all of the pure
components prior to estimation. Estimated amounts from this combination were
found to be similar to those obtained from a standard method, multivariate
curve resolution-alternating least squares (MCR-ALS), on a simple, three
component mixture dataset. Studies from two experimental datasets demonstrate
where the combination of BTEM and T-PLS could model the pure component spectra
and obtain concentration profiles of minor components but MCR-ALS could not.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kneale_C/0/1/0/all/0/1&quot;&gt;Casey Kneale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brown_S/0/1/0/all/0/1&quot;&gt;Steven D. Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04220">
<title>Augment and Reduce: Stochastic Inference for Large Categorical Distributions. (arXiv:1802.04220v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04220</link>
<description rdf:parseType="Literal">&lt;p&gt;Categorical distributions are ubiquitous in machine learning, e.g., in
classification, language models, and recommendation systems. They are also at
the core of discrete choice models. However, when the number of possible
outcomes is very large, using categorical distributions becomes computationally
expensive, as the complexity scales linearly with the number of outcomes. To
address this problem, we propose augment and reduce (A&amp;amp;R), a method to
alleviate the computational complexity. A&amp;amp;R uses two ideas: latent variable
augmentation and stochastic variational inference. It maximizes a lower bound
on the marginal likelihood of the data. Unlike existing methods which are
specific to softmax, A&amp;amp;R is more general and is amenable to other categorical
models, such as multinomial probit. On several large-scale classification
problems, we show that A&amp;amp;R provides a tighter bound on the marginal likelihood
and has better predictive performance than existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ruiz_F/0/1/0/all/0/1&quot;&gt;Francisco J. R. Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Titsias_M/0/1/0/all/0/1&quot;&gt;Michalis K. Titsias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dieng_A/0/1/0/all/0/1&quot;&gt;Adji B. Dieng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02312">
<title>Dimensionality Reduction for Stationary Time Series via Stochastic Nonconvex Optimization. (arXiv:1803.02312v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02312</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic optimization naturally arises in machine learning. Efficient
algorithms with provable guarantees, however, are still largely missing, when
the objective function is nonconvex and the data points are dependent. This
paper studies this fundamental challenge through a streaming PCA problem for
stationary time series data. Specifically, our goal is to estimate the
principle component of time series data with respect to the covariance matrix
of the stationary distribution. Computationally, we propose a variant of Oja&apos;s
algorithm combined with downsampling to control the bias of the stochastic
gradient caused by the data dependency. Theoretically, we quantify the
uncertainty of our proposed stochastic algorithm based on diffusion
approximations. This allows us to prove the global convergence in terms of the
continuous time limiting solution trajectory and further implies near optimal
sample complexity. Numerical experiments are provided to support our analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minshuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03607">
<title>On Generation of Adversarial Examples using Convex Programming. (arXiv:1803.03607v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03607</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been observed that deep learning architectures tend to make erroneous
decisions with high reliability for particularly designed adversarial
instances. In this work, we show that the perturbation analysis of these
architectures provides a framework for generating adversarial instances by
convex programming which, for classification tasks, is able to recover variants
of existing non-adaptive adversarial methods. The proposed framework can be
used for the design of adversarial noise under various desirable constraints
and different types of networks. Moreover, this framework is capable of
explaining various existing adversarial methods and can be used to derive new
algorithms as well. Furthermore, we make use of these results to obtain novel
algorithms. Experiments show the competitive performance of the obtained
solutions, in terms of fooling ratio, when benchmarked with well-known
adversarial methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balda_E/0/1/0/all/0/1&quot;&gt;Emilio Rafael Balda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behboodi_A/0/1/0/all/0/1&quot;&gt;Arash Behboodi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathar_R/0/1/0/all/0/1&quot;&gt;Rudolph Mathar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04926">
<title>Active Reinforcement Learning with Monte-Carlo Tree Search. (arXiv:1803.04926v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04926</link>
<description rdf:parseType="Literal">&lt;p&gt;Active Reinforcement Learning (ARL) is a twist on RL where the agent observes
reward information only if it pays a cost. This subtle change makes exploration
substantially more challenging. Powerful principles in RL like optimism,
Thompson sampling, and random exploration do not help with ARL. We relate ARL
in tabular environments to Bayes-Adaptive MDPs. We provide an ARL algorithm
using Monte-Carlo Tree Search that is asymptotically Bayes optimal.
Experimentally, this algorithm is near-optimal on small Bandit problems and
MDPs. On larger MDPs it outperforms a Q-learner augmented with specialised
heuristics for ARL. By analysing exploration behaviour in detail, we uncover
obstacles to scaling up simulation-based algorithms for ARL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulze_S/0/1/0/all/0/1&quot;&gt;Sebastian Schulze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1&quot;&gt;Owain Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07954">
<title>Resilient Monotone Sequential Maximization. (arXiv:1803.07954v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07954</link>
<description rdf:parseType="Literal">&lt;p&gt;Applications in machine learning, optimization, and control require the
sequential selection of a few system elements, such as sensors, data, or
actuators, to optimize the system performance across multiple time steps.
However, in failure-prone and adversarial environments, sensors get attacked,
data get deleted, and actuators fail. Thence, traditional sequential design
paradigms become insufficient and, in contrast, resilient sequential designs
that adapt against system-wide attacks, deletions, or failures become
important. In general, resilient sequential design problems are computationally
hard. Also, even though they often involve objective functions that are
monotone and (possibly) submodular, no scalable approximation algorithms are
known for their solution. In this paper, we provide the first scalable
algorithm, that achieves the following characteristics: system-wide resiliency,
i.e., the algorithm is valid for any number of denial-of-service attacks,
deletions, or failures; adaptiveness, i.e., at each time step, the algorithm
selects system elements based on the history of inflicted attacks, deletions,
or failures; and provable approximation performance, i.e., the algorithm
guarantees for monotone objective functions a solution close to the optimal. We
quantify the algorithm&apos;s approximation performance using a notion of curvature
for monotone (not necessarily submodular) set functions. Finally, we support
our theoretical analyses with simulated experiments, by considering a
control-aware sensor scheduling scenario, namely, sensing-constrained robot
navigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tzoumas_V/0/1/0/all/0/1&quot;&gt;Vasileios Tzoumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jadbabaie_A/0/1/0/all/0/1&quot;&gt;Ali Jadbabaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pappas_G/0/1/0/all/0/1&quot;&gt;George J. Pappas&lt;/a&gt;</dc:creator>
</item></rdf:RDF>