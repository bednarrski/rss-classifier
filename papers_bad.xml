<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-07T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02178"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02195"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02203"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02271"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02528"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10546"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02219"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02274"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02434"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02468"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02534"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02565"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.04389"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06778"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07440"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.01549"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02137"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02163"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02242"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02290"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02343"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02500"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02538"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02550"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02557"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02558"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.08142"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.07019"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.09634"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.07196"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.08120"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00636"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07222"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.02178">
<title>LightNN: Filling the Gap between Conventional Deep Neural Networks and Binarized Networks. (arXiv:1802.02178v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.02178</link>
<description rdf:parseType="Literal">&lt;p&gt;Application-specific integrated circuit (ASIC) implementations for Deep
Neural Networks (DNNs) have been adopted in many systems because of their
higher classification speed. However, although they may be characterized by
better accuracy, larger DNNs require significant energy and area, thereby
limiting their wide adoption. The energy consumption of DNNs is driven by both
memory accesses and computation. Binarized Neural Networks (BNNs), as a
trade-off between accuracy and energy consumption, can achieve great energy
reduction, and have good accuracy for large DNNs due to its regularization
effect. However, BNNs show poor accuracy when a smaller DNN configuration is
adopted. In this paper, we propose a new DNN model, LightNN, which replaces the
multiplications to one shift or a constrained number of shifts and adds. For a
fixed DNN configuration, LightNNs have better accuracy at a slight energy
increase than BNNs, yet are more energy efficient with only slightly less
accuracy than conventional DNNs. Therefore, LightNNs provide more options for
hardware designers to make trade-offs between accuracy and energy. Moreover,
for large DNN configurations, LightNNs have a regularization effect, making
them better in accuracy than conventional DNNs. These conclusions are verified
by experiment using the MNIST and CIFAR-10 datasets for different DNN
configurations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1&quot;&gt;Ruizhou Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zeye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1&quot;&gt;Rongye Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1&quot;&gt;Diana Marculescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanton_R/0/1/0/all/0/1&quot;&gt;R. D. Blanton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02195">
<title>Granger-causal Attentive Mixtures of Experts. (arXiv:1802.02195v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.02195</link>
<description rdf:parseType="Literal">&lt;p&gt;Several methods have recently been proposed to detect salient input features
for outputs of neural networks. Those methods offer a qualitative glimpse at
feature importance, but they fall short of providing quantifiable attributions
that can be compared across decisions and measures of the expected quality of
their explanations. To address these shortcomings, we present an attentive
mixture of experts (AME) that couples attentive gating with a Granger-causal
objective to jointly produce accurate predictions as well as measures of
feature importance. We demonstrate the utility of AMEs by determining factors
driving demand for medical prescriptions, comparing predictive features for
Parkinson&apos;s disease and pinpointing discriminatory genes across cancer types.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwab_P/0/1/0/all/0/1&quot;&gt;Patrick Schwab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlen_W/0/1/0/all/0/1&quot;&gt;Walter Karlen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02203">
<title>Automatic construction of Chinese herbal prescription from tongue image via convolution networks and auxiliary latent therapy topics. (arXiv:1802.02203v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.02203</link>
<description rdf:parseType="Literal">&lt;p&gt;The tongue image is an important physical information of human, it is of
great importance to the diagnosis and treatment in clinical medicine. Herbal
prescriptions are simple, noninvasive and low side effects, and are widely
applied in China. Researches on automatic construction technology of herbal
prescription based on tongue image have great significance for deep learning to
explore the relevance from tongue image to herbal prescription, and can be
applied to healthcare services in mobile medical system. In order to adapt to
the tongue image in a variety of photographing environments and construct the
herbal prescriptions, a neural network framework for prescriptions construction
is designed, which includes single / double convolution channels and fully
connected layers, and propose the mechanism of auxiliary therapy topic loss to
model the therapy of Chinese doctors then alleviate the interference of sparse
output labels to the diversity of results. The experimental data include the
patient tongue images and their corresponding prescriptions from real world
outpatient clinic, and the experiment results can generate the prescriptions
that are close to the real samples, which verifies the feasibility of the
proposed method for automatic construction of herbal prescription from tongue
image. Also, provides a reference for automatic herbal prescription
construction from more physical information (or integrated body information).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_G/0/1/0/all/0/1&quot;&gt;Guihua Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Huiqiang Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02271">
<title>Universal Deep Neural Network Compression. (arXiv:1802.02271v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.02271</link>
<description rdf:parseType="Literal">&lt;p&gt;Compression of deep neural networks (DNNs) for memory- and
computation-efficient compact feature representations becomes a critical
problem particularly for deployment of DNNs on resource-limited platforms. In
this paper, we investigate lossy compression of DNNs by weight quantization and
lossless source coding for memory-efficient inference. Whereas the previous
work addressed non-universal scalar quantization and entropy coding of DNN
weights, we for the first time introduce universal DNN compression by universal
vector quantization and universal source coding. In particular, we examine
universal randomized lattice quantization of DNNs, which randomizes DNN weights
by uniform random dithering before lattice quantization and can perform
near-optimally on any source without relying on knowledge of its probability
distribution. Entropy coding schemes such as Huffman codes require prior
calculation of source statistics, which is computationally consuming. Instead,
we propose universal lossless source coding schemes such as variants of
Lempel-Ziv-Welch or the Burrows-Wheeler transform. Finally, we present the
methods of fine-tuning vector quantized DNNs to recover the performance loss
after quantization. Our experimental results show that the proposed universal
DNN compression scheme achieves compression ratios of 124.80, 47.10 and 42.46
for LeNet5, 32-layer ResNet and AlexNet, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yoojin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Khamy_M/0/1/0/all/0/1&quot;&gt;Mostafa El-Khamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungwon Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02342">
<title>Energy-Efficient CMOS Memristive Synapses for Mixed-Signal Neuromorphic System-on-a-Chip. (arXiv:1802.02342v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.02342</link>
<description rdf:parseType="Literal">&lt;p&gt;Emerging non-volatile memory (NVM), or memristive, devices promise
energy-efficient realization of deep learning, when efficiently integrated with
mixed-signal integrated circuits on a CMOS substrate. Even though several
algorithmic challenges need to be addressed to turn the vision of memristive
Neuromorphic Systems-on-a-Chip (NeuSoCs) into reality, issues at the device and
circuit interface need immediate attention from the community. In this work, we
perform energy-estimation of a NeuSoC system and predict the desirable circuit
and device parameters for energy-efficiency optimization. Also, CMOS synapse
circuits based on the concept of CMOS memristor emulator are presented as a
system prototyping methodology, while practical memristor devices are being
developed and integrated with general-purpose CMOS. The proposed mixed-signal
memristive synapse can be designed and fabricated using standard CMOS
technologies and open doors to interesting applications in cognitive computing
circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_V/0/1/0/all/0/1&quot;&gt;Vishal Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kehan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02528">
<title>Classification of Things in DBpedia using Deep Neural Networks. (arXiv:1802.02528v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.02528</link>
<description rdf:parseType="Literal">&lt;p&gt;The Semantic Web aims at representing knowledge about the real world at web
scale - things, their attributes and relationships among them can be
represented as nodes and edges in an inter-linked semantic graph. In the
presence of noisy data, as is typical of data on the Semantic Web, a software
Agent needs to be able to robustly infer one or more associated actionable
classes for the individuals in order to act automatically on it. We model this
problem as a multi-label classification task where we want to robustly identify
types of the individuals in a semantic graph such as DBpedia, which we use as
an exemplary dataset on the Semantic Web. Our approach first extracts multiple
features for the individuals using random walks and then performs multi-label
classification using fully-connected Neural Networks. Through systematic
exploration and experimentation, we identify the effect of hyper-parameters of
the feature extraction and the fully-connected Neural Network structure on the
classification performance. Our final results show that our method performs
better than state-of-the-art inferencing systems like SDtype and SLCN, from
which we can conclude that random-walk-based feature extraction of individuals
and their multi-label classification using Deep Neural Networks is a promising
alternative to these systems for type classification of individuals on the
Semantic Web. The main contribution of our work is to introduce a novel
approach that allows us to use Deep Neural Networks to identify types of
individuals in a noisy semantic graph by extracting features using random walks
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parundekar_R/0/1/0/all/0/1&quot;&gt;Rahul Parundekar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10546">
<title>Multi-Layer Competitive-Cooperative Framework for Performance Enhancement of Differential Evolution. (arXiv:1801.10546v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1801.10546</link>
<description rdf:parseType="Literal">&lt;p&gt;Differential Evolution (DE) is one of the most powerful optimizers in the
evolutionary algorithm (EA) family. In recent years, many DE variants have been
proposed to enhance performance. However, when compared with each other,
significant differences in performances are seldomly observed. To meet this
challenge of a more significant improvement, this paper proposes a multi-layer
competitive-cooperative (MLCC) framework to combine the advantages of multiple
DEs. Existing multi-method strategies commonly use a multi-population based
structure, which classifies the entire population into several subpopulations
and evolve individuals only in their corresponding subgroups. MLCC proposes to
implement a parallel structure with the entire population simultaneously
monitored by multiple DEs assigned in multiple layers. Each individual can
store, utilize and update its evolution information in different layers by
using a novel individual preference based layer selecting (IPLS) mechanism and
a computational resource allocation bias (RAB) mechanism. In IPLS, individuals
only connect to one favorite layer. While in RAB, high quality solutions are
evolved by considering all the layers. In this way, the multiple layers work in
a competitive and cooperative manner. The proposed MLCC framework has been
implemented on several highly competitive DEs. Experimental studies show that
MLCC variants significantly outperform the baseline DEs as well as several
state-of-the-art and up-to-date DEs on the CEC benchmark functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sheng Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Li Ming Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Kit Sang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shao Yong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1&quot;&gt;Wing Shing Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02219">
<title>Scalable Meta-Learning for Bayesian Optimization. (arXiv:1802.02219v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.02219</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization has become a standard technique for hyperparameter
optimization, including data-intensive models such as deep neural networks that
may take days or weeks to train. We consider the setting where previous
optimization runs are available, and we wish to use their results to warm-start
a new optimization run. We develop an ensemble model that can incorporate the
results of past optimization runs, while avoiding the poor scaling that comes
with putting all results into a single Gaussian process model. The ensemble
combines models from past runs according to estimates of their generalization
performance on the current optimization. Results from a large collection of
hyperparameter optimization benchmark problems and from optimization of a
production computer vision platform at Facebook show that the ensemble can
substantially reduce the time it takes to obtain near-optimal configurations,
and is useful for warm-starting expensive searches or running quick
re-optimizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feurer_M/0/1/0/all/0/1&quot;&gt;Matthias Feurer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Letham_B/0/1/0/all/0/1&quot;&gt;Benjamin Letham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bakshy_E/0/1/0/all/0/1&quot;&gt;Eytan Bakshy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02274">
<title>A Critical Investigation of Deep Reinforcement Learning for Navigation. (arXiv:1802.02274v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1802.02274</link>
<description rdf:parseType="Literal">&lt;p&gt;The navigation problem is classically approached in two steps: an exploration
step, where map-information about the environment is gathered; and an
exploitation step, where this information is used to navigate efficiently. Deep
reinforcement learning (DRL) algorithms, alternatively, approach the problem of
navigation in an end-to-end fashion. Inspired by the classical approach, we ask
whether DRL algorithms are able to inherently explore, gather and exploit
map-information over the course of navigation. We build upon Mirowski et al.
[2017] work and introduce a systematic suite of experiments that vary three
parameters: the agent&apos;s starting location, the agent&apos;s target location, and the
maze structure. We choose evaluation metrics that explicitly measure the
algorithm&apos;s ability to gather and exploit map-information. Our experiments show
that when trained and tested on the same maps, the algorithm successfully
gathers and exploits map-information. However, when trained and tested on
different sets of maps, the algorithm fails to transfer the ability to gather
and exploit map-information to unseen maps. Furthermore, we find that when the
goal location is randomized and the map is kept static, the algorithm is able
to gather and exploit map-information but the exploitation is far from optimal.
We open-source our experimental suite in the hopes that it serves as a
framework for the comparison of future algorithms and leads to the discovery of
robust alternatives to classical navigation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhiman_V/0/1/0/all/0/1&quot;&gt;Vikas Dhiman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1&quot;&gt;Shurjo Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffin_B/0/1/0/all/0/1&quot;&gt;Brent Griffin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siskind_J/0/1/0/all/0/1&quot;&gt;Jeffrey M Siskind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1&quot;&gt;Jason J Corso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02434">
<title>Evolutionary Computation plus Dynamic Programming for the Bi-Objective Travelling Thief Problem. (arXiv:1802.02434v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.02434</link>
<description rdf:parseType="Literal">&lt;p&gt;This research proposes a novel indicator-based hybrid evolutionary approach
that combines approximate and exact algorithms. We apply it to a new
bi-criteria formulation of the travelling thief problem, which is known to the
Evolutionary Computation community as a benchmark multi-component optimisation
problem that interconnects two classical NP-hard problems: the travelling
salesman problem and the 0-1 knapsack problem. Our approach employs the exact
dynamic programming algorithm for the underlying Packing-While-Travelling (PWT)
problem as a subroutine within a bi-objective evolutionary algorithm. This
design takes advantage of the data extracted from Pareto fronts generated by
the dynamic program to achieve better solutions. Furthermore, we develop a
number of novel indicators and selection mechanisms to strengthen synergy of
the two algorithmic components of our approach. The results of computational
experiments show that the approach is capable to outperform the
state-of-the-art results for the single-objective case of the problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junhua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polyakovskiy_S/0/1/0/all/0/1&quot;&gt;Sergey Polyakovskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1&quot;&gt;Markus Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_F/0/1/0/all/0/1&quot;&gt;Frank Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02468">
<title>Efficient Learning of Bounded-Treewidth Bayesian Networks from Complete and Incomplete Data Sets. (arXiv:1802.02468v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.02468</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning a Bayesian networks with bounded treewidth is important for reducing
the complexity of the inferences. We present a novel anytime algorithm (k-MAX)
method for this task, which scales up to thousands of variables. Through
extensive experiments we show that it consistently yields higher-scoring
structures than its competitors on complete data sets. We then consider the
problem of structure learning from incomplete data sets. This can be addressed
by structural EM, which however is computationally very demanding. We thus
adopt the novel k-MAX algorithm in the maximization step of structural EM,
obtaining an efficient computation of the expected sufficient statistics. We
test the resulting structural EM method on the task of imputing missing data,
comparing it against the state-of-the-art approach based on random forests. Our
approach achieves the same imputation accuracy of the competitors, but in about
one tenth of the time. Furthermore we show that it has worst-case complexity
linear in the input size, and that it is easily parallelizable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scanagatta_M/0/1/0/all/0/1&quot;&gt;Mauro Scanagatta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corani_G/0/1/0/all/0/1&quot;&gt;Giorgio Corani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaffalon_M/0/1/0/all/0/1&quot;&gt;Marco Zaffalon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jaemin Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_U/0/1/0/all/0/1&quot;&gt;U Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02534">
<title>FixaTons: A collection of Human Fixations Datasets and Metrics for Scanpath Similarity. (arXiv:1802.02534v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.02534</link>
<description rdf:parseType="Literal">&lt;p&gt;In the last three decades, human visual attention has been a topic of great
interest in various disciplines. In computer vision, many models have been
proposed to predict the distribution of human fixations on a visual input.
Recently, thanks to the creation of large collections of data, machine learning
algorithms have obtained state-of-the-art performance on the task of saliency
map estimation. On the other hand, computational models of scanpath are much
less studied. Works are often only descriptive or task specific. Computational
models of scanpath with general purpose are present in the literature, but are
then evaluated in tasks of saliency prediction, losing therefore information
about the dynamics and the behaviour. This is due to the fact that the scanpath
is harder to model because it must include the description of a dynamic. In
addition to the difficulty of the problem itself, two technical reasons have
limited the research. The first reason is the lack of robust and uniformly used
set of metrics to compare the similarity between scanpath. The second reason is
the lack of sufficiently large and varied scanpath datasets. In this report we
want to help in both directions. We present FixaTons, a large collection of
human scanpaths (and saliency maps). It comes along with a software library for
easy data usage, statistics calculation and measures for scanpaths (and
saliency maps) similarity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1&quot;&gt;Dario Zanca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serchi_V/0/1/0/all/0/1&quot;&gt;Valeria Serchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piu_P/0/1/0/all/0/1&quot;&gt;Pietro Piu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosini_F/0/1/0/all/0/1&quot;&gt;Francesca Rosini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rufa_A/0/1/0/all/0/1&quot;&gt;Alessandra Rufa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02565">
<title>Applying Cooperative Machine Learning to Speed Up the Annotation of Social Signals in Large Multi-modal Corpora. (arXiv:1802.02565v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.02565</link>
<description rdf:parseType="Literal">&lt;p&gt;Scientific disciplines, such as Behavioural Psychology, Anthropology and
recently Social Signal Processing are concerned with the systematic exploration
of human behaviour. A typical work-flow includes the manual annotation (also
called coding) of social signals in multi-modal corpora of considerable size.
For the involved annotators this defines an exhausting and time-consuming task.
In the article at hand we present a novel method and also provide the tools to
speed up the coding procedure. To this end, we suggest and evaluate the use of
Cooperative Machine Learning (CML) techniques to reduce manual labelling
efforts by combining the power of computational capabilities and human
intelligence. The proposed CML strategy starts with a small number of labelled
instances and concentrates on predicting local parts first. Afterwards, a
session-independent classification model is created to finish the remaining
parts of the database. Confidence values are computed to guide the manual
inspection and correction of the predictions. To bring the proposed approach
into application we introduce NOVA - an open-source tool for collaborative and
machine-aided annotations. In particular, it gives labellers immediate access
to CML strategies and directly provides visual feedback on the results. Our
experiments show that the proposed method has the potential to significantly
reduce human labelling efforts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1&quot;&gt;Johannes Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baur_T/0/1/0/all/0/1&quot;&gt;Tobias Baur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valstar_M/0/1/0/all/0/1&quot;&gt;Michel F. Valstar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Schuller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1&quot;&gt;Elisabeth Andr&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.04389">
<title>Bayesian Optimization with Gradients. (arXiv:1703.04389v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.04389</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization has been successful at global optimization of
expensive-to-evaluate multimodal objective functions. However, unlike most
optimization methods, Bayesian optimization typically does not use derivative
information. In this paper we show how Bayesian optimization can exploit
derivative information to decrease the number of objective function evaluations
required for good performance. In particular, we develop a novel Bayesian
optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for
which we show one-step Bayes-optimality, asymptotic consistency, and greater
one-step value of information than is possible in the derivative-free setting.
Our procedure accommodates noisy and incomplete derivative information, comes
in both sequential and batch forms, and can optionally reduce the computational
cost of inference through automatically selected retention of a single
directional derivative. We also compute the d-KG acquisition function and its
gradient using a novel fast discretization-free technique. We show d-KG
provides state-of-the-art performance compared to a wide range of optimization
procedures with and without gradients, on benchmarks including logistic
regression, deep learning, kernel learning, and k-nearest neighbors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poloczek_M/0/1/0/all/0/1&quot;&gt;Matthias Poloczek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frazier_P/0/1/0/all/0/1&quot;&gt;Peter I. Frazier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06778">
<title>Learning Representations from Road Network for End-to-End Urban Growth Simulation. (arXiv:1712.06778v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06778</link>
<description rdf:parseType="Literal">&lt;p&gt;From our experiences in the past, we have seen that the growth of cities is
very much dependent on the transportation networks. In mega cities,
transportation networks determine to a significant extent as to where the
people will move and houses will be built. Hence, transportation network data
is crucial to an urban growth prediction system. Existing works have used
manually derived distance based features based on the road networks to build
models on urban growth. But due to the non-generic and laborious nature of the
manual feature engineering process, we can shift to End-to-End systems which do
not rely on manual feature engineering. In this paper, we propose a method to
integrate road network data to an existing Rule based End-to-End framework
without manual feature engineering. Our method employs recurrent neural
networks to represent road networks in a structured way such that it can be
plugged into the previously proposed End-to-End framework. The proposed
approach enhances the performance in terms of Figure of Merit, Producer&apos;s
accuracy, User&apos;s accuracy and Overall accuracy of the existing Rule based
End-to-End framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1&quot;&gt;Saptarshi Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Soumya K Ghosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07440">
<title>Curiosity-driven reinforcement learning with homeostatic regulation. (arXiv:1801.07440v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07440</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a curiosity reward based on information theory principles and
consistent with the animal instinct to maintain certain critical parameters
within a bounded range. Our experimental validation shows the added value of
the additional homeostatic drive to enhance the overall information gain of a
reinforcement learning agent interacting with a complex environment using
continuous actions. Our method builds upon two ideas: i) To take advantage of a
new Bellman-like equation of information gain and ii) to simplify the
computation of the local rewards by avoiding the approximation of complex
distributions over continuous states and actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abril_I/0/1/0/all/0/1&quot;&gt;Ildefons Magrans de Abril&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanai_R/0/1/0/all/0/1&quot;&gt;Ryota Kanai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.01549">
<title>Blind Pre-Processing: A Robust Defense Method Against Adversarial Examples. (arXiv:1802.01549v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.01549</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning algorithms and networks are vulnerable to perturbed inputs
which is known as the adversarial attack. Many defense methodologies have been
investigated to defend against such adversarial attack. In this work, we
propose a novel methodology to defend the existing powerful attack model. We
for the first time introduce a new attacking scheme for the attacker and set a
practical constraint for white box attack. Under this proposed attacking
scheme, we present the best defense ever reported against some of the recent
strong attacks. It consists of a set of nonlinear function to process the input
data which will make it more robust over the adversarial attack. However, we
make this processing layer completely hidden from the attacker. Blind
pre-processing improves the white box attack accuracy of MNIST from 94.3\% to
98.7\%. Even with increasing defense when others defenses completely fail,
blind pre-processing remains one of the strongest ever reported. Another
strength of our defense is that it eliminates the need for adversarial training
as it can significantly increase the MNIST accuracy without adversarial
training as well. Additionally, blind pre-processing can also increase the
inference accuracy in the face of a powerful attack on CIFAR-10 and SVHN data
set as well without much sacrificing clean data accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakin_A/0/1/0/all/0/1&quot;&gt;Adnan Siraj Rakin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhezhi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1&quot;&gt;Boqing Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Deliang Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02137">
<title>An Occluded Stacked Hourglass Approach to Facial Landmark Localization and Occlusion Estimation. (arXiv:1802.02137v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.02137</link>
<description rdf:parseType="Literal">&lt;p&gt;A key step to driver safety is to observe the driver&apos;s activities with the
face being a key step in this process to extracting information such as head
pose, blink rate, yawns, talking to passenger which can then help derive higher
level information such as distraction, drowsiness, intent, and where they are
looking. In the context of driving safety, it is important for the system
perform robust estimation under harsh lighting and occlusion but also be able
to detect when the occlusion occurs so that information predicted from occluded
parts of the face can be taken into account properly. This paper introduces the
Occluded Stacked Hourglass, based on the work of original Stacked Hourglass
network for body pose joint estimation, which is retrained to process a
detected face window and output 68 occlusion heat maps, each corresponding to a
facial landmark. Landmark location, occlusion levels and a refined face
detection score, to reject false positives, are extracted from these heat maps.
Using the facial landmark locations, features such as head pose and eye/mouth
openness can be extracted to derive driver attention and activity. The system
is evaluated for face detection, head pose, and occlusion estimation on various
datasets in the wild, both quantitatively and qualitatively, and shows
state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuen_K/0/1/0/all/0/1&quot;&gt;Kevan Yuen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1&quot;&gt;Mohan M. Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02163">
<title>How to Make Causal Inferences Using Texts. (arXiv:1802.02163v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.02163</link>
<description rdf:parseType="Literal">&lt;p&gt;New text as data techniques offer a great promise: the ability to inductively
discover measures that are useful for testing social science theories of
interest from large collections of text. We introduce a conceptual framework
for making causal inferences with discovered measures as a treatment or
outcome. Our framework enables researchers to discover high-dimensional textual
interventions and estimate the ways that observed treatments affect text-based
outcomes. We argue that nearly all text-based causal inferences depend upon a
latent representation of the text and we provide a framework to learn the
latent representation. But estimating this latent representation, we show,
creates new risks: we may introduce an identification problem or overfit. To
address these risks we describe a split-sample framework and apply it to
estimate causal effects from an experiment on immigration attitudes and a study
on bureaucratic response. Our work provides a rigorous foundation for
text-based causal inferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Egami_N/0/1/0/all/0/1&quot;&gt;Naoki Egami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fong_C/0/1/0/all/0/1&quot;&gt;Christian J. Fong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grimmer_J/0/1/0/all/0/1&quot;&gt;Justin Grimmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_M/0/1/0/all/0/1&quot;&gt;Margaret E. Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stewart_B/0/1/0/all/0/1&quot;&gt;Brandon M. Stewart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02242">
<title>Full-pulse Tomographic Reconstruction with Deep Neural Networks. (arXiv:1802.02242v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/1802.02242</link>
<description rdf:parseType="Literal">&lt;p&gt;Plasma tomography consists in reconstructing the 2D radiation profile in a
poloidal cross-section of a fusion device, based on line-integrated
measurements along several lines of sight. The reconstruction process is
computationally intensive and, in practice, only a few reconstructions are
usually computed per pulse. In this work, we trained a deep neural network
based on a large collection of sample tomograms that have been produced at JET
over several years. Once trained, the network is able to reproduce those
results with high accuracy. More importantly, it can compute all the
tomographic reconstructions for a given pulse in just a few seconds. This makes
it possible to visualize several phenomena -- such as plasma heating,
disruptions and impurity transport -- over the course of a discharge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ferreira_D/0/1/0/all/0/1&quot;&gt;Diogo R. Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Carvalho_P/0/1/0/all/0/1&quot;&gt;Pedro J. Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Fernandes_H/0/1/0/all/0/1&quot;&gt;Hor&amp;#xe1;cio Fernandes&lt;/a&gt; (JET Contributors)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02290">
<title>Spectral Image Visualization Using Generative Adversarial Networks. (arXiv:1802.02290v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.02290</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral images captured by satellites and radio-telescopes are analyzed to
obtain information about geological compositions distributions, distant asters
as well as undersea terrain. Spectral images usually contain tens to hundreds
of continuous narrow spectral bands and are widely used in various fields. But
the vast majority of those image signals are beyond the visible range, which
calls for special visualization technique. The visualizations of spectral
images shall convey as much information as possible from the original signal
and facilitate image interpretation. However, most of the existing visualizatio
methods display spectral images in false colors, which contradict with human&apos;s
experience and expectation. In this paper, we present a novel visualization
generative adversarial network (GAN) to display spectral images in natural
colors. To achieve our goal, we propose a loss function which consists of an
adversarial loss and a structure loss. The adversarial loss pushes our solution
to the natural image distribution using a discriminator network that is trained
to differentiate between false-color images and natural-color images. We also
use a cycle loss as the structure constraint to guarantee structure
consistency. Experimental results show that our method is able to generate
structure-preserved and natural-looking visualizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1&quot;&gt;Danping Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yuntao Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02343">
<title>Multi-View Bayesian Correlated Component Analysis. (arXiv:1802.02343v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.02343</link>
<description rdf:parseType="Literal">&lt;p&gt;Correlated component analysis as proposed by Dmochowski et al. (2012) is a
tool for investigating brain process similarity in the responses to multiple
views of a given stimulus. Correlated components are identified under the
assumption that the involved spatial networks are identical. Here we propose a
hierarchical probabilistic model that can infer the level of universality in
such multi-view data, from completely unrelated representations, corresponding
to canonical correlation analysis, to identical representations as in
correlated component analysis. This new model, which we denote Bayesian
correlated component analysis, evaluates favourably against three relevant
algorithms in simulated data. A well-established benchmark EEG dataset is used
to further validate the new model and infer the variability of spatial
representations across multiple subjects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kamronn_S/0/1/0/all/0/1&quot;&gt;Simon Kamronn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poulsen_A/0/1/0/all/0/1&quot;&gt;Andreas Trier Poulsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hansen_L/0/1/0/all/0/1&quot;&gt;Lars Kai Hansen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02500">
<title>Cadre Modeling: Simultaneously Discovering Subpopulations and Predictive Models. (arXiv:1802.02500v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.02500</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem in regression analysis of identifying subpopulations
that exhibit different patterns of response, where each subpopulation requires
a different underlying model. Unlike statistical cohorts, these subpopulations
are not known a priori; thus, we refer to them as cadres. When the cadres and
their associated models are interpretable, modeling leads to insights about the
subpopulations and their associations with the regression target. We introduce
a discriminative model that simultaneously learns cadre assignment and
target-prediction rules. Sparsity-inducing priors are placed on the model
parameters, under which independent feature selection is performed for both the
cadre assignment and target-prediction processes. We learn models using
adaptive step size stochastic gradient descent, and we assess cadre quality
with bootstrapped sample analysis. We present simulated results showing that,
when the true clustering rule does not depend on the entire set of features,
our method significantly outperforms methods that learn subpopulation-discovery
and target-prediction rules separately. In a materials-by-design case study,
our model provides state-of-the-art prediction of polymer glass transition
temperature. Importantly, the method identifies cadres of polymers that respond
differently to structural perturbations, thus providing design insight for
targeting or avoiding specific transition temperature ranges. It identifies
chemically meaningful cadres, each with interpretable models. Further
experimental results show that cadre methods have generalization that is
competitive with linear and nonlinear regression models and can identify robust
subpopulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+New_A/0/1/0/all/0/1&quot;&gt;Alexander New&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Breneman_C/0/1/0/all/0/1&quot;&gt;Curt Breneman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bennett_K/0/1/0/all/0/1&quot;&gt;Kristin P. Bennett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02538">
<title>Yes, but Did It Work?: Evaluating Variational Inference. (arXiv:1802.02538v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.02538</link>
<description rdf:parseType="Literal">&lt;p&gt;While it&apos;s always possible to compute a variational approximation to a
posterior distribution, it can be difficult to discover problems with this
approximation&quot;. We propose two diagnostic algorithms to alleviate this problem.
The Pareto-smoothed importance sampling (PSIS) diagnostic gives a goodness of
fit measurement for joint distributions, while simultaneously improving the
error in the estimate. The variational simulation-based calibration (VSBC)
assesses the average performance of point estimates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuling Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vehtari_A/0/1/0/all/0/1&quot;&gt;Aki Vehtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Simpson_D/0/1/0/all/0/1&quot;&gt;Daniel Simpson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gelman_A/0/1/0/all/0/1&quot;&gt;Andrew Gelman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02550">
<title>Semi-Amortized Variational Autoencoders. (arXiv:1802.02550v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.02550</link>
<description rdf:parseType="Literal">&lt;p&gt;Amortized variational inference (AVI) replaces instance-specific local
inference with a global inference network. While AVI has enabled efficient
training of deep generative models such as variational autoencoders (VAE),
recent empirical work suggests that inference networks can produce suboptimal
variational parameters. We propose a hybrid approach, to use AVI to initialize
the variational parameters and run stochastic variational inference (SVI) to
refine them. Crucially, the local SVI procedure is itself differentiable, so
the inference network and generative model can be trained end-to-end with
gradient-based optimization. This semi-amortized approach enables the use of
rich generative models without experiencing the posterior-collapse phenomenon
common in training VAEs for problems like text generation. Experiments show
this approach outperforms strong autoregressive and variational baselines on
standard text and image datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wiseman_S/0/1/0/all/0/1&quot;&gt;Sam Wiseman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Miller_A/0/1/0/all/0/1&quot;&gt;Andrew C. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sontag_D/0/1/0/all/0/1&quot;&gt;David Sontag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rush_A/0/1/0/all/0/1&quot;&gt;Alexander M. Rush&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02557">
<title>Sparse Linear Discriminant Analysis under the Neyman-Pearson Paradigm. (arXiv:1802.02557v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1802.02557</link>
<description rdf:parseType="Literal">&lt;p&gt;In contrast to the classical binary classification paradigm that minimizes
the overall classification error, the Neyman-Pearson (NP) paradigm seeks
classifiers with a minimal type II error while having a constrained type I
error under a user-specified level, addressing asymmetric type I/II error
priorities. In this work, we present NP-sLDA, a new binary NP classifier that
explicitly takes into account feature dependency under high-dimensional NP
settings. This method adapts the popular sparse linear discriminant analysis
(sLDA, Mai et al. (2012)) to the NP paradigm. We borrow the threshold
determination method from the umbrella algorithm in Tong et al. (2017). On the
theoretical front, we formulate a new conditional margin assumption and a new
conditional detection condition to accommodate unbounded feature support, and
show that NP-sLDA satisfies the NP oracle inequalities, which are natural NP
paradigm counterparts of the oracle inequalities in classical classification.
Numerical results show that NP-sLDA is a valuable addition to existing NP
classifiers. We also suggest a general data-adaptive sample splitting scheme
that, in many scenarios, improves the classification performance upon the
default half-half class $0$ split used in Tong et al. (2017), and this new
splitting scheme has been incorporated into a new version of the R package
nproc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tong_X/0/1/0/all/0/1&quot;&gt;Xin Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lucy Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiacheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02558">
<title>Intentional control of type I error over unconscious data distortion: a Neyman-Pearson classification approach. (arXiv:1802.02558v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1802.02558</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of social media enables millions of citizens to generate information
on sensitive political issues and social events, which is scarce in
authoritarian countries and is tremendously valuable for surveillance and
social studies. In the enormous efforts to utilize social media information,
censorship stands as a formidable obstacle for informative description and
accurate statistical inference. Likewise, in medical research, disease type
proportions in the samples might not represent the proportions in the general
population. To solve the information distortion problem caused by unconscious
data distortion, such as non-predictable censorship and non-representative
sampling, we propose a new distortion-invariant statistical approach to parse
data, based on the Neyman-Pearson (NP) classification paradigm. Under general
conditions, we derive explicit formulas for the after-distortion oracle
classifier with explicit dependency on the distortion rates $\beta_0$ and
$\beta_1$ on Class 0 and Class 1 respectively, and show that the NP oracle
classifier is independent of the distortion scheme. We illustrate the working
of this new method by combining the recently developed NP umbrella algorithm
with topic modeling to automatically detect posts that are related to strikes
and corruption in samples of randomly selected posts extracted from Sina
Weibo-the Chinese equivalent to Twitter. In situations where type I errors are
unacceptably large under the classical classification framework, the use of our
proposed approach allows for controlling type I errors under a desirable upper
bound.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lucy Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Richard Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanhui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tong_X/0/1/0/all/0/1&quot;&gt;Xin Tong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.08142">
<title>Modelling Preference Data with the Wallenius Distribution. (arXiv:1701.08142v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1701.08142</link>
<description rdf:parseType="Literal">&lt;p&gt;The Wallenius distribution is a generalisation of the Hypergeometric
distribution where weights are assigned to balls of different colours. This
naturally defines a model for ranking categories which can be used for
classification purposes. Since, in general, the resulting likelihood is not
analytically available, we adopt an approximate Bayesian computational (ABC)
approach for estimating the importance of the categories. We illustrate the
performance of the estimation procedure on simulated datasets. Finally, we use
the new model for analysing two datasets about movies ratings and Italian
academic statisticians&apos; journal preferences. The latter is a novel dataset
collected by the authors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grazian_C/0/1/0/all/0/1&quot;&gt;Clara Grazian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Leisen_F/0/1/0/all/0/1&quot;&gt;Fabrizio Leisen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liseo_B/0/1/0/all/0/1&quot;&gt;Brunero Liseo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.07019">
<title>Model-Robust Counterfactual Prediction Method. (arXiv:1705.07019v4 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1705.07019</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a method for assessing counterfactual predictions with multiple
groups. It is tuning-free and operational in high-dimensional covariate
scenarios, with a runtime that scales linearly in the number of datapoints. The
computational efficiency is leveraged to produce valid confidence intervals
using the conformal prediction approach. The method is model-robust in that it
enables inferences from observational data even when the data model is
misspecified. The approach is illustrated using both real and synthetic
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zachariah_D/0/1/0/all/0/1&quot;&gt;Dave Zachariah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Stoica_P/0/1/0/all/0/1&quot;&gt;Petre Stoica&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.09634">
<title>Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration. (arXiv:1705.09634v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1705.09634</link>
<description rdf:parseType="Literal">&lt;p&gt;Computing optimal transport distances such as the earth mover&apos;s distance is a
fundamental problem in machine learning, statistics, and computer vision.
Despite the recent introduction of several algorithms with good empirical
performance, it is unknown whether general optimal transport distances can be
approximated in near-linear time. This paper demonstrates that this ambitious
goal is in fact achieved by Cuturi&apos;s Sinkhorn Distances. This result relies on
a new analysis of Sinkhorn iteration, which also directly suggests a new greedy
coordinate descent algorithm, Greenkhorn, with the same theoretical guarantees.
Numerical simulations illustrate that Greenkhorn significantly outperforms the
classical Sinkhorn algorithm in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altschuler_J/0/1/0/all/0/1&quot;&gt;Jason Altschuler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weed_J/0/1/0/all/0/1&quot;&gt;Jonathan Weed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rigollet_P/0/1/0/all/0/1&quot;&gt;Philippe Rigollet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.07196">
<title>Sketched Subspace Clustering. (arXiv:1707.07196v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.07196</link>
<description rdf:parseType="Literal">&lt;p&gt;The immense amount of daily generated and communicated data presents unique
challenges in their processing. Clustering, the grouping of data without the
presence of ground-truth labels, is an important tool for drawing inferences
from data. Subspace clustering (SC) is a relatively recent method that is able
to successfully classify nonlinearly separable data in a multitude of settings.
In spite of their high clustering accuracy, SC methods incur prohibitively high
computational complexity when processing large volumes of high-dimensional
data. Inspired by random sketching approaches for dimensionality reduction, the
present paper introduces a randomized scheme for SC, termed Sketch-SC, tailored
for large volumes of high-dimensional data. Sketch-SC accelerates the
computationally heavy parts of state-of-the-art SC approaches by compressing
the data matrix across both dimensions using random projections, thus enabling
fast and accurate large-scale SC. Performance analysis as well as extensive
numerical tests on real data corroborate the potential of Sketch-SC and its
competitive performance relative to state-of-the-art scalable SC approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Traganitis_P/0/1/0/all/0/1&quot;&gt;Panagiotis A. Traganitis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.08120">
<title>GP-SUM. Gaussian Processes Filtering of non-Gaussian Beliefs. (arXiv:1709.08120v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1709.08120</link>
<description rdf:parseType="Literal">&lt;p&gt;This work studies the problem of stochastic dynamic filtering and state
propagation with complex beliefs. The main contribution is GP-SUM, a filtering
algorithm tailored to dynamic systems and observation models expressed as
Gaussian processes (GP), that does not rely on linearizations or unimodal
Gaussian approximations of the belief. The algorithm can be seen as a
combination of a sampling-based filter and a probabilistic Bayes filter. GP-SUM
operates by sampling the state distribution and propagating each sample through
the dynamic system and observation models. Effective sampling and accurate
probabilistic propagation are possible by relying on the GP form of the system,
and a Gaussian mixture form of the belief. We show that GP-SUM outperforms
several GP-Bayes and Particle Filters on a standard benchmark. We also
illustrate its practical use in a pushing task, and demonstrate that it
predicts heteroscedasticity, i.e., different amounts of uncertainty, and
multi-modality when naturally occurring in pushing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1&quot;&gt;Maria Bauza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Alberto Rodriguez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00636">
<title>Bayesian Recurrent Neural Network Models for Forecasting and Quantifying Uncertainty in Spatial-Temporal Data. (arXiv:1711.00636v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00636</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks (RNNs) are nonlinear dynamical models commonly used
in the machine learning and dynamical systems literature to represent complex
dynamical or sequential relationships between variables. More recently, as deep
learning models have become more common, RNNs have been used to forecast
increasingly complicated systems. Dynamical spatio-temporal processes represent
a class of complex systems that can potentially benefit from these types of
models. Although the RNN literature is expansive and highly developed,
uncertainty quantification is often ignored. Even when considered, the
uncertainty is generally quantified without the use of a rigorous framework,
such as a fully Bayesian setting. Here we attempt to quantify uncertainty in a
more formal framework while maintaining the forecast accuracy that makes these
models appealing, by presenting a Bayesian RNN model for nonlinear
spatio-temporal forecasting. Additionally, we make simple modifications to the
basic RNN to help accommodate the unique nature of nonlinear spatio-temporal
data. The proposed model is applied to a Lorenz simulation and two real-world
nonlinear spatio-temporal forecasting applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McDermott_P/0/1/0/all/0/1&quot;&gt;Patrick L. McDermott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wikle_C/0/1/0/all/0/1&quot;&gt;Christopher K. Wikle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00329">
<title>ZOOpt: Toolbox for Derivative-Free Optimization. (arXiv:1801.00329v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.00329</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances of derivative-free optimization allow efficient approximating
the global optimal solutions of sophisticated functions, such as functions with
many local optima, non-differentiable and non-continuous functions. This
article describes the ZOOpt (https://github.com/eyounx/ZOOpt) toolbox that
provides efficient derivative-free solvers and are designed easy to use. ZOOpt
provides a Python package for single-thread optimization, and a light-weighted
distributed version with the help of the Julia language for Python described
functions. ZOOpt toolbox particularly focuses on optimization problems in
machine learning, addressing high-dimensional, noisy, and large-scale problems.
The toolbox is being maintained toward ready-to-use tool in real-world machine
learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu-Ren Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yi-Qi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1&quot;&gt;Hong Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chao Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07222">
<title>Rover Descent: Learning to optimize by learning to navigate on prototypical loss surfaces. (arXiv:1801.07222v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07222</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to optimize - the idea that we can learn from data algorithms that
optimize a numerical criterion - has recently been at the heart of a growing
number of research efforts. One of the most challenging issues within this
approach is to learn a policy that is able to optimize over classes of
functions that are fairly different from the ones that it was trained on. We
propose a novel way of framing learning to optimize as a problem of learning a
good navigation policy on a partially observable loss surface. To this end, we
develop Rover Descent, a solution that allows us to learn a fairly broad
optimization policy from training on a small set of prototypical
two-dimensional surfaces that encompasses the classically hard cases such as
valleys, plateaus, cliffs and saddles and by using strictly zero-order
information. We show that, without having access to gradient or curvature
information, we achieve state-of-the-art convergence speed on optimization
problems not presented at training time such as the Rosenbrock function and
other hard cases in two dimensions. We extend our framework to optimize over
high dimensional landscapes, while still handling only two-dimensional local
landscape information and show good preliminary results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faury_L/0/1/0/all/0/1&quot;&gt;Louis Faury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasile_F/0/1/0/all/0/1&quot;&gt;Flavian Vasile&lt;/a&gt;</dc:creator>
</item></rdf:RDF>