<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-02T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00458"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00524"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00675"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00496"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00679"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00727"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00758"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00934"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07099"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04854"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06055"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09997"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11615"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00490"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00516"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00523"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00560"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00563"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00564"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00601"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00616"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00628"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00629"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00665"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00668"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00723"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00741"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00803"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00814"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00818"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00831"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00845"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00892"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00911"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00935"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05597"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03987"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08898"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09730"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11811"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04489"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.00458">
<title>Matrix optimization on universal unitary photonic devices. (arXiv:1808.00458v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1808.00458</link>
<description rdf:parseType="Literal">&lt;p&gt;Universal unitary photonic devices are capable of applying arbitrary unitary
transformations to multi-port coherent light inputs and provide a promising
hardware platform for fast and energy-efficient machine learning. We address
the problem of training universal photonic devices composed of meshes of
tunable beamsplitters to learn unknown unitary matrices. The
locally-interacting nature of the mesh components limits the fidelity of the
learned matrices if phase shifts are randomly initialized. We propose an
initialization procedure derived from the Haar measure over unitary matrices
that overcomes this limitation. We also embed various model architectures
within a standard rectangular mesh &quot;canvas,&quot; and our numerical experiments show
significantly improved scalability and training speed, even in the presence of
fabrication errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pai_S/0/1/0/all/0/1&quot;&gt;Sunil Pai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bartlett_B/0/1/0/all/0/1&quot;&gt;Ben Bartlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Solgaard_O/0/1/0/all/0/1&quot;&gt;Olav Solgaard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Miller_D/0/1/0/all/0/1&quot;&gt;David A. B. Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00524">
<title>Cooperative Group Optimization with Ants (CGO-AS): Leverage Optimization with Mixed Individual and Social Learning. (arXiv:1808.00524v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.00524</link>
<description rdf:parseType="Literal">&lt;p&gt;We present CGO-AS, a generalized Ant System (AS) implemented in the framework
of Cooperative Group Optimization (CGO), to show the leveraged optimization
with a mixed individual and social learning. Ant colony is a simple yet
efficient natural system for understanding the effects of primary intelligence
on optimization. However, existing AS algorithms are mostly focusing on their
capability of using social heuristic cues while ignoring their individual
learning. CGO can integrate the advantages of a cooperative group and a
low-level algorithm portfolio design, and the agents of CGO can explore both
individual and social search. In CGO-AS, each ant (agent) is added with an
individual memory, and is implemented with a novel search strategy to use
individual and social cues in a controlled proportion. The presented CGO-AS is
therefore especially useful in exposing the power of the mixed individual and
social learning for improving optimization. The optimization performance is
tested with instances of the Traveling Salesman Problem (TSP). The results
prove that a cooperative ant group using both individual and social learning
obtains a better performance than the systems solely using either individual or
social learning. The best performance is achieved under the condition when
agents use individual memory as their primary information source, and
simultaneously use social memory as their searching guidance. In comparison
with existing AS systems, CGO-AS retains a faster learning speed toward those
higher-quality solutions, especially in the later learning cycles. The leverage
in optimization by CGO-AS is highly possible due to its inherent feature of
adaptively maintaining the population diversity in the individual memory of
agents, and of accelerating the learning process with accumulated knowledge in
the social memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiao-Feng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zun-Jing Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00675">
<title>Winner-Take-All as Basic Probabilistic Inference Unit of Neuronal Circuits. (arXiv:1808.00675v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1808.00675</link>
<description rdf:parseType="Literal">&lt;p&gt;Experimental observations of neuroscience suggest that the brain is working a
probabilistic way when computing information with uncertainty. This processing
could be modeled as Bayesian inference. However, it remains unclear how
Bayesian inference could be implemented at the level of neuronal circuits of
the brain. In this study, we propose a novel general-purpose neural
implementation of probabilistic inference based on a ubiquitous network of
cortical microcircuits, termed winner-take-all (WTA) circuit. We show that each
WTA circuit could encode the distribution of states defined on a variable. By
connecting multiple WTA circuits together, the joint distribution can be
represented for arbitrary probabilistic graphical models. Moreover, we prove
that the neural dynamics of WTA circuit is able to implement one of the most
powerful inference methods in probabilistic graphical models, mean-field
inference. We show that the synaptic drive of each spiking neuron in the WTA
circuit encodes the marginal probability of the variable in each state, and the
firing probability (or firing rate) of each neuron is proportional to the
marginal probability. Theoretical analysis and experimental results demonstrate
that the WTA circuits can get comparable inference result as mean-field
approximation. Taken together, our results suggest that the WTA circuit could
be seen as the minimal inference unit of neuronal circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tiejun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jian K. Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00496">
<title>SlimNets: An Exploration of Deep Model Compression and Acceleration. (arXiv:1808.00496v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.00496</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have achieved increasingly accurate results on a wide
variety of complex tasks. However, much of this improvement is due to the
growing use and availability of computational resources (e.g use of GPUs, more
layers, more parameters, etc). Most state-of-the-art deep networks, despite
performing well, over-parameterize approximate functions and take a significant
amount of time to train. With increased focus on deploying deep neural networks
on resource constrained devices like smart phones, there has been a push to
evaluate why these models are so resource hungry and how they can be made more
efficient. This work evaluates and compares three distinct methods for deep
model compression and acceleration: weight pruning, low rank factorization, and
knowledge distillation. Comparisons on VGG nets trained on CIFAR10 show that
each of the models on their own are effective, but that the true power lies in
combining them. We show that by combining pruning and knowledge distillation
methods we can create a compressed network 85 times smaller than the original,
all while retaining 96% of the original model&apos;s accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oguntola_I/0/1/0/all/0/1&quot;&gt;Ini Oguntola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olubeko_S/0/1/0/all/0/1&quot;&gt;Subby Olubeko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sweeney_C/0/1/0/all/0/1&quot;&gt;Christopher Sweeney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00679">
<title>Memristor-based Synaptic Sampling Machines. (arXiv:1808.00679v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1808.00679</link>
<description rdf:parseType="Literal">&lt;p&gt;Synaptic Sampling Machine (SSM) is a type of neural network model that
considers biological unreliability of the synapses. We propose the circuit
design of the SSM neural network which is realized through the memristive-CMOS
crossbar structure with the synaptic sampling cell (SSC) being used as a basic
stochastic unit. The increase in the edge computing devices in the Internet of
things era, drives the need for hardware acceleration for data processing and
computing. The computational considerations of the processing speed and
possibility for the real-time realization pushes the synaptic sampling
algorithm that demonstrated promising results on software for hardware
implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolzhikova_I/0/1/0/all/0/1&quot;&gt;Irina Dolzhikova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salama_K/0/1/0/all/0/1&quot;&gt;Khaled Salama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kizheppatt_V/0/1/0/all/0/1&quot;&gt;Vipin Kizheppatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_A/0/1/0/all/0/1&quot;&gt;Alex Pappachen James&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00727">
<title>Inlining External Sources in Answer Set Programs. (arXiv:1808.00727v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.00727</link>
<description rdf:parseType="Literal">&lt;p&gt;HEX-programs are an extension of answer set programs (ASP) with external
sources. To this end, external atoms provide a bidirectional interface between
the program and an external source. The traditional evaluation algorithm for
HEX-programs is based on guessing truth values of external atoms and verifying
them by explicit calls of the external source. The approach was optimized by
techniques that reduce the number of necessary verification calls or speed them
up, but the remaining external calls are still expensive. In this paper we
present an alternative evaluation approach based on inlining of external atoms,
motivated by existing but less general approaches for specialized formalisms
such as DL-programs. External atoms are then compiled away such that no
verification calls are necessary. The approach is implemented in the dlvhex
reasoner. Experiments show a significant performance gain. Besides performance
improvements, we further exploit inlining for extending previous (semantic)
characterizations of program equivalence from ASP to HEX-programs, including
those of strong equivalence, uniform equivalence and H, B -equivalence.
Finally, based on these equivalence criteria, we characterize also
inconsistency of programs wrt. extensions. Since well-known ASP extensions
(such as constraint ASP) are special cases of HEX, the results are interesting
beyond the particular formalism. Under consideration in Theory and Practice of
Logic Programming (TPLP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redl_C/0/1/0/all/0/1&quot;&gt;Christoph Redl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00758">
<title>Attentional Aggregation of Deep Feature Sets for Multi-view 3D Reconstruction. (arXiv:1808.00758v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.00758</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of recovering an underlying 3D shape from a set of
images. Existing learning based approaches usually resort to recurrent neural
nets, e.g., GRU, or intuitive pooling operations, e.g., max/mean pooling, to
fuse multiple deep features encoded from input images. However, GRU based
approaches are unable to consistently estimate 3D shapes given the same set of
input images as the recurrent unit is permutation variant. It is also unlikely
to refine the 3D shape given more images due to the long-term memory loss of
GRU. The widely used pooling approaches are limited to capturing only the first
order/moment information, ignoring other valuable features. In this paper, we
present a new feed-forward neural module, named AttSets, together with a
dedicated training algorithm, named JTSO, to attentionally aggregate an
arbitrary sized deep feature set for multi-view 3D reconstruction. AttSets is
permutation invariant, computationally efficient, flexible and robust to
multiple input images. We thoroughly evaluate various properties of AttSets on
large public datasets. Extensive experiments show AttSets together with JTSO
algorithm significantly outperforms existing aggregation approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1&quot;&gt;Andrew Markham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1&quot;&gt;Niki Trigoni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00934">
<title>Streaming Kernel PCA with $\tilde{O}(\sqrt{n})$ Random Features. (arXiv:1808.00934v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.00934</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the statistical and computational aspects of kernel principal
component analysis using random Fourier features and show that under mild
assumptions, $O(\sqrt{n} \log n)$ features suffices to achieve
$O(1/\epsilon^2)$ sample complexity. Furthermore, we give a memory efficient
streaming algorithm based on classical Oja&apos;s algorithm that achieves this rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullah_E/0/1/0/all/0/1&quot;&gt;Enayat Ullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mianjy_P/0/1/0/all/0/1&quot;&gt;Poorya Mianjy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinov_T/0/1/0/all/0/1&quot;&gt;Teodor V. Marinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_R/0/1/0/all/0/1&quot;&gt;Raman Arora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05497">
<title>What Can This Robot Do? Learning from Appearance and Experiments. (arXiv:1712.05497v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.05497</link>
<description rdf:parseType="Literal">&lt;p&gt;When presented with an unknown robot (subject) how can an autonomous agent
(learner) figure out what this new robot can do? The subject&apos;s appearance can
provide cues to its physical as well as cognitive capabilities. Seeing a
humanoid can make one wonder if it can kick balls, climb stairs or recognize
faces. What if the learner can request the subject to perform these tasks? We
present an approach to make the learner build a model of the subject at a task
based on the latter&apos;s appearance and refine it by experimentation. Apart from
the subject&apos;s inherent capabilities, certain extrinsic factors may affect its
performance at a task. Based on the subject&apos;s appearance and prior knowledge
about the task a learner can identify a set of potential factors, a subset of
which we assume are controllable. Our approach picks values of controllable
factors to generate the most informative experiments to test the subject at.
Additionally, we present a metric to determine if a factor should be
incorporated in the model. We present results of our approach on modeling a
humanoid robot at the task of kicking a ball. Firstly, we show that actively
picking values for controllable factors, even in noisy experiments, leads to
faster learning of the subject&apos;s model for the task. Secondly, starting from a
minimal set of factors our metric identifies the set of relevant factors to
incorporate in the model. Lastly, we show that the refined model better
represents the subject&apos;s performance at the task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadke_A/0/1/0/all/0/1&quot;&gt;Ashwin Khadke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1&quot;&gt;Manuela Veloso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07099">
<title>Loop Restricted Existential Rules and First-order Rewritability for Query Answering. (arXiv:1804.07099v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07099</link>
<description rdf:parseType="Literal">&lt;p&gt;In ontology-based data access (OBDA), the classical database is enhanced with
an ontology in the form of logical assertions generating new intensional
knowledge. A powerful form of such logical assertions is the tuple-generating
dependencies (TGDs), also called existential rules, where Horn rules are
extended by allowing existential quantifiers to appear in the rule heads. In
this paper we introduce a new language called loop restricted (LR) TGDs
(existential rules), which are TGDs with certain restrictions on the loops
embedded in the underlying rule set. We study the complexity of this new
language. We show that the conjunctive query answering (CQA) under the LR TGDs
is decid- able. In particular, we prove that this language satisfies the
so-called bounded derivation-depth prop- erty (BDDP), which implies that the
CQA is first-order rewritable, and its data complexity is in AC0 . We also
prove that the combined complexity of the CQA is EXPTIME complete, while the
language membership is PSPACE complete. Then we extend the LR TGDs language to
the generalised loop restricted (GLR) TGDs language, and prove that this class
of TGDs still remains to be first-order rewritable and properly contains most
of other first-order rewritable TGDs classes discovered in the literature so
far.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asuncion_V/0/1/0/all/0/1&quot;&gt;Vernon Asuncion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Heng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yun Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_W/0/1/0/all/0/1&quot;&gt;Weisheng Si&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04854">
<title>Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam. (arXiv:1806.04854v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04854</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty computation in deep learning is essential to design robust and
reliable systems. Variational inference (VI) is a promising approach for such
computation, but requires more effort to implement and execute compared to
maximum-likelihood methods. In this paper, we propose new natural-gradient
algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms
can be implemented within the Adam optimizer by perturbing the network weights
during gradient evaluations, and uncertainty estimates can be cheaply obtained
by using the vector that adapts the learning rate. This requires lower memory,
computation, and implementation effort than existing VI methods, while
obtaining uncertainty estimates of comparable quality. Our empirical results
confirm this and further suggest that the weight-perturbation in our algorithm
could be useful for exploration in reinforcement learning and stochastic
optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nielsen_D/0/1/0/all/0/1&quot;&gt;Didrik Nielsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tangkaratt_V/0/1/0/all/0/1&quot;&gt;Voot Tangkaratt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gal_Y/0/1/0/all/0/1&quot;&gt;Yarin Gal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Akash Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06055">
<title>Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees. (arXiv:1806.06055v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.06055</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing classification algorithms that are fair with respect to sensitive
attributes of the data has become an important problem due to the growing
deployment of classification algorithms in various social contexts. Several
recent works have focused on fairness with respect to a specific metric,
modeled the corresponding fair classification problem as a constrained
optimization problem, and developed tailored algorithms to solve them. Despite
this, there still remain important metrics for which we do not have fair
classifiers and many of the aforementioned algorithms do not come with
theoretical guarantees; perhaps because the resulting optimization problem is
non-convex. The main contribution of this paper is a new meta-algorithm for
classification that takes as input a large class of fairness constraints, with
respect to multiple non-disjoint sensitive attributes, and which comes with
provable guarantees. This is achieved by first developing a meta-algorithm for
a large family of classification problems with convex constraints, and then
showing that classification problems with general types of fairness constraints
can be reduced to those in this family. We present empirical results that show
that our algorithm can achieve near-perfect fairness with respect to various
fairness metrics, and that the loss in accuracy due to the imposed fairness
constraints is often small. Overall, this work unifies several prior works on
fair classification, presents a practical algorithm with theoretical
guarantees, and can handle fairness metrics that were previously not possible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celis_L/0/1/0/all/0/1&quot;&gt;L. Elisa Celis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lingxiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keswani_V/0/1/0/all/0/1&quot;&gt;Vijay Keswani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1&quot;&gt;Nisheeth K. Vishnoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09997">
<title>Probabilistic Inference Using Generators - The Statues Algorithm. (arXiv:1806.09997v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.09997</link>
<description rdf:parseType="Literal">&lt;p&gt;We present here a new probabilistic inference algorithm that gives exact
results in the domain of discrete probability distributions. This algorithm,
named the Statues algorithm, calculates the marginal probability distribution
on probabilistic models defined as direct acyclic graphs. These models are made
up of well-defined primitives that allow to express, in particular, joint
probability distributions, Bayesian networks, discrete Markov chains,
conditioning and probabilistic arithmetic. The Statues algorithm relies on a
variable binding mechanism based on the generator construct, a special form of
coroutine; being related to the enumeration algorithm, this new algorithm
brings important improvements in terms of efficiency, which makes it valuable
in regard to other exact marginalization algorithms. After introduction of
several definitions, primitives and compositional rules, we present in details
the Statues algorithm. Then, we briefly discuss the interest of this algorithm
compared to others and we present possible extensions. Finally, we introduce
Lea and MicroLea, two Python libraries implementing the Statues algorithm,
along with several use cases. A proof of the correctness of the algorithm is
provided in appendix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denis_P/0/1/0/all/0/1&quot;&gt;Pierre Denis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11615">
<title>Semantic DMN: Formalizing and Reasoning About Decisions in the Presence of Background Knowledge. (arXiv:1807.11615v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1807.11615</link>
<description rdf:parseType="Literal">&lt;p&gt;The Decision Model and Notation (DMN) is a recent OMG standard for the
elicitation and representation of decision models, and for managing their
interconnection with business processes. DMN builds on the notion of decision
table, and their combination into more complex decision requirements graphs
(DRGs), which bridge between business process models and decision logic models.
DRGs may rely on additional, external business knowledge models, whose
functioning is not part of the standard. In this work, we consider one of the
most important types of business knowledge, namely background knowledge that
conceptually accounts for the structural aspects of the domain of interest, and
propose decision requirement knowledge bases (DKBs), where DRGs are modeled in
DMN, and domain knowledge is captured by means of first-order logic with
datatypes. We provide a logic-based semantics for such an integration, and
formalize different DMN reasoning tasks for DKBs. We then consider background
knowledge formulated as a description logic ontology with datatypes, and show
how the main verification tasks for DMN in this enriched setting, can be
formalized as standard DL reasoning services, and actually carried out in
ExpTime. We discuss the effectiveness of our framework on a case study in
maritime security. This work is under consideration for publication in Theory
and Practice of Logic Programming (TPLP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calvanese_D/0/1/0/all/0/1&quot;&gt;Diego Calvanese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_M/0/1/0/all/0/1&quot;&gt;Marlon Dumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Maria Maggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montali_M/0/1/0/all/0/1&quot;&gt;Marco Montali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00490">
<title>Deep Reinforcement Learning for Distributed Dynamic Power Allocation in Wireless Networks. (arXiv:1808.00490v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1808.00490</link>
<description rdf:parseType="Literal">&lt;p&gt;This work demonstrates the potential of deep reinforcement learning
techniques for transmit power control in emerging and future wireless networks.
Various techniques have been proposed in the literature to find near-optimal
power allocations, often by solving a challenging optimization problem. Most of
these algorithms are not scalable to large networks in real-world scenarios
because of their computational complexity and instantaneous cross-cell channel
state information (CSI) requirement. In this paper, a model-free distributed
dynamic power allocation scheme is developed based on deep reinforcement
learning. Each transmitter collects CSI and quality of service (QoS)
information from several neighbors and adapts its own transmit power
accordingly. The objective is to maximize a weighted sum-rate utility function,
which can be particularized to achieve maximum sum-rate or proportionally fair
scheduling (with weights that are changing over time). Both random variations
and delays in the CSI are inherently addressed using deep Q-learning. For a
typical network architecture, the proposed algorithm is shown to achieve
near-optimal power allocation in real time based on delayed CSI measurements
available to the agents. This work indicates that deep reinforcement learning
based radio resource management can be very fast and deliver highly competitive
performance, especially in practical scenarios where the system model is
inaccurate and CSI delay is non-negligible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nasir_Y/0/1/0/all/0/1&quot;&gt;Yasar Sinan Nasir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Dongning Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00516">
<title>A Learning-Based Framework for Two-Dimensional Vehicle Maneuver Prediction over V2V Networks. (arXiv:1808.00516v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1808.00516</link>
<description rdf:parseType="Literal">&lt;p&gt;Situational awareness in vehicular networks could be substantially improved
utilizing reliable trajectory prediction methods. More precise situational
awareness, in turn, results in notably better performance of critical safety
applications, such as Forward Collision Warning (FCW), as well as comfort
applications like Cooperative Adaptive Cruise Control (CACC). Therefore,
vehicle trajectory prediction problem needs to be deeply investigated in order
to come up with an end to end framework with enough precision required by the
safety applications&apos; controllers. This problem has been tackled in the
literature using different methods. However, machine learning, which is a
promising and emerging field with remarkable potential for time series
prediction, has not been explored enough for this purpose. In this paper, a
two-layer neural network-based system is developed which predicts the future
values of vehicle parameters, such as velocity, acceleration, and yaw rate, in
the first layer and then predicts the two-dimensional, i.e. longitudinal and
lateral, trajectory points based on the first layer&apos;s outputs. The performance
of the proposed framework has been evaluated in realistic cut-in scenarios from
Safety Pilot Model Deployment (SPMD) dataset and the results show a noticeable
improvement in the prediction accuracy in comparison with the kinematics model
which is the dominant employed model by the automotive industry. Both ideal and
nonideal communication circumstances have been investigated for our system
evaluation. For non-ideal case, an estimation step is included in the framework
before the parameter prediction block to handle the drawbacks of packet drops
or sensor failures and reconstruct the time series of vehicle parameters at a
desirable frequency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahjoub_H/0/1/0/all/0/1&quot;&gt;Hossein Nourkhiz Mahjoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tahmasbi_Sarvestani_A/0/1/0/all/0/1&quot;&gt;Amin Tahmasbi-Sarvestani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazemi_H/0/1/0/all/0/1&quot;&gt;Hadi Kazemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fallah_Y/0/1/0/all/0/1&quot;&gt;Yaser P. Fallah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00523">
<title>Mod-DeepESN: Modular Deep Echo State Network. (arXiv:1808.00523v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.00523</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuro-inspired recurrent neural network algorithms, such as echo state
networks, are computationally lightweight and thereby map well onto untethered
devices. The baseline echo state network algorithms are shown to be efficient
in solving small-scale spatio-temporal problems. However, they underperform for
complex tasks that are characterized by multi-scale structures. In this
research, an intrinsic plasticity-infused modular deep echo state network
architecture is proposed to solve complex and multiple timescale temporal
tasks. It outperforms state-of-the-art for time series prediction tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmichael_Z/0/1/0/all/0/1&quot;&gt;Zachariah Carmichael&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syed_H/0/1/0/all/0/1&quot;&gt;Humza Syed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burtner_S/0/1/0/all/0/1&quot;&gt;Stuart Burtner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kudithipudi_D/0/1/0/all/0/1&quot;&gt;Dhireesha Kudithipudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00560">
<title>Spectral Mixture Kernels with Time and Phase Delay Dependencies. (arXiv:1808.00560v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.00560</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral Mixture (SM) kernels form a powerful class of kernels for Gaussian
processes, capable to discover patterns, extrapolate, and model negative
co-variances. In SM kernels, spectral mixture components are linearly combined
to construct a final flexible kernel. As a consequence SM kernels does not
explicitly model correlations between components and dependencies related to
time and phase delays between components, because only the auto-convolution of
base components are used. To address these drawbacks we introduce Generalized
Convolution Spectral Mixture (GCSM) kernels. We incorporate time and phase
delay into the base spectral mixture and use cross-convolution between a base
component and the complex conjugate of another base component to construct a
complex-valued and positive definite kernel representing correlations between
base components. In this way the total number of components in GCSM becomes
quadratic. We perform a thorough comparative experimental analysis of GCSM on
synthetic and real-life datasets. Results indicate the beneficial effect of the
extra features of GCSM. This is illustrated in the problem of forecasting the
long range trend of a river flow to monitor environment evolution, where GCSM
is capable of discovering correlated patterns that SM cannot and improving
patterns recognition ability of SM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groot_P/0/1/0/all/0/1&quot;&gt;Perry Groot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchiori_E/0/1/0/all/0/1&quot;&gt;Elena Marchiori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00563">
<title>Data Augmentation for Robust Keyword Spotting under Playback Interference. (arXiv:1808.00563v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.00563</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate on-device keyword spotting (KWS) with low false accept and false
reject rate is crucial to customer experience for far-field voice control of
conversational agents. It is particularly challenging to maintain low false
reject rate in real world conditions where there is (a) ambient noise from
external sources such as TV, household appliances, or other speech that is not
directed at the device (b) imperfect cancellation of the audio playback from
the device, resulting in residual echo, after being processed by the Acoustic
Echo Cancellation (AEC) system. In this paper, we propose a data augmentation
strategy to improve keyword spotting performance under these challenging
conditions. The training set audio is artificially corrupted by mixing in music
and TV/movie audio, at different signal to interference ratios. Our results
show that we get around 30-45% relative reduction in false reject rates, at a
range of false alarm rates, under audio playback from such devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1&quot;&gt;Anirudh Raju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panchapagesan_S/0/1/0/all/0/1&quot;&gt;Sankaran Panchapagesan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandal_A/0/1/0/all/0/1&quot;&gt;Arindam Mandal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strom_N/0/1/0/all/0/1&quot;&gt;Nikko Strom&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00564">
<title>Towards fully automated protein structure elucidation with NMR spectroscopy. (arXiv:1808.00564v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1808.00564</link>
<description rdf:parseType="Literal">&lt;p&gt;Nuclear magnetic resonance (NMR) spectroscopy is one of the leading
techniques for protein studies. The method features a number of properties,
allowing to explain macromolecular interactions mechanistically and resolve
structures with atomic resolution. However, due to laborious data analysis, a
full potential of NMR spectroscopy remains unexploited. Here we present an
approach aiming at automation of two major bottlenecks in the analysis
pipeline, namely, peak picking and chemical shift assignment. Our approach
combines deep learning, non-parametric models and combinatorial optimization,
and is able to detect signals of interest in a multidimensional NMR data with
high accuracy and match them with atoms in medium-length protein sequences,
which is a preliminary step to solve protein spatial structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Klukowski_P/0/1/0/all/0/1&quot;&gt;Piotr Klukowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gonczarek_A/0/1/0/all/0/1&quot;&gt;Adam Gonczarek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00601">
<title>Classification of Building Information Model (BIM) Structures with Deep Learning. (arXiv:1808.00601v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.00601</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we study an application of machine learning to the construction
industry and we use classical and modern machine learning methods to categorize
images of building designs into three classes: Apartment building, Industrial
building or Other. No real images are used, but only images extracted from
Building Information Model (BIM) software, as these are used by the
construction industry to store building designs. For this task, we compared
four different methods: the first is based on classical machine learning, where
Histogram of Oriented Gradients (HOG) was used for feature extraction and a
Support Vector Machine (SVM) for classification; the other three methods are
based on deep learning, covering common pre-trained networks as well as ones
designed from scratch. To validate the accuracy of the models, a database of
240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and
above 89% for the neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lomio_F/0/1/0/all/0/1&quot;&gt;Francesco Lomio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farinha_R/0/1/0/all/0/1&quot;&gt;Ricardo Farinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laasonen_M/0/1/0/all/0/1&quot;&gt;Mauri Laasonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huttunen_H/0/1/0/all/0/1&quot;&gt;Heikki Huttunen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00616">
<title>Mixture Matrix Completion. (arXiv:1808.00616v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.00616</link>
<description rdf:parseType="Literal">&lt;p&gt;Completing a data matrix X has become an ubiquitous problem in modern data
science, with applications in recommender systems, computer vision, and
networks inference, to name a few. One typical assumption is that X is
low-rank. A more general model assumes that each column of X corresponds to one
of several low-rank matrices. This paper generalizes these models to what we
call mixture matrix completion (MMC): the case where each entry of X
corresponds to one of several low-rank matrices. MMC is a more accurate model
for recommender systems, and brings more flexibility to other completion and
clustering problems. We make four fundamental contributions about this new
model. First, we show that MMC is theoretically possible (well-posed). Second,
we give its precise information-theoretic identifiability conditions. Third, we
derive the sample complexity of MMC. Finally, we give a practical algorithm for
MMC with performance comparable to the state-of-the-art for simpler related
problems, both on synthetic and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pimentel_Alarcon_D/0/1/0/all/0/1&quot;&gt;Daniel L. Pimentel-Alarc&amp;#xf3;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00628">
<title>Fusion Subspace Clustering: Full and Incomplete Data. (arXiv:1808.00628v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.00628</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern inference and learning often hinge on identifying low-dimensional
structures that approximate large scale data. Subspace clustering achieves this
through a union of linear subspaces. However, in contemporary applications data
is increasingly often incomplete, rendering standard (full-data) methods
inapplicable. On the other hand, existing incomplete-data methods present major
drawbacks, like lifting an already high-dimensional problem, or requiring a
super polynomial number of samples. Motivated by this, we introduce a new
subspace clustering algorithm inspired by fusion penalties. The main idea is to
permanently assign each datum to a subspace of its own, and minimize the
distance between the subspaces of all data, so that subspaces of the same
cluster get fused together. Our approach is entirely new to both, full and
missing data, and unlike other methods, it directly allows noise, it requires
no liftings, it allows low, high, and even full-rank data, it approaches
optimal (information-theoretic) sampling rates, and it does not rely on other
methods such as low-rank matrix completion to handle missing data. Furthermore,
our extensive experiments on both real and synthetic data show that our
approach performs comparably to the state-of-the-art with complete data, and
dramatically better if data is missing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pimentel_Alarcon_D/0/1/0/all/0/1&quot;&gt;Daniel L. Pimentel-Alarc&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmood_U/0/1/0/all/0/1&quot;&gt;Usman Mahmood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00629">
<title>Induction of Non-Monotonic Logic Programs to Explain Boosted Tree Models Using LIME. (arXiv:1808.00629v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.00629</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a heuristic based algorithm to induce non-monotonic logic programs
that would explain the behavior of XGBoost trained classifiers. We use the LIME
technique to locally select the most important features contributing to the
classification decision. Then, in order to explain the model&apos;s global behavior,
we propose the UFOLD algorithm ---a heuristic-based ILP algorithm capable of
learning non-monotonic logic programs--- that we apply to a transformed dataset
produced by LIME. Our experiments with UCI standard benchmarks suggest a
significant improvement in terms of the classification evaluation metrics.
Meanwhile, the number of induced rules dramatically decreases compared ALEPH, a
state-of-the-art ILP system. While the proposed approach is agnostic to the
choice of ILP algorithm, our experiments suggest that the UFOLD algorithm
almost always outperforms ALEPH once incorporated in this approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakerin_F/0/1/0/all/0/1&quot;&gt;Farhad Shakerin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1&quot;&gt;Gopal Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00665">
<title>Investigating accuracy of pitch-accent annotations in neural network-based speech synthesis and denoising effects. (arXiv:1808.00665v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1808.00665</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigated the impact of noisy linguistic features on the performance of
a Japanese speech synthesis system based on neural network that uses WaveNet
vocoder. We compared an ideal system that uses manually corrected linguistic
features including phoneme and prosodic information in training and test sets
against a few other systems that use corrupted linguistic features. Both
subjective and objective results demonstrate that corrupted linguistic
features, especially those in the test set, affected the ideal system&apos;s
performance significantly in a statistical sense due to a mismatched condition
between the training and test sets. Interestingly, while an utterance-level
Turing test showed that listeners had a difficult time differentiating
synthetic speech from natural speech, it further indicated that adding noise to
the linguistic features in the training set can partially reduce the effect of
the mismatch, regularize the model, and help the system perform better when
linguistic features of the test set are noisy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luong_H/0/1/0/all/0/1&quot;&gt;Hieu-Thi Luong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nishizawa_N/0/1/0/all/0/1&quot;&gt;Nobuyuki Nishizawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00668">
<title>On the achievability of blind source separation for high-dimensional nonlinear source mixtures. (arXiv:1808.00668v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.00668</link>
<description rdf:parseType="Literal">&lt;p&gt;For many years, a combination of principal component analysis (PCA) and
independent component analysis (ICA) has been used as a blind source separation
(BSS) technique to separate hidden sources of natural data. However, it is
unclear why these linear methods work well because most real-world data involve
nonlinear mixtures of sources. We show that a cascade of PCA and ICA can solve
this nonlinear BSS problem accurately as the variety of input signals
increases. Specifically, we present two theorems that guarantee asymptotically
zero-error BSS when sources are mixed by a feedforward network with two
processing layers. Our first theorem analytically quantifies the performance of
an optimal linear encoder that reconstructs independent sources. Zero-error is
asymptotically reached when the number of sources is large and the numbers of
inputs and nonlinear bases are large relative to the number of sources. The
next question involves finding an optimal linear encoder without observing the
underlying sources. Our second theorem guarantees that PCA can reliably extract
all the subspace represented by the optimal linear encoder, so that a
subsequent application of ICA can separate all sources. Thereby, for almost all
nonlinear generative processes with sufficient variety, the cascade of PCA and
ICA performs asymptotically zero-error BSS in an unsupervised manner. We
analytically and numerically validate the theorems. These results highlight the
utility of linear BSS techniques for accurately recovering nonlinearly mixed
sources when observations are sufficiently diverse. We also discuss a possible
biological BSS implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Isomura_T/0/1/0/all/0/1&quot;&gt;Takuya Isomura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Toyoizumi_T/0/1/0/all/0/1&quot;&gt;Taro Toyoizumi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00723">
<title>High-dimensional regression in practice: an empirical study of finite-sample prediction, variable selection and ranking. (arXiv:1808.00723v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1808.00723</link>
<description rdf:parseType="Literal">&lt;p&gt;Penalized likelihood methods are widely used for high-dimensional regression.
Although many methods have been proposed and the associated theory is now
well-developed, the relative efficacy of different methods in finite-sample
settings, as encountered in practice, remains incompletely understood. There is
therefore a need for empirical investigations in this area that can offer
practical insight and guidance to users of these methods. In this paper we
present a large-scale comparison of penalized regression methods. We
distinguish between three related goals: prediction, variable selection and
variable ranking. Our results span more than 1,800 data-generating scenarios,
allowing us to systematically consider the influence of various factors (sample
size, dimensionality, sparsity, signal strength and multicollinearity). We
consider several widely-used methods (Lasso, Elastic Net, Ridge Regression,
SCAD, the Dantzig Selector as well as Stability Selection). We find
considerable variation in performance between methods, with results dependent
on details of the data-generating scenario and the specific goal. Our results
support a `no panacea&apos; view, with no unambiguous winner across all scenarios,
even in this restricted setting where all data align well with the assumptions
underlying the methods. Lasso is well-behaved, performing competitively in many
scenarios, while SCAD is highly variable. Substantial benefits from a
Ridge-penalty are only seen in the most challenging scenarios with strong
multi-collinearity. The results are supported by semi-synthetic analyzes using
gene expression data from cancer samples. Our empirical results complement
existing theory and provide a resource to compare methods across a range of
scenarios and metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Sach Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Richardson_S/0/1/0/all/0/1&quot;&gt;Sylvia Richardson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hill_S/0/1/0/all/0/1&quot;&gt;Steven M. Hill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00741">
<title>Online Aggregation of Unbounded Losses Using Shifting Experts with Confidence. (arXiv:1808.00741v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.00741</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop the setting of sequential prediction based on shifting experts and
on a &quot;smooth&quot; version of the method of specialized experts. To aggregate
experts predictions, we use the AdaHedge algorithm, which is a version of the
Hedge algorithm with adaptive learning rate, and extend it by the
meta-algorithm Fixed Share. Due to this, we combine the advantages of both
algorithms: (1) we use the shifting regret which is a more optimal
characteristic of the algorithm; (2) regret bounds are valid in the case of
signed unbounded losses of the experts. Also, (3) we incorporate in this scheme
a &quot;smooth&quot; version of the method of specialized experts which allows us to make
more flexible and accurate predictions. All results are obtained in the
adversarial setting -- no assumptions are made about the nature of data source.
We present results of numerical experiments for short-term forecasting of
electricity consumption based on a real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyugin_V/0/1/0/all/0/1&quot;&gt;Vladimir V&amp;#x27;yugin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trunov_V/0/1/0/all/0/1&quot;&gt;Vladimir Trunov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00803">
<title>Mobile big data analysis with machine learning. (arXiv:1808.00803v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.00803</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates to identify the requirement and the development of
machine learning-based mobile big data analysis through discussing the insights
of challenges in the mobile big data (MBD). Furthermore, it reviews the
state-of-the-art applications of data analysis in the area of MBD. Firstly, we
introduce the development of MBD. Secondly, the frequently adopted methods of
data analysis are reviewed. Three typical applications of MBD analysis, namely
wireless channel modeling, human online and offline behavior analysis, and
speech recognition in the internet of vehicles, are introduced respectively.
Finally, we summarize the main challenges and future development directions of
mobile big data analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jiyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zeyu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yupeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00814">
<title>Classification of EEG Signal based on non-Gaussian Neutral Vector. (arXiv:1808.00814v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.00814</link>
<description rdf:parseType="Literal">&lt;p&gt;In the design of brain-computer interface systems, classification of
Electroencephalogram (EEG) signals is the essential part and a challenging
task. Recently, as the marginalized discrete wavelet transform (mDWT)
representations can reveal features related to the transient nature of the EEG
signals, the mDWT coefficients have been frequently used in EEG signal
classification. In our previous work, we have proposed a super-Dirichlet
distribution-based classifier, which utilized the nonnegative and sum-to-one
properties of the mDWT coefficients. The proposed classifier performed better
than the state-of-the-art support vector machine-based classifier. In this
paper, we further study the neutrality of the mDWT coefficients. Assuming the
mDWT vector coefficients to be a neutral vector, we transform them non-linearly
into a set of independent scalar coefficients. Feature selection strategy is
proposed on the transformed feature domain. Experimental results show that the
feature selection strategy helps improving the classification accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00818">
<title>Dirichlet Mixture Model based VQ Performance Prediction for Line Spectral Frequency. (arXiv:1808.00818v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.00818</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we continue our previous work on the Dirichlet mixture model
(DMM)-based VQ to derive the performance bound of the LSF VQ. The LSF
parameters are transformed into the $\Delta$LSF domain and the underlying
distribution of the $\Delta$LSF parameters are modelled by a DMM with finite
number of mixture components. The quantization distortion, in terms of the mean
squared error (MSE), is calculated with the high rate theory. The mapping
relation between the perceptually motivated log spectral distortion (LSD) and
the MSE is empirically approximated by a polynomial. With this mapping
function, the minimum required bit rate for transparent coding of the LSF is
estimated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00831">
<title>Efficient Bayesian Inference of Sigmoidal Gaussian Cox Processes. (arXiv:1808.00831v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.00831</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an approximate Bayesian inference approach for estimating the
intensity of a inhomogeneous Poisson process, where the intensity function is
modelled using a Gaussian process (GP) prior via a sigmoid link function.
Augmenting the model using a latent marked Poisson process and P\&apos;olya--Gamma
random variables we obtain a representation of the likelihood which is
conjugate to the GP prior. We approximate the posterior using a free--form mean
field approximation together with the framework of sparse GPs. Furthermore, as
alternative approximation we suggest a sparse Laplace approximation of the
posterior, for which an efficient expectation--maximisation algorithm is
derived to find the posterior&apos;s mode. Results of both algorithms compare well
with exact inference obtained by a Markov Chain Monte Carlo sampler and
standard variational Gauss approach, while being one order of magnitude faster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Donner_C/0/1/0/all/0/1&quot;&gt;Christian Donner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Opper_M/0/1/0/all/0/1&quot;&gt;Manfred Opper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00845">
<title>RGB Video Based Tennis Action Recognition Using a Deep Weighted Long Short-Term Memory. (arXiv:1808.00845v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.00845</link>
<description rdf:parseType="Literal">&lt;p&gt;Action recognition has attracted increasing attention from RGB input in
computer vision partially due to potential applications on somatic simulation
and statistics of sport such as virtual tennis game and tennis techniques and
tactics analysis by video. Recently, deep learning based methods have achieved
promising performance for action recognition. In this paper, we propose
weighted Long Short-Term Memory adopted with convolutional neural network
representations for three dimensional tennis shots recognition. First, the
local two-dimensional convolutional neural network spatial representations are
extracted from each video frame individually using a pre-trained Inception
network. Then, a weighted Long Short-Term Memory decoder is introduced to take
the output state at time t and the historical embedding feature at time t-1 to
generate feature vector using a score weighting scheme. Finally, we use the
adopted CNN and weighted LSTM to map the original visual features into a vector
space to generate the spatial-temporal semantical description of visual
sequences and classify the action video content. Experiments on the benchmark
demonstrate that our method using only simple raw RGB video can achieve better
performance than the state-of-the-art baselines for tennis shot recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jiaxin Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00892">
<title>Semi-blind source separation with multichannel variational autoencoder. (arXiv:1808.00892v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.00892</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a multichannel source separation method called the
multichannel variational autoencoder (MVAE), which uses VAE to model and
estimate the power spectrograms of the sources in a mixture. The MVAE is
noteworthy in that (1) it takes full advantage of the strong representation
power of deep neural networks for source power spectrogram modeling, (2) the
convergence of the source separation algorithm is guaranteed, and (3) the
criteria for the VAE training and source separation are consistent. Through
experimental evaluations, the MVAE showed higher separation performance than a
baseline method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kameoka_H/0/1/0/all/0/1&quot;&gt;Hirokazu Kameoka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Inoue_S/0/1/0/all/0/1&quot;&gt;Shota Inoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Makino_S/0/1/0/all/0/1&quot;&gt;Shoji Makino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00911">
<title>Detector monitoring with artificial neural networks at the CMS experiment at the CERN Large Hadron Collider. (arXiv:1808.00911v1 [physics.data-an])</title>
<link>http://arxiv.org/abs/1808.00911</link>
<description rdf:parseType="Literal">&lt;p&gt;Reliable data quality monitoring is a key asset in delivering collision data
suitable for physics analysis in any modern large-scale High Energy Physics
experiment. This paper focuses on the use of artificial neural networks for
supervised and semi-supervised problems related to the identification of
anomalies in the data collected by the CMS muon detectors. We use deep neural
networks to analyze LHC collision data, represented as images organized
geographically. We train a classifier capable of detecting the known anomalous
behaviors with unprecedented efficiency and explore the usage of convolutional
autoencoders to extend anomaly detection capabilities to unforeseen failure
modes. A generalization of this strategy could pave the way to the automation
of the data quality assessment process for present and future high-energy
physics experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pol_A/0/1/0/all/0/1&quot;&gt;Adrian Alan Pol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cerminara_G/0/1/0/all/0/1&quot;&gt;Gianluca Cerminara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Germain_C/0/1/0/all/0/1&quot;&gt;Cecile Germain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pierini_M/0/1/0/all/0/1&quot;&gt;Maurizio Pierini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Seth_A/0/1/0/all/0/1&quot;&gt;Agrima Seth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00935">
<title>Inferring Parameters Through Inverse Multiobjective Optimization. (arXiv:1808.00935v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.00935</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a set of human&apos;s decisions that are observed, inverse optimization has
been developed and utilized to infer the underlying decision making problem.
The majority of existing studies assumes that the decision making problem is
with a single objective function, and attributes data divergence to noises,
errors or bounded rationality, which, however, could lead to a corrupted
inference when decisions are tradeoffs among multiple criteria. In this paper,
we take a data-driven approach and design a more sophisticated inverse
optimization formulation to explicitly infer parameters of a multiobjective
decision making problem from noisy observations. This framework, together with
our mathematical analyses and advanced algorithm developments, demonstrates a
strong capacity in estimating critical parameters, decoupling &quot;interpretable&quot;
components from noises or errors, deriving the denoised \emph{optimal}
decisions, and ensuring statistical significance. In particular, for the whole
decision maker population, if suitable conditions hold, we will be able to
understand the overall diversity and the distribution of their preferences over
multiple criteria, which is important when a precise inference on every single
decision maker is practically unnecessary or infeasible. Numerical results on a
large number of experiments are reported to confirm the effectiveness of our
unique inverse optimization model and the computational efficacy of the
developed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chaosheng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeng_B/0/1/0/all/0/1&quot;&gt;Bo Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00342">
<title>Orthogonal Machine Learning: Power and Limitations. (arXiv:1711.00342v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00342</link>
<description rdf:parseType="Literal">&lt;p&gt;Double machine learning provides $\sqrt{n}$-consistent estimates of
parameters of interest even when high-dimensional or nonparametric nuisance
parameters are estimated at an $n^{-1/4}$ rate. The key is to employ
Neyman-orthogonal moment equations which are first-order insensitive to
perturbations in the nuisance parameters. We show that the $n^{-1/4}$
requirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order
notion of orthogonality that grants robustness to more complex or
higher-dimensional nuisance parameters. In the partially linear regression
setting popular in causal inference, we show that we can construct second-order
orthogonal moments if and only if the treatment residual is not normally
distributed. Our proof relies on Stein&apos;s lemma and may be of independent
interest. We conclude by demonstrating the robustness benefits of an explicit
doubly-orthogonal estimation procedure for treatment effect.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zadik_I/0/1/0/all/0/1&quot;&gt;Ilias Zadik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05597">
<title>Advances in Variational Inference. (arXiv:1711.05597v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05597</link>
<description rdf:parseType="Literal">&lt;p&gt;Many modern unsupervised or semi-supervised machine learning algorithms rely
on Bayesian probabilistic models. These models are usually intractable and thus
require approximate inference. Variational inference (VI) lets us approximate a
high-dimensional Bayesian posterior with a simpler variational distribution by
solving an optimization problem. This approach has been successfully used in
various models and large-scale applications. In this review, we give an
overview of recent trends in variational inference. We first introduce standard
mean field variational inference, then review recent advances focusing on the
following aspects: (a) scalable VI, which includes stochastic approximations,
(b) generic VI, which extends the applicability of VI to a large class of
otherwise intractable models, such as non-conjugate models, (c) accurate VI,
which includes variational models beyond the mean field approximation or with
atypical divergences, and (d) amortized VI, which implements the inference over
local latent variables with inference networks. Finally, we provide a summary
of promising future research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Butepage_J/0/1/0/all/0/1&quot;&gt;Judith Butepage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1&quot;&gt;Hedvig Kjellstrom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03987">
<title>Latent Variable Time-varying Network Inference. (arXiv:1802.03987v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03987</link>
<description rdf:parseType="Literal">&lt;p&gt;In many applications of finance, biology and sociology, complex systems
involve entities interacting with each other. These processes have the
peculiarity of evolving over time and of comprising latent factors, which
influence the system without being explicitly measured. In this work we present
latent variable time-varying graphical lasso (LTGL), a method for multivariate
time-series graphical modelling that considers the influence of hidden or
unmeasurable factors. The estimation of the contribution of the latent factors
is embedded in the model which produces both sparse and low-rank components for
each time point. In particular, the first component represents the connectivity
structure of observable variables of the system, while the second represents
the influence of hidden factors, assumed to be few with respect to the observed
variables. Our model includes temporal consistency on both components,
providing an accurate evolutionary pattern of the system. We derive a tractable
optimisation algorithm based on alternating direction method of multipliers,
and develop a scalable and efficient implementation which exploits proximity
operators in closed form. LTGL is extensively validated on synthetic data,
achieving optimal performance in terms of accuracy, structure learning and
scalability with respect to ground truth and state-of-the-art methods for
graphical inference. We conclude with the application of LTGL to real case
studies, from biology and finance, to illustrate how our method can be
successfully employed to gain insights on multivariate time-series data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tomasi_F/0/1/0/all/0/1&quot;&gt;Federico Tomasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tozzo_V/0/1/0/all/0/1&quot;&gt;Veronica Tozzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salzo_S/0/1/0/all/0/1&quot;&gt;Saverio Salzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Verri_A/0/1/0/all/0/1&quot;&gt;Alessandro Verri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08898">
<title>Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo. (arXiv:1802.08898v4 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08898</link>
<description rdf:parseType="Literal">&lt;p&gt;Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from
high-dimensional distributions in Statistics and Machine learning. HMC is known
to run very efficiently in practice and its popular second-order &quot;leapfrog&quot;
implementation has long been conjectured to run in $d^{1/4}$ gradient
evaluations. Here we show that this conjecture is true when sampling from
strongly log-concave target distributions that satisfy a weak third-order
regularity property associated with the input data. Our regularity condition is
weaker than the Lipschitz Hessian property and allows us to show faster
convergence bounds for a much larger class of distributions than would be
possible with the usual Lipschitz Hessian constant alone. Important
distributions that satisfy our regularity condition include posterior
distributions used in Bayesian logistic regression for which the data satisfies
an &quot;incoherence&quot; property. Our result compares favorably with the best
available bounds for the class of strongly log-concave distributions, which
grow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, our
simulations on synthetic data suggest that, when our regularity condition is
satisfied, leapfrog HMC performs better than its competitors -- both in terms
of accuracy and in terms of the number of gradient evaluations it requires.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mangoubi_O/0/1/0/all/0/1&quot;&gt;Oren Mangoubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1&quot;&gt;Nisheeth K. Vishnoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04497">
<title>Automated software vulnerability detection with machine learning. (arXiv:1803.04497v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04497</link>
<description rdf:parseType="Literal">&lt;p&gt;Thousands of security vulnerabilities are discovered in production software
each year, either reported publicly to the Common Vulnerabilities and Exposures
database or discovered internally in proprietary code. Vulnerabilities often
manifest themselves in subtle ways that are not obvious to code reviewers or
the developers themselves. With the wealth of open source code available for
analysis, there is an opportunity to learn the patterns of bugs that can lead
to security vulnerabilities directly from data. In this paper, we present a
data-driven approach to vulnerability detection using machine learning,
specifically applied to C and C++ programs. We first compile a large dataset of
hundreds of thousands of open-source functions labeled with the outputs of a
static analyzer. We then compare methods applied directly to source code with
methods applied to artifacts extracted from the build process, finding that
source-based models perform better. We also compare the application of deep
neural network models with more traditional models such as random forests and
find the best performance comes from combining features learned by deep models
with tree-based models. Ultimately, our highest performing model achieves an
area under the precision-recall curve of 0.49 and an area under the ROC curve
of 0.87.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harer_J/0/1/0/all/0/1&quot;&gt;Jacob A. Harer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_L/0/1/0/all/0/1&quot;&gt;Louis Y. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_R/0/1/0/all/0/1&quot;&gt;Rebecca L. Russell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozdemir_O/0/1/0/all/0/1&quot;&gt;Onur Ozdemir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosta_L/0/1/0/all/0/1&quot;&gt;Leonard R. Kosta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rangamani_A/0/1/0/all/0/1&quot;&gt;Akshay Rangamani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamilton_L/0/1/0/all/0/1&quot;&gt;Lei H. Hamilton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Centeno_G/0/1/0/all/0/1&quot;&gt;Gabriel I. Centeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Key_J/0/1/0/all/0/1&quot;&gt;Jonathan R. Key&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellingwood_P/0/1/0/all/0/1&quot;&gt;Paul M. Ellingwood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antelman_E/0/1/0/all/0/1&quot;&gt;Erik Antelman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mackay_A/0/1/0/all/0/1&quot;&gt;Alan Mackay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McConley_M/0/1/0/all/0/1&quot;&gt;Marc W. McConley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Opper_J/0/1/0/all/0/1&quot;&gt;Jeffrey M. Opper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chin_P/0/1/0/all/0/1&quot;&gt;Peter Chin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazovich_T/0/1/0/all/0/1&quot;&gt;Tomo Lazovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09730">
<title>Resilient Active Information Gathering with Mobile Robots. (arXiv:1803.09730v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09730</link>
<description rdf:parseType="Literal">&lt;p&gt;Applications of safety, security, and rescue in robotics, such as multi-robot
target tracking, involve the execution of information acquisition tasks by
teams of mobile robots. However, in failure-prone or adversarial environments,
robots get attacked, their communication channels get jammed, and their sensors
may fail, resulting in the withdrawal of robots from the collective task, and
consequently the inability of the remaining active robots to coordinate with
each other. As a result, traditional design paradigms become insufficient and,
in contrast, resilient designs against system-wide failures and attacks become
important. In general, resilient design problems are hard, and even though they
often involve objective functions that are monotone or submodular, scalable
approximation algorithms for their solution have been hitherto unknown. In this
paper, we provide the first algorithm, enabling the following capabilities:
minimal communication, i.e., the algorithm is executed by the robots based only
on minimal communication between them; system-wide resiliency, i.e., the
algorithm is valid for any number of denial-of-service attacks and failures;
and provable approximation performance, i.e., the algorithm ensures for all
monotone (and not necessarily submodular) objective functions a solution that
is finitely close to the optimal. We quantify our algorithm&apos;s approximation
performance using a notion of curvature for monotone set functions. We support
our theoretical analyses with simulated and real-world experiments, by
considering an active information gathering scenario, namely, multi-robot
target tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlotfeldt_B/0/1/0/all/0/1&quot;&gt;Brent Schlotfeldt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzoumas_V/0/1/0/all/0/1&quot;&gt;Vasileios Tzoumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakur_D/0/1/0/all/0/1&quot;&gt;Dinesh Thakur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1&quot;&gt;George J. Pappas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11811">
<title>Stochastic Zeroth-order Optimization via Variance Reduction method. (arXiv:1805.11811v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11811</link>
<description rdf:parseType="Literal">&lt;p&gt;Derivative-free optimization has become an important technique used in
machine learning for optimizing black-box models. To conduct updates without
explicitly computing gradient, most current approaches iteratively sample a
random search direction from Gaussian distribution and compute the estimated
gradient along that direction. However, due to the variance in the search
direction, the convergence rates and query complexities of existing methods
suffer from a factor of $d$, where $d$ is the problem dimension. In this paper,
we introduce a novel Stochastic Zeroth-order method with Variance Reduction
under Gaussian smoothing (SZVR-G) and establish the complexity for optimizing
non-convex problems. With variance reduction on both sample space and search
space, the complexity of our algorithm is sublinear to $d$ and is strictly
better than current approaches, in both smooth and non-smooth cases. Moreover,
we extend the proposed method to the mini-batch version. Our experimental
results demonstrate the superior performance of the proposed method over
existing derivative-free optimization techniques. Furthermore, we successfully
apply our method to conduct a universal black-box attack to deep neural
networks and present some interesting results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Minhao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04489">
<title>Fast yet Simple Natural-Gradient Descent for Variational Inference in Complex Models. (arXiv:1807.04489v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.04489</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian inference plays an important role in advancing machine learning, but
faces computational challenges when applied to complex models such as deep
neural networks. Variational inference circumvents these challenges by
formulating Bayesian inference as an optimization problem and solving it using
gradient-based optimization. In this paper, we argue in favor of
natural-gradient approaches which, unlike their gradient-based counterparts,
can improve convergence by exploiting the information geometry of the
solutions. We show how to derive fast yet simple natural-gradient updates by
using a duality associated with exponential-family distributions. An attractive
feature of these methods is that, by using natural-gradients, they are able to
extract accurate local approximations for individual model components. We
summarize recent results for Bayesian deep learning showing the superiority of
natural-gradient approaches over their gradient counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nielsen_D/0/1/0/all/0/1&quot;&gt;Didrik Nielsen&lt;/a&gt;</dc:creator>
</item></rdf:RDF>