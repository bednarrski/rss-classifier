<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01883"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02112"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.02345"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00970"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03166"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06541"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08889"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01984"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02027"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02056"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02137"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02239"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02242"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02256"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02281"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.03661"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.07608"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04205"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07779"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01844"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01851"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01856"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01879"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01896"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01899"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01947"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01949"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02032"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02046"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02071"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02121"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02136"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02146"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02185"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02199"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02246"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02252"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02261"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02282"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02300"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02315"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02321"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.03389"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04212"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07742"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00673"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02124"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04153"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04687"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04715"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04907"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05249"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05983"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07481"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09086"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01420"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09539"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11485"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09699"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11326"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05383"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00416"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01047"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01260"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01471"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.04464"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00894"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.01883">
<title>Routes to Open-Endedness in Evolutionary Systems. (arXiv:1806.01883v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.01883</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a high-level conceptual framework to help orient the
discussion and implementation of open-endedness in evolutionary systems.
Drawing upon earlier work by Banzhaf et al., three different kinds of
open-endedness are identified: exploratory, expansive, and transformational.
These are characterised in terms of their relationship to the search space of
phenotypic behaviours. A formalism is introduced to describe three key
processes required for an evolutionary process: the generation of a phenotype
from a genetic description, the evaluation of that phenotype, and the
reproduction with variation of individuals according to their evaluation. The
formalism makes explicit various influences in each of these processes that can
easily be overlooked. The distinction is made between intrinsic and extrinsic
implementations of these processes. A discussion then investigates how various
interactions between these processes, and their modes of implementation, can
lead to open-endedness. However, it is demonstrated that these considerations
relate to exploratory open-endedness only. Conditions for the implementation of
the more interesting kinds of open-endedness - expansive and transformational -
are also discussed, emphasizing factors such as multiple domains of behaviour,
transdomain bridges, and non-additive compositional systems. In contrast to a
traditional &quot;neo-Darwinian&quot; analysis, these factors relate not to the generic
evolutionary properties of individuals, but rather to the nature of the
building blocks out of which individual organisms are constructed, and the laws
and properties of the environment in which they exist. The paper ends with
suggestions of how the framework can be used to categorise and compare the
open-ended evolutionary potential of different systems, and how it might guide
the design of systems with greater capacity for open-ended evolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_T/0/1/0/all/0/1&quot;&gt;Tim Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02112">
<title>Bounding Bloat in Genetic Programming. (arXiv:1806.02112v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.02112</link>
<description rdf:parseType="Literal">&lt;p&gt;While many optimization problems work with a fixed number of decision
variables and thus a fixed-length representation of possible solutions, genetic
programming (GP) works on variable-length representations. A naturally
occurring problem is that of bloat (unnecessary growth of solutions) slowing
down optimization. Theoretical analyses could so far not bound bloat and
required explicit assumptions on the magnitude of bloat. In this paper we
analyze bloat in mutation-based genetic programming for the two test functions
ORDER and MAJORITY. We overcome previous assumptions on the magnitude of bloat
and give matching or close-to-matching upper and lower bounds for the expected
optimization time. In particular, we show that the (1+1) GP takes (i)
$\Theta(T_{init} + n \log n)$ iterations with bloat control on ORDER as well as
MAJORITY; and (ii) $O(T_{init} \log T_{init} + n (\log n)^3)$ and
$\Omega(T_{init} + n \log n)$ (and $\Omega(T_{init} \log T_{init})$ for $n=1$)
iterations without bloat control on MAJORITY.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotzing_T/0/1/0/all/0/1&quot;&gt;Timo K&amp;#xf6;tzing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lagodzinski_J/0/1/0/all/0/1&quot;&gt;J. A. Gregor Lagodzinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lengler_J/0/1/0/all/0/1&quot;&gt;Johannes Lengler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.02345">
<title>Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks. (arXiv:1611.02345v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1611.02345</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern convolutional networks, incorporating rectifiers and max-pooling, are
neither smooth nor convex; standard guarantees therefore do not apply.
Nevertheless, methods from convex optimization such as gradient descent and
Adam are widely used as building blocks for deep learning algorithms. This
paper provides the first convergence guarantee applicable to modern convnets,
which furthermore matches a lower bound for convex nonsmooth functions. The key
technical tool is the neural Taylor approximation -- a straightforward
application of Taylor expansions to neural networks -- and the associated
Taylor loss. Experiments on a range of optimizers, layers, and tasks provide
evidence that the analysis accurately captures the dynamics of neural
optimization. The second half of the paper applies the Taylor approximation to
isolate the main difficulty in training rectifier nets -- that gradients are
shattered -- and investigates the hypothesis that, by exploring the space of
activation configurations more thoroughly, adaptive optimizers such as RMSProp
and Adam are able to converge to better solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balduzzi_D/0/1/0/all/0/1&quot;&gt;David Balduzzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McWilliams_B/0/1/0/all/0/1&quot;&gt;Brian McWilliams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Butler_Yeoman_T/0/1/0/all/0/1&quot;&gt;Tony Butler-Yeoman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00970">
<title>A Classification-Based Study of Covariate Shift in GAN Distributions. (arXiv:1711.00970v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00970</link>
<description rdf:parseType="Literal">&lt;p&gt;A basic, and still largely unanswered, question in the context of Generative
Adversarial Networks (GANs) is whether they are truly able to capture all the
fundamental characteristics of the distributions they are trained on. In
particular, evaluating the diversity of GAN distributions is challenging and
existing methods provide only a partial understanding of this issue. In this
paper, we develop quantitative and scalable tools for assessing the diversity
of GAN distributions. Specifically, we take a classification-based perspective
and view loss of diversity as a form of covariate shift introduced by GANs. We
examine two specific forms of such shift: mode collapse and boundary
distortion. In contrast to prior work, our methods need only minimal human
supervision and can be readily applied to state-of-the-art GANs on large,
canonical datasets. Examining popular GANs using our tools indicates that these
GANs have significant problems in reproducing the more distributional
properties of their training dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santurkar_S/0/1/0/all/0/1&quot;&gt;Shibani Santurkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1&quot;&gt;Aleksander M&amp;#x105;dry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03166">
<title>Simplex Search Based Brain Storm Optimization. (arXiv:1712.03166v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1712.03166</link>
<description rdf:parseType="Literal">&lt;p&gt;Through modeling human&apos;s brainstorming process, the brain storm optimization
(BSO) algorithm has become a promising population-based evolutionary algorithm.
However, BSO is pointed out that it possesses a degenerated L-curve phenomenon,
i.e., it often gets near optimum quickly but needs much more cost to improve
the accuracy. To overcome this question in this paper, an excellent direct
search based local solver, the Nelder-Mead Simplex (NMS) method is adopted in
BSO. Through combining BSO&apos;s exploration ability and NMS&apos;s exploitation ability
together, a simplex search based BSO (Simplex-BSO) is developed via a better
balance between global exploration and local exploitation. Simplex-BSO is shown
to be able to eliminate the degenerated L-curve phenomenon on unimodal
functions, and alleviate significantly this phenomenon on multimodal functions.
Large number of experimental results show that Simplex-BSO is a promising
algorithm for global optimization problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;YingYing Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Shi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qunfeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06541">
<title>Size-Independent Sample Complexity of Neural Networks. (arXiv:1712.06541v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06541</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the sample complexity of learning neural networks, by providing new
bounds on their Rademacher complexity assuming norm constraints on the
parameter matrix of each layer. Compared to previous work, these complexity
bounds have improved dependence on the network depth, and under some additional
assumptions, are fully independent of the network size (both depth and width).
These results are derived using some novel techniques, which may be of
independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golowich_N/0/1/0/all/0/1&quot;&gt;Noah Golowich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakhlin_A/0/1/0/all/0/1&quot;&gt;Alexander Rakhlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1&quot;&gt;Ohad Shamir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08889">
<title>Spiking Linear Dynamical Systems on Neuromorphic Hardware for Low-Power Brain-Machine Interfaces. (arXiv:1805.08889v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08889</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuromorphic architectures achieve low-power operation by using many simple
spiking neurons in lieu of traditional hardware. Here, we develop methods for
precise linear computations in spiking neural networks and use these methods to
map the evolution of a linear dynamical system (LDS) onto an existing
neuromorphic chip: IBM&apos;s TrueNorth. We analytically characterize, and
numerically validate, the discrepancy between the spiking LDS state sequence
and that of its non-spiking counterpart. These analytical results shed light on
the multiway tradeoff between time, space, energy, and accuracy in neuromorphic
computation. To demonstrate the utility of our work, we implemented a
neuromorphic Kalman filter (KF) and used it for offline decoding of human vocal
pitch from neural data. The neuromorphic KF could be used for low-power
filtering in domains beyond neuroscience, such as navigation or robotics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_D/0/1/0/all/0/1&quot;&gt;David G. Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Livezey_J/0/1/0/all/0/1&quot;&gt;Jesse A. Livezey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1&quot;&gt;Edward F. Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1&quot;&gt;Kristofer E. Bouchard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01984">
<title>Learning to rank for censored survival data. (arXiv:1806.01984v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.01984</link>
<description rdf:parseType="Literal">&lt;p&gt;Survival analysis is a type of semi-supervised ranking task where the target
output (the survival time) is often right-censored. Utilizing this information
is a challenge because it is not obvious how to correctly incorporate these
censored examples into a model. We study how three categories of loss
functions, namely partial likelihood methods, rank methods, and our
classification method based on a Wasserstein metric (WM) and the non-parametric
Kaplan Meier estimate of the probability density to impute the labels of
censored examples, can take advantage of this information. The proposed method
allows us to have a model that predict the probability distribution of an
event. If a clinician had access to the detailed probability of an event over
time this would help in treatment planning. For example, determining if the
risk of kidney graft rejection is constant or peaked after some time. Also, we
demonstrate that this approach directly optimizes the expected C-index which is
the most common evaluation metric for ranking survival models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luck_M/0/1/0/all/0/1&quot;&gt;Margaux Luck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sylvain_T/0/1/0/all/0/1&quot;&gt;Tristan Sylvain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1&quot;&gt;Joseph Paul Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardinal_H/0/1/0/all/0/1&quot;&gt;Heloise Cardinal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lodi_A/0/1/0/all/0/1&quot;&gt;Andrea Lodi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02027">
<title>On Discrete-Continuous Mixtures in Probabilistic Programming: Generalized Semantics and Inference Algorithms. (arXiv:1806.02027v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.02027</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite of the recent successes of probabilistic programming languages (PPLs)
in AI applications, PPLs offer only limited support for discrete-continuous
mixture random variables. We develop the notion of measure-theoretic Bayesian
networks (MTBNs), and use it to provide more general semantics for PPLs with
arbitrarily many random variables defined over arbitrary measure spaces. We
develop two new general sampling algorithms which are provably correct under
the MTBN framework: lexicographic likelihood weighting (LLW) for general MTBNs
and lexicographic particle filter (LPF), a specialized algorithm for state
space models. We further integrate MTBN into a widely used PPL system, BLOG,
and verify the effectiveness of our new inference algorithms through
representative examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1&quot;&gt;Siddharth Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hay_N/0/1/0/all/0/1&quot;&gt;Nicholas Hay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1&quot;&gt;Stuart Russell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02056">
<title>Learning Hierarchical Item Categories from Implicit Feedback Data for Efficient Recommendations and Browsing. (arXiv:1806.02056v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1806.02056</link>
<description rdf:parseType="Literal">&lt;p&gt;Searching, browsing, and recommendations are common ways in which the &quot;choice
overload&quot; faced by users in the online marketplace can be mitigated. In this
paper we propose the use of hierarchical item categories, obtained from
implicit feedback data, to enable efficient browsing and recommendations. We
present a method of creating hierarchical item categories from implicit
feedback data only i.e., without any other information on the items like name,
genre etc. Categories created in this fashion are based on users&apos;
co-consumption of items. Thus, they can be more useful for users in finding
interesting and relevant items while they are browsing through the hierarchy.
We also show that this item hierarchy can be useful in making category based
recommendations, which makes the recommendations more explainable and increases
the diversity of the recommendations without compromising much on the accuracy.
Item hierarchy can also be useful in the creation of an automatic item taxonomy
skeleton by bypassing manual labeling and annotation. This can especially be
useful for small vendors. Our data-driven hierarchical categories are based on
hierarchical latent tree analysis (HLTA) which has been previously used for
text analysis. We present a scaled up learning algorithm \emph{HLTA-Forest} so
that HLTA can be applied to implicit feedback data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khawar_F/0/1/0/all/0/1&quot;&gt;Farhan Khawar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Nevin L. Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02137">
<title>A New Framework for Machine Intelligence: Concepts and Prototype. (arXiv:1806.02137v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.02137</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) and artificial intelligence (AI) have become hot topics
in many information processing areas, from chatbots to scientific data
analysis. At the same time, there is uncertainty about the possibility of
extending predominant ML technologies to become general solutions with
continuous learning capabilities. Here, a simple, yet comprehensive,
theoretical framework for intelligent systems is presented. A combination of
Mirror Compositional Representations (MCR) and a Solution-Critic Loop (SCL) is
proposed as a generic approach for different types of problems. A prototype
implementation is presented for document comparison using English Wikipedia
corpus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montoya_A/0/1/0/all/0/1&quot;&gt;Abel Torres Montoya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02239">
<title>Constrained Counting and Sampling: Bridging the Gap between Theory and Practice. (arXiv:1806.02239v1 [cs.LO])</title>
<link>http://arxiv.org/abs/1806.02239</link>
<description rdf:parseType="Literal">&lt;p&gt;Constrained counting and sampling are two fundamental problems in Computer
Science with numerous applications, including network reliability, privacy,
probabilistic reasoning, and constrained-random verification. In constrained
counting, the task is to compute the total weight, subject to a given weighting
function, of the set of solutions of the given constraints. In constrained
sampling, the task is to sample randomly, subject to a given weighting
function, from the set of solutions to a set of given constraints.
Consequently, constrained counting and sampling have been subject to intense
theoretical and empirical investigations over the years. Prior work, however,
offered either heuristic techniques with poor guarantees of accuracy or
approaches with proven guarantees but poor performance in practice.
&lt;/p&gt;
&lt;p&gt;In this thesis, we introduce a novel hashing-based algorithmic framework for
constrained sampling and counting that combines the classical algorithmic
technique of universal hashing with the dramatic progress made in combinatorial
reasoning tools, in particular, SAT and SMT, over the past two decades. The
resulting frameworks for counting (ApproxMC2) and sampling (UniGen) can handle
formulas with up to million variables representing a significant boost up from
the prior state of the art tools&apos; capability to handle few hundreds of
variables. If the initial set of constraints is expressed as Disjunctive Normal
Form (DNF), ApproxMC is the only known Fully Polynomial Randomized
Approximation Scheme (FPRAS) that does not involve Monte Carlo steps. By
exploiting the connection between definability of formulas and variance of the
distribution of solutions in a cell defined by 3-universal hash functions, we
introduced an algorithmic technique, MIS, that reduced the size of XOR
constraints employed in the underlying universal hash functions by as much as
two orders of magnitude.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meel_K/0/1/0/all/0/1&quot;&gt;Kuldeep S. Meel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02242">
<title>Extraction Of Technical Information From Normative Documents Using Automated Methods Based On Ontologies : Application To The Iso 15531 Mandate Standard - Methodology And First Results. (arXiv:1806.02242v1 [cs.SE])</title>
<link>http://arxiv.org/abs/1806.02242</link>
<description rdf:parseType="Literal">&lt;p&gt;Problems faced by international standardization bodies become more and more
crucial as the number and the size of the standards they produce increase.
Sometimes, also, the lack of coordination among the committees in charge of the
development of standards may lead to overlaps, mistakes or incompatibilities in
the documents. The aim of this study is to present a methodology enabling an
automatic extraction of the technical concepts (terms) found in normative
documents, through the use of semantic tools coming from the field of language
processing. The first part of the paper provides a description of the
standardization world, its structure, its way of working and the problems
faced; we then introduce the concepts of semantic annotation, information
extraction and the software tools available in this domain. The next section
explains the concept of ontology and its potential use in the field of
standardization. We propose here a methodology enabling the extraction of
technical information from a given normative corpus, based on a semantic
annotation process done according to reference ontologies. The application to
the ISO 15531 MANDATE corpus provides a first use case of the methodology
described in this paper. The paper ends with the description of the first
experimental results produced by this approach, and with some issues and
perspectives, notably its application to other standards and, or Technical
Committees and the possibility offered to create pre-defined technical
dictionaries of terms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cutting_Decelle_A/0/1/0/all/0/1&quot;&gt;A.F. Cutting-Decelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Digeon_A/0/1/0/all/0/1&quot;&gt;A. Digeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_R/0/1/0/all/0/1&quot;&gt;R.I. Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barraud_J/0/1/0/all/0/1&quot;&gt;J.L. Barraud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamboley_P/0/1/0/all/0/1&quot;&gt;P. Lamboley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02256">
<title>Adversarial Regression with Multiple Learners. (arXiv:1806.02256v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02256</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the considerable success enjoyed by machine learning techniques in
practice, numerous studies demonstrated that many approaches are vulnerable to
attacks. An important class of such attacks involves adversaries changing
features at test time to cause incorrect predictions. Previous investigations
of this problem pit a single learner against an adversary. However, in many
situations an adversary&apos;s decision is aimed at a collection of learners, rather
than specifically targeted at each independently. We study the problem of
adversarial linear regression with multiple learners. We approximate the
resulting game by exhibiting an upper bound on learner loss functions, and show
that the resulting game has a unique symmetric equilibrium. We present an
algorithm for computing this equilibrium, and show through extensive
experiments that equilibrium models are significantly more robust than
conventional regularized linear regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1&quot;&gt;Liang Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Sixie Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alfeld_S/0/1/0/all/0/1&quot;&gt;Scott Alfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1&quot;&gt;Yevgeniy Vorobeychik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02281">
<title>Deploying Deep Ranking Models for Search Verticals. (arXiv:1806.02281v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1806.02281</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present an architecture executing a complex machine
learning model such as a neural network capturing semantic similarity between a
query and a document; and deploy to a real-world production system serving
500M+users. We present the challenges that arise in a real-world system and how
we solve them. We demonstrate that our architecture provides competitive
modeling capability without any significant performance impact to the system in
terms of latency. Our modular solution and insights can be used by other
real-world search systems to realize and productionize recent gains in neural
networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanath_R/0/1/0/all/0/1&quot;&gt;Rohan Ramanath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polatkan_G/0/1/0/all/0/1&quot;&gt;Gungor Polatkan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Liqin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Harold Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.03661">
<title>Learning Representations for Counterfactual Inference. (arXiv:1605.03661v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1605.03661</link>
<description rdf:parseType="Literal">&lt;p&gt;Observational studies are rising in importance due to the widespread
accumulation of data in fields such as healthcare, education, employment and
ecology. We consider the task of answering counterfactual questions such as,
&quot;Would this patient have lower blood sugar had she received a different
medication?&quot;. We propose a new algorithmic framework for counterfactual
inference which brings together ideas from domain adaptation and representation
learning. In addition to a theoretical justification, we perform an empirical
comparison with previous approaches to causal inference from observational
data. Our deep learning algorithm significantly outperforms the previous
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Johansson_F/0/1/0/all/0/1&quot;&gt;Fredrik D. Johansson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shalit_U/0/1/0/all/0/1&quot;&gt;Uri Shalit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sontag_D/0/1/0/all/0/1&quot;&gt;David Sontag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.07608">
<title>Deep Exploration via Randomized Value Functions. (arXiv:1703.07608v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.07608</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the use of randomized value functions to guide deep exploration in
reinforcement learning. This offers an elegant means for synthesizing
statistically and computationally efficient exploration with common practical
approaches to value function learning. We present several reinforcement
learning algorithms that leverage randomized value functions and demonstrate
their efficacy through computational studies. We also prove a regret bound that
establishes statistical efficiency with a tabular representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osband_I/0/1/0/all/0/1&quot;&gt;Ian Osband&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roy_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Russo_D/0/1/0/all/0/1&quot;&gt;Daniel Russo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wen_Z/0/1/0/all/0/1&quot;&gt;Zheng Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04205">
<title>Efficient Hierarchical Robot Motion Planning Under Uncertainty and Hybrid Dynamics. (arXiv:1802.04205v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04205</link>
<description rdf:parseType="Literal">&lt;p&gt;Noisy observations coupled with nonlinear dynamics pose one of the biggest
challenges in robot motion planning. By decomposing the nonlinear dynamics into
a discrete set of local dynamics models, hybrid dynamics provide a natural way
to model nonlinear dynamics, especially in systems with sudden &quot;jumps&quot; in the
dynamics, due to factors such as contacts. We propose a hierarchical POMDP
planner that develops locally optimal motion plans for hybrid dynamics models.
The hierarchical planner first develops a high-level motion plan to sequence
the local dynamics models to be visited. The high-level plan is then converted
into a detailed cost-optimized continuous state plan. This hierarchical
planning approach results in a decomposition of the POMDP planning problem into
smaller sub-parts that can be solved with significantly lower computational
costs. The ability to sequence the visitation of local dynamics models also
provides a powerful way to leverage the hybrid dynamics to reduce state
uncertainty. We evaluate the proposed planner for two navigation and
localization tasks in simulated domains, as well as an assembly task with a
real robotic manipulator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Ajinkya Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1&quot;&gt;Scott Niekum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07779">
<title>PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making. (arXiv:1804.07779v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07779</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning and symbolic planning have both been used to build
intelligent autonomous agents. Reinforcement learning relies on learning from
interactions with real world, which often requires an unfeasibly large amount
of experience. Symbolic planning relies on manually crafted symbolic knowledge,
which may not be robust to domain uncertainties and changes. In this paper we
present a unified framework {\em PEORL} that integrates symbolic planning with
hierarchical reinforcement learning (HRL) to cope with decision-making in a
dynamic environment with uncertainties.
&lt;/p&gt;
&lt;p&gt;Symbolic plans are used to guide the agent&apos;s task execution and learning, and
the learned experience is fed back to symbolic knowledge to improve planning.
This method leads to rapid policy search and robust symbolic plans in complex
domains. The framework is tested on benchmark domains of HRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fangkai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_D/0/1/0/all/0/1&quot;&gt;Daoming Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gustafson_S/0/1/0/all/0/1&quot;&gt;Steven Gustafson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01844">
<title>SBAF: A New Activation Function for Artificial Neural Net based Habitability Classification. (arXiv:1806.01844v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.01844</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the efficacy of using a novel activation function in Artificial
Neural Networks (ANN) in characterizing exoplanets into different classes. We
call this Saha-Bora Activation Function (SBAF) as the motivation is derived
from long standing understanding of using advanced calculus in modeling
habitability score of Exoplanets. The function is demonstrated to possess nice
analytical properties and doesn&apos;t seem to suffer from local oscillation
problems. The manuscript presents the analytical properties of the activation
function and the architecture implemented on the function. Keywords:
Astroinformatics, Machine Learning, Exoplanets, ANN, Activation Function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Snehanshu Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1&quot;&gt;Archana Mathur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bora_K/0/1/0/all/0/1&quot;&gt;Kakoli Bora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1&quot;&gt;Surbhi Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basak_S/0/1/0/all/0/1&quot;&gt;Suryoday Basak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01851">
<title>Pathwise Derivatives Beyond the Reparameterization Trick. (arXiv:1806.01851v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.01851</link>
<description rdf:parseType="Literal">&lt;p&gt;We observe that gradients computed via the reparameterization trick are in
direct correspondence with solutions of the transport equation in the formalism
of optimal transport. We use this perspective to compute (approximate) pathwise
gradients for probability distributions not directly amenable to the
reparameterization trick: Gamma, Beta, and Dirichlet. We further observe that
when the reparameterization trick is applied to the Cholesky-factorized
multivariate Normal distribution, the resulting gradients are suboptimal in the
sense of optimal transport. We derive the optimal gradients and show that they
have reduced variance in a Gaussian Process regression task. We demonstrate
with a variety of synthetic experiments and stochastic variational inference
tasks that our pathwise gradients are competitive with other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jankowiak_M/0/1/0/all/0/1&quot;&gt;Martin Jankowiak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Obermeyer_F/0/1/0/all/0/1&quot;&gt;Fritz Obermeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01856">
<title>Pathwise Derivatives for Multivariate Distributions. (arXiv:1806.01856v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.01856</link>
<description rdf:parseType="Literal">&lt;p&gt;We exploit the link between the transport equation and derivatives of
expectations to construct efficient pathwise gradient estimators for
multivariate distributions. We focus on two main threads. First, we use null
solutions of the transport equation to construct adaptive control variates that
can be used to construct gradient estimators with reduced variance. Second, we
consider the case of multivariate mixture distributions. In particular we show
how to compute pathwise derivatives for mixtures of multivariate Normal
distributions with arbitrary means and diagonal covariances. We demonstrate in
a variety of experiments in the context of variational inference that our
gradient estimators can outperform other methods, especially in high
dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jankowiak_M/0/1/0/all/0/1&quot;&gt;Martin Jankowiak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karaletsos_T/0/1/0/all/0/1&quot;&gt;Theofanis Karaletsos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01879">
<title>An explicit analysis of the entropic penalty in linear programming. (arXiv:1806.01879v1 [math.OC])</title>
<link>http://arxiv.org/abs/1806.01879</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving linear programs by using entropic penalization has recently attracted
new interest in the optimization community, since this strategy forms the basis
for the fastest-known algorithms for the optimal transport problem, with many
applications in modern large-scale machine learning. Crucial to these
applications has been an analysis of how quickly solutions to the penalized
program approach true optima to the original linear program. More than 20 years
ago, Cominetti and San Mart\&apos;in showed that this convergence is exponentially
fast; however, their proof is asymptotic and does not give any indication of
how accurately the entropic program approximates the original program for any
particular choice of the penalization parameter. We close this long-standing
gap in the literature regarding entropic penalization by giving a new proof of
the exponential convergence, valid for any linear program. Our proof is
non-asymptotic, yields explicit constants, and has the virtue of being
extremely simple. We provide matching lower bounds and show that the entropic
approach does not lead to a near-linear time approximation scheme for the
linear assignment problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Weed_J/0/1/0/all/0/1&quot;&gt;Jonathan Weed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01896">
<title>Performance Evaluation of Deep Learning Networks for Semantic Segmentation of Traffic Stereo-Pair Images. (arXiv:1806.01896v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.01896</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic image segmentation is one the most demanding task, especially for
analysis of traffic conditions for self-driving cars. Here the results of
application of several deep learning architectures (PSPNet and ICNet) for
semantic image segmentation of traffic stereo-pair images are presented. The
images from Cityscapes dataset and custom urban images were analyzed as to the
segmentation accuracy and image inference time. For the models pre-trained on
Cityscapes dataset, the inference time was equal in the limits of standard
deviation, but the segmentation accuracy was different for various cities and
stereo channels even. The distributions of accuracy (mean intersection over
union - mIoU) values for each city and channel are asymmetric, long-tailed, and
have many extreme outliers, especially for PSPNet network in comparison to
ICNet network. Some statistical properties of these distributions (skewness,
kurtosis) allow us to distinguish these two networks and open the question
about relations between architecture of deep learning networks and statistical
distribution of the predicted results (mIoU here). The results obtained
demonstrated the different sensitivity of these networks to: (1) the local
street view peculiarities in different cities that should be taken into account
during the targeted fine tuning the models before their practical applications,
(2) the right and left data channels in stereo-pairs. For both networks, the
difference in the predicted results (mIoU here) for the right and left data
channels in stereo-pairs is out of the limits of statistical error in relation
to mIoU values. It means that the traffic stereo pairs can be effectively used
not only for depth calculations (as it is usually used), but also as an
additional data channel that can provide much more information about scene
objects than simple duplication of the same street view images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taran_V/0/1/0/all/0/1&quot;&gt;Vlad Taran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordienko_N/0/1/0/all/0/1&quot;&gt;Nikita Gordienko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochura_Y/0/1/0/all/0/1&quot;&gt;Yuriy Kochura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordienko_Y/0/1/0/all/0/1&quot;&gt;Yuri Gordienko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rokovyi_A/0/1/0/all/0/1&quot;&gt;Alexandr Rokovyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alienin_O/0/1/0/all/0/1&quot;&gt;Oleg Alienin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stirenko_S/0/1/0/all/0/1&quot;&gt;Sergii Stirenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01899">
<title>MRPC: An R package for accurate inference of causal graphs. (arXiv:1806.01899v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.01899</link>
<description rdf:parseType="Literal">&lt;p&gt;We present MRPC, an R package that learns causal graphs with improved
accuracy over existing packages, such as pcalg and bnlearn. Our algorithm
builds on the powerful PC algorithm, the canonical algorithm in computer
science for learning directed acyclic graphs. The improvement in accuracy
results from online control of the false discovery rate (FDR) that reduces
false positive edges, a more accurate approach to identifying v-structures
(i.e., $T_1 \rightarrow T_2 \leftarrow T_3$), and robust estimation of the
correlation matrix among nodes. For genomic data that contain genotypes and
gene expression for each sample, MRPC incorporates the principle of Mendelian
randomization to orient the edges. Our package can be applied to continuous and
discrete data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Badsha_M/0/1/0/all/0/1&quot;&gt;Md. Bahadur Badsha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Martin_E/0/1/0/all/0/1&quot;&gt;Evan A Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fu_A/0/1/0/all/0/1&quot;&gt;Audrey Qiuyan Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01947">
<title>A linear time method for the detection of point and collective anomalies. (arXiv:1806.01947v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.01947</link>
<description rdf:parseType="Literal">&lt;p&gt;The challenge of efficiently identifying anomalies in data sequences is an
important statistical problem that now arises in many applications. Whilst
there has been substantial work aimed at making statistical analyses robust to
outliers, or point anomalies, there has been much less work on detecting
anomalous segments, or collective anomalies. By bringing together ideas from
changepoint detection and robust statistics, we introduce Collective And Point
Anomalies (CAPA), a computationally efficient approach that is suitable when
collective anomalies are characterised by either a change in mean, variance, or
both, and distinguishes them from point anomalies. Theoretical results
establish the consistency of CAPA at detecting collective anomalies and
empirical results show that CAPA has close to linear computational cost as well
as being more accurate at detecting and locating collective anomalies than
other approaches. We demonstrate the utility of CAPA through its ability to
detect exoplanets from light curve data from the Kepler telescope.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fisch_A/0/1/0/all/0/1&quot;&gt;Alexander Tristan Maximilian Fisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eckley_I/0/1/0/all/0/1&quot;&gt;Idris Arthur Eckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fearnhead_P/0/1/0/all/0/1&quot;&gt;Paul Fearnhead&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01949">
<title>Reduced-Order Modeling through Machine Learning Approaches for Brittle Fracture Applications. (arXiv:1806.01949v1 [cs.CE])</title>
<link>http://arxiv.org/abs/1806.01949</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, five different approaches for reduced-order modeling of
brittle fracture in geomaterials, specifically concrete, are presented and
compared. Four of the five methods rely on machine learning (ML) algorithms to
approximate important aspects of the brittle fracture problem. In addition to
the ML algorithms, each method incorporates different physics-based assumptions
in order to reduce the computational complexity while maintaining the physics
as much as possible. This work specifically focuses on using the ML approaches
to model a 2D concrete sample under low strain rate pure tensile loading
conditions with 20 preexisting cracks present. A high-fidelity finite
element-discrete element model is used to both produce a training dataset of
150 simulations and an additional 35 simulations for validation. Results from
the ML approaches are directly compared against the results from the
high-fidelity model. Strengths and weaknesses of each approach are discussed
and the most important conclusion is that a combination of physics-informed and
data-driven features are necessary for emulating the physics of crack
propagation, interaction and coalescence. All of the models presented here have
runtimes that are orders of magnitude faster than the original high-fidelity
model and pave the path for developing accurate reduced order models that could
be used to inform larger length-scale models with important sub-scale physics
that often cannot be accounted for due to computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hunter_A/0/1/0/all/0/1&quot;&gt;A. Hunter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_B/0/1/0/all/0/1&quot;&gt;B. A. Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mudunuru_M/0/1/0/all/0/1&quot;&gt;M. K. Mudunuru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_V/0/1/0/all/0/1&quot;&gt;V. T. Chau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_R/0/1/0/all/0/1&quot;&gt;R. L. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tchoua_R/0/1/0/all/0/1&quot;&gt;R. B. Tchoua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nyshadham_C/0/1/0/all/0/1&quot;&gt;C. Nyshadham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karra_S/0/1/0/all/0/1&quot;&gt;S. Karra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malley_D/0/1/0/all/0/1&quot;&gt;D. O. Malley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rougier_E/0/1/0/all/0/1&quot;&gt;E. Rougier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viswanathan_H/0/1/0/all/0/1&quot;&gt;H. S. Viswanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_G/0/1/0/all/0/1&quot;&gt;G. Srinivasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02032">
<title>Killing Three Birds with one Gaussian Process: Analyzing Attack Vectors on Classification. (arXiv:1806.02032v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1806.02032</link>
<description rdf:parseType="Literal">&lt;p&gt;The wide usage of Machine Learning (ML) has lead to research on the attack
vectors and vulnerability of these systems. The defenses in this area are
however still an open problem, and often lead to an arms race. We define a
naive, secure classifier at test time and show that a Gaussian Process (GP) is
an instance of this classifier given two assumptions: one concerns the
distances in the training data, the other rejection at test time. Using these
assumptions, we are able to show that a classifier is either secure, or
generalizes and thus learns. Our analysis also points towards another factor
influencing robustness, the curvature of the classifier. This connection is not
unknown for linear models, but GP offer an ideal framework to study this
relationship for nonlinear classifiers. We evaluate on five security and two
computer vision datasets applying test and training time attacks and membership
inference. We show that we only change which attacks are needed to succeed,
instead of alleviating the threat. Only for membership inference, there is a
setting in which attacks are unsuccessful (&amp;lt;10% increase in accuracy over
random guess). Given these results, we define a classification scheme based on
voting, ParGP. This allows us to decide how many points vote and how large the
agreement on a class has to be. This ensures a classification output only in
cases when there is evidence for a decision, where evidence is parametrized. We
evaluate this scheme and obtain promising results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_K/0/1/0/all/0/1&quot;&gt;Kathrin Grosse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1&quot;&gt;Michael T. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Backes_M/0/1/0/all/0/1&quot;&gt;Michael Backes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02034">
<title>Degrees of Freedom and Model Selection for kmeans Clustering. (arXiv:1806.02034v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02034</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the problem of model selection for kmeans clustering,
based on conservative estimates of the model degrees of freedom. An extension
of Stein&apos;s lemma, which is used in unbiased risk estimation, is used to obtain
an expression which allows one to approximate the degrees of freedom.
Empirically based estimates of this approximation are obtained. The degrees of
freedom estimates are then used within the popular Bayesian Information
Criterion to perform model selection. The proposed estimation procedure is
validated in a thorough simulation study, and the robustness is assessed
through relaxations of the modelling assumptions and on data from real
applications. Comparisons with popular existing techniques suggest that this
approach performs extremely well when the modelling assumptions
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hofmeyr_D/0/1/0/all/0/1&quot;&gt;David P. Hofmeyr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02046">
<title>Implicit regularization and solution uniqueness in over-parameterized matrix sensing. (arXiv:1806.02046v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02046</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider whether algorithmic choices in over-parameterized linear matrix
factorization introduce implicit regularization. We focus on noiseless matrix
sensing over rank-$r$ positive semi-definite (PSD) matrices in $\mathbb{R}^{n
\times n}$, with a sensing mechanism that satisfies the restricted isometry
property (RIP). The algorithm we study is that of \emph{factored gradient
descent}, where we model the low-rankness and PSD constraints with the
factorization $UU^\top$, where $U \in \mathbb{R}^{n \times r}$. Surprisingly,
recent work argues that the choice of $r \leq n$ is not pivotal: even setting
$U \in \mathbb{R}^{n \times n}$ is sufficient for factored gradient descent to
find the rank-$r$ solution, which suggests that operating over the factors
leads to an implicit regularization.
&lt;/p&gt;
&lt;p&gt;In this note, we provide a different perspective. We show that, in the
noiseless case, under certain conditions, the PSD constraint by itself is
sufficient to lead to a unique rank-$r$ matrix recovery, without implicit or
explicit low-rank regularization. \emph{I.e.}, under assumptions, the set of
PSD matrices, that are consistent with the observed data, is a singleton,
irrespective of the algorithm used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kyrillidis_A/0/1/0/all/0/1&quot;&gt;Anastasios Kyrillidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kalev_A/0/1/0/all/0/1&quot;&gt;Amir Kalev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02071">
<title>Deep Fluids: A Generative Network for Parameterized Fluid Simulations. (arXiv:1806.02071v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02071</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel generative model to synthesize fluid simulations
from a set of reduced parameters. A convolutional neural network is trained on
a collection of discrete, parameterizable fluid simulation velocity fields. Due
to the capability of deep learning architectures to learn representative
features of the data, our generative model is able to accurately approximate
the training data set, while providing plausible interpolated in-betweens. The
proposed generative model is optimized for fluids by a novel loss function that
guarantees divergence-free velocity fields at all times. In addition, we
demonstrate that we can handle complex parameterizations in reduced spaces, and
advance simulations in time by integrating in the latent space with a second
network. Our method models a wide variety of fluid behaviors, thus enabling
applications such as fast construction of simulations, interpolation of fluids
with different parameters, time re-sampling, latent space simulations, and
compression of fluid simulation data. Reconstructed velocity fields are
generated up to 700x faster than traditional CPU solvers, while achieving
compression rates of over 1300x.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Byungsoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azevedo_V/0/1/0/all/0/1&quot;&gt;Vinicius C. Azevedo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thuerey_N/0/1/0/all/0/1&quot;&gt;Nils Thuerey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Theodore Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_M/0/1/0/all/0/1&quot;&gt;Markus Gross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solenthaler_B/0/1/0/all/0/1&quot;&gt;Barbara Solenthaler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02121">
<title>TextRay: Mining Clinical Reports to Gain a Broad Understanding of Chest X-rays. (arXiv:1806.02121v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.02121</link>
<description rdf:parseType="Literal">&lt;p&gt;The chest X-ray (CXR) is by far the most commonly performed radiological
examination for screening and diagnosis of many cardiac and pulmonary diseases.
There is an immense world-wide shortage of physicians capable of providing
rapid and accurate interpretation of this study. A radiologist-driven analysis
of over two million CXR reports generated an ontology including the 40 most
prevalent pathologies on CXR. By manually tagging a relatively small set of
sentences, we were able to construct a training set of 959k studies. A deep
learning model was trained to predict the findings given the patient frontal
and lateral scans. For 12 of the findings we compare the model performance
against a team of radiologists and show that in most cases the radiologists
agree on average more with the algorithm than with each other.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laserson_J/0/1/0/all/0/1&quot;&gt;Jonathan Laserson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lantsman_C/0/1/0/all/0/1&quot;&gt;Christine Dan Lantsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Sfady_M/0/1/0/all/0/1&quot;&gt;Michal Cohen-Sfady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamir_I/0/1/0/all/0/1&quot;&gt;Itamar Tamir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goz_E/0/1/0/all/0/1&quot;&gt;Eli Goz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brestel_C/0/1/0/all/0/1&quot;&gt;Chen Brestel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bar_S/0/1/0/all/0/1&quot;&gt;Shir Bar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atar_M/0/1/0/all/0/1&quot;&gt;Maya Atar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elnekave_E/0/1/0/all/0/1&quot;&gt;Eldad Elnekave&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02136">
<title>Efficient Differentiable Programming in a Functional Array-Processing Language. (arXiv:1806.02136v1 [cs.MS])</title>
<link>http://arxiv.org/abs/1806.02136</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a system for the automatic differentiation of a higher-order
functional array-processing language. The core functional language underlying
this system simultaneously supports both source-to-source automatic
differentiation and global optimizations such as loop transformations. Thanks
to this feature, we demonstrate how for some real-world machine learning and
computer vision benchmarks, the system outperforms the state-of-the-art
automatic differentiation tools.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaikhha_A/0/1/0/all/0/1&quot;&gt;Amir Shaikhha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fitzgibbon_A/0/1/0/all/0/1&quot;&gt;Andrew Fitzgibbon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Vytiniotis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_S/0/1/0/all/0/1&quot;&gt;Simon Peyton Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_C/0/1/0/all/0/1&quot;&gt;Christoph Koch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02146">
<title>Adversarial Auto-encoders for Speech Based Emotion Recognition. (arXiv:1806.02146v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02146</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, generative adversarial networks and adversarial autoencoders have
gained a lot of attention in machine learning community due to their
exceptional performance in tasks such as digit classification and face
recognition. They map the autoencoder&apos;s bottleneck layer output (termed as code
vectors) to different noise Probability Distribution Functions (PDFs), that can
be further regularized to cluster based on class information. In addition, they
also allow a generation of synthetic samples by sampling the code vectors from
the mapped PDFs. Inspired by these properties, we investigate the application
of adversarial autoencoders to the domain of emotion recognition. Specifically,
we conduct experiments on the following two aspects: (i) their ability to
encode high dimensional feature vector representations for emotional utterances
into a compressed space (with a minimal loss of emotion class discriminability
in the compressed space), and (ii) their ability to regenerate synthetic
samples in the original feature space, to be later used for purposes such as
training emotion recognition classifiers. We demonstrate the promise of
adversarial autoencoders with regards to these aspects on the Interactive
Emotional Dyadic Motion Capture (IEMOCAP) corpus and present our analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sahu_S/0/1/0/all/0/1&quot;&gt;Saurabh Sahu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gupta_R/0/1/0/all/0/1&quot;&gt;Rahul Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sivaraman_G/0/1/0/all/0/1&quot;&gt;Ganesh Sivaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+AbdAlmageed_W/0/1/0/all/0/1&quot;&gt;Wael AbdAlmageed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Espy_Wilson_C/0/1/0/all/0/1&quot;&gt;Carol Espy-Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02185">
<title>Boosting Black Box Variational Inference. (arXiv:1806.02185v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02185</link>
<description rdf:parseType="Literal">&lt;p&gt;Approximating a probability density in a tractable manner is a central task
in Bayesian statistics. Variational Inference (VI) is a popular technique that
achieves tractability by choosing a relatively simple variational family.
Borrowing ideas from the classic boosting framework, recent approaches attempt
to \emph{boost} VI by replacing the selection of a single density with a
greedily constructed mixture of densities. In order to guarantee convergence,
previous works impose stringent assumptions that require significant effort for
practitioners. Specifically, they require a custom implementation of the greedy
step (called the LMO) for every probabilistic model with respect to an
unnatural variational family of truncated distributions. Our work fixes these
issues with novel theoretical and algorithmic insights. On the theoretical
side, we show that boosting VI satisfies a relaxed smoothness assumption which
is sufficient for the convergence of the functional Frank-Wolfe (FW) algorithm.
Furthermore, we rephrase the LMO problem and propose to maximize the Residual
ELBO (RELBO) which replaces the standard ELBO optimization in VI. These
theoretical enhancements allow for black box implementation of the boosting
subroutine. Finally, we present a stopping criterion drawn from the duality gap
in the classic FW analyses and exhaustive experiments to illustrate the
usefulness of our theoretical and algorithmic contributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dresdner_G/0/1/0/all/0/1&quot;&gt;Gideon Dresdner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khanna_R/0/1/0/all/0/1&quot;&gt;Rajiv Khanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Valera_I/0/1/0/all/0/1&quot;&gt;Isabel Valera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1&quot;&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02199">
<title>Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series. (arXiv:1806.02199v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02199</link>
<description rdf:parseType="Literal">&lt;p&gt;Human professionals are often required to make decisions based on complex
multivariate time series measurements in an online setting, e.g. in health
care. Since human cognition is not optimized to work well in high-dimensional
spaces, these decisions benefit from interpretable low-dimensional
representations. However, many representation learning algorithms for time
series data are difficult to interpret. This is due to non-intuitive mappings
from data features to salient properties of the representation and
non-smoothness over time. To address this problem, we propose to couple a
variational autoencoder to a discrete latent space and introduce a topological
structure through the use of self-organizing maps. This allows us to learn
discrete representations of time series, which give rise to smooth and
interpretable embeddings with superior clustering performance. Furthermore, to
allow for a probabilistic interpretation of our method, we integrate a Markov
model in the latent space. This model uncovers the temporal transition
structure, improves clustering performance even further and provides additional
explanatory insights as well as a natural representation of uncertainty. We
evaluate our model on static (Fashion-)MNIST data, a time series of linearly
interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two
macro states, as well as on a challenging real world medical time series
application. In the latter experiment, our representation uncovers meaningful
structure in the acute physiological state of a patient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1&quot;&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huser_M/0/1/0/all/0/1&quot;&gt;Matthias H&amp;#xfc;ser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strathmann_H/0/1/0/all/0/1&quot;&gt;Heiko Strathmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratsch_G/0/1/0/all/0/1&quot;&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02246">
<title>Improving the Privacy and Accuracy of ADMM-Based Distributed Algorithms. (arXiv:1806.02246v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02246</link>
<description rdf:parseType="Literal">&lt;p&gt;Alternating direction method of multiplier (ADMM) is a popular method used to
design distributed versions of a machine learning algorithm, whereby local
computations are performed on local data with the output exchanged among
neighbors in an iterative fashion. During this iterative process the leakage of
data privacy arises. A differentially private ADMM was proposed in prior work
(Zhang &amp;amp; Zhu, 2017) where only the privacy loss of a single node during one
iteration was bounded, a method that makes it difficult to balance the tradeoff
between the utility attained through distributed computation and privacy
guarantees when considering the total privacy loss of all nodes over the entire
iterative process. We propose a perturbation method for ADMM where the
perturbed term is correlated with the penalty parameters; this is shown to
improve the utility and privacy simultaneously. The method is based on a
modified ADMM where each node independently determines its own penalty
parameter in every iteration and decouples it from the dual updating step size.
The condition for convergence of the modified ADMM and the lower bound on the
convergence rate are also derived.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xueru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalili_M/0/1/0/all/0/1&quot;&gt;Mohammad Mahdi Khalili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingyan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02252">
<title>Causal Bandits with Propagating Inference. (arXiv:1806.02252v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02252</link>
<description rdf:parseType="Literal">&lt;p&gt;Bandit is a framework for designing sequential experiments. In each
experiment, a learner selects an arm $A \in \mathcal{A}$ and obtains an
observation corresponding to $A$. Theoretically, the tight regret lower-bound
for the general bandit is polynomial with respect to the number of arms
$|\mathcal{A}|$. This makes bandit incapable of handling an exponentially large
number of arms, hence the bandit problem with side-information is often
considered to overcome this lower bound. Recently, a bandit framework over a
causal graph was introduced, where the structure of the causal graph is
available as side-information. A causal graph is a fundamental model that is
frequently used with a variety of real problems. In this setting, the arms are
identified with interventions on a given causal graph, and the effect of an
intervention propagates throughout all over the causal graph. The task is to
find the best intervention that maximizes the expected value on a target node.
Existing algorithms for causal bandit overcame the
$\Omega(\sqrt{|\mathcal{A}|/T})$ simple-regret lower-bound; however, their
algorithms work only when the interventions $\mathcal{A}$ are localized around
a single node (i.e., an intervention propagates only to its neighbors).
&lt;/p&gt;
&lt;p&gt;We propose a novel causal bandit algorithm for an arbitrary set of
interventions, which can propagate throughout the causal graph. We also show
that it achieves $O(\sqrt{ \gamma^*\log(|\mathcal{A}|T) / T})$ regret bound,
where $\gamma^*$ is determined by using a causal graph structure. In
particular, if the in-degree of the causal graph is bounded, then $\gamma^* =
O(N^2)$, where $N$ is the number $N$ of nodes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yabe_A/0/1/0/all/0/1&quot;&gt;Akihiro Yabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hatano_D/0/1/0/all/0/1&quot;&gt;Daisuke Hatano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sumita_H/0/1/0/all/0/1&quot;&gt;Hanna Sumita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ito_S/0/1/0/all/0/1&quot;&gt;Shinji Ito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kakimura_N/0/1/0/all/0/1&quot;&gt;Naonori Kakimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fukunaga_T/0/1/0/all/0/1&quot;&gt;Takuro Fukunaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kawarabayashi_K/0/1/0/all/0/1&quot;&gt;Ken-ichi Kawarabayashi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02261">
<title>Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with $\beta$-Divergences. (arXiv:1806.02261v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02261</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the very first robust Bayesian Online Changepoint Detection
algorithm through General Bayesian Inference (GBI) with $\beta$-divergences.
The resulting inference procedure is doubly robust for both the predictive and
the changepoint (CP) posterior, with linear time and constant space complexity.
We provide a construction for exponential models and demonstrate it on the
Bayesian Linear Regression model. In so doing, we make two additional
contributions: Firstly, we make GBI scalable using Structural Variational
approximations that are exact as $\beta \to 0$. Secondly, we give a principled
way of choosing the divergence parameter $\beta$ by minimizing expected
predictive loss on-line. We offer the state of the art and improve the False
Discovery Rate of CPs by more than 80% on real world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Knoblauch_J/0/1/0/all/0/1&quot;&gt;Jeremias Knoblauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jewson_J/0/1/0/all/0/1&quot;&gt;Jack Jewson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Damoulas_T/0/1/0/all/0/1&quot;&gt;Theodoros Damoulas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02282">
<title>Finding the Bandit in a Graph: Sequential Search-and-Stop. (arXiv:1806.02282v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.02282</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem where an agent wants to find a hidden object that is
randomly located in some vertex of a directed acyclic graph (DAG) according to
a fixed but possibly unknown distribution. The agent can only examine vertices
whose in-neighbors have already been examined. In scheduling theory, this
problem is denoted by $1|prec|\sum w_jC_j$. However, in this paper, we address
learning setting where we allow the agent to stop before having found the
object and restart searching on a new independent instance of the same problem.
The goal is to maximize the total number of hidden objects found under a time
constraint. The agent can thus skip an instance after realizing that it would
spend too much time on it. Our contributions are both to the search theory and
multi-armed bandits. If the distribution is known, we provide a quasi-optimal
greedy strategy with the help of known computationally efficient algorithms for
solving $1|prec|\sum w_jC_j$ under some assumption on the DAG. If the
distribution is unknown, we show how to sequentially learn it and, at the same
time, act near-optimally in order to collect as many hidden objects as
possible. We provide an algorithm, prove theoretical guarantees, and
empirically show that it outperforms the na\&quot;ive baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perrault_P/0/1/0/all/0/1&quot;&gt;Pierre Perrault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perchet_V/0/1/0/all/0/1&quot;&gt;Vianney Perchet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Valko_M/0/1/0/all/0/1&quot;&gt;Michal Valko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02300">
<title>Data-driven Probabilistic Atlases Capture Whole-brain Individual Variation. (arXiv:1806.02300v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02300</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic atlases provide essential spatial contextual information for
image interpretation, Bayesian modeling, and algorithmic processing. Such
atlases are typically constructed by grouping subjects with similar demographic
information. Importantly, use of the same scanner minimizes inter-group
variability. However, generalizability and spatial specificity of such
approaches is more limited than one might like. Inspired by Commowick
&quot;Frankenstein&apos;s creature paradigm&quot; which builds a personal specific anatomical
atlas, we propose a data-driven framework to build a personal specific
probabilistic atlas under the large-scale data scheme. The data-driven
framework clusters regions with similar features using a point distribution
model to learn different anatomical phenotypes. Regional structural atlases and
corresponding regional probabilistic atlases are used as indices and targets in
the dictionary. By indexing the dictionary, the whole brain probabilistic
atlases adapt to each new subject quickly and can be used as spatial priors for
visualization and processing. The novelties of this approach are (1) it
provides a new perspective of generating personal specific whole brain
probabilistic atlases (132 regions) under data-driven scheme across sites. (2)
The framework employs the large amount of heterogeneous data (2349 images). (3)
The proposed framework achieves low computational cost since only one affine
registration and Pearson correlation operation are required for a new subject.
Our method matches individual regions better with higher Dice similarity value
when testing the probabilistic atlases. Importantly, the advantage the
large-scale scheme is demonstrated by the better performance of using
large-scale training data (1888 images) than smaller training set (720 images).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1&quot;&gt;Yuankai Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swett_K/0/1/0/all/0/1&quot;&gt;Katherine Swett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Resnick_S/0/1/0/all/0/1&quot;&gt;Susan M. Resnick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cutting_L/0/1/0/all/0/1&quot;&gt;Laurie E. Cutting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1&quot;&gt;Bennett A. Landman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02315">
<title>Randomized Value Functions via Multiplicative Normalizing Flows. (arXiv:1806.02315v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.02315</link>
<description rdf:parseType="Literal">&lt;p&gt;Randomized value functions offer a promising approach towards the challenge
of efficient exploration in complex environments with high dimensional state
and action spaces. Unlike traditional point estimate methods, randomized value
functions maintain a posterior distribution over action-space values. This
prevents the agent&apos;s behavior policy from prematurely exploiting early
estimates and falling into local optima. In this work, we leverage recent
advances in variational Bayesian neural networks and combine these with
traditional Deep Q-Networks (DQN) to achieve randomized value functions for
high-dimensional domains. In particular, we augment DQN with multiplicative
normalizing flows in order to track an approximate posterior distribution over
its parameters. This allows the agent to perform approximate Thompson sampling
in a computationally efficient manner via stochastic gradient methods. We
demonstrate the benefits of our approach through an empirical comparison in
high dimensional environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Touati_A/0/1/0/all/0/1&quot;&gt;Ahmed Touati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satija_H/0/1/0/all/0/1&quot;&gt;Harsh Satija&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romoff_J/0/1/0/all/0/1&quot;&gt;Joshua Romoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1&quot;&gt;Joelle Pineau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1&quot;&gt;Pascal Vincent&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02321">
<title>Fitting a deeply-nested hierarchical model to a large book review dataset using a moment-based estimator. (arXiv:1806.02321v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1806.02321</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a particular instance of a common problem in recommender systems:
using a database of book reviews to inform user-targeted recommendations. In
our dataset, books are categorized into genres and sub-genres. To exploit this
nested taxonomy, we use a hierarchical model that enables information pooling
across across similar items at many levels within the genre hierarchy. The main
challenge in deploying this model is computational: the data sizes are large,
and fitting the model at scale using off-the-shelf maximum likelihood
procedures is prohibitive. To get around this computational bottleneck, we
extend a moment-based fitting procedure proposed for fitting single-level
hierarchical models to the general case of arbitrarily deep hierarchies. This
extension is an order of magnetite faster than standard maximum likelihood
procedures. The fitting method can be deployed beyond recommender systems to
general contexts with deeply-nested hierarchical generalized linear mixed
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningshan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmaus_K/0/1/0/all/0/1&quot;&gt;Kyle Schmaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perry_P/0/1/0/all/0/1&quot;&gt;Patrick O. Perry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.03389">
<title>SCAN: Learning Hierarchical Compositional Visual Concepts. (arXiv:1707.03389v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.03389</link>
<description rdf:parseType="Literal">&lt;p&gt;The seemingly infinite diversity of the natural world arises from a
relatively small set of coherent rules, such as the laws of physics or
chemistry. We conjecture that these rules give rise to regularities that can be
discovered through primarily unsupervised experiences and represented as
abstract concepts. If such representations are compositional and hierarchical,
they can be recombined into an exponentially large set of new concepts. This
paper describes SCAN (Symbol-Concept Association Network), a new framework for
learning such abstractions in the visual domain. SCAN learns concepts through
fast symbol association, grounding them in disentangled visual primitives that
are discovered in an unsupervised manner. Unlike state of the art multimodal
generative model baselines, our approach requires very few pairings between
symbols and images and makes no assumptions about the form of symbol
representations. Once trained, SCAN is capable of multimodal bi-directional
inference, generating a diverse set of image samples from symbolic descriptions
and vice versa. It also allows for traversal and manipulation of the implicit
hierarchy of visual concepts through symbolic instructions and learnt logical
recombination operations. Such manipulations enable SCAN to break away from its
training data distribution and imagine novel visual concepts through
symbolically instructed recombination of previously learnt concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Higgins_I/0/1/0/all/0/1&quot;&gt;Irina Higgins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sonnerat_N/0/1/0/all/0/1&quot;&gt;Nicolas Sonnerat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Matthey_L/0/1/0/all/0/1&quot;&gt;Loic Matthey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pal_A/0/1/0/all/0/1&quot;&gt;Arka Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Burgess_C/0/1/0/all/0/1&quot;&gt;Christopher P Burgess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bosnjak_M/0/1/0/all/0/1&quot;&gt;Matko Bosnjak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shanahan_M/0/1/0/all/0/1&quot;&gt;Murray Shanahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Botvinick_M/0/1/0/all/0/1&quot;&gt;Matthew Botvinick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hassabis_D/0/1/0/all/0/1&quot;&gt;Demis Hassabis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lerchner_A/0/1/0/all/0/1&quot;&gt;Alexander Lerchner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04212">
<title>Asymptotic Bayesian Generalization Error in a General Stochastic Matrix Factorization. (arXiv:1709.04212v4 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04212</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic matrix factorization (SMF) can be regarded as a restriction of
non-negative matrix factorization (NMF). SMF is useful for inference of topic
models, NMF for binary matrices data, Markov chains, and Bayesian networks.
However, SMF needs strong assumptions to reach a unique factorization and its
theoretical prediction accuracy has not yet been clarified. In this paper, we
study the maximum the pole of zeta function (real log canonical threshold) of a
general SMF and derive an upper bound of the generalization error in Bayesian
inference. The results give a foundation for a widely applicable and rigorous
factorization method of SMF and mean that the generalization error in SMF
becomes smaller than regular statistical models by Bayesian inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hayashi_N/0/1/0/all/0/1&quot;&gt;Naoki Hayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Watanabe_S/0/1/0/all/0/1&quot;&gt;Sumio Watanabe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07742">
<title>Towards Black-box Iterative Machine Teaching. (arXiv:1710.07742v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07742</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we make an important step towards the black-box machine
teaching by considering the cross-space machine teaching, where the teacher and
the learner use different feature representations and the teacher can not fully
observe the learner&apos;s model. In such scenario, we study how the teacher is
still able to teach the learner to achieve faster convergence rate than the
traditional passive learning. We propose an active teacher model that can
actively query the learner (i.e., make the learner take exams) for estimating
the learner&apos;s status and provably guide the learner to achieve faster
convergence. The sample complexities for both teaching and query are provided.
In the experiments, we compare the proposed active teacher with the omniscient
teacher and verify the effectiveness of the active teacher model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rehg_J/0/1/0/all/0/1&quot;&gt;James M. Rehg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Le Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00673">
<title>Fast Information-theoretic Bayesian Optimisation. (arXiv:1711.00673v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00673</link>
<description rdf:parseType="Literal">&lt;p&gt;Information-theoretic Bayesian optimisation techniques have demonstrated
state-of-the-art performance in tackling important global optimisation
problems. However, current information-theoretic approaches require many
approximations in implementation, introduce often-prohibitive computational
overhead and limit the choice of kernels available to model the objective. We
develop a fast information-theoretic Bayesian Optimisation method, FITBO, that
avoids the need for sampling the global minimiser, thus significantly reducing
computational overhead. Moreover, in comparison with existing approaches, our
method faces fewer constraints on kernel choice and enjoys the merits of
dealing with the output space. We demonstrate empirically that FITBO inherits
the performance associated with information-theoretic Bayesian optimisation,
while being even faster than simpler Bayesian optimisation approaches, such as
Expected Improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ru_B/0/1/0/all/0/1&quot;&gt;Binxin Ru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McLeod_M/0/1/0/all/0/1&quot;&gt;Mark McLeod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Granziol_D/0/1/0/all/0/1&quot;&gt;Diego Granziol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osborne_M/0/1/0/all/0/1&quot;&gt;Michael A. Osborne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02124">
<title>Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations. (arXiv:1801.02124v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02124</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers the problem of inverse reinforcement learning in
zero-sum stochastic games when expert demonstrations are known to be not
optimal. Compared to previous works that decouple agents in the game by
assuming optimality in expert strategies, we introduce a new objective function
that directly pits experts against Nash Equilibrium strategies, and we design
an algorithm to solve for the reward function in the context of inverse
reinforcement learning with deep neural networks as model approximations. In
our setting the model and algorithm do not decouple by agent. In order to find
Nash Equilibrium in large-scale games, we also propose an adversarial training
algorithm for zero-sum stochastic games, and show the theoretical appeal of
non-existence of local optima in its objective function. In our numerical
experiments, we demonstrate that our Nash Equilibrium and inverse reinforcement
learning algorithms address games that are not amenable to previous approaches
using tabular representations. Moreover, with sub-optimal expert demonstrations
our algorithms recover both reward functions and strategies with good quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klabjan_D/0/1/0/all/0/1&quot;&gt;Diego Klabjan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04153">
<title>Bayesian Quadrature for Multiple Related Integrals. (arXiv:1801.04153v5 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04153</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian probabilistic numerical methods are a set of tools providing
posterior distributions on the output of numerical methods. The use of these
methods is usually motivated by the fact that they can represent our
uncertainty due to incomplete/finite information about the continuous
mathematical problem being approximated. In this paper, we demonstrate that
this paradigm can provide additional advantages, such as the possibility of
transferring information between several numerical methods. This allows users
to represent uncertainty in a more faithful manner and, as a by-product,
provide increased numerical efficiency. We propose the first such numerical
method by extending the well-known Bayesian quadrature algorithm to the case
where we are interested in computing the integral of several related functions.
We then prove convergence rates for the method in the well-specified and
misspecified cases, and demonstrate its efficiency in the context of
multi-fidelity models for complex engineering systems and a problem of global
illumination in computer graphics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xi_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Briol_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois-Xavier Briol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1&quot;&gt;Mark Girolami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04687">
<title>Neural Relational Inference for Interacting Systems. (arXiv:1802.04687v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04687</link>
<description rdf:parseType="Literal">&lt;p&gt;Interacting systems are prevalent in nature, from dynamical systems in
physics to complex societal dynamics. The interplay of components can give rise
to complex behavior, which can often be explained using a simple model of the
system&apos;s constituent parts. In this work, we introduce the neural relational
inference (NRI) model: an unsupervised model that learns to infer interactions
while simultaneously learning the dynamics purely from observational data. Our
model takes the form of a variational auto-encoder, in which the latent code
represents the underlying interaction graph and the reconstruction is based on
graph neural networks. In experiments on simulated physical systems, we show
that our NRI model can accurately recover ground-truth interactions in an
unsupervised manner. We further demonstrate that we can find an interpretable
structure and predict complex dynamics in real motion capture and sports
tracking data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kipf_T/0/1/0/all/0/1&quot;&gt;Thomas Kipf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fetaya_E/0/1/0/all/0/1&quot;&gt;Ethan Fetaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kuan-Chieh Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04715">
<title>Online Variance Reduction for Stochastic Optimization. (arXiv:1802.04715v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04715</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern stochastic optimization methods often rely on uniform sampling which
is agnostic to the underlying characteristics of the data. This might degrade
the convergence by yielding estimates that suffer from a high variance. A
possible remedy is to employ non-uniform importance sampling techniques, which
take the structure of the dataset into account. In this work, we investigate a
recently proposed setting which poses variance reduction as an online
optimization problem with bandit feedback. We devise a novel and efficient
algorithm for this setting that finds a sequence of importance sampling
distributions competitive with the best fixed distribution in hindsight, the
first result of this kind. While we present our method for sampling datapoints,
it naturally extends to selecting coordinates or even blocks of thereof.
Empirical validations underline the benefits of our method in several settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Borsos_Z/0/1/0/all/0/1&quot;&gt;Zal&amp;#xe1;n Borsos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Levy_K/0/1/0/all/0/1&quot;&gt;Kfir Y. Levy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04907">
<title>Compressive Sensing with Low Precision Data Representation: Theory and Applications. (arXiv:1802.04907v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04907</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern scientific instruments produce vast amounts of data, which can
overwhelm the processing ability of computer systems. Lossy compression of data
is an intriguing solution, but comes with its own dangers, such as potential
signal loss, and the need for careful parameter optimization. In this work, we
focus on a setting where this problem is especially acute -compressive sensing
frameworks for radio astronomy- and ask: Can the precision of the data
representation be lowered for all inputs, with both recovery guarantees and
practical performance?
&lt;/p&gt;
&lt;p&gt;Our first contribution is a theoretical analysis of the Iterative Hard
Thresholding (IHT) algorithm when all input data, that is, the measurement
matrix and the observation, are quantized aggressively to as little as 2 bits
per value. Under reasonable constraints, we show that there exists a variant of
low precision IHT that can still provide recovery guarantees. The second
contribution is an analysis of our general quantized framework tailored to
radio astronomy, showing that its conditions are satisfied in this case. We
evaluate our approach using CPU and FPGA implementations, and show that it can
achieve up to 9.19x speed up with negligible loss of recovery quality, on real
telescope data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gurel_N/0/1/0/all/0/1&quot;&gt;Nezihe Merve G&amp;#xfc;rel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kara_K/0/1/0/all/0/1&quot;&gt;Kaan Kara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stojanov_A/0/1/0/all/0/1&quot;&gt;Alen Stojanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smith_T/0/1/0/all/0/1&quot;&gt;Tyler Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alistarh_D/0/1/0/all/0/1&quot;&gt;Dan Alistarh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Puschel_M/0/1/0/all/0/1&quot;&gt;Markus P&amp;#xfc;schel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Ce Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05249">
<title>Distributionally Robust Submodular Maximization. (arXiv:1802.05249v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05249</link>
<description rdf:parseType="Literal">&lt;p&gt;Submodular functions have applications throughout machine learning, but in
many settings, we do not have direct access to the underlying function $f$. We
focus on stochastic functions that are given as an expectation of functions
over a distribution $P$. In practice, we often have only a limited set of
samples $f_i$ from $P$. The standard approach indirectly optimizes $f$ by
maximizing the sum of $f_i$. However, this ignores generalization to the true
(unknown) distribution. In this paper, we achieve better performance on the
actual underlying function $f$ by directly optimizing a combination of bias and
variance. Algorithmically, we accomplish this by showing how to carry out
distributionally robust optimization (DRO) for submodular functions, providing
efficient algorithms backed by theoretical guarantees which leverage several
novel contributions to the general theory of DRO. We also show compelling
empirical evidence that DRO improves generalization to the unknown stochastic
submodular function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staib_M/0/1/0/all/0/1&quot;&gt;Matthew Staib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilder_B/0/1/0/all/0/1&quot;&gt;Bryan Wilder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05983">
<title>Disentangling by Factorising. (arXiv:1802.05983v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05983</link>
<description rdf:parseType="Literal">&lt;p&gt;We define and address the problem of unsupervised learning of disentangled
representations on data generated from independent factors of variation. We
propose FactorVAE, a method that disentangles by encouraging the distribution
of representations to be factorial and hence independent across the dimensions.
We show that it improves upon $\beta$-VAE by providing a better trade-off
between disentanglement and reconstruction quality. Moreover, we highlight the
problems of a commonly used disentanglement metric and introduce a new metric
that does not suffer from them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunjik Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mnih_A/0/1/0/all/0/1&quot;&gt;Andriy Mnih&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07481">
<title>Celer: a Fast Solver for the Lasso with Dual Extrapolation. (arXiv:1802.07481v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07481</link>
<description rdf:parseType="Literal">&lt;p&gt;Convex sparsity-inducing regularizations are ubiquitous in high-dimensional
machine learning, but solving the resulting optimization problems can be slow.
To accelerate solvers, state-of-the-art approaches consist in reducing the size
of the optimization problem at hand. In the context of regression, this can be
achieved either by discarding irrelevant features (screening techniques) or by
prioritizing features likely to be included in the support of the solution
(working set techniques). Duality comes into play at several steps in these
techniques. Here, we propose an extrapolation technique starting from a
sequence of iterates in the dual that leads to the construction of improved
dual points. This enables a tighter control of optimality as used in stopping
criterion, as well as better screening performance of Gap Safe rules. Finally,
we propose a working set strategy based on an aggressive use of Gap Safe
screening rules. Thanks to our new dual point construction, we show significant
computational speedups on multiple real-world problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Massias_M/0/1/0/all/0/1&quot;&gt;Mathurin Massias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gramfort_A/0/1/0/all/0/1&quot;&gt;Alexandre Gramfort&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salmon_J/0/1/0/all/0/1&quot;&gt;Joseph Salmon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09086">
<title>Conditionally Independent Multiresolution Gaussian Processes. (arXiv:1802.09086v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09086</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a multiresolution Gaussian process (GP) model which assumes
conditional independence among GPs across resolutions. The model is built on
the hierarchical application of predictive processes using a particular
representation of the GP via the Karhunen-Loeve expansion with a Bingham prior
model, where each basis vector of the expansion consists of an axis and a scale
factor, referred to as the basis axis and the basis-axis scale. The basis axes
have unique characteristics: They are zero-mean by construction and live on the
unit sphere. These properties allow us to further assume that the axes are
shared across all resolutions while their scales remain resolution specific.
The properties of the Bingham distribution makes it the natural choice when it
comes to modeling the axes. We drive a fully Bayesian inference for the model
using a structured variational inference with a partially factorized mean-field
approximation which learns a joint Gaussian-Bingham posterior distribution over
the basis-axis scales and the basis axes. Relaxing the full independence
assumption enables the construction of models which are robust to overfitting
in the sense of sensitivity to the chosen resolution and predictions that are
smooth at the boundaries. Our new model and inference algorithm are compared
against current state of the art on 2 synthetic and 9 real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Taghia_J/0/1/0/all/0/1&quot;&gt;Jalil Taghia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schon_T/0/1/0/all/0/1&quot;&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01420">
<title>Detecting Correlations with Little Memory and Communication. (arXiv:1803.01420v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01420</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of identifying correlations in multivariate data, under
information constraints: Either on the amount of memory that can be used by the
algorithm, or the amount of communication when the data is distributed across
several machines. We prove a tight trade-off between the memory/communication
complexity and the sample complexity, implying (for example) that to detect
pairwise correlations with optimal sample complexity, the number of required
memory/communication bits is at least quadratic in the dimension. Our results
substantially improve those of Shamir [2014], which studied a similar question
in a much more restricted setting. To the best of our knowledge, these are the
first provable sample/memory/communication trade-offs for a practical
estimation problem, using standard distributions, and in the natural regime
where the memory/communication budget is larger than the size of a single data
point. To derive our theorems, we prove a new information-theoretic result,
which may be relevant for studying other information-constrained learning
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dagan_Y/0/1/0/all/0/1&quot;&gt;Yuval Dagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1&quot;&gt;Ohad Shamir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09539">
<title>On Matching Pursuit and Coordinate Descent. (arXiv:1803.09539v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09539</link>
<description rdf:parseType="Literal">&lt;p&gt;Two popular examples of first-order optimization methods over linear spaces
are coordinate descent and matching pursuit algorithms, with their randomized
variants. While the former targets the optimization by moving along
coordinates, the latter considers a generalized notion of directions.
Exploiting the connection between the two algorithms, we present a unified
analysis of both, providing affine invariant sublinear $\mathcal{O}((1/t)$
rates on smooth objectives and linear convergence on strongly convex
objectives. As a byproduct of our affine invariant analysis of matching
pursuit, our rates for steepest coordinate descent are the tightest known.
Furthermore, we show the first accelerated convergence rate
$\mathcal{O}((1/t^2)$ for matching pursuit on convex objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Anant Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karimireddy_S/0/1/0/all/0/1&quot;&gt;Sai Praneeth Karimireddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ratsch_G/0/1/0/all/0/1&quot;&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stich_S/0/1/0/all/0/1&quot;&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11485">
<title>QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning. (arXiv:1803.11485v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.11485</link>
<description rdf:parseType="Literal">&lt;p&gt;In many real-world settings, a team of agents must coordinate their behaviour
while acting in a decentralised way. At the same time, it is often possible to
train the agents in a centralised fashion in a simulated or laboratory setting,
where global state information is available and communication constraints are
lifted. Learning joint action-values conditioned on extra state information is
an attractive way to exploit centralised learning, but the best strategy for
then extracting decentralised policies is unclear. Our solution is QMIX, a
novel value-based method that can train decentralised policies in a centralised
end-to-end fashion. QMIX employs a network that estimates joint action-values
as a complex non-linear combination of per-agent values that condition only on
local observations. We structurally enforce that the joint-action value is
monotonic in the per-agent values, which allows tractable maximisation of the
joint action-value in off-policy learning, and guarantees consistency between
the centralised and decentralised policies. We evaluate QMIX on a challenging
set of StarCraft II micromanagement tasks, and show that QMIX significantly
outperforms existing value-based multi-agent reinforcement learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1&quot;&gt;Tabish Rashid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samvelyan_M/0/1/0/all/0/1&quot;&gt;Mikayel Samvelyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1&quot;&gt;Christian Schroeder de Witt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farquhar_G/0/1/0/all/0/1&quot;&gt;Gregory Farquhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1&quot;&gt;Jakob Foerster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09699">
<title>Towards Fast Computation of Certified Robustness for ReLU Networks. (arXiv:1804.09699v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.09699</link>
<description rdf:parseType="Literal">&lt;p&gt;Verifying the robustness property of a general Rectified Linear Unit (ReLU)
network is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer
CAV17]. Although finding the exact minimum adversarial distortion is hard,
giving a certified lower bound of the minimum distortion is possible. Current
available methods of computing such a bound are either time-consuming or
delivering low quality bounds that are too loose to be useful. In this paper,
we exploit the special structure of ReLU networks and provide two
computationally efficient algorithms Fast-Lin and Fast-Lip that are able to
certify non-trivial lower bounds of minimum distortions, by bounding the ReLU
units with appropriate linear functions Fast-Lin, or by bounding the local
Lipschitz constant Fast-Lip. Experiments show that (1) our proposed methods
deliver bounds close to (the gap is 2-3X) exact minimum distortion found by
Reluplex in small MNIST networks while our algorithms are more than 10,000
times faster; (2) our methods deliver similar quality of bounds (the gap is
within 35% and usually around 10%; sometimes our bounds are even better) for
larger networks compared to the methods based on solving linear programming
problems but our algorithms are 33-14,000 times faster; (3) our method is
capable of solving large MNIST and CIFAR networks up to 7 layers with more than
10,000 neurons within tens of seconds on a single CPU core.
&lt;/p&gt;
&lt;p&gt;In addition, we show that, in fact, there is no polynomial time algorithm
that can approximately find the minimum $\ell_1$ adversarial distortion of a
ReLU network with a $0.99\ln n$ approximation ratio unless
$\mathsf{NP}$=$\mathsf{P}$, where $n$ is the number of neurons in the network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weng_T/0/1/0/all/0/1&quot;&gt;Tsui-Wei Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongge Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boning_D/0/1/0/all/0/1&quot;&gt;Duane Boning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhillon_I/0/1/0/all/0/1&quot;&gt;Inderjit S. Dhillon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Daniel_L/0/1/0/all/0/1&quot;&gt;Luca Daniel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11326">
<title>Supervised learning with quantum enhanced feature spaces. (arXiv:1804.11326v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1804.11326</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning and quantum computing are two technologies each with the
potential for altering how computation is performed to address previously
untenable problems. Kernel methods for machine learning are ubiquitous for
pattern recognition, with support vector machines (SVMs) being the most
well-known method for classification problems. However, there are limitations
to the successful solution to such problems when the feature space becomes
large, and the kernel functions become computationally expensive to estimate. A
core element to computational speed-ups afforded by quantum algorithms is the
exploitation of an exponentially large quantum state space through controllable
entanglement and interference. Here, we propose and experimentally implement
two novel methods on a superconducting processor. Both methods represent the
feature space of a classification problem by a quantum state, taking advantage
of the large dimensionality of quantum Hilbert space to obtain an enhanced
solution. One method, the quantum variational classifier builds on [1,2] and
operates through using a variational quantum circuit to classify a training set
in direct analogy to conventional SVMs. In the second, a quantum kernel
estimator, we estimate the kernel function and optimize the classifier
directly. The two methods present a new class of tools for exploring the
applications of noisy intermediate scale quantum computers [3] to machine
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Havlicek_V/0/1/0/all/0/1&quot;&gt;Vojtech Havlicek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Corcoles_A/0/1/0/all/0/1&quot;&gt;Antonio D. C&amp;#xf3;rcoles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Temme_K/0/1/0/all/0/1&quot;&gt;Kristan Temme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Harrow_A/0/1/0/all/0/1&quot;&gt;Aram W. Harrow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kandala_A/0/1/0/all/0/1&quot;&gt;Abhinav Kandala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chow_J/0/1/0/all/0/1&quot;&gt;Jerry M. Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gambetta_J/0/1/0/all/0/1&quot;&gt;Jay M. Gambetta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05383">
<title>Spatio-temporal Bayesian On-line Changepoint Detection with Model Selection. (arXiv:1805.05383v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05383</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian On-line Changepoint Detection is extended to on-line model selection
and non-stationary spatio-temporal processes. We propose spatially structured
Vector Autoregressions (VARs) for modelling the process between changepoints
(CPs) and give an upper bound on the approximation error of such models. The
resulting algorithm performs prediction, model selection and CP detection
on-line. Its time complexity is linear and its space complexity constant, and
thus it is two orders of magnitudes faster than its closest competitor. In
addition, it outperforms the state of the art for multivariate data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Knoblauch_J/0/1/0/all/0/1&quot;&gt;Jeremias Knoblauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Damoulas_T/0/1/0/all/0/1&quot;&gt;Theodoros Damoulas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00416">
<title>Pattern Search Multidimensional Scaling. (arXiv:1806.00416v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00416</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel view of nonlinear manifold learning using derivative-free
optimization techniques. Specifically, we propose an extension of the classical
multi-dimensional scaling (MDS) method, where instead of performing gradient
descent, we sample and evaluate possible &quot;moves&quot; in a sphere of fixed radius
for each point in the embedded space. A fixed-point convergence guarantee can
be shown by formulating the proposed algorithm as an instance of General
Pattern Search (GPS) framework. Evaluation on both clean and noisy synthetic
datasets shows that pattern search MDS can accurately infer the intrinsic
geometry of manifolds embedded in high-dimensional spaces. Additionally,
experiments on real data, even under noisy conditions, demonstrate that the
proposed pattern search MDS yields state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paraskevopoulos_G/0/1/0/all/0/1&quot;&gt;Georgios Paraskevopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1&quot;&gt;Efthymios Tzinis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlatakis_Gkaragkounis_E/0/1/0/all/0/1&quot;&gt;Emmanuel-Vasileios Vlatakis-Gkaragkounis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potamianos_A/0/1/0/all/0/1&quot;&gt;Alexandros Potamianos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01047">
<title>Normative Modeling of Neuroimaging Data using Scalable Multi-Task Gaussian Processes. (arXiv:1806.01047v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01047</link>
<description rdf:parseType="Literal">&lt;p&gt;Normative modeling has recently been proposed as an alternative for the
case-control approach in modeling heterogeneity within clinical cohorts.
Normative modeling is based on single-output Gaussian process regression that
provides coherent estimates of uncertainty required by the method but does not
consider spatial covariance structure. Here, we introduce a scalable multi-task
Gaussian process regression (S-MTGPR) approach to address this problem. To this
end, we exploit a combination of a low-rank approximation of the spatial
covariance matrix with algebraic properties of Kronecker product in order to
reduce the computational complexity of Gaussian process regression in
high-dimensional output spaces. On a public fMRI dataset, we show that S-MTGPR:
1) leads to substantial computational improvements that allow us to estimate
normative models for high-dimensional fMRI data whilst accounting for spatial
structure in data; 2) by modeling both spatial and across-sample variances, it
provides higher sensitivity in novelty detection scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kia_S/0/1/0/all/0/1&quot;&gt;Seyed Mostafa Kia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marquand_A/0/1/0/all/0/1&quot;&gt;Andre Marquand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01260">
<title>Digging Into Self-Supervised Monocular Depth Estimation. (arXiv:1806.01260v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01260</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth-sensing is important for both navigation and scene understanding.
However, procuring RGB images with corresponding depth data for training deep
models is challenging; large-scale, varied, datasets with ground truth training
data are scarce. Consequently, several recent methods have proposed treating
the training of monocular color-to-depth estimation networks as an image
reconstruction problem, thus forgoing the need for ground truth depth.
&lt;/p&gt;
&lt;p&gt;There are multiple concepts and design decisions for these networks that seem
sensible, but give mixed or surprising results when tested. For example,
binocular stereo as the source of self-supervision seems cumbersome and hard to
scale, yet results are less blurry compared to training with monocular videos.
Such decisions also interplay with questions about architectures, loss
functions, image scales, and motion handling. In this paper, we propose a
simple yet effective model, with several general architectural and loss
innovations, that surpasses all other self-supervised depth estimation
approaches on KITTI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godard_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Godard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1&quot;&gt;Oisin Mac Aodha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brostow_G/0/1/0/all/0/1&quot;&gt;Gabriel Brostow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01471">
<title>PAC-learning in the presence of evasion adversaries. (arXiv:1806.01471v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01471</link>
<description rdf:parseType="Literal">&lt;p&gt;The existence of evasion attacks during the test phase of machine learning
algorithms represents a significant challenge to both their deployment and
understanding. These attacks can be carried out by adding imperceptible
perturbations to inputs to generate adversarial examples and finding effective
defenses and detectors has proven to be difficult. In this paper, we step away
from the attack-defense arms race and seek to understand the limits of what can
be learned in the presence of an evasion adversary. In particular, we extend
the Probably Approximately Correct (PAC)-learning framework to account for the
presence of an adversary. We first define corrupted hypothesis classes which
arise from standard binary hypothesis classes in the presence of an evasion
adversary and derive the Vapnik-Chervonenkis (VC)-dimension for these, denoted
as the adversarial VC-dimension. We then show that sample complexity upper
bounds from the Fundamental Theorem of Statistical learning can be extended to
the case of evasion adversaries, where the sample complexity is controlled by
the adversarial VC-dimension. We then explicitly derive the adversarial
VC-dimension for halfspace classifiers in the presence of a sample-wise
norm-constrained adversary of the type commonly studied for evasion attacks and
show that it is the same as the standard VC-dimension, closing an open
question. Finally, we prove that the adversarial VC-dimension can be either
larger or smaller than the standard VC-dimension depending on the hypothesis
class and adversary, making it an interesting object of study in its own right.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cullina_D/0/1/0/all/0/1&quot;&gt;Daniel Cullina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bhagoji_A/0/1/0/all/0/1&quot;&gt;Arjun Nitin Bhagoji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mittal_P/0/1/0/all/0/1&quot;&gt;Prateek Mittal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.04464">
<title>Sequential geophysical and flow inversion to characterize fracture networks in subsurface systems. (arXiv:1606.04464v3 [cs.CE] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1606.04464</link>
<description rdf:parseType="Literal">&lt;p&gt;Subsurface applications including geothermal, geological carbon
sequestration, oil and gas, etc., typically involve maximizing either the
extraction of energy or the storage of fluids. Characterizing the subsurface is
extremely complex due to heterogeneity and anisotropy. Due to this complexity,
there are uncertainties in the subsurface parameters, which need to be
estimated from multiple diverse as well as fragmented data streams. In this
paper, we present a non-intrusive sequential inversion framework, for
integrating data from geophysical and flow sources to constraint subsurface
Discrete Fracture Networks (DFN). In this approach, we first estimate bounds on
the statistics for the DFN fracture orientations using microseismic data. These
bounds are estimated through a combination of a focal mechanism (physics-based
approach) and clustering analysis (statistical approach) of seismic data. Then,
the fracture lengths are constrained based on the flow data. The efficacy of
this multi-physics based sequential inversion is demonstrated through a
representative synthetic example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mudunuru_M/0/1/0/all/0/1&quot;&gt;M. K. Mudunuru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karra_S/0/1/0/all/0/1&quot;&gt;S. Karra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makedonska_N/0/1/0/all/0/1&quot;&gt;N. Makedonska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;T. Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00894">
<title>Infrastructure Quality Assessment in Africa using Satellite Imagery and Deep Learning. (arXiv:1806.00894v1 [cs.CY] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1806.00894</link>
<description rdf:parseType="Literal">&lt;p&gt;The UN Sustainable Development Goals allude to the importance of
infrastructure quality in three of its seventeen goals. However, monitoring
infrastructure quality in developing regions remains prohibitively expensive
and impedes efforts to measure progress toward these goals. To this end, we
investigate the use of widely available remote sensing data for the prediction
of infrastructure quality in Africa. We train a convolutional neural network to
predict ground truth labels from the Afrobarometer Round 6 survey using Landsat
8 and Sentinel 1 satellite imagery.
&lt;/p&gt;
&lt;p&gt;Our best models predict infrastructure quality with AUROC scores of 0.881 on
Electricity, 0.862 on Sewerage, 0.739 on Piped Water, and 0.786 on Roads using
Landsat 8. These performances are significantly better than models that
leverage OpenStreetMap or nighttime light intensity on the same tasks. We also
demonstrate that our trained model can accurately make predictions in an unseen
country after fine-tuning on a small sample of images. Furthermore, the model
can be deployed in regions with limited samples to predict infrastructure
outcomes with higher performance than nearest neighbor spatial interpolation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oshri_B/0/1/0/all/0/1&quot;&gt;Barak Oshri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1&quot;&gt;Annie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adelson_P/0/1/0/all/0/1&quot;&gt;Peter Adelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dupas_P/0/1/0/all/0/1&quot;&gt;Pascaline Dupas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinstein_J/0/1/0/all/0/1&quot;&gt;Jeremy Weinstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1&quot;&gt;Marshall Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1&quot;&gt;David Lobell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item></rdf:RDF>