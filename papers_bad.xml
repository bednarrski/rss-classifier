<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02448"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06379"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01000"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.07604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10248"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00885"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01019"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01061"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.03928"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.00864"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.04934"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.11381"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07042"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00513"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1711.02448">
<title>Cortical microcircuits as gated-recurrent neural networks. (arXiv:1711.02448v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/1711.02448</link>
<description rdf:parseType="Literal">&lt;p&gt;Cortical circuits exhibit intricate recurrent architectures that are
remarkably similar across different brain areas. Such stereotyped structure
suggests the existence of common computational principles. However, such
principles have remained largely elusive. Inspired by gated-memory networks,
namely long short-term memory networks (LSTMs), we introduce a recurrent neural
network in which information is gated through inhibitory cells that are
subtractive (subLSTM). We propose a natural mapping of subLSTMs onto known
canonical excitatory-inhibitory cortical microcircuits. Our empirical
evaluation across sequential image classification and language modelling tasks
shows that subLSTM units can achieve similar performance to LSTM units. These
results suggest that cortical circuits can be optimised to solve complex
contextual problems and proposes a novel view on their computational function.
Overall our work provides a step towards unifying recurrent networks as used in
machine learning with their biological counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Costa_R/0/1/0/all/0/1&quot;&gt;Rui Ponte Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Assael_Y/0/1/0/all/0/1&quot;&gt;Yannis M. Assael&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shillingford_B/0/1/0/all/0/1&quot;&gt;Brendan Shillingford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Freitas_N/0/1/0/all/0/1&quot;&gt;Nando de Freitas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Vogels_T/0/1/0/all/0/1&quot;&gt;Tim P. Vogels&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06379">
<title>Improvements to context based self-supervised learning. (arXiv:1711.06379v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06379</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a set of methods to improve on the results of self-supervised
learning using context. We start with a baseline of patch based arrangement
context learning and go from there. Our methods address some overt problems
such as chromatic aberration as well as other potential problems such as
spatial skew and mid-level feature neglect. We prevent problems with testing
generalization on common self-supervised benchmark tests by using different
datasets during our development. The results of our methods combined yield top
scores on all standard self-supervised benchmarks, including classification and
detection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and &quot;linear
tests&quot; on the ImageNet and CSAIL Places datasets. We obtain an improvement over
our baseline method of between 4.0 to 7.1 percentage points on transfer
learning classification tests. We also show results on different standard
network architectures to demonstrate generalization as well as portability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mundhenk_T/0/1/0/all/0/1&quot;&gt;T. Nathan Mundhenk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1&quot;&gt;Daniel Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Barry Y. Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01000">
<title>ViZDoom: DRQN with Prioritized Experience Replay, Double-Q Learning, &amp; Snapshot Ensembling. (arXiv:1801.01000v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.01000</link>
<description rdf:parseType="Literal">&lt;p&gt;ViZDoom is a robust, first-person shooter reinforcement learning environment,
characterized by a significant degree of latent state information. In this
paper, double-Q learning and prioritized experience replay methods are tested
under a certain ViZDoom combat scenario using a competitive deep recurrent
Q-network (DRQN) architecture. In addition, an ensembling technique known as
snapshot ensembling is employed using a specific annealed learning rate to
observe differences in ensembling efficacy under these two methods. Annealed
learning rates are important in general to the training of deep neural network
models, as they shake up the status-quo and counter a model&apos;s tending towards
local optima. While both variants show performance exceeding those of built-in
AI agents of the game, the known stabilizing effects of double-Q learning are
illustrated, and priority experience replay is again validated in its
usefulness by showing immediate results early on in agent development, with the
caveat that value overestimation is accelerated in this case. In addition, some
unique behaviors are observed to develop for priority experience replay (PER)
and double-Q (DDQ) variants, and snapshot ensembling of both PER and DDQ proves
a valuable method for improving performance of the ViZDoom Marine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulze_C/0/1/0/all/0/1&quot;&gt;Christopher Schulze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulze_M/0/1/0/all/0/1&quot;&gt;Marcus Schulze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.07604">
<title>A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications. (arXiv:1709.07604v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.07604</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph is an important data representation which appears in a wide diversity
of real-world scenarios. Effective graph analytics provides users a deeper
understanding of what is behind the data, and thus can benefit a lot of useful
applications such as node classification, node recommendation, link prediction,
etc. However, most graph analytics methods suffer the high computation and
space cost. Graph embedding is an effective yet efficient way to solve the
graph analytics problem. It converts the graph data into a low dimensional
space in which the graph structural information and graph properties are
maximally preserved. In this survey, we conduct a comprehensive review of the
literature in graph embedding. We first introduce the formal definition of
graph embedding as well as the related concepts. After that, we propose two
taxonomies of graph embedding which correspond to what challenges exist in
different graph embedding problem settings and how the existing work address
these challenges in their solutions. Finally, we summarize the applications
that graph embedding enables and suggest four promising future research
directions in terms of computation efficiency, problem settings, techniques and
application scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Hongyun Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_V/0/1/0/all/0/1&quot;&gt;Vincent W. Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kevin Chen-Chuan Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10248">
<title>Deep Learning Interior Tomography for Region-of-Interest Reconstruction. (arXiv:1712.10248v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.10248</link>
<description rdf:parseType="Literal">&lt;p&gt;Interior tomography for the region-of-interest (ROI) imaging has advantages
of using a small detector and reducing X-ray radiation dose. However, standard
analytic reconstruction suffers from severe cupping artifacts due to existence
of null space in the truncated Radon transform. Existing penalized
reconstruction methods may address this problem but they require extensive
computations due to the iterative reconstruction. Inspired by the recent deep
learning approaches to low-dose and sparse view CT, here we propose a deep
learning architecture that removes null space signals from the FBP
reconstruction. Experimental results have shown that the proposed method
provides near-perfect reconstruction with about 7-10 dB improvement in PSNR
over existing methods in spite of significantly reduced run-time complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yoseob Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jawook Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00885">
<title>Gradient-based Optimization for Regression in the Functional Tensor-Train Format. (arXiv:1801.00885v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1801.00885</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the task of low-multilinear-rank functional regression, i.e.,
learning a low-rank parametric representation of functions from scattered
real-valued data. Our first contribution is the development and analysis of an
efficient gradient computation that enables gradient-based optimization
procedures, including stochastic gradient descent and quasi-Newton methods, for
learning the parameters of a functional tensor-train (FT). The functional
tensor-train uses the tensor-train (TT) representation of low-rank arrays as an
ansatz for a class of low-multilinear-rank functions. The FT is represented by
a set of matrix-valued functions that contain a set of univariate functions,
and the regression task is to learn the parameters of these univariate
functions. Our second contribution demonstrates that using nonlinearly
parameterized univariate functions, e.g., symmetric kernels with moving
centers, within each core can outperform the standard approach of using a
linear expansion of basis functions. Our final contributions are new rank
adaptation and group-sparsity regularization procedures to minimize
overfitting. We use several benchmark problems to demonstrate at least an order
of magnitude lower accuracy with gradient-based optimization methods than
standard alternating least squares procedures in the low-sample number regime.
We also demonstrate an order of magnitude reduction in accuracy on a test
problem resulting from using nonlinear parameterizations over linear
parameterizations. Finally we compare regression performance with 22 other
nonparametric and parametric regression methods on 10 real-world data sets. We
achieve top-five accuracy for seven of the data sets and best accuracy for two
of the data sets. These rankings are the best amongst parametric models and
competetive with the best non-parametric methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gorodetsky_A/0/1/0/all/0/1&quot;&gt;Alex A. Gorodetsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jakeman_J/0/1/0/all/0/1&quot;&gt;John D. Jakeman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01019">
<title>Proteomics Analysis of FLT3-ITD Mutation in Acute Myeloid Leukemia Using Deep Learning Neural Network. (arXiv:1801.01019v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1801.01019</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning can significantly benefit cancer proteomics and genomics. In
this study, we attempt to determine a set of critical proteins that are
associated with the FLT3-ITD mutation in newly-diagnosed acute myeloid leukemia
patients. A Deep Learning network consisting of autoencoders forming a
hierarchical model from which high-level features are extracted without labeled
training data. Dimensional reduction reduced the number of critical proteins
from 231 to 20. Deep Learning found an excellent correlation between FLT3-ITD
mutation with the levels of these 20 critical proteins (accuracy 97%,
sensitivity 90%, specificity 100%). Our Deep Learning network could hone in on
20 proteins with the strongest association with FLT3-ITD. The results of this
study allow a novel approach to determine critical protein pathways in the
FLT3-ITD mutation, and provide proof-of-concept for an accurate approach to
model big data in cancer proteomics and genomics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Christine A. Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wahed_A/0/1/0/all/0/1&quot;&gt;Amer Wahed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Andy N.D. Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01061">
<title>Intrinsic Gaussian processes on complex constrained domains. (arXiv:1801.01061v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.01061</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a class of intrinsic Gaussian processes (in-GPs) for
interpolation, regression and classification on manifolds with a primary focus
on complex constrained domains or irregular shaped spaces arising as subsets or
submanifolds of R, R2, R3 and beyond. For example, in-GPs can accommodate
spatial domains arising as complex subsets of Euclidean space. in-GPs respect
the potentially complex boundary or interior conditions as well as the
intrinsic geometry of the spaces. The key novelty of the proposed approach is
to utilise the relationship between heat kernels and the transition density of
Brownian motion on manifolds for constructing and approximating valid and
computationally feasible covariance kernels. This enables in-GPs to be
practically applied in great generality, while existing approaches for
smoothing on constrained domains are limited to simple special cases. The broad
utilities of the in-GP approach is illustrated through simulation studies and
data examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Niu_M/0/1/0/all/0/1&quot;&gt;Mu Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheung_P/0/1/0/all/0/1&quot;&gt;Pokman Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lizhen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zhenwen Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1&quot;&gt;Neil Lawrence&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dunson_D/0/1/0/all/0/1&quot;&gt;David Dunson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.03928">
<title>Hybrid Jacobian and Gauss-Seidel proximal block coordinate update methods for linearly constrained convex programming. (arXiv:1608.03928v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1608.03928</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed the rapid development of block coordinate update
(BCU) methods, which are particularly suitable for problems involving
large-sized data and/or variables. In optimization, BCU first appears as the
coordinate descent method that works well for smooth problems or those with
separable nonsmooth terms and/or separable constraints. As nonseparable
constraints exist, BCU can be applied under primal-dual settings.
&lt;/p&gt;
&lt;p&gt;In the literature, it has been shown that for weakly convex problems with
nonseparable linear constraint, BCU with fully Gauss-Seidel updating rule may
fail to converge and that with fully Jacobian rule can converge sublinearly.
However, empirically the method with Jacobian update is usually slower than
that with Gauss-Seidel rule. To maintain their advantages, we propose a hybrid
Jacobian and Gauss-Seidel BCU method for solving linearly constrained
multi-block structured convex programming, where the objective may have a
nonseparable quadratic term and separable nonsmooth terms. At each primal block
variable update, the method approximates the augmented Lagrangian function at
an affine combination of the previous two iterates, and the affinely mixing
matrix with desired nice properties can be chosen through solving a
semidefinite programming. We show that the hybrid method enjoys the theoretical
convergence guarantee as Jacobian BCU. In addition, we numerically demonstrate
that the method can perform as well as Gauss-Seidel method and better than a
recently proposed randomized primal-dual BCU method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yangyang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.00864">
<title>The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings. (arXiv:1703.00864v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.00864</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine a class of embeddings based on structured random matrices with
orthogonal rows which can be applied in many machine learning applications
including dimensionality reduction and kernel approximation. For both the
Johnson-Lindenstrauss transform and the angular kernel, we show that we can
select matrices yielding guaranteed improved performance in accuracy and/or
speed compared to earlier methods. We introduce matrices with complex entries
which give significant further accuracy improvement. We provide geometric and
Markov chain-based perspectives to help understand the benefits, and empirical
results which suggest that the approach is helpful in a wider range of
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Choromanski_K/0/1/0/all/0/1&quot;&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rowland_M/0/1/0/all/0/1&quot;&gt;Mark Rowland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.04934">
<title>RADNET: Radiologist Level Accuracy using Deep Learning for HEMORRHAGE detection in CT Scans. (arXiv:1710.04934v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1710.04934</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a deep learning approach for automated brain hemorrhage detection
from computed tomography (CT) scans. Our model emulates the procedure followed
by radiologists to analyse a 3D CT scan in real-world. Similar to radiologists,
the model sifts through 2D cross-sectional slices while paying close attention
to potential hemorrhagic regions. Further, the model utilizes 3D context from
neighboring slices to improve predictions at each slice and subsequently,
aggregates the slice-level predictions to provide diagnosis at CT level. We
refer to our proposed approach as Recurrent Attention DenseNet (RADnet) as it
employs original DenseNet architecture along with adding the components of
attention for slice level predictions and recurrent neural network layer for
incorporating 3D context. The real-world performance of RADnet has been
benchmarked against independent analysis performed by three senior radiologists
for 77 brain CTs. RADnet demonstrates 81.82% hemorrhage prediction accuracy at
CT level that is comparable to radiologists. Further, RADnet achieves higher
recall than two of the three radiologists, which is remarkable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grewal_M/0/1/0/all/0/1&quot;&gt;Monika Grewal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1&quot;&gt;Muktabh Mayank Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1&quot;&gt;Pulkit Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varadarajan_S/0/1/0/all/0/1&quot;&gt;Srikrishna Varadarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.11381">
<title>Semantic Interpolation in Implicit Models. (arXiv:1710.11381v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.11381</link>
<description rdf:parseType="Literal">&lt;p&gt;In implicit models, one often interpolates between sampled points in latent
space. As we show in this paper, care needs to be taken to match-up the
distributional assumptions on code vectors with the geometry of the
interpolating paths. Otherwise, typical assumptions about the quality and
semantics of in-between points may not be justified. Based on our analysis we
propose to modify the prior code distribution to put significantly more
probability mass closer to the origin. As a result, linear interpolation paths
are not only shortest paths, but they are also guaranteed to pass through
high-density regions, irrespective of the dimensionality of the latent space.
Experiments on standard benchmark image datasets demonstrate clear visual
improvements in the quality of the generated samples and exhibit more
meaningful interpolation paths.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilcher_Y/0/1/0/all/0/1&quot;&gt;Yannic Kilcher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucchi_A/0/1/0/all/0/1&quot;&gt;Aurelien Lucchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1&quot;&gt;Thomas Hofmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07042">
<title>Development and evaluation of a deep learning model for protein-ligand binding affinity prediction. (arXiv:1712.07042v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07042</link>
<description rdf:parseType="Literal">&lt;p&gt;Structure based ligand discovery is one of the most successful approaches for
augmenting the drug discovery process. Currently, there is a notable shift
towards machine learning (ML) methodologies to aid such procedures. Deep
learning has recently gained considerable attention as it allows the model to
&quot;learn&quot; to extract features that are relevant for the task at hand. We have
developed a novel deep neural network estimating the binding affinity of
ligand-receptor complexes. The complex is represented with a 3D grid, and the
model utilizes a 3D convolution to produce a feature map of this
representation, treating the atoms of both proteins and ligands in the same
manner. Our network was tested on the CASF &quot;scoring power&quot; benchmark and Astex
Diverse Set and outperformed classical scoring functions. The model, together
with usage instructions and examples, is available as a git repository at
&lt;a href=&quot;http://gitlab.com/cheminfIBB/pafnucy&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stepniewska_Dziubinska_M/0/1/0/all/0/1&quot;&gt;Marta M. Stepniewska-Dziubinska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zielenkiewicz_P/0/1/0/all/0/1&quot;&gt;Piotr Zielenkiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siedlecki_P/0/1/0/all/0/1&quot;&gt;Pawel Siedlecki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00513">
<title>An elementary derivation of the Chinese restaurant process from Sethuraman&apos;s stick-breaking process. (arXiv:1801.00513v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1801.00513</link>
<description rdf:parseType="Literal">&lt;p&gt;The Chinese restaurant process and the stick-breaking process are the two
most commonly used representations of the Dirichlet process. However, the usual
proof of the connection between them is indirect, relying on abstract
properties of the Dirichlet process that are difficult for nonexperts to
verify. This short note provides a direct proof that the stick-breaking process
gives rise to the Chinese restaurant process, without using any measure theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Miller_J/0/1/0/all/0/1&quot;&gt;Jeffrey W. Miller&lt;/a&gt;</dc:creator>
</item></rdf:RDF>