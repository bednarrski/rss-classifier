<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01137"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01150"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11215"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00981"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00634"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02303"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1607.07762"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00959"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00973"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01006"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01056"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01095"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01126"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01145"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01181"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01199"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03437"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06567"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09318"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04097"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01623"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02128"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07987"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09386"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.01137">
<title>When Does Hillclimbing Fail on Monotone Functions: An entropy compression argument. (arXiv:1808.01137v1 [math.PR])</title>
<link>http://arxiv.org/abs/1808.01137</link>
<description rdf:parseType="Literal">&lt;p&gt;Hillclimbing is an essential part of any optimization algorithm. An important
benchmark for hillclimbing algorithms on pseudo-Boolean functions $f: \{0,1\}^n
\to \mathbb{R}$ are (strictly) montone functions, on which a surprising number
of hillclimbers fail to be efficient. For example, the $(1+1)$-Evolutionary
Algorithm is a standard hillclimber which flips each bit independently with
probability $c/n$ in each round. Perhaps surprisingly, this algorithm shows a
phase transition: it optimizes any monotone pseudo-boolean function in
quasilinear time if $c&amp;lt;1$, but there are monotone functions for which the
algorithm needs exponential time if $c&amp;gt;2.2$. But so far it was unclear whether
the threshold is at $c=1$.
&lt;/p&gt;
&lt;p&gt;In this paper we show how Moser&apos;s entropy compression argument can be adapted
to this situation, that is, we show that a long runtime would allow us to
encode the random steps of the algorithm with less bits than their entropy.
Thus there exists a $c_0 &amp;gt; 1$ such that for all $0&amp;lt;c\le c_0$ the
$(1+1)$-Evolutionary Algorithm with rate $c/n$ finds the optimum in $O(n \log^2
n)$ steps in expectation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lengler_J/0/1/0/all/0/1&quot;&gt;Johannes Lengler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Martinsson_A/0/1/0/all/0/1&quot;&gt;Anders Martinsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Steger_A/0/1/0/all/0/1&quot;&gt;Angelika Steger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01150">
<title>A Two-Dimensional (2-D) Learning Framework for Particle Swarm based Feature Selection. (arXiv:1808.01150v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.01150</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a new generalized two dimensional learning approach for
particle swarm based feature selection. The core idea of the proposed approach
is to include the information about the subset cardinality into the learning
framework by extending the dimension of the velocity. The 2D-learning framework
retains all the key features of the original PSO, despite the extra learning
dimension. Most of the popular variants of PSO can easily be adapted into this
2D learning framework for feature selection problems. The efficacy of the
proposed learning approach has been evaluated considering several benchmark
data and two induction algorithms: Naive-Bayes and k-Nearest Neighbor. The
results of the comparative investigation including the time-complexity analysis
with GA, ACO and five other PSO variants illustrate that the proposed 2D
learning approach gives feature subset with relatively smaller cardinality and
better classification performance with shorter run times.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hafiz_F/0/1/0/all/0/1&quot;&gt;Faizal Hafiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swain_A/0/1/0/all/0/1&quot;&gt;Akshya Swain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1&quot;&gt;Nitish Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_C/0/1/0/all/0/1&quot;&gt;Chirag Naik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11215">
<title>CAKE: Compact and Accurate K-dimensional representation of Emotion. (arXiv:1807.11215v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1807.11215</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous models describing the human emotional states have been built by the
psychology community. Alongside, Deep Neural Networks (DNN) are reaching
excellent performances and are becoming interesting features extraction tools
in many computer vision tasks.Inspired by works from the psychology community,
we first study the link between the compact two-dimensional representation of
the emotion known as arousal-valence, and discrete emotion classes (e.g. anger,
happiness, sadness, etc.) used in the computer vision community. It enables to
assess the benefits -- in terms of discrete emotion inference -- of adding an
extra dimension to arousal-valence (usually named dominance). Building on these
observations, we propose CAKE, a 3-dimensional representation of emotion
learned in a multi-domain fashion, achieving accurate emotion recognition on
several public datasets. Moreover, we visualize how emotions boundaries are
organized inside DNN representations and show that DNNs are implicitly learning
arousal-valence-like descriptions of emotions. Finally, we use the CAKE
representation to compare the quality of the annotations of different public
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kervadec_C/0/1/0/all/0/1&quot;&gt;Corentin Kervadec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vielzeuf_V/0/1/0/all/0/1&quot;&gt;Valentin Vielzeuf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pateux_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Pateux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lechervy_A/0/1/0/all/0/1&quot;&gt;Alexis Lechervy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurie_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;ric Jurie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00981">
<title>Constructionist Steps Towards an Autonomously Empathetic System. (arXiv:1808.00981v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.00981</link>
<description rdf:parseType="Literal">&lt;p&gt;Prior efforts to create an autonomous computer system capable of predicting
what a human being is thinking or feeling from facial expression data have been
largely based on outdated, inaccurate models of how emotions work that rely on
many scientifically questionable assumptions. In our research, we are creating
an empathetic system that incorporates the latest provable scientific
understanding of emotions: that they are constructs of the human mind, rather
than universal expressions of distinct internal states. Thus, our system uses a
user-dependent method of analysis and relies heavily on contextual information
to make predictions about what subjects are experiencing. Our system&apos;s accuracy
and therefore usefulness are built on provable ground truths that prohibit the
drawing of inaccurate conclusions that other systems could too easily make.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buteau_T/0/1/0/all/0/1&quot;&gt;Trevor Buteau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyons_D/0/1/0/all/0/1&quot;&gt;Damian Lyons&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00634">
<title>A Probabilistic Extension of Action Language BC+. (arXiv:1805.00634v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00634</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a probabilistic extension of action language BC+. Just like BC+ is
defined as a high-level notation of answer set programs for describing
transition systems, the proposed language, which we call pBC+, is defined as a
high-level notation of LPMLN programs---a probabilistic extension of answer set
programs. We show how probabilistic reasoning about transition systems, such as
prediction, postdiction, and planning problems, as well as probabilistic
diagnosis for dynamic domains, can be modeled in pBC+ and computed using an
implementation of LPMLN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joohyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02303">
<title>A survey on policy search algorithms for learning robot controllers in a handful of trials. (arXiv:1807.02303v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1807.02303</link>
<description rdf:parseType="Literal">&lt;p&gt;Most policy search algorithms require thousands of training episodes to find
an effective policy, which is often infeasible with a physical robot. This
survey article focuses on the extreme other end of the spectrum: how can a
robot adapt with only a handful of trials (a dozen) and a few minutes? By
analogy with the word &quot;big-data&quot;, we refer to this challenge as &quot;micro-data
reinforcement learning&quot;. We show that a first strategy is to leverage prior
knowledge on the policy structure (e.g., dynamic movement primitives), on the
policy parameters (e.g., demonstrations), or on the dynamics (e.g.,
simulators). A second strategy is to create data-driven surrogate models of the
expected reward (e.g., Bayesian optimization) or the dynamical model (e.g.,
model-based policy search), so that the policy optimizer queries the model
instead of the real system. Overall, all successful micro-data algorithms
combine these two strategies by varying the kind of model and prior knowledge.
The current scientific challenges essentially revolve around scaling up to
complex robots (e.g., humanoids), designing generic priors, and optimizing the
computing time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatzilygeroudis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Chatzilygeroudis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vassiliades_V/0/1/0/all/0/1&quot;&gt;Vassilis Vassiliades&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stulp_F/0/1/0/all/0/1&quot;&gt;Freek Stulp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calinon_S/0/1/0/all/0/1&quot;&gt;Sylvain Calinon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Mouret&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1607.07762">
<title>Focused Model-Learning and Planning for Non-Gaussian Continuous State-Action Systems. (arXiv:1607.07762v4 [cs.AI] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1607.07762</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a framework for model learning and planning in stochastic
domains with continuous state and action spaces and non-Gaussian transition
models. It is efficient because (1) local models are estimated only when the
planner requires them; (2) the planner focuses on the most relevant states to
the current planning problem; and (3) the planner focuses on the most
informative and/or high-value actions. Our theoretical analysis shows the
validity and asymptotic optimality of the proposed approach. Empirically, we
demonstrate the effectiveness of our algorithm on a simulated multi-modal
pushing problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie Pack Kaelbling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Lozano-P&amp;#xe9;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00959">
<title>Histogram Transform-based Speaker Identification. (arXiv:1808.00959v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1808.00959</link>
<description rdf:parseType="Literal">&lt;p&gt;A novel text-independent speaker identification (SI) method is proposed. This
method uses the Mel-frequency Cepstral coefficients (MFCCs) and the dynamic
information among adjacent frames as feature sets to capture speaker&apos;s
characteristics. In order to utilize dynamic information, we design super-MFCCs
features by cascading three neighboring MFCCs frames together. The probability
density function (PDF) of these super-MFCCs features is estimated by the
recently proposed histogram transform~(HT) method, which generates more
training data by random transforms to realize the histogram PDF estimation and
recedes the commonly occurred discontinuity problem in multivariate histograms
computing. Compared to the conventional PDF estimation methods, such as
Gaussian mixture models, the HT model shows promising improvement in the SI
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00973">
<title>Likelihood-free inference with an improved cross-entropy estimator. (arXiv:1808.00973v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.00973</link>
<description rdf:parseType="Literal">&lt;p&gt;We extend recent work (Brehmer, et. al., 2018) that use neural networks as
surrogate models for likelihood-free inference. As in the previous work, we
exploit the fact that the joint likelihood ratio and joint score, conditioned
on both observed and latent variables, can often be extracted from an implicit
generative model or simulator to augment the training data for these surrogate
models. We show how this augmented training data can be used to provide a new
cross-entropy estimator, which provides improved sample efficiency compared to
previous loss functions exploiting this augmented training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stoye_M/0/1/0/all/0/1&quot;&gt;Markus Stoye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brehmer_J/0/1/0/all/0/1&quot;&gt;Johann Brehmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Louppe_G/0/1/0/all/0/1&quot;&gt;Gilles Louppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pavez_J/0/1/0/all/0/1&quot;&gt;Juan Pavez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cranmer_K/0/1/0/all/0/1&quot;&gt;Kyle Cranmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01006">
<title>A Hybrid Variational Autoencoder for Collaborative Filtering. (arXiv:1808.01006v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.01006</link>
<description rdf:parseType="Literal">&lt;p&gt;In today&apos;s day and age when almost every industry has an online presence with
users interacting in online marketplaces, personalized recommendations have
become quite important. Traditionally, the problem of collaborative filtering
has been tackled using Matrix Factorization which is linear in nature. We
extend the work of [11] on using variational autoencoders (VAEs) for
collaborative filtering with implicit feedback by proposing a hybrid,
multi-modal approach. Our approach combines movie embeddings (learned from a
sibling VAE network) with user ratings from the Movielens 20M dataset and
applies it to the task of movie recommendation. We empirically show how the VAE
network is empowered by incorporating movie embeddings. We also visualize movie
and user embeddings by clustering their latent representations obtained from a
VAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1&quot;&gt;Kilol Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghuprasad_M/0/1/0/all/0/1&quot;&gt;Mukund Yelahanka Raghuprasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1&quot;&gt;Pankhuri Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01056">
<title>Robust Regression for Automatic Fusion Plasma Analysis based on Generative Modeling. (arXiv:1808.01056v1 [physics.plasm-ph])</title>
<link>http://arxiv.org/abs/1808.01056</link>
<description rdf:parseType="Literal">&lt;p&gt;The first step to realize automatic experimental data analysis for fusion
plasma experiments is fitting noisy data of temperature and density spatial
profiles, which are obtained routinely. However, it has been difficult to
construct algorithms that fit all the data without over- and under-fitting. In
this paper, we show that this difficulty originates from the lack of knowledge
of the probability distribution that the measurement data follow. We
demonstrate the use of a machine learning technique to estimate the data
distribution and to construct an optimal generative model. We show that the
fitting algorithm based on the generative modeling outperforms classical
heuristic methods in terms of the stability as well as the accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Fujii_K/0/1/0/all/0/1&quot;&gt;Keisuke Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Suzuki_C/0/1/0/all/0/1&quot;&gt;Chihiro Suzuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hasuo_M/0/1/0/all/0/1&quot;&gt;Masahiro Hasuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01095">
<title>Helix: Accelerating Human-in-the-loop Machine Learning. (arXiv:1808.01095v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01095</link>
<description rdf:parseType="Literal">&lt;p&gt;Data application developers and data scientists spend an inordinate amount of
time iterating on machine learning (ML) workflows -- by modifying the data
pre-processing, model training, and post-processing steps -- via
trial-and-error to achieve the desired model performance. Existing work on
accelerating machine learning focuses on speeding up one-shot execution of
workflows, failing to address the incremental and dynamic nature of typical ML
development. We propose Helix, a declarative machine learning system that
accelerates iterative development by optimizing workflow execution end-to-end
and across iterations. Helix minimizes the runtime per iteration via program
analysis and intelligent reuse of previous results, which are selectively
materialized -- trading off the cost of materialization for potential future
benefits -- to speed up future iterations. Additionally, Helix offers a
graphical interface to visualize workflow DAGs and compare versions to
facilitate iterative development. Through two ML applications, in
classification and in structured prediction, attendees will experience the
succinctness of Helix programming interface and the speed and ease of iterative
development using Helix. In our evaluations, Helix achieved up to an order of
magnitude reduction in cumulative run time compared to state-of-the-art machine
learning tools.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_D/0/1/0/all/0/1&quot;&gt;Doris Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Litian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jialin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macke_S/0/1/0/all/0/1&quot;&gt;Stephen Macke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuchen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parameswaran_A/0/1/0/all/0/1&quot;&gt;Aditya Parameswaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01126">
<title>Information-Theoretic Scoring Rules to Learn Additive Bayesian Network Applied to Epidemiology. (arXiv:1808.01126v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.01126</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian network modelling is a well adapted approach to study messy and
highly correlated datasets which are very common in, e.g., systems
epidemiology. A popular approach to learn a Bayesian network from an
observational datasets is to identify the maximum a posteriori network in a
search-and-score approach. Many scores have been proposed both Bayesian or
frequentist based. In an applied perspective, a suitable approach would allow
multiple distributions for the data and is robust enough to run autonomously. A
promising framework to compute scores are generalized linear models. Indeed,
there exists fast algorithms for estimation and many tailored solutions to
common epidemiological issues. The purpose of this paper is to present an R
package abn that has an implementation of multiple frequentist scores and some
realistic simulations that show its usability and performance. It includes
features to deal efficiently with data separation and adjustment which are very
common in systems epidemiology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kratzer_G/0/1/0/all/0/1&quot;&gt;Gilles Kratzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Furrer_R/0/1/0/all/0/1&quot;&gt;Reinhard Furrer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01145">
<title>Hoeffding Trees with nmin adaptation. (arXiv:1808.01145v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01145</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning software accounts for a significant amount of energy
consumed in data centers. These algorithms are usually optimized towards
predictive performance, i.e. accuracy, and scalability. This is the case of
data stream mining algorithms. Although these algorithms are adaptive to the
incoming data, they have fixed parameters from the beginning of the execution.
We have observed that having fixed parameters lead to unnecessary computations,
thus making the algorithm energy inefficient. In this paper we present the nmin
adaptation method for Hoeffding trees. This method adapts the value of the nmin
parameter, which significantly affects the energy consumption of the algorithm.
The method reduces unnecessary computations and memory accesses, thus reducing
the energy, while the accuracy is only marginally affected. We experimentally
compared VFDT (Very Fast Decision Tree, the first Hoeffding tree algorithm) and
CVFDT (Concept-adapting VFDT) with the VFDT-nmin (VFDT with nmin adaptation).
The results show that VFDT-nmin consumes up to 27% less energy than the
standard VFDT, and up to 92% less energy than CVFDT, trading off a few percent
of accuracy in a few datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Martin_E/0/1/0/all/0/1&quot;&gt;Eva Garc&amp;#xed;a-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavesson_N/0/1/0/all/0/1&quot;&gt;Niklas Lavesson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grahn_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe5;kan Grahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casalicchio_E/0/1/0/all/0/1&quot;&gt;Emiliano Casalicchio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boeva_V/0/1/0/all/0/1&quot;&gt;Veselka Boeva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01181">
<title>Robust Spectral Filtering and Anomaly Detection. (arXiv:1808.01181v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.01181</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a setting, where the output of a linear dynamical system (LDS)
is, with an unknown but fixed probability, replaced by noise. There, we present
a robust method for the prediction of the outputs of the LDS and identification
of the samples of noise, and prove guarantees on its statistical performance.
One application lies in anomaly detection: the samples of noise, unlikely to
have been generated by the dynamics, can be flagged to operators of the system
for further study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marecek_J/0/1/0/all/0/1&quot;&gt;Jakub Marecek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tchrakian_T/0/1/0/all/0/1&quot;&gt;Tigran Tchrakian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01199">
<title>Generation Meets Recommendation: Proposing Novel Items for Groups of Users. (arXiv:1808.01199v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.01199</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider a movie studio aiming to produce a set of new movies for summer
release: What types of movies it should produce? Who would the movies appeal
to? How many movies should it make? Similar issues are encountered by a variety
of organizations, e.g., mobile-phone manufacturers and online magazines, who
have to create new (non-existent) items to satisfy groups of users with
different preferences. In this paper, we present a joint problem formalization
of these interrelated issues, and propose generative methods that address these
questions simultaneously. Specifically, we leverage the latent space obtained
by training a deep generative model---the Variational Autoencoder (VAE)---via a
loss function that incorporates both rating performance and item reconstruction
terms. We then apply a greedy search algorithm that utilizes this learned
latent space to jointly obtain K plausible new items, and user groups that
would find the items appealing. An evaluation of our methods on a synthetic
dataset indicates that our approach is able to generate novel items similar to
highly-desirable unobserved items. As case studies on real-world data, we
applied our method on the MART abstract art and Movielens Tag Genome dataset,
which resulted in promising results: small and diverse sets of novel items.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thanh_V/0/1/0/all/0/1&quot;&gt;Vinh Vo Thanh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soh_H/0/1/0/all/0/1&quot;&gt;Harold Soh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03437">
<title>Approximation beats concentration? An approximation view on inference with smooth radial kernels. (arXiv:1801.03437v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03437</link>
<description rdf:parseType="Literal">&lt;p&gt;Positive definite kernels and their associated Reproducing Kernel Hilbert
Spaces provide a mathematically compelling and practically competitive
framework for learning from data.
&lt;/p&gt;
&lt;p&gt;In this paper we take the approximation theory point of view to explore
various aspects of smooth kernels related to their inferential properties. We
analyze eigenvalue decay of kernels operators and matrices, properties of
eigenfunctions/eigenvectors and &quot;Fourier&quot; coefficients of functions in the
kernel space restricted to a discrete set of data points. We also investigate
the fitting capacity of kernels, giving explicit bounds on the fat shattering
dimension of the balls in Reproducing Kernel Hilbert spaces. Interestingly, the
same properties that make kernels very effective approximators for functions in
their &quot;native&quot; kernel space, also limit their capacity to represent arbitrary
functions. We discuss various implications, including those for gradient
descent type methods.
&lt;/p&gt;
&lt;p&gt;It is important to note that most of our bounds are measure independent.
Moreover, at least in moderate dimension, the bounds for eigenvalues are much
tighter than the bounds which can be obtained from the usual matrix
concentration results. For example, we see that the eigenvalues of kernel
matrices show nearly exponential decay with constants depending only on the
kernel and the domain. We call this &quot;approximation beats concentration&quot;
phenomenon as even when the data are sampled from a probability distribution,
some of their aspects are better understood in terms of approximation theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1&quot;&gt;Mikhail Belkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06567">
<title>A Dual Approach to Scalable Verification of Deep Networks. (arXiv:1803.06567v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06567</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the problem of formally verifying desirable properties
of neural networks, i.e., obtaining provable guarantees that neural networks
satisfy specifications relating their inputs and outputs (robustness to bounded
norm adversarial perturbations, for example). Most previous work on this topic
was limited in its applicability by the size of the network, network
architecture and the complexity of properties to be verified. In contrast, our
framework applies to a general class of activation functions and specifications
on neural network inputs and outputs. We formulate verification as an
optimization problem (seeking to find the largest violation of the
specification) and solve a Lagrangian relaxation of the optimization problem to
obtain an upper bound on the worst case violation of the specification being
verified. Our approach is anytime i.e. it can be stopped at any time and a
valid bound on the maximum violation can be obtained. We develop specialized
verification algorithms with provable tightness guarantees under special
assumptions and demonstrate the practical significance of our general
verification approach on a variety of verification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy/0/1/0/all/0/1&quot;&gt;Krishnamurthy&lt;/a&gt; (Dj) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dvijotham/0/1/0/all/0/1&quot;&gt;Dvijotham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanforth_R/0/1/0/all/0/1&quot;&gt;Robert Stanforth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1&quot;&gt;Sven Gowal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mann_T/0/1/0/all/0/1&quot;&gt;Timothy Mann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohli_P/0/1/0/all/0/1&quot;&gt;Pushmeet Kohli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09318">
<title>Data-driven Discovery of Closure Models. (arXiv:1803.09318v2 [math.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09318</link>
<description rdf:parseType="Literal">&lt;p&gt;Derivation of reduced order representations of dynamical systems requires the
modeling of the truncated dynamics on the retained dynamics. In its most
general form, this so-called closure model has to account for memory effects.
In this work, we present a framework of operator inference to extract the
governing dynamics of closure from data in a compact, non-Markovian form. We
employ sparse polynomial regression and artificial neural networks to extract
the underlying operator. For a special class of non-linear systems,
observability of the closure in terms of the resolved dynamics is analyzed and
theoretical results are presented on the compactness of the memory. The
proposed framework is evaluated on examples consisting of linear to nonlinear
systems with and without chaotic dynamics, with an emphasis on predictive
performance on unseen data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shaowu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Duraisamy_K/0/1/0/all/0/1&quot;&gt;Karthik Duraisamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04097">
<title>End-to-end Deep Learning of Optical Fiber Communications. (arXiv:1804.04097v3 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04097</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we implement an optical fiber communication system as an
end-to-end deep neural network, including the complete chain of transmitter,
channel model, and receiver. This approach enables the optimization of the
transceiver in a single end-to-end process. We illustrate the benefits of this
method by applying it to intensity modulation/direct detection (IM/DD) systems
and show that we can achieve bit error rates below the 6.7\% hard-decision
forward error correction (HD-FEC) threshold. We model all componentry of the
transmitter and receiver, as well as the fiber channel, and apply deep learning
to find transmitter and receiver configurations minimizing the symbol error
rate. We propose and verify in simulations a training method that yields robust
and flexible transceivers that allow---without reconfiguration---reliable
transmission over a large range of link dispersions. The results from
end-to-end deep learning are successfully verified for the first time in an
experiment. In particular, we achieve information rates of 42\,Gb/s below the
HD-FEC threshold at distances beyond 40\,km. We find that our results
outperform conventional IM/DD solutions based on 2 and 4 level pulse amplitude
modulation (PAM2/PAM4) with feedforward equalization (FFE) at the receiver. Our
study is the first step towards end-to-end deep learning-based optimization of
optical fiber communication systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karanov_B/0/1/0/all/0/1&quot;&gt;Boris Karanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chagnon_M/0/1/0/all/0/1&quot;&gt;Mathieu Chagnon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thouin_F/0/1/0/all/0/1&quot;&gt;F&amp;#xe9;lix Thouin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eriksson_T/0/1/0/all/0/1&quot;&gt;Tobias A. Eriksson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulow_H/0/1/0/all/0/1&quot;&gt;Henning B&amp;#xfc;low&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavery_D/0/1/0/all/0/1&quot;&gt;Domani&amp;#xe7; Lavery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayvel_P/0/1/0/all/0/1&quot;&gt;Polina Bayvel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmalen_L/0/1/0/all/0/1&quot;&gt;Laurent Schmalen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01623">
<title>Modeling outcomes of soccer matches. (arXiv:1807.01623v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01623</link>
<description rdf:parseType="Literal">&lt;p&gt;We compare various extensions of the Bradley-Terry model and a hierarchical
Poisson log-linear model in terms of their performance in predicting the
outcome of soccer matches (win, draw, or loss). The parameters of the
Bradley-Terry extensions are estimated by maximizing the log-likelihood, or an
appropriately penalized version of it, while the posterior densities of the
parameters of the hierarchical Poisson log-linear model are approximated using
integrated nested Laplace approximations. The prediction performance of the
various modeling approaches is assessed using a novel, context-specific
framework for temporal validation that is found to deliver accurate estimates
of the test error. The direct modeling of outcomes via the various
Bradley-Terry extensions and the modeling of match scores using the
hierarchical Poisson log-linear model demonstrate similar behavior in terms of
predictive performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tsokos_A/0/1/0/all/0/1&quot;&gt;Alkeos Tsokos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Narayanan_S/0/1/0/all/0/1&quot;&gt;Santhosh Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kosmidis_I/0/1/0/all/0/1&quot;&gt;Ioannis Kosmidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baio_G/0/1/0/all/0/1&quot;&gt;Gianluca Baio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cucuringu_M/0/1/0/all/0/1&quot;&gt;Mihai Cucuringu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Whitaker_G/0/1/0/all/0/1&quot;&gt;Gavin Whitaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kiraly_F/0/1/0/all/0/1&quot;&gt;Franz J. Kir&amp;#xe1;ly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02128">
<title>Adaptive Path-Integral Autoencoder: Representation Learning and Planning for Dynamical Systems. (arXiv:1807.02128v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.02128</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a representation learning algorithm that learns a low-dimensional
latent dynamical system from high-dimensional sequential raw data, e.g., video.
The framework builds upon recent advances in amortized inference methods that
use both an inference network and a refinement procedure to output samples from
a variational distribution given an observation sequence, and takes advantage
of the duality between control and inference to approximately solve the
intractable inference problem using the path integral control approach. The
learned dynamical model can be used to predict and plan the future states; we
also present the efficient planning method that exploits the learned
low-dimensional latent dynamics. Numerical experiments show that the proposed
path-integral control based variational inference method leads to tighter lower
bounds in statistical model learning of sequential data. The supplementary
video can be found at https://youtu.be/NLM0GQ0o4jM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1&quot;&gt;Jung-Su Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Young-Jin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chae_H/0/1/0/all/0/1&quot;&gt;Hyeok-Joo Chae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Soon-Seo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Han-Lim Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07987">
<title>Deep Learning. (arXiv:1807.07987v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.07987</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) is a high dimensional data reduction technique for
constructing high-dimensional predictors in input-output models. DL is a form
of machine learning that uses hierarchical layers of latent features. In this
article, we review the state-of-the-art of deep learning from a modeling and
algorithmic perspective. We provide a list of successful areas of applications
in Artificial Intelligence (AI), Image Processing, Robotics and Automation.
Deep learning is predictive in its nature rather then inferential and can be
viewed as a black-box methodology for high-dimensional function estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Polson_N/0/1/0/all/0/1&quot;&gt;Nicholas G. Polson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sokolov_V/0/1/0/all/0/1&quot;&gt;Vadim O. Sokolov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09386">
<title>On the Randomized Complexity of Minimizing a Convex Quadratic Function. (arXiv:1807.09386v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.09386</link>
<description rdf:parseType="Literal">&lt;p&gt;Minimizing a convex, quadratic objective is a fundamental problem in machine
learning and optimization. In this work, we study prove
$\textit{information-theoretic}$, gradient query complexity lower bounds for
minimizing convex quadratic functions, which, unlike prior works, apply even
for $\textit{randomized}$ algorithms. Specifically, we construct a distribution
over quadratic functions that witnesses lower bounds which match those known
for deterministic algorithms, up to multiplicative constants. The distribution
which witnesses our lower bound is in fact quite benign: it is both closed
form, and derived from classical ensembles in random matrix theory. We believe
that our construction constitutes a plausible &quot;average case&quot; setting, and thus
provides compelling evidence that the worst case and average case complexity of
convex-quadratic optimization are essentially identical.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1&quot;&gt;Max Simchowitz&lt;/a&gt;</dc:creator>
</item></rdf:RDF>