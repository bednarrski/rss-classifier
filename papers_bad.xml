<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02726"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02797"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02805"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02827"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02901"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02940"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03003"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03039"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03058"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.03296"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.04046"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.06690"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07970"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06957"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02736"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02788"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02858"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02937"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02939"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02949"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02950"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02961"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02982"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03050"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.04181"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.05193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01711"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.01955"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.08464"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.02726">
<title>Near Maximum Likelihood Decoding with Deep Learning. (arXiv:1801.02726v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1801.02726</link>
<description rdf:parseType="Literal">&lt;p&gt;A novel and efficient neural decoder algorithm is proposed. The proposed
decoder is based on the neural Belief Propagation algorithm and the
Automorphism Group. By combining neural belief propagation with permutations
from the Automorphism Group we achieve near maximum likelihood performance for
High Density Parity Check codes. Moreover, the proposed decoder significantly
improves the decoding complexity, compared to our earlier work on the topic. We
also investigate the training process and show how it can be accelerated.
Simulations of the hessian and the condition number show why the learning
process is accelerated. We demonstrate the decoding algorithm for various
linear block codes of length up to 63 bits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1&quot;&gt;Eliya Nachmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachar_Y/0/1/0/all/0/1&quot;&gt;Yaron Bachar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marciano_E/0/1/0/all/0/1&quot;&gt;Elad Marciano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burshtein_D/0/1/0/all/0/1&quot;&gt;David Burshtein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beery_Y/0/1/0/all/0/1&quot;&gt;Yair Be&amp;#x27;ery&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02797">
<title>Dendritic-Inspired Processing Enables Bio-Plausible STDP in Compound Binary Synapses. (arXiv:1801.02797v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.02797</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain-inspired learning mechanisms, e.g. spike timing dependent plasticity
(STDP), enable agile and fast on-the-fly adaptation capability in a spiking
neural network. When incorporating emerging nanoscale resistive non-volatile
memory (NVM) devices, with ultra-low power consumption and high-density
integration capability, a spiking neural network hardware would result in
several orders of magnitude reduction in energy consumption at a very small
form factor and potentially herald autonomous learning machines. However,
actual memory devices have shown to be intrinsically binary with stochastic
switching, and thus impede the realization of ideal STDP with continuous analog
values. In this work, a dendritic-inspired processing architecture is proposed
in addition to novel CMOS neuron circuits. The utilization of spike
attenuations and delays transforms the traditionally undesired stochastic
behavior of binary NVMs into a useful leverage that enables
biologically-plausible STDP learning. As a result, this work paves a pathway to
adopt practical binary emerging NVM devices in brain-inspired neuromorphic
computing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_V/0/1/0/all/0/1&quot;&gt;Vishal Saxena&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02805">
<title>DeepTraffic: Driving Fast through Dense Traffic with Deep Reinforcement Learning. (arXiv:1801.02805v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.02805</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a micro-traffic simulation (named &quot;DeepTraffic&quot;) where the
perception, control, and planning systems for one of the cars are all handled
by a single neural network as part of a model-free, off-policy reinforcement
learning process. The primary goal of DeepTraffic is to make the hands-on study
of deep reinforcement learning accessible to thousands of students, educators,
and researchers in order to inspire and fuel the exploration and evaluation of
DQN variants and hyperparameter configurations through large-scale, open
competition. This paper investigates the crowd-sourced hyperparameter tuning of
the policy network that resulted from the first iteration of the DeepTraffic
competition where thousands of participants actively searched through the
hyperparameter space with the objective of their neural network submission to
make it onto the top-10 leaderboard.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fridman_L/0/1/0/all/0/1&quot;&gt;Lex Fridman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenik_B/0/1/0/all/0/1&quot;&gt;Benedikt Jenik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terwilliger_J/0/1/0/all/0/1&quot;&gt;Jack Terwilliger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02827">
<title>Novel Methods for Enhancing the Performance of Genetic Algorithms. (arXiv:1801.02827v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.02827</link>
<description rdf:parseType="Literal">&lt;p&gt;In this thesis we propose new methods for crossover operator namely: cut on
worst gene (COWGC), cut on worst L+R gene (COWLRGC) and Collision Crossovers.
And also we propose several types of mutation operator such as: worst gene with
random gene mutation (WGWRGM) , worst LR gene with random gene mutation
(WLRGWRGM), worst gene with worst gene mutation (WGWWGM), worst gene with
nearest neighbour mutation (WGWNNM), worst gene with the worst around the
nearest neighbour mutation (WGWWNNM), worst gene inserted beside nearest
neighbour mutation (WGIBNNM), random gene inserted beside nearest neighbour
mutation (RGIBNNM), Swap worst gene locally mutation (SWGLM), Insert best
random gene before worst gene mutation (IBRGBWGM) and Insert best random gene
before random gene mutation (IBRGBRGM). In addition to proposing four selection
strategies, namely: select any crossover (SAC), select any mutation (SAM),
select best crossover (SBC) and select best mutation (SBM). The first two are
based on selection of the best crossover and mutation operator respectively,
and the other two strategies randomly select any operator. So we investigate
the use of more than one crossover/mutation operator (based on the proposed
strategies) to enhance the performance of genetic algorithms. Our experiments,
conducted on several Travelling Salesman Problems (TSP), show the superiority
of some of the proposed methods in crossover and mutation over some of the
well-known crossover and mutation operators described in the literature. In
addition, using any of the four strategies (SAC, SAM, SBC and SBM), found to be
better than using one crossover/mutation operator in general, because those
allow the GA to avoid local optima, or the so-called premature convergence.
Keywords: GAs, Collision crossover, Multi crossovers, Multi mutations, TSP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkafaween_E/0/1/0/all/0/1&quot;&gt;Esra&amp;#x27;a O Alkafaween&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02901">
<title>Convexification of Neural Graph. (arXiv:1801.02901v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.02901</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditionally, most complex intelligence architectures are extremely
non-convex, which could not be well performed by convex optimization. However,
this paper decomposes complex structures into three types of nodes: operators,
algorithms and functions. Further, iteratively propagating from node to node
along edge, we prove that &quot;regarding the neural graph without triangles, it is
nearly convex in each variable, when the other variables are fixed.&quot; In fact,
the non-convex properties stem from triangles and functions, which could be
transformed to be convex with our proposed \textit{\textbf{convexification
inequality}}. In conclusion, we generally depict the landscape for the
objective of neural graph and propose the methodology to convexify neural
graph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Han Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02940">
<title>An Ontology for Satellite Databases. (arXiv:1801.02940v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.02940</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper demonstrates the development of ontology for satellite databases.
First, I create a computational ontology for the Union of Concerned Scientists
(UCS) Satellite Database (UCSSD for short), called the UCS Satellite Ontology
(or UCSSO). Second, in developing UCSSO I show that The Space Situational
Awareness Ontology (SSAO) (Rovetto and Kelso 2016)--an existing space domain
reference ontology--and related ontology work by the author (Rovetto 2015,
2016) can be used either (i) with a database-specific local ontology such as
UCSSO, or (ii) in its stead. In case (i), local ontologies such as UCSSO can
reuse SSAO terms, perform term mappings, or extend it. In case (ii), the
author&apos;s orbital space ontology work, such as the SSAO, is usable by the UCSSD
and organizations with other space object catalogs, as a reference ontology
suite providing a common semantically-rich domain model. The SSAO, UCSSO, and
the broader Orbital Space Environment Domain Ontology project is online at
&lt;a href=&quot;http://purl.org/space-ontology&quot;&gt;this http URL&lt;/a&gt; and GitHub. This ontology effort aims, in part,
to provide accurate formal representations of the domain for various
applications. Ontology engineering has the potential to facilitate the sharing
and integration of satellite data from federated databases and sensors for
safer spaceflight.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rovetto_R/0/1/0/all/0/1&quot;&gt;Robert J. Rovetto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03003">
<title>Between collective intelligence and semantic web : hypermediating sites. Contribution to technologies of intelligence. (arXiv:1801.03003v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.03003</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present a new form of access to knowledge through what we
call &quot;hypermediator websites&quot;. These hypermediator sites are intermediate
between information devices that just scan the book culture and a &quot;real&quot;
hypertext writing format.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verlaet_L/0/1/0/all/0/1&quot;&gt;Lise Verlaet&lt;/a&gt; (LERASS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallot_S/0/1/0/all/0/1&quot;&gt;Sidonie Gallot&lt;/a&gt; (LERASS)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03039">
<title>EBIC: an artificial intelligence-based parallel biclustering algorithm for pattern discovery. (arXiv:1801.03039v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.03039</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper a novel biclustering algorithm based on artificial intelligence
(AI) is introduced. The method called EBIC aims to detect biologically
meaningful, order-preserving patterns in complex data. The proposed algorithm
is probably the first one capable of discovering with accuracy exceeding 50\%
multiple complex patterns in real gene expression datasets. It is also one of
the very few biclustering methods designed for parallel environments with
multiple graphics processing units (GPUs). We demonstrate that EBIC outperforms
state-of-the-art biclustering methods, in terms of recovery and relevance, on
both synthetic and genetic datasets. EBIC also yields results over 12 times
faster than the most accurate reference algorithms. The proposed algorithm is
anticipated to be added to the repertoire of unsupervised machine learning
algorithms for the analysis of datasets, including those from large-scale
genomic studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orzechowski_P/0/1/0/all/0/1&quot;&gt;Patryk Orzechowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sipper_M/0/1/0/all/0/1&quot;&gt;Moshe Sipper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiuzhen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1&quot;&gt;Jason H. Moore&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03058">
<title>Probabilistic Prognostic Estimates of Survival in Metastatic Cancer Patients (PPES-Met) Utilizing Free-Text Clinical Narratives. (arXiv:1801.03058v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.03058</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a deep learning model - Probabilistic Prognostic Estimates of
Survival in Metastatic Cancer Patients (PPES-Met) for estimating short-term
life expectancy (3 months) of the patients by analyzing free-text clinical
notes in the electronic medical record, while maintaining the temporal visit
sequence. In a single framework, we integrated semantic data mapping and neural
embedding technique to produce a text processing method that extracts relevant
information from heterogeneous types of clinical notes in an unsupervised
manner, and we designed a recurrent neural network to model the temporal
dependency of the patient visits. The model was trained on a large dataset
(10,293 patients) and validated on a separated dataset (1818 patients). Our
method achieved an area under the ROC curve (AUC) of 0.89. To provide
explain-ability, we developed an interactive graphical tool that may improve
physician understanding of the basis for the model&apos;s predictions. The high
accuracy and explain-ability of the PPES-Met model may enable our model to be
used as a decision support tool to personalize metastatic cancer treatment and
provide valuable assistance to the physicians.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1&quot;&gt;Imon Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gensheimer_M/0/1/0/all/0/1&quot;&gt;Michael Francis Gensheimer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wood_D/0/1/0/all/0/1&quot;&gt;Douglas J. Wood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henry_S/0/1/0/all/0/1&quot;&gt;Solomon Henry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1&quot;&gt;Daniel Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1&quot;&gt;Daniel L. Rubin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.03296">
<title>Interpretable Explanations of Black Boxes by Meaningful Perturbation. (arXiv:1704.03296v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1704.03296</link>
<description rdf:parseType="Literal">&lt;p&gt;As machine learning algorithms are increasingly applied to high impact yet
high risk tasks, such as medical diagnosis or autonomous driving, it is
critical that researchers can explain how such algorithms arrived at their
predictions. In recent years, a number of image saliency methods have been
developed to summarize where highly complex neural networks &quot;look&quot; in an image
for evidence for their predictions. However, these techniques are limited by
their heuristic nature and architectural constraints. In this paper, we make
two main contributions: First, we propose a general framework for learning
different kinds of explanations for any black box algorithm. Second, we
specialise the framework to find the part of an image most responsible for a
classifier decision. Unlike previous works, our method is model-agnostic and
testable because it is grounded in explicit and interpretable image
perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fong_R/0/1/0/all/0/1&quot;&gt;Ruth Fong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1&quot;&gt;Andrea Vedaldi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.04046">
<title>Stable Distribution Alignment Using the Dual of the Adversarial Distance. (arXiv:1707.04046v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1707.04046</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods that align distributions by minimizing an adversarial distance
between them have recently achieved impressive results. However, these
approaches are difficult to optimize with gradient descent and they often do
not converge well without careful hyperparameter tuning and proper
initialization. We investigate whether turning the adversarial min-max problem
into an optimization problem by replacing the maximization part with its dual
improves the quality of the resulting alignment and explore its connections to
Maximum Mean Discrepancy. Our empirical results suggest that using the dual
formulation for the restricted family of linear discriminators results in a
more stable convergence to a desirable solution when compared with the
performance of a primal min-max GAN-like objective and an MMD objective under
the same restrictions. We test our hypothesis on the problem of aligning two
synthetic point clouds on a plane and on a real-image domain adaptation problem
on digits. In both cases, the dual formulation yields an iterative procedure
that gives more stable and monotonic improvement over time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usman_B/0/1/0/all/0/1&quot;&gt;Ben Usman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1&quot;&gt;Kate Saenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulis_B/0/1/0/all/0/1&quot;&gt;Brian Kulis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.06690">
<title>DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning. (arXiv:1707.06690v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1707.06690</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of learning to reason in large scale knowledge graphs
(KGs). More specifically, we describe a novel reinforcement learning framework
for learning multi-hop relational paths: we use a policy-based agent with
continuous states based on knowledge graph embeddings, which reasons in a KG
vector space by sampling the most promising relation to extend its path. In
contrast to prior work, our approach includes a reward function that takes the
accuracy, diversity, and efficiency into consideration. Experimentally, we show
that our proposed method outperforms a path-ranking based algorithm and
knowledge graph embedding methods on Freebase and Never-Ending Language
Learning datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wenhan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1&quot;&gt;Thien Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07970">
<title>Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge. (arXiv:1711.07970v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07970</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the use of Deep Learning methods for modeling complex phenomena
like those occurring in natural physical processes. With the large amount of
data gathered on these phenomena the data intensive paradigm could begin to
challenge more traditional approaches elaborated over the years in fields like
maths or physics. However, despite considerable successes in a variety of
application domains, the machine learning field is not yet ready to handle the
level of complexity required by such problems. Using an example application,
namely Sea Surface Temperature Prediction, we show how general background
knowledge gained from physics could be used as a guideline for designing
efficient Deep Learning models. In order to motivate the approach and to assess
its generality we demonstrate a formal link between the solution of a class of
differential equations underlying a large family of physical phenomena and the
proposed model. Experiments and comparison with series of baselines including a
state of the art numerical approach is then provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bezenac_E/0/1/0/all/0/1&quot;&gt;Emmanuel de Bezenac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pajot_A/0/1/0/all/0/1&quot;&gt;Arthur Pajot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1&quot;&gt;Patrick Gallinari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06957">
<title>MURA Dataset: Towards Radiologist-Level Abnormality Detection in Musculoskeletal Radiographs. (arXiv:1712.06957v3 [physics.med-ph] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.06957</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce MURA, a large dataset of musculoskeletal radiographs containing
40,895 images from 14,982 studies, where each study is manually labeled by
radiologists as either normal or abnormal. On this dataset, we train a
169-layer densely connected convolutional network to detect and localize
abnormalities. To evaluate our model robustly and to get an estimate of
radiologist performance, we collect additional labels from board-certified
Stanford radiologists on the test set, consisting of 209 musculoskeletal
studies. We compared our model and radiologists on the Cohen&apos;s kappa statistic,
which expresses the agreement of our model and of each radiologist with the
gold standard, defined as the majority vote of a disjoint group of
radiologists. We find that our model achieves performance comparable to that of
radiologists. Model performance is higher than the best radiologist performance
in detecting abnormalities on finger studies and equivalent on wrist studies.
However, model performance is lower than best radiologist performance in
detecting abnormalities on elbow, forearm, hand, humerus, and shoulder studies,
indicating that the task is a good challenge for future research. To encourage
advances, we have made our dataset freely available at
https://stanfordmlgroup.github.io/projects/mura
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rajpurkar_P/0/1/0/all/0/1&quot;&gt;Pranav Rajpurkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Irvin_J/0/1/0/all/0/1&quot;&gt;Jeremy Irvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bagul_A/0/1/0/all/0/1&quot;&gt;Aarti Bagul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ding_D/0/1/0/all/0/1&quot;&gt;Daisy Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Duan_T/0/1/0/all/0/1&quot;&gt;Tony Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mehta_H/0/1/0/all/0/1&quot;&gt;Hershel Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Brandon Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kaylie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Laird_D/0/1/0/all/0/1&quot;&gt;Dillon Laird&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ball_R/0/1/0/all/0/1&quot;&gt;Robyn L. Ball&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Langlotz_C/0/1/0/all/0/1&quot;&gt;Curtis Langlotz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shpanskaya_K/0/1/0/all/0/1&quot;&gt;Katie Shpanskaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lungren_M/0/1/0/all/0/1&quot;&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ng_A/0/1/0/all/0/1&quot;&gt;Andrew Ng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02736">
<title>Modeling sepsis progression using hidden Markov models. (arXiv:1801.02736v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.02736</link>
<description rdf:parseType="Literal">&lt;p&gt;Characterizing a patient&apos;s progression through stages of sepsis is critical
for enabling risk stratification and adaptive, personalized treatment. However,
commonly used sepsis diagnostic criteria fail to account for significant
underlying heterogeneity, both between patients as well as over time in a
single patient. We introduce a hidden Markov model of sepsis progression that
explicitly accounts for patient heterogeneity. Benchmarked against two sepsis
diagnostic criteria, the model provides a useful tool to uncover a patient&apos;s
latent sepsis trajectory and to identify high-risk patients in whom more
aggressive therapy may be indicated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Petersen_B/0/1/0/all/0/1&quot;&gt;Brenden K. Petersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mayhew_M/0/1/0/all/0/1&quot;&gt;Michael B. Mayhew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ogbuefi_K/0/1/0/all/0/1&quot;&gt;Kalvin O. E. Ogbuefi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Greene_J/0/1/0/all/0/1&quot;&gt;John D. Greene&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_V/0/1/0/all/0/1&quot;&gt;Vincent X. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ray_P/0/1/0/all/0/1&quot;&gt;Priyadip Ray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02788">
<title>Sequential Preference-Based Optimization. (arXiv:1801.02788v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.02788</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world engineering problems rely on human preferences to guide their
design and optimization. We present PrefOpt, an open source package to simplify
sequential optimization tasks that incorporate human preference feedback. Our
approach extends an existing latent variable model for binary preferences to
allow for observations of equivalent preference from users.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dewancker_I/0/1/0/all/0/1&quot;&gt;Ian Dewancker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_J/0/1/0/all/0/1&quot;&gt;Jakob Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCourt_M/0/1/0/all/0/1&quot;&gt;Michael McCourt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02858">
<title>Scalable high-resolution forecasting of sparse spatiotemporal events with kernel methods: a winning solution to the NIJ &quot;Real-Time Crime Forecasting Challenge&quot;. (arXiv:1801.02858v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.02858</link>
<description rdf:parseType="Literal">&lt;p&gt;This article describes Team Kernel Glitches&apos; solution to the National
Institute of Justice&apos;s (NIJ) Real-Time Crime Forecasting Challenge. The goal of
the NIJ Real-Time Crime Forecasting Competition was to maximize two different
crime hotspot scoring metrics for calls-for-service to the Portland Police
Bureau (PPB) in Portland, Oregon during the period from March 1, 2017 to May
31, 2017. Our solution to the challenge is a spatiotemporal forecasting model
combining scalable randomized Reproducing Kernel Hilbert Space (RKHS) methods
for approximating Gaussian processes with autoregressive smoothing kernels in a
regularized supervised learning framework. Our model can be understood as an
approximation to the popular log-Gaussian Cox Process model: we discretize the
spatiotemporal point pattern and learn a log intensity function using the
Poisson likelihood and highly efficient gradient-based optimization methods.
Model hyperparameters including quality of RKHS approximation, spatial and
temporal kernel lengthscales, number of autoregressive lags, bandwidths for
smoothing kernels, as well as cell shape, size, and rotation, were learned
using crossvalidation. Resulting predictions exceeded baseline KDE estimates by
0.157. Performance improvement over baseline predictions were particularly
large for sparse crimes over short forecasting horizons.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Flaxman_S/0/1/0/all/0/1&quot;&gt;Seth Flaxman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chirico_M/0/1/0/all/0/1&quot;&gt;Michael Chirico&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pereira_P/0/1/0/all/0/1&quot;&gt;Pau Pereira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loeffler_C/0/1/0/all/0/1&quot;&gt;Charles Loeffler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02937">
<title>Online Cluster Validity Indices for Streaming Data. (arXiv:1801.02937v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.02937</link>
<description rdf:parseType="Literal">&lt;p&gt;Cluster analysis is used to explore structure in unlabeled data sets in a
wide range of applications. An important part of cluster analysis is validating
the quality of computationally obtained clusters. A large number of different
internal indices have been developed for validation in the offline setting.
However, this concept has not been extended to the online setting. A key
challenge is to find an efficient incremental formulation of an index that can
capture both cohesion and separation of the clusters over potentially infinite
data streams. In this paper, we develop two online versions (with and without
forgetting factors) of the Xie-Beni and Davies-Bouldin internal validity
indices, and analyze their characteristics, using two streaming clustering
algorithms (sk-means and online ellipsoidal clustering), and illustrate their
use in monitoring evolving clusters in streaming data. We also show that
incremental cluster validity indices are capable of sending a distress signal
to online monitors when evolving clusters go awry. Our numerical examples
indicate that the incremental Xie-Beni index with forgetting factor is superior
to the other three indices tested.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moshtaghi_M/0/1/0/all/0/1&quot;&gt;Masud Moshtaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bezdek_J/0/1/0/all/0/1&quot;&gt;James C. Bezdek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Erfani_S/0/1/0/all/0/1&quot;&gt;Sarah M. Erfani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Leckie_C/0/1/0/all/0/1&quot;&gt;Christopher Leckie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bailey_J/0/1/0/all/0/1&quot;&gt;James Bailey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02939">
<title>Deep Gaussian Processes with Decoupled Inducing Inputs. (arXiv:1801.02939v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.02939</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Gaussian Processes (DGP) are hierarchical generalizations of Gaussian
Processes (GP) that have proven to work effectively on a multiple supervised
regression tasks. They combine the well calibrated uncertainty estimates of GPs
with the great flexibility of multilayer models. In DGPs, given the inputs, the
outputs of the layers are Gaussian distributions parameterized by their means
and covariances. These layers are realized as Sparse GPs where the training
data is approximated using a small set of pseudo points. In this work, we show
that the computational cost of DGPs can be reduced with no loss in performance
by using a separate, smaller set of pseudo points when calculating the
layerwise variance while using a larger set of pseudo points when calculating
the layerwise mean. This enabled us to train larger models that have lower cost
and better predictive performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Havasi_M/0/1/0/all/0/1&quot;&gt;Marton Havasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Miguel Hern&amp;#xe1;ndez-Lobato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Murillo_Fuentes_J/0/1/0/all/0/1&quot;&gt;Juan Jos&amp;#xe9; Murillo-Fuentes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02949">
<title>An efficient K -means clustering algorithm for massive data. (arXiv:1801.02949v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.02949</link>
<description rdf:parseType="Literal">&lt;p&gt;The analysis of continously larger datasets is a task of major importance in
a wide variety of scientific fields. In this sense, cluster analysis algorithms
are a key element of exploratory data analysis, due to their easiness in the
implementation and relatively low computational cost. Among these algorithms,
the K -means algorithm stands out as the most popular approach, besides its
high dependency on the initial conditions, as well as to the fact that it might
not scale well on massive datasets. In this article, we propose a recursive and
parallel approximation to the K -means algorithm that scales well on both the
number of instances and dimensionality of the problem, without affecting the
quality of the approximation. In order to achieve this, instead of analyzing
the entire dataset, we work on small weighted sets of points that mostly intend
to extract information from those regions where it is harder to determine the
correct cluster assignment of the original instances. In addition to different
theoretical properties, which deduce the reasoning behind the algorithm,
experimental results indicate that our method outperforms the state-of-the-art
in terms of the trade-off between number of distance computations and the
quality of the solution obtained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Capo_M/0/1/0/all/0/1&quot;&gt;Marco Cap&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perez_A/0/1/0/all/0/1&quot;&gt;Aritz P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lozano_J/0/1/0/all/0/1&quot;&gt;Jose A. Lozano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02950">
<title>Adversarial Deep Learning for Robust Detection of Binary Encoded Malware. (arXiv:1801.02950v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1801.02950</link>
<description rdf:parseType="Literal">&lt;p&gt;Malware is constantly adapting in order to avoid detection. Model based
malware detectors, such as SVM and neural networks, are vulnerable to so-called
adversarial examples which are modest changes to detectable malware that allows
the resulting malware to evade detection. Continuous-valued methods that are
robust to adversarial examples of images have been developed using saddle-point
optimization formulations. We are inspired by them to develop similar methods
for the discrete, e.g. binary, domain which characterizes the features of
malware. A specific extra challenge of malware is that the adversarial examples
must be generated in a way that preserves their malicious functionality. We
introduce methods capable of generating functionally preserved adversarial
malware examples in the binary domain. Using the saddle-point formulation, we
incorporate the adversarial examples into the training of models that are
robust to them. We evaluate the effectiveness of the methods and others in the
literature on a set of Portable Execution~(PE) files. Comparison prompts our
introduction of an online measure computed during training to assess general
expectation of robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1&quot;&gt;Alex Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Dujaili_A/0/1/0/all/0/1&quot;&gt;Abdullah Al-Dujaili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemberg_E/0/1/0/all/0/1&quot;&gt;Erik Hemberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OReilly_U/0/1/0/all/0/1&quot;&gt;Una-May O&amp;#x27;Reilly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02961">
<title>A Predictive Approach Using Deep Feature Learning for Electronic Medical Records: A Comparative Study. (arXiv:1801.02961v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.02961</link>
<description rdf:parseType="Literal">&lt;p&gt;Massive amount of electronic medical records accumulating from patients and
populations motivates clinicians and data scientists to collaborate for the
advanced analytics to extract knowledge that is essential to address the
extensive personalized insights needed for patients, clinicians, providers,
scientists, and health policy makers. In this paper, we propose a new
predictive approach based on feature representation using deep feature learning
and word embedding techniques. Our method uses different deep architectures for
feature representation in higher-level abstraction to obtain effective and more
robust features from EMRs, and then build prediction models on the top of them.
Our approach is particularly useful when the unlabeled data is abundant whereas
labeled one is scarce. We investigate the performance of representation
learning through a supervised approach. First, we apply our method on a small
dataset related to a specific precision medicine problem, which focuses on
prediction of left ventricular mass indexed to body surface area (LVMI) as an
indicator of heart damage risk in a vulnerable demographic subgroup
(African-Americans). Then we use two large datasets from eICU collaborative
research database to predict the length of stay in Cardiac-ICU and Neuro-ICU
based on high dimensional features. Finally we provide a comparative study and
show that our predictive approach leads to better results in comparison with
others.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nezhad_M/0/1/0/all/0/1&quot;&gt;Milad Zafar Nezhad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dongxiao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadati_N/0/1/0/all/0/1&quot;&gt;Najibesadat Sadati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kai Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02982">
<title>How To Make the Gradients Small Stochastically. (arXiv:1801.02982v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.02982</link>
<description rdf:parseType="Literal">&lt;p&gt;In convex stochastic optimization, convergence rates in terms of minimizing
the objective have been well-established. However, in terms of making the
gradients small, the best known convergence rate was $O(\varepsilon^{-8/3})$
and it was left open how to improve it.
&lt;/p&gt;
&lt;p&gt;In this paper, we improve this rate to $\tilde{O}(\varepsilon^{-2})$, which
is optimal up to log factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Allen-Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03050">
<title>Sales forecasting and risk management under uncertainty in the media industry. (arXiv:1801.03050v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.03050</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we propose a data-driven modelization approach for the
management of advertising investments of a firm. First, we propose an
application of dynamic linear models to the prediction of an economic variable,
such as global sales, which can use information from the environment and the
investment levels of the company in different channels. After we build a robust
and precise model, we propose a metric of risk, which can help the firm to
manage their advertisement plans, thus leading to a robust, risk-aware
optimization of their revenue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gallego_V/0/1/0/all/0/1&quot;&gt;V&amp;#xed;ctor Gallego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Angulo_P/0/1/0/all/0/1&quot;&gt;Pablo Angulo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suarez_Garcia_P/0/1/0/all/0/1&quot;&gt;Pablo Su&amp;#xe1;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gomez_Ullate_D/0/1/0/all/0/1&quot;&gt;David G&amp;#xf3;mez-Ullate&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.04181">
<title>Removal of Batch Effects using Distribution-Matching Residual Networks. (arXiv:1610.04181v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.04181</link>
<description rdf:parseType="Literal">&lt;p&gt;Sources of variability in experimentally derived data include measurement
error in addition to the physical phenomena of interest. This measurement error
is a combination of systematic components, originating from the measuring
instrument, and random measurement errors. Several novel biological
technologies, such as mass cytometry and single-cell RNA-seq, are plagued with
systematic errors that may severely affect statistical analysis if the data is
not properly calibrated. We propose a novel deep learning approach for removing
systematic batch effects. Our method is based on a residual network, trained to
minimize the Maximum Mean Discrepancy (MMD) between the multivariate
distributions of two replicates, measured in different batches. We apply our
method to mass cytometry and single-cell RNA-seq datasets, and demonstrate that
it effectively attenuates batch effects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shaham_U/0/1/0/all/0/1&quot;&gt;Uri Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stanton_K/0/1/0/all/0/1&quot;&gt;Kelly P. Stanton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huamin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raddassi_K/0/1/0/all/0/1&quot;&gt;Khadir Raddassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Montgomery_R/0/1/0/all/0/1&quot;&gt;Ruth Montgomery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kluger_Y/0/1/0/all/0/1&quot;&gt;Yuval Kluger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.05193">
<title>Accelerated Distributed Dual Averaging over Evolving Networks of Growing Connectivity. (arXiv:1704.05193v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.05193</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of accelerating distributed optimization in
multi-agent networks by sequentially adding edges. Specifically, we extend the
distributed dual averaging (DDA) subgradient algorithm to evolving networks of
growing connectivity and analyze the corresponding improvement in convergence
rate. It is known that the convergence rate of DDA is influenced by the
algebraic connectivity of the underlying network, where better connectivity
leads to faster convergence. However, the impact of network topology design on
the convergence rate of DDA has not been fully understood. In this paper, we
begin by designing network topologies via edge selection and scheduling. For
edge selection, we determine the best set of candidate edges that achieves the
optimal tradeoff between the growth of network connectivity and the usage of
network resources. The dynamics of network evolution is then incurred by edge
scheduling. Further, we provide a tractable approach to analyze the improvement
in the convergence rate of DDA induced by the growth of network connectivity.
Our analysis reveals the connection between network topology design and the
convergence rate of DDA, and provides quantitative evaluation of DDA
acceleration for distributed optimization that is absent in the existing
analysis. Lastly, numerical experiments show that DDA can be significantly
accelerated using a sequence of well-designed networks, and our theoretical
predictions are well matched to its empirical convergence behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hero_A/0/1/0/all/0/1&quot;&gt;Alfred O. Hero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01711">
<title>Mode-Seeking Clustering and Density Ridge Estimation via Direct Estimation of Density-Derivative-Ratios. (arXiv:1707.01711v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01711</link>
<description rdf:parseType="Literal">&lt;p&gt;Modes and ridges of the probability density function behind observed data are
useful geometric features. Mode-seeking clustering assigns cluster labels by
associating data samples with the nearest modes, and estimation of density
ridges enables us to find lower-dimensional structures hidden in data. A key
technical challenge both in mode-seeking clustering and density ridge
estimation is accurate estimation of the ratios of the first- and second-order
density derivatives to the density. A naive approach takes a three-step
approach of first estimating the data density, then computing its derivatives,
and finally taking their ratios. However, this three-step approach can be
unreliable because a good density estimator does not necessarily mean a good
density derivative estimator, and division by the estimated density could
significantly magnify the estimation error. To cope with these problems, we
propose a novel estimator for the \emph{density-derivative-ratios}. The
proposed estimator does not involve density estimation, but rather
\emph{directly} approximates the ratios of density derivatives of any order.
Moreover, we establish a convergence rate of the proposed estimator. Based on
the proposed estimator, new methods both for mode-seeking clustering and
density ridge estimation are developed, and corresponding convergence rates to
the mode and ridge of the underlying density are also established. Finally, we
experimentally demonstrate that the developed methods significantly outperform
existing methods, particularly for relatively high-dimensional data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sasaki_H/0/1/0/all/0/1&quot;&gt;Hiroaki Sasaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kanamori_T/0/1/0/all/0/1&quot;&gt;Takafumi Kanamori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hyvarinen_A/0/1/0/all/0/1&quot;&gt;Aapo Hyv&amp;#xe4;rinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Niu_G/0/1/0/all/0/1&quot;&gt;Gang Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.01955">
<title>Wasserstein Dictionary Learning: Optimal Transport-based unsupervised non-linear dictionary learning. (arXiv:1708.01955v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.01955</link>
<description rdf:parseType="Literal">&lt;p&gt;This article introduces a new non-linear dictionary learning method for
histograms in the probability simplex. The method leverages optimal transport
theory, in the sense that our aim is to reconstruct histograms using so called
displacement interpolations (a.k.a. Wasserstein barycenters) between dictionary
atoms; such atoms are themselves synthetic histograms in the probability
simplex. Our method simultaneously estimates such atoms, and, for each
datapoint, the vector of weights that can optimally reconstruct it as an
optimal transport barycenter of such atoms. Our method is computationally
tractable thanks to the addition of an entropic regularization to the usual
optimal transportation problem, leading to an approximation scheme that is
efficient, parallel and simple to differentiate. Both atoms and weights are
learned using a gradient-based descent method. Gradients are obtained by
automatic differentiation of the generalized Sinkhorn iterations that yield
barycenters with entropic smoothing. Because of its formulation relying on
Wasserstein barycenters instead of the usual matrix product between dictionary
and codes, our method allows for non-linear relationships between atoms and the
reconstruction of input data. We illustrate its application in several
different image processing settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmitz_M/0/1/0/all/0/1&quot;&gt;Morgan A. Schmitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Heitz_M/0/1/0/all/0/1&quot;&gt;Matthieu Heitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bonneel_N/0/1/0/all/0/1&quot;&gt;Nicolas Bonneel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mboula_F/0/1/0/all/0/1&quot;&gt;Fred Maurice Ngol&amp;#xe8; Mboula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Coeurjolly_D/0/1/0/all/0/1&quot;&gt;David Coeurjolly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cuturi_M/0/1/0/all/0/1&quot;&gt;Marco Cuturi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peyre_G/0/1/0/all/0/1&quot;&gt;Gabriel Peyr&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Starck_J/0/1/0/all/0/1&quot;&gt;Jean-Luc Starck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.08464">
<title>Interpretable Machine Learning for Privacy-Preserving Pervasive Systems. (arXiv:1710.08464v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.08464</link>
<description rdf:parseType="Literal">&lt;p&gt;The presence of pervasive systems in our everyday lives and the interaction
of users with connected devices such as smartphones or home appliances generate
increasing amounts of traces that reflect users&apos; behavior. A plethora of
machine learning techniques enable service providers to process these traces to
extract latent information about the users. While most of the existing projects
have focused on the accuracy of these techniques, little work has been done on
the interpretation of the inference and identification algorithms based on
them. In this paper, we propose a machine learning interpretability framework
for inference algorithms based on data collected through pervasive systems and
we outline the open challenges in this research area. Our interpretability
framework enable users to understand how the traces they generate could expose
their privacy, while allowing for usable and personalized services at the same
time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baron_B/0/1/0/all/0/1&quot;&gt;Benjamin Baron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Musolesi_M/0/1/0/all/0/1&quot;&gt;Mirco Musolesi&lt;/a&gt;</dc:creator>
</item></rdf:RDF>