<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-09-02T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10866"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10568"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10654"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10750"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10784"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10813"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10831"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10241"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10532"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10549"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10551"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10552"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10556"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10594"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10648"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10650"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10663"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10664"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10724"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10788"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.07450"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05006"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07276"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07174"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10101"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.10866">
<title>Algoritmos Gen\&apos;eticos Aplicado ao Problema de Roteamento de Ve\&apos;iculos. (arXiv:1808.10866v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.10866</link>
<description rdf:parseType="Literal">&lt;p&gt;Routing problems are often faced by companies who serve costumers through
vehicles. Such problems have a challenging structure to optimize, despite the
recent advances in combinatorial optimization. The goal of this project is to
study and propose optimization algorithms to the vehicle routing problems
(VRP). Focus will be on the problem variant in which the length of the route is
restricted by a constant. A real problem will be tackled: optimization of
postmen routes. Such problem was modeled as {multi-objective} in a roadmap with
25 vehicles and {30,000 deliveries} per day.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_F/0/1/0/all/0/1&quot;&gt;Felipe F. M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meira_L/0/1/0/all/0/1&quot;&gt;Luis A. A. Meira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10568">
<title>Multi-Hop Knowledge Graph Reasoning with Reward Shaping. (arXiv:1808.10568v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.10568</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-hop reasoning is an effective approach for query answering (QA) over
incomplete knowledge graphs (KGs). The problem can be formulated in a
reinforcement learning (RL) setup, where a policy-based agent sequentially
extends its inference path until it reaches a target. However, in an incomplete
KG environment, the agent receives low-quality rewards corrupted by false
negatives in the training data, which harms generalization at test time.
Furthermore, since no golden action sequence is used for training, the agent
can be misled by spurious search trajectories that incidentally lead to the
correct answer. We propose two modeling advances to address both issues: (1) we
reduce the impact of false negative supervision by adopting a pretrained
one-hop embedding model to estimate the reward of unobserved facts; (2) we
counter the sensitivity to spurious paths of on-policy RL by forcing the agent
to explore a diverse set of paths using randomly generated edge masks. Our
approach significantly improves over existing path-based KGQA models on several
benchmark datasets and is comparable or better than embedding-based models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xi Victoria Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10654">
<title>Gibson Env: Real-World Perception for Embodied Agents. (arXiv:1808.10654v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.10654</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing visual perception models for active agents and sensorimotor
control are cumbersome to be done in the physical world, as existing algorithms
are too slow to efficiently learn in real-time and robots are fragile and
costly. This has given rise to learning-in-simulation which consequently casts
a question on whether the results transfer to real-world. In this paper, we are
concerned with the problem of developing real-world perception for active
agents, propose Gibson Virtual Environment for this purpose, and showcase
sample perceptual tasks learned therein. Gibson is based on virtualizing real
spaces, rather than using artificially designed ones, and currently includes
over 1400 floor spaces from 572 full buildings. The main characteristics of
Gibson are: I. being from the real-world and reflecting its semantic
complexity, II. having an internal synthesis mechanism, &quot;Goggles&quot;, enabling
deploying the trained models in real-world without needing further domain
adaptation, III. embodiment of agents and making them subject to constraints of
physics and space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamir_A/0/1/0/all/0/1&quot;&gt;Amir Zamir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhi-Yang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sax_A/0/1/0/all/0/1&quot;&gt;Alexander Sax&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1&quot;&gt;Silvio Savarese&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10750">
<title>Victory Probability in the Fire Emblem Arena. (arXiv:1808.10750v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.10750</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate how to efficiently compute the probability of victory in Fire
Emblem arena battles. The probability can be expressed in terms of a
multivariate recurrence relation which lends itself to a straightforward
dynamic programming solution. Some implementation issues are addressed, and a
full implementation is provided in code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brockmann_A/0/1/0/all/0/1&quot;&gt;Andrew Brockmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10784">
<title>Using a Game Engine to Simulate Critical Incidents and Data Collection by Autonomous Drones. (arXiv:1808.10784v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.10784</link>
<description rdf:parseType="Literal">&lt;p&gt;Using a game engine, we have developed a virtual environment which models
important aspects of critical incident scenarios. We focused on modelling
phenomena relating to the identification and gathering of key forensic
evidence, in order to develop and test a system which can handle chemical,
biological, radiological/nuclear or explosive (CBRNe) events autonomously. This
allows us to build and validate AI-based technologies, which can be trained and
tested in our custom virtual environment before being deployed in real-world
scenarios. We have used our virtual scenario to rapidly prototype a system
which can use simulated Remote Aerial Vehicles (RAVs) to gather images from the
environment for the purpose of mapping. Our environment provides us with an
effective medium through which we can develop and test various AI methodologies
for critical incident scene assessment, in a safe and controlled manner
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smyth_D/0/1/0/all/0/1&quot;&gt;David L. Smyth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glavin_F/0/1/0/all/0/1&quot;&gt;Frank G. Glavin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madden_M/0/1/0/all/0/1&quot;&gt;Michael G. Madden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10813">
<title>Boosting Binary Optimization via Binary Classification: A Case Study of Job Shop Scheduling. (arXiv:1808.10813v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.10813</link>
<description rdf:parseType="Literal">&lt;p&gt;Many optimization techniques evaluate solutions consecutively, where the next
candidate for evaluation is determined by the results of previous evaluations.
For example, these include iterative methods, &quot;black box&quot; optimization
algorithms, simulated annealing, evolutionary algorithms and tabu search, to
name a few. When solving an optimization problem, these algorithms evaluate a
large number of solutions, which raises the following question: Is it possible
to learn something about the optimum using these solutions? In this paper, we
define this &quot;learning&quot; question in terms of a logistic regression model and
explore its predictive accuracy computationally. The proposed model uses a
collection of solutions to predict the components of the optimal solutions. To
illustrate the utility of such predictions, we embed the logistic regression
model into the tabu search algorithm for job shop scheduling problem. The
resulting framework is simple to implement, yet provides a significant boost to
the performance of the standard tabu search.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shylo_O/0/1/0/all/0/1&quot;&gt;Oleg V. Shylo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shams_H/0/1/0/all/0/1&quot;&gt;Hesam Shams&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10831">
<title>Finite LTL Synthesis with Environment Assumptions and Quality Measures. (arXiv:1808.10831v1 [cs.LO])</title>
<link>http://arxiv.org/abs/1808.10831</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the problem of synthesizing strategies for
linear temporal logic (LTL) specifications that are interpreted over finite
traces -- a problem that is central to the automated construction of
controllers, robot programs, and business processes. We study a natural variant
of the finite LTL synthesis problem in which strategy guarantees are predicated
on specified environment behavior. We further explore a quantitative extension
of LTL that supports specification of quality measures, utilizing it to
synthesize high-quality strategies. We propose new notions of optimality and
associated algorithms that yield strategies that best satisfy specified quality
measures. Our algorithms utilize an automata-game approach, positioning them
well for future implementation via existing state-of-the-art techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camacho_A/0/1/0/all/0/1&quot;&gt;Alberto Camacho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bienvenu_M/0/1/0/all/0/1&quot;&gt;Meghyn Bienvenu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1&quot;&gt;Sheila A. McIlraith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10241">
<title>The Price of Diversity in Assignment Problems. (arXiv:1711.10241v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10241</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce and analyze an extension to the matching problem on a weighted
bipartite graph: Assignment with Type Constraints. The two parts of the graph
are partitioned into subsets called types and blocks; we seek a matching with
the largest sum of weights under the constraint that there is a pre-specified
cap on the number of vertices matched in every type-block pair. Our primary
motivation stems from the public housing program of Singapore, accounting for
over 70\% of its residential real estate. To promote ethnic diversity within
its housing projects, Singapore imposes ethnicity quotas: each new housing
development comprises blocks of flats and each ethnicity-based group in the
population must not own more than a certain percentage of flats in a block.
Other domains using similar hard capacity constraints include matching
prospective students to schools or medical residents to hospitals. Limiting
agents&apos; choices for ensuring diversity in this manner naturally entails some
welfare loss. One of our goals is to study the trade-off between diversity and
social welfare in such settings. We first show that, while the classic
assignment program is polynomial-time computable, adding diversity constraints
makes it computationally intractable; however, we identify a
$\tfrac{1}{2}$-approximation algorithm, as well as reasonable assumptions on
the weights that permit poly-time algorithms. Next, we provide two upper bounds
on the {\em price of diversity} -- a measure of the loss in welfare incurred by
imposing diversity constraints -- as functions of natural problem parameters.
We conclude the paper with simulations based on publicly available data from
two diversity-constrained allocation problems -- Singapore Public Housing and
Chicago School Choice -- which shed light on how the constrained maximization
as well as lottery-based variants perform in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benabbou_N/0/1/0/all/0/1&quot;&gt;Nawal Benabbou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_M/0/1/0/all/0/1&quot;&gt;Mithun Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_V/0/1/0/all/0/1&quot;&gt;Vinh Ho Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sliwinski_J/0/1/0/all/0/1&quot;&gt;Jakub Sliwinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zick_Y/0/1/0/all/0/1&quot;&gt;Yair Zick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10532">
<title>Uniform Inference in High-Dimensional Gaussian Graphical Models. (arXiv:1808.10532v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1808.10532</link>
<description rdf:parseType="Literal">&lt;p&gt;Graphical models have become a very popular tool for representing
dependencies within a large set of variables and are key for representing
causal structures. We provide results for uniform inference on high-dimensional
graphical models with the number of target parameters being possible much
larger than sample size. This is in particular important when certain features
or structures of a causal model should be recovered. Our results highlight how
in high-dimensional settings graphical models can be estimated and recovered
with modern machine learning methods in complex data sets. We also demonstrate
in simulation study that our procedure has good small sample properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klaassen_S/0/1/0/all/0/1&quot;&gt;Sven Klaassen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kuck_J/0/1/0/all/0/1&quot;&gt;Jannis K&amp;#xfc;ck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spindler_M/0/1/0/all/0/1&quot;&gt;Martin Spindler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10549">
<title>Fair Algorithms for Learning in Allocation Problems. (arXiv:1808.10549v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10549</link>
<description rdf:parseType="Literal">&lt;p&gt;Settings such as lending and policing can be modeled by a centralized agent
allocating a resource (loans or police officers) amongst several groups, in
order to maximize some objective (loans given that are repaid or criminals that
are apprehended). Often in such problems fairness is also a concern. A natural
notion of fairness, based on general principles of equality of opportunity,
asks that conditional on an individual being a candidate for the resource, the
probability of actually receiving it is approximately independent of the
individual&apos;s group. In lending this means that equally creditworthy individuals
in different racial groups have roughly equal chances of receiving a loan. In
policing it means that two individuals committing the same crime in different
districts would have roughly equal chances of being arrested.
&lt;/p&gt;
&lt;p&gt;We formalize this fairness notion for allocation problems and investigate its
algorithmic consequences. Our main technical results include an efficient
learning algorithm that converges to an optimal fair allocation even when the
frequency of candidates (creditworthy individuals or criminals) in each group
is unknown. The algorithm operates in a censored feedback model in which only
the number of candidates who received the resource in a given allocation can be
observed, rather than the true number of candidates. This models the fact that
we do not learn the creditworthiness of individuals we do not give loans to nor
learn about crimes committed if the police presence in a district is low.
&lt;/p&gt;
&lt;p&gt;As an application of our framework, we consider the predictive policing
problem. The learning algorithm is trained on arrest data gathered from its own
deployments on previous days, resulting in a potential feedback loop that our
algorithm provably overcomes. We empirically investigate the performance of our
algorithm on the Philadelphia Crime Incidents dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elzayn_H/0/1/0/all/0/1&quot;&gt;Hadi Elzayn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jabbari_S/0/1/0/all/0/1&quot;&gt;Shahin Jabbari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1&quot;&gt;Christopher Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1&quot;&gt;Michael Kearns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neel_S/0/1/0/all/0/1&quot;&gt;Seth Neel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1&quot;&gt;Aaron Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schutzman_Z/0/1/0/all/0/1&quot;&gt;Zachary Schutzman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10551">
<title>Dynamic mode decomposition in vector-valued reproducing kernel Hilbert spaces for extracting dynamical structure among observables. (arXiv:1808.10551v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.10551</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding nonlinear dynamical systems (NLDSs) is challenging in a variety
of engineering and scientific fields. Dynamic mode decomposition (DMD), which
is a numerical algorithm for the spectral analysis of Koopman operators, has
been attracting attention as a way of obtaining global modal descriptions of
NLDSs without requiring explicit prior knowledge. However, since existing DMD
algorithms are in principle formulated based on the concatenation of scalar
observables, it is not directly applicable to data with dependent structures
among observables, which take, for example, the form of a sequence of graphs.
In this paper, we formulate Koopman spectral analysis for NLDSs with structures
among observables and propose an estimation algorithm for this problem. This
method can extract and visualize the underlying low-dimensional global dynamics
of NLDSs with structures among observables from data, which can be useful in
understanding the underlying dynamics of such NLDSs. To this end, we first
formulate the problem of estimating spectra of the Koopman operator defined in
vector-valued reproducing kernel Hilbert spaces, and then develop an estimation
procedure for this problem by reformulating tensor-based DMD. As a special case
of our method, we propose the method named as Graph DMD, which is a numerical
algorithm for Koopman spectral analysis of graph dynamical systems, using a
sequence of adjacency matrices. We investigate the empirical performance of our
method by using synthetic and real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fujii_K/0/1/0/all/0/1&quot;&gt;Keisuke Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kawahara_Y/0/1/0/all/0/1&quot;&gt;Yoshinobu Kawahara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10552">
<title>Directed Exploration in PAC Model-Free Reinforcement Learning. (arXiv:1808.10552v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10552</link>
<description rdf:parseType="Literal">&lt;p&gt;We study an exploration method for model-free RL that generalizes the
counter-based exploration bonus methods and takes into account long term
exploratory value of actions rather than a single step look-ahead. We propose a
model-free RL method that modifies Delayed Q-learning and utilizes the
long-term exploration bonus with provable efficiency. We show that our proposed
method finds a near-optimal policy in polynomial time (PAC-MDP), and also
provide experimental evidence that our proposed algorithm is an efficient
exploration method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1&quot;&gt;Min-hwan Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyengar_G/0/1/0/all/0/1&quot;&gt;Garud Iyengar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10556">
<title>Speaker Fluency Level Classification Using Machine Learning Techniques. (arXiv:1808.10556v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.10556</link>
<description rdf:parseType="Literal">&lt;p&gt;Level assessment for foreign language students is necessary for putting them
in the right level group, furthermore, interviewing students is a very
time-consuming task, so we propose to automate the evaluation of speaker
fluency level by implementing machine learning techniques. This work presents
an audio processing system capable of classifying the level of fluency of
non-native English speakers using five different machine learning models. As a
first step, we have built our own dataset, which consists of labeled audio
conversations in English between people ranging in different fluency
domains/classes (low, intermediate, high). We segment the audio conversations
into 5s non-overlapped audio clips to perform feature extraction on them. We
start by extracting Mel cepstral coefficients from the audios, selecting 20
coefficients is an appropriate quantity for our data. We thereafter extracted
zero-crossing rate, root mean square energy and spectral flux features, proving
that this improves model performance. Out of a total of 1424 audio segments,
with 70% training data and 30% test data, one of our trained models (support
vector machine) achieved a classification accuracy of 94.39%, whereas the other
four models passed an 89% classification accuracy threshold.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Preciado_Grijalva_A/0/1/0/all/0/1&quot;&gt;Alan Preciado-Grijalva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brena_R/0/1/0/all/0/1&quot;&gt;Ramon F. Brena&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10594">
<title>Proximity Forest: An effective and scalable distance-based classifier for time series. (arXiv:1808.10594v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10594</link>
<description rdf:parseType="Literal">&lt;p&gt;Research into the classification of time series has made enormous progress in
the last decade. The UCR time series archive has played a significant role in
challenging and guiding the development of new learners for time series
classification. The largest dataset in the UCR archive holds 10 thousand time
series only; which may explain why the primary research focus has been in
creating algorithms that have high accuracy on relatively small datasets.
&lt;/p&gt;
&lt;p&gt;This paper introduces Proximity Forest, an algorithm that learns accurate
models from datasets with millions of time series, and classifies a time series
in milliseconds. The models are ensembles of highly randomized Proximity Trees.
Whereas conventional decision trees branch on attribute values (and usually
perform poorly on time series), Proximity Trees branch on the proximity of time
series to one exemplar time series or another; allowing us to leverage the
decades of work into developing relevant measures for time series. Proximity
Forest gains both efficiency and accuracy by stochastic selection of both
exemplars and similarity measures.
&lt;/p&gt;
&lt;p&gt;Our work is motivated by recent time series applications that provide orders
of magnitude more time series than the UCR benchmarks. Our experiments
demonstrate that Proximity Forest is highly competitive on the UCR archive: it
ranks among the most accurate classifiers while being significantly faster. We
demonstrate on a 1M time series Earth observation dataset that Proximity Forest
retains this accuracy on datasets that are many orders of magnitude greater
than those in the UCR repository, while learning its models at least 100,000
times faster than current state of the art models Elastic Ensemble and COTE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_B/0/1/0/all/0/1&quot;&gt;Benjamin Lucas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shifaz_A/0/1/0/all/0/1&quot;&gt;Ahmed Shifaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelletier_C/0/1/0/all/0/1&quot;&gt;Charlotte Pelletier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ONeill_L/0/1/0/all/0/1&quot;&gt;Lachlan O&amp;#x27;Neill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaidi_N/0/1/0/all/0/1&quot;&gt;Nayyar Zaidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goethals_B/0/1/0/all/0/1&quot;&gt;Bart Goethals&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petitjean_F/0/1/0/all/0/1&quot;&gt;Francois Petitjean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1&quot;&gt;Geoffrey I. Webb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10648">
<title>Adaptation and Robust Learning of Probabilistic Movement Primitives. (arXiv:1808.10648v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10648</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic representations of movement primitives open important new
possibilities for machine learning in robotics. These representations are able
to capture the variability of the demonstrations from a teacher as a
probability distribution over trajectories, providing a sensible region of
exploration and the ability to adapt to changes in the robot environment.
However, to be able to capture variability and correlations between different
joints, a probabilistic movement primitive requires the estimation of a larger
number of parameters compared to their deterministic counterparts, that focus
on modeling only the mean behavior. In this paper, we make use of prior
distributions over the parameters of a probabilistic movement primitive to make
robust estimates of the parameters with few training instances. In addition, we
introduce general purpose operators to adapt movement primitives in joint and
task space. The proposed training method and adaptation operators are tested in
a coffee preparation and in robot table tennis task. In the coffee preparation
task we evaluate the generalization performance to changes in the location of
the coffee grinder and brewing chamber in a target area, achieving the desired
behavior after only two demonstrations. In the table tennis task we evaluate
the hit and return rates, outperforming previous approaches while using fewer
task specific heuristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_Gonzalez_S/0/1/0/all/0/1&quot;&gt;Sebastian Gomez-Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1&quot;&gt;Jan Peters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10650">
<title>Graph reduction by local variation. (arXiv:1808.10650v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1808.10650</link>
<description rdf:parseType="Literal">&lt;p&gt;How can we reduce the size of a graph without significantly altering its
basic properties? We approach the graph reduction problem from the perspective
of restricted similarity, a modification of a well-known measure for graph
approximation. Our choice is motivated by the observation that restricted
similarity implies strong spectral guarantees and can be used to prove
statements about certain unsupervised learning problems. The paper then focuses
on coarsening, a popular type of graph reduction. We derive sufficient
conditions for a small graph to approximate a larger one in the sense of
restricted similarity. Our theoretical findings give rise to a novel
quasi-linear algorithm. Compared to both standard and advanced graph reduction
methods, the proposed algorithm finds coarse graphs of improved quality -often
by a large margin- without sacrificing speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loukas_A/0/1/0/all/0/1&quot;&gt;Andreas Loukas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10663">
<title>A Multi-layer Gaussian Process for Motor Symptom Estimation in People with Parkinson&apos;s Disease. (arXiv:1808.10663v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10663</link>
<description rdf:parseType="Literal">&lt;p&gt;The assessment of Parkinson&apos;s disease (PD) poses a significant challenge as
it is influenced by various factors which lead to a complex and fluctuating
symptom manifestation. Thus, a frequent and objective PD assessment is highly
valuable for effective health management of people with Parkinson&apos;s disease
(PwP). Here, we propose a method for monitoring PwP by stochastically modeling
the relationships between their wrist movements during unscripted daily
activities and corresponding annotations about clinical displays of movement
abnormalities. We approach the estimation of PD motor signs by independently
modeling and hierarchically stacking Gaussian process models for three classes
of commonly observed movement abnormalities in PwP including tremor,
(non-tremulous) bradykinesia, and (non-tremulous) dyskinesia. We use clinically
adopted severity measures as annotations for training the models, thus allowing
our multi-layer Gaussian process prediction models to estimate not only their
presence but also their severities. The experimental validation of our approach
demonstrates strong agreement of the model predictions with these PD
annotations. Our results show the proposed method produces promising results in
objective monitoring of movement abnormalities of PD in the presence of
arbitrary and unknown voluntary motions, and makes an important step towards
continuous monitoring of PD in the home environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_M/0/1/0/all/0/1&quot;&gt;Muriel Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fietzek_U/0/1/0/all/0/1&quot;&gt;Urban Fietzek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frohner_J/0/1/0/all/0/1&quot;&gt;Jakob Fr&amp;#xf6;hner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_F/0/1/0/all/0/1&quot;&gt;Franz M.J. Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pichler_D/0/1/0/all/0/1&quot;&gt;Daniel Pichler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abedinpour_K/0/1/0/all/0/1&quot;&gt;Kian Abedinpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Um_T/0/1/0/all/0/1&quot;&gt;Terry T. Um&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulic_D/0/1/0/all/0/1&quot;&gt;Dana Kuli&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Endo_S/0/1/0/all/0/1&quot;&gt;Satoshi Endo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirche_S/0/1/0/all/0/1&quot;&gt;Sandra Hirche&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10664">
<title>A novel graph-based model for hybrid recommendations in cold-start scenarios. (arXiv:1808.10664v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10664</link>
<description rdf:parseType="Literal">&lt;p&gt;Cold-start is a very common and still open problem in the Recommender Systems
literature. Since cold start items do not have any interaction, collaborative
algorithms are not applicable. One of the main strategies is to use pure or
hybrid content-based approaches, which usually yield to lower recommendation
quality than collaborative ones. Some techniques to optimize performance of
this type of approaches have been studied in recent past. One of them is called
feature weighting, which assigns to every feature a real value, called weight,
that estimates its importance. Statistical techniques for feature weighting
commonly used in Information Retrieval, like TF-IDF, have been adapted for
Recommender Systems, but they often do not provide sufficient quality
improvements. More recent approaches, FBSM and LFW, estimate weights by
leveraging collaborative information via machine learning, in order to learn
the importance of a feature based on other users opinions. This type of models
have shown promising results compared to classic statistical analyzes cited
previously. We propose a novel graph, feature-based machine learning model to
face the cold-start item scenario, learning the relevance of features from
probabilities of item-based collaborative filtering algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernardis_C/0/1/0/all/0/1&quot;&gt;Cesare Bernardis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dacrema_M/0/1/0/all/0/1&quot;&gt;Maurizio Ferrari Dacrema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremonesi_P/0/1/0/all/0/1&quot;&gt;Paolo Cremonesi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10724">
<title>Learning Data-adaptive Nonparametric Kernels. (arXiv:1808.10724v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.10724</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional kernels or their combinations are often not sufficiently flexible
to fit the data in complicated practical tasks. In this paper, we present a
Data-Adaptive Nonparametric Kernel (DANK) learning framework by imposing an
adaptive matrix on the kernel/Gram matrix in an entry-wise strategy. Since we
do not specify the formulation of the adaptive matrix, each entry in it can be
directly and flexibly learned from the data. Therefore, the solution space of
the learned kernel is largely expanded, which makes DANK flexible to adapt to
the data. Specifically, the proposed kernel learning framework can be
seamlessly embedded to support vector machines (SVM) and support vector
regression (SVR), which has the capability of enlarging the margin between
classes and reducing the model generalization error. Theoretically, we
demonstrate that the objective function of our devised model is
gradient-Lipschitz continuous. Thereby, the training process for kernel and
parameter learning in SVM/SVR can be efficiently optimized in a unified
framework. Further, to address the scalability issue in DANK, a
decomposition-based scalable approach is developed, of which the effectiveness
is demonstrated by both empirical studies and theoretical guarantees.
Experimentally, our method outperforms other representative kernel learning
based algorithms on various classification and regression benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fanghui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10788">
<title>Data-driven discovery of PDEs in complex datasets. (arXiv:1808.10788v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.10788</link>
<description rdf:parseType="Literal">&lt;p&gt;Many processes in science and engineering can be described by partial
differential equations (PDEs). Traditionally, PDEs are derived by considering
first principles of physics to derive the relations between the involved
physical quantities of interest. A different approach is to measure the
quantities of interest and use deep learning to reverse engineer the PDEs which
are describing the physical process.
&lt;/p&gt;
&lt;p&gt;In this paper we use machine learning, and deep learning in particular, to
discover PDEs hidden in complex data sets from measurement data. We include
examples of data from a known model problem, and real data from weather station
measurements. We show how necessary transformations of the input data amounts
to coordinate transformations in the discovered PDE, and we elaborate on
feature and model selection. It is shown that the dynamics of a non-linear,
second order PDE can be accurately described by an ordinary differential
equation which is automatically discovered by our deep learning algorithm. Even
more interestingly, we show that similar results apply in the context of more
complex simulations of the Swedish temperature distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Berg_J/0/1/0/all/0/1&quot;&gt;Jens Berg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nystrom_K/0/1/0/all/0/1&quot;&gt;Kaj Nystr&amp;#xf6;m&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.07450">
<title>Revised Note on Learning Algorithms for Quadratic Assignment with Graph Neural Networks. (arXiv:1706.07450v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.07450</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse problems correspond to a certain type of optimization problems
formulated over appropriate input distributions. Recently, there has been a
growing interest in understanding the computational hardness of these
optimization problems, not only in the worst case, but in an average-complexity
sense under this same input distribution.
&lt;/p&gt;
&lt;p&gt;In this revised note, we are interested in studying another aspect of
hardness, related to the ability to learn how to solve a problem by simply
observing a collection of previously solved instances. These &apos;planted
solutions&apos; are used to supervise the training of an appropriate predictive
model that parametrizes a broad class of algorithms, with the hope that the
resulting model will provide good accuracy-complexity tradeoffs in the average
sense.
&lt;/p&gt;
&lt;p&gt;We illustrate this setup on the Quadratic Assignment Problem, a fundamental
problem in Network Science. We observe that data-driven models based on Graph
Neural Networks offer intriguingly good performance, even in regimes where
standard relaxation based techniques appear to suffer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nowak_A/0/1/0/all/0/1&quot;&gt;Alex Nowak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Villar_S/0/1/0/all/0/1&quot;&gt;Soledad Villar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bandeira_A/0/1/0/all/0/1&quot;&gt;Afonso S. Bandeira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bruna_J/0/1/0/all/0/1&quot;&gt;Joan Bruna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05006">
<title>Two-sample Statistics Based on Anisotropic Kernels. (arXiv:1709.05006v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05006</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper introduces a new kernel-based Maximum Mean Discrepancy (MMD)
statistic for measuring the distance between two distributions given
finitely-many multivariate samples. When the distributions are locally
low-dimensional, the proposed test can be made more powerful to distinguish
certain alternatives by incorporating local covariance matrices and
constructing an anisotropic kernel. The kernel matrix is asymmetric; it
computes the affinity between $n$ data points and a set of $n_R$ reference
points, where $n_R$ can be drastically smaller than $n$. While the proposed
statistic can be viewed as a special class of Reproducing Kernel Hilbert Space
MMD, the consistency of the test is proved, under mild assumptions of the
kernel, as long as $\|p-q\| \sqrt{n} \to \infty $, and a finite-sample lower
bound of the testing power is obtained. Applications to flow cytometry and
diffusion MRI datasets are demonstrated, which motivate the proposed approach
to compare distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiuyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cloninger_A/0/1/0/all/0/1&quot;&gt;Alexander Cloninger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Coifman_R/0/1/0/all/0/1&quot;&gt;Ronald R. Coifman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07276">
<title>Removing Confounding Factors Associated Weights in Deep Neural Networks Improves the Prediction Accuracy for Healthcare Applications. (arXiv:1803.07276v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07276</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of healthcare data has brought the opportunities of
applying data-driven approaches, such as machine learning methods, to assist
diagnosis. Recently, many deep learning methods have been shown with impressive
successes in predicting disease status with raw input data. However, the
&quot;black-box&quot; nature of deep learning and the high-reliability requirement of
biomedical applications have created new challenges regarding the existence of
confounding factors. In this paper, with a brief argument that inappropriate
handling of confounding factors will lead to models&apos; sub-optimal performance in
real-world applications, we present an efficient method that can remove the
influences of confounding factors such as age or gender to improve the
across-cohort prediction accuracy of neural networks. One distinct advantage of
our method is that it only requires minimal changes of the baseline model&apos;s
architecture so that it can be plugged into most of the existing neural
networks. We conduct experiments across CT-scan, MRA, and EEG brain wave with
convolutional neural networks and LSTM to verify the efficiency of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhenglin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07174">
<title>FRnet-DTI: Deep Convolutional Neural Networks with Evolutionary and Structural Features for Drug-Target Interaction. (arXiv:1806.07174v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.07174</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of drug-target interaction prediction holds significant importance
in pharmacology and therapeutic drug design. In this paper, we present
FRnet-DTI, an auto encoder and a convolutional classifier for feature
manipulation and drug target interaction prediction. Two convolutional neural
neworks are proposed where one model is used for feature manipulation and the
other one for classification. Using the first method FRnet-1, we generate 4096
features for each of the instances in each of the datasets and use the second
method, FRnet-2, to identify interaction probability employing those features.
We have tested our method on four gold standard datasets exhaustively used by
other researchers. Experimental results shows that our method significantly
improves over the state-of-the-art method on three of the four drug-target
interaction gold standard datasets on both area under curve for Receiver
Operating Characteristic(auROC) and area under Precision Recall curve(auPR)
metric. We also introduce twenty new potential drug-target pairs for
interaction based on high prediction scores. Codes Available: https: // github.
com/ farshidrayhanuiu/ FRnet-DTI/ Web Implementation: http: // farshidrayhan.
pythonanywhere. com/ FRnet-DTI/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rayhan_F/0/1/0/all/0/1&quot;&gt;Farshid Rayhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Sajid Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavian_Z/0/1/0/all/0/1&quot;&gt;Zaynab Mousavian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farid_D/0/1/0/all/0/1&quot;&gt;Dewan Md Farid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shatabda_S/0/1/0/all/0/1&quot;&gt;Swakkhar Shatabda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09794">
<title>Correlated Time Series Forecasting using Deep Neural Networks: A Summary of Results. (arXiv:1808.09794v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.09794</link>
<description rdf:parseType="Literal">&lt;p&gt;Cyber-physical systems often consist of entities that interact with each
other over time. Meanwhile, as part of the continued digitization of industrial
processes, various sensor technologies are deployed that enable us to record
time-varying attributes (a.k.a., time series) of such entities, thus producing
correlated time series. To enable accurate forecasting on such correlated time
series, this paper proposes two models that combine convolutional neural
networks (CNNs) and recurrent neural networks (RNNs). The first model employs a
CNN on each individual time series, combines the convoluted features, and then
applies an RNN on top of the convoluted features in the end to enable
forecasting. The second model adds additional auto-encoders into the individual
CNNs, making the second model a multi-task learning model, which provides
accurate and robust forecasting. Experiments on two real-world correlated time
series data set suggest that the proposed two models are effective and
outperform baselines in most settings.
&lt;/p&gt;
&lt;p&gt;This report extends the paper &quot;Correlated Time Series Forecasting using
Multi-Task Deep Neural Networks,&quot; to appear in ACM CIKM 2018, by providing
additional experimental results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cirstea_R/0/1/0/all/0/1&quot;&gt;Razvan-Gabriel Cirstea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micu_D/0/1/0/all/0/1&quot;&gt;Darius-Valer Micu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muresan_G/0/1/0/all/0/1&quot;&gt;Gabriel-Marcel Muresan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chenjuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10101">
<title>DP-ADMM: ADMM-based Distributed Learning with Differential Privacy. (arXiv:1808.10101v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.10101</link>
<description rdf:parseType="Literal">&lt;p&gt;Privacy-preserving distributed machine learning has become more important
than ever due to the high demand of large-scale data processing. This paper
focuses on a class of machine learning problems that can be formulated as
regularized empirical risk minimization, and develops a privacy-preserving
approach to such learning problems. We use Alternating Direction Method of
Multipliers (ADMM) to decentralize the learning algorithm, and apply Gaussian
mechanisms to provide local differential privacy guarantee. However, simply
combining ADMM and local randomization mechanisms would result in a
nonconvergent algorithm with bad performance even under moderate privacy
guarantees. Besides, this approach cannot be applied when the objective
functions of the learning problems are non-smooth. To address these concerns,
we propose an improved ADMM-based Differentially Private distributed learning
algorithm, DP-ADMM, where an approximate augmented Lagrangian function and
Gaussian mechanisms with time-varying variance are utilized. We also apply the
moment accountant method to bound the total privacy loss. Our theoretical
analysis shows that DP-ADMM can be applied to convex learning problems with
both smooth and non-smooth objectives, provides differential privacy guarantee,
and achieves a convergence rate of $O(1/\sqrt{t})$, where $t$ is the number of
iterations. Our evaluations demonstrate that our approach can achieve good
convergence and accuracy with strong privacy guarantee.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zonghao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Rui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_Tin_E/0/1/0/all/0/1&quot;&gt;Eric Chan-Tin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yanmin Gong&lt;/a&gt;</dc:creator>
</item></rdf:RDF>