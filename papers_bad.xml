<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02328"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02077"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02099"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02276"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02322"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.03398"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.02240"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04143"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09904"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06581"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01508"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02081"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02097"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02101"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02181"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02199"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02204"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02246"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02253"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.02967"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07113"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03800"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.10031"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07976"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00102"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.02328">
<title>Generative Adversarial Perturbations. (arXiv:1712.02328v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.02328</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose novel generative models for creating adversarial
examples, slightly perturbed images resembling natural images but maliciously
crafted to fool pre-trained models. We present trainable deep neural networks
for transforming images to adversarial perturbations. Our proposed models can
produce image-agnostic and image-dependent perturbations for both targeted and
non-targeted attacks. We also demonstrate that similar architectures can
achieve impressive results in fooling classification and semantic segmentation
models, obviating the need for hand-crafting attack methods for each task.
Using extensive experiments on challenging high-resolution datasets such as
ImageNet and Cityscapes, we show that our perturbations achieve high fooling
rates with small perturbation norms. Moreover, our attacks are considerably
faster than current iterative methods at inference time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poursaeed_O/0/1/0/all/0/1&quot;&gt;Omid Poursaeed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsman_I/0/1/0/all/0/1&quot;&gt;Isay Katsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1&quot;&gt;Bicheng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1&quot;&gt;Serge Belongie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02077">
<title>Noise-resistant Deep Learning for Object Classification in 3D Point Clouds Using a Point Pair Descriptor. (arXiv:1804.02077v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.02077</link>
<description rdf:parseType="Literal">&lt;p&gt;Object retrieval and classification in point cloud data is challenged by
noise, irregular sampling density and occlusion. To address this issue, we
propose a point pair descriptor that is robust to noise and occlusion and
achieves high retrieval accuracy. We further show how the proposed descriptor
can be used in a 4D convolutional neural network for the task of object
classification. We propose a novel 4D convolutional layer that is able to learn
class-specific clusters in the descriptor histograms. Finally, we provide
experimental validation on 3 benchmark datasets, which confirms the superiority
of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bobkov_D/0/1/0/all/0/1&quot;&gt;Dmytro Bobkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sili Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_R/0/1/0/all/0/1&quot;&gt;Ruiqing Jian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iqbal_M/0/1/0/all/0/1&quot;&gt;Muhammad Iqbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinbach_E/0/1/0/all/0/1&quot;&gt;Eckehard Steinbach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02099">
<title>Reinforcement Learning based QoS/QoE-aware Service Function Chaining in Software-Driven 5G Slices. (arXiv:1804.02099v1 [cs.NI])</title>
<link>http://arxiv.org/abs/1804.02099</link>
<description rdf:parseType="Literal">&lt;p&gt;With the ever growing diversity of devices and applications that will be
connected to 5G networks, flexible and agile service orchestration with
acknowledged QoE that satisfies end-user&apos;s functional and QoS requirements is
necessary. SDN (Software-Defined Networking) and NFV (Network Function
Virtualization) are considered key enabling technologies for 5G core networks.
In this regard, this paper proposes a reinforcement learning based
QoS/QoE-aware Service Function Chaining (SFC) in SDN/NFV-enabled 5G slices.
First, it implements a lightweight QoS information collector based on LLDP,
which works in a piggyback fashion on the southbound interface of the SDN
controller, to enable QoS-awareness. Then, a DQN (Deep Q Network) based agent
framework is designed to support SFC in the context of NFV. The agent takes
into account the QoE and QoS as key aspects to formulate the reward so that it
is expected to maximize QoE while respecting QoS constraints. The experiment
results show that this framework exhibits good performance in QoE provisioning
and QoS requirements maintenance for SFC in dynamic network environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zonghang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yupeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_R/0/1/0/all/0/1&quot;&gt;Ruiming Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongfang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiaojiang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1&quot;&gt;Mohsen Guizani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02276">
<title>End-to-End Learning of Communications Systems Without a Channel Model. (arXiv:1804.02276v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1804.02276</link>
<description rdf:parseType="Literal">&lt;p&gt;The idea of end-to-end learning of communications systems through neural
network -based autoencoders has the shortcoming that it requires a
differentiable channel model. We present in this paper a novel learning
algorithm which alleviates this problem. The algorithm iterates between
supervised training of the receiver and reinforcement learning -based training
of the transmitter. We demonstrate that this approach works as well as fully
supervised methods on additive white Gaussian noise (AWGN) and Rayleigh
block-fading (RBF) channels. Surprisingly, while our method converges slower on
AWGN channels than supervised training, it converges faster on RBF channels.
Our results are a first step towards learning of communications systems over
any type of channel without prior assumptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1&quot;&gt;Fay&amp;#xe7;al Ait Aoudia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1&quot;&gt;Jakob Hoydis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02322">
<title>Comparing Dependencies in Probability Theory and General Rough Sets: Part-A. (arXiv:1804.02322v1 [math.LO])</title>
<link>http://arxiv.org/abs/1804.02322</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of comparing concepts of dependence in general rough sets with
those in probability theory had been initiated by the present author in some of
her recent papers. This problem relates to the identification of the
limitations of translating between the methodologies and possibilities in the
identification of concepts. Comparison of ideas of dependence in the approaches
had been attempted from a set-valuation based minimalist perspective by the
present author. The deviant probability framework has been the result of such
an approach. Other Bayesian reasoning perspectives (involving numeric
valuations) and frequentist approaches are also known. In this research,
duality results are adapted to demonstrate the possibility of improved
comparisons across implications between ontologically distinct concepts in a
common logic-based framework by the present author. Both positive and negative
results are proved that delimit possible comparisons in a clearer way by her.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mani_A/0/1/0/all/0/1&quot;&gt;A Mani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.03398">
<title>XCSP3: An Integrated Format for Benchmarking Combinatorial Constrained Problems. (arXiv:1611.03398v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1611.03398</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a major revision of the format XCSP 2.1, called XCSP3, to build
integrated representations of combinatorial constrained problems. This new
format is able to deal with mono/multi optimization, many types of variables,
cost functions, reification, views, annotations, variable quantification,
distributed, probabilistic and qualitative reasoning. The new format is made
compact, highly readable, and rather easy to parse. Interestingly, it captures
the structure of the problem models, through the possibilities of declaring
arrays of variables, and identifying syntactic and semantic groups of
constraints. The number of constraints is kept under control by introducing a
limited set of basic constraint forms, and producing almost automatically some
of their variations through lifting, restriction, sliding, logical combination
and relaxation mechanisms. As a result, XCSP3 encompasses practically all
constraints that can be found in major constraint solvers developed by the CP
community. A website, which is developed conjointly with the format, contains
many models and series of instances. The user can make sophisticated queries
for selecting instances from very precise criteria. The objective of XCSP3 is
to ease the effort required to test and compare different algorithms by
providing a common test-bed of combinatorial constrained instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boussemart_F/0/1/0/all/0/1&quot;&gt;Frederic Boussemart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lecoutre_C/0/1/0/all/0/1&quot;&gt;Christophe Lecoutre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Audemard_G/0/1/0/all/0/1&quot;&gt;Gilles Audemard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piette_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Piette&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.02240">
<title>Recurrent computations for visual pattern completion. (arXiv:1706.02240v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/1706.02240</link>
<description rdf:parseType="Literal">&lt;p&gt;Making inferences from partial information constitutes a critical aspect of
cognition. During visual perception, pattern completion enables recognition of
poorly visible or occluded objects. We combined psychophysics, physiology and
computational models to test the hypothesis that pattern completion is
implemented by recurrent computations and present three pieces of evidence that
are consistent with this hypothesis. First, subjects robustly recognized
objects even when rendered &amp;lt;15% visible, but recognition was largely impaired
when processing was interrupted by backward masking. Second, invasive
physiological responses along the human ventral cortex exhibited visually
selective responses to partially visible objects that were delayed compared to
whole objects, suggesting the need for additional computations. These
physiological delays were correlated with the effects of backward masking.
Third, state-of-the-art feed-forward computational architectures were not
robust to partial visibility. However, recognition performance was recovered
when the model was augmented with attractor-based recurrent connectivity. These
results provide a strong argument of plausibility for the role of recurrent
computations in making visual inferences from partial information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hanlin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Schrimpf_M/0/1/0/all/0/1&quot;&gt;Martin Schrimpf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lotter_B/0/1/0/all/0/1&quot;&gt;Bill Lotter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Moerman_C/0/1/0/all/0/1&quot;&gt;Charlotte Moerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Paredes_A/0/1/0/all/0/1&quot;&gt;Ana Paredes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Caro_J/0/1/0/all/0/1&quot;&gt;Josue Ortega Caro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hardesty_W/0/1/0/all/0/1&quot;&gt;Walter Hardesty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cox_D/0/1/0/all/0/1&quot;&gt;David Cox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kreiman_G/0/1/0/all/0/1&quot;&gt;Gabriel Kreiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04143">
<title>Benchmarking Single Image Dehazing and Beyond. (arXiv:1712.04143v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04143</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a comprehensive study and evaluation of existing
single image dehazing algorithms, using a new large-scale benchmark consisting
of both synthetic and real-world hazy images, called REalistic Single Image
DEhazing (RESIDE). RESIDE highlights diverse data sources and image contents,
and is divided into five subsets, each serving different training or evaluation
purposes. We further provide a rich variety of criteria for dehazing algorithm
evaluation, ranging from full-reference metrics, to no-reference metrics, to
subjective evaluation and the novel task-driven evaluation. Experiments on
RESIDE shed light on the comparisons and limitations of state-of-the-art
dehazing algorithms, and suggest promising future directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1&quot;&gt;Wenqi Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Dengpan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1&quot;&gt;Dan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09904">
<title>Ab initio Algorithmic Causal Deconvolution of Intertwined Programs and Networks by Generative Mechanism. (arXiv:1802.09904v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09904</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex data is usually produced by interacting sources with different
mechanisms. Here we introduce a parameter-free model-based approach, based upon
the seminal concept of Algorithmic Probability, that decomposes an observation
and signal into its most likely algorithmic generative sources. Our methods use
a causal calculus to infer model representations. We demonstrate the method
ability to distinguish interacting mechanisms and deconvolve them, regardless
of whether the objects produce strings, space-time evolution diagrams, images
or networks. We numerically test and evaluate our causal separation methods and
find that it can disentangle examples of observations from discrete dynamical
systems, and complex networks. We think that these causal separating techniques
can contribute to tackle the challenge of causation for estimations of better
rooted probability distributions thereby complementing more limited
statistical-oriented techniques that otherwise would lack model inference
capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenil_H/0/1/0/all/0/1&quot;&gt;Hector Zenil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiani_N/0/1/0/all/0/1&quot;&gt;Narsis A. Kiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zea_A/0/1/0/all/0/1&quot;&gt;Allan A. Zea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1&quot;&gt;Jesper Tegn&amp;#xe9;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06581">
<title>Variational Knowledge Graph Reasoning. (arXiv:1803.06581v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06581</link>
<description rdf:parseType="Literal">&lt;p&gt;Inferring missing links in knowledge graphs (KG) has attracted a lot of
attention from the research community. In this paper, we tackle a practical
query answering task involving predicting the relation of a given entity pair.
We frame this prediction problem as an inference problem in a probabilistic
graphical model and aim at resolving it from a variational inference
perspective. In order to model the relation between the query entity pair, we
assume that there exists an underlying latent variable (paths connecting two
nodes) in the KG, which carries the equivalent semantics of their relations.
However, due to the intractability of connections in large KGs, we propose to
use variation inference to maximize the evidence lower bound. More
specifically, our framework (\textsc{Diva}) is composed of three modules, i.e.
a posterior approximator, a prior (path finder), and a likelihood (path
reasoner). By using variational inference, we are able to incorporate them
closely into a unified architecture and jointly optimize them to perform KG
reasoning. With active interactions among these sub-modules, \textsc{Diva} is
better at handling noise and coping with more complex reasoning scenarios. In
order to evaluate our method, we conduct the experiment of the link prediction
task on multiple datasets and achieve state-of-the-art performances on both
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wenhan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xifeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01508">
<title>The Tsetlin Machine - A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic. (arXiv:1804.01508v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01508</link>
<description rdf:parseType="Literal">&lt;p&gt;Although simple individually, artificial neurons provide state-of-the-art
performance when interconnected in deep networks. Unknown to many, there exists
an arguably even simpler and more versatile learning mechanism, namely, the
Tsetlin Automaton. Merely by means of a single integer as memory, it learns the
optimal action in stochastic environments. In this paper, we introduce the
Tsetlin Machine, which solves complex pattern recognition problems with
easy-to-interpret propositional formulas, composed by a collective of Tsetlin
Automata. To eliminate the longstanding problem of vanishing signal-to-noise
ratio, the Tsetlin Machine orchestrates the automata using a novel game. Our
theoretical analysis establishes that the Nash equilibria of the game are
aligned with the propositional formulas that provide optimal pattern
recognition accuracy. This translates to learning without local optima, only
global ones. We argue that the Tsetlin Machine finds the propositional formula
that provides optimal accuracy, with probability arbitrarily close to unity. In
four distinct benchmarks, the Tsetlin Machine outperforms both Neural Networks,
SVMs, Random Forests, the Naive Bayes Classifier and Logistic Regression. It
further turns out that the accuracy advantage of the Tsetlin Machine increases
with lack of data. The Tsetlin Machine has a significant computational
performance advantage since both inputs, patterns, and outputs are expressed as
bits, while recognition of patterns relies on bit manipulation. The combination
of accuracy, interpretability, and computational simplicity makes the Tsetlin
Machine a promising tool for a wide range of domains, including safety-critical
medicine. Being the first of its kind, we believe the Tsetlin Machine will
kick-start completely new paths of research, with a potentially significant
impact on the AI field and the applications of AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1&quot;&gt;Ole-Christoffer Granmo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02081">
<title>Adaptive Diffusions for Scalable Learning over Graphs. (arXiv:1804.02081v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.02081</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion-based classifiers such as those relying on the Personalized
PageRank and the Heat kernel, enjoy remarkable classification accuracy at
modest computational requirements. Their performance however is affected by the
extent to which the chosen diffusion captures a typically unknown label
propagation mechanism, that can be specific to the underlying graph, and
potentially different for each class. The present work introduces a
disciplined, data-efficient approach to learning class-specific diffusion
functions adapted to the underlying network topology. The novel learning
approach leverages the notion of &quot;landing probabilities&quot; of class-specific
random walks, which can be computed efficiently, thereby ensuring scalability
to large graphs. This is supported by rigorous analysis of the properties of
the model as well as the proposed algorithms. Furthermore, a robust version of
the classifier facilitates learning even in noisy environments.
&lt;/p&gt;
&lt;p&gt;Classification tests on real networks demonstrate that adapting the diffusion
function to the given graph and observed labels, significantly improves the
performance over fixed diffusions; reaching -- and many times surpassing -- the
classification accuracy of computationally heavier state-of-the-art competing
methods, that rely on node embeddings and deep neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Berberidis_D/0/1/0/all/0/1&quot;&gt;Dimitris Berberidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nikolakopoulos_A/0/1/0/all/0/1&quot;&gt;Athanasios N. Nikolakopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02097">
<title>Multi-view Banded Spectral Clustering with application to ICD9 clustering. (arXiv:1804.02097v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1804.02097</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent development in methodology, community detection remains a
challenging problem. Existing literature largely focuses on the standard
setting where a network is learned using an observed adjacency matrix from a
single data source. Constructing a shared network from multiple data sources is
more challenging due to the heterogeneity across populations. Additionally,
when a natural ordering on the nodes of interest arises, no existing method
takes such information into account. Motivated by grouping the International
classification of diseases, ninth revision, (ICD9) codes to represent
clinically meaningful phenotypes, we propose a novel spectral clustering method
that optimally combines multiple data sources while leveraging the prior
ordering knowledge. The proposed method combines a banding step to encourage a
desired moving average structure with a subsequent weighting step to maximize
consensus across multiple sources. Its statistical performance is thoroughly
studied under a multi-view stochastic block model. We also provide a simple
rule of choosing weights in practice. The efficacy and robustness of the method
is fully demonstrated through extensive simulations. Finally, we apply the
method to the ICD9 coding system and yield a very insightful clustering
structure by integrating information from a large claim database and two
healthcare systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Luwan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liao_K/0/1/0/all/0/1&quot;&gt;Katherine Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohane_I/0/1/0/all/0/1&quot;&gt;Issac Kohane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1&quot;&gt;Tianxi Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02101">
<title>Modeling Popularity in Asynchronous Social Media Streams with Recurrent Neural Networks. (arXiv:1804.02101v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1804.02101</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding and predicting the popularity of online items is an important
open problem in social media analysis. Considerable progress has been made
recently in data-driven predictions, and in linking popularity to external
promotions. However, the existing methods typically focus on a single source of
external influence, whereas for many types of online content such as YouTube
videos or news articles, attention is driven by multiple heterogeneous sources
simultaneously - e.g. microblogs or traditional media coverage. Here, we
propose RNN-MAS, a recurrent neural network for modeling asynchronous streams.
It is a sequence generator that connects multiple streams of different
granularity via joint inference. We show RNN-MAS not only to outperform the
current state-of-the-art Youtube popularity prediction system by 17%, but also
to capture complex dynamics, such as seasonal trends of unseen influence. We
define two new metrics: promotion score quantifies the gain in popularity from
one unit of promotion for a Youtube video; the loudness level captures the
effects of a particular user tweeting about the video. We use the loudness
level to compare the effects of a video being promoted by a single
highly-followed user (in the top 1% most followed users) against being promoted
by a group of mid-followed users. We find that results depend on the type of
content being promoted: superusers are more successful in promoting Howto and
Gaming videos, whereas the cohort of regular users are more influential for
Activism videos. This work provides more accurate and explainable popularity
predictions, as well as computational tools for content producers and marketers
to allocate resources for promotion campaigns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Swapnil Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1&quot;&gt;Marian-Andrei Rizoiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lexing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02181">
<title>Generative adversarial network-based approach to signal reconstruction from magnitude spectrograms. (arXiv:1804.02181v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1804.02181</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address the problem of reconstructing a time-domain signal
(or a phase spectrogram) solely from a magnitude spectrogram. Since magnitude
spectrograms do not contain phase information, we must restore or infer phase
information to reconstruct a time-domain signal. One widely used approach for
dealing with the signal reconstruction problem was proposed by Griffin and Lim.
This method usually requires many iterations for the signal reconstruction
process and depending on the inputs, it does not always produce high-quality
audio signals. To overcome these shortcomings, we apply a learning-based
approach to the signal reconstruction problem by modeling the signal
reconstruction process using a deep neural network and training it using the
idea of a generative adversarial network. Experimental evaluations revealed
that our method was able to reconstruct signals faster with higher quality than
the Griffin-Lim method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oyamada_K/0/1/0/all/0/1&quot;&gt;Keisuke Oyamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kameoka_H/0/1/0/all/0/1&quot;&gt;Hirokazu Kameoka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kaneko_T/0/1/0/all/0/1&quot;&gt;Takuhiro Kaneko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tanaka_K/0/1/0/all/0/1&quot;&gt;Kou Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hojo_N/0/1/0/all/0/1&quot;&gt;Nobukatsu Hojo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ando_H/0/1/0/all/0/1&quot;&gt;Hiroyasu Ando&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02199">
<title>Mix and match networks: encoder-decoder alignment for zero-pair image translation. (arXiv:1804.02199v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.02199</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of image translation between domains or modalities for
which no direct paired data is available (i.e. zero-pair translation). We
propose mix and match networks, based on multiple encoders and decoders aligned
in such a way that other encoder-decoder pairs can be composed at test time to
perform unseen image translation tasks between domains or modalities for which
explicit paired samples were not seen during training. We study the impact of
autoencoders, side information and losses in improving the alignment and
transferability of trained pairwise translation models to unseen translations.
We show our approach is scalable and can perform colorization and style
transfer between unseen combinations of domains. We evaluate our system in a
challenging cross-modal setting where semantic segmentation is estimated from
depth images, without explicit access to any depth-semantic segmentation
training pairs. Our model outperforms baselines based on pix2pix and CycleGAN
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaxing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1&quot;&gt;Joost van de Weijer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1&quot;&gt;Luis Herranz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02204">
<title>Sequence Training of DNN Acoustic Models With Natural Gradient. (arXiv:1804.02204v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.02204</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Network (DNN) acoustic models often use discriminative sequence
training that optimises an objective function that better approximates the word
error rate (WER) than frame-based training. Sequence training is normally
implemented using Stochastic Gradient Descent (SGD) or Hessian Free (HF)
training. This paper proposes an alternative batch style optimisation framework
that employs a Natural Gradient (NG) approach to traverse through the parameter
space. By correcting the gradient according to the local curvature of the
KL-divergence, the NG optimisation process converges more quickly than HF.
Furthermore, the proposed NG approach can be applied to any sequence
discriminative training criterion. The efficacy of the NG method is shown using
experiments on a Multi-Genre Broadcast (MGB) transcription task that
demonstrates both the computational efficiency and the accuracy of the
resulting DNN models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haider_A/0/1/0/all/0/1&quot;&gt;Adnan Haider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1&quot;&gt;Philip C. Woodland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02246">
<title>Adaptive Cost-sensitive Online Classification. (arXiv:1804.02246v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.02246</link>
<description rdf:parseType="Literal">&lt;p&gt;Cost-Sensitive Online Classification has drawn extensive attention in recent
years, where the main approach is to directly online optimize two well-known
cost-sensitive metrics: (i) weighted sum of sensitivity and specificity; (ii)
weighted misclassification cost. However, previous existing methods only
considered first-order information of data stream. It is insufficient in
practice, since many recent studies have proved that incorporating second-order
information enhances the prediction performance of classification models. Thus,
we propose a family of cost-sensitive online classification algorithms with
adaptive regularization in this paper. We theoretically analyze the proposed
algorithms and empirically validate their effectiveness and properties in
extensive experiments. Then, for better trade off between the performance and
efficiency, we further introduce the sketching technique into our algorithms,
which significantly accelerates the computational speed with quite slight
performance loss. Finally, we apply our algorithms to tackle several online
anomaly detection tasks from real world. Promising results prove that the
proposed algorithms are effective and efficient in solving cost-sensitive
online classification problems in various real-world domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peilin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1&quot;&gt;Steven C. H. Hoi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1&quot;&gt;Mingkui Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Junzhou Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02253">
<title>A comparison of deep networks with ReLU activation function and linear spline-type methods. (arXiv:1804.02253v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.02253</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) generate much richer function spaces than shallow
networks. Since the function spaces induced by shallow networks have several
approximation theoretic drawbacks, this explains, however, not necessarily the
success of deep networks. In this article we take another route by comparing
the expressive power of DNNs with ReLU activation function to piecewise linear
spline methods. We show that MARS (multivariate adaptive regression splines) is
improper learnable by DNNs in the sense that for any given function that can be
expressed as a function in MARS with $M$ parameters there exists a multilayer
neural network with $O(M \log (M/\varepsilon))$ parameters that approximates
this function up to sup-norm error $\varepsilon.$ We show a similar result for
expansions with respect to the Faber-Schauder system. Based on this, we derive
risk comparison inequalities that bound the statistical risk of fitting a
neural network by the statistical risk of spline-based methods. This shows that
deep networks perform better or only slightly worse than the considered spline
methods. We provide a constructive proof for the function approximations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eckle_K/0/1/0/all/0/1&quot;&gt;Konstantin Eckle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmidt_Hieber_J/0/1/0/all/0/1&quot;&gt;Johannes Schmidt-Hieber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.02967">
<title>Distributed Convex Optimization with Many Convex Constraints. (arXiv:1610.02967v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1610.02967</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of solving convex optimization problems with many
convex constraints in a distributed setting. Our approach is based on an
extension of the alternating direction method of multipliers (ADMM) that
recently gained a lot of attention in the Big Data context. Although it has
been invented decades ago, ADMM so far can be applied only to unconstrained
problems and problems with linear equality or inequality constraints. Our
extension can handle arbitrary inequality constraints directly. It combines the
ability of ADMM to solve convex optimization problems in a distributed setting
with the ability of the Augmented Lagrangian method to solve constrained
optimization problems, and as we show, it inherits the convergence guarantees
of ADMM and the Augmented Lagrangian method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Giesen_J/0/1/0/all/0/1&quot;&gt;Joachim Giesen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Laue_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Laue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07113">
<title>Query-Efficient Black-box Adversarial Examples (superceded). (arXiv:1712.07113v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07113</link>
<description rdf:parseType="Literal">&lt;p&gt;Note that this paper is superceded by &quot;Black-Box Adversarial Attacks with
Limited Queries and Information.&quot;
&lt;/p&gt;
&lt;p&gt;Current neural network-based image classifiers are susceptible to adversarial
examples, even in the black-box setting, where the attacker is limited to query
access without access to gradients. Previous methods --- substitute networks
and coordinate-based finite-difference methods --- are either unreliable or
query-inefficient, making these methods impractical for certain problems.
&lt;/p&gt;
&lt;p&gt;We introduce a new method for reliably generating adversarial examples under
more restricted, practical black-box threat models. First, we apply natural
evolution strategies to perform black-box attacks using two to three orders of
magnitude fewer queries than previous methods. Second, we introduce a new
algorithm to perform targeted adversarial attacks in the partial-information
setting, where the attacker only has access to a limited number of target
classes. Using these techniques, we successfully perform the first targeted
adversarial attack against a commercially deployed machine learning system, the
Google Cloud Vision API, in the partial information setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilyas_A/0/1/0/all/0/1&quot;&gt;Andrew Ilyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engstrom_L/0/1/0/all/0/1&quot;&gt;Logan Engstrom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athalye_A/0/1/0/all/0/1&quot;&gt;Anish Athalye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jessy Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03800">
<title>Drug response prediction by ensemble learning and drug-induced gene expression signatures. (arXiv:1802.03800v2 [q-bio.GN] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03800</link>
<description rdf:parseType="Literal">&lt;p&gt;Chemotherapeutic response of cancer cells to a given compound is one of the
most fundamental information one requires to design anti-cancer drugs. Recent
advances in producing large drug screens against cancer cell lines provided an
opportunity to apply machine learning methods for this purpose. In addition to
cytotoxicity databases, considerable amount of drug-induced gene expression
data has also become publicly available. Following this, several methods that
exploit omics data were proposed to predict drug activity on cancer cells.
However, due to the complexity of cancer drug mechanisms, none of the existing
methods are perfect. One possible direction, therefore, is to combine the
strengths of both the methods and the databases for improved performance. We
demonstrate that integrating a large number of predictions by the proposed
method improves the performance for this task. The predictors in the ensemble
differ in several aspects such as the method itself, the number of tasks method
considers (multi-task vs. single-task) and the subset of data considered
(sub-sampling). We show that all these different aspects contribute to the
success of the final ensemble. In addition, we attempt to use the drug screen
data together with two novel signatures produced from the drug-induced gene
expression profiles of cancer cell lines. Finally, we evaluate the method
predictions by in vitro experiments in addition to the tests on data sets.The
predictions of the methods, the signatures and the software are available from
\url{&lt;a href=&quot;http://mtan.etu.edu.tr/drug-response-prediction/&quot;&gt;this http URL&lt;/a&gt;}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tan_M/0/1/0/all/0/1&quot;&gt;Mehmet Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ozgul_O/0/1/0/all/0/1&quot;&gt;Ozan F&amp;#x131;rat &amp;#xd6;zg&amp;#xfc;l&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bardak_B/0/1/0/all/0/1&quot;&gt;Batuhan Bardak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Eksioglu_I/0/1/0/all/0/1&quot;&gt;I&amp;#x15f;&amp;#x131;ksu Ek&amp;#x15f;io&amp;#x11f;lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sabuncuoglu_S/0/1/0/all/0/1&quot;&gt;Suna Sabuncuo&amp;#x11f;lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.10031">
<title>The Mirage of Action-Dependent Baselines in Reinforcement Learning. (arXiv:1802.10031v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.10031</link>
<description rdf:parseType="Literal">&lt;p&gt;Policy gradient methods are a widely used class of model-free reinforcement
learning algorithms where a state-dependent baseline is used to reduce gradient
estimator variance. Several recent papers extend the baseline to depend on both
the state and action and suggest that this significantly reduces variance and
improves sample efficiency without introducing bias into the gradient
estimates. To better understand this development, we decompose the variance of
the policy gradient estimator and numerically show that learned
state-action-dependent baselines do not in fact reduce variance over a
state-dependent baseline in commonly tested benchmark domains. We confirm this
unexpected result by reviewing the open-source code accompanying these prior
papers, and show that subtle implementation decisions cause deviations from the
methods presented in the papers and explain the source of the previously
observed empirical gains. Furthermore, the variance decomposition highlights
areas for improvement, which we demonstrate by illustrating a simple change to
the typical value function parameterization that can significantly improve
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucker_G/0/1/0/all/0/1&quot;&gt;George Tucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhupatiraju_S/0/1/0/all/0/1&quot;&gt;Surya Bhupatiraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Shixiang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard E. Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghahramani_Z/0/1/0/all/0/1&quot;&gt;Zoubin Ghahramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07976">
<title>A Survey on Application of Machine Learning Techniques in Optical Networks. (arXiv:1803.07976v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07976</link>
<description rdf:parseType="Literal">&lt;p&gt;Today, the amount of data that can be retrieved from communications networks
is extremely high and diverse (e.g., data regarding users behavior, traffic
traces, network alarms, signal quality indicators, etc.). Advanced mathematical
tools are required to extract useful information from this large set of network
data. In particular, Machine Learning (ML) is regarded as a promising
methodological area to perform network-data analysis and enable, e.g.,
automatized network self-configuration and fault management. In this survey we
classify and describe relevant studies dealing with the applications of ML to
optical communications and networking. Optical networks and system are facing
an unprecedented growth in terms of complexity due to the introduction of a
huge number of adjustable parameters (such as routing configurations,
modulation format, symbol rate, coding schemes, etc.), mainly due to the
adoption of, among the others, coherent transmission/reception technology,
advanced digital signal processing and to the presence of nonlinear effects in
optical fiber systems. Although a good number of research papers have appeared
in the last years, the application of ML to optical networks is still in its
early stage. In this survey we provide an introductory reference for
researchers and practitioners interested in this field. To stimulate further
work in this area, we conclude the paper proposing new possible research
directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musumeci_F/0/1/0/all/0/1&quot;&gt;Francesco Musumeci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rottondi_C/0/1/0/all/0/1&quot;&gt;Cristina Rottondi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_A/0/1/0/all/0/1&quot;&gt;Avishek Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macaluso_I/0/1/0/all/0/1&quot;&gt;Irene Macaluso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zibar_D/0/1/0/all/0/1&quot;&gt;Darko Zibar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruffini_M/0/1/0/all/0/1&quot;&gt;Marco Ruffini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tornatore_M/0/1/0/all/0/1&quot;&gt;Massimo Tornatore&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09374">
<title>Generalized Hadamard-Product Fusion Operators for Visual Question Answering. (arXiv:1803.09374v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09374</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a generalized class of multimodal fusion operators for the task of
visual question answering (VQA). We identify generalizations of existing
multimodal fusion operators based on the Hadamard product, and show that
specific non-trivial instantiations of this generalized fusion operator exhibit
superior performance in terms of OpenEnded accuracy on the VQA task. In
particular, we introduce Nonlinearity Ensembling, Feature Gating, and
post-fusion neural network layers as fusion operator components, culminating in
an absolute percentage point improvement of $1.1\%$ on the VQA 2.0 test-dev set
over baseline fusion operators, which use the same features as input. We use
our findings as evidence that our generalized class of fusion operators could
lead to the discovery of even superior task-specific operators when used as a
search space in an architecture search over fusion operators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duke_B/0/1/0/all/0/1&quot;&gt;Brendan Duke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1&quot;&gt;Graham W. Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00102">
<title>Collaborative targeted inference from continuously indexed nuisance parameter estimators. (arXiv:1804.00102v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00102</link>
<description rdf:parseType="Literal">&lt;p&gt;We wish to infer the value of a parameter at a law from which we sample
independent observations. The parameter is smooth and we can define two
variation-independent features of the law, its $Q$- and $G$-components, such
that estimating them consistently at a fast enough product of rates allows to
build a confidence interval (CI) with a given asymptotic level from a plain
targeted minimum loss estimator (TMLE). Say that the above product is not fast
enough and the algorithm for the $G$-component is fine-tuned by a real-valued
$h$. A plain TMLE with an $h$ chosen by cross-validation would typically not
yield a CI. We construct a collaborative TMLE (C-TMLE) and show under mild
conditions that, if there exists an oracle $h$ that makes a bulky remainder
term asymptotically Gaussian, then the C-TMLE yields a CI. We illustrate our
findings with the inference of the average treatment effect. We conduct a
simulation study where the $G$-component is estimated by the LASSO and $h$ is
the bound on the coefficients&apos; norms. It sheds light on small sample
properties, in the face of low- to high-dimensional baseline covariates, and
possibly positivity violation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ju_C/0/1/0/all/0/1&quot;&gt;Cheng Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chambaz_A/0/1/0/all/0/1&quot;&gt;Antoine Chambaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laan_M/0/1/0/all/0/1&quot;&gt;Mark J. van der Laan&lt;/a&gt;</dc:creator>
</item></rdf:RDF>