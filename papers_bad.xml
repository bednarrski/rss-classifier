<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-09-13T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04632"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04982"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10694"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04737"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04739"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04861"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04918"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04942"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04988"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.05001"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.05021"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.05070"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09904"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.07876"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05085"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04598"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04663"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04668"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04729"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04747"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04758"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04790"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04855"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04933"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04951"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04967"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04993"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04997"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.05032"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.05042"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.05043"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.05074"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.05077"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1505.00398"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.01845"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00658"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08571"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00534"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02977"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03218"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01771"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.02266"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04768"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02652"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02840"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03839"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04069"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04188"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04294"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.04430"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1809.04632">
<title>Efficient Global Optimization using Deep Gaussian Processes. (arXiv:1809.04632v1 [math.OC])</title>
<link>http://arxiv.org/abs/1809.04632</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient Global Optimization (EGO) is widely used for the optimization of
computationally expensive black-box functions. It uses a surrogate modeling
technique based on Gaussian Processes (Kriging). However, due to the use of a
stationary covariance, Kriging is not well suited for approximating non
stationary functions. This paper explores the integration of Deep Gaussian
processes (DGP) in EGO framework to deal with the non-stationary issues and
investigates the induced challenges and opportunities. Numerical
experimentations are performed on analytical problems to highlight the
different aspects of DGP and EGO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hebbal_A/0/1/0/all/0/1&quot;&gt;Ali Hebbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Brevault_L/0/1/0/all/0/1&quot;&gt;Loic Brevault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Balesdent_M/0/1/0/all/0/1&quot;&gt;Mathieu Balesdent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Talbi_E/0/1/0/all/0/1&quot;&gt;El-Ghazali Talbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Melab_N/0/1/0/all/0/1&quot;&gt;Nouredine Melab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04982">
<title>High-Accuracy Inference in Neuromorphic Circuits using Hardware-Aware Training. (arXiv:1809.04982v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1809.04982</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuromorphic Multiply-And-Accumulate (MAC) circuits utilizing synaptic weight
elements based on SRAM or novel Non-Volatile Memories (NVMs) provide a
promising approach for highly efficient hardware representations of neural
networks. NVM density and robustness requirements suggest that off-line
training is the right choice for &quot;edge&quot; devices, since the requirements for
synapse precision are much less stringent. However, off-line training using
ideal mathematical weights and activations can result in significant loss of
inference accuracy when applied to non-ideal hardware. Non-idealities such as
multi-bit quantization of weights and activations, non-linearity of weights,
finite max/min ratios of NVM elements, and asymmetry of positive and negative
weight components all result in degraded inference accuracy. In this work, it
is demonstrated that non-ideal Multi-Layer Perceptron (MLP) architectures using
low bitwidth weights and activations can be trained with negligible loss of
inference accuracy relative to their Floating Point-trained counterparts using
a proposed off-line, continuously differentiable HW-aware training algorithm.
The proposed algorithm is applicable to a wide range of hardware models, and
uses only standard neural network training methods. The algorithm is
demonstrated on the MNIST and EMNIST datasets, using standard MLPs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obradovic_B/0/1/0/all/0/1&quot;&gt;Borna Obradovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakshit_T/0/1/0/all/0/1&quot;&gt;Titash Rakshit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatcher_R/0/1/0/all/0/1&quot;&gt;Ryan Hatcher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kittl_J/0/1/0/all/0/1&quot;&gt;Jorge A. Kittl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodder_M/0/1/0/all/0/1&quot;&gt;Mark S. Rodder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10694">
<title>Tiramisu: A Code Optimization Framework for High Performance Systems. (arXiv:1804.10694v2 [cs.PL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10694</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Tiramisu, a polyhedral framework designed to generate
high performance code for multiple platforms including multicores, GPUs, and
distributed machines. Tiramisu introduces a scheduling language with novel
extensions to explicitly manage the complexities that arise when targeting
these systems. The extensions include explicit communication, synchronization,
and mapping buffers to different memory hierarchies. Tiramisu relies on a
flexible representation based on the polyhedral model and explicitly uses a
well-defined four-level IR that allows full separation between the algorithms,
loop transformations, data-layouts, and communication. This separation
simplifies targeting multiple hardware architectures with the same algorithm.
We evaluate Tiramisu by writing a set of image processing and stencil
benchmarks and compare it with state-of-the-art compilers. We show that
Tiramisu matches or outperforms existing compilers on different hardware
architectures, including multicore CPUs, GPUs, and distributed machines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baghdadi_R/0/1/0/all/0/1&quot;&gt;Riyadh Baghdadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_J/0/1/0/all/0/1&quot;&gt;Jessica Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romdhane_M/0/1/0/all/0/1&quot;&gt;Malek Ben Romdhane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sozzo_E/0/1/0/all/0/1&quot;&gt;Emanuele Del Sozzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suriana_P/0/1/0/all/0/1&quot;&gt;Patricia Suriana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akkas_A/0/1/0/all/0/1&quot;&gt;Abdurrahman Akkas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamil_S/0/1/0/all/0/1&quot;&gt;Shoaib Kamil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amarasinghe_S/0/1/0/all/0/1&quot;&gt;Saman Amarasinghe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04737">
<title>Fairness-aware Classification: Criterion, Convexity, and Bounds. (arXiv:1809.04737v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04737</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness-aware classification is receiving increasing attention in the
machine learning fields. Recently research proposes to formulate the
fairness-aware classification as constrained optimization problems. However,
several limitations exist in previous works due to the lack of a theoretical
framework for guiding the formulation. In this paper, we propose a general
framework for learning fair classifiers which addresses previous limitations.
The framework formulates various commonly-used fairness metrics as convex
constraints that can be directly incorporated into classic classification
models. Within the framework, we propose a constraint-free criterion on the
training data which ensures that any classifier learned from the data is fair.
We also derive the constraints which ensure that the real fairness metric is
satisfied when surrogate functions are used to achieve convexity. Our framework
can be used to for formulating fairness-aware classification with fairness
guarantee and computational efficiency. The experiments using real-world
datasets demonstrate our theoretical results and show the effectiveness of
proposed framework and methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yongkai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xintao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04739">
<title>SafeCity: Understanding Diverse Forms of Sexual Harassment Personal Stories. (arXiv:1809.04739v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.04739</link>
<description rdf:parseType="Literal">&lt;p&gt;With the recent rise of #MeToo, an increasing number of personal stories
about sexual harassment and sexual abuse have been shared online. In order to
push forward the fight against such harassment and abuse, we present the task
of automatically categorizing and analyzing various forms of sexual harassment,
based on stories shared on the online forum SafeCity. For the labels of
groping, ogling, and commenting, our single-label CNN-RNN model achieves an
accuracy of 86.5%, and our multi-label model achieves a Hamming score of 82.5%.
Furthermore, we present analysis using LIME, first-derivative saliency
heatmaps, activation clustering, and embedding visualization to interpret
neural model predictions and demonstrate how this extracts features that can
help automatically fill out incident reports, identify unsafe areas, avoid
unsafe practices, and &apos;pin the creeps&apos;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlekar_S/0/1/0/all/0/1&quot;&gt;Sweta Karlekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04861">
<title>Relevance in Structured Argumentation. (arXiv:1809.04861v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.04861</link>
<description rdf:parseType="Literal">&lt;p&gt;We study properties related to relevance in non-monotonic consequence
relations obtained by systems of structured argumentation. Relevance desiderata
concern the robustness of a consequence relation under the addition of
irrelevant information. For an account of what (ir)relevance amounts to we use
syntactic and semantic considerations. Syntactic criteria have been proposed in
the domain of relevance logic and were recently used in argumentation theory
under the names of non-interference and crash-resistance. The basic idea is
that the conclusions of a given argumentative theory should be robust under
adding information that shares no propositional variables with the original
database. Some semantic relevance criteria are known from non-monotonic logic.
For instance, cautious monotony states that if we obtain certain conclusions
from an argumentation theory, we may expect to still obtain the same
conclusions if we add some of them to the given database. In this paper we
investigate properties of structured argumentation systems that warrant
relevance desiderata.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borg_A/0/1/0/all/0/1&quot;&gt;AnneMarie Borg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strasser_C/0/1/0/all/0/1&quot;&gt;Christian Stra&amp;#xdf;er&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04918">
<title>Coordination-driven learning in multi-agent problem spaces. (arXiv:1809.04918v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1809.04918</link>
<description rdf:parseType="Literal">&lt;p&gt;We discuss the role of coordination as a direct learning objective in
multi-agent reinforcement learning (MARL) domains. To this end, we present a
novel means of quantifying coordination in multi-agent systems, and discuss the
implications of using such a measure to optimize coordinated agent policies.
This concept has important implications for adversary-aware RL, which we take
to be a sub-domain of multi-agent learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barton_S/0/1/0/all/0/1&quot;&gt;Sean L. Barton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waytowich_N/0/1/0/all/0/1&quot;&gt;Nicholas R. Waytowich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asher_D/0/1/0/all/0/1&quot;&gt;Derrik E. Asher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04942">
<title>Fixed set search applied to the traveling salesman problem. (arXiv:1809.04942v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.04942</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present a new population based metaheuristic called the
fixed set search (FSS). The proposed approach represents a method of adding a
learning mechanism to the greedy randomized adaptive search procedure (GRASP).
The basic concept of FSS is to avoid focusing on specific high quality
solutions but on parts or elements that such solutions have. This is done
through fixing a set of elements that exist in such solutions and dedicating
computational effort to finding near optimal solutions for the underlying
subproblem. The simplicity of implementing the proposed method is illustrated
on the traveling salesman problem. Our computational experiments show that the
FSS manages to find significantly better solutions than the GRASP it is based
on and also the dynamic convexized method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jovanovic_R/0/1/0/all/0/1&quot;&gt;Raka Jovanovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuba_M/0/1/0/all/0/1&quot;&gt;Milan Tuba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voss_S/0/1/0/all/0/1&quot;&gt;Stefan Voss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04988">
<title>Sequential Coordination of Deep Models for Learning Visual Arithmetic. (arXiv:1809.04988v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04988</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving machine intelligence requires a smooth integration of perception
and reasoning, yet models developed to date tend to specialize in one or the
other; sophisticated manipulation of symbols acquired from rich perceptual
spaces has so far proved elusive. Consider a visual arithmetic task, where the
goal is to carry out simple arithmetical algorithms on digits presented under
natural conditions (e.g. hand-written, placed randomly). We propose a
two-tiered architecture for tackling this problem. The lower tier consists of a
heterogeneous collection of information processing modules, which can include
pre-trained deep neural networks for locating and extracting characters from
the image, as well as modules performing symbolic transformations on the
representations extracted by perception. The higher tier consists of a
controller, trained using reinforcement learning, which coordinates the modules
in order to solve the high-level task. For instance, the controller may learn
in what contexts to execute the perceptual networks and what symbolic
transformations to apply to their outputs. The resulting model is able to solve
a variety of tasks in the visual arithmetic domain, and has several advantages
over standard, architecturally homogeneous feedforward networks including
improved sample efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crawford_E/0/1/0/all/0/1&quot;&gt;Eric Crawford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1&quot;&gt;Guillaume Rabusseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1&quot;&gt;Joelle Pineau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.05001">
<title>Reductive property of new fuzzy reasoning method based on distance measure. (arXiv:1809.05001v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.05001</link>
<description rdf:parseType="Literal">&lt;p&gt;Firstly in this paper we propose a new criterion function for evaluation of
the reductive property about the fuzzy reasoning result for fuzzy modus ponens
and fuzzy modus tollens. Secondly unlike fuzzy reasoning methods based on the
similarity measure, we propose a new fuzzy reasoning method based on distance
measure. Thirdly the reductive property for 5 fuzzy reasoning methods are
checked with respect to fuzzy modus ponens and fuzzy modus tollens. Through the
experiment, we show that proposed method is better than the previous methods in
accordance with human thinking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1&quot;&gt;Son-il Kwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gum-ju Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugeno_M/0/1/0/all/0/1&quot;&gt;Michio Sugeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gwang-chol Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Son_M/0/1/0/all/0/1&quot;&gt;Myong-suk Son&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyok-chol Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_U/0/1/0/all/0/1&quot;&gt;Un-ha Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.05021">
<title>State-Space Identification of Unmanned Helicopter Dynamics using Invasive Weed Optimization Algorithm on Flight Data. (arXiv:1809.05021v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1809.05021</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to achieve a good level of autonomy in unmanned helicopters, an
accurate replication of vehicle dynamics is required, which is achievable
through precise mathematical modeling. This paper aims to identify a parametric
state-space system for an unmanned helicopter to a good level of accuracy using
Invasive Weed Optimization (IWO) algorithm. The flight data of Align TREX 550
flybarless helicopter is used in the identification process. The rigid-body
dynamics of the helicopter is modeled in a state-space form that has 40
parameters, which serve as control variables for the IWO algorithm. The results
after 1000 iterations were compared with the traditionally used Prediction
Error Minimization (PEM) method and also with Genetic Algorithm (GA), which
serve as references. Results show a better level of correlation between the
actual and estimated responses of the system identified using IWO to that of
PEM and GA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+B_N/0/1/0/all/0/1&quot;&gt;Navaneethkrishnan B&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_P/0/1/0/all/0/1&quot;&gt;Pranjal Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saksena_S/0/1/0/all/0/1&quot;&gt;Saumya Kumaar Saksena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anand_G/0/1/0/all/0/1&quot;&gt;Gautham Anand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omkar_S/0/1/0/all/0/1&quot;&gt;S N Omkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.05070">
<title>Physical Primitive Decomposition. (arXiv:1809.05070v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.05070</link>
<description rdf:parseType="Literal">&lt;p&gt;Objects are made of parts, each with distinct geometry, physics,
functionality, and affordances. Developing such a distributed, physical,
interpretable representation of objects will facilitate intelligent agents to
better explore and interact with the world. In this paper, we study physical
primitive decomposition---understanding an object through its components, each
with physical and geometric attributes. As annotated data for object parts and
physics are rare, we propose a novel formulation that learns physical
primitives by explaining both an object&apos;s appearance and its behaviors in
physical events. Our model performs well on block towers and tools in both
synthetic and real scenarios; we also demonstrate that visual and physical
observations often provide complementary signals. We further present ablation
and behavioral studies to better understand our model and contrast it with
human performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhijian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1&quot;&gt;William T. Freeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09904">
<title>Algorithmic Causal Deconvolution of Intertwined Programs and Networks by Generative Mechanism. (arXiv:1802.09904v8 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09904</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex data usually results from the interaction of objects produced by
different generating mechanisms. Here we introduce a universal, unsupervised
and parameter-free model-oriented approach, based upon the seminal concept of
algorithmic probability, that decomposes an observation into its most likely
algorithmic generative sources. Our approach uses a causal calculus to infer
model representations. We demonstrate its ability to deconvolve interacting
mechanisms regardless of whether the resultant objects are strings, space-time
evolution diagrams, images or networks. While this is mostly a conceptual
contribution and a novel framework, we provide numerical evidence evaluating
the ability of our methods to separate data from observations produced by
discrete dynamical systems such as cellular automata and complex networks. We
think that these separating techniques can contribute to tackling the challenge
of causation, thus complementing other statistically oriented approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenil_H/0/1/0/all/0/1&quot;&gt;Hector Zenil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiani_N/0/1/0/all/0/1&quot;&gt;Narsis A. Kiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zea_A/0/1/0/all/0/1&quot;&gt;Allan A. Zea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1&quot;&gt;Jesper Tegn&amp;#xe9;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02193">
<title>Logical Rule Induction and Theory Learning Using Neural Theorem Proving. (arXiv:1809.02193v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1809.02193</link>
<description rdf:parseType="Literal">&lt;p&gt;A hallmark of human cognition is the ability to continually acquire and
distill observations of the world into meaningful, predictive theories. In this
paper we present a new mechanism for logical theory acquisition which takes a
set of observed facts and learns to extract from them a set of logical rules
and a small set of core facts which together entail the observations. Our
approach is neuro-symbolic in the sense that the rule pred- icates and core
facts are given dense vector representations. The rules are applied to the core
facts using a soft unification procedure to infer additional facts. After k
steps of forward inference, the consequences are compared to the initial
observations and the rules and core facts are then encouraged towards
representations that more faithfully generate the observations through
inference. Our approach is based on a novel neural forward-chaining
differentiable rule induction network. The rules are interpretable and learned
compositionally from their predicates, which may be invented. We demonstrate
the efficacy of our approach on a variety of ILP rule induction and domain
theory learning datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campero_A/0/1/0/all/0/1&quot;&gt;Andres Campero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pareja_A/0/1/0/all/0/1&quot;&gt;Aldo Pareja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klinger_T/0/1/0/all/0/1&quot;&gt;Tim Klinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Josh Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1&quot;&gt;Sebastian Riedel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.07876">
<title>Fast, Robust, and Versatile Event Detection through HMM Belief State Gradient Measures. (arXiv:1709.07876v3 [cs.RO] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1709.07876</link>
<description rdf:parseType="Literal">&lt;p&gt;Event detection is a critical feature in data-driven systems as it assists
with the identification of nominal and anomalous behavior. Event detection is
increasingly relevant in robotics as robots operate with greater autonomy in
increasingly unstructured environments. In this work, we present an accurate,
robust, fast, and versatile measure for skill and anomaly identification. A
theoretical proof establishes the link between the derivative of the
log-likelihood of the HMM filtered belief state and the latest emission
probabilities. The key insight is the inverse relationship in which gradient
analysis is used for skill and anomaly identification. Our measure showed
better performance across all metrics than related state-of-the art works. The
result is broadly applicable to domains that use HMMs for event detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Shuangqi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hongmin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hongbin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1&quot;&gt;Shuangda Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1&quot;&gt;Yisheng Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojas_J/0/1/0/all/0/1&quot;&gt;Juan Rojas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05085">
<title>Your 2 is My 1, Your 3 is My 9: Handling Arbitrary Miscalibrations in Ratings. (arXiv:1806.05085v2 [stat.ML] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1806.05085</link>
<description rdf:parseType="Literal">&lt;p&gt;Cardinal scores (numeric ratings) collected from people are well known to
suffer from miscalibrations. A popular approach to address this issue is to
assume simplistic models of miscalibration (such as linear biases) to de-bias
the scores. This approach, however, often fares poorly because people&apos;s
miscalibrations are typically far more complex and not well understood. In the
absence of simplifying assumptions on the miscalibration, it is widely believed
by the crowdsourcing community that the only useful information in the cardinal
scores is the induced ranking. In this paper, inspired by the framework of
Stein&apos;s shrinkage, empirical Bayes, and the classic two-envelope problem, we
contest this widespread belief. Specifically, we consider cardinal scores with
arbitrary (or even adversarially chosen) miscalibrations which are only
required to be consistent with the induced ranking. We design estimators which
despite making no assumptions on the miscalibration, strictly and uniformly
outperform all possible estimators that rely on only the ranking. Our
estimators are flexible in that they can be used as a plug-in for a variety of
applications, and we provide a proof-of-concept for A/B testing and ranking.
Our results thus provide novel insights in the eternal debate between cardinal
and ordinal data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nihar B. Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04598">
<title>Bayesian sparse reconstruction: a brute-force approach to astronomical imaging and machine learning. (arXiv:1809.04598v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/1809.04598</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a principled Bayesian framework for signal reconstruction, in
which the signal is modelled by basis functions whose number (and form, if
required) is determined by the data themselves. This approach is based on a
Bayesian interpretation of conventional sparse reconstruction and
regularisation techniques, in which sparsity is imposed through priors via
Bayesian model selection. We demonstrate our method for noisy 1- and
2-dimensional signals, including astronomical images. Furthermore, by using a
product-space approach, the number and type of basis functions can be treated
as integer parameters and their posterior distributions sampled directly. We
show that order-of-magnitude increases in computational efficiency are possible
from this technique compared to calculating the Bayesian evidences separately,
and that further computational gains are possible using it in combination with
dynamic nested sampling. Our approach can be readily applied to neural
networks, where it allows the network architecture to be determined by the data
in a principled Bayesian manner by treating the number of nodes and hidden
layers as parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Higson_E/0/1/0/all/0/1&quot;&gt;Edward Higson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Handley_W/0/1/0/all/0/1&quot;&gt;Will Handley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Hobson_M/0/1/0/all/0/1&quot;&gt;Michael Hobson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Lasenby_A/0/1/0/all/0/1&quot;&gt;Anthony Lasenby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04663">
<title>Creating Fair Models of Atherosclerotic Cardiovascular Disease Risk. (arXiv:1809.04663v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04663</link>
<description rdf:parseType="Literal">&lt;p&gt;Guidelines for the management of atherosclerotic cardiovascular disease
(ASCVD) recommend the use of risk stratification models to identify patients
most likely to benefit from cholesterol-lowering and other therapies. These
models have differential performance across race and gender groups with
inconsistent behavior across studies, potentially resulting in an inequitable
distribution of beneficial therapy. In this work, we leverage adversarial
learning and a large observational cohort extracted from electronic health
records (EHRs) to develop a &quot;fair&quot; ASCVD risk prediction model with reduced
variability in error rates across groups. We empirically demonstrate that our
approach is capable of aligning the distribution of risk predictions
conditioned on the outcome across several groups simultaneously for models
built from high-dimensional EHR data. We also discuss the relevance of these
results in the context of the empirical trade-off between fairness and model
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfohl_S/0/1/0/all/0/1&quot;&gt;Stephen Pfohl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marafino_B/0/1/0/all/0/1&quot;&gt;Ben Marafino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coulet_A/0/1/0/all/0/1&quot;&gt;Adrien Coulet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_F/0/1/0/all/0/1&quot;&gt;Fatima Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palaniappan_L/0/1/0/all/0/1&quot;&gt;Latha Palaniappan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nigam H. Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04668">
<title>PARyOpt: A software for Parallel Asynchronous Remote Bayesian Optimization. (arXiv:1809.04668v1 [math.OC])</title>
<link>http://arxiv.org/abs/1809.04668</link>
<description rdf:parseType="Literal">&lt;p&gt;PARyOpt is a python based implementation of the Bayesian optimization routine
designed for remote and asynchronous function evaluations. Bayesian
optimization is especially attractive for computational optimization due to its
low cost function footprint as well as the ability to account for uncertainties
in data. A key challenge to efficiently deploy any optimization strategy on
distributed computing systems is the synchronization step, where data from
multiple function calls is assimilated to identify the next campaign of
function calls. Bayesian optimization provides an elegant approach to overcome
this issue via asynchronous updates. We formulate, develop and implement a
parallel, asynchronous variant of Bayesian optimization. The framework is
robust and resilient to external failures. We show how such asynchronous
evaluations help reduce the total optimization wall clock time for a suite of
test problems. Additionally, we show how the software design of the framework
allows easy extension to response surface reconstruction (Kriging), providing a
high performance software for autonomous exploration. The software is available
on PyPI, with examples and documentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pokuri_B/0/1/0/all/0/1&quot;&gt;Balaji Sesha Sarath Pokuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lofquist_A/0/1/0/all/0/1&quot;&gt;Alec Lofquist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Risko_C/0/1/0/all/0/1&quot;&gt;Chad M Risko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ganapathysubramanian_B/0/1/0/all/0/1&quot;&gt;Baskar Ganapathysubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04729">
<title>Does Your Model Know the Digit 6 Is Not a Cat? A Less Biased Evaluation of &quot;Outlier&quot; Detectors. (arXiv:1809.04729v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04729</link>
<description rdf:parseType="Literal">&lt;p&gt;In the real world, a learning system could receive an input that looks
nothing like anything it has seen during training, and this can lead to
unpredictable behaviour. We thus need to know whether any given input belongs
to the population distribution of the training data to prevent unpredictable
behaviour in deployed systems. A recent surge of interest on this problem has
led to the development of sophisticated techniques in the deep learning
literature. However, due to the absence of a standardized problem formulation
or an exhaustive evaluation, it is not evident if we can rely on these methods
in practice. What makes this problem different from a typical supervised
learning setting is that we cannot model the diversity of out-of-distribution
samples in practice. The distribution of outliers used in training may not be
the same as the distribution of outliers encountered in the application.
Therefore, classical approaches that learn inliers vs. outliers with only two
datasets can yield optimistic results. We introduce OD-test, a three-dataset
evaluation scheme as a practical and more reliable strategy to assess progress
on this problem. The OD-test benchmark provides a straightforward means of
comparison for methods that address the out-of-distribution sample detection
problem. We present an exhaustive evaluation of a broad set of methods from
related areas on image classification tasks. Furthermore, we show that for
realistic applications of high-dimensional images, the existing methods have
low accuracy. Our analysis reveals areas of strength and weakness of each
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafaei_A/0/1/0/all/0/1&quot;&gt;Alireza Shafaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1&quot;&gt;Mark Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Little_J/0/1/0/all/0/1&quot;&gt;James J. Little&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04747">
<title>Geodesic Clustering in Deep Generative Models. (arXiv:1809.04747v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04747</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models are tremendously successful in learning
low-dimensional latent representations that well-describe the data. These
representations, however, tend to much distort relationships between points,
i.e. pairwise distances tend to not reflect semantic similarities well. This
renders unsupervised tasks, such as clustering, difficult when working with the
latent representations. We demonstrate that taking the geometry of the
generative model into account is sufficient to make simple clustering
algorithms work well over latent representations. Leaning on the recent finding
that deep generative models constitute stochastically immersed Riemannian
manifolds, we propose an efficient algorithm for computing geodesics (shortest
paths) and computing distances in the latent space, while taking its distortion
into account. We further propose a new architecture for modeling uncertainty in
variational autoencoders, which is essential for understanding the geometry of
deep generative models. Experiments show that the geodesic distance is very
likely to reflect the internal structure of the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arvanitidis_G/0/1/0/all/0/1&quot;&gt;Georgios Arvanitidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Dongmei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaogang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04758">
<title>Anomaly Detection with Generative Adversarial Networks for Multivariate Time Series. (arXiv:1809.04758v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04758</link>
<description rdf:parseType="Literal">&lt;p&gt;Today&apos;s Cyber-Physical Systems (CPSs) are large, complex, and affixed with
networked sensors and actuators that are targets for cyber-attacks.
Conventional detection techniques are unable to deal with the increasingly
dynamic and complex nature of the CPSs. On the other hand, the networked
sensors and actuators generate large amounts of data streams that can be
continuously monitored for intrusion events. Unsupervised machine learning
techniques can be used to model the system behaviour and classify deviant
behaviours as possible attacks. In this work, we proposed a novel Generative
Adversarial Networks-based Anomaly Detection (GAN-AD) method for such complex
networked CPSs. We used LSTM-RNN in our GAN to capture the distribution of the
multivariate time series of the sensors and actuators under normal working
conditions of a CPS. Instead of treating each sensor&apos;s and actuator&apos;s time
series independently, we model the time series of multiple sensors and
actuators in the CPS concurrently to take into account of potential latent
interactions between them. To exploit both the generator and the discriminator
of our GAN, we deployed the GAN-trained discriminator together with the
residuals between generator-reconstructed data and the actual samples to detect
possible anomalies in the complex CPS. We used our GAN-AD to distinguish
abnormal attacked situations from normal working conditions for a complex
six-stage Secure Water Treatment (SWaT) system. Experimental results showed
that the proposed strategy is effective in identifying anomalies caused by
various attacks with high detection rate and low false positive rate as
compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dacheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goh_J/0/1/0/all/0/1&quot;&gt;Jonathan Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1&quot;&gt;See-kiong Ng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04790">
<title>Adversarial Examples: Opportunities and Challenges. (arXiv:1809.04790v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04790</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advent of the era of artificial intelligence(AI), deep neural
networks (DNNs) have shown huge superiority over human in image recognition,
speech processing, autonomous vehicles and medical diagnosis. However, recent
studies indicate that DNNs are vulnerable to adversarial examples (AEs) which
are designed by attackers to fool deep learning models. Different from real
examples, AEs can hardly be distinguished from human eyes, but mislead the
model to predict incorrect outputs and therefore threaten security critical
deep-learning applications. In recent years, the generation and defense of AEs
have become a research hotspot in the field of AI security. This article
reviews the latest research progress of AEs. First, we introduce the concept,
cause, characteristic and evaluation metrics of AEs, then give a survey on the
state-of-the-art AE generation methods with the discussion of advantages and
disadvantages. After that we review the existing defenses and discuss their
limitations. Finally, the future research opportunities and challenges of AEs
are prospected.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiaoxiong Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04855">
<title>Stochastic Variational Optimization. (arXiv:1809.04855v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.04855</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational Optimization forms a differentiable upper bound on an objective.
We show that approaches such as Natural Evolution Strategies and Gaussian
Perturbation, are special cases of Variational Optimization in which the
expectations are approximated by Gaussian sampling. These approaches are of
particular interest because they are parallelizable. We calculate the
approximate bias and variance of the corresponding gradient estimators and
demonstrate that using antithetic sampling or a baseline is crucial to mitigate
their problems. We contrast these methods with an alternative parallelizable
method, namely Directional Derivatives. We conclude that, for differentiable
objectives, using Directional Derivatives is preferable to using Variational
Optimization to perform parallel Stochastic Gradient Descent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bird_T/0/1/0/all/0/1&quot;&gt;Thomas Bird&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kunze_J/0/1/0/all/0/1&quot;&gt;Julius Kunze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barber_D/0/1/0/all/0/1&quot;&gt;David Barber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04933">
<title>Identifying Real Estate Opportunities using Machine Learning. (arXiv:1809.04933v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1809.04933</link>
<description rdf:parseType="Literal">&lt;p&gt;The real estate market is exposed to many fluctuations in prices, because of
existing correlations with many variables, some of which cannot be controlled
or might even be unknown. Housing prices can increase rapidly (or in some
cases, also drop very fast), yet the numerous listings available online where
houses are sold or rented are not likely to be updated that often. In some
cases, individuals interested in selling a house (or apartment) might include
it in some online listing, and forget about updating the price. In other cases,
some individuals might be interested in deliberately setting a price below the
market price in order to sell the home faster, for various reasons.
&lt;/p&gt;
&lt;p&gt;In this paper we aim at developing a machine learning application that
identifies opportunities in the real estate market in real time, i.e., houses
that are listed with a price substantially below the market price. This program
can be useful for investors interested in the housing market.
&lt;/p&gt;
&lt;p&gt;The application is formally implemented as a regression problem, that tries
to estimate the market price of a house given features retrieved from public
online listings. For building this application, we have performed a feature
engineering stage in order to discover relevant features that allows attaining
a high predictive performance. Several machine learning algorithms have been
tested, including regression trees, k-NN and neural networks, identifying
advantages and handicaps of each of them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baldominos_A/0/1/0/all/0/1&quot;&gt;Alejandro Baldominos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moreno_A/0/1/0/all/0/1&quot;&gt;Antonio Jos&amp;#xe9; Moreno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Iturrarte_R/0/1/0/all/0/1&quot;&gt;Rub&amp;#xe9;n Iturrarte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bernardez_O/0/1/0/all/0/1&quot;&gt;&amp;#xd3;scar Bern&amp;#xe1;rdez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Afonso_C/0/1/0/all/0/1&quot;&gt;Carlos Afonso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04951">
<title>Valid Simultaneous Inference in High-Dimensional Settings (with the hdm package for R). (arXiv:1809.04951v1 [econ.EM])</title>
<link>http://arxiv.org/abs/1809.04951</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the increasing availability of high-dimensional empirical applications
in many research disciplines, valid simultaneous inference becomes more and
more important. For instance, high-dimensional settings might arise in economic
studies due to very rich data sets with many potential covariates or in the
analysis of treatment heterogeneities. Also the evaluation of potentially more
complicated (non-linear) functional forms of the regression relationship leads
to many potential variables for which simultaneous inferential statements might
be of interest. Here we provide a review of classical and modern methods for
simultaneous inference in (high-dimensional) settings and illustrate their use
by a case study using the R package hdm. The R package hdm implements valid
joint powerful and efficient hypothesis tests for a potentially large number of
coeffcients as well as the construction of simultaneous confidence intervals
and, therefore, provides useful methods to perform valid post-selection
inference based on the LASSO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Bach_P/0/1/0/all/0/1&quot;&gt;Philipp Bach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Chernozhukov_V/0/1/0/all/0/1&quot;&gt;Victor Chernozhukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Spindler_M/0/1/0/all/0/1&quot;&gt;Martin Spindler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04967">
<title>Gaussian process classification using posterior linearisation. (arXiv:1809.04967v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04967</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a new algorithm for Gaussian process classification based
on posterior linearisation (PL). In PL, a Gaussian approximation to the
posterior density is obtained iteratively using the best possible linearisation
of the conditional mean of the labels and accounting for the linearisation
error. Considering three widely-used likelihood functions, in general, PL
provides lower classification errors in real data sets than expectation
propagation and Laplace algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Fernandez_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;ngel F. Garc&amp;#xed;a-Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tronarp_F/0/1/0/all/0/1&quot;&gt;Filip Tronarp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkka_S/0/1/0/all/0/1&quot;&gt;Simo S&amp;#xe4;rkk&amp;#xe4;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04993">
<title>Learning Hybrid Models to Control a Ball in a Circular Maze. (arXiv:1809.04993v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1809.04993</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a problem of model learning to navigate a ball to a goal
state in a circular maze environment with two degrees of freedom. Motion of the
ball in the maze environment is influenced by several non-linear effects such
as friction and contacts, which are difficult to model. We propose a hybrid
model to estimate the dynamics of the ball in the maze based on Gaussian
Process Regression equipped with basis functions obtained from physic first
principles. The accuracy of the hybrid model is compared with standard
algorithms for model learning to highlight its efficacy. The learned model is
then used to design trajectories for the ball using a trajectory optimization
algorithm. We also hope that the system presented in the paper can be used as a
benchmark problem for reinforcement and robot learning for its interesting and
challenging dynamics and its ease of reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romeres_D/0/1/0/all/0/1&quot;&gt;Diego Romeres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1&quot;&gt;Devesh Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Libera_A/0/1/0/all/0/1&quot;&gt;Alberto Dalla Libera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yerazunis_W/0/1/0/all/0/1&quot;&gt;William Yerazunis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikovski_D/0/1/0/all/0/1&quot;&gt;Daniel Nikovski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04997">
<title>Clipped Matrix Completion: a Remedy for Ceiling Effects. (arXiv:1809.04997v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.04997</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the recovery of a low-rank matrix from its clipped observations.
Clipping is a common prohibiting factor in many scientific areas that obstructs
statistical analyses. On the other hand, matrix completion (MC) methods can
recover a low-rank matrix from various information deficits by using the
principle of low-rank completion. However, the current theoretical guarantees
for low-rank MC do not apply to clipped matrices, as the deficit depends on the
underlying values. Therefore, the feasibility of clipped matrix completion
(CMC) is not trivial. In this paper, we first provide a theoretical guarantee
for an exact recovery of CMC by using a trace norm minimization algorithm.
Furthermore, we introduce practical CMC algorithms by extending MC methods. The
simple idea is to use the squared hinge loss in place of the squared loss well
used in MC methods for reducing the penalty of over-estimation on clipped
entries. We also propose a novel regularization term tailored for CMC. It is a
combination of two trace norm terms, and we theoretically bound the recovery
error under the regularization. We demonstrate the effectiveness of the
proposed methods through experiments using both synthetic data and real-world
benchmark data for recommendation systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teshima_T/0/1/0/all/0/1&quot;&gt;Takeshi Teshima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Miao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1&quot;&gt;Issei Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.05032">
<title>IPAD: Stable Interpretable Forecasting with Knockoffs Inference. (arXiv:1809.05032v1 [math.ST])</title>
<link>http://arxiv.org/abs/1809.05032</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretability and stability are two important features that are desired in
many contemporary big data applications arising in economics and finance. While
the former is enjoyed to some extent by many existing forecasting approaches,
the latter in the sense of controlling the fraction of wrongly discovered
features which can enhance greatly the interpretability is still largely
underdeveloped in the econometric settings. To this end, in this paper we
exploit the general framework of model-X knockoffs introduced recently in
Cand\`{e}s, Fan, Janson and Lv (2018), which is nonconventional for
reproducible large-scale inference in that the framework is completely free of
the use of p-values for significance testing, and suggest a new method of
intertwined probabilistic factors decoupling (IPAD) for stable interpretable
forecasting with knockoffs inference in high-dimensional models. The recipe of
the method is constructing the knockoff variables by assuming a latent factor
model that is exploited widely in economics and finance for the association
structure of covariates. Our method and work are distinct from the existing
literature in that we estimate the covariate distribution from data instead of
assuming that it is known when constructing the knockoff variables, our
procedure does not require any sample splitting, we provide theoretical
justifications on the asymptotic false discovery rate control, and the theory
for the power analysis is also established. Several simulation examples and the
real data analysis further demonstrate that the newly suggested method has
appealing finite-sample performance with desired interpretability and stability
compared to some popularly used forecasting methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yingying Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jinchi Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sharifvaghefi_M/0/1/0/all/0/1&quot;&gt;Mahrad Sharifvaghefi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Uematsu_Y/0/1/0/all/0/1&quot;&gt;Yoshimasa Uematsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.05042">
<title>Hamiltonian Descent Methods. (arXiv:1809.05042v1 [math.OC])</title>
<link>http://arxiv.org/abs/1809.05042</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a family of optimization methods that achieve linear convergence
using first-order gradient information and constant step sizes on a class of
convex functions much larger than the smooth and strongly convex ones. This
larger class includes functions whose second derivatives may be singular or
unbounded at their minima. Our methods are discretizations of conformal
Hamiltonian dynamics, which generalize the classical momentum method to model
the motion of a particle with non-standard kinetic energy exposed to a
dissipative force and the gradient field of the function of interest. They are
first-order in the sense that they require only gradient computation. Yet,
crucially the kinetic gradient map can be designed to incorporate information
about the convex conjugate in a fashion that allows for linear convergence on
convex functions that may be non-smooth or non-strongly convex. We study in
detail one implicit and two explicit methods. For one explicit method, we
provide conditions under which it converges to stationary points of non-convex
functions. For all, we provide conditions on the convex function and kinetic
energy pair that guarantee linear convergence, and show that these conditions
can be satisfied by functions with power growth. In sum, these methods expand
the class of convex functions on which linear convergence is possible with
first-order computation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Maddison_C/0/1/0/all/0/1&quot;&gt;Chris J. Maddison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Paulin_D/0/1/0/all/0/1&quot;&gt;Daniel Paulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Teh_Y/0/1/0/all/0/1&quot;&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+ODonoghue_B/0/1/0/all/0/1&quot;&gt;Brendan O&amp;#x27;Donoghue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Doucet_A/0/1/0/all/0/1&quot;&gt;Arnaud Doucet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.05043">
<title>PhD Dissertation: Generalized Independent Components Analysis Over Finite Alphabets. (arXiv:1809.05043v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.05043</link>
<description rdf:parseType="Literal">&lt;p&gt;Independent component analysis (ICA) is a statistical method for transforming
an observable multi-dimensional random vector into components that are as
statistically independent as possible from each other. Usually the ICA
framework assumes a model according to which the observations are generated
(such as a linear transformation with additive noise). ICA over finite fields
is a special case of ICA in which both the observations and the independent
components are over a finite alphabet. In this thesis we consider a formulation
of the finite-field case in which an observation vector is decomposed to its
independent components (as much as possible) with no prior assumption on the
way it was generated. This generalization is also known as Barlow&apos;s minimal
redundancy representation and is considered an open problem. We propose several
theorems and show that this hard problem can be accurately solved with a branch
and bound search tree algorithm, or tightly approximated with a series of
linear problems. Moreover, we show that there exists a simple transformation
(namely, order permutation) which provides a greedy yet very effective
approximation of the optimal solution. We further show that while not every
random vector can be efficiently decomposed into independent components, the
vast majority of vectors do decompose very well (that is, within a small
constant cost), as the dimension increases. In addition, we show that we may
practically achieve this favorable constant cost with a complexity that is
asymptotically linear in the alphabet size. Our contribution provides the first
efficient set of solutions to Barlow&apos;s problem with theoretical and
computational guarantees. Finally, we demonstrate our suggested framework in
multiple source coding applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Painsky_A/0/1/0/all/0/1&quot;&gt;Amichai Painsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.05074">
<title>Derivative-free online learning of inverse dynamics models. (arXiv:1809.05074v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.05074</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper discusses online algorithms for inverse dynamics modelling in
robotics. Several model classes including rigid body dynamics (RBD) models,
data-driven models and semiparametric models (which are a combination of the
previous two classes) are placed in a common framework. While model classes
used in the literature typically exploit joint velocities and accelerations,
which need to be approximated resorting to numerical differentiation schemes,
in this paper a new `derivative-free&apos; framework is proposed that does not
require this preprocessing step. An extensive experimental study with real data
from the right arm of the iCub robot is presented, comparing different model
classes and estimation procedures, showing that the proposed `derivative-free&apos;
methods outperform existing methodologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romeres_D/0/1/0/all/0/1&quot;&gt;Diego Romeres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zorzi_M/0/1/0/all/0/1&quot;&gt;Mattia Zorzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camoriano_R/0/1/0/all/0/1&quot;&gt;Raffaello Camoriano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traversaro_S/0/1/0/all/0/1&quot;&gt;Silvio Traversaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiuso_A/0/1/0/all/0/1&quot;&gt;Alessandro Chiuso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.05077">
<title>MSc Dissertation: Exclusive Row Biclustering for Gene Expression Using a Combinatorial Auction Approach. (arXiv:1809.05077v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.05077</link>
<description rdf:parseType="Literal">&lt;p&gt;The availability of large microarray data has led to a growing interest in
biclustering methods in the past decade. Several algorithms have been proposed
to identify subsets of genes and conditions according to different similarity
measures and under varying constraints. In this paper we focus on the exclusive
row biclustering problem for gene expression data sets, in which each row can
only be a member of a single bicluster while columns can participate in
multiple ones. This type of biclustering may be adequate, for example, for
clustering groups of cancer patients where each patient (row) is expected to be
carrying only a single type of cancer, while each cancer type is associated
with multiple (and possibly overlapping) genes (columns). We present a novel
method to identify these exclusive row biclusters through a combination of
existing biclustering algorithms and combinatorial auction techniques. We
devise an approach for tuning the threshold for our algorithm based on
comparison to a null model in the spirit of the Gap statistic approach. We
demonstrate our approach on both synthetic and real-world gene expression data
and show its power in identifying large span non-overlapping rows sub matrices,
while considering their unique nature. The Gap statistic approach succeeds in
identifying appropriate thresholds in all our examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Painsky_A/0/1/0/all/0/1&quot;&gt;Amichai Painsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1505.00398">
<title>Block Basis Factorization for Scalable Kernel Matrix Evaluation. (arXiv:1505.00398v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1505.00398</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel methods are widespread in machine learning; however, they are limited
by the quadratic complexity of the construction, application, and storage of
kernel matrices. Low-rank matrix approximation algorithms are widely used to
address this problem and reduce the arithmetic and storage cost. However, we
observed that for some datasets with wide intra-class variability, the optimal
kernel parameter for smaller classes yields a matrix that is less well
approximated by low-rank methods. In this paper, we propose an efficient
structured low-rank approximation method---the Block Basis Factorization
(BBF)---and its fast construction algorithm to approximate radial basis
function (RBF) kernel matrices. Our approach has linear memory cost and
floating point operations. BBF works for a wide range of kernel bandwidth
parameters and extends the domain of applicability of low-rank approximation
methods significantly. Our empirical results demonstrate the stability and
superiority over the state-of-art kernel approximation algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruoxi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingzhou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mahoney_M/0/1/0/all/0/1&quot;&gt;Michael W. Mahoney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Darve_E/0/1/0/all/0/1&quot;&gt;Eric Darve&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.01845">
<title>Urban MV and LV Distribution Grid Topology Estimation via Group Lasso. (arXiv:1611.01845v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.01845</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing penetration of distributed energy resources poses numerous
reliability issues to the urban distribution grid. The topology estimation is a
critical step to ensure the robustness of distribution grid operation. However,
the bus connectivity and grid topology estimation are usually hard in
distribution grids. For example, it is technically challenging and costly to
monitor the bus connectivity in urban grids, e.g., underground lines. It is
also inappropriate to use the radial topology assumption exclusively because
the grids of metropolitan cities and regions with dense loads could be with
many mesh structures. To resolve these drawbacks, we propose a data-driven
topology estimation method for MV and LV distribution grids by only utilizing
the historical smart meter measurements. Particularly, a probabilistic
graphical model is utilized to capture the statistical dependencies amongst bus
voltages. We prove that the bus connectivity and grid topology estimation
problems, in radial and mesh structures, can be formulated as a linear
regression with a least absolute shrinkage regularization on grouped variables
(\textit{group lasso}). Simulations show highly accurate results in eight MV
and LV distribution networks at different sizes and 22 topology configurations
using PG\&amp;amp;E residential smart meter data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Yizheng Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weng_Y/0/1/0/all/0/1&quot;&gt;Yang Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guangyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rajagopal_R/0/1/0/all/0/1&quot;&gt;Ram Rajagopal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00658">
<title>Candidates vs. Noises Estimation for Large Multi-Class Classification Problem. (arXiv:1711.00658v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00658</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a method for multi-class classification problems, where
the number of classes K is large. The method, referred to as Candidates vs.
Noises Estimation (CANE), selects a small subset of candidate classes and
samples the remaining classes. We show that CANE is always consistent and
computationally efficient. Moreover, the resulting estimator has low
statistical variance approaching that of the maximum likelihood estimator, when
the observed label belongs to the selected candidates with high probability. In
practice, we use a tree structure with leaves as classes to promote fast beam
search for candidate selection. We further apply the CANE method to estimate
word probabilities in learning large neural language models. Extensive
experimental results show that CANE achieves better prediction accuracy over
the Noise-Contrastive Estimation (NCE), its variants and a number of the
state-of-the-art tree classifiers, while it gains significant speedup compared
to standard O(K) methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08571">
<title>On Coresets for Logistic Regression. (arXiv:1805.08571v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08571</link>
<description rdf:parseType="Literal">&lt;p&gt;Coresets are one of the central methods to facilitate the analysis of large
data sets. We continue a recent line of research applying the theory of
coresets to logistic regression. First, we show a negative result, namely, that
no strongly sublinear sized coresets exist for logistic regression. To deal
with intractable worst-case instances we introduce a complexity measure
$\mu(X)$, which quantifies the hardness of compressing a data set for logistic
regression. $\mu(X)$ has an intuitive statistical interpretation that may be of
independent interest. For data sets with bounded $\mu(X)$-complexity, we show
that a novel sensitivity sampling scheme produces the first provably sublinear
$(1\pm\varepsilon)$-coreset. We illustrate the performance of our method by
comparing to uniform sampling as well as to state of the art methods in the
area. The experiments are conducted on real world benchmark data for logistic
regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munteanu_A/0/1/0/all/0/1&quot;&gt;Alexander Munteanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwiegelshohn_C/0/1/0/all/0/1&quot;&gt;Chris Schwiegelshohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohler_C/0/1/0/all/0/1&quot;&gt;Christian Sohler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1&quot;&gt;David P. Woodruff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00534">
<title>Run Procrustes, Run! On the convergence of accelerated Procrustes Flow. (arXiv:1806.00534v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00534</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present theoretical results on the convergence of non-convex
accelerated gradient descent in matrix factorization models. The technique is
applied to matrix sensing problems with squared loss, for the estimation of a
rank $r$ optimal solution $X^\star \in \mathbb{R}^{n \times n}$. We show that
the acceleration leads to linear convergence rate, even under non-convex
settings where the variable $X$ is represented as $U U^\top$ for $U \in
\mathbb{R}^{n \times r}$. Our result has the same dependence on the condition
number of the objective --and the optimal solution-- as that of the recent
results on non-accelerated algorithms. However, acceleration is observed in
practice, both in synthetic examples and in two real applications: neuronal
multi-unit activities recovery from single electrode recordings, and quantum
state tomography on quantum computing simulators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1&quot;&gt;Anastasios Kyrillidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ubaru_S/0/1/0/all/0/1&quot;&gt;Shashanka Ubaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollias_G/0/1/0/all/0/1&quot;&gt;Georgios Kollias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1&quot;&gt;Kristofer Bouchard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02977">
<title>Monge beats Bayes: Hardness Results for Adversarial Training. (arXiv:1806.02977v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02977</link>
<description rdf:parseType="Literal">&lt;p&gt;The last few years have seen extensive empirical study of the robustness of
neural networks, with a concerning conclusion: several state-of-the-art
approaches are highly sensitive to adversarial perturbations of their inputs.
There has been an accompanying surge of interest in learning including defense
mechanisms against specific adversaries, known as adversarial training. Despite
some impressive advances, little remains known on how to best frame a
resource-bounded adversary so that it can be severely detrimental to learning,
a non-trivial problem which entails at a minimum the choice of loss and
classifiers.
&lt;/p&gt;
&lt;p&gt;We suggest here a formal answer to this question, and pin down a simple
sufficient property for any given class of adversaries to be detrimental to
learning. This property involves a central measure of `harmfulness&apos; which
generalizes the well-known class of integral probability metrics. A key feature
of our result is that it holds for \textit{all} proper losses, and for a
popular subset of these, the optimisation of this central measure appears to be
independent of the loss. We show how weakly contractive adversaries for a RKHS
can be self-combined to build a maximally detrimental adversary, we show that
some implemented existing adversaries involve proxies of our optimal transport
adversaries and finally provide a toy experiment assessing such adversaries in
a simple context, displaying that additional robustness on testing can be
granted through adversarial training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cranko_Z/0/1/0/all/0/1&quot;&gt;Zac Cranko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1&quot;&gt;Aditya Krishna Menon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nock_R/0/1/0/all/0/1&quot;&gt;Richard Nock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_C/0/1/0/all/0/1&quot;&gt;Cheng-Soon Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1&quot;&gt;Christian Walder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03218">
<title>Data-driven model for the identification of the rock type at a drilling bit. (arXiv:1806.03218v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.03218</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to bridge the gap of more than 15m between the drilling bit and
high-fidelity rock type sensors during the directional drilling, we present a
novel approach for identifying rock type at the drilling bit. The approach is
based on application of machine learning techniques for Measurements While
Drilling (MWD) data. We demonstrate capabilities of the developed approach for
distinguishing between the rock types corresponding to (1) a target oil bearing
interval of a reservoir and (2) a non-productive shale layer and compare it to
more traditional physics-driven approaches. The dataset includes MWD data and
lithology mapping along multiple wellbores obtained by processing of Logging
While Drilling (LWD) measurements from a massive drilling effort on one of the
major newly developed oilfield in the North of Western Siberia. We compare
various machine-learning algorithms, examine extra features coming from
physical modeling of drilling mechanics, and show that the classification error
can be reduced from 13.5% to 9%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klyuchnikov_N/0/1/0/all/0/1&quot;&gt;Nikita Klyuchnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1&quot;&gt;Alexey Zaytsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gruzdev_A/0/1/0/all/0/1&quot;&gt;Arseniy Gruzdev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ovchinnikov_G/0/1/0/all/0/1&quot;&gt;Georgiy Ovchinnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antipova_K/0/1/0/all/0/1&quot;&gt;Ksenia Antipova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ismailova_L/0/1/0/all/0/1&quot;&gt;Leyla Ismailova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muravleva_E/0/1/0/all/0/1&quot;&gt;Ekaterina Muravleva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1&quot;&gt;Evgeny Burnaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semenikhin_A/0/1/0/all/0/1&quot;&gt;Artyom Semenikhin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherepanov_A/0/1/0/all/0/1&quot;&gt;Alexey Cherepanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koryabkin_V/0/1/0/all/0/1&quot;&gt;Vitaliy Koryabkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simon_I/0/1/0/all/0/1&quot;&gt;Igor Simon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsurgan_A/0/1/0/all/0/1&quot;&gt;Alexey Tsurgan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krasnov_F/0/1/0/all/0/1&quot;&gt;Fedor Krasnov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koroteev_D/0/1/0/all/0/1&quot;&gt;Dmitry Koroteev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01771">
<title>Direct Uncertainty Prediction for Medical Second Opinions. (arXiv:1807.01771v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01771</link>
<description rdf:parseType="Literal">&lt;p&gt;A persistent challenge in the practice of medicine (and machine learning) is
the disagreement of highly trained human experts on data instances, such as
patient image scans. We study the application of machine learning to predict
which instances are likely to give rise to maximal expert disagreement. As
necessitated by this, we develop predictors on datasets with noisy and scarce
labels. Our central methodological finding is that direct prediction of a
scalar uncertainty score performs better than the two-step process of (i)
training a classifier (ii) using the classifier outputs to derive an
uncertainty score. This is seen in both a synthetic setting whose parameters we
can control, and a paradigmatic healthcare application involving multiple
labels by medical domain experts. We evaluate these direct uncertainty models
on a gold standard adjudicated set, where they accurately predict when an
individual expert will disagree with an unknown ground truth. We explore the
consequences for using these predictors to identify the need for a medical
second opinion and a machine learning data curation application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1&quot;&gt;Maithra Raghu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blumer_K/0/1/0/all/0/1&quot;&gt;Katy Blumer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayres_R/0/1/0/all/0/1&quot;&gt;Rory Sayres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obermeyer_Z/0/1/0/all/0/1&quot;&gt;Ziad Obermeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullainathan_S/0/1/0/all/0/1&quot;&gt;Sendhil Mullainathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1&quot;&gt;Jon Kleinberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.02266">
<title>Multi-Output Convolution Spectral Mixture for Gaussian Processes. (arXiv:1808.02266v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.02266</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-output Gaussian processes (MOGPs) are recently extended by using
spectral mixture kernel, which enables expressively pattern extrapolation with
a strong interpretation. In particular, Multi-Output Spectral Mixture kernel
(MOSM) is a recent, powerful state of the art method. However, MOSM cannot
reduce to the ordinary spectral mixture kernel (SM) when using a single
channel. Moreover, when the spectral density of different channels is either
very close or very far from each other in the frequency domain, MOSM generates
unreasonable scale effects on cross weights which produces an incorrect
description of the channel correlation structure. In this paper, we tackle
these drawbacks and introduce a principled multi-output convolution spectral
mixture kernel (MOCSM) framework. In our framework, we model channel
dependencies through convolution of time and phase delayed spectral mixtures
between different channels. Results of extensive experiments on synthetic and
real datasets demontrate the advantages of MOCSM and its state of the art
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groot_P/0/1/0/all/0/1&quot;&gt;Perry Groot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchiori_E/0/1/0/all/0/1&quot;&gt;Elena Marchiori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04768">
<title>Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical Models. (arXiv:1808.04768v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.04768</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a method which enables a recurrent dynamics model to be
temporally abstract. Our approach, which we call Adaptive Skip Intervals (ASI),
is based on the observation that in many sequential prediction tasks, the exact
time at which events occur is irrelevant to the underlying objective. Moreover,
in many situations, there exist prediction intervals which result in
particularly easy-to-predict transitions. We show that there are prediction
tasks for which we gain both computational efficiency and prediction accuracy
by allowing the model to make predictions at a sampling rate which it can
choose itself.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neitz_A/0/1/0/all/0/1&quot;&gt;Alexander Neitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parascandolo_G/0/1/0/all/0/1&quot;&gt;Giambattista Parascandolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1&quot;&gt;Stefan Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02652">
<title>Are You Sure You Want To Do That? Classification with Verification. (arXiv:1809.02652v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.02652</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification systems typically act in isolation, meaning they are required
to implicitly memorize the characteristics of all candidate classes in order to
classify. The cost of this is increased memory usage and poor sample
efficiency. We propose a model which instead verifies using reference images
during the classification process, reducing the burden of memorization. The
model uses iterative nondifferentiable queries in order to classify an image.
We demonstrate that such a model is feasible to train and can match baseline
accuracy while being more parameter efficient. However, we show that finding
the correct balance between image recognition and verification is essential to
pushing the model towards desired behavior, suggesting that a pipeline of
recognition followed by verification is a more promising approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1&quot;&gt;Harris Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhury_A/0/1/0/all/0/1&quot;&gt;Atef Chaudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1&quot;&gt;Kevin Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02840">
<title>Neural Guided Constraint Logic Programming for Program Synthesis. (arXiv:1809.02840v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.02840</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthesizing programs using example input/outputs is a classic problem in
artificial intelligence. We present a method for solving Programming By Example
(PBE) problems by using a neural model to guide the search of a constraint
logic programming system called miniKanren. Crucially, the neural model uses
miniKanren&apos;s internal representation as input; miniKanren represents a PBE
problem as recursive constraints imposed by the provided examples. We explore
Recurrent Neural Network and Graph Neural Network models. We contribute a
modified miniKanren, drivable by an external agent, available at
https://github.com/xuexue/neuralkanren. We show that our neural-guided approach
using constraints can synthesize programs faster in many cases, and
importantly, can generalize to larger problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lisa Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenblatt_G/0/1/0/all/0/1&quot;&gt;Gregory Rosenblatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1&quot;&gt;Ethan Fetaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1&quot;&gt;Renjie Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byrd_W/0/1/0/all/0/1&quot;&gt;William E. Byrd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Might_M/0/1/0/all/0/1&quot;&gt;Matthew Might&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03839">
<title>Unsupervised Domain Adaptation Based on Source-guided Discrepancy. (arXiv:1809.03839v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.03839</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation is the problem setting where data generating
distributions in the source and target domains are different, and labels in the
target domain are unavailable. One important question in unsupervised domain
adaptation is how to measure the difference between the source and target
domains. A previously proposed discrepancy that does not use the source domain
labels requires high computational cost to estimate and may lead to a loose
generalization error bound in the target domain. To mitigate these problems, we
propose a novel discrepancy called source-guided discrepancy ($S$-disc), which
exploits labels in the source domain. As a consequence, $S$-disc can be
computed efficiently with a finite sample convergence guarantee. In addition,
we show that $S$-disc can provide a tighter generalization error bound than the
one based on an existing discrepancy. Finally, we report experimental results
that demonstrate the advantages of $S$-disc over the existing discrepancies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuroki_S/0/1/0/all/0/1&quot;&gt;Seiichi Kuroki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charoenphakdee_N/0/1/0/all/0/1&quot;&gt;Nontawat Charoenphakdee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1&quot;&gt;Han Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honda_J/0/1/0/all/0/1&quot;&gt;Junya Honda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1&quot;&gt;Issei Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04069">
<title>Estimate the Warfarin Dose by Ensemble of Machine Learning Algorithms. (arXiv:1809.04069v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/1809.04069</link>
<description rdf:parseType="Literal">&lt;p&gt;Warfarin dosing remains challenging due to narrow therapeutic index and
highly individual variability. Incorrect warfarin dosing is associated with
devastating adverse events. Remarkable efforts have been made to develop the
machine learning based warfarin dosing algorithms incorporating clinical
factors and genetic variants such as polymorphisms in CYP2C9 and VKORC1. The
most widely validated pharmacogenetic algorithm is the IWPC algorithm based on
multivariate linear regression (MLR). However, with only a single algorithm,
the prediction performance may reach an upper limit even with optimal
parameters. Here, we present novel algorithms using stacked generalization
frameworks to estimate the warfarin dose, within which different types of
machine learning algorithms function together through a meta-machine learning
model to maximize the prediction accuracy. Compared to the IWPC-derived MLR
algorithm, Stack 1 and 2 based on stacked generalization frameworks performed
significantly better overall. Subgroup analysis revealed that the mean of the
percentage of patients whose predicted dose of warfarin within 20% of the
actual stable therapeutic dose (mean percentage within 20%) for Stack 1 was
improved by 12.7% (from 42.47% to 47.86%) in Asians and by 13.5% (from 22.08%
to 25.05%) in the low-dose group compared to that for MLR, respectively. These
data suggest that our algorithms would especially benefit patients required low
warfarin maintenance dose, as subtle changes in warfarin dose could lead to
adverse clinical events (thrombosis or bleeding) in patients with low dose. Our
study offers novel pharmacogenetic algorithms for clinical trials and practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Ping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zehui Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruobing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Khalighi_K/0/1/0/all/0/1&quot;&gt;Koroush Khalighi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04188">
<title>Layerwise Perturbation-Based Adversarial Training for Hard Drive Health Degree Prediction. (arXiv:1809.04188v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.04188</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of cloud computing and big data, the reliability of data
storage systems becomes increasingly important. Previous researchers have shown
that machine learning algorithms based on SMART attributes are effective
methods to predict hard drive failures. In this paper, we use SMART attributes
to predict hard drive health degrees which are helpful for taking different
fault tolerant actions in advance. Given the highly imbalanced SMART datasets,
it is a nontrivial work to predict the health degree precisely. The proposed
model would encounter overfitting and biased fitting problems if it is trained
by the traditional methods. In order to resolve this problem, we propose two
strategies to better utilize imbalanced data and improve performance. Firstly,
we design a layerwise perturbation-based adversarial training method which can
add perturbations to any layers of a neural network to improve the
generalization of the network. Secondly, we extend the training method to the
semi-supervised settings. Then, it is possible to utilize unlabeled data that
have a potential of failure to further improve the performance of the model.
Our extensive experiments on two real-world hard drive datasets demonstrate the
superiority of the proposed schemes for both supervised and semi-supervised
classification. The model trained by the proposed method can correctly predict
the hard drive health status 5 and 15 days in advance. Finally, we verify the
generality of the proposed training method in other similar anomaly detection
tasks where the dataset is imbalanced. The results argue that the proposed
methods are applicable to other domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianguo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Ji Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lifang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04294">
<title>Cluster Variational Approximations for Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data. (arXiv:1809.04294v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1809.04294</link>
<description rdf:parseType="Literal">&lt;p&gt;Continuous-time Bayesian networks (CTBNs) constitute a general and powerful
framework for modeling continuous-time stochastic processes on networks. This
makes them particularly attractive for learning the directed structures among
interacting entities. However, if the available data is incomplete, one needs
to simulate the prohibitively complex CTBN dynamics. Existing approximation
techniques, such as sampling and low-order variational methods, either scale
unfavorably in system size, or are unsatisfactory in terms of accuracy.
Inspired by recent advances in statistical physics, we present a new
approximation scheme based on cluster-variational methods significantly
improving upon existing variational approximations. We can analytically
marginalize the parameters of the approximate CTBN, as these are of secondary
importance for structure learning. This recovers a scalable scheme for direct
structure learning from incomplete and noisy time-series data. Our approach
outperforms existing methods in terms of scalability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Linzner_D/0/1/0/all/0/1&quot;&gt;Dominik Linzner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Koeppl_H/0/1/0/all/0/1&quot;&gt;Heinz Koeppl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.04430">
<title>Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy. (arXiv:1809.04430v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1809.04430</link>
<description rdf:parseType="Literal">&lt;p&gt;Over half a million individuals are diagnosed with head and neck cancer each
year worldwide. Radiotherapy is an important curative treatment for this
disease, but it requires manually intensive delineation of radiosensitive
organs at risk (OARs). This planning process can delay treatment commencement.
While auto-segmentation algorithms offer a potentially time-saving solution,
the challenges in defining, quantifying and achieving expert performance
remain. Adopting a deep learning approach, we demonstrate a 3D U-Net
architecture that achieves performance similar to experts in delineating a wide
range of head and neck OARs. The model was trained on a dataset of 663
deidentified computed tomography (CT) scans acquired in routine clinical
practice and segmented according to consensus OAR definitions. We demonstrate
its generalisability through application to an independent test set of 24 CT
scans available from The Cancer Imaging Archive collected at multiple
international sites previously unseen to the model, each segmented by two
independent experts and consisting of 21 OARs commonly segmented in clinical
practice. With appropriate validation studies and regulatory approvals, this
system could improve the effectiveness of radiotherapy pathways.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolov_S/0/1/0/all/0/1&quot;&gt;Stanislav Nikolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blackwell_S/0/1/0/all/0/1&quot;&gt;Sam Blackwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendes_R/0/1/0/all/0/1&quot;&gt;Ruheena Mendes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fauw_J/0/1/0/all/0/1&quot;&gt;Jeffrey De Fauw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1&quot;&gt;Clemens Meyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hughes_C/0/1/0/all/0/1&quot;&gt;C&amp;#xed;an Hughes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Askham_H/0/1/0/all/0/1&quot;&gt;Harry Askham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romera_Paredes_B/0/1/0/all/0/1&quot;&gt;Bernardino Romera-Paredes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1&quot;&gt;Alan Karthikesalingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1&quot;&gt;Carlton Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carnell_D/0/1/0/all/0/1&quot;&gt;Dawn Carnell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boon_C/0/1/0/all/0/1&quot;&gt;Cheng Boon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DSouza_D/0/1/0/all/0/1&quot;&gt;Derek D&amp;#x27;Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moinuddin_S/0/1/0/all/0/1&quot;&gt;Syed Ali Moinuddin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sullivan_K/0/1/0/all/0/1&quot;&gt;Kevin Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Consortium_D/0/1/0/all/0/1&quot;&gt;DeepMind Radiographer Consortium&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montgomery_H/0/1/0/all/0/1&quot;&gt;Hugh Montgomery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rees_G/0/1/0/all/0/1&quot;&gt;Geraint Rees&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;Ricky Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suleyman_M/0/1/0/all/0/1&quot;&gt;Mustafa Suleyman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Back_T/0/1/0/all/0/1&quot;&gt;Trevor Back&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ledsam_J/0/1/0/all/0/1&quot;&gt;Joseph R. Ledsam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronneberger_O/0/1/0/all/0/1&quot;&gt;Olaf Ronneberger&lt;/a&gt;</dc:creator>
</item></rdf:RDF>