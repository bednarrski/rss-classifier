<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-06-24T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08547"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09877"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08463"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08465"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08663"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08686"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.09156"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05931"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07564"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06459"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08975"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.01664"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06920"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08449"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08456"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08593"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08675"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08716"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08734"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08764"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.08782"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.09112"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.01751"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08475"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08841"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.01667"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.02199"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07174"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.08547">
<title>Analysis of Evolutionary Algorithms in Dynamic and Stochastic Environments. (arXiv:1806.08547v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1806.08547</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world optimization problems occur in environments that change
dynamically or involve stochastic components. Evolutionary algorithms and other
bio-inspired algorithms have been widely applied to dynamic and stochastic
problems. This survey gives an overview of major theoretical developments in
the area of runtime analysis for these problems. We review recent theoretical
studies of evolutionary algorithms and ant colony optimization for problems
where the objective functions or the constraints change over time. Furthermore,
we consider stochastic problems under various noise models and point out some
directions for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roostapour_V/0/1/0/all/0/1&quot;&gt;Vahid Roostapour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pourhassan_M/0/1/0/all/0/1&quot;&gt;Mojgan Pourhassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_F/0/1/0/all/0/1&quot;&gt;Frank Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09877">
<title>DRACO: Byzantine-resilient Distributed Training via Redundant Gradients. (arXiv:1803.09877v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09877</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed model training is vulnerable to byzantine system failures and
adversarial compute nodes, i.e., nodes that use malicious updates to corrupt
the global model stored at a parameter server (PS). To guarantee some form of
robustness, recent work suggests using variants of the geometric median as an
aggregation rule, in place of gradient averaging. Unfortunately, median-based
rules can incur a prohibitive computational overhead in large-scale settings,
and their convergence guarantees often require strong assumptions. In this
work, we present DRACO, a scalable framework for robust distributed training
that uses ideas from coding theory. In DRACO, each compute node evaluates
redundant gradients that are used by the parameter server to eliminate the
effects of adversarial updates. DRACO comes with problem-independent robustness
guarantees, and the model that it trains is identical to the one trained in the
adversary-free setup. We provide extensive experiments on real datasets and
distributed setups across a variety of large-scale models, where we show that
DRACO is several times, to orders of magnitude faster than median-based
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lingjiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Charles_Z/0/1/0/all/0/1&quot;&gt;Zachary Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papailiopoulos_D/0/1/0/all/0/1&quot;&gt;Dimitris Papailiopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08463">
<title>TriResNet: A Deep Triple-stream Residual Network for Histopathology Grading. (arXiv:1806.08463v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1806.08463</link>
<description rdf:parseType="Literal">&lt;p&gt;While microscopic analysis of histopathological slides is generally
considered as the gold standard method for performing cancer diagnosis and
grading, the current method for analysis is extremely time consuming and labour
intensive as it requires pathologists to visually inspect tissue samples in a
detailed fashion for the presence of cancer. As such, there has been
significant recent interest in computer aided diagnosis systems for analysing
histopathological slides for cancer grading to aid pathologists to perform
cancer diagnosis and grading in a more efficient, accurate, and consistent
manner. In this work, we investigate and explore a deep triple-stream residual
network (TriResNet) architecture for the purpose of tile-level histopathology
grading, which is the critical first step to computer-aided whole-slide
histopathology grading. In particular, the design mentality behind the proposed
TriResNet network architecture is to facilitate for the learning of a more
diverse set of quantitative features to better characterize the complex tissue
characteristics found in histopathology samples. Experimental results on two
widely-used computer-aided histopathology benchmark datasets (CAMELYON16
dataset and Invasive Ductal Carcinoma (IDC) dataset) demonstrated that the
proposed TriResNet network architecture was able to achieve noticeably improved
accuracies when compared with two other state-of-the-art deep convolutional
neural network architectures. Based on these promising results, the hope is
that the proposed TriResNet network architecture could become a useful tool to
aiding pathologists increase the consistency, speed, and accuracy of the
histopathology grading process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bidart_R/0/1/0/all/0/1&quot;&gt;Rene Bidart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alexander Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08465">
<title>A Novel ECOC Algorithm with Centroid Distance Based Soft Coding Scheme. (arXiv:1806.08465v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.08465</link>
<description rdf:parseType="Literal">&lt;p&gt;In ECOC framework, the ternary coding strategy is widely deployed in coding
process. It relabels classes with {&quot;-1,0,1&quot; }, where -1/1 means to assign the
corresponding classes to the negative/positive group, and label 0 leads to
ignore the corresponding classes in the training process. However, the
application of hard labels may lose some information about the tendency of
class distributions. Instead, we propose a Centroid distance-based Soft coding
scheme to indicate such tendency, named as CSECOC. In our algorithm, Sequential
Forward Floating Selection (SFFS) is applied to search an optimal class
assignment by minimizing the ratio of intra-group and inter-group distance. In
this way, a hard coding matrix is generated initially. Then we propose a
measure, named as coverage, to describe the probability of a sample in a class
falling to a correct group. The coverage of a class a group replace the
corresponding hard element, so as to form a soft coding matrix. Compared with
the hard ones, such soft elements can reflect the tendency of a class belonging
to positive or negative group. Instead of classifiers, regressors are used as
base learners in this algorithm. To the best of our knowledge, it is the first
time that soft coding scheme has been proposed. The results on five UCI
datasets show that compared with some state-of-art ECOC algorithms, our
algorithm can produce comparable or better classification accuracy with small
scale ensembles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Kaijie Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kunhong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Beizhan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08663">
<title>Simulation Study on a New Peer Review Approach. (arXiv:1806.08663v1 [cs.DL])</title>
<link>http://arxiv.org/abs/1806.08663</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing volume of scientific publications and grant proposals has
generated an unprecedentedly high workload to scientific communities.
Consequently, review quality has been decreasing and review outcomes have
become less correlated with the real merits of the papers and proposals. A
novel distributed peer review (DPR) approach has recently been proposed to
address these issues. The new approach assigns principal investigators (PIs)
who submitted proposals (or papers) to the same program as reviewers. Each PI
reviews and ranks a small number (such as seven) of other PIs&apos; proposals. The
individual rankings are then used to estimate a global ranking of all proposals
using the Modified Borda Count (MBC). In this study, we perform simulation
studies to investigate several parameters important for the decision making
when adopting this new approach. We also propose a new method called
Concordance Index-based Global Ranking (CIGR) to estimate global ranking from
individual rankings. An efficient simulated annealing algorithm is designed to
search the optimal Concordance Index (CI). Moreover, we design a new balanced
review assignment procedure, which can result in significantly better
performance for both MBC and CIGR methods. We found that CIGR performs better
than MBC when the review quality is relatively high. As review quality and
review difficulty are tightly correlated, we constructed a boundary in the
space of review quality vs review difficulty that separates the CIGR-superior
and MBC-superior regions. Finally, we propose a multi-stage DPR strategy based
on CIGR, which has the potential to substantially improve the overall review
performance while reducing the review workload.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steppi_A/0/1/0/all/0/1&quot;&gt;Albert Steppi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1&quot;&gt;Jinchan Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1&quot;&gt;Minjing Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tingting Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_X/0/1/0/all/0/1&quot;&gt;Xiaodong Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08686">
<title>A Predictive Model for Music Based on Learned Interval Representations. (arXiv:1806.08686v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1806.08686</link>
<description rdf:parseType="Literal">&lt;p&gt;Connectionist sequence models (e.g., RNNs) applied to musical sequences
suffer from two known problems: First, they have strictly &quot;absolute pitch
perception&quot;. Therefore, they fail to generalize over musical concepts which are
commonly perceived in terms of relative distances between pitches (e.g.,
melodies, scale types, modes, cadences, or chord types). Second, they fall
short of capturing the concepts of repetition and musical form. In this paper
we introduce the recurrent gated autoencoder (RGAE), a recurrent neural network
which learns and operates on interval representations of musical sequences. The
relative pitch modeling increases generalization and reduces sparsity in the
input data. Furthermore, it can learn sequences of copy-and-shift operations
(i.e. chromatically transposed copies of musical fragments)---a promising
capability for learning musical repetition structure. We show that the RGAE
improves the state of the art for general connectionist sequence models in
learning to predict monophonic melodies, and that ensembles of relative and
absolute music processing models improve the results appreciably. Furthermore,
we show that the relative pitch processing of the RGAE naturally facilitates
the learning and the generation of sequences of copy-and-shift operations,
wherefore the RGAE greatly outperforms a common absolute pitch recurrent neural
network on this task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lattner_S/0/1/0/all/0/1&quot;&gt;Stefan Lattner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grachten_M/0/1/0/all/0/1&quot;&gt;Maarten Grachten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1&quot;&gt;Gerhard Widmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.09156">
<title>Fuzzy Bayesian Learning. (arXiv:1610.09156v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.09156</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose a novel approach for learning from data using rule
based fuzzy inference systems where the model parameters are estimated using
Bayesian inference and Markov Chain Monte Carlo (MCMC) techniques. We show the
applicability of the method for regression and classification tasks using
synthetic data-sets and also a real world example in the financial services
industry. Then we demonstrate how the method can be extended for knowledge
extraction to select the individual rules in a Bayesian way which best explains
the given data. Finally we discuss the advantages and pitfalls of using this
method over state-of-the-art techniques and highlight the specific class of
problems where this would be useful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pan_I/0/1/0/all/0/1&quot;&gt;Indranil Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bester_D/0/1/0/all/0/1&quot;&gt;Dirk Bester&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05931">
<title>Faster Learning by Reduction of Data Access Time. (arXiv:1801.05931v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.05931</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, the major challenge in machine learning is the Big Data challenge.
The big data problems due to large number of data points or large number of
features in each data point, or both, the training of models have become very
slow. The training time has two major components: Time to access the data and
time to process (learn from) the data. So far, the research has focused only on
the second part, i.e., learning from the data. In this paper, we have proposed
one possible solution to handle the big data problems in machine learning. The
idea is to reduce the training time through reducing data access time by
proposing systematic sampling and cyclic/sequential sampling to select
mini-batches from the dataset. To prove the effectiveness of proposed sampling
techniques, we have used Empirical Risk Minimization, which is commonly used
machine learning problem, for strongly convex and smooth case. The problem has
been solved using SAG, SAGA, SVRG, SAAG-II and MBSGD (Mini-batched SGD), each
using two step determination techniques, namely, constant step size and
backtracking line search method. Theoretical results prove the same convergence
for systematic sampling, cyclic sampling and the widely used random sampling
technique, in expectation. Experimental results with bench marked datasets
prove the efficacy of the proposed sampling techniques and show up to six times
faster training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chauhan_V/0/1/0/all/0/1&quot;&gt;Vinod Kumar Chauhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Anuj Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1&quot;&gt;Kalpana Dahiya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07564">
<title>Clipped Action Policy Gradient. (arXiv:1802.07564v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07564</link>
<description rdf:parseType="Literal">&lt;p&gt;Many continuous control tasks have bounded action spaces. When policy
gradient methods are applied to such tasks, out-of-bound actions need to be
clipped before execution, while policies are usually optimized as if the
actions are not clipped. We propose a policy gradient estimator that exploits
the knowledge of actions being clipped to reduce the variance in estimation. We
prove that our estimator, named clipped action policy gradient (CAPG), is
unbiased and achieves lower variance than the conventional estimator that
ignores action bounds. Experimental results demonstrate that CAPG generally
outperforms the conventional estimator, indicating that it is a better policy
gradient estimator for continuous control tasks. The source code is available
at https://github.com/pfnet-research/capg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujita_Y/0/1/0/all/0/1&quot;&gt;Yasuhiro Fujita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maeda_S/0/1/0/all/0/1&quot;&gt;Shin-ichi Maeda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06459">
<title>On Learning Intrinsic Rewards for Policy Gradient Methods. (arXiv:1804.06459v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06459</link>
<description rdf:parseType="Literal">&lt;p&gt;In many sequential decision making tasks, it is challenging to design reward
functions that help an RL agent efficiently learn behavior that is considered
good by the agent designer. A number of different formulations of the
reward-design problem, or close variants thereof, have been proposed in the
literature. In this paper we build on the Optimal Rewards Framework of Singh
et.al. that defines the optimal intrinsic reward function as one that when used
by an RL agent achieves behavior that optimizes the task-specifying or
extrinsic reward function. Previous work in this framework has shown how good
intrinsic reward functions can be learned for lookahead search based planning
agents. Whether it is possible to learn intrinsic reward functions for learning
agents remains an open problem. In this paper we derive a novel algorithm for
learning intrinsic rewards for policy-gradient based learning agents. We
compare the performance of an augmented agent that uses our algorithm to
provide additive intrinsic rewards to an A2C-based policy learner (for Atari
games) and a PPO-based policy learner (for Mujoco domains) with a baseline
agent that uses the same policy learners but with only extrinsic rewards. Our
results show improved performance on most but not all of the domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zeyu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Junhyuk Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Satinder Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08975">
<title>Particle Filter Networks with Application to Visual Localization. (arXiv:1805.08975v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08975</link>
<description rdf:parseType="Literal">&lt;p&gt;Particle filtering is a powerful method for sequential state estimation and
is extensively used in many domains, including robot localization, visual
tracking, etc. To apply particle filters in practice, a main challenge is to
construct an effective probabilistic system model, especially when the system
exhibits complex dynamic behavior or processes rich sensor information from,
e.g., visual cameras. This paper introduces the Particle Filter Network
(PF-Net), which captures both a system model and the particle filter algorithm
in a single neural network. This unified network representation enables
end-to-end model learning, which trains the model in the context of a specific
algorithm, resulting in improved performance, compared with conventional
model-learning methods. We apply PF-net to visual robot localization. The robot
must localize in rich 3-D environments, using only a schematic 2-D floor map.
In preliminary experiments, PF-Net consistently outperformed alternative
learning architectures, as well as conventional model-based localization
methods. PF-net learns effective models that generalize to new, unseen
environments. It can also incorporate semantic labels on the floor map.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karkus_P/0/1/0/all/0/1&quot;&gt;Peter Karkus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;David Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wee Sun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.01664">
<title>Hierarchical Graph Clustering using Node Pair Sampling. (arXiv:1806.01664v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.01664</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel hierarchical graph clustering algorithm inspired by
modularity-based clustering techniques. The algorithm is agglomerative and
based on a simple distance between clusters induced by the probability of
sampling node pairs. We prove that this distance is reducible, which enables
the use of the nearest-neighbor chain to speed up the agglomeration. The output
of the algorithm is a regular dendrogram, which reveals the multi-scale
structure of the graph. The results are illustrated on both synthetic and real
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonald_T/0/1/0/all/0/1&quot;&gt;Thomas Bonald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charpentier_B/0/1/0/all/0/1&quot;&gt;Bertrand Charpentier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galland_A/0/1/0/all/0/1&quot;&gt;Alexis Galland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hollocou_A/0/1/0/all/0/1&quot;&gt;Alexandre Hollocou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06920">
<title>Maximum a Posteriori Policy Optimisation. (arXiv:1806.06920v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1806.06920</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new algorithm for reinforcement learning called Maximum
aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative
entropy objective. We show that several existing methods can directly be
related to our derivation. We develop two off-policy algorithms and demonstrate
that they are competitive with the state-of-the-art in deep reinforcement
learning. In particular, for continuous control, our method outperforms
existing methods with respect to sample efficiency, premature convergence and
robustness to hyperparameter settings while achieving similar or better final
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdolmaleki_A/0/1/0/all/0/1&quot;&gt;Abbas Abdolmaleki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Springenberg_J/0/1/0/all/0/1&quot;&gt;Jost Tobias Springenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tassa_Y/0/1/0/all/0/1&quot;&gt;Yuval Tassa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1&quot;&gt;Remi Munos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1&quot;&gt;Nicolas Heess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1&quot;&gt;Martin Riedmiller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08449">
<title>Deep Orthogonal Representations: Fundamental Properties and Applications. (arXiv:1806.08449v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.08449</link>
<description rdf:parseType="Literal">&lt;p&gt;Several representation learning and, more broadly, dimensionality reduction
techniques seek to produce representations of the data that are orthogonal
(uncorrelated). Examples include PCA, CCA, Kernel/Deep CCA, the ACE algorithm
and correspondence analysis (CA). For a fixed data distribution, all finite
variance representations belong to the same function space regardless of how
they are derived. In this work, we present a theoretical framework for
analyzing this function space, and demonstrate how a basis for this space can
be found using neural networks. We show that this framework (i) underlies
recent multi-view representation learning methods, (ii) enables classical
exploratory statistical techniques such as CA to be scaled via neural networks,
and (iii) can be used to derive new methods for comparing black-box models. We
illustrate these applications empirically through different datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1&quot;&gt;Hsiang Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salamatian_S/0/1/0/all/0/1&quot;&gt;Salman Salamatian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calmon_F/0/1/0/all/0/1&quot;&gt;Flavio P. Calmon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08456">
<title>Bayesian hierarchical models for SNP discovery from genome-wide association studies, a semi-supervised machine learning approach. (arXiv:1806.08456v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1806.08456</link>
<description rdf:parseType="Literal">&lt;p&gt;Genome-wide association studies (GWASs) aim to detect genetic risk factors
for complex human diseases by identifying disease-associated single-nucleotide
polymorphisms (SNPs). SNP-wise approach, the standard method for analyzing
GWAS, tests each SNP individually. Then the P-values are adjusted for multiple
testing. Multiple testing adjustment (purely based on p-values) is
over-conservative and causes lack of power in many GWASs, due to insufficiently
modelling the relationship among SNPs. To address this problem, we propose a
novel method, which borrows information across SNPs by grouping SNPs into three
clusters. We pre-specify the patterns of clusters by minor allele frequencies
of SNPs between cases and controls, and enforce the patterns with prior
distributions. Therefore, compared with the traditional approach, it better
controls false discovery rate (FDR) and shows higher sensitivity, which is
confirmed by our simulation studies. We re-analyzed real data studies on
identifying SNPs associated with severe bortezomib-induced peripheral
neuropathy (BiPN) in patients with multiple myeloma. The original analysis in
the literature failed to identify SNPs after FDR adjustment. Our proposed
method not only detected the reported SNPs after FDR adjustment but also
discovered a novel SNP rs4351714 that has been reported to be related to
multiple myeloma in another study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xing_L/0/1/0/all/0/1&quot;&gt;Li Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jessica Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuekui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qiu_W/0/1/0/all/0/1&quot;&gt;Weiliang Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08593">
<title>Tensor Monte Carlo: particle methods for the GPU era. (arXiv:1806.08593v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.08593</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-sample objectives improve over single-sample estimates by giving
tighter variational bounds and more accurate estimates of posterior
uncertainty. However, these multi-sample techniques scale poorly, in the sense
that the number of samples required to maintain the same quality of posterior
approximation scales exponentially in the number of latent dimensions. One
approach to addressing these issues is sequential Monte Carlo (SMC). However
for many problems SMC is prohibitively slow because the resampling steps
imposes an inherently sequential structure on the computation, which is
difficult to effectively parallelise on GPU hardware. We developed tensor
Monte-Carlo to address these issues. In particular, whereas the usual
multi-sample objective draws $K$ samples from a joint distribution over all
latent variables, we draw $K$ samples for each of the $n$ individual latent
variables, and form our bound by averaging over all $K^n$ combinations of
samples from each individual latent. While this sum over exponentially many
terms might seem to be intractable, in many cases it can be efficiently
computed by exploiting conditional independence structure. In particular, we
generalise and simplify classical algorithms such as message passing by noting
that these sums can be computed can be written in an extremely simple, general
form: a series of tensor inner-products which can be depicted graphically as
reductions of a factor graph. As such, we can straightforwardly combine
summation over discrete variables with importance sampling over importance
sampling over continuous variables.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1&quot;&gt;Laurence Aitchison&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08675">
<title>Addressing Class Imbalance in Classification Problems of Noisy Signals by using Fourier Transform Surrogates. (arXiv:1806.08675v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1806.08675</link>
<description rdf:parseType="Literal">&lt;p&gt;Randomizing the Fourier-transform (FT) phases of temporal-spatial data
generates surrogates that approximate examples from the data-generating
distribution. We propose such FT surrogates as a novel tool to augment and
analyze training of neural networks and explore the approach in the example of
sleep-stage classification. By computing FT surrogates of raw EEG, EOG, and EMG
signals of under-represented sleep stages, we balanced the CAPSLPDB sleep
database. We then trained and tested a convolutional neural network for sleep
stage classification, and found that our surrogate-based augmentation improved
the mean F1-score by 7%. As another application of FT surrogates, we formulated
an approach to compute saliency maps for individual sleep epochs. The
visualization is based on the response of inferred class probabilities under
replacement of short data segments by partial surrogates. To quantify how well
the distributions of the surrogates and the original data match, we evaluated a
trained classifier on surrogates of correctly classified examples, and
summarized these conditional predictions in a confusion matrix. We show how
such conditional confusion matrices can qualitatively explain the performance
of surrogates in class balancing. The FT-surrogate augmentation approach may
improve classification on noisy signals if carefully adapted to the data
distribution under analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schwabedal_J/0/1/0/all/0/1&quot;&gt;Justus T. C. Schwabedal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Snyder_J/0/1/0/all/0/1&quot;&gt;John C. Snyder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cakmak_A/0/1/0/all/0/1&quot;&gt;Ayse Cakmak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nemati_S/0/1/0/all/0/1&quot;&gt;Shamim Nemati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Clifford_G/0/1/0/all/0/1&quot;&gt;Gari D. Clifford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08716">
<title>Learning Qualitatively Diverse and Interpretable Rules for Classification. (arXiv:1806.08716v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.08716</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been growing interest in developing accurate models that can also
be explained to humans. Unfortunately, if there exist multiple distinct but
accurate models for some dataset, current machine learning methods are unlikely
to find them: standard techniques will likely recover a complex model that
combines them. In this work, we introduce a way to identify a maximal set of
distinct but accurate models for a dataset. We demonstrate empirically that, in
situations where the data supports multiple accurate classifiers, we tend to
recover simpler, more interpretable classifiers rather than more complex ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1&quot;&gt;Andrew Slavin Ross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1&quot;&gt;Weiwei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08734">
<title>On the Spectral Bias of Deep Neural Networks. (arXiv:1806.08734v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.08734</link>
<description rdf:parseType="Literal">&lt;p&gt;It is well known that over-parametrized deep neural networks (DNNs) are an
overly expressive class of functions that can memorize even random data with
$100\%$ training accuracy. This raises the question why they do not easily
overfit real data. To answer this question, we study deep networks using
Fourier analysis. We show that deep networks with finite weights (or trained
for finite number of steps) are inherently biased towards representing smooth
functions over the input space. Specifically, the magnitude of a particular
frequency component ($k$) of deep ReLU network function decays at least as fast
as $\mathcal{O}(k^{-2})$, with width and depth helping polynomially and
exponentially (respectively) in modeling higher frequencies. This shows for
instance why DNNs cannot perfectly \textit{memorize} peaky delta-like
functions. We also show that DNNs can exploit the geometry of low dimensional
data manifolds to approximate complex functions that exist along the manifold
with simple functions when seen with respect to the input space. As a
consequence, we find that all samples (including adversarial samples)
classified by a network to belong to a certain class are connected by a path
such that the prediction of the network along that path does not change.
Finally we find that DNN parameters corresponding to functions with higher
frequency components occupy a smaller volume in the parameter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rahaman_N/0/1/0/all/0/1&quot;&gt;Nasim Rahaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arpit_D/0/1/0/all/0/1&quot;&gt;Devansh Arpit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baratin_A/0/1/0/all/0/1&quot;&gt;Aristide Baratin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Draxler_F/0/1/0/all/0/1&quot;&gt;Felix Draxler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Min Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hamprecht_F/0/1/0/all/0/1&quot;&gt;Fred A. Hamprecht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08764">
<title>Learning Traffic Flow Dynamics using Random Fields. (arXiv:1806.08764v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.08764</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a mesoscopic stochastic model for the reconstruction of
vehicle trajectories from data made available by subsets of (probe) vehicles.
Long-range vehicle interactions are applied in a totally asymmetric simple
exclusion process to capture information made available to connected and
autonomous vehicles. The dynamics are represented by a factor graph, which
enables learning of traffic dynamics from historical data using Bayesian belief
propagation. Adequate probe penetration levels for faithful reconstruction on
single-lane roads is investigated. The estimation technique is tested using a
vehicle trajectory dataset generated using an independent microscopic traffic
simulator. Although the parameters of the traffic state estimation model are
learned from (simulated) historical data, the proposed algorithm is found to be
robust to unpredictable conditions. Moreover, by exposing the algorithm to
varying traffic conditions with increasingly larger datasets, the probe
penetration rates required to capture the traffic dynamics effectively can be
substantially reduced. The results also highlight the need to take into account
randomness in the spatio-temporal coverage associated with probe data for
reliable state estimation algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dilip_D/0/1/0/all/0/1&quot;&gt;Deepthi Mary Dilip&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;DianChao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jabari_S/0/1/0/all/0/1&quot;&gt;Saif Eddin Jabari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.08782">
<title>Finding Local Minima via Stochastic Nested Variance Reduction. (arXiv:1806.08782v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.08782</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose two algorithms that can find local minima faster than the
state-of-the-art algorithms in both finite-sum and general stochastic nonconvex
optimization. At the core of the proposed algorithms is
$\text{One-epoch-SNVRG}^+$ using stochastic nested variance reduction (Zhou et
al., 2018a), which outperforms the state-of-the-art variance reduction
algorithms such as SCSG (Lei et al., 2017). In particular, for finite-sum
optimization problems, the proposed
$\text{SNVRG}^{+}+\text{Neon2}^{\text{finite}}$ algorithm achieves
$\tilde{O}(n^{1/2}\epsilon^{-2}+n\epsilon_H^{-3}+n^{3/4}\epsilon_H^{-7/2})$
gradient complexity to converge to an $(\epsilon, \epsilon_H)$-second-order
stationary point, which outperforms $\text{SVRG}+\text{Neon2}^{\text{finite}}$
(Allen-Zhu and Li, 2017) , the best existing algorithm, in a wide regime. For
general stochastic optimization problems, the proposed
$\text{SNVRG}^{+}+\text{Neon2}^{\text{online}}$ achieves
$\tilde{O}(\epsilon^{-3}+\epsilon_H^{-5}+\epsilon^{-2}\epsilon_H^{-3})$
gradient complexity, which is better than both
$\text{SVRG}+\text{Neon2}^{\text{online}}$ (Allen-Zhu and Li, 2017) and
Natasha2 (Allen-Zhu, 2017) in certain regimes. Furthermore, we explore the
acceleration brought by third-order smoothness of the objective function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dongruo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Pan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1&quot;&gt;Quanquan Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.09112">
<title>Sparse Multi-Output Gaussian Processes for Medical Time Series Prediction. (arXiv:1703.09112v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.09112</link>
<description rdf:parseType="Literal">&lt;p&gt;In the scenario of real-time monitoring of hospital patients, high-quality
inference of patients&apos; health status using all information available from
clinical covariates and lab tests is essential to enable successful medical
interventions and improve patient outcomes. Developing a computational
framework that can learn from observational large-scale electronic health
records (EHRs) and make accurate real-time predictions is a critical step. In
this work, we develop and explore a Bayesian nonparametric model based on
Gaussian process (GP) regression for hospital patient monitoring. We propose
MedGP, a statistical framework that incorporates 24 clinical and lab covariates
and supports a rich reference data set from which relationships between
observed covariates may be inferred and exploited for high-quality inference of
patient state over time. To do this, we develop a highly structured sparse GP
kernel to enable tractable computation over tens of thousands of time points
while estimating correlations among clinical covariates, patients, and
periodicity in patient observations. MedGP has a number of benefits over
current methods, including (i) not requiring an alignment of the time series
data, (ii) quantifying confidence regions in the predictions, (iii) exploiting
a vast and rich database of patients, and (iv) inferring interpretable
relationships among clinical covariates. We evaluate and compare results from
MedGP on the task of online prediction for three patient subgroups from two
medical data sets across 8,043 patients. We found MedGP improves online
prediction over baseline methods for nearly all covariates across different
disease subgroups and studies. The publicly available code is at
https://github.com/bee-hive/MedGP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Li-Fang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Darnell_G/0/1/0/all/0/1&quot;&gt;Gregory Darnell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dumitrascu_B/0/1/0/all/0/1&quot;&gt;Bianca Dumitrascu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chivers_C/0/1/0/all/0/1&quot;&gt;Corey Chivers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Draugelis_M/0/1/0/all/0/1&quot;&gt;Michael E Draugelis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Engelhardt_B/0/1/0/all/0/1&quot;&gt;Barbara E Engelhardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.01751">
<title>Near-Optimal Coresets of Kernel Density Estimates. (arXiv:1802.01751v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.01751</link>
<description rdf:parseType="Literal">&lt;p&gt;We construct near-optimal coresets for kernel density estimate for points in
$\mathbb{R^d}$ when the kernel is positive definite. Specifically we show a
polynomial time construction for a coreset of size $O(\sqrt{d\log
(1/\epsilon)}/\epsilon)$, and we show a near-matching lower bound of size
$\Omega(\sqrt{d}/\epsilon)$. The upper bound is a polynomial in $1/\epsilon$
improvement when $d \in [3,1/\epsilon^2)$ (for all kernels except the Gaussian
kernel which had a previous upper bound of $O((1/\epsilon) \log^d
(1/\epsilon))$) and the lower bound is the first known lower bound to depend on
$d$ for this problem. Moreover, the upper bound restriction that the kernel is
positive definite is significant in that it applies to a wide-variety of
kernels, specifically those most important for machine learning. This includes
kernels for information distances and the sinc kernel which can be negative.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1&quot;&gt;Jeff M. Phillips&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_W/0/1/0/all/0/1&quot;&gt;Wai Ming Tai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08475">
<title>Attention Solves Your TSP, Approximately. (arXiv:1803.08475v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08475</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of efficient (heuristic) algorithms for practical
combinatorial optimization problems is costly, so we want to automatically
learn them instead. We show the feasibility of this approach on the important
Travelling Salesman Problem (TSP). We learn a heuristic algorithm that uses a
Neural Network policy to construct a tour. As an alternative to the Pointer
Network, our model is based entirely on (graph) attention layers and is
invariant to the input order of the nodes. We train the model efficiently using
REINFORCE with a simple and robust baseline based on a deterministic (greedy)
rollout of the best policy so far. We significantly improve over results from
previous works that consider learned heuristics for the TSP, reducing the
optimality gap for a single tour construction from 1.51% to 0.32% for instances
with 20 nodes, from 4.59% to 1.71% for 50 nodes and from 6.89% to 4.43% for 100
nodes. Additionally, we improve over a recent Reinforcement Learning framework
for two variants of the Vehicle Routing Problem (VRP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kool_W/0/1/0/all/0/1&quot;&gt;Wouter Kool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hoof_H/0/1/0/all/0/1&quot;&gt;Herke van Hoof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08841">
<title>The Convergence of Stochastic Gradient Descent in Asynchronous Shared Memory. (arXiv:1803.08841v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08841</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic Gradient Descent (SGD) is a fundamental algorithm in machine
learning, representing the optimization backbone for training several classic
models, from regression to neural networks. Given the recent practical focus on
distributed machine learning, significant work has been dedicated to the
convergence properties of this algorithm under the inconsistent and noisy
updates arising from execution in a distributed environment. However,
surprisingly, the convergence properties of this classic algorithm in the
standard shared-memory model are still not well-understood.
&lt;/p&gt;
&lt;p&gt;In this work, we address this gap, and provide new convergence bounds for
lock-free concurrent stochastic gradient descent, executing in the classic
asynchronous shared memory model, against a strong adaptive adversary. Our
results give improved upper and lower bounds on the &quot;price of asynchrony&quot; when
executing the fundamental SGD algorithm in a concurrent setting. They show that
this classic optimization tool can converge faster and with a wider range of
parameters than previously known under asynchronous iterations. At the same
time, we exhibit a fundamental trade-off between the maximum delay in the
system and the rate at which SGD can converge, which governs the set of
parameters under which this algorithm can still work efficiently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1&quot;&gt;Dan Alistarh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1&quot;&gt;Christopher De Sa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konstantinov_N/0/1/0/all/0/1&quot;&gt;Nikola Konstantinov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.01667">
<title>Intracranial Error Detection via Deep Learning. (arXiv:1805.01667v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.01667</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning techniques have revolutionized the field of machine learning
and were recently successfully applied to various classification problems in
noninvasive electroencephalography (EEG). However, these methods were so far
only rarely evaluated for use in intracranial EEG. We employed convolutional
neural networks (CNNs) to classify and characterize the error-related brain
response as measured in 24 intracranial EEG recordings. Decoding accuracies of
CNNs were significantly higher than those of a regularized linear discriminant
analysis. Using time-resolved deep decoding, it was possible to classify errors
in various regions in the human brain, and further to decode errors over 200 ms
before the actual erroneous button press, e.g., in the precentral gyrus.
Moreover, deeper networks performed better than shallower networks in
distinguishing correct from error trials in all-channel decoding. In single
recordings, up to 100 % decoding accuracy was achieved. Visualization of the
networks&apos; learned features indicated that multivariate decoding on an ensemble
of channels yields related, albeit non-redundant information compared to
single-channel decoding. In summary, here we show the usefulness of deep
learning for both intracranial error decoding and mapping of the
spatio-temporal structure of the human error processing network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volker_M/0/1/0/all/0/1&quot;&gt;Martin V&amp;#xf6;lker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammer_J/0/1/0/all/0/1&quot;&gt;Ji&amp;#x159;&amp;#xed; Hammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1&quot;&gt;Robin T. Schirrmeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behncke_J/0/1/0/all/0/1&quot;&gt;Joos Behncke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiederer_L/0/1/0/all/0/1&quot;&gt;Lukas D.J. Fiederer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulze_Bonhage_A/0/1/0/all/0/1&quot;&gt;Andreas Schulze-Bonhage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marusic_P/0/1/0/all/0/1&quot;&gt;Petr Marusi&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1&quot;&gt;Wolfram Burgard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1&quot;&gt;Tonio Ball&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08268">
<title>Effective Dimension of Exp-concave Optimization. (arXiv:1805.08268v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08268</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the role of the effective (a.k.a. statistical) dimension in
determining both the statistical and the computational costs associated with
exp-concave stochastic minimization. We derive sample complexity bounds that
scale with $\frac{d_{\lambda}}{\epsilon}$, where $d_{\lambda}$ is the effective
dimension associated with the regularization parameter $\lambda$. These are the
first fast rates in this setting that do not exhibit any explicit dependence
either on the intrinsic dimension or the $\ell_{2}$-norm of the optimal
classifier.
&lt;/p&gt;
&lt;p&gt;We also propose fast preconditioned method that solves the ERM problem in
time $\tO \left(\min \left \{\frac{\lambda&apos;}{\lambda} \left(
nnz(A)+\,d_{\lambda&apos;}^{2}d\right) :\,\lambda&apos; \ge \lambda \right \} \right)$,
where $nnz(A)$ is the number of nonzero entries in the data.
&lt;/p&gt;
&lt;p&gt;Our analysis emphasizes interesting connections between leverage scores,
algorithmic stability and regularization. In particular, our algorithm involves
a novel technique for optimizing a tradeoff between oracle complexity and
effective dimension. All of our results extend to the kernel setting.?
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1&quot;&gt;Naman Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonen_A/0/1/0/all/0/1&quot;&gt;Alon Gonen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.02199">
<title>Deep Self-Organization: Interpretable Discrete Representation Learning on Time Series. (arXiv:1806.02199v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.02199</link>
<description rdf:parseType="Literal">&lt;p&gt;Human professionals are often required to make decisions based on complex
multivariate time series measurements in an online setting, e.g. in health
care. Since human cognition is not optimized to work well in high-dimensional
spaces, these decisions benefit from interpretable low-dimensional
representations. However, many representation learning algorithms for time
series data are difficult to interpret. This is due to non-intuitive mappings
from data features to salient properties of the representation and
non-smoothness over time. To address this problem, we propose to couple a
variational autoencoder to a discrete latent space and introduce a topological
structure through the use of self-organizing maps. This allows us to learn
discrete representations of time series, which give rise to smooth and
interpretable embeddings with superior clustering performance. Furthermore, to
allow for a probabilistic interpretation of our method, we integrate a Markov
model in the latent space. This model uncovers the temporal transition
structure, improves clustering performance even further and provides additional
explanatory insights as well as a natural representation of uncertainty. We
evaluate our model on static (Fashion-)MNIST data, a time series of linearly
interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two
macro states, as well as on a challenging real world medical time series
application. In the latter experiment, our representation uncovers meaningful
structure in the acute physiological state of a patient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1&quot;&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huser_M/0/1/0/all/0/1&quot;&gt;Matthias H&amp;#xfc;ser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strathmann_H/0/1/0/all/0/1&quot;&gt;Heiko Strathmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratsch_G/0/1/0/all/0/1&quot;&gt;Gunnar R&amp;#xe4;tsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07174">
<title>FRnet-DTI: Convolutional Neural Networks for Drug-Target Interaction. (arXiv:1806.07174v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.07174</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of drug-target interaction prediction holds significant importance
in pharmacology and therapeutic drug design. In this paper, we present
FRnet-DTI, an auto encoder and a convolutional classifier for feature
manipulation and drug target interaction prediction. Two convolutional neural
neworks are proposed where one model is used for feature manipulation and the
other one for classification. Using the first method FRnet-1, we generate 4096
features for each of the instances in each of the datasets and use the second
method, FRnet-2, to identify interaction probability employing those features.
We have tested our method on four gold standard datasets exhaustively used by
other researchers. Experimental results shows that our method significantly
improves over the state-of-the-art method on three of the four drug-target
interaction gold standard datasets on both area under curve for Receiver
Operating Characteristic(auROC) and area under Precision Recall curve(auPR)
metric. We also introduce twenty new potential drug-target pairs for
interaction based on high prediction scores. Codes Available: https: // github.
com/ farshidrayhanuiu/ FRnet-DTI/ Web Implementation: http: // farshidrayhan.
pythonanywhere. com/ FRnet-DTI/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rayhan_F/0/1/0/all/0/1&quot;&gt;Farshid Rayhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Sajid Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavian_Z/0/1/0/all/0/1&quot;&gt;Zaynab Mousavian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farid_D/0/1/0/all/0/1&quot;&gt;Dewan Md Farid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shatabda_S/0/1/0/all/0/1&quot;&gt;Swakkhar Shatabda&lt;/a&gt;</dc:creator>
</item></rdf:RDF>