<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06508"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06682"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06732"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06774"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06808"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00541"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03209"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06439"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06451"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06459"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06620"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06647"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06748"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06760"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06764"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1503.08381"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.07045"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.08163"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06416"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05044"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06481"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06498"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06500"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06673"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06679"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06755"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06776"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06802"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1604.04706"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.09499"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.00105"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06798"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11057"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03471"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03713"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06352"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.06508">
<title>UCNN: Exploiting Computational Reuse in Deep Neural Networks via Weight Repetition. (arXiv:1804.06508v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.06508</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) have begun to permeate all corners of
electronic society (from voice recognition to scene generation) due to their
high accuracy and machine efficiency per operation. At their core, CNN
computations are made up of multi-dimensional dot products between weight and
input vectors. This paper studies how weight repetition ---when the same weight
occurs multiple times in or across weight vectors--- can be exploited to save
energy and improve performance during CNN inference. This generalizes a popular
line of work to improve efficiency from CNN weight sparsity, as reducing
computation due to repeated zero weights is a special case of reducing
computation due to repeated weights.
&lt;/p&gt;
&lt;p&gt;To exploit weight repetition, this paper proposes a new CNN accelerator
called the Unique Weight CNN Accelerator (UCNN). UCNN uses weight repetition to
reuse CNN sub-computations (e.g., dot products) and to reduce CNN model size
when stored in off-chip DRAM ---both of which save energy. UCNN further
improves performance by exploiting sparsity in weights. We evaluate UCNN with
an accelerator-level cycle and energy model and with an RTL implementation of
the UCNN processing element. On three contemporary CNNs, UCNN improves
throughput-normalized energy consumption by 1.2x - 4x, relative to a similarly
provisioned baseline accelerator that uses Eyeriss-style sparsity
optimizations. At the same time, the UCNN processing element adds only 17-24%
area overhead relative to the same baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegde_K/0/1/0/all/0/1&quot;&gt;Kartik Hegde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jiyong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_R/0/1/0/all/0/1&quot;&gt;Rohit Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Mengjia Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellauer_M/0/1/0/all/0/1&quot;&gt;Michael Pellauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fletcher_C/0/1/0/all/0/1&quot;&gt;Christopher W. Fletcher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06682">
<title>A Robot to Shape your Natural Plant: The Machine Learning Approach to Model and Control Bio-Hybrid Systems. (arXiv:1804.06682v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.06682</link>
<description rdf:parseType="Literal">&lt;p&gt;Bio-hybrid systems---close couplings of natural organisms with
technology---are high potential and still underexplored. In existing work,
robots have mostly influenced group behaviors of animals. We explore the
possibilities of mixing robots with natural plants, merging useful attributes.
Significant synergies arise by combining the plants&apos; ability to efficiently
produce shaped material and the robots&apos; ability to extend sensing and
decision-making behaviors. However, programming robots to control plant motion
and shape requires good knowledge of complex plant behaviors. Therefore, we use
machine learning to create a holistic plant model and evolve robot controllers.
As a benchmark task we choose obstacle avoidance. We use computer vision to
construct a model of plant stem stiffening and motion dynamics by training an
LSTM network. The LSTM network acts as a forward model predicting change in the
plant, driving the evolution of neural network robot controllers. The evolved
controllers augment the plants&apos; natural light-finding and tissue-stiffening
behaviors to avoid obstacles and grow desired shapes. We successfully verify
the robot controllers and bio-hybrid behavior in reality, with a physical setup
and actual plants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahby_M/0/1/0/all/0/1&quot;&gt;Mostafa Wahby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinrich_M/0/1/0/all/0/1&quot;&gt;Mary Katherine Heinrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofstadler_D/0/1/0/all/0/1&quot;&gt;Daniel Nicolas Hofstadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zahadat_P/0/1/0/all/0/1&quot;&gt;Payam Zahadat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1&quot;&gt;Sebastian Risi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayres_P/0/1/0/all/0/1&quot;&gt;Phil Ayres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmickl_T/0/1/0/all/0/1&quot;&gt;Thomas Schmickl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamann_H/0/1/0/all/0/1&quot;&gt;Heiko Hamann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06732">
<title>DPRed: Making Typical Activation Values Matter In Deep Learning Computing. (arXiv:1804.06732v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.06732</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that selecting a fixed precision for all activations in Convolutional
Neural Networks, even if that precision is different per layer, amounts to
worst case design. We show that much lower precisions can be used, if we could
target the common case instead by tailoring the precision at a much finer
granularity than that of a layer. We propose Dynamic Prediction Reduction
(DPRed) where hardware on-the-fly detects the precision activations need and at
a much finer granularity than a whole layer. We demonstrate a practical
implementation of DPRed with DPRed Stripes (DPRS), a data-parallel hardware
accelerator that adjusts precision on-the-fly to accommodate the values of the
activations it processes concurrently. DPRS accelerates convolutional layers
and executes unmodified convolutional neural networks. DPRS is 2.61x faster and
1.84x more energy efficient than a fixed-precision accelerator for a set of
convolutional neural networks. We further extend DPRS to exploit activation and
weight precisions for fully-connected layers. The enhanced design improves
average performance and energy efficiency respectively by 2.59x and 1.19x over
the fixed-precision accelerator for a broader set of neural networks. We also
consider a lower cost variant that supports only even precision widths which
offers better energy efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delmas_A/0/1/0/all/0/1&quot;&gt;Alberto Delmas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharify_S/0/1/0/all/0/1&quot;&gt;Sayeh Sharify&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Judd_P/0/1/0/all/0/1&quot;&gt;Patrick Judd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolic_M/0/1/0/all/0/1&quot;&gt;Milos Nikolic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moshovos_A/0/1/0/all/0/1&quot;&gt;Andreas Moshovos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06774">
<title>Encoding Longer-term Contextual Multi-modal Information in a Predictive Coding Model. (arXiv:1804.06774v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.06774</link>
<description rdf:parseType="Literal">&lt;p&gt;Studies suggest that within the hierarchical architecture, the topological
higher level possibly represents a conscious category of the current sensory
events with slower changing activities. They attempt to predict the activities
on the lower level by relaying the predicted information. On the other hand,
the incoming sensory information corrects such prediction of the events on the
higher level by the novel or surprising signal. We propose a predictive
hierarchical artificial neural network model that examines this hypothesis on
neurorobotic platforms, based on the AFA-PredNet model. In this neural network
model, there are different temporal scales of predictions exist on different
levels of the hierarchical predictive coding, which are defined in the temporal
parameters in the neurons. Also, both the fast and the slow-changing neural
activities are modulated by the active motor activities. A neurorobotic
experiment based on the architecture was also conducted based on the data
collected from the VRep simulator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1&quot;&gt;Junpei Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogata_T/0/1/0/all/0/1&quot;&gt;Tetsuya Ogata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cangelosi_A/0/1/0/all/0/1&quot;&gt;Angelo Cangelosi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06808">
<title>Solving the Exponential Growth of Symbolic Regression Trees in Geometric Semantic Genetic Programming. (arXiv:1804.06808v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.06808</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in Geometric Semantic Genetic Programming (GSGP) have shown that
this variant of Genetic Programming (GP) reaches better results than its
predecessor for supervised machine learning problems, particularly in the task
of symbolic regression. However, by construction, the geometric semantic
crossover operator generates individuals that grow exponentially with the
number of generations, resulting in solutions with limited use. This paper
presents a new method for individual simplification named GSGP with Reduced
trees (GSGP-Red). GSGP-Red works by expanding the functions generated by the
geometric semantic operators. The resulting expanded function is guaranteed to
be a linear combination that, in a second step, has its repeated structures and
respective coefficients aggregated. Experiments in 12 real-world datasets show
that it is not only possible to create smaller and completely equivalent
individuals in competitive computational time, but also to reduce the number of
nodes composing them by 58 orders of magnitude, on average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martins_J/0/1/0/all/0/1&quot;&gt;Joao Francisco B. S. Martins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1&quot;&gt;Luiz Otavio V. B. Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miranda_L/0/1/0/all/0/1&quot;&gt;Luis F. Miranda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casadei_F/0/1/0/all/0/1&quot;&gt;Felipe Casadei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappa_G/0/1/0/all/0/1&quot;&gt;Gisele L. Pappa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00541">
<title>TasNet: time-domain audio separation network for real-time, single-channel speech separation. (arXiv:1711.00541v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00541</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust speech processing in multi-talker environments requires effective
speech separation. Recent deep learning systems have made significant progress
toward solving this problem, yet it remains challenging particularly in
real-time, short latency applications. Most methods attempt to construct a mask
for each source in time-frequency representation of the mixture signal which is
not necessarily an optimal representation for speech separation. In addition,
time-frequency decomposition results in inherent problems such as
phase/magnitude decoupling and long time window which is required to achieve
sufficient frequency resolution. We propose Time-domain Audio Separation
Network (TasNet) to overcome these limitations. We directly model the signal in
the time-domain using an encoder-decoder framework and perform the source
separation on nonnegative encoder outputs. This method removes the frequency
decomposition step and reduces the separation problem to estimation of source
masks on encoder outputs which is then synthesized by the decoder. Our system
outperforms the current state-of-the-art causal and noncausal speech separation
algorithms, reduces the computational cost of speech separation, and
significantly reduces the minimum required latency of the output. This makes
TasNet suitable for applications where low-power, real-time implementation is
desirable such as in hearable and telecommunication devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mesgarani_N/0/1/0/all/0/1&quot;&gt;Nima Mesgarani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03209">
<title>Drift Theory in Continuous Search Spaces: Expected Hitting Time of the (1+1)-ES with 1/5 Success Rule. (arXiv:1802.03209v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03209</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the use of the standard approach for proving runtime
bounds in discrete domains---often referred to as drift analysis---in the
context of optimization on a continuous domain. Using this framework we analyze
the (1+1) Evolution Strategy with one-fifth success rule on the sphere
function. To deal with potential functions that are not lower-bounded, we
formulate novel drift theorems. We then use the theorems to prove bounds on the
expected hitting time to reach a certain target fitness in finite dimension
$d$. The bounds are akin to linear convergence. We then study the dependency of
the different terms on $d$ proving a convergence rate dependency of
$\Theta(1/d)$. Our results constitute the first non-asymptotic analysis for the
algorithm considered as well as the first explicit application of drift
analysis to a randomized search heuristic with continuous domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akimoto_Y/0/1/0/all/0/1&quot;&gt;Youhei Akimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auger_A/0/1/0/all/0/1&quot;&gt;Anne Auger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glasmachers_T/0/1/0/all/0/1&quot;&gt;Tobias Glasmachers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06439">
<title>Personalized neural language models for real-world query auto completion. (arXiv:1804.06439v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.06439</link>
<description rdf:parseType="Literal">&lt;p&gt;Query auto completion (QAC) systems are a standard part of search engines in
industry, helping users formulate their query. Such systems update their
suggestions after the user types each character, predicting the user&apos;s intent
using various signals - one of the most common being popularity. Recently, deep
learning approaches have been proposed for the QAC task, to specifically
address the main limitation of previous popularity-based methods: the inability
to predict unseen queries. In this work we improve previous methods based on
neural language modeling, with the goal of building an end-to-end system. We
particularly focus on using real-world data by integrating user information for
personalized suggestions when possible. We also make use of time information
and study how to increase diversity in the suggestions while studying the
impact on scalability. Our empirical results demonstrate a marked improvement
on two separate datasets over previous best methods in both accuracy and
scalability, making a step towards neural query auto-completion in production
search engines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorini_N/0/1/0/all/0/1&quot;&gt;Nicolas Fiorini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06451">
<title>Multi-Reward Reinforced Summarization with Saliency and Entailment. (arXiv:1804.06451v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.06451</link>
<description rdf:parseType="Literal">&lt;p&gt;Abstractive text summarization is the task of compressing and rewriting a
long document into a short summary while maintaining saliency, directed logical
entailment, and non-redundancy. In this work, we address these three important
aspects of a good summary via a reinforcement learning approach with two novel
reward functions: ROUGESal and Entail, on top of a coverage-based baseline. The
ROUGESal reward modifies the ROUGE metric by up-weighting the salient
phrases/words detected via a keyphrase classifier. The Entail reward gives high
(length-normalized) scores to logically-entailed summaries using an entailment
classifier. Further, we show superior performance improvement when these
rewards are combined with traditional metric (ROUGE) based rewards, via our
novel and effective multi-reward approach of optimizing multiple rewards
simultaneously in alternate mini-batches. Our method achieves the new
state-of-the-art results on CNN/Daily Mail dataset as well as strong
improvements in a test-only transfer setup on DUC-2002.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1&quot;&gt;Ramakanth Pasunuru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06459">
<title>On Learning Intrinsic Rewards for Policy Gradient Methods. (arXiv:1804.06459v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.06459</link>
<description rdf:parseType="Literal">&lt;p&gt;In many sequential decision making tasks, it is challenging to design reward
functions that help an RL agent efficiently learn behavior that is considered
good by the agent designer. A number of different formulations of the
reward-design problem, or close variants thereof, have been proposed in the
literature. In this paper we build on the Optimal Rewards Framework of Singh
et.al. that defines the optimal intrinsic reward function as one that when used
by an RL agent achieves behavior that optimizes the task-specifying or
extrinsic reward function. Previous work in this framework has shown how good
intrinsic reward functions can be learned for lookahead search based planning
agents. Whether it is possible to learn intrinsic reward functions for learning
agents remains an open problem. In this paper we derive a novel algorithm for
learning intrinsic rewards for policy-gradient based learning agents. We
compare the performance of an augmented agent that uses our algorithm to
provide additive intrinsic rewards to an A2C-based policy learner (for Atari
games) and a PPO-based policy learner (for Mujoco domains) with a baseline
agent that uses the same policy learners but with only extrinsic rewards. Our
results show improved performance on most but not all of the domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zeyu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Junhyuk Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Satinder Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06620">
<title>Visualizing the Feature Importance for Black Box Models. (arXiv:1804.06620v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.06620</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, a large amount of model-agnostic methods to improve the
transparency, trustability and interpretability of machine learning models have
been developed. We introduce local feature importance as a local version of a
recent model-agnostic global feature importance method. Based on local feature
importance, we propose two visual tools: partial importance (PI) and individual
conditional importance (ICI) plots which visualize how changes in a feature
affect the model performance on average, as well as for individual
observations. Our proposed methods are related to partial dependence (PD) and
individual conditional expectation (ICE) plots, but visualize the expected
(conditional) feature importance instead of the expected (conditional)
prediction. Furthermore, we show that averaging ICI curves across observations
yields a PI curve, and integrating the PI curve with respect to the
distribution of the considered feature results in the global feature
importance. Another contribution of our paper is the Shapley feature
importance, which fairly distributes the overall performance of a model among
the features according to the marginal contributions and which can be used to
compare the feature importance across different models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Casalicchio_G/0/1/0/all/0/1&quot;&gt;Giuseppe Casalicchio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Molnar_C/0/1/0/all/0/1&quot;&gt;Christoph Molnar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06647">
<title>Modular Verification of Vehicle Platooning with Respect to Decisions, Space and Time. (arXiv:1804.06647v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.06647</link>
<description rdf:parseType="Literal">&lt;p&gt;The spread of autonomous systems into safety-critical areas has increased the
demand for their formal verification, not only due to stronger certification
requirements but also to public uncertainty over these new technologies.
However, the complex nature of such systems, for example, the intricate
combination of discrete and continuous aspects, ensures that whole system
verification is often infeasible. This motivates the need for novel analysis
approaches that modularise the problem, allowing us to restrict our analysis to
one particular aspect of the system while abstracting away from others. For
instance, while verifying the real-time properties of an autonomous system we
might hide the details of the internal decision-making components. In this
paper we describe verification of a range of properties across distinct
dimesnions on a practical hybrid agent architecture. This allows us to verify
the autonomous decision-making, real-time aspects, and spatial aspects of an
autonomous vehicle platooning system. This modular approach also illustrates
how both algorithmic and deductive verification techniques can be applied for
the analysis of different system subcomponents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamali_M/0/1/0/all/0/1&quot;&gt;Maryam Kamali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linker_S/0/1/0/all/0/1&quot;&gt;Sven Linker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1&quot;&gt;Michael Fisher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06748">
<title>State-Space Abstractions for Probabilistic Inference: A Systematic Review. (arXiv:1804.06748v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.06748</link>
<description rdf:parseType="Literal">&lt;p&gt;Tasks such as social network analysis, human behavior recognition, or
modeling biochemical reactions, can be solved elegantly by using the
probabilistic inference framework. However, standard probabilistic inference
algorithms work at a propositional level, and thus cannot capture the
symmetries and redundancies that are present in these tasks. Algorithms that
exploit those symmetries have been devised in different research fields, for
example by the lifted inference-, multiple object tracking-, and modeling and
simulation-communities. The common idea, that we call state space abstraction,
is to perform inference over compact representations of sets of symmetric
states. Although they are concerned with a similar topic, the relationship
between these approaches has not been investigated systematically. This survey
provides the following contributions. We perform a systematic literature review
to outline the state of the art in probabilistic inference methods exploiting
symmetries. From an initial set of more than 4,000 papers, we identify 116
relevant papers. Furthermore, we provide new high-level categories that
classify the approaches, based on the problem classes the different approaches
can solve. Researchers from different fields that are confronted with a state
space explosion problem in a probabilistic system can use this classification
to identify possible solutions. Finally, based on this conceptualization, we
identify potentials for future research, as some relevant application domains
are not addressed by current approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ludtke_S/0/1/0/all/0/1&quot;&gt;Stefan L&amp;#xfc;dtke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroder_M/0/1/0/all/0/1&quot;&gt;Max Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruger_F/0/1/0/all/0/1&quot;&gt;Frank Kr&amp;#xfc;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bader_S/0/1/0/all/0/1&quot;&gt;Sebastian Bader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirste_T/0/1/0/all/0/1&quot;&gt;Thomas Kirste&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06760">
<title>Simulation-based Adversarial Test Generation for Autonomous Vehicles with Machine Learning Components. (arXiv:1804.06760v1 [cs.SY])</title>
<link>http://arxiv.org/abs/1804.06760</link>
<description rdf:parseType="Literal">&lt;p&gt;Many organizations are developing autonomous driving systems, which are
expected to be deployed at a large scale in the near future. Despite this,
there is a lack of agreement on appropriate methods to test, debug, and certify
the performance of these systems. One of the main challenges is that many
autonomous driving systems have machine learning components, such as deep
neural networks, for which formal properties are difficult to characterize. We
present a testing framework that is compatible with test case generation and
automatic falsification methods, which are used to evaluate cyber-physical
systems. We demonstrate how the framework can be used to evaluate closed-loop
properties of an autonomous driving system model that includes the ML
components, all within a virtual environment. We demonstrate how to use test
case generation methods, such as covering arrays, as well as requirement
falsification methods to automatically identify problematic test scenarios. The
resulting framework can be used to increase the reliability of autonomous
driving systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuncali_C/0/1/0/all/0/1&quot;&gt;Cumhur Erkan Tuncali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fainekos_G/0/1/0/all/0/1&quot;&gt;Georgios Fainekos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ito_H/0/1/0/all/0/1&quot;&gt;Hisahiro Ito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapinski_J/0/1/0/all/0/1&quot;&gt;James Kapinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06764">
<title>A Parallel/Distributed Algorithmic Framework for Mining All Quantitative Association Rules. (arXiv:1804.06764v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.06764</link>
<description rdf:parseType="Literal">&lt;p&gt;We present QARMA, an efficient novel parallel algorithm for mining all
Quantitative Association Rules in large multidimensional datasets where items
are required to have at least a single common attribute to be specified in the
rules single consequent item. Given a minimum support level and a set of
threshold criteria of interestingness measures such as confidence, conviction
etc. our algorithm guarantees the generation of all non-dominated Quantitative
Association Rules that meet the minimum support and interestingness
requirements. Such rules can be of great importance to marketing departments
seeking to optimize targeted campaigns, or general market segmentation. They
can also be of value in medical applications, financial as well as predictive
maintenance domains. We provide computational results showing the scalability
of our algorithm, and its capability to produce all rules to be found in large
scale synthetic and real world datasets such as Movie Lens, within a few
seconds or minutes of computational time on commodity hardware.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christou_I/0/1/0/all/0/1&quot;&gt;Ioannis T. Christou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amolochitis_E/0/1/0/all/0/1&quot;&gt;Emmanouil Amolochitis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zheng-Hua Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1503.08381">
<title>Conditional Random Fields with Decode-based Learning: Simpler and Faster. (arXiv:1503.08381v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1503.08381</link>
<description rdf:parseType="Literal">&lt;p&gt;Conditional random fields (CRF) is one of the most famous approaches for
structured classification. It is a structured gradient-based method, which has
high accuracy but with drawbacks: very slow training, hard to implement in the
tasks with complex structures, and no support of decode-based optimization
(which is important in many cases). To address these issues, we propose a
simple and fast solution, a decode-based probabilistic online learning method,
called CRF with decode-based learning (DBL-CRF). The proposed DBL-CRF decodes
the output candidates, derives probabilities, and conduct efficient online
learning. The method has the similar probabilistic information as CRF, but
supports decode-based optimization and does not need gradient computation. We
show that this method is with fast training, very simple to implement, with top
accuracy, and with theoretical guarantees of convergence. Experiments on
well-known tasks show that our method has better accuracy and much faster speed
than our strong baseline CRF systems. The code is available at
https://github.com/lancopku/Decode-CRF
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shuming Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.07045">
<title>pg-Causality: Identifying Spatiotemporal Causal Pathways for Air Pollutants with Urban Big Data. (arXiv:1610.07045v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1610.07045</link>
<description rdf:parseType="Literal">&lt;p&gt;Many countries are suffering from severe air pollution. Understanding how
different air pollutants accumulate and propagate is critical to making
relevant public policies. In this paper, we use urban big data (air quality
data and meteorological data) to identify the \emph{spatiotemporal (ST) causal
pathways} for air pollutants. This problem is challenging because: (1) there
are numerous noisy and low-pollution periods in the raw air quality data, which
may lead to unreliable causality analysis, (2) for large-scale data in the ST
space, the computational complexity of constructing a causal structure is very
high, and (3) the \emph{ST causal pathways} are complex due to the interactions
of multiple pollutants and the influence of environmental factors. Therefore,
we present \emph{p-Causality}, a novel pattern-aided causality analysis
approach that combines the strengths of \emph{pattern mining} and
\emph{Bayesian learning} to efficiently and faithfully identify the \emph{ST
causal pathways}. First, \emph{Pattern mining} helps suppress the noise by
capturing frequent evolving patterns (FEPs) of each monitoring sensor, and
greatly reduce the complexity by selecting the pattern-matched sensors as
&quot;causers&quot;. Then, \emph{Bayesian learning} carefully encodes the local and ST
causal relations with a Gaussian Bayesian network (GBN)-based graphical model,
which also integrates environmental influences to minimize biases in the final
results. We evaluate our approach with three real-world data sets containing
982 air quality sensors, in three regions of China from 01-Jun-2013 to
19-Dec-2015. Results show that our approach outperforms the traditional causal
structure learning methods in time efficiency, inference accuracy and
interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Julie Yixuan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huichu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhi_S/0/1/0/all/0/1&quot;&gt;Shi Zhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_V/0/1/0/all/0/1&quot;&gt;Victor O.K. Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yu Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.08163">
<title>Intrusions in Marked Renewal Processes. (arXiv:1709.08163v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.08163</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a probabilistic model of an intrusion in a marked renewal process.
Given a process and a sequence of events, an intrusion is a subsequence of
events that is not produced by the process. Applications of the model are, for
example, online payment fraud with the fraudster taking over a user&apos;s account
and performing payments on the user&apos;s behalf, or unexpected equipment failures
due to unintended use.
&lt;/p&gt;
&lt;p&gt;We adopt Bayesian approach to infer the probability of an intrusion in a
sequence of events, a MAP subsequence of events constituting the intrusion, and
the marginal probability of each event in a sequence to belong to the
intrusion. We evaluate the model for intrusion detection on synthetic data, as
well as on anonymized data from an online payment system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolpin_D/0/1/0/all/0/1&quot;&gt;David Tolpin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06416">
<title>Sim-to-Real Optimization of Complex Real World Mobile Network with Imperfect Information via Deep Reinforcement Learning from Self-play. (arXiv:1802.06416v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06416</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobile network that millions of people use every day is one of the most
complex systems in real world. Optimization of mobile network to meet exploding
customer demand and reduce CAPEX/OPEX poses greater challenges than in prior
works. Actually, learning to solve complex problems in real world to benefit
everyone and make the world better has long been ultimate goal of AI. However,
application of deep reinforcement learning (DRL) to complex problems in real
world still remains unsolved, due to imperfect information, data scarcity and
complex rules in real world, potential negative impact to real world, etc. To
bridge this reality gap, we propose a sim-to-real framework to direct transfer
learning from simulation to real world without any training in real world.
First, we distill temporal-spatial relationships between cells and mobile users
to scalable 3D image-like tensor to best characterize partially observed mobile
network. Second, inspired by AlphaGo, we introduce a novel self-play mechanism
to empower DRL agents to gradually improve intelligence by competing for best
record on multiple tasks, just like athletes compete for world record in
decathlon. Third, a decentralized DRL method is proposed to coordinate
multi-agents to compete and cooperate as a team to maximize global reward and
minimize potential negative impact. Using 7693 unseen test tasks over 160
unseen mobile networks in another simulator as well as 6 field trials on 4
commercial mobile networks in real world, we demonstrate the capability of this
sim-to-real framework to direct transfer the learning not only from one
simulator to another simulator, but also from simulation to real world. This is
the first time that a DRL agent successfully transfers its learning directly
from simulation to very complex real world problems with imperfect information,
complex rules, huge state/action space, and multi-agent interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yongxi Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1&quot;&gt;Qitao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunjun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zhangxiang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1&quot;&gt;Zhenqiang Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05044">
<title>Monitoring and Executing Workflows in Linked Data Environments. (arXiv:1804.05044v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05044</link>
<description rdf:parseType="Literal">&lt;p&gt;The W3C&apos;s Web of Things working group is aimed at addressing the
interoperability problem on the Internet of Things using Linked Data as uniform
interface. While Linked Data paves the way towards combining such devices into
integrated applications, traditional solutions for specifying the control flow
of applications do not work seamlessly with Linked Data. We therefore tackle
the problem of the specification, execution, and monitoring of applications in
the context of Linked Data. We present a novel approach that combines
workflows, semantic reasoning, and RESTful interaction into one integrated
solution. We contribute to the state of the art by (1) defining an ontology for
describing workflow models and instances, (2) providing operational semantics
for the ontology that allows for the execution and monitoring of workflow
instances, (3) presenting a benchmark to evaluate our solution. Moreover, we
showcase how we used the ontology and the operational semantics to monitor
pilots executing workflows in virtual aircraft cockpits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kafer_T/0/1/0/all/0/1&quot;&gt;Tobias K&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harth_A/0/1/0/all/0/1&quot;&gt;Andreas Harth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06481">
<title>Unlearn What You Have Learned: Adaptive Crowd Teaching with Exponentially Decayed Memory Learners. (arXiv:1804.06481v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.06481</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing demand for large amount of labeled data, crowdsourcing
has been used in many large-scale data mining applications. However, most
existing works in crowdsourcing mainly focus on label inference and incentive
design. In this paper, we address a different problem of adaptive crowd
teaching, which is a sub-area of machine teaching in the context of
crowdsourcing. Compared with machines, human beings are extremely good at
learning a specific target concept (e.g., classifying the images into given
categories) and they can also easily transfer the learned concepts into similar
learning tasks. Therefore, a more effective way of utilizing crowdsourcing is
by supervising the crowd to label in the form of teaching. In order to perform
the teaching and expertise estimation simultaneously, we propose an adaptive
teaching framework named JEDI to construct the personalized optimal teaching
set for the crowdsourcing workers. In JEDI teaching, the teacher assumes that
each learner has an exponentially decayed memory. Furthermore, it ensures
comprehensiveness in the learning process by carefully balancing teaching
diversity and learner&apos;s accurate learning in terms of teaching usefulness.
Finally, we validate the effectiveness and efficacy of JEDI teaching in
comparison with the state-of-the-art techniques on multiple data sets with both
synthetic learners and real crowdsourcing workers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nelakurthi_A/0/1/0/all/0/1&quot;&gt;Arun Reddy Nelakurthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jingrui He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06498">
<title>Deep Multimodal Subspace Clustering Networks. (arXiv:1804.06498v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06498</link>
<description rdf:parseType="Literal">&lt;p&gt;We present convolutional neural network (CNN) based approaches for
unsupervised multimodal subspace clustering. The proposed framework consists of
three main stages - multimodal encoder, self-expressive layer, and multimodal
decoder. The encoder takes multimodal data as input and fuses them to a latent
space representation. We investigate early, late and intermediate fusion
techniques and propose three different encoders corresponding to them for
spatial fusion. The self-expressive layers and multimodal decoders are
essentially the same for different spatial fusion-based approaches. In addition
to various spatial fusion-based methods, an affinity fusion-based network is
also proposed in which the self-expressiveness layer corresponding to different
modalities is enforced to be the same. Extensive experiments on three datasets
show that the proposed methods significantly outperform the state-of-the-art
multimodal subspace clustering methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abavisani_M/0/1/0/all/0/1&quot;&gt;Mahdi Abavisani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1&quot;&gt;Vishal M. Patel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06500">
<title>Two-Player Games for Efficient Non-Convex Constrained Optimization. (arXiv:1804.06500v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06500</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, constrained optimization has become increasingly relevant to
the machine learning community, with applications including Neyman-Pearson
classification, robust optimization, and fair machine learning. A natural
approach to constrained optimization is to optimize the Lagrangian, but this is
not guaranteed to work in the non-convex setting. Instead, we prove that, given
a Bayesian optimization oracle, a modified Lagrangian approach can be used to
find a distribution over no more than m+1 models (where m is the number of
constraints) that is nearly-optimal and nearly-feasible w.r.t. the original
constrained problem. Interestingly, our method can be extended to
non-differentiable--even discontinuous--constraints (where assuming a Bayesian
optimization oracle is not realistic) by viewing constrained optimization as a
non-zero-sum two-player game. The first player minimizes external regret in
terms of easy-to-optimize &quot;proxy constraints&quot;, while the second player enforces
the original constraints by minimizing swap-regret.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotter_A/0/1/0/all/0/1&quot;&gt;Andrew Cotter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Heinrich Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1&quot;&gt;Karthik Sridharan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06673">
<title>Bayesian Metabolic Flux Analysis reveals intracellular flux couplings. (arXiv:1804.06673v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.06673</link>
<description rdf:parseType="Literal">&lt;p&gt;Metabolic flux balance analyses are a standard tool in analysing metabolic
reaction rates compatible with measurements, steady-state and the metabolic
reaction network stoichiometry. Flux analysis methods commonly place
unrealistic assumptions on fluxes due to the convenience of formulating the
problem as a linear programming model, and most methods ignore the notable
uncertainty in flux estimates. We introduce a novel paradigm of Bayesian
metabolic flux analysis that models the reactions of the whole genome-scale
cellular system in probabilistic terms, and can infer the full flux vector
distribution of genome-scale metabolic systems based on exchange and
intracellular (e.g. 13C) flux measurements, steady-state assumptions, and
target function assumptions. The Bayesian model couples all fluxes jointly
together in a simple truncated multivariate posterior distribution, which
reveals informative flux couplings. Our model is a plug-in replacement to
conventional metabolic balance methods, such as flux balance analysis (FBA).
Our experiments indicate that we can characterise the genome-scale flux
covariances, reveal flux couplings, and determine more intracellular unobserved
fluxes in C. acetobutylicum from 13C data than flux variability analysis. The
COBRA compatible software is available at github.com/markusheinonen/bamfa
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Heinonen_M/0/1/0/all/0/1&quot;&gt;Markus Heinonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osmala_M/0/1/0/all/0/1&quot;&gt;Maria Osmala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mannerstrom_H/0/1/0/all/0/1&quot;&gt;Henrik Mannerstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wallenius_J/0/1/0/all/0/1&quot;&gt;Janne Wallenius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kaski_S/0/1/0/all/0/1&quot;&gt;Samuel Kaski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rousu_J/0/1/0/all/0/1&quot;&gt;Juho Rousu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lahdesmaki_H/0/1/0/all/0/1&quot;&gt;Harri L&amp;#xe4;hdesm&amp;#xe4;ki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06679">
<title>Understanding Individual Neuron Importance Using Information Theory. (arXiv:1804.06679v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06679</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we characterize the outputs of individual neurons in a trained
feed-forward neural network by entropy, mutual information with the class
variable, and a class selectivity measure based on Kullback-Leibler divergence.
By cumulatively ablating neurons in the network, we connect these
information-theoretic measures to the impact their removal has on
classification performance on the test set. We observe that, looking at the
neural network as a whole, none of these measures is a good indicator for
classification performance, thus confirming recent results by Morcos et al.
However, looking at specific layers separately, both mutual information and
class selectivity are positively correlated with classification performance. We
thus conclude that it is ill-advised to compare these measures across layers,
and that different layers may be most appropriately characterized by different
measures.
&lt;/p&gt;
&lt;p&gt;We then discuss pruning neurons from neural networks to reduce computational
complexity of inference. Drawing from our results, we perform pruning based on
information-theoretic measures on a fully connected feed-forward neural network
with two hidden layers trained on MNIST dataset and compare the results to a
recently proposed pruning method. We furthermore show that the common practice
of re-training after pruning can partly be obviated by a surgery step called
bias balancing, without incurring significant performance degradation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kairen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amjad_R/0/1/0/all/0/1&quot;&gt;Rana Ali Amjad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_B/0/1/0/all/0/1&quot;&gt;Bernhard C. Geiger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06755">
<title>Exact Distributed Training: Random Forest with Billions of Examples. (arXiv:1804.06755v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06755</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an exact distributed algorithm to train Random Forest models as
well as other decision forest models without relying on approximating best
split search. We explain the proposed algorithm and compare it to related
approaches for various complexity measures (time, ram, disk, and network
complexity analysis). We report its running performances on artificial and
real-world datasets of up to 18 billions examples. This figure is several
orders of magnitude larger than datasets tackled in the existing literature.
Finally, we empirically show that Random Forest benefits from being trained on
more data, even in the case of already gigantic datasets. Given a dataset with
17.3B examples with 82 features (3 numerical, other categorical with high
arity), our implementation trains a tree in 22h.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillame_Bert_M/0/1/0/all/0/1&quot;&gt;Mathieu Guillame-Bert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teytaud_O/0/1/0/all/0/1&quot;&gt;Olivier Teytaud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06776">
<title>Improving Long-Horizon Forecasts with Expectation-Biased LSTM Networks. (arXiv:1804.06776v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.06776</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art forecasting methods using Recurrent Neural Net- works (RNN)
based on Long-Short Term Memory (LSTM) cells have shown exceptional performance
targeting short-horizon forecasts, e.g given a set of predictor features,
forecast a target value for the next few time steps in the future. However, in
many applica- tions, the performance of these methods decays as the forecasting
horizon extends beyond these few time steps. This paper aims to explore the
challenges of long-horizon forecasting using LSTM networks. Here, we illustrate
the long-horizon forecasting problem in datasets from neuroscience and energy
supply management. We then propose expectation-biasing, an approach motivated
by the literature of Dynamic Belief Networks, as a solution to improve
long-horizon forecasting using LSTMs. We propose two LSTM ar- chitectures along
with two methods for expectation biasing that significantly outperforms
standard practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ismail_A/0/1/0/all/0/1&quot;&gt;Aya Abdelsalam Ismail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wood_T/0/1/0/all/0/1&quot;&gt;Timothy Wood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bravo_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe9;ctor Corrada Bravo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06802">
<title>Entropic Spectral Learning in Large Scale Networks. (arXiv:1804.06802v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.06802</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel algorithm for learning the spectral density of large scale
networks using stochastic trace estimation and the method of maximum entropy.
The complexity of the algorithm is linear in the number of non-zero elements of
the matrix, offering a computational advantage over other algorithms. We apply
our algorithm to the problem of community detection in large networks. We show
state-of-the-art performance on both synthetic and real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Granziol_D/0/1/0/all/0/1&quot;&gt;Diego Granziol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ru_B/0/1/0/all/0/1&quot;&gt;Binxin Ru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zohren_S/0/1/0/all/0/1&quot;&gt;Stefan Zohren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiaowen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osborne_M/0/1/0/all/0/1&quot;&gt;Michael Osborne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1604.04706">
<title>DS-MLR: Exploiting Double Separability for Scaling up Distributed Multinomial Logistic Regression. (arXiv:1604.04706v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1604.04706</link>
<description rdf:parseType="Literal">&lt;p&gt;Scaling multinomial logistic regression to datasets with very large number of
data points and classes has not been trivial. This is primarily because one
needs to compute the log-partition function on every data point. This makes
distributing the computation hard. In this paper, we present a distributed
stochastic gradient descent based optimization method (DS-MLR) for scaling up
multinomial logistic regression problems to massive scale datasets without
hitting any storage constraints on the data and model parameters. Our algorithm
exploits double-separability, an attractive property we observe in the
objective functions of several models in machine learning, that allows us to
achieve both data as well as model parallelism simultaneously. In addition to
being parallelizable, our algorithm can also easily be made non-blocking and
asynchronous. We demonstrate the effectiveness of DS-MLR empirically on several
real-world datasets, the largest being a reddit dataset created out of 1.7
billion user comments, where the data and parameter sizes are 228 GB and 358 GB
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_P/0/1/0/all/0/1&quot;&gt;Parameswaran Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1&quot;&gt;Sriram Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsushima_S/0/1/0/all/0/1&quot;&gt;Shin Matsushima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinhua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1&quot;&gt;Hyokun Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwanathan_S/0/1/0/all/0/1&quot;&gt;S.V.N. Vishwanathan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.09499">
<title>Extreme Stochastic Variational Inference: Distributed and Asynchronous. (arXiv:1605.09499v7 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1605.09499</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose extreme stochastic variational inference (ESVI), an asynchronous
and lock-free algorithm to perform variational inference for mixture models on
massive real world datasets. Stochastic variational inference (SVI), the
state-of-the-art algorithm for scaling variational inference to large-datasets,
is inherently serial. Moreover, it requires the parameters to fit in the memory
of a single processor; this is problematic when the number of parameters is in
billions. ESVI overcomes these limitations by requiring that each processor
only access a subset of the data and a subset of the parameters, thus providing
data and model parallelism simultaneously. We demonstrate the effectiveness of
ESVI by running Latent Dirichlet Allocation (LDA) on UMBC-3B, a dataset that
has a vocabulary of 3 million and a token size of 3 billion. To best of our
knowledge, this is an order of magnitude larger than the largest dataset on
which results using variational inference have been reported in literature. In
our experiments, we found that ESVI outperforms VI and SVI, and also achieves a
better quality solution. In addition, we propose a strategy to speed up
computation and save memory when fitting large number of topics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raman_P/0/1/0/all/0/1&quot;&gt;Parameswaran Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shihao Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hsiang-Fu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vishwanathan_S/0/1/0/all/0/1&quot;&gt;S.V.N. Vishwanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhillon_I/0/1/0/all/0/1&quot;&gt;Inderjit S. Dhillon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.00105">
<title>Representation Learning and Pairwise Ranking for Implicit Feedback in Recommendation Systems. (arXiv:1705.00105v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.00105</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel ranking framework for collaborative
filtering with the overall aim of learning user preferences over items by
minimizing a pairwise ranking loss. We show the minimization problem involves
dependent random variables and provide a theoretical analysis by proving the
consistency of the empirical risk minimization in the worst case where all
users choose a minimal number of positive and negative items. We further derive
a Neural-Network model that jointly learns a new representation of users and
items in an embedded space as well as the preference relation of users over the
pairs of items. The learning objective is based on three scenarios of ranking
losses that control the ability of the model to maintain the ordering over the
items induced from the users&apos; preferences, as well as, the capacity of the
dot-product defined in the learned embedded space to produce the ordering. The
proposed model is by nature suitable for implicit feedback and involves the
estimation of only very few parameters. Through extensive experiments on
several real-world benchmarks on implicit data, we show the interest of
learning the preference and the embedding simultaneously when compared to
learning those separately. We also demonstrate that our approach is very
competitive with the best state-of-the-art collaborative filtering techniques
proposed for implicit feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sidana_S/0/1/0/all/0/1&quot;&gt;Sumit Sidana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trofimov_M/0/1/0/all/0/1&quot;&gt;Mikhail Trofimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Horodnitskii_O/0/1/0/all/0/1&quot;&gt;Oleg Horodnitskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laclau_C/0/1/0/all/0/1&quot;&gt;Charlotte Laclau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maximov_Y/0/1/0/all/0/1&quot;&gt;Yury Maximov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Amini_M/0/1/0/all/0/1&quot;&gt;Massih-Reza Amini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06798">
<title>MorphNet: Fast &amp; Simple Resource-Constrained Structure Learning of Deep Networks. (arXiv:1711.06798v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06798</link>
<description rdf:parseType="Literal">&lt;p&gt;We present MorphNet, an approach to automate the design of neural network
structures. MorphNet iteratively shrinks and expands a network, shrinking via a
resource-weighted sparsifying regularizer on activations and expanding via a
uniform multiplicative factor on all layers. In contrast to previous
approaches, our method is scalable to large networks, adaptable to specific
resource constraints (e.g. the number of floating-point operations per
inference), and capable of increasing the network&apos;s performance. When applied
to standard network architectures on a wide variety of datasets, our approach
discovers novel structures in each domain, obtaining higher performance while
respecting the resource constraint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordon_A/0/1/0/all/0/1&quot;&gt;Ariel Gordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eban_E/0/1/0/all/0/1&quot;&gt;Elad Eban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1&quot;&gt;Ofir Nachum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tien-Ju Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Edward Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11057">
<title>On the use of bootstrap with variational inference: Theory, interpretation, and a two-sample test example. (arXiv:1711.11057v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11057</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational inference is a general approach for approximating complex density
functions, such as those arising in latent variable models, popular in machine
learning. It has been applied to approximate the maximum likelihood estimator
and to carry out Bayesian inference, however, quantification of uncertainty
with variational inference remains challenging from both theoretical and
practical perspectives. This paper is concerned with developing uncertainty
measures for variational inference by using bootstrap procedures. We first
develop two general bootstrap approaches for assessing the uncertainty of a
variational estimate and the study the underlying bootstrap theory in both
fixed- and increasing-dimension settings. We then use the bootstrap approach
and our theoretical results in the context of mixed membership modeling with
multivariate binary data on functional disability from the National Long Term
Care Survey. We carry out a two-sample approach to test for changes in the
repeated measures of functional disability for the subset of individuals
present in 1989 and 1994 waves.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yen-Chi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Y. Samuel Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Erosheva_E/0/1/0/all/0/1&quot;&gt;Elena A. Erosheva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03471">
<title>Identifiability of Kronecker-structured Dictionaries for Tensor Data. (arXiv:1712.03471v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.03471</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper derives sufficient conditions for local recovery of coordinate
dictionaries comprising a Kronecker-structured dictionary that is used for
representing $K$th-order tensor data. Tensor observations are assumed to be
generated from a Kronecker-structured dictionary multiplied by sparse
coefficient tensors that follow the separable sparsity model. This work
provides sufficient conditions on the underlying coordinate dictionaries,
coefficient and noise distributions, and number of samples that guarantee
recovery of the individual coordinate dictionaries up to a specified error, as
a local minimum of the objective function, with high probability. In
particular, the sample complexity to recover $K$ coordinate dictionaries with
dimensions $m_k \times p_k$ up to estimation error $\varepsilon_k$ is shown to
be $\max_{k \in [K]}\mathcal{O}(m_kp_k^3\varepsilon_k^{-2})$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shakeri_Z/0/1/0/all/0/1&quot;&gt;Zahra Shakeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sarwate_A/0/1/0/all/0/1&quot;&gt;Anand D. Sarwate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bajwa_W/0/1/0/all/0/1&quot;&gt;Waheed U. Bajwa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03713">
<title>Optimizing Neural Networks in the Equivalence Class Space. (arXiv:1802.03713v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03713</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been widely observed that many activation functions and pooling
methods of neural network models have (positive-) rescaling-invariant property,
including ReLU, PReLU, max-pooling, and average pooling, which makes
fully-connected neural networks (FNNs) and convolutional neural networks (CNNs)
invariant to (positive) rescaling operation across layers. This may cause
unneglectable problems with their optimization: (1) different NN models could
be equivalent, but their gradients can be very different from each other; (2)
it can be proven that the loss functions may have many spurious critical points
in the redundant weight space. To tackle these problems, in this paper, we
first characterize the rescaling-invariant properties of NN models using
equivalence classes and prove that the dimension of the equivalence class space
is significantly smaller than the dimension of the original weight space. Then
we represent the loss function in the compact equivalence class space and
develop novel algorithms that conduct optimization of the NN models directly in
the equivalence class space. We call these algorithms Equivalence Class
Optimization (abbreviated as EC-Opt) algorithms. Moreover, we design efficient
tricks to compute the gradients in the equivalence class, which almost have no
extra computational complexity as compared to standard back-propagation (BP).
We conducted experimental study to demonstrate the effectiveness of our
proposed new optimization algorithms. In particular, we show that by using the
idea of EC-Opt, we can significantly improve the accuracy of the learned model
(for both FNN and CNN), as compared to using conventional stochastic gradient
descent algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meng_Q/0/1/0/all/0/1&quot;&gt;Qi Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shuxin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qiwei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tie-Yan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06352">
<title>High Dimensional Time Series Generators. (arXiv:1804.06352v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06352</link>
<description rdf:parseType="Literal">&lt;p&gt;Multidimensional time series are sequences of real valued vectors. They occur
in different areas, for example handwritten characters, GPS tracking, and
gestures of modern virtual reality motion controllers. Within these areas, a
common task is to search for similar time series. Dynamic Time Warping (DTW) is
a common distance function to compare two time series. The Edit Distance with
Real Penalty (ERP) and the Dog Keeper Distance (DK) are two more distance
functions on time series. Their behaviour has been analyzed on 1-dimensional
time series. However, it is not easy to evaluate their behaviour in relation to
growing dimensionality. For this reason we propose two new data synthesizers
generating multidimensional time series. The first synthesizer extends the well
known cylinder-bell-funnel (CBF) dataset to multidimensional time series. Here,
each time series has an arbitrary type (cylinder, bell, or funnel) in each
dimension, thus for $d$-dimensional time series there are $3^{d}$ different
classes. The second synthesizer (RAM) creates time series with ideas adapted
from Brownian motions which is a common model of movement in physics. Finally,
we evaluate the applicability of a 1-nearest neighbor classifier using DTW on
datasets generated by our synthesizers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachmann_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg P. Bachmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freytag_J/0/1/0/all/0/1&quot;&gt;Johann-Christoph Freytag&lt;/a&gt;</dc:creator>
</item></rdf:RDF>