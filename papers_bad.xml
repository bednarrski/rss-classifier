<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-30T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10694"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10727"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11127"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11129"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11239"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11285"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11313"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01623"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05394"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01208"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10669"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10711"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10817"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10822"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10829"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10850"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10899"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10922"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10960"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10992"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11002"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11022"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11044"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11049"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11192"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07983"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01275"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05707"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07419"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09021"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10601"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10201"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10653"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10690"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10742"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10776"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10801"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10821"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10839"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10942"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10961"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10988"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11005"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11021"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11062"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11132"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11195"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11242"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11271"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11326"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.09466"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.05037"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05790"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.09412"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05869"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00753"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07129"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09383"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08615"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10318"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.10694">
<title>Tiramisu: A Code Optimization Framework for High Performance Systems. (arXiv:1804.10694v1 [cs.PL])</title>
<link>http://arxiv.org/abs/1804.10694</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Tiramisu, an optimization framework designed to
generate efficient code for high-performance systems such as multicores, GPUs,
FPGAs, distributed machines, or any combination of these. Tiramisu relies on a
flexible representation based on the polyhedral model and introduces a novel
four-level IR that allows full separation between algorithms, schedules,
data-layouts and communication. This separation simplifies targeting multiple
hardware architectures from the same algorithm. We evaluate Tiramisu by writing
a set of linear algebra and DNN kernels and by integrating it as a pass in the
Halide compiler. We show that Tiramisu extends Halide with many new
capabilities, and that Tiramisu can generate efficient code for multicores,
GPUs, FPGAs and distributed heterogeneous systems. The performance of code
generated by the Tiramisu backends matches or exceeds hand-optimized reference
implementations. For example, the multicore backend matches the highly
optimized Intel MKL library on many kernels and shows speedups reaching 4x over
the original Halide.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baghdadi_R/0/1/0/all/0/1&quot;&gt;Riyadh Baghdadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_J/0/1/0/all/0/1&quot;&gt;Jessica Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romdhane_M/0/1/0/all/0/1&quot;&gt;Malek Ben Romdhane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sozzo_E/0/1/0/all/0/1&quot;&gt;Emanuele Del Sozzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suriana_P/0/1/0/all/0/1&quot;&gt;Patricia Suriana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamil_S/0/1/0/all/0/1&quot;&gt;Shoaib Kamil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amarasinghe_S/0/1/0/all/0/1&quot;&gt;Saman Amarasinghe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10727">
<title>Low-memory convolutional neural networks through incremental depth-first processing. (arXiv:1804.10727v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.10727</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an incremental processing scheme for convolutional neural
network (CNN) inference, targeted at embedded applications with limited memory
budgets. Instead of processing layers one by one, individual input pixels are
propagated through all parts of the network they can influence under the given
structural constraints. This depth-first updating scheme comes with hard bounds
on the memory footprint: the memory required is constant in the case of 1D
input and proportional to the square root of the input dimension in the case of
2D input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Binas_J/0/1/0/all/0/1&quot;&gt;Jonathan Binas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11127">
<title>Investigations on End-to-End Audiovisual Fusion. (arXiv:1804.11127v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.11127</link>
<description rdf:parseType="Literal">&lt;p&gt;Audiovisual speech recognition (AVSR) is a method to alleviate the adverse
effect of noise in the acoustic signal. Leveraging recent developments in deep
neural network-based speech recognition, we present an AVSR neural network
architecture which is trained end-to-end, without the need to separately model
the process of decision fusion as in conventional (e.g. HMM-based) systems. The
fusion system outperforms single-modality recognition under all noise
conditions. Investigation of the saliency of the input features shows that the
neural network automatically adapts to different noise levels in the acoustic
signal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wand_M/0/1/0/all/0/1&quot;&gt;Michael Wand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1&quot;&gt;Ngoc Thang Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1&quot;&gt;Juergen Schmidhuber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11129">
<title>Optimal Neural Network Feature Selection for Spatial-Temporal Forecasting. (arXiv:1804.11129v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.11129</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we show empirical evidence on how to construct the optimal
feature selection or input representation used by the input layer of a
feedforward neural network for the propose of forecasting spatial-temporal
signals. The approach is based on results from dynamical systems theory, namely
the non-linear embedding theorems. We demonstrate it for a variety of
spatial-temporal signals, with one spatial and one temporal dimensions, and
show that the optimal input layer representation consists of a grid, with
spatial/temporal lags determined by the minimum of the mutual information of
the spatial/temporal signals and the number of points taken in space/time
decided by the embedding dimension of the signal. We present evidence of this
proposal by running a Monte Carlo simulation of several combinations of input
layer feature designs and show that the one predicted by the non-linear
embedding theorems seems to be optimal or close of optimal. In total we show
evidence in four unrelated systems: a series of coupled Henon maps; a series of
couple Ordinary Differential Equations (Lorenz-96) phenomenologically modelling
atmospheric dynamics; the Kuramoto-Sivashinsky equation, a partial differential
equation used in studies of instabilities in laminar flame fronts and finally
real physical data from sunspot areas in the Sun (in latitude and time) from
1874 to 2015.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Covas_E/0/1/0/all/0/1&quot;&gt;Eurico Covas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1&quot;&gt;Emmanouil Benetos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11239">
<title>Structured Weight Matrices-Based Hardware Accelerators in Deep Neural Networks: FPGAs and ASICs. (arXiv:1804.11239v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1804.11239</link>
<description rdf:parseType="Literal">&lt;p&gt;Both industry and academia have extensively investigated hardware
accelerations. In this work, to address the increasing demands in computational
capability and memory requirement, we propose structured weight matrices
(SWM)-based compression techniques for both \emph{field programmable gate
array} (FPGA) and \emph{application-specific integrated circuit} (ASIC)
implementations. In algorithm part, SWM-based framework adopts block-circulant
matrices to achieve a fine-grained tradeoff between accuracy and compression
ratio. The SWM-based technique can reduce computational complexity from
O($n^2$) to O($n\log n$) and storage complexity from O($n^2$) to O($n$) for
each layer and both training and inference phases. For FPGA implementations on
deep convolutional neural networks (DCNNs), we achieve at least 152X and 72X
improvement in performance and energy efficiency, respectively using the
SWM-based framework, compared with the baseline of IBM TrueNorth processor
under same accuracy constraints using the data set of MNIST, SVHN, and
CIFAR-10. For FPGA implementations on long short term memory (LSTM) networks,
the proposed SWM-based LSTM can achieve up to 21X enhancement in performance
and 33.5X gains in energy efficiency compared with the baseline accelerator.
For ASIC implementations, the SWM-based ASIC design exhibits impressive
advantages in terms of power, throughput, and energy efficiency. Experimental
results indicate that this method is greatly suitable for applying DNNs onto
both FPGAs and mobile/IoT devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Caiwen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_A/0/1/0/all/0/1&quot;&gt;Ao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1&quot;&gt;Geng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiayu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1&quot;&gt;Bo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11285">
<title>Adversarially Robust Generalization Requires More Data. (arXiv:1804.11285v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.11285</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models are often susceptible to adversarial perturbations of
their inputs. Even small perturbations can cause state-of-the-art classifiers
with high &quot;standard&quot; accuracy to produce an incorrect prediction with high
confidence. To better understand this phenomenon, we study adversarially robust
learning from the viewpoint of generalization. We show that already in a simple
natural data model, the sample complexity of robust learning can be
significantly larger than that of &quot;standard&quot; learning. This gap is information
theoretic and holds irrespective of the training algorithm or the model family.
We complement our theoretical results with experiments on popular image
classification datasets and show that a similar gap exists here as well. We
postulate that the difficulty of training robust classifiers stems, at least
partially, from this inherently larger sample complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santurkar_S/0/1/0/all/0/1&quot;&gt;Shibani Santurkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsipras_D/0/1/0/all/0/1&quot;&gt;Dimitris Tsipras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1&quot;&gt;Kunal Talwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1&quot;&gt;Aleksander M&amp;#x105;dry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11313">
<title>How Robust are Deep Neural Networks?. (arXiv:1804.11313v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.11313</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional and Recurrent, deep neural networks have been successful in
machine learning systems for computer vision, reinforcement learning, and other
allied fields. However, the robustness of such neural networks is seldom
apprised, especially after high classification accuracy has been attained. In
this paper, we evaluate the robustness of three recurrent neural networks to
tiny perturbations, on three widely used datasets, to argue that high accuracy
does not always mean a stable and a robust (to bounded perturbations,
adversarial attacks, etc.) system. Especially, normalizing the spectrum of the
discrete recurrent network to bound the spectrum (using power method, Rayleigh
quotient, etc.) on a unit disk produces stable, albeit highly non-robust neural
networks. Furthermore, using the $\epsilon$-pseudo-spectrum, we show that
training of recurrent networks, say using gradient-based methods, often result
in non-normal matrices that may or may not be diagonalizable. Therefore, the
open problem lies in constructing methods that optimize not only for accuracy
but also for the stability and the robustness of the underlying neural network,
a criterion that is distinct from the other.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_B/0/1/0/all/0/1&quot;&gt;Biswa Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friston_K/0/1/0/all/0/1&quot;&gt;Karl J. Friston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01623">
<title>RIDDLE: Race and ethnicity Imputation from Disease history with Deep LEarning. (arXiv:1707.01623v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01623</link>
<description rdf:parseType="Literal">&lt;p&gt;Anonymized electronic medical records are an increasingly popular source of
research data. However, these datasets often lack race and ethnicity
information. This creates problems for researchers modeling human disease, as
race and ethnicity are powerful confounders for many health exposures and
treatment outcomes; race and ethnicity are closely linked to
population-specific genetic variation. We showed that deep neural networks
generate more accurate estimates for missing racial and ethnic information than
competing methods (e.g., logistic regression, random forest). RIDDLE yielded
significantly better classification performance across all metrics that were
considered: accuracy, cross-entropy loss (error), and area under the curve for
receiver operating characteristic plots (all $p &amp;lt; 10^{-6}$). We made specific
efforts to interpret the trained neural network models to identify, quantify,
and visualize medical features which are predictive of race and ethnicity. We
used these characterizations of informative features to perform a systematic
comparison of differential disease patterns by race and ethnicity. The fact
that clinical histories are informative for imputing race and ethnicity could
reflect (1) a skewed distribution of blue- and white-collar professions across
racial and ethnic groups, (2) uneven accessibility and subjective importance of
prophylactic health, (3) possible variation in lifestyle, such as dietary
habits, and (4) differences in background genetic variation which predispose to
diseases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Ji-Sung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rzhetsky_A/0/1/0/all/0/1&quot;&gt;Andrey Rzhetsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05394">
<title>A probabilistic and multi-objective analysis of lexicase selection and epsilon-lexicase selection. (arXiv:1709.05394v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05394</link>
<description rdf:parseType="Literal">&lt;p&gt;Lexicase selection is a parent selection method that considers training cases
individually, rather than in aggregate, when performing parent selection.
Whereas previous work has demonstrated the ability of lexicase selection to
solve difficult problems in program synthesis and symbolic regression, the
central goal of this paper is to develop the theoretical underpinnings that
explain its performance. To this end, we derive an analytical formula that
gives the expected probabilities of selection under lexicase selection, given a
population and its behavior. In addition, we expand upon the relation of
lexicase selection to many-objective optimization methods to describe the
behavior of lexicase selection, which is to select individuals on the
boundaries of Pareto fronts in high-dimensional space. We show analytically why
lexicase selection performs more poorly for certain sizes of population and
training cases, and show why it has been shown to perform more poorly in
continuous error spaces. To address this last concern, we propose new variants
of epsilon-lexicase selection, a method that modifies the pass condition in
lexicase selection to allow near-elite individuals to pass cases, thereby
improving selection performance with continuous errors. We show that
epsilon-lexicase outperforms several diversity-maintenance strategies on a
number of real-world and synthetic regression problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cava_W/0/1/0/all/0/1&quot;&gt;William La Cava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Helmuth_T/0/1/0/all/0/1&quot;&gt;Thomas Helmuth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spector_L/0/1/0/all/0/1&quot;&gt;Lee Spector&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1&quot;&gt;Jason H. Moore&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01208">
<title>The Case for Learned Index Structures. (arXiv:1712.01208v3 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01208</link>
<description rdf:parseType="Literal">&lt;p&gt;Indexes are models: a B-Tree-Index can be seen as a model to map a key to the
position of a record within a sorted array, a Hash-Index as a model to map a
key to a position of a record within an unsorted array, and a BitMap-Index as a
model to indicate if a data record exists or not. In this exploratory research
paper, we start from this premise and posit that all existing index structures
can be replaced with other types of models, including deep-learning models,
which we term learned indexes. The key idea is that a model can learn the sort
order or structure of lookup keys and use this signal to effectively predict
the position or existence of records. We theoretically analyze under which
conditions learned indexes outperform traditional index structures and describe
the main challenges in designing learned index structures. Our initial results
show, that by using neural nets we are able to outperform cache-optimized
B-Trees by up to 70% in speed while saving an order-of-magnitude in memory over
several real-world data sets. More importantly though, we believe that the idea
of replacing core components of a data management system through learned models
has far reaching implications for future systems designs and that this work
just provides a glimpse of what might be possible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraska_T/0/1/0/all/0/1&quot;&gt;Tim Kraska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1&quot;&gt;Alex Beutel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1&quot;&gt;Ed H. Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1&quot;&gt;Jeffrey Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polyzotis_N/0/1/0/all/0/1&quot;&gt;Neoklis Polyzotis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10669">
<title>Deep Speech Denoising with Vector Space Projections. (arXiv:1804.10669v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1804.10669</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an algorithm to denoise speakers from a single microphone in the
presence of non-stationary and dynamic noise. Our approach is inspired by the
recent success of neural network models separating speakers from other speakers
and singers from instrumental accompaniment. Unlike prior art, we leverage
embedding spaces produced with source-contrastive estimation, a technique
derived from negative sampling techniques in natural language processing, while
simultaneously obtaining a continuous inference mask. Our embedding space
directly optimizes for the discrimination of speaker and noise by jointly
modeling their characteristics. This space is generalizable in that it is not
speaker or noise specific and is capable of denoising speech even if the model
has not seen the speaker in the training set. Parameters are trained with dual
objectives: one that promotes a selective bandpass filter that eliminates noise
at time-frequency positions that exceed signal power, and another that
proportionally splits time-frequency content between signal and noise. We
compare to state of the art algorithms as well as traditional sparse
non-negative matrix factorization solutions. The resulting algorithm avoids
severe computational burden by providing a more intuitive and easily optimized
approach, while achieving competitive accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hetherly_J/0/1/0/all/0/1&quot;&gt;Jeff Hetherly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gamble_P/0/1/0/all/0/1&quot;&gt;Paul Gamble&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrios_M/0/1/0/all/0/1&quot;&gt;Maria Barrios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stephenson_C/0/1/0/all/0/1&quot;&gt;Cory Stephenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_K/0/1/0/all/0/1&quot;&gt;Karl Ni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10711">
<title>Modified Apriori Graph Algorithm for Frequent Pattern Mining. (arXiv:1804.10711v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.10711</link>
<description rdf:parseType="Literal">&lt;p&gt;Web Usage Mining is an application of Data Mining Techniques to discover
interesting usage patterns from web data in order to understand and better
serve the needs of web-based applications. The paper proposes an algorithm for
finding these usage patterns using a modified version of Apriori Algorithm
called Apriori-Graph. These rules will help service providers to predict, which
web pages, the user is likely to visit next. This will optimize the website in
terms of efficiency, bandwidth and will have positive economic benefits for
them. The proposed Apriori Graph Algorithm O((V)(E)) works faster compared to
the existing Apriori Algorithm and is well suitable for real-time application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuvraj_P/0/1/0/all/0/1&quot;&gt;Pritish Yuvraj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_S/0/1/0/all/0/1&quot;&gt;Suneetha K. R&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10817">
<title>A Logic of Agent Organizations. (arXiv:1804.10817v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.10817</link>
<description rdf:parseType="Literal">&lt;p&gt;Organization concepts and models are increasingly being adopted for the
design and specification of multi-agent systems. Agent organizations can be
seen as mechanisms of social order, created to achieve global (or
organizational) objectives by more or less autonomous agents. In order to
develop a theory on the relation between organizational structures,
organizational objectives and the actions of agents fulfilling roles in the
organization a theoretical framework is needed to describe organizational
structures and actions of (groups of) agents. Current logical formalisms focus
on specific aspects of organizations (e.g. power, delegation, agent actions, or
normative issues) but a framework that integrates and relates different aspects
is missing. Given the amount of aspects involved and the subsequent complexity
of a formalism encompassing them all, it is difficult to realize. In this
paper, a first step is taken to solve this problem. We present a generic formal
model that enables to specify and relate the main concepts of an organization
(including, activity, structure, environment and others) so that organizations
can be analyzed at a high level of abstraction. However, for some aspects we
use a simplified model in order to avoid the complexity of combining many
different types of (modal) operators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dignum_V/0/1/0/all/0/1&quot;&gt;Virginia Dignum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dignum_F/0/1/0/all/0/1&quot;&gt;Frank Dignum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10822">
<title>A Bimodal Learning Approach to Assist Multi-sensory Effects Synchronization. (arXiv:1804.10822v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.10822</link>
<description rdf:parseType="Literal">&lt;p&gt;In mulsemedia applications, traditional media content (text, image, audio,
video, etc.) can be related to media objects that target other human senses
(e.g., smell, haptics, taste). Such applications aim at bridging the virtual
and real worlds through sensors and actuators. Actuators are responsible for
the execution of sensory effects (e.g., wind, heat, light), which produce
sensory stimulations on the users. In these applications sensory stimulation
must happen in a timely manner regarding the other traditional media content
being presented. For example, at the moment in which an explosion is presented
in the audiovisual content, it may be adequate to activate actuators that
produce heat and light. It is common to use some declarative multimedia
authoring language to relate the timestamp in which each media object is to be
presented to the execution of some sensory effect. One problem in this setting
is that the synchronization of media objects and sensory effects is done
manually by the author(s) of the application, a process which is time-consuming
and error prone. In this paper, we present a bimodal neural network
architecture to assist the synchronization task in mulsemedia applications. Our
approach is based on the idea that audio and video signals can be used
simultaneously to identify the timestamps in which some sensory effect should
be executed. Our learning architecture combines audio and video signals for the
prediction of scene components. For evaluation purposes, we construct a dataset
based on Google&apos;s AudioSet. We provide experiments to validate our bimodal
architecture. Our results show that the bimodal approach produces better
results when compared to several variants of unimodal architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abreu_R/0/1/0/all/0/1&quot;&gt;Raphael Abreu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1&quot;&gt;Joel dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bezerra_E/0/1/0/all/0/1&quot;&gt;Eduardo Bezerra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10829">
<title>Formal Security Analysis of Neural Networks using Symbolic Intervals. (arXiv:1804.10829v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.10829</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the increasing deployment of Deep Neural Networks (DNNs) in real-world
security-critical domains including autonomous vehicles and collision avoidance
systems, formally checking security properties of DNNs, especially under
different attacker capabilities, is becoming crucial. Most existing security
testing techniques for DNNs try to find adversarial examples without providing
any formal security guarantees about the non-existence of adversarial examples.
Recently, several projects have used different types of Satisfiability Modulo
Theory (SMT) solvers to formally check security properties of DNNs. However,
all of these approaches are limited by the high overhead caused by the solver.
&lt;/p&gt;
&lt;p&gt;In this paper, we present a new direction for formally checking security
properties of DNNs without using SMT solvers. Instead, we leverage interval
arithmetic to formally check security properties by computing rigorous bounds
on the DNN outputs. Our approach, unlike existing solver-based approaches, is
easily parallelizable. We further present symbolic interval analysis along with
several other optimizations to minimize overestimations.
&lt;/p&gt;
&lt;p&gt;We design, implement, and evaluate our approach as part of ReluVal, a system
for formally checking security properties of Relu-based DNNs. Our extensive
empirical results show that ReluVal outperforms Reluplex, a state-of-the-art
solver-based system, by 200 times on average for the same security properties.
ReluVal is able to prove a security property within 4 hours on a single 8-core
machine without GPUs, while Reluplex deemed inconclusive due to timeout (more
than 5 days). Our experiments demonstrate that symbolic interval analysis is a
promising new direction towards rigorously analyzing different security
properties of DNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_K/0/1/0/all/0/1&quot;&gt;Kexin Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitehouse_J/0/1/0/all/0/1&quot;&gt;Justin Whitehouse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Junfeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1&quot;&gt;Suman Jana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10850">
<title>Drug Similarity Integration Through Attentive Multi-view Graph Auto-Encoders. (arXiv:1804.10850v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.10850</link>
<description rdf:parseType="Literal">&lt;p&gt;Drug similarity has been studied to support downstream clinical tasks such as
inferring novel properties of drugs (e.g. side effects, indications,
interactions) from known properties. The growing availability of new types of
drug features brings the opportunity of learning a more comprehensive and
accurate drug similarity that represents the full spectrum of underlying drug
relations. However, it is challenging to integrate these heterogeneous, noisy,
nonlinear-related information to learn accurate similarity measures especially
when labels are scarce. Moreover, there is a trade-off between accuracy and
interpretability. In this paper, we propose to learn accurate and interpretable
similarity measures from multiple types of drug features. In particular, we
model the integration using multi-view graph auto-encoders, and add attentive
mechanism to determine the weights for each view with respect to corresponding
tasks and features for better interpretability. Our model has flexible design
for both semi-supervised and unsupervised settings. Experimental results
demonstrated significant predictive accuracy improvement. Case studies also
showed better model capacity (e.g. embed node features) and interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengfei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Cao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiayu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10899">
<title>Scalable Angular Discriminative Deep Metric Learning for Face Recognition. (arXiv:1804.10899v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.10899</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of deep learning, Deep Metric Learning (DML) has
achieved great improvements in face recognition. Specifically, the widely used
softmax loss in the training process often bring large intra-class variations,
and feature normalization is only exploited in the testing process to compute
the pair similarities. To bridge the gap, we impose the intra-class cosine
similarity between the features and weight vectors in softmax loss larger than
a margin in the training step, and extend it from four aspects. First, we
explore the effect of a hard sample mining strategy. To alleviate the human
labor of adjusting the margin hyper-parameter, a self-adaptive margin updating
strategy is proposed. Then, a normalized version is given to take full
advantage of the cosine similarity constraint. Furthermore, we enhance the
former constraint to force the intra-class cosine similarity larger than the
mean inter-class cosine similarity with a margin in the exponential feature
projection space. Extensive experiments on Labeled Face in the Wild (LFW),
Youtube Faces (YTF) and IARPA Janus Benchmark A (IJB-A) datasets demonstrate
that the proposed methods outperform the mainstream DML methods and approach
the state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bowen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhangling Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Huaming Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10922">
<title>OPA2Vec: combining formal and informal content of biomedical ontologies to improve similarity-based prediction. (arXiv:1804.10922v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.10922</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivation: Ontologies are widely used in biology for data annotation,
integration, and analysis. In addition to formally structured axioms,
ontologies contain meta-data in the form of annotation axioms which provide
valuable pieces of information that characterize ontology classes. Annotations
commonly used in ontologies include class labels, descriptions, or synonyms.
Despite being a rich source of semantic information, the ontology meta-data are
generally unexploited by ontology-based analysis methods such as semantic
similarity measures. Results: We propose a novel method, OPA2Vec, to generate
vector representations of biological entities in ontologies by combining formal
ontology axioms and annotation axioms from the ontology meta-data. We apply a
Word2Vec model that has been pre-trained on PubMed abstracts to produce feature
vectors from our collected data. We validate our method in two different ways:
first, we use the obtained vector representations of proteins as a similarity
measure to predict protein-protein interaction (PPI) on two different datasets.
Second, we evaluate our method on predicting gene-disease associations based on
phenotype similarity by generating vector representations of genes and diseases
using a phenotype ontology, and applying the obtained vectors to predict
gene-disease associations. These two experiments are just an illustration of
the possible applications of our method. OPA2Vec can be used to produce vector
representations of any biomedical entity given any type of biomedical ontology.
Availability: https://github.com/bio-ontology-research-group/opa2vec Contact:
robert.hoehndorf@kaust.edu.sa and xin.gao@kaust.edu.sa.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smaili_F/0/1/0/all/0/1&quot;&gt;Fatima Zohra Smaili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoehndorf_R/0/1/0/all/0/1&quot;&gt;Robert Hoehndorf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10960">
<title>Generating Interpretable Fuzzy Controllers using Particle Swarm Optimization and Genetic Programming. (arXiv:1804.10960v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.10960</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomously training interpretable control strategies, called policies,
using pre-existing plant trajectory data is of great interest in industrial
applications. Fuzzy controllers have been used in industry for decades as
interpretable and efficient system controllers. In this study, we introduce a
fuzzy genetic programming (GP) approach called fuzzy GP reinforcement learning
(FGPRL) that can select the relevant state features, determine the size of the
required fuzzy rule set, and automatically adjust all the controller parameters
simultaneously. Each GP individual&apos;s fitness is computed using model-based
batch reinforcement learning (RL), which first trains a model using available
system samples and subsequently performs Monte Carlo rollouts to predict each
policy candidate&apos;s performance. We compare FGPRL to an extended version of a
related method called fuzzy particle swarm reinforcement learning (FPSRL),
which uses swarm intelligence to tune the fuzzy policy parameters. Experiments
using an industrial benchmark show that FGPRL is able to autonomously learn
interpretable fuzzy policies with high control performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hein_D/0/1/0/all/0/1&quot;&gt;Daniel Hein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udluft_S/0/1/0/all/0/1&quot;&gt;Steffen Udluft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Runkler_T/0/1/0/all/0/1&quot;&gt;Thomas A. Runkler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10992">
<title>Semi-parametric Image Synthesis. (arXiv:1804.10992v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.10992</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a semi-parametric approach to photographic image synthesis from
semantic layouts. The approach combines the complementary strengths of
parametric and nonparametric techniques. The nonparametric component is a
memory bank of image segments constructed from a training set of images. Given
a novel semantic layout at test time, the memory bank is used to retrieve
photographic references that are provided as source material to a deep network.
The synthesis is performed by a deep network that draws on the provided
photographic material. Experiments on multiple semantic segmentation datasets
show that the presented approach yields considerably more realistic images than
recent purely parametric techniques. The results are shown in the supplementary
video at https://youtu.be/U4Q98lenGLQ
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jiaya Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1&quot;&gt;Vladlen Koltun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11002">
<title>Precision Medicine as an Accelerator for Next Generation Cognitive Supercomputing. (arXiv:1804.11002v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.11002</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past several years, we have taken advantage of a number of
opportunities to advance the intersection of next generation high-performance
computing AI and big data technologies through partnerships in precision
medicine. Today we are in the throes of piecing together what is likely the
most unique convergence of medical data and computer technologies. But more
deeply, we observe that the traditional paradigm of computer simulation and
prediction needs fundamental revision. This is the time for a number of
reasons. We will review what the drivers are, why now, how this has been
approached over the past several years, and where we are heading.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Begoli_E/0/1/0/all/0/1&quot;&gt;Edmon Begoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brase_J/0/1/0/all/0/1&quot;&gt;Jim Brase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeLaRosa_B/0/1/0/all/0/1&quot;&gt;Bambi DeLaRosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_P/0/1/0/all/0/1&quot;&gt;Penelope Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusnezov_D/0/1/0/all/0/1&quot;&gt;Dimitri Kusnezov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paragas_J/0/1/0/all/0/1&quot;&gt;Jason Paragas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stevens_R/0/1/0/all/0/1&quot;&gt;Rick Stevens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Streitz_F/0/1/0/all/0/1&quot;&gt;Fred Streitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tourassi_G/0/1/0/all/0/1&quot;&gt;Georgia Tourassi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11022">
<title>Adversarial Regression for Detecting Attacks in Cyber-Physical Systems. (arXiv:1804.11022v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.11022</link>
<description rdf:parseType="Literal">&lt;p&gt;Attacks in cyber-physical systems (CPS) which manipulate sensor readings can
cause enormous physical damage if undetected. Detection of attacks on sensors
is crucial to mitigate this issue. We study supervised regression as a means to
detect anomalous sensor readings, where each sensor&apos;s measurement is predicted
as a function of other sensors. We show that several common learning approaches
in this context are still vulnerable to \emph{stealthy attacks}, which
carefully modify readings of compromised sensors to cause desired damage while
remaining undetected. Next, we model the interaction between the CPS defender
and attacker as a Stackelberg game in which the defender chooses detection
thresholds, while the attacker deploys a stealthy attack in response. We
present a heuristic algorithm for finding an approximately optimal threshold
for the defender in this game, and show that it increases system resilience to
attacks without significantly increasing the false alarm rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghafouri_A/0/1/0/all/0/1&quot;&gt;Amin Ghafouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1&quot;&gt;Yevgeniy Vorobeychik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koutsoukos_X/0/1/0/all/0/1&quot;&gt;Xenofon Koutsoukos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11044">
<title>Prospects for Declarative Mathematical Modeling of Complex Biological Systems. (arXiv:1804.11044v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1804.11044</link>
<description rdf:parseType="Literal">&lt;p&gt;Declarative modeling uses symbolic expressions to represent models. With such
expressions one can formalize high-level mathematical computations on models
that would be difficult or impossible to perform directly on a lower-level
simulation program, in a general-purpose programming language. Examples of such
computations on models include model analysis, relatively general-purpose
model-reduction maps, and the initial phases of model implementation, all of
which should preserve or approximate the mathematical semantics of a complex
biological model. Multiscale modeling benefits from both the expressive power
of declarative modeling languages and the application of model reduction
methods to link models across scale. Based on previous work, here we define
declarative modeling of complex biological systems by defining the semantics of
an increasingly powerful series of declarative modeling languages including
reaction-like dynamics of parameterized and extended objects, we define
semantics-preserving implementation and semantics-approximating model reduction
transformations, and we outline a &quot;meta-hierarchy&quot; for organizing declarative
models and the mathematical methods that can fruitfully manipulate them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mjolsness_E/0/1/0/all/0/1&quot;&gt;Eric Mjolsness&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11049">
<title>Non-Intrusive Signature Extraction for Major Residential Loads. (arXiv:1804.11049v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1804.11049</link>
<description rdf:parseType="Literal">&lt;p&gt;The data collected by smart meters contain a lot of useful information. One
potential use of the data is to track the energy consumptions and operating
statuses of major home appliances.The results will enable homeowners to make
sound decisions on how to save energy and how to participate in demand response
programs. This paper presents a new method to breakdown the total power demand
measured by a smart meter to those used by individual appliances. A unique
feature of the proposed method is that it utilizes diverse signatures
associated with the entire operating window of an appliance for identification.
As a result, appliances with complicated middle process can be tracked. A novel
appliance registration device and scheme is also proposed to automate the
creation of appliance signature database and to eliminate the need of massive
training before identification. The software and system have been developed and
deployed to real houses in order to verify the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;M. Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meira_P/0/1/0/all/0/1&quot;&gt;P. C. M. Meira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;W. Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chung_C/0/1/0/all/0/1&quot;&gt;C. Y. Chung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11192">
<title>Explainable Recommendation: A Survey and New Perspectives. (arXiv:1804.11192v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1804.11192</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable Recommendation refers to the personalized recommendation
algorithms that address the problem of why -- they not only provide the user
with the recommendations, but also make the user aware why such items are
recommended by generating recommendation explanations, which help to improve
the effectiveness, efficiency, persuasiveness, and user satisfaction of
recommender systems. In recent years, a large number of explainable
recommendation approaches -- especially model-based explainable recommendation
algorithms -- have been proposed and adopted in real-world systems.
&lt;/p&gt;
&lt;p&gt;In this survey, we review the work on explainable recommendation that has
been published in or before the year of 2018. We first high-light the position
of explainable recommendation in recommender system research by categorizing
recommendation problems into the 5W, i.e., what, when, who, where, and why. We
then conduct a comprehensive survey of explainable recommendation itself in
terms of three aspects: 1) We provide a chronological research line of
explanations in recommender systems, including the user study approaches in the
early years, as well as the more recent model-based approaches. 2) We provide a
taxonomy for explainable recommendation algorithms, including user-based,
item-based, model-based, and post-model explanations. 3) We summarize the
application of explainable recommendation in different recommendation tasks,
including product recommendation, social recommendation, POI recommendation,
etc. We devote a chapter to discuss the explanation perspectives in the broader
IR and machine learning settings, as well as their relationship with
explainable recommendation research. We end the survey by discussing potential
future research directions to promote the explainable recommendation research
area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07983">
<title>Safety-Aware Apprenticeship Learning. (arXiv:1710.07983v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07983</link>
<description rdf:parseType="Literal">&lt;p&gt;Apprenticeship learning (AL) is a kind of Learning from Demonstration
techniques where the reward function of a Markov Decision Process (MDP) is
unknown to the learning agent and the agent has to derive a good policy by
observing an expert&apos;s demonstrations. In this paper, we study the problem of
how to make AL algorithms inherently safe while still meeting its learning
objective. We consider a setting where the unknown reward function is assumed
to be a linear combination of a set of state features, and the safety property
is specified in Probabilistic Computation Tree Logic (PCTL). By embedding
probabilistic model checking inside AL, we propose a novel
counterexample-guided approach that can ensure safety while retaining
performance of the learnt policy. We demonstrate the effectiveness of our
approach on several challenging AL scenarios where safety is essential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Weichao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenchao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01275">
<title>A Deeper Look at Experience Replay. (arXiv:1712.01275v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01275</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently experience replay is widely used in various deep reinforcement
learning (RL) algorithms, in this paper we rethink the utility of experience
replay. It introduces a new hyper-parameter, the memory buffer size, which
needs carefully tuning. However unfortunately the importance of this new
hyper-parameter has been underestimated in the community for a long time. In
this paper we did a systematic empirical study of experience replay under
various function representations. We showcase that a large replay buffer can
significantly hurt the performance. Moreover, we propose a simple O(1) method
to remedy the negative influence of a large replay buffer. We showcase its
utility in both simple grid world and challenging domains like Atari games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shangtong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1&quot;&gt;Richard S. Sutton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05707">
<title>Quantum dynamical mode (QDM): A possible extension of belief function. (arXiv:1801.05707v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.05707</link>
<description rdf:parseType="Literal">&lt;p&gt;Dempster-Shafer evidence theory has been widely used in various fields of
applications, because of the flexibility and effectiveness in modeling
uncertainties without prior information. Besides, it has been proven that the
quantum theory has powerful capabilities of solving the decision making
problems, especially for modelling human decision and cognition. However, the
classical Dempster-Shafer evidence theory modelled by real numbers cannot be
integrated directly with the quantum theory modelled by complex numbers. So,
how can we establish a bridge of communications between the classical
Dempster-Shafer evidence theory and the quantum theory? To answer this
question, a generalized Dempster-Shafer evidence theory is proposed in this
paper. The main contribution in this study is that, unlike the existing
evidence theory, a mass function in the generalized Dempster-Shafer evidence
theory is modelled by a complex number, called as a complex mass function. In
addition, compared with the classical Dempster&apos;s combination rule, the
condition in terms of the conflict coefficient between two evidences K &amp;lt; 1 is
released in the generalized Dempster&apos;s combination rule so that it is more
general and applicable than the classical Dempster&apos;s combination rule. When the
complex mass function is degenerated from complex numbers to real numbers, the
generalized Dempster&apos;s combination rule degenerates to the classical evidence
theory under the condition that the conflict coefficient between the evidences
K is less than 1. Numerical examples are illustrated to show the efficiency of
the generalized Dempster-Shafer evidence theory. Finally, an application of an
evidential quantum dynamical model is implemented by integrating the
generalized Dempster-Shafer evidence theory with the quantum dynamical model.
From the experimental results, it validates the feasibility and effectiveness
of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1&quot;&gt;Fuyuan Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03048">
<title>Clustrophile 2: Guided Visual Clustering Analysis. (arXiv:1804.03048v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/1804.03048</link>
<description rdf:parseType="Literal">&lt;p&gt;Data clustering is a common unsupervised learning method frequently used in
exploratory data analysis. However, identifying relevant structures in
unlabeled, high-dimensional data is nontrivial, requiring iterative
experimentation with clustering parameters as well as data features and
instances. The space of possible clusterings for a typical dataset is vast, and
navigating in this vast space is also challenging. The absence of ground-truth
labels makes it impossible to define an optimal solution, thus requiring user
judgment to establish what can be considered a satisfiable clustering result.
Data scientists need adequate interactive tools to effectively explore and
navigate the large space of clusterings so as to improve the effectiveness of
exploratory clustering analysis. We introduce \textit{Clustrophile 2}, a new
interactive tool for guided clustering analysis. \textit{Clustrophile 2} guides
users in clustering-based exploratory analysis, adapts user feedback to improve
user guidance, facilitates the interpretation of clusters, and helps quickly
reason about differences between clusterings. To this end, \textit{Clustrophile
2} contributes a novel feature, the clustering tour, to help users choose
clustering parameters and assess the quality of different clustering results in
relation to current analysis goals and user expectations. We evaluate
\textit{Clustrophile 2} through a user study with 12 data scientists, who used
our tool to explore and interpret sub-cohorts in a dataset of Parkinson&apos;s
disease patients. Results suggest that \textit{Clustrophile 2} improves the
speed and effectiveness of exploratory clustering analysis for both experts and
non-experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavallo_M/0/1/0/all/0/1&quot;&gt;Marco Cavallo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1&quot;&gt;&amp;#xc7;a&amp;#x11f;atay Demiralp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07419">
<title>An Ensemble Generation Method Based on Instance Hardness. (arXiv:1804.07419v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07419</link>
<description rdf:parseType="Literal">&lt;p&gt;In Machine Learning, ensemble methods have been receiving a great deal of
attention. Techniques such as Bagging and Boosting have been successfully
applied to a variety of problems. Nevertheless, such techniques are still
susceptible to the effects of noise and outliers in the training data. We
propose a new method for the generation of pools of classifiers based on
Bagging, in which the probability of an instance being selected during the
resampling process is inversely proportional to its instance hardness, which
can be understood as the likelihood of an instance being misclassified,
regardless of the choice of classifier. The goal of the proposed method is to
remove noisy data without sacrificing the hard instances which are likely to be
found on class boundaries. We evaluate the performance of the method in
nineteen public data sets, and compare it to the performance of the Bagging and
Random Subspace algorithms. Our experiments show that in high noise scenarios
the accuracy of our method is significantly better than that of Bagging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walmsley_F/0/1/0/all/0/1&quot;&gt;Felipe N. Walmsley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavalcanti_G/0/1/0/all/0/1&quot;&gt;George D. C. Cavalcanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1&quot;&gt;Dayvid V. R. Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_R/0/1/0/all/0/1&quot;&gt;Rafael M. O. Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabourin_R/0/1/0/all/0/1&quot;&gt;Robert Sabourin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09021">
<title>Label-aware Double Transfer Learning for Cross-Specialty Medical Named Entity Recognition. (arXiv:1804.09021v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.09021</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of named entity recognition (NER) from electronic
medical records, which is one of the most fundamental and critical problems for
medical text mining. Medical records which are written by clinicians from
different specialties usually contain quite different terminologies and writing
styles. The difference of specialties and the cost of human annotation makes it
particularly difficult to train a universal medical NER system. In this paper,
we propose a label-aware double transfer learning framework (La-DTL) for
cross-specialty NER, so that a medical NER system designed for one specialty
could be conveniently applied to another one with minimal annotation efforts.
The transferability is guaranteed by two components: (i) we propose label-aware
MMD for feature representation transfer, and (ii) we perform parameter transfer
with a theoretical upper bound which is also label aware. We conduct extensive
experiments on 12 cross-specialty NER tasks. The experimental results
demonstrate that La-DTL provides consistent accuracy improvement over strong
baselines. Besides, the promising experimental results on non-medical NER
scenarios indicate that La-DTL is potential to be seamlessly adapted to a wide
range of NER tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenghui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1&quot;&gt;Yanru Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jian Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaodian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yimei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1&quot;&gt;Gen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ken Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10601">
<title>Expectation Optimization with Probabilistic Guarantees in POMDPs with Discounted-sum Objectives. (arXiv:1804.10601v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10601</link>
<description rdf:parseType="Literal">&lt;p&gt;Partially-observable Markov decision processes (POMDPs) with discounted-sum
payoff are a standard framework to model a wide range of problems related to
decision making under uncertainty. Traditionally, the goal has been to obtain
policies that optimize the expectation of the discounted-sum payoff. A key
drawback of the expectation measure is that even low probability events with
extreme payoff can significantly affect the expectation, and thus the obtained
policies are not necessarily risk-averse. An alternate approach is to optimize
the probability that the payoff is above a certain threshold, which allows
obtaining risk-averse policies, but ignores optimization of the expectation. We
consider the expectation optimization with probabilistic guarantee (EOPG)
problem, where the goal is to optimize the expectation ensuring that the payoff
is above a given threshold with at least a specified probability. We present
several results on the EOPG problem, including the first algorithm to solve it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_K/0/1/0/all/0/1&quot;&gt;Krishnendu Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elgyutt_A/0/1/0/all/0/1&quot;&gt;Adri&amp;#xe1;n Elgy&amp;#xfc;tt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novotny_P/0/1/0/all/0/1&quot;&gt;Petr Novotn&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rouille_O/0/1/0/all/0/1&quot;&gt;Owen Rouill&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10201">
<title>The Intelligent ICU Pilot Study: Using Artificial Intelligence Technology for Autonomous Patient Monitoring. (arXiv:1804.10201v1 [cs.HC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1804.10201</link>
<description rdf:parseType="Literal">&lt;p&gt;Currently, many critical care indices are repetitively assessed and recorded
by overburdened nurses, e.g. physical function or facial pain expressions of
nonverbal patients. In addition, many essential information on patients and
their environment are not captured at all, or are captured in a non-granular
manner, e.g. sleep disturbance factors such as bright light, loud background
noise, or excessive visitations. In this pilot study, we examined the
feasibility of using pervasive sensing technology and artificial intelligence
for autonomous and granular monitoring of critically ill patients and their
environment in the Intensive Care Unit (ICU). As an exemplar prevalent
condition, we also characterized delirious and non-delirious patients and their
environment. We used wearable sensors, light and sound sensors, and a
high-resolution camera to collected data on patients and their environment. We
analyzed collected data using deep learning and statistical analysis. Our
system performed face detection, face recognition, facial action unit
detection, head pose detection, facial expression recognition, posture
recognition, actigraphy analysis, sound pressure and light level detection, and
visitation frequency detection. We were able to detect patient&apos;s face (Mean
average precision (mAP)=0.94), recognize patient&apos;s face (mAP=0.80), and their
postures (F1=0.94). We also found that all facial expressions, 11 activity
features, visitation frequency during the day, visitation frequency during the
night, light levels, and sound pressure levels during the night were
significantly different between delirious and non-delirious patients
(p-value&amp;lt;0.05). In summary, we showed that granular and autonomous monitoring
of critically ill patients and their environment is feasible and can be used
for characterizing critical care conditions and related environment factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davoudi_A/0/1/0/all/0/1&quot;&gt;Anis Davoudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malhotra_K/0/1/0/all/0/1&quot;&gt;Kumar Rohit Malhotra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shickel_B/0/1/0/all/0/1&quot;&gt;Benjamin Shickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegel_S/0/1/0/all/0/1&quot;&gt;Scott Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_S/0/1/0/all/0/1&quot;&gt;Seth Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruppert_M/0/1/0/all/0/1&quot;&gt;Matthew Ruppert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bihorac_E/0/1/0/all/0/1&quot;&gt;Emel Bihorac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozrazgat_Baslanti_T/0/1/0/all/0/1&quot;&gt;Tezcan Ozrazgat-Baslanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tighe_P/0/1/0/all/0/1&quot;&gt;Patrick J. Tighe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bihorac_A/0/1/0/all/0/1&quot;&gt;Azra Bihorac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashidi_P/0/1/0/all/0/1&quot;&gt;Parisa Rashidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10653">
<title>Sparse Group Inductive Matrix Completion. (arXiv:1804.10653v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.10653</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of inductive matrix completion under the assumption
that many features are non-informative, which leads to row- and column-sparse
structure of coefficient matrix. Under the additional assumption on the low
rank of coefficient matrix we propose the matrix factorization framework with
group-lasso regularization on parameter matrices. We suggest efficient
optimization algorithm for the solution of the obtained problem. From
theoretical point of view, we prove the oracle generalization bound on the
expected error of matrix completion. Corresponding sample complexity bounds
show the benefits of the proposed approach compared to competitors in the
sparse problems. The experiments on synthetic and real-world datasets show the
state-of-the-art efficiency of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nazarov_I/0/1/0/all/0/1&quot;&gt;Ivan Nazarov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shirokikh_B/0/1/0/all/0/1&quot;&gt;Boris Shirokikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Burkina_M/0/1/0/all/0/1&quot;&gt;Maria Burkina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fedonin_G/0/1/0/all/0/1&quot;&gt;Gennady Fedonin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Panov_M/0/1/0/all/0/1&quot;&gt;Maxim Panov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10690">
<title>Negative Log Likelihood Ratio Loss for Deep Neural Network Classification. (arXiv:1804.10690v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.10690</link>
<description rdf:parseType="Literal">&lt;p&gt;In deep neural network, the cross-entropy loss function is commonly used for
classification. Minimizing cross-entropy is equivalent to maximizing likelihood
under assumptions of uniform feature and class distributions. It belongs to
generative training criteria which does not directly discriminate correct class
from competing classes. We propose a discriminative loss function with negative
log likelihood ratio between correct and competing classes. It significantly
outperforms the cross-entropy loss on the CIFAR-10 image classification task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Donglai Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Hengshuai Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Peng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10742">
<title>Novel Prediction Techniques Based on Clusterwise Linear Regression. (arXiv:1804.10742v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.10742</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we explore different regression models based on Clusterwise
Linear Regression (CLR). CLR aims to find the partition of the data into $k$
clusters, such that linear regressions fitted to each of the clusters minimize
overall mean squared error on the whole data. The main obstacle preventing to
use found regression models for prediction on the unseen test points is the
absence of a reasonable way to obtain CLR cluster labels when the values of
target variable are unknown. In this paper we propose two novel approaches on
how to solve this problem. The first approach, predictive CLR builds a separate
classification model to predict test CLR labels. The second approach,
constrained CLR utilizes a set of user-specified constraints that enforce
certain points to go to the same clusters. Assuming the constraint values are
known for the test points, they can be directly used to assign CLR labels. We
evaluate these two approaches on three UCI ML datasets as well as on a large
corpus of health insurance claims. We show that both of the proposed algorithms
significantly improve over the known CLR-based regression methods. Moreover,
predictive CLR consistently outperforms linear regression and random forest,
and shows comparable performance to support vector regression on UCI ML
datasets. The constrained CLR approach achieves the best performance on the
health insurance dataset, while enjoying only $\approx 20$ times increased
computational time over linear regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gitman_I/0/1/0/all/0/1&quot;&gt;Igor Gitman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jieshi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_E/0/1/0/all/0/1&quot;&gt;Eric Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1&quot;&gt;Artur Dubrawski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10776">
<title>Multi Layered-Parallel Graph Convolutional Network (ML-PGCN) for Disease Prediction. (arXiv:1804.10776v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.10776</link>
<description rdf:parseType="Literal">&lt;p&gt;Structural data from Electronic Health Records as complementary information
to imaging data for disease prediction. We incorporate novel weighting layer
into the Graph Convolutional Networks, which weights every element of
structural data by exploring its relation to the underlying disease. We
demonstrate the superiority of our developed technique in terms of
computational speed and obtained encouraging results where our method
outperforms the state-of-the-art methods when applied to two publicly available
datasets ABIDE and Chest X-ray in terms of relative performance for the
accuracy of prediction by 5.31 % and 8.15 % and for the area under the ROC
curve by 4.96 % and 10.36 % respectively. Additionally, the model is
lightweight, fast and easily trainable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazi_A/0/1/0/all/0/1&quot;&gt;Anees Kazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albarqouni_S/0/1/0/all/0/1&quot;&gt;Shadi Albarqouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kortuem_K/0/1/0/all/0/1&quot;&gt;Karsten Kortuem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10801">
<title>A Cost-Sensitive Deep Belief Network for Imbalanced Classification. (arXiv:1804.10801v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.10801</link>
<description rdf:parseType="Literal">&lt;p&gt;Imbalanced data with a skewed class distribution are common in many
real-world applications. Deep Belief Network (DBN) is a machine learning
technique that is effective in classification tasks. However, conventional DBN
does not work well for imbalanced data classification because it assumes equal
costs for each class. To deal with this problem, cost-sensitive approaches
assign different misclassification costs for different classes without
disrupting the true data sample distributions. However, due to lack of prior
knowledge, the misclassification costs are usually unknown and hard to choose
in practice. Moreover, it has not been well studied as to how cost-sensitive
learning could improve DBN performance on imbalanced data problems. This paper
proposes an evolutionary cost-sensitive deep belief network (ECS-DBN) for
imbalanced classification. ECS-DBN uses adaptive differential evolution to
optimize the misclassification costs based on training data, that presents an
effective approach to incorporating the evaluation measure (i.e. G-mean) into
the objective function. We first optimize the misclassification costs, then
apply them to deep belief network. Adaptive differential evolution optimization
is implemented as the optimization algorithm that automatically updates its
corresponding parameters without the need of prior domain knowledge. The
experiments have shown that the proposed approach consistently outperforms the
state-of-the-art on both benchmark datasets and real-world dataset for fault
diagnosis in tool condition monitoring.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kay Chen Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haizhou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_G/0/1/0/all/0/1&quot;&gt;Geok Soon Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10821">
<title>On Convergence of Moments for Approximating Processes and Applications to Surrogate Models. (arXiv:1804.10821v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.10821</link>
<description rdf:parseType="Literal">&lt;p&gt;We study critera for a pair $ (\{ X_n \} $, $ \{ Y_n \}) $ of approximating
processes which guarantee closeness of moments by generalizing known results
for the special case that $ Y_n = Y $ for all $n$ and $ X_n $ converges to $Y$
in probability. This problem especially arises when working with surrogate
models, e.g. to enrich observed data by simulated data, where the surrogates
$Y_n$&apos;s are constructed to justify that they approximate the $ X_n $&apos;s.
&lt;/p&gt;
&lt;p&gt;The results of this paper deal with sequences of random variables. Since this
framework does not cover many applications where surrogate models such as deep
neural networks are used to approximate more general stochastic processes, we
extend the results to the more general framework of random fields of stochastic
processes. This framework especially covers image data and sequences of images.
We show that uniform integrability is sufficient, and this holds even for the
case of processes provided they satisfy a weak stationarity condition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Steland_A/0/1/0/all/0/1&quot;&gt;Ansgar Steland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10834">
<title>A Unified Framework for Domain Adaptation using Metric Learning on Manifolds. (arXiv:1804.10834v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.10834</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel framework for domain adaptation, whereby both geometric
and statistical differences between a labeled source domain and unlabeled
target domain can be integrated by exploiting the curved Riemannian geometry of
statistical manifolds. Our approach is based on formulating transfer from
source to target as a problem of geometric mean metric learning on manifolds.
Specifically, we exploit the curved Riemannian manifold geometry of symmetric
positive definite (SPD) covariance matrices. We exploit a simple but important
observation that as the space of covariance matrices is both a Riemannian space
as well as a homogeneous space, the shortest path geodesic between two
covariances on the manifold can be computed analytically. Statistics on the SPD
matrix manifold, such as the geometric mean of two matrices can be reduced to
solving the well-known Riccati equation. We show how the Ricatti-based solution
can be constrained to not only reduce the statistical differences between the
source and target domains, such as aligning second order covariances and
minimizing the maximum mean discrepancy, but also the underlying geometry of
the source and target domains using diffusions on the underlying source and
target manifolds. A key strength of our proposed approach is that it enables
integrating multiple sources of variation between source and target in a
unified way, by reducing the combined objective function to a nested set of
Ricatti equations where the solution can be represented by a cascaded series of
geometric mean computations. In addition to showing the theoretical optimality
of our solution, we present detailed experiments using standard transfer
learning testbeds from computer vision comparing our proposed algorithms to
past work in domain adaptation, showing improved results over a large variety
of previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahadevan_S/0/1/0/all/0/1&quot;&gt;Sridhar Mahadevan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bamdev Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shalini Ghosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10839">
<title>Learning from multivariate discrete sequential data using a restricted Boltzmann machine model. (arXiv:1804.10839v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.10839</link>
<description rdf:parseType="Literal">&lt;p&gt;A restricted Boltzmann machine (RBM) is a generative neural-network model
with many novel applications such as collaborative filtering and acoustic
modeling. An RBM lacks the capacity to retain memory, making it inappropriate
for dynamic data modeling as in time-series analysis. In this paper we address
this issue by proposing the p-RBM model, a generalization of the regular RBM
model, capable of retaining memory of p past states. We further show how to
train the p-RBM model using contrastive divergence and test our model on the
problem of predicting the stock market direction considering 100 stocks of the
NASDAQ-100 index. Obtained results show that the p-RBM offer promising
prediction potential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hernandez_J/0/1/0/all/0/1&quot;&gt;Jefferson Hernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abad_A/0/1/0/all/0/1&quot;&gt;Andres G. Abad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10942">
<title>Learning Data Dependency with Communication Cost. (arXiv:1804.10942v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.10942</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the problem of recovering a graph that represents
the statistical data dependency among nodes for a set of data samples generated
by nodes, which provides the basic structure to perform an inference task, such
as MAP (maximum a posteriori). This problem is referred to as structure
learning. When nodes are spatially separated in different locations, running an
inference algorithm requires a non-negligible amount of message passing,
incurring some communication cost. We inevitably have the trade-off between the
accuracy of structure learning and the cost we need to pay to perform a given
message-passing based inference task because the learnt edge structures of data
dependency and physical connectivity graph are often highly different. In this
paper, we formalize this trade-off in an optimization problem which outputs the
data dependency graph that jointly considers learning accuracy and
message-passing costs. We focus on a distributed MAP as the target inference
task, and consider two different implementations, ASYNC-MAP and SYNC-MAP that
have different message-passing mechanisms and thus different cost structures.
In ASYNC- MAP, we propose a polynomial time learning algorithm that is optimal,
motivated by the problem of finding a maximum weight spanning tree. In
SYNC-MAP, we first prove that it is NP-hard and propose a greedy heuristic. For
both implementations, we then quantify how the probability that the resulting
data graphs from those learning algorithms differ from the ideal data graph
decays as the number of data samples grows, using the large deviation
principle, where the decaying rate is characterized by some topological
structures of both original data dependency and physical connectivity graphs as
well as the degree of the trade-off. We validate our theoretical findings
through extensive simulations, which confirms that it has a good match.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jang_H/0/1/0/all/0/1&quot;&gt;Hyeryung Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;HyungSeok Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yi_Y/0/1/0/all/0/1&quot;&gt;Yung Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10961">
<title>Simultaneous Parameter Learning and Bi-Clustering for Multi-Response Models. (arXiv:1804.10961v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.10961</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider multi-response and multitask regression models, where the
parameter matrix to be estimated is expected to have an unknown grouping
structure. The groupings can be along tasks, or features, or both, the last one
indicating a bi-cluster or &quot;checkerboard&quot; structure. Discovering this grouping
structure along with parameter inference makes sense in several applications,
such as multi-response Genome-Wide Association Studies. This additional
structure can not only can be leveraged for more accurate parameter estimation,
but it also provides valuable information on the underlying data mechanisms
(e.g. relationships among genotypes and phenotypes in GWAS). In this paper, we
propose two formulations to simultaneously learn the parameter matrix and its
group structures, based on convex regularization penalties. We present
optimization approaches to solve the resulting problems and provide numerical
convergence guarantees. Our approaches are validated on extensive simulations
and real datasets concerning phenotypes and genotypes of plant varieties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Ming Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramamurthy_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thompson_A/0/1/0/all/0/1&quot;&gt;Addie Thompson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lozano_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lie Lozano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10988">
<title>SHADE: Information-Based Regularization for Deep Learning. (arXiv:1804.10988v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.10988</link>
<description rdf:parseType="Literal">&lt;p&gt;Regularization is a big issue for training deep neural networks. In this
paper, we propose a new information-theory-based regularization scheme named
SHADE for SHAnnon DEcay. The originality of the approach is to define a prior
based on conditional entropy, which explicitly decouples the learning of
invariant representations in the regularizer and the learning of correlations
between inputs and labels in the data fitting term. Our second contribution is
to derive a stochastic version of the regularizer compatible with deep
learning, resulting in a tractable training scheme. We empirically validate the
efficiency of our approach to improve classification performances compared to
common regularization schemes on several standard architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blot_M/0/1/0/all/0/1&quot;&gt;Michael Blot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Robert_T/0/1/0/all/0/1&quot;&gt;Thomas Robert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thome_N/0/1/0/all/0/1&quot;&gt;Nicolas Thome&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cord_M/0/1/0/all/0/1&quot;&gt;Matthieu Cord&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11005">
<title>Building Models for Biopathway Dynamics Using Intrinsic Dimensionality Analysis. (arXiv:1804.11005v1 [q-bio.MN])</title>
<link>http://arxiv.org/abs/1804.11005</link>
<description rdf:parseType="Literal">&lt;p&gt;An important task for many if not all the scientific domains is efficient
knowledge integration, testing and codification. It is often solved with model
construction in a controllable computational environment. In spite of that, the
throughput of in-silico simulation-based observations become similarly
intractable for thorough analysis. This is especially the case in molecular
biology, which served as a subject for this study. In this project, we aimed to
test some approaches developed to deal with the curse of dimensionality. Among
these we found dimension reduction techniques especially appealing. They can be
used to identify irrelevant variability and help to understand critical
processes underlying high-dimensional datasets. Additionally, we subjected our
data sets to nonlinear time series analysis, as those are well established
methods for results comparison. To investigate the usefulness of dimension
reduction methods, we decided to base our study on a concrete sample set. The
example was taken from the domain of systems biology concerning dynamic
evolution of sub-cellular signaling. Particularly, the dataset relates to the
yeast pheromone pathway and is studied in-silico with a stochastic model. The
model reconstructs signal propagation stimulated by a mating pheromone. In the
paper, we elaborate on the reason of multidimensional analysis problem in the
context of molecular signaling, and next, we introduce the model of choice,
simulation details and obtained time series dynamics. A description of used
methods followed by a discussion of results and their biological interpretation
finalize the paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wysocka_E/0/1/0/all/0/1&quot;&gt;Emilia M. Wysocka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dzutsev_V/0/1/0/all/0/1&quot;&gt;Valeriy Dzutsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bandyopadhyay_T/0/1/0/all/0/1&quot;&gt;Tirthankar Bandyopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Condon_L/0/1/0/all/0/1&quot;&gt;Laura Condon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Sahil Garg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11021">
<title>On the Effect of Suboptimal Estimation of Mutual Information in Feature Selection and Classification. (arXiv:1804.11021v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.11021</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new property of estimators of the strength of
statistical association, which helps characterize how well an estimator will
perform in scenarios where dependencies between continuous and discrete random
variables need to be rank ordered. The new property, termed the estimator
response curve, is easily computable and provides a marginal distribution
agnostic way to assess an estimator&apos;s performance. It overcomes notable
drawbacks of current metrics of assessment, including statistical power, bias,
and consistency. We utilize the estimator response curve to test various
measures of the strength of association that satisfy the data processing
inequality (DPI), and show that the CIM estimator&apos;s performance compares
favorably to kNN, vME, AP, and H_{MI} estimators of mutual information. The
estimators which were identified to be suboptimal, according to the estimator
response curve, perform worse than the more optimal estimators when tested with
real-world data from four different areas of science, all with varying
dimensionalities and sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karra_K/0/1/0/all/0/1&quot;&gt;Kiran Karra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mili_L/0/1/0/all/0/1&quot;&gt;Lamine Mili&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11062">
<title>Equivalent Lipschitz surrogates for zero-norm and rank optimization problems. (arXiv:1804.11062v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.11062</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a mechanism to produce equivalent Lipschitz surrogates
for zero-norm and rank optimization problems by means of the global exact
penalty for their equivalent mathematical programs with an equilibrium
constraint (MPECs). Specifically, we reformulate these combinatorial problems
as equivalent MPECs by the variational characterization of the zero-norm and
rank function, show that their penalized problems, yielded by moving the
equilibrium constraint into the objective, are the global exact penalization,
and obtain the equivalent Lipschitz surrogates by eliminating the dual variable
in the global exact penalty. These surrogates, including the popular SCAD
function in statistics, are also difference of two convex functions (D.C.) if
the function and constraint set involved in zero-norm and rank optimization
problems are convex. We illustrate an application by designing a multi-stage
convex relaxation approach to the rank plus zero-norm regularized minimization
problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yulan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bi_S/0/1/0/all/0/1&quot;&gt;Shujun Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shaohua Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11132">
<title>Hyperspectral unmixing with spectral variability using adaptive bundles and double sparsity. (arXiv:1804.11132v1 [eess.IV])</title>
<link>http://arxiv.org/abs/1804.11132</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral variability is one of the major issue when conducting hyperspectral
unmixing. Within a given image composed of some elementary materials (herein
referred to as endmember classes), the spectral signature characterizing these
classes may spatially vary due to intrinsic component fluctuations or external
factors (illumination). These redundant multiple endmember spectra within each
class adversely affect the performance of unmixing methods. This paper proposes
a mixing model that explicitly incorporates a hierarchical structure of
redundant multiple spectra representing each class. The proposed method is
designed to promote sparsity on the selection of both spectra and classes
within each pixel. The resulting unmixing algorithm is able to adaptively
recover several bundles of endmember spectra associated with each class and
robustly estimate abundances. In addition, its flexibility allows a variable
number of classes to be present within each pixel of the hyperspectral image to
be unmixed. The proposed method is compared with other state-of-the-art
unmixing methods that incorporate sparsity using both simulated and real
hyperspectral data. The results show that the proposed method can successfully
determine the variable number of classes present within each class and estimate
the corresponding class abundances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uezato_T/0/1/0/all/0/1&quot;&gt;Tatsumi Uezato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fauvel_M/0/1/0/all/0/1&quot;&gt;Mathieu Fauvel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dobigeon_N/0/1/0/all/0/1&quot;&gt;Nicolas Dobigeon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11195">
<title>Revealing patterns in HIV viral load data and classifying patients via a novel machine learning cluster summarization method. (arXiv:1804.11195v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1804.11195</link>
<description rdf:parseType="Literal">&lt;p&gt;HIV RNA viral load (VL) is an important outcome variable in studies of HIV
infected persons. There exists only a handful of methods which classify
patients by viral load patterns. Most methods place limits on the use of viral
load measurements, are often specific to a particular study design, and do not
account for complex, temporal variation. To address this issue, we propose a
set of four unambiguous computable characteristics (features) of time-varying
HIV viral load patterns, along with a novel centroid-based classification
algorithm, which we use to classify a population of 1,576 HIV positive clinic
patients into one of five different viral load patterns (clusters) often found
in the literature: durably suppressed viral load (DSVL), sustained low viral
load (SLVL), sustained high viral load (SHVL), high viral load suppression
(HVLS), and rebounding viral load (RVL). The centroid algorithm summarizes
these clusters in terms of their centroids and radii. We show that this allows
new viral load patterns to be assigned pattern membership based on the distance
from the centroid relative to its radius, which we term radial normalization
classification. This method has the benefit of providing an objective and
quantitative method to assign viral load pattern membership with a concise and
interpretable model that aids clinical decision making. This method also
facilitates meta-analyses by providing computably distinct HIV categories.
Finally we propose that this novel centroid algorithm could also be useful in
the areas of cluster comparison for outcomes research and data reduction in
machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Farooq_S/0/1/0/all/0/1&quot;&gt;Samir Farooq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Weisenthal_S/0/1/0/all/0/1&quot;&gt;Samuel J. Weisenthal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Trayhan_M/0/1/0/all/0/1&quot;&gt;Melissa Trayhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+White_R/0/1/0/all/0/1&quot;&gt;Robert J. White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bush_K/0/1/0/all/0/1&quot;&gt;Kristen Bush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mariuz_P/0/1/0/all/0/1&quot;&gt;Peter R. Mariuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zand_M/0/1/0/all/0/1&quot;&gt;Martin S. Zand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11242">
<title>MOG: Mapper on Graphs for Relationship Preserving Clustering. (arXiv:1804.11242v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1804.11242</link>
<description rdf:parseType="Literal">&lt;p&gt;The interconnected nature of graphs often results in difficult to interpret
clutter. Typically techniques focus on either decluttering by clustering nodes
with similar properties or grouping edges with similar relationship. We propose
using mapper, a powerful topological data analysis tool, to summarize the
structure of a graph in a way that both clusters data with similar properties
and preserves relationships. Typically, mapper operates on a given data by
utilizing a scalar function defined on every point in the data and a cover for
scalar function codomain. The output of mapper is a graph that summarize the
shape of the space. In this paper, we outline how to use this mapper
construction on an input graphs, outline three filter functions that capture
important structures of the input graph, and provide an interface for
interactively modifying the cover. To validate our approach, we conduct several
case studies on synthetic and real world data sets and demonstrate how our
method can give meaningful summaries for graphs with various complexities
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1&quot;&gt;Mustafa Hajij&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosen_P/0/1/0/all/0/1&quot;&gt;Paul Rosen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11271">
<title>Gaussian Process Behaviour in Wide Deep Neural Networks. (arXiv:1804.11271v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.11271</link>
<description rdf:parseType="Literal">&lt;p&gt;Whilst deep neural networks have shown great empirical success, there is
still much work to be done to understand their theoretical properties. In this
paper, we study the relationship between Gaussian processes with a recursive
kernel definition and random wide fully connected feedforward networks with
more than one hidden layer. We show that, under broad conditions, as we make
the architecture increasingly wide, the implied random function converges in
distribution to a Gaussian process, formalising and extending existing results
by Neal (1996) to deep networks. To evaluate convergence rates empirically, we
use maximum mean discrepancy. We then exhibit situations where existing
Bayesian deep networks are close to Gaussian processes in terms of the key
quantities of interest. Any Gaussian process has a flat representation. Since
this behaviour may be undesirable in certain situations we discuss ways in
which it might be prevented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Matthews_A/0/1/0/all/0/1&quot;&gt;Alexander G. de G. Matthews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rowland_M/0/1/0/all/0/1&quot;&gt;Mark Rowland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hron_J/0/1/0/all/0/1&quot;&gt;Jiri Hron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard E. Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghahramani_Z/0/1/0/all/0/1&quot;&gt;Zoubin Ghahramani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11326">
<title>Supervised learning with quantum enhanced feature spaces. (arXiv:1804.11326v1 [quant-ph])</title>
<link>http://arxiv.org/abs/1804.11326</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning and quantum computing are two technologies each with the
potential for altering how computation is performed to address previously
untenable problems. Kernel methods for machine learning are ubiquitous for
pattern recognition, with support vector machines (SVMs) being the most
well-known method for classification problems. However, there are limitations
to the successful solution to such problems when the feature space becomes
large, and the kernel functions become computationally expensive to estimate. A
core element to computational speed-ups afforded by quantum algorithms is the
exploitation of an exponentially large quantum state space through controllable
entanglement and interference. Here, we propose and use two novel methods which
represent the feature space of a classification problem by a quantum state,
taking advantage of the large dimensionality of quantum Hilbert space to obtain
an enhanced solution. One method, the quantum variational classifier builds on
[1,2] and operates through using a variational quantum circuit to classify a
training set in direct analogy to conventional SVMs. In the second, a quantum
kernel estimator, we estimate the kernel function and optimize the classifier
directly. The two methods present a new class of tools for exploring the
applications of noisy intermediate scale quantum computers to machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Havlicek_V/0/1/0/all/0/1&quot;&gt;Vojtech Havlicek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Corcoles_A/0/1/0/all/0/1&quot;&gt;Antonio D. C&amp;#xf3;rcoles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Temme_K/0/1/0/all/0/1&quot;&gt;Kristan Temme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Harrow_A/0/1/0/all/0/1&quot;&gt;Aram W. Harrow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chow_J/0/1/0/all/0/1&quot;&gt;Jerry M. Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gambetta_J/0/1/0/all/0/1&quot;&gt;Jay M. Gambetta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.09466">
<title>Double Coupled Canonical Polyadic Decomposition for Joint Blind Source Separation. (arXiv:1612.09466v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1612.09466</link>
<description rdf:parseType="Literal">&lt;p&gt;Joint blind source separation (J-BSS) is an emerging data-driven technique
for multi-set data-fusion. In this paper, J-BSS is addressed from a tensorial
perspective. We show how, by using second-order multi-set statistics in J-BSS,
a specific double coupled canonical polyadic decomposition (DC-CPD) problem can
be formulated. We propose an algebraic DC-CPD algorithm based on a coupled
rank-1 detection mapping. This algorithm converts a possibly underdetermined
DC-CPD to a set of overdetermined CPDs. The latter can be solved algebraically
via a generalized eigenvalue decomposition based scheme. Therefore, this
algorithm is deterministic and returns the exact solution in the noiseless
case. In the noisy case, it can be used to effectively initialize optimization
based DC-CPD algorithms. In addition, we obtain the determini- stic and generic
uniqueness conditions for DC-CPD, which are shown to be more relaxed than their
CPD counterpart. Experiment results are given to illustrate the superiority of
DC-CPD over standard CPD based BSS methods and several existing J-BSS methods,
with regards to uniqueness and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gong_X/0/1/0/all/0/1&quot;&gt;Xiao-Feng Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qiu-Hua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cong_F/0/1/0/all/0/1&quot;&gt;Feng-Yu Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lathauwer_L/0/1/0/all/0/1&quot;&gt;Lieven De Lathauwer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.05037">
<title>Additive Models with Trend Filtering. (arXiv:1702.05037v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1702.05037</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider additive models built with trend filtering, i.e., additive models
whose components are each regularized by the (discrete) total variation of
their $(k+1)$st (discrete) derivative, for a chosen integer $k \geq 0$. This
results in $k$th degree piecewise polynomial components, (e.g., $k=0$ gives
piecewise constant components, $k=1$ gives piecewise linear, $k=2$ gives
piecewise quadratic, etc.). Analogous to its advantages in the univariate case,
additive trend filtering has favorable theoretical and computational
properties, thanks in large part to the localized nature of the (discrete)
total variation regularizer that it uses. On the theory side, we derive fast
error rates for additive trend filtering estimates, and show these rates are
minimax optimal when the underlying function is additive and has component
functions whose derivatives are of bounded variation. We also show that these
rates are unattainable by additive smoothing splines (and by additive models
built from linear smoothers, in general). On the computational side, as per the
standard in additive models, backfitting is an appealing method for
optimization, but it is particularly appealing for additive trend filtering
because we can leverage a few highly efficient univariate trend filtering
solvers. Going one step further, we derive a new backfitting algorithm whose
iterations can be run in parallel, which (as far as we know) is the first of
its kind. Lastly, we present experiments that examine the empirical performance
of additive trend filtering, and outline some possible extensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sadhanala_V/0/1/0/all/0/1&quot;&gt;Veeranjaneyulu Sadhanala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tibshirani_R/0/1/0/all/0/1&quot;&gt;Ryan J. Tibshirani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05790">
<title>Learning Disordered Topological Phases by Statistical Recovery of Symmetry. (arXiv:1709.05790v3 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05790</link>
<description rdf:parseType="Literal">&lt;p&gt;In this letter, we apply the artificial neural network in a supervised manner
to map out the quantum phase diagram of disordered topological superconductor
in class DIII. Given the disorder that keeps the discrete symmetries of the
ensemble as a whole, translational symmetry which is broken in the
quasiparticle distribution individually is recovered statistically by taking an
ensemble average. By using this, we classify the phases by the artificial
neural network that learned the quasiparticle distribution in the clean limit,
and show that the result is totally consistent with the calculation by the
transfer matrix method or noncommutative geometry approach. If all three
phases, namely the $\mathbb{Z}_2$, trivial, and the thermal metal phases appear
in the clean limit, the machine can classify them with high confidence over the
entire phase diagram. If only the former two phases are present, we find that
the machine remains confused in the certain region, leading us to conclude the
detection of the unknown phase which is eventually identified as the thermal
metal phase. In our method, only the first moment of the quasiparticle
distribution is used for input, but application to a wider variety of systems
is expected by the inclusion of higher moments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Yoshioka_N/0/1/0/all/0/1&quot;&gt;Nobuyuki Yoshioka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Akagi_Y/0/1/0/all/0/1&quot;&gt;Yutaka Akagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Katsura_H/0/1/0/all/0/1&quot;&gt;Hosho Katsura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.09412">
<title>mixup: Beyond Empirical Risk Minimization. (arXiv:1710.09412v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.09412</link>
<description rdf:parseType="Literal">&lt;p&gt;Large deep neural networks are powerful, but exhibit undesirable behaviors
such as memorization and sensitivity to adversarial examples. In this work, we
propose mixup, a simple learning principle to alleviate these issues. In
essence, mixup trains a neural network on convex combinations of pairs of
examples and their labels. By doing so, mixup regularizes the neural network to
favor simple linear behavior in-between training examples. Our experiments on
the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show
that mixup improves the generalization of state-of-the-art neural network
architectures. We also find that mixup reduces the memorization of corrupt
labels, increases the robustness to adversarial examples, and stabilizes the
training of generative adversarial networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cisse_M/0/1/0/all/0/1&quot;&gt;Moustapha Cisse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dauphin_Y/0/1/0/all/0/1&quot;&gt;Yann N. Dauphin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_Paz_D/0/1/0/all/0/1&quot;&gt;David Lopez-Paz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05869">
<title>Predictive Independence Testing, Predictive Conditional Independence Testing, and Predictive Graphical Modelling. (arXiv:1711.05869v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05869</link>
<description rdf:parseType="Literal">&lt;p&gt;Testing (conditional) independence of multivariate random variables is a task
central to statistical inference and modelling in general - though
unfortunately one for which to date there does not exist a practicable
workflow. State-of-art workflows suffer from the need for heuristic or
subjective manual choices, high computational complexity, or strong parametric
assumptions.
&lt;/p&gt;
&lt;p&gt;We address these problems by establishing a theoretical link between
multivariate/conditional independence testing, and model comparison in the
multivariate predictive modelling aka supervised learning task. This link
allows advances in the extensively studied supervised learning workflow to be
directly transferred to independence testing workflows - including automated
tuning of machine learning type which addresses the need for a heuristic
choice, the ability to quantitatively trade-off computational demand with
accuracy, and the modern black-box philosophy for checking and interfacing.
&lt;/p&gt;
&lt;p&gt;As a practical implementation of this link between the two workflows, we
present a python package &apos;pcit&apos;, which implements our novel multivariate and
conditional independence tests, interfacing the supervised learning API of the
scikit-learn package. Theory and package also allow for straightforward
independence test based learning of graphical model structure.
&lt;/p&gt;
&lt;p&gt;We empirically show that our proposed predictive independence test outperform
or are on par to current practice, and the derived graphical model structure
learning algorithms asymptotically recover the &apos;true&apos; graph. This paper, and
the &apos;pcit&apos; package accompanying it, thus provide powerful, scalable,
generalizable, and easy-to-use methods for multivariate and conditional
independence testing, as well as for graphical model structure learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Burkart_S/0/1/0/all/0/1&quot;&gt;Samuel Burkart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kiraly_F/0/1/0/all/0/1&quot;&gt;Franz J Kir&amp;#xe1;ly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00753">
<title>Probabilistic supervised learning. (arXiv:1801.00753v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.00753</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive modelling and supervised learning are central to modern data
science. With predictions from an ever-expanding number of supervised black-box
strategies - e.g., kernel methods, random forests, deep learning aka neural
networks - being employed as a basis for decision making processes, it is
crucial to understand the statistical uncertainty associated with these
predictions.
&lt;/p&gt;
&lt;p&gt;As a general means to approach the issue, we present an overarching framework
for black-box prediction strategies that not only predict the target but also
their own predictions&apos; uncertainty. Moreover, the framework allows for fair
assessment and comparison of disparate prediction strategies. For this, we
formally consider strategies capable of predicting full distributions from
feature variables, so-called probabilistic supervised learning strategies.
&lt;/p&gt;
&lt;p&gt;Our work draws from prior work including Bayesian statistics, information
theory, and modern supervised machine learning, and in a novel synthesis leads
to (a) new theoretical insights such as a probabilistic bias-variance
decomposition and an entropic formulation of prediction, as well as to (b) new
algorithms and meta-algorithms, such as composite prediction strategies,
probabilistic boosting and bagging, and a probabilistic predictive independence
test.
&lt;/p&gt;
&lt;p&gt;Our black-box formulation also leads (c) to a new modular interface view on
probabilistic supervised learning and a modelling workflow API design, which we
have implemented in the newly released skpro machine learning toolbox,
extending the familiar modelling interface and meta-modelling functionality of
sklearn. The skpro package provides interfaces for construction, composition,
and tuning of probabilistic supervised learning strategies, together with
orchestration features for validation and comparison of any such strategy - be
it frequentist, Bayesian, or other.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gressmann_F/0/1/0/all/0/1&quot;&gt;Frithjof Gressmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kiraly_F/0/1/0/all/0/1&quot;&gt;Franz J. Kir&amp;#xe1;ly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mateen_B/0/1/0/all/0/1&quot;&gt;Bilal Mateen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oberhauser_H/0/1/0/all/0/1&quot;&gt;Harald Oberhauser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07129">
<title>Deep BCD-Net Using Identical Encoding-Decoding CNN Structures for Iterative Image Recovery. (arXiv:1802.07129v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07129</link>
<description rdf:parseType="Literal">&lt;p&gt;In &quot;extreme&quot; computational imaging that collects extremely undersampled or
noisy measurements, obtaining an accurate image within a reasonable computing
time is challenging. Incorporating image mapping convolutional neural networks
(CNN) into iterative image recovery has great potential to resolve this issue.
This paper 1) incorporates image mapping CNN using identical convolutional
kernels in both encoders and decoders into a block coordinate descent (BCD)
signal recovery method and 2) applies alternating direction method of
multipliers to train the aforementioned image mapping CNN. We refer to the
proposed recurrent network as BCD-Net using identical encoding-decoding CNN
structures. Numerical experiments show that, for a) denoising low
signal-to-noise-ratio images and b) extremely undersampled magnetic resonance
imaging, the proposed BCD-Net achieves significantly more accurate image
recovery, compared to BCD-Net using distinct encoding-decoding structures
and/or the conventional image recovery model using both wavelets and total
variation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chun_I/0/1/0/all/0/1&quot;&gt;Il Yong Chun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fessler_J/0/1/0/all/0/1&quot;&gt;Jeffrey A. Fessler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09383">
<title>Online Second Order Methods for Non-Convex Stochastic Optimizations. (arXiv:1803.09383v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09383</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a family of online second order methods for possibly
non-convex stochastic optimizations based on the theory of preconditioned
stochastic gradient descent (PSGD), which can be regarded as an enhance
stochastic Newton method with the ability to handle gradient noise and
non-convexity simultaneously. We have improved the implementations of the
original PSGD in several ways, e.g., new forms of preconditioners, more
accurate Hessian vector product calculations, and better numerical stability
with vanishing or ill-conditioned Hessian, etc.. We also have unrevealed the
relationship between feature normalization and PSGD with Kronecker product
preconditioners, which explains the excellent performance of Kronecker product
preconditioners in deep neural network learning. A software package
(https://github.com/lixilinx/psgd_tf) implemented in Tensorflow is provided to
compare variations of stochastic gradient descent (SGD) and PSGD with five
different preconditioners on a wide range of benchmark problems with commonly
used neural network architectures, e.g., convolutional and recurrent neural
networks. Experimental results clearly demonstrate the advantages of PSGD in
terms of generalization performance and convergence speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xi-Lin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08615">
<title>QSAR Classification Modeling for Bioactivity of Molecular Structure via SPL-Logsum. (arXiv:1804.08615v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08615</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantitative structure-activity relationship (QSAR) modelling is effective
&apos;bridge&apos; to search the reliable relationship related bioactivity to molecular
structure. A QSAR classification model contains a lager number of redundant,
noisy and irrelevant descriptors. To address this problem, various of methods
have been proposed for descriptor selection. Generally, they can be grouped
into three categories: filters, wrappers, and embedded methods. Regularization
method is an important embedded technology, which can be used for continuous
shrinkage and automatic descriptors selection. In recent years, the interest of
researchers in the application of regularization techniques is increasing in
descriptors selection , such as, logistic regression(LR) with $L_1$ penalty. In
this paper, we proposed a novel descriptor selection method based on self-paced
learning(SPL) with Logsum penalized LR for predicting the bioactivity of
molecular structure. SPL inspired by the learning process of humans and animals
that gradually learns from easy samples(smaller losses) to hard samples(bigger
losses) samples into training and Logsum regularization has capacity to select
few meaningful and significant molecular descriptors, respectively.
Experimental results on simulation and three public QSAR datasets show that our
proposed SPL-Logsum method outperforms other commonly used sparse methods in
terms of classification performance and model interpretation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Liang-Yong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qing-Yong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10318">
<title>Efficiently Learning Nonstationary Gaussian Processes for Real World Impact. (arXiv:1804.10318v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10318</link>
<description rdf:parseType="Literal">&lt;p&gt;Most real world phenomena such as sunlight distribution under a forest
canopy, minerals concentration, stock valuation, exhibit nonstationary dynamics
i.e. phenomenon variation changes depending on the locality. Nonstationary
dynamics pose both theoretical and practical challenges to statistical machine
learning algorithms that aim to accurately capture the complexities governing
the evolution of such processes. Typically the nonstationary dynamics are
modeled using nonstationary Gaussian Process models (NGPS) that employ local
latent dynamics parameterization to correspondingly model the nonstationary
real observable dynamics. Recently, an approach based on most likely induced
latent dynamics representation attracted research community&apos;s attention for a
while. The approach could not be employed for large scale real world
applications because learning a most likely latent dynamics representation
involves maximization of marginal likelihood of the observed real dynamics that
becomes intractable as the number of induced latent points grows with problem
size. We have established a direct relationship between informativeness of the
induced latent dynamics and the marginal likelihood of the observed real
dynamics. This opens up the possibility of maximizing marginal likelihood of
observed real dynamics indirectly by near optimally maximizing entropy or
mutual information gain on the induced latent dynamics using greedy algorithms.
Therefore, for an efficient yet accurate inference, we propose to build an
induced latent dynamics representation using a novel algorithm LISAL that
adaptively maximizes entropy or mutual information on the induced latent
dynamics and marginal likelihood of observed real dynamics in an iterative
manner. The relevance of LISAL is validated using real world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Sahil Garg&lt;/a&gt;</dc:creator>
</item></rdf:RDF>