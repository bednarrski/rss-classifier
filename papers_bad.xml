<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-02T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00149"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.08382"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11395"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00038"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00064"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00198"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00211"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00325"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00373"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00421"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00423"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00432"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00456"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00492"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00499"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00506"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00595"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00617"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.02314"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01788"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00057"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00069"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00102"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00104"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00130"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00217"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00243"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00281"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00292"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00341"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00379"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00445"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00636"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1603.09638"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.06376"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.03228"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.03774"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01711"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.02538"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06900"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07462"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06969"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04667"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09383"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10840"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.00149">
<title>Hardware design of LIF with Latency neuron model with memristive STDP synapses. (arXiv:1804.00149v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1804.00149</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, the hardware implementation of a neuromorphic system is
presented. This system is composed of a Leaky Integrate-and-Fire with Latency
(LIFL) neuron and a Spike-Timing Dependent Plasticity (STDP) synapse. LIFL
neuron model allows to encode more information than the common
Integrate-and-Fire models, typically considered for neuromorphic
implementations. In our system LIFL neuron is implemented using CMOS circuits
while memristor is used for the implementation of the STDP synapse. A
description of the entire circuit is provided. Finally, the capabilities of the
proposed architecture have been evaluated by simulating a motif composed of
three neurons and two synapses. The simulation results confirm the validity of
the proposed system and its suitability for the design of more complex spiking
neural networks
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acciarito_S/0/1/0/all/0/1&quot;&gt;Simone Acciarito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardarilli_G/0/1/0/all/0/1&quot;&gt;Gian Carlo Cardarilli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cristini_A/0/1/0/all/0/1&quot;&gt;Alessandro Cristini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nunzio_L/0/1/0/all/0/1&quot;&gt;Luca Di Nunzio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazzolari_R/0/1/0/all/0/1&quot;&gt;Rocco Fazzolari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanal_G/0/1/0/all/0/1&quot;&gt;Gaurav Mani Khanal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Re_M/0/1/0/all/0/1&quot;&gt;Marco Re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susi_G/0/1/0/all/0/1&quot;&gt;Gianluca Susi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.08382">
<title>Identifying Quantum Phase Transitions with Adversarial Neural Networks. (arXiv:1710.08382v2 [cond-mat.stat-mech] UPDATED)</title>
<link>http://arxiv.org/abs/1710.08382</link>
<description rdf:parseType="Literal">&lt;p&gt;The identification of phases of matter is a challenging task, especially in
quantum mechanics, where the complexity of the ground state appears to grow
exponentially with the size of the system. We address this problem with
state-of-the-art deep learning techniques: adversarial domain adaptation. We
derive the phase diagram of the whole parameter space starting from a fixed and
known subspace using unsupervised learning. The input data set contains both
labeled and unlabeled data instances. The first kind is a system that admits an
accurate analytical or numerical solution, and one can recover its phase
diagram. The second type is the physical system with an unknown phase diagram.
Adversarial domain adaptation uses both types of data to create invariant
feature extracting layers in a deep learning architecture. Once these layers
are trained, we can attach an unsupervised learner to the network to find phase
transitions. We show the success of this technique by applying it on several
paradigmatic models: the Ising model with different temperatures, the
Bose-Hubbard model, and the SSH model with disorder. The input is the ground
state without any manual feature engineering, and the dimension of the
parameter space is unrestricted. The method finds unknown transitions
successfully and predicts transition points in close agreement with standard
methods. This study opens the door to the classification of physical systems
where the phases boundaries are complex such as the many-body localization
problem or the Bose glass phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Huembeli_P/0/1/0/all/0/1&quot;&gt;Patrick Huembeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Dauphin_A/0/1/0/all/0/1&quot;&gt;Alexandre Dauphin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Wittek_P/0/1/0/all/0/1&quot;&gt;Peter Wittek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02342">
<title>Energy-Efficient CMOS Memristive Synapses for Mixed-Signal Neuromorphic System-on-a-Chip. (arXiv:1802.02342v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02342</link>
<description rdf:parseType="Literal">&lt;p&gt;Emerging non-volatile memory (NVM), or memristive, devices promise
energy-efficient realization of deep learning, when efficiently integrated with
mixed-signal integrated circuits on a CMOS substrate. Even though several
algorithmic challenges need to be addressed to turn the vision of memristive
Neuromorphic Systems-on-a-Chip (NeuSoCs) into reality, issues at the device and
circuit interface need immediate attention from the community. In this work, we
perform energy-estimation of a NeuSoC system and predict the desirable circuit
and device parameters for energy-efficiency optimization. Also, CMOS synapse
circuits based on the concept of CMOS memristor emulator are presented as a
system prototyping methodology, while practical memristor devices are being
developed and integrated with general-purpose CMOS. The proposed mixed-signal
memristive synapse can be designed and fabricated using standard CMOS
technologies and open doors to interesting applications in cognitive computing
circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_V/0/1/0/all/0/1&quot;&gt;Vishal Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kehan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11395">
<title>Contrast-Oriented Deep Neural Networks for Salient Object Detection. (arXiv:1803.11395v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1803.11395</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional neural networks have become a key element in the recent
breakthrough of salient object detection. However, existing CNN-based methods
are based on either patch-wise (region-wise) training and inference or fully
convolutional networks. Methods in the former category are generally
time-consuming due to severe storage and computational redundancies among
overlapping patches. To overcome this deficiency, methods in the second
category attempt to directly map a raw input image to a predicted dense
saliency map in a single network forward pass. Though being very efficient, it
is arduous for these methods to detect salient objects of different scales or
salient regions with weak semantic information. In this paper, we develop
hybrid contrast-oriented deep neural networks to overcome the aforementioned
limitations. Each of our deep networks is composed of two complementary
components, including a fully convolutional stream for dense prediction and a
segment-level spatial pooling stream for sparse saliency inference. We further
propose an attentional module that learns weight maps for fusing the two
saliency predictions from these two streams. A tailored alternate scheme is
designed to train these deep networks by fine-tuning pre-trained baseline
models. Finally, a customized fully connected CRF model incorporating a salient
contour feature embedding can be optionally applied as a post-processing step
to improve spatial coherence and contour positioning in the fused result from
these two streams. Extensive experiments on six benchmark datasets demonstrate
that our proposed model can significantly outperform the state of the art in
terms of all popular evaluation metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yizhou Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00038">
<title>Overview: A Hierarchical Framework for Plan Generation and Execution in Multi-Robot Systems. (arXiv:1804.00038v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.00038</link>
<description rdf:parseType="Literal">&lt;p&gt;The authors present an overview of a hierarchical framework for coordinating
task- and motion-level operations in multirobot systems. Their framework is
based on the idea of using simple temporal networks to simultaneously reason
about precedence/causal constraints required for task-level coordination and
simple temporal constraints required to take some kinematic constraints of
robots into account. In the plan-generation phase, the framework provides a
computationally scalable method for generating plans that achieve high-level
tasks for groups of robots and take some of their kinematic constraints into
account. In the plan-execution phase, the framework provides a method for
absorbing an imperfect plan execution to avoid time-consuming re-planning in
many cases. The authors use the multirobot path-planning problem as a case
study to present the key ideas behind their framework for the long-term
autonomy of multirobot systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honig_W/0/1/0/all/0/1&quot;&gt;Wolfgang H&amp;#xf6;nig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_L/0/1/0/all/0/1&quot;&gt;Liron Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uras_T/0/1/0/all/0/1&quot;&gt;Tansel Uras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_T/0/1/0/all/0/1&quot;&gt;T. K. Satish Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayanian_N/0/1/0/all/0/1&quot;&gt;Nora Ayanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koenig_S/0/1/0/all/0/1&quot;&gt;Sven Koenig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00064">
<title>Learning Beyond Human Expertise with Generative Models for Dental Restorations. (arXiv:1804.00064v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.00064</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer vision has advanced significantly that many discriminative
approaches such as object recognition are now widely used in real applications.
We present another exciting development that utilizes generative models for the
mass customization of medical products such as dental crowns. In the dental
industry, it takes a technician years of training to design synthetic crowns
that restore the function and integrity of missing teeth. Each crown must be
customized to individual patients, and it requires human expertise in a
time-consuming and labor-intensive process, even with computer-assisted design
software. We develop a fully automatic approach that learns not only from human
designs of dental crowns, but also from natural spatial profiles between
opposing teeth. The latter is hard to account for by technicians but important
for proper biting and chewing functions. Built upon a Generative Adversar-ial
Network architecture (GAN), our deep learning model predicts the customized
crown-filled depth scan from the crown-missing depth scan and opposing depth
scan. We propose to incorporate additional space constraints and statistical
compatibility into learning. Our automatic designs exceed human technicians&apos;
standards for good morphology and functionality, and our algorithm is being
tested for production use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jyh-Jing Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azernikov_S/0/1/0/all/0/1&quot;&gt;Sergei Azernikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Stella X. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00198">
<title>Learning to Run challenge: Synthesizing physiologically accurate motion using deep reinforcement learning. (arXiv:1804.00198v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.00198</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthesizing physiologically-accurate human movement in a variety of
conditions can help practitioners plan surgeries, design experiments, or
prototype assistive devices in simulated environments, reducing time and costs
and improving treatment outcomes. Because of the large and complex solution
spaces of biomechanical models, current methods are constrained to specific
movements and models, requiring careful design of a controller and hindering
many possible applications. We sought to discover if modern optimization
methods efficiently explore these complex spaces. To do this, we posed the
problem as a competition in which participants were tasked with developing a
controller to enable a physiologically-based human model to navigate a complex
obstacle course as quickly as possible, without using any experimental data.
They were provided with a human musculoskeletal model and a physics-based
simulation environment. In this paper, we discuss the design of the
competition, technical difficulties, results, and analysis of the top
controllers. The challenge proved that deep reinforcement learning techniques,
despite their high computational cost, can be successfully employed as an
optimization method for synthesizing physiologically feasible motion in
high-dimensional biomechanical systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kidzinski_L/0/1/0/all/0/1&quot;&gt;&amp;#x141;ukasz Kidzi&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1&quot;&gt;Sharada P. Mohanty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_C/0/1/0/all/0/1&quot;&gt;Carmichael Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hicks_J/0/1/0/all/0/1&quot;&gt;Jennifer L. Hicks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carroll_S/0/1/0/all/0/1&quot;&gt;Sean F. Carroll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salathe_M/0/1/0/all/0/1&quot;&gt;Marcel Salath&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delp_S/0/1/0/all/0/1&quot;&gt;Scott L. Delp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00211">
<title>Efficient Encodings of Conditional Cardinality Constraints. (arXiv:1804.00211v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.00211</link>
<description rdf:parseType="Literal">&lt;p&gt;In the encoding of many real-world problems to propositional satisfiability,
the cardinality constraint is a recurrent constraint that needs to be managed
effectively. Several efficient encodings have been proposed while missing that
such a constraint can be involved in a more general propositional formulation.
To avoid combinatorial explosion, Tseitin principle usually used to translate
such general propositional formula to Conjunctive Normal Form (CNF), introduces
fresh propositional variables to represent sub-formulas and/or complex
contraints. Thanks to Plaisted and Greenbaum improvement, the polarity of the
sub-formula $\Phi$ is taken into account leading to conditional constraints of
the form $y\rightarrow \Phi$, or $\Phi\rightarrow y$, where $y$ is a fresh
propositional variable. In the case where $\Phi$ represents a cardinality
constraint, such translation leads to conditional cardinality constraints
subject of the present paper. We first show that when all the clauses encoding
the cardinality constraint are augmented with an additional new variable, most
of the well-known encodings cease to maintain the generalized arc consistency
property. Then, we consider some of these encodings and show how they can be
extended to recover such important property. An experimental validation is
conducted on a SAT-based pattern mining application, where such conditional
cardinality constraints is a cornerstone, showing the relevance of our proposed
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boudane_A/0/1/0/all/0/1&quot;&gt;Abdelhamid Boudane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jabbour_S/0/1/0/all/0/1&quot;&gt;Said Jabbour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raddaoui_B/0/1/0/all/0/1&quot;&gt;Badran Raddaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sais_L/0/1/0/all/0/1&quot;&gt;Lakhdar Sais&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00325">
<title>Aggregated Momentum: Stability Through Passive Damping. (arXiv:1804.00325v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00325</link>
<description rdf:parseType="Literal">&lt;p&gt;Momentum is a simple and widely used trick which allows gradient-based
optimizers to pick up speed in low curvature directions. Its performance
depends crucially on a damping coefficient $\beta$. Large $\beta$ values can
potentially deliver much larger speedups, but are prone to oscillations and
instability; hence one typically resorts to small values such as 0.5 or 0.9. We
propose Aggregated Momentum (AggMo), a variant of momentum which combines
multiple velocity vectors with different $\beta$ parameters. AggMo is trivial
to implement, but significantly dampens oscillations, enabling it to remain
stable even for aggressive $\beta$ values such as 0.999. We reinterpret
Nesterov&apos;s accelerated gradient descent as a special case of AggMo and provide
theoretical convergence bounds for online convex optimization. Empirically, we
find that AggMo is a suitable drop-in replacement for other momentum methods,
and frequently delivers faster convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_J/0/1/0/all/0/1&quot;&gt;James Lucas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1&quot;&gt;Roger Grosse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00373">
<title>TipsC: Tips and Corrections for programming MOOCs. (arXiv:1804.00373v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.00373</link>
<description rdf:parseType="Literal">&lt;p&gt;With the widespread adoption of MOOCs in academic institutions, it has become
imperative to come up with better techniques to solve the tutoring and grading
problems posed by programming courses. Programming being the new &apos;writing&apos;, it
becomes a challenge to ensure that a large section of the society is exposed to
programming. Due to the gradient in learning abilities of students, the course
instructor must ensure that everyone can cope up with the material, and receive
adequate help in completing assignments while learning along the way. We
introduce TipsC for this task. By analyzing a large number of correct
submissions, TipsC can search for correct codes resembling a given incorrect
solution. Without revealing the actual code, TipsC then suggests changes in the
incorrect code to help the student fix logical runtime errors. In addition,
this also serves as a cluster visualization tool for the instructor, revealing
different patterns in user submissions. We evaluated the effectiveness of
TipsC&apos;s clustering algorithm on data collected from previous offerings of an
introductory programming course conducted at IIT Kanpur where the grades were
given by human TAs. The results show the weighted average variance of marks for
clusters when similar submissions are grouped together is 47% less compared to
the case when all programs are grouped together.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Saksham Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1&quot;&gt;Pallav Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mor_P/0/1/0/all/0/1&quot;&gt;Parv Mor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karkare_A/0/1/0/all/0/1&quot;&gt;Amey Karkare&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00421">
<title>A Study of Student Learning Skills Using Fuzzy Relation Equations. (arXiv:1804.00421v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.00421</link>
<description rdf:parseType="Literal">&lt;p&gt;Fuzzy relation equations (FRE)are associated with the composition of binary
fuzzy relations. In the present work FRE are used as a tool for studying the
process of learning a new subject matter by a student class. A classroom
application and other csuitable examples connected to the student learning of
the derivative are also presented illustrating our results and useful
conclusions are obtained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voskoglou_M/0/1/0/all/0/1&quot;&gt;Michael Gr. Voskoglou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00423">
<title>Application of Grey Numbers to Assessment Processes. (arXiv:1804.00423v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.00423</link>
<description rdf:parseType="Literal">&lt;p&gt;The theory of grey systems plays an important role in science,engineering and
in the everyday life in general for handling approximate data. In the present
paper grey numbers are used as a tool for assessing with linguistic expressions
the mean performance of a group of objects participating in a certain activity.
Two applications to student and football player assessment are also presented
illustrating our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voskoglou_M/0/1/0/all/0/1&quot;&gt;Michael Gr. Voskoglou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theodorou_Y/0/1/0/all/0/1&quot;&gt;Yiannis Theodorou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00432">
<title>Deep Residual Learning for Accelerated MRI using Magnitude and Phase Networks. (arXiv:1804.00432v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.00432</link>
<description rdf:parseType="Literal">&lt;p&gt;Accelerated magnetic resonance (MR) scan acquisition with compressed sensing
(CS) and parallel imaging is a powerful method to reduce MR imaging scan time.
However, many reconstruction algorithms have high computational costs. To
address this, we investigate deep residual learning networks to remove aliasing
artifacts from artifact corrupted images. The proposed deep residual learning
networks are composed of magnitude and phase networks that are separately
trained. If both phase and magnitude information are available, the proposed
algorithm can work as an iterative k-space interpolation algorithm using
framelet representation. When only magnitude data is available, the proposed
approach works as an image domain post-processing algorithm. Even with strong
coherent aliasing artifacts, the proposed network successfully learned and
removed the aliasing artifacts, whereas current parallel and CS reconstruction
methods were unable to remove these artifacts. Comparisons using single and
multiple coil show that the proposed residual network provides good
reconstruction results with orders of magnitude faster computational time than
existing compressed sensing methods. The proposed deep learning framework may
have a great potential for accelerated MR reconstruction by generating accurate
results immediately.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongwook Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jaejun Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tak_S/0/1/0/all/0/1&quot;&gt;Sungho Tak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00456">
<title>Curiosity-driven Exploration for Mapless Navigation with Deep Reinforcement Learning. (arXiv:1804.00456v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1804.00456</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates exploration strategies of Deep Reinforcement Learning
(DRL) methods to learn navigation policies for mobile robots. In particular, we
augment the normal external reward for training DRL algorithms with intrinsic
reward signals measured by curiosity. We test our approach in a mapless
navigation setting, where the autonomous agent is required to navigate without
the occupancy map of the environment, to targets whose relative locations can
be easily acquired through low-cost solutions (e.g., visible light
localization, Wi-Fi signal localization). We validate that the intrinsic
motivation is crucial for improving DRL performance in tasks with challenging
exploration requirements. Our experimental results show that our proposed
method is able to more effectively learn navigation policies, and has better
generalization capabilities in previously unseen environments. A video of our
experimental results can be found at https://goo.gl/pWbpcF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhelo_O/0/1/0/all/0/1&quot;&gt;Oleksii Zhelo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_L/0/1/0/all/0/1&quot;&gt;Lei Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1&quot;&gt;Wolfram Burgard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00492">
<title>Regional Priority Based Anomaly Detection using Autoencoders. (arXiv:1804.00492v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.00492</link>
<description rdf:parseType="Literal">&lt;p&gt;In the recent times, autoencoders, besides being used for compression, have
been proven quite useful even for regenerating similar images or help in image
denoising. They have also been explored for anomaly detection in a few cases.
However, due to location invariance property of convolutional neural network,
autoencoders tend to learn from or search for learned features in the complete
image. This creates issues when all the items in the image are not equally
important and their location matters. For such cases, a semi supervised
solution - regional priority based autoencoder (RPAE) has been proposed. In
this model, similar to object detection models, a region proposal network
identifies the relevant areas in the images as belonging to one of the
predefined categories and then those bounding boxes are fed into appropriate
decoder based on the category they belong to. Finally, the error scores from
all the decoders are combined based on their importance to provide total
reconstruction error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1&quot;&gt;Shruti Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1&quot;&gt;Dattaraj Rao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00499">
<title>Semantic Adversarial Examples. (arXiv:1804.00499v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.00499</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are known to be vulnerable to adversarial examples,
i.e., images that are maliciously perturbed to fool the model. Generating
adversarial examples has been mostly limited to finding small perturbations
that maximize the model prediction error. Such images, however, contain
artificial perturbations that make them somewhat distinguishable from natural
images. This property is used by several defense methods to counter adversarial
examples by applying denoising filters or training the model to be robust to
small perturbations.
&lt;/p&gt;
&lt;p&gt;In this paper, we introduce a new class of adversarial examples, namely
&quot;Semantic Adversarial Examples,&quot; as images that are arbitrarily perturbed to
fool the model, but in such a way that the modified image semantically
represents the same object as the original image. We formulate the problem of
generating such images as a constrained optimization problem and develop an
adversarial transformation based on the shape bias property of human cognitive
system. In our method, we generate adversarial images by first converting the
RGB image into the HSV (Hue, Saturation and Value) color space and then
randomly shifting the Hue and Saturation components, while keeping the Value
component the same. Our experimental results on CIFAR10 dataset show that the
accuracy of VGG16 network on adversarial color-shifted images is 5.7%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_H/0/1/0/all/0/1&quot;&gt;Hossein Hosseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poovendran_R/0/1/0/all/0/1&quot;&gt;Radha Poovendran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00506">
<title>Towards Explanation of DNN-based Prediction with Guided Feature Inversion. (arXiv:1804.00506v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.00506</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep neural networks (DNN) have become an effective computational tool,
the prediction results are often criticized by the lack of interpretability,
which is essential in many real-world applications such as health informatics.
Existing attempts based on local interpretations aim to identify relevant
features contributing the most to the prediction of DNN by monitoring the
neighborhood of a given input. They usually simply ignore the intermediate
layers of the DNN that might contain rich information for interpretation. To
bridge the gap, in this paper, we propose to investigate a guided feature
inversion framework for taking advantage of the deep architectures towards
effective interpretation. The proposed framework not only determines the
contribution of each feature in the input but also provides insights into the
decision-making process of DNN models. By further interacting with the neuron
of the target category at the output layer of the DNN, we enforce the
interpretation result to be class-discriminative. We apply the proposed
interpretation model to different CNN architectures to provide explanations for
image data and conduct extensive experiments on ImageNet and PASCAL VOC07
datasets. The interpretation results demonstrate the effectiveness of our
proposed framework in providing class-discriminative interpretation for
DNN-based prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1&quot;&gt;Qingquan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xia Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00595">
<title>Learning to Reason with HOL4 tactics. (arXiv:1804.00595v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.00595</link>
<description rdf:parseType="Literal">&lt;p&gt;Techniques combining machine learning with translation to automated reasoning
have recently become an important component of formal proof assistants. Such
&quot;hammer&quot; tech- niques complement traditional proof assistant automation as
implemented by tactics and decision procedures. In this paper we present a
unified proof assistant automation approach which attempts to automate the
selection of appropriate tactics and tactic-sequences com- bined with an
optimized small-scale hammering approach. We implement the technique as a
tactic-level automation for HOL4: TacticToe. It implements a modified
A*-algorithm directly in HOL4 that explores different tactic-level proof paths,
guiding their selection by learning from a large number of previous
tactic-level proofs. Unlike the existing hammer methods, TacticToe avoids
translation to FOL, working directly on the HOL level. By combining tactic
prediction and premise selection, TacticToe is able to re-prove 39 percent of
7902 HOL4 theorems in 5 seconds whereas the best single HOL(y)Hammer strategy
solves 32 percent in the same amount of time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauthier_T/0/1/0/all/0/1&quot;&gt;Thibault Gauthier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaliszyk_C/0/1/0/all/0/1&quot;&gt;Cezary Kaliszyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1&quot;&gt;Josef Urban&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00617">
<title>Specification-Driven Multi-Perspective Predictive Business Process Monitoring (Extended Version). (arXiv:1804.00617v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.00617</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive analysis in business process monitoring aims at forecasting the
future information of a running business process. The prediction is typically
made based on the model extracted from historical process execution logs (event
logs). In practice, different business domains might require different kinds of
predictions. Hence, it is important to have a means for properly specifying the
desired prediction tasks, and a mechanism to deal with these various prediction
tasks. Although there have been many studies in this area, they mostly focus on
a specific prediction task. This work introduces a language for specifying the
desired prediction tasks, and this language allows us to express various kinds
of prediction tasks. This work also presents a mechanism for automatically
creating the corresponding prediction model based on the given specification.
Thus, different from previous studies, our approach enables us to deal with
various kinds of prediction tasks based on the given specification. A prototype
implementing our approach has been developed and experiments using a real-life
event log have been conducted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santoso_A/0/1/0/all/0/1&quot;&gt;Ario Santoso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.02314">
<title>Representation Learning for Visual-Relational Knowledge Graphs. (arXiv:1709.02314v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.02314</link>
<description rdf:parseType="Literal">&lt;p&gt;A visual-relational knowledge graph (KG) is a multi-relational graph whose
entities are associated with images. We introduce ImageGraph, a KG with 1,330
relation types, 14,870 entities, and 829,931 images. Visual-relational KGs lead
to novel probabilistic query types where images are treated as first-class
citizens. Both the prediction of relations between unseen images and
multi-relational image retrieval can be formulated as query types in a
visual-relational KG. We approach the problem of answering such queries with a
novel combination of deep convolutional networks and models for learning
knowledge graph embeddings. The resulting models can answer queries such as
&quot;How are these two unseen images related to each other?&quot; We also explore a
zero-shot learning scenario where an image of an entirely new entity is linked
with multiple relations to entities of an existing KG. The multi-relational
grounding of unseen entity images into a knowledge graph serves as the
description of such an entity. We conduct experiments to demonstrate that the
proposed deep architectures in combination with KG embedding objectives can
answer the visual-relational queries efficiently and accurately.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onoro_Rubio_D/0/1/0/all/0/1&quot;&gt;Daniel O&amp;#xf1;oro-Rubio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Duran_A/0/1/0/all/0/1&quot;&gt;Alberto Garc&amp;#xed;a-Dur&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_R/0/1/0/all/0/1&quot;&gt;Roberto Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_Sastre_R/0/1/0/all/0/1&quot;&gt;Roberto J. L&amp;#xf3;pez-Sastre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01788">
<title>A Reliability Theory of Truth. (arXiv:1801.01788v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01788</link>
<description rdf:parseType="Literal">&lt;p&gt;Our approach is basically a coherence approach, but we avoid the well-known
pitfalls of coherence theories of truth. Consistency is replaced by
reliability, which expresses support and attack, and, in principle, every
theory (or agent, message) counts. At the same time, we do not require a
priviledged access to &quot;reality&quot;. A centerpiece of our approach is that we
attribute reliability also to agents, messages, etc., so an unreliable source
of information will be less important in future. Our ideas can also be extended
to value systems, and even actions, e.g., of animals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlechta_K/0/1/0/all/0/1&quot;&gt;Karl Schlechta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00057">
<title>Understanding Autoencoders with Information Theoretic Concepts. (arXiv:1804.00057v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00057</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their great success in practical applications, there is still a lack
of theoretical and systematic methods to analyze deep neural networks. In this
paper, we illustrate an advanced information theoretic methodology to
understand the dynamics of learning and the design of autoencoders, a special
type of deep learning architectures that resembles a communication channel. By
generalizing the information plane to any cost function, and inspecting the
roles and dynamics of different layers using layer-wise information quantities,
we emphasize the role that mutual information plays in quantifying learning
from data. We further propose and also experimentally validate, for mean square
error training, two hypotheses regarding the layer-wise flow of information and
intrinsic dimensionality of the bottleneck layer, using respectively the data
processing inequality and the identification of a bifurcation point in the
information plane that is controlled by the given data. Our observations have
direct impact on the optimal design of autoencoders, the design of alternative
feedforward training methods, and even in the problem of generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shujian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jose C. Principe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00069">
<title>Engineering a Simplified 0-Bit Consistent Weighted Sampling. (arXiv:1804.00069v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.00069</link>
<description rdf:parseType="Literal">&lt;p&gt;The Min-Hashing approach to sketching has become an important tool in data
analysis, search, and classification. To apply it to real-valued datasets, the
ICWS algorithm has become a seminal approach that is widely used, and provides
state-of-the-art performance for this problem space. However, ICWS suffers a
computational burden as the sketch size K increases. We develop a new
Simplified approach to the ICWS algorithm, that enables us to obtain over 20x
speedups compared to the standard algorithm. The veracity of our approach is
demonstrated empirically on multiple datasets, showing that our new Simplified
CWS obtains the same quality of results while being an order of magnitude
faster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raff_E/0/1/0/all/0/1&quot;&gt;Edward Raff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sylvester_J/0/1/0/all/0/1&quot;&gt;Jared Sylvester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nicholas_C/0/1/0/all/0/1&quot;&gt;Charles Nicholas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00102">
<title>Collaborative targeted minimum loss inference from continuously indexed nuisance parameter estimators. (arXiv:1804.00102v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1804.00102</link>
<description rdf:parseType="Literal">&lt;p&gt;Suppose that we wish to infer the value of a statistical parameter at a law
from which we sample independent observations. Suppose that this parameter is
smooth and that we can define two variation-independent, infinite-dimensional
features of the law, its so called Q- and G-components (comp.), such that if we
estimate them consistently at a fast enough product of rates, then we can build
a confidence interval (CI) with a given asymptotic level based on a plain
targeted minimum loss estimator (TMLE). The estimators of the Q- and G-comp.
would typically be by products of machine learning algorithms. We focus on the
case that the machine learning algorithm for the G-comp. is fine-tuned by a
real-valued parameter h. Then, a plain TMLE with an h chosen by
cross-validation would typically not lend itself to the construction of a CI,
because the selection of h would trade-off its empirical bias with something
akin to the empirical variance of the estimator of the G-comp. as opposed to
that of the TMLE. A collaborative TMLE (C-TMLE) might, however, succeed in
achieving the relevant trade-off. We construct a C-TMLE and show that, under
high-level empirical processes conditions, and if there exists an oracle h that
makes a bulky remainder term asymptotically Gaussian, then the C-TMLE is
asymptotically Gaussian hence amenable to building a CI provided that its
asymptotic variance can be estimated too. We illustrate the construction and
main result with the inference of the average treatment effect, where the
Q-comp. consists in a marginal law and a conditional expectation, and the
G-comp. is a propensity score (a conditional probability). We also conduct a
multi-faceted simulation study to investigate the empirical properties of the
collaborative TMLE when the G-comp. is estimated by the LASSO. Here, h is the
bound on the l1-norm of the candidate coefficients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ju_C/0/1/0/all/0/1&quot;&gt;Cheng Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chambaz_A/0/1/0/all/0/1&quot;&gt;Antoine Chambaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laan_M/0/1/0/all/0/1&quot;&gt;Mark J. van der Laan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00104">
<title>Joint-VAE: Learning Disentangled Joint Continuous and Discrete Representations. (arXiv:1804.00104v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.00104</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a framework for learning disentangled and interpretable jointly
continuous and discrete representations in an unsupervised manner. By
augmenting the continuous latent distribution of variational autoencoders with
a relaxed discrete distribution and controlling the amount of information
encoded in each latent unit, we show how continuous and categorical factors of
variation can be discovered automatically from data. The learned model also
contains an inference network which can infer quantities such as angle and
width of objects from image data in a completely unsupervised manner. Our
experiments show that the framework disentangles continuous and discrete
generative factors on various datasets, including disentangling digit type from
stroke thickness, angle and width on MNIST, chair type from azimuth and width
on the Chairs dataset and age from azimuth on CelebA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dupont_E/0/1/0/all/0/1&quot;&gt;Emilien Dupont&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00130">
<title>Locally Convex Sparse Learning over Networks. (arXiv:1804.00130v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.00130</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a distributed learning setup where a sparse signal is estimated
over a network. Our main interest is to save communication resource for
information exchange over the network and reduce processing time. Each node of
the network uses a convex optimization based algorithm that provides a locally
optimum solution for that node. The nodes exchange their signal estimates over
the network in order to refine their local estimates. At a node, the
optimization algorithm is based on an $\ell_1$-norm minimization with
appropriate modifications to promote sparsity as well as to include influence
of estimates from neighboring nodes. Our expectation is that local estimates in
each node improve fast and converge, resulting in a limited demand for
communication of estimates between nodes and reducing the processing time. We
provide restricted-isometry-property (RIP)-based theoretical analysis on
estimation quality. In the scenario of clean observation, it is shown that the
local estimates converge to the exact sparse signal under certain technical
conditions. Simulation results show that the proposed algorithms show
competitive performance compared to a globally optimum distributed LASSO
algorithm in the sense of convergence speed and estimation error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zaki_A/0/1/0/all/0/1&quot;&gt;Ahmed Zaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chatterjee_S/0/1/0/all/0/1&quot;&gt;Saikat Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mitra_P/0/1/0/all/0/1&quot;&gt;Partha P. Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rasmussen_L/0/1/0/all/0/1&quot;&gt;Lars K. Rasmussen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00217">
<title>Fundamental Resource Trade-offs for Encoded Distributed Optimization. (arXiv:1804.00217v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1804.00217</link>
<description rdf:parseType="Literal">&lt;p&gt;Dealing with the shear size and complexity of today&apos;s massive data sets
requires computational platforms that can analyze data in a parallelized and
distributed fashion. A major bottleneck that arises in such modern distributed
computing environments is that some of the worker nodes may run slow. These
nodes a.k.a.~stragglers can significantly slow down computation as the slowest
node may dictate the overall computational time. A recent computational
framework, called encoded optimization, creates redundancy in the data to
mitigate the effect of stragglers. In this paper we develop novel mathematical
understanding for this framework demonstrating its effectiveness in much
broader settings than was previously understood. We also analyze the
convergence behavior of iterative encoded optimization algorithms, allowing us
to characterize fundamental trade-offs between convergence rate, size of data
set, accuracy, computational load (or data redundancy), and straggler
toleration in this framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avestimehr_A/0/1/0/all/0/1&quot;&gt;A. Salman Avestimehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalan_S/0/1/0/all/0/1&quot;&gt;Seyed Mohammadreza Mousavi Kalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1&quot;&gt;Mahdi Soltanolkotabi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00243">
<title>The Structure Transfer Machine Theory and Applications. (arXiv:1804.00243v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.00243</link>
<description rdf:parseType="Literal">&lt;p&gt;Representation learning is a fundamental but challenging problem, especially
when the distribution of data is unknown. We propose a new representation
learning method, termed Structure Transfer Machine (STM), which enables feature
learning process to converge at the representation expectation in a
probabilistic way. We theoretically show that such an expected value of the
representation (mean) is achievable if the manifold structure can be
transferred from the data space to the feature space. The resulting structure
regularization term, named manifold loss, is incorporated into the loss
function of the typical deep learning pipeline. The STM architecture is
constructed to enforce the learned deep representation to satisfy the intrinsic
manifold structure from the data, which results in robust features that suit
various application scenarios, such as digit recognition, image classification
and object tracking. Compared to state-of-the-art CNN architectures, we achieve
the better results on several commonly used benchmarks\footnote{The source code
is available. https://github.com/stmstmstm/stm }.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baochang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_L/0/1/0/all/0/1&quot;&gt;Lian Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jungong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1&quot;&gt;Xiantong Zhen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00281">
<title>A note on state preparation for quantum machine learning. (arXiv:1804.00281v1 [quant-ph])</title>
<link>http://arxiv.org/abs/1804.00281</link>
<description rdf:parseType="Literal">&lt;p&gt;The intersection between the fields of machine learning and quantum
information processing is proving to be a fruitful field for the discovery of
new quantum algorithms, which potentially offer an exponential speed-up over
their classical counterparts. However, many such algorithms require the ability
to produce states proportional to vectors stored in quantum memory. Even given
access to quantum databases which store exponentially long vectors, the
construction of which is considered a one-off overhead, it has been argued that
the cost of preparing such amplitude-encoded states may offset any exponential
quantum advantage. Here we argue that specifically in the context of machine
learning applications it suffices to prepare a state close to the ideal state
only in the $\infty$-norm, and that this can be achieved with only a constant
number of memory queries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhikuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Dunjko_V/0/1/0/all/0/1&quot;&gt;Vedran Dunjko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Fitzsimons_J/0/1/0/all/0/1&quot;&gt;Jack K. Fitzsimons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Rebentrost_P/0/1/0/all/0/1&quot;&gt;Patrick Rebentrost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Fitzsimons_J/0/1/0/all/0/1&quot;&gt;Joseph F. Fitzsimons&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00292">
<title>EarthMapper: A Tool Box for the Semantic Segmentation of Remote Sensing Imagery. (arXiv:1804.00292v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.00292</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning continues to push state-of-the-art performance for the semantic
segmentation of color (i.e., RGB) imagery; however, the lack of annotated data
for many remote sensing sensors (i.e. hyperspectral imagery (HSI)) prevents
researchers from taking advantage of this recent success. Since generating
sensor specific datasets is time intensive and cost prohibitive, remote sensing
researchers have embraced deep unsupervised feature extraction. Although these
methods have pushed state-of-the-art performance on current HSI benchmarks,
many of these tools are not readily accessible to many researchers. In this
letter, we introduce a software pipeline, which we call EarthMapper, for the
semantic segmentation of non-RGB remote sensing imagery. It includes
self-taught spatial-spectral feature extraction, various standard and deep
learning classifiers, and undirected graphical models for post-processing. We
evaluated EarthMapper on the Indian Pines and Pavia University datasets and
have released this code for public use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kemker_R/0/1/0/all/0/1&quot;&gt;Ronald Kemker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gewali_U/0/1/0/all/0/1&quot;&gt;Utsav B. Gewali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kanan_C/0/1/0/all/0/1&quot;&gt;Christopher Kanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00341">
<title>Sparse Principal Component Analysis via Variable Projection. (arXiv:1804.00341v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.00341</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse principal component analysis (SPCA) has emerged as a powerful
technique for modern data analysis. We discuss a robust and scalable algorithm
for computing sparse principal component analysis. Specifically, we model SPCA
as a matrix factorization problem with orthogonality constraints, and develop
specialized optimization algorithms that partially minimize a subset of the
variables (variable projection). The framework incorporates a wide variety of
sparsity-inducing regularizers for SPCA. We also extend the variable projection
approach to robust SPCA, for any robust loss that can be expressed as the
Moreau envelope of a simple function, with the canonical example of the Huber
loss. Finally, randomized methods for linear algebra are used to extend the
approach to the large-scale (big data) setting. The proposed algorithms are
demonstrated using both synthetic and real world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Erichson_N/0/1/0/all/0/1&quot;&gt;N. Benjamin Erichson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeng_P/0/1/0/all/0/1&quot;&gt;Peng Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Manohar_K/0/1/0/all/0/1&quot;&gt;Krithika Manohar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brunton_S/0/1/0/all/0/1&quot;&gt;Steven L. Brunton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kutz_J/0/1/0/all/0/1&quot;&gt;J. Nathan Kutz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aravkin_A/0/1/0/all/0/1&quot;&gt;Aleksandr Y. Aravkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00379">
<title>Recall Traces: Backtracking Models for Efficient Reinforcement Learning. (arXiv:1804.00379v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.00379</link>
<description rdf:parseType="Literal">&lt;p&gt;In many environments only a tiny subset of all states yield high reward. In
these cases, few of the interactions with the environment provide a relevant
learning signal. Hence, we may want to preferentially train on those
high-reward states and the probable trajectories leading to them. To this end,
we advocate for the use of a backtracking model that predicts the preceding
states that terminate at a given high-reward state. We can train a model which,
starting from a high value state (or one that is estimated to have high value),
predicts and sample for which the (state, action)-tuples may have led to that
high value state. These traces of (state, action) pairs, which we refer to as
Recall Traces, sampled from this backtracking model starting from a high value
state, are informative as they terminate in good states, and hence we can use
these traces to improve a policy. We provide a variational interpretation for
this idea and a practical algorithm in which the backtracking model samples
from an approximate posterior distribution over trajectories which lead to
large rewards. Our method improves the sample efficiency of both on- and
off-policy RL algorithms across several environments and tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goyal_A/0/1/0/all/0/1&quot;&gt;Anirudh Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brakel_P/0/1/0/all/0/1&quot;&gt;Philemon Brakel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fedus_W/0/1/0/all/0/1&quot;&gt;William Fedus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lillicrap_T/0/1/0/all/0/1&quot;&gt;Timothy Lillicrap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Larochelle_H/0/1/0/all/0/1&quot;&gt;Hugo Larochelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00445">
<title>On the Computation of Kantorovich-Wasserstein Distances between 2D-Histograms by Uncapacitated Minimum Cost Flows. (arXiv:1804.00445v1 [math.OC])</title>
<link>http://arxiv.org/abs/1804.00445</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a method to compute the Kantorovich distance, that
is, the Wasserstein distance of order one, between a pair of two-dimensional
histograms. Recent works in Computer Vision and Machine Learning have shown the
benefits of measuring Wasserstein distances of order one between histograms
with $N$ bins, by solving a classical transportation problem on (very large)
complete bipartite graphs with $N$ nodes and $N^2$ edges. The main contribution
of our work is to approximate the original transportation problem by an
uncapacitated min cost flow problem on a reduced flow network of size $O(N)$.
More precisely, when the distance among the bin centers is measured with the
1-norm or the $\infty$-norm, our approach provides an optimal solution. When
the distance amongst bins is measured with the 2-norm: (i) we derive a
quantitative estimate on the error between optimal and approximate solution;
(ii) given the error, we construct a reduced flow network of size $O(N)$. We
numerically show the benefits of our approach by computing Wasserstein
distances of order one on a set of grey scale images used as benchmarks in the
literature. We show how our approach scales with the size of the images with
1-norm, 2-norm and $\infty$-norm ground distances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bassetti_F/0/1/0/all/0/1&quot;&gt;Federico Bassetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gualandi_S/0/1/0/all/0/1&quot;&gt;Stefano Gualandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Veneroni_M/0/1/0/all/0/1&quot;&gt;Marco Veneroni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00636">
<title>Recursive Optimization of Convex Risk Measures: Mean-Semideviation Models. (arXiv:1804.00636v1 [math.OC])</title>
<link>http://arxiv.org/abs/1804.00636</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop and analyze stochastic subgradient methods for optimizing a new,
versatile, application-friendly and tractable class of convex risk measures,
termed here as mean-semideviations. Their construction relies on on the concept
of a risk regularizer, a one-dimensional nonlinear map with certain properties,
essentially generalizing the positive part weighting function in the
mean-upper-semideviation risk measure. After we formally introduce
mean-semideviations, we study their basic properties, and we present a
fundamental constructive characterization result, demonstrating their
generality.
&lt;/p&gt;
&lt;p&gt;We then introduce and rigorously analyze the MESSAGEp algorithm, an efficient
stochastic subgradient procedure for iteratively solving convex
mean-semideviation risk-averse problems to optimality. The MESSAGEp algorithm
may be derived as an application of the T-SCGD algorithm of (Yang et al.,
2018). However, the generic theoretical framework of (Yang et al., 2018) is too
narrow and structurally restrictive, as far as optimization of
mean-semideviations is concerned, including the classical
mean-upper-semideviation risk measure. By exploiting problem structure, we
propose a substantially weaker theoretical framework, under which we establish
pathwise convergence of the MESSAGEp algorithm, under the same strong sense as
in (Yang et al., 2018). The new framework reveals a fundamental trade-off
between the smoothness of the random position function and that of the
particular mean-semideviation risk measure under consideration. Further, we
explicitly show that the class of mean-semideviation problems supported under
our framework is strictly larger than the respective class of problems
supported in (Yang et al., 2018). Thus, applicability of compositional
stochastic optimization is established for a strictly wider spectrum of
mean-semideviation problems, justifying the purpose of our work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kalogerias_D/0/1/0/all/0/1&quot;&gt;Dionysios S. Kalogerias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Powell_W/0/1/0/all/0/1&quot;&gt;Warren B. Powell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1603.09638">
<title>Detection under Privileged Information. (arXiv:1603.09638v4 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1603.09638</link>
<description rdf:parseType="Literal">&lt;p&gt;For well over a quarter century, detection systems have been driven by models
learned from input features collected from real or simulated environments. An
artifact (e.g., network event, potential malware sample, suspicious email) is
deemed malicious or non-malicious based on its similarity to the learned model
at runtime. However, the training of the models has been historically limited
to only those features available at runtime. In this paper, we consider an
alternate learning approach that trains models using &quot;privileged&quot;
information--features available at training time but not at runtime--to improve
the accuracy and resilience of detection systems. In particular, we adapt and
extend recent advances in knowledge transfer, model influence, and distillation
to enable the use of forensic or other data unavailable at runtime in a range
of security domains. An empirical evaluation shows that privileged information
increases precision and recall over a system with no privileged information: we
observe up to 7.7% relative decrease in detection error for fast-flux bot
detection, 8.6% for malware traffic detection, 7.3% for malware classification,
and 16.9% for face recognition. We explore the limitations and applications of
different privileged information techniques in detection systems. Such
techniques provide a new means for detection systems to learn from data that
would otherwise not be available at runtime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celik_Z/0/1/0/all/0/1&quot;&gt;Z. Berkay Celik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDaniel_P/0/1/0/all/0/1&quot;&gt;Patrick McDaniel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izmailov_R/0/1/0/all/0/1&quot;&gt;Rauf Izmailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1&quot;&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheatsley_R/0/1/0/all/0/1&quot;&gt;Ryan Sheatsley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_R/0/1/0/all/0/1&quot;&gt;Raquel Alvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swami_A/0/1/0/all/0/1&quot;&gt;Ananthram Swami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.06376">
<title>Fast $\epsilon$-free Inference of Simulation Models with Bayesian Conditional Density Estimation. (arXiv:1605.06376v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1605.06376</link>
<description rdf:parseType="Literal">&lt;p&gt;Many statistical models can be simulated forwards but have intractable
likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer
properties of these models from data. Traditionally these methods approximate
the posterior over parameters by conditioning on data being inside an
$\epsilon$-ball around the observed data, which is only correct in the limit
$\epsilon\!\rightarrow\!0$. Monte Carlo methods can then draw samples from the
approximate posterior to approximate predictions or error bars on parameters.
These algorithms critically slow down as $\epsilon\!\rightarrow\!0$, and in
practice draw samples from a broader distribution than the posterior. We
propose a new approach to likelihood-free inference based on Bayesian
conditional density estimation. Preliminary inferences based on limited
simulation data are used to guide later simulations. In some cases, learning an
accurate parametric representation of the entire true posterior distribution
requires fewer model simulations than Monte Carlo ABC methods need to produce a
single sample from an approximate posterior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papamakarios_G/0/1/0/all/0/1&quot;&gt;George Papamakarios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Murray_I/0/1/0/all/0/1&quot;&gt;Iain Murray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.03228">
<title>Supervised multiway factorization. (arXiv:1609.03228v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1609.03228</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a probabilistic PARAFAC/CANDECOMP (CP) factorization for multiway
(i.e., tensor) data that incorporates auxiliary covariates, SupCP. SupCP
generalizes the supervised singular value decomposition (SupSVD) for
vector-valued observations, to allow for observations that have the form of a
matrix or higher-order array. Such data are increasingly encountered in
biomedical research and other fields. We describe a likelihood-based latent
variable representation of the CP factorization, in which the latent variables
are informed by additional covariates. We give conditions for identifiability,
and develop an EM algorithm for simultaneous estimation of all model
parameters. SupCP can be used for dimension reduction, capturing latent
structures that are more accurate and interpretable due to covariate
supervision. Moreover, SupCP specifies a full probability distribution for a
multiway data observation with given covariate values, which can be used for
predictive modeling. We conduct comprehensive simulations to evaluate the SupCP
algorithm. We apply it to a facial image database with facial descriptors
(e.g., smiling / not smiling) as covariates, and to a study of amino acid
fluorescence. Software is available at https://github.com/lockEF/SupCP .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lock_E/0/1/0/all/0/1&quot;&gt;Eric F. Lock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.03774">
<title>Parallelizing Stochastic Gradient Descent for Least Squares Regression: mini-batching, averaging, and model misspecification. (arXiv:1610.03774v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.03774</link>
<description rdf:parseType="Literal">&lt;p&gt;This work characterizes the benefits of averaging schemes widely used in
conjunction with stochastic gradient descent (SGD). In particular, this work
provides a sharp analysis of: (1) mini-batching, a method of averaging many
samples of a stochastic gradient to both reduce the variance of the stochastic
gradient estimate and for parallelizing SGD and (2) tail-averaging, a method
involving averaging the final few iterates of SGD to decrease the variance in
SGD&apos;s final iterate. This work presents non-asymptotic excess risk bounds for
these schemes for the stochastic approximation problem of least squares
regression.
&lt;/p&gt;
&lt;p&gt;Furthermore, this work establishes a precise problem-dependent extent to
which mini-batch SGD yields provable near-linear parallelization speedups over
SGD with batch size one. This allows for understanding learning rate versus
batch size tradeoffs for the final iterate of an SGD method. These results are
then utilized in providing a highly parallelizable SGD method that obtains the
minimax risk with nearly the same number of serial updates as batch gradient
descent, improving significantly over existing SGD methods. A non-asymptotic
analysis of communication efficient parallelization schemes such as
model-averaging/parameter mixing methods is then provided.
&lt;/p&gt;
&lt;p&gt;Finally, this work sheds light on some fundamental differences in SGD&apos;s
behavior when dealing with agnostic noise in the (non-realizable) least squares
regression problem. In particular, the work shows that the stepsizes that
ensure minimax risk for the agnostic case must be a function of the noise
properties.
&lt;/p&gt;
&lt;p&gt;This paper builds on the operator view of analyzing SGD methods, introduced
by Defossez and Bach (2015), followed by developing a novel analysis in
bounding these operators to characterize the excess risk. These techniques are
of broader interest in analyzing computational aspects of stochastic
approximation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jain_P/0/1/0/all/0/1&quot;&gt;Prateek Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kakade_S/0/1/0/all/0/1&quot;&gt;Sham M. Kakade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kidambi_R/0/1/0/all/0/1&quot;&gt;Rahul Kidambi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Netrapalli_P/0/1/0/all/0/1&quot;&gt;Praneeth Netrapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sidford_A/0/1/0/all/0/1&quot;&gt;Aaron Sidford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01711">
<title>Mode-Seeking Clustering and Density Ridge Estimation via Direct Estimation of Density-Derivative-Ratios. (arXiv:1707.01711v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01711</link>
<description rdf:parseType="Literal">&lt;p&gt;Modes and ridges of the probability density function behind observed data are
useful geometric features. Mode-seeking clustering assigns cluster labels by
associating data samples with the nearest modes, and estimation of density
ridges enables us to find lower-dimensional structures hidden in data. A key
technical challenge both in mode-seeking clustering and density ridge
estimation is accurate estimation of the ratios of the first- and second-order
density derivatives to the density. A naive approach takes a three-step
approach of first estimating the data density, then computing its derivatives,
and finally taking their ratios. However, this three-step approach can be
unreliable because a good density estimator does not necessarily mean a good
density derivative estimator, and division by the estimated density could
significantly magnify the estimation error. To cope with these problems, we
propose a novel estimator for the \emph{density-derivative-ratios}. The
proposed estimator does not involve density estimation, but rather
\emph{directly} approximates the ratios of density derivatives of any order.
Moreover, we establish a convergence rate of the proposed estimator. Based on
the proposed estimator, novel methods both for mode-seeking clustering and
density ridge estimation are developed, and the respective convergence rates to
the mode and ridge of the underlying density are also established. Finally, we
experimentally demonstrate that the developed methods significantly outperform
existing methods, particularly for relatively high-dimensional data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sasaki_H/0/1/0/all/0/1&quot;&gt;Hiroaki Sasaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kanamori_T/0/1/0/all/0/1&quot;&gt;Takafumi Kanamori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hyvarinen_A/0/1/0/all/0/1&quot;&gt;Aapo Hyv&amp;#xe4;rinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Niu_G/0/1/0/all/0/1&quot;&gt;Gang Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.02538">
<title>CuRTAIL: ChaRacterizing and Thwarting AdversarIal deep Learning. (arXiv:1709.02538v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1709.02538</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in adversarial Deep Learning (DL) have opened up a new and
largely unexplored surface for malicious attacks jeopardizing the integrity of
autonomous DL systems. This paper introduces CuRTAIL, a novel end-to-end
computing framework to characterize and thwart potential adversarial attacks
and significantly improve the reliability (safety) of a victim DL model. We
formalize the goal of preventing adversarial attacks as an optimization problem
to minimize the rarely observed regions in the latent feature space spanned by
a DL network. To solve the aforementioned minimization problem, a set of
complementary but disjoint modular redundancies are trained to validate the
legitimacy of the input samples. The proposed countermeasure is unsupervised,
meaning that no adversarial sample is leveraged to train modular redundancies.
This, in turn, ensures the effectiveness of the defense in the face of generic
attacks. We evaluate the robustness of our proposed methodology against the
state-of-the-art adaptive attacks in a white-box setting considering that the
adversary knows everything about the victim model and its defenders. Extensive
evaluations for analyzing MNIST, CIFAR10, and ImageNet data corroborate the
effectiveness of CuRTAIL framework against adversarial samples. The
computations in each modular redundancy can be performed independently of the
other redundancy modules. As such, CuRTAIL detection algorithm can be
completely parallelized among multiple hardware settings to achieve maximum
throughput. We further provide an open-source Application Programming Interface
(API) to facilitate the adoption of the proposed framework for various
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rouhani_B/0/1/0/all/0/1&quot;&gt;Bita Darvish Rouhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samragh_M/0/1/0/all/0/1&quot;&gt;Mohammad Samragh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1&quot;&gt;Tara Javidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1&quot;&gt;Farinaz Koushanfar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06900">
<title>Temporally-Reweighted Chinese Restaurant Process Mixtures for Clustering, Imputing, and Forecasting Multivariate Time Series. (arXiv:1710.06900v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06900</link>
<description rdf:parseType="Literal">&lt;p&gt;This article proposes a Bayesian nonparametric method for forecasting,
imputation, and clustering in sparsely observed, multivariate time series data.
The method is appropriate for jointly modeling hundreds of time series with
widely varying, non-stationary dynamics. Given a collection of $N$ time series,
the Bayesian model first partitions them into independent clusters using a
Chinese restaurant process prior. Within a cluster, all time series are modeled
jointly using a novel &quot;temporally-reweighted&quot; extension of the Chinese
restaurant process mixture. Markov chain Monte Carlo techniques are used to
obtain samples from the posterior distribution, which are then used to form
predictive inferences. We apply the technique to challenging forecasting and
imputation tasks using seasonal flu data from the US Center for Disease Control
and Prevention, demonstrating superior forecasting accuracy and competitive
imputation accuracy as compared to multiple widely used baselines. We further
show that the model discovers interpretable clusters in datasets with hundreds
of time series, using macroeconomic data from the Gapminder Foundation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saad_F/0/1/0/all/0/1&quot;&gt;Feras A. Saad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mansinghka_V/0/1/0/all/0/1&quot;&gt;Vikash K. Mansinghka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07462">
<title>Tracking the gradients using the Hessian: A new look at variance reducing stochastic methods. (arXiv:1710.07462v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07462</link>
<description rdf:parseType="Literal">&lt;p&gt;Our goal is to improve variance reducing stochastic methods through better
control variates. We first propose a modification of SVRG which uses the
Hessian to track gradients over time, rather than to recondition, increasing
the correlation of the control variates and leading to faster theoretical
convergence close to the optimum. We then propose accurate and computationally
efficient approximations to the Hessian, both using a diagonal and a low-rank
matrix. Finally, we demonstrate the effectiveness of our method on a wide range
of problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gower_R/0/1/0/all/0/1&quot;&gt;Robert M. Gower&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Roux_N/0/1/0/all/0/1&quot;&gt;Nicolas Le Roux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis Bach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06969">
<title>Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation. (arXiv:1711.06969v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06969</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Domain Adaptation is a problem of immense importance in computer
vision. Previous approaches showcase the inability of even deep neural networks
to learn informative representations across domain shift. This problem is more
severe for tasks where acquiring hand labeled data is extremely hard and
tedious. In this work, we focus on adapting the representations learned by
segmentation networks across synthetic and real domains. Contrary to previous
approaches that use a simple adversarial objective or superpixel information to
aid the process, we propose an approach based on Generative Adversarial
Networks (GANs) that brings the embeddings closer in the learned feature space.
To showcase the generality and scalability of our approach, we show that we can
achieve state of the art results on two challenging scenarios of synthetic to
real domain adaptation. Additional exploratory experiments show that our
approach: (1) generalizes to unseen domains and (2) results in improved
alignment of source and target distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankaranarayanan_S/0/1/0/all/0/1&quot;&gt;Swami Sankaranarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balaji_Y/0/1/0/all/0/1&quot;&gt;Yogesh Balaji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Arpit Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser Nam Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1&quot;&gt;Rama Chellappa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04667">
<title>Variance reduction via empirical variance minimization: convergence and complexity. (arXiv:1712.04667v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04667</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose and study a generic variance reduction approach. The
proposed method is based on minimization of the empirical variance over a
suitable class of zero mean control functionals. We discuss several
possibilities of constructing zero mean control functionals and present the
corresponding convergence analysis. Finally, a simulation study showing the
numerical efficiency of the proposed approach is presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Belomestny_D/0/1/0/all/0/1&quot;&gt;D. Belomestny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Iosipoi_L/0/1/0/all/0/1&quot;&gt;L. Iosipoi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhivotovskiy_N/0/1/0/all/0/1&quot;&gt;N. Zhivotovskiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09383">
<title>On the Performance of Preconditioned Stochastic Gradient Descent. (arXiv:1803.09383v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09383</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the performance of preconditioned stochastic gradient
descent (PSGD), which can be regarded as an enhance stochastic Newton method
with the ability to handle gradient noise and non-convexity at the same time.
We have improved the implementation of PSGD, unrevealed its relationship to
equilibrated stochastic gradient descent (ESGD) and feature normalization, and
provided a software package (https://github.com/lixilinx/psgd_tf) implemented
in Tensorflow to compare PSGD with four different preconditioners and
variations of stochastic gradient descent (SGD) on a wide range of benchmark
problems with commonly used neural network models, e.g., convolutional and
recurrent neural networks. Comparison results clearly demonstrate the
advantages of PSGD in terms of convergence speeds and generalization
performances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xi-Lin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10840">
<title>Defending against Adversarial Images using Basis Functions Transformations. (arXiv:1803.10840v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10840</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the effectiveness of various approaches that defend against
adversarial attacks on deep networks via manipulations based on basis function
representations of images. Specifically, we experiment with low-pass filtering,
PCA, JPEG compression, low resolution wavelet approximation, and
soft-thresholding. We evaluate these defense techniques using three types of
popular attacks in black, gray and white-box settings. Our results show JPEG
compression tends to outperform the other tested defenses in most of the
settings considered, in addition to soft-thresholding, which performs well in
specific cases, and yields a more mild decrease in accuracy on benign examples.
In addition, we also mathematically derive a novel white-box attack in which
the adversarial perturbation is composed only of terms corresponding a to
pre-determined subset of the basis functions, of which a &quot;low frequency attack&quot;
is a special case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shaham_U/0/1/0/all/0/1&quot;&gt;Uri Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garritano_J/0/1/0/all/0/1&quot;&gt;James Garritano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yamada_Y/0/1/0/all/0/1&quot;&gt;Yutaro Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weinberger_E/0/1/0/all/0/1&quot;&gt;Ethan Weinberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cloninger_A/0/1/0/all/0/1&quot;&gt;Alex Cloninger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiuyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stanton_K/0/1/0/all/0/1&quot;&gt;Kelly Stanton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kluger_Y/0/1/0/all/0/1&quot;&gt;Yuval Kluger&lt;/a&gt;</dc:creator>
</item></rdf:RDF>