<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-31T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12270"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12352"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12388"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.08034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12183"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12279"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12387"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12495"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.08224"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05746"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10907"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10055"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09197"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11548"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12168"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12218"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12243"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12244"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12254"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12296"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12298"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12313"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12317"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12321"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12324"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12332"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12421"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12467"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12472"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12528"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12529"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12547"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12549"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12573"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.08159"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.00090"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.07193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01799"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04170"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04846"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02312"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08591"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09202"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04918"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11221"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11454"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11959"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12111"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.12270">
<title>Optimized Participation of Multiple Fusion Functions in Consensus Creation: An Evolutionary Approach. (arXiv:1805.12270v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.12270</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies show that ensemble methods enhance the stability and
robustness of unsupervised learning. These approaches are successfully utilized
to construct multiple clustering and combine them into a one representative
consensus clustering of an improved quality. The quality of the consensus
clustering is directly depended on fusion functions used in combination. In
this article, the hierarchical clustering ensemble techniques are extended by
introducing a new evolutionary fusion function. In the proposed method,
multiple hierarchical clustering methods are generated via bagging. Thereafter,
the consensus clustering is obtained using the search capability of genetic
algorithm among different aggregated clustering methods made by different
fusion functions. Putting some popular data sets to empirical study, the
quality of the proposed method is compared with regular clustering ensembles.
Experimental results demonstrate the accuracy improvement of the aggregated
clustering results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashedi_E/0/1/0/all/0/1&quot;&gt;Elaheh Rashedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirzaei_A/0/1/0/all/0/1&quot;&gt;Abdolreza Mirzaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12352">
<title>DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder. (arXiv:1805.12352v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.12352</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational autoencoders (VAEs) have shown a promise in data-driven
conversation modeling. However, most VAE conversation models match the
approximate posterior distribution over the latent variables to a simple prior
such as standard normal distribution, thereby restricting the generated
responses to a relatively simple (e.g., single-modal) scope. In this paper, we
propose DialogWAE, a conditional Wasserstein autoencoder (WAE) specially
designed for dialogue modeling. Unlike VAEs that impose a simple distribution
over the latent variables, DialogWAE models the distribution of data by
training a GAN within the latent variable space. Specifically, our model
samples from the prior and posterior distributions over the latent variables by
transforming context-dependent random noise using neural networks and minimizes
the Wasserstein distance between the two distributions. We further develop a
Gaussian mixture prior network to enrich the latent space. Experiments on two
widely-used datasets show that DialogWAE outperforms the state-of-the-art
approaches in generating more coherent, informative and diverse responses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1&quot;&gt;Jungwoo Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sunghun Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12388">
<title>Sample Reuse via Importance Sampling in Information Geometric Optimization. (arXiv:1805.12388v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1805.12388</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose a technique to reduce the number of function
evaluations, which is often the bottleneck of the black-box optimization, in
the information geometric optimization (IGO) that is a generic framework of the
probability model-based black-box optimization algorithms and generalizes
several well-known evolutionary algorithms, such as the population-based
incremental learning (PBIL) and the pure rank-$\mu$ update covariance matrix
adaptation evolution strategy (CMA-ES). In each iteration, the IGO algorithms
update the parameters of the probability distribution to the natural gradient
direction estimated by Monte-Carlo with the samples drawn from the current
distribution. Our strategy is to reuse previously generated and evaluated
samples based on the importance sampling. It is a technique to reduce the
estimation variance without introducing a bias in Monte-Carlo estimation. We
apply the sample reuse technique to the PBIL and the pure rank-$\mu$ update
CMA-ES and empirically investigate its effect. The experimental results show
that the sample reuse helps to reduce the number of function evaluations on
many benchmark functions for both the PBIL and the pure rank-$\mu$ update
CMA-ES. Moreover, we demonstrate how to combine the importance sampling
technique with a variant of the CMA-ES involving an algorithmic component that
is not derived in the IGO framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirakawa_S/0/1/0/all/0/1&quot;&gt;Shinichi Shirakawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akimoto_Y/0/1/0/all/0/1&quot;&gt;Youhei Akimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouchi_K/0/1/0/all/0/1&quot;&gt;Kazuki Ouchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohara_K/0/1/0/all/0/1&quot;&gt;Kouzou Ohara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.08034">
<title>Never look back - A modified EnKF method and its application to the training of neural networks without back propagation. (arXiv:1805.08034v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/1805.08034</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a new derivative-free optimization method and
investigate its use for training neural networks. Our method is motivated by
the Ensemble Kalman Filter (EnKF), which has been used successfully for solving
optimization problems that involve large-scale, highly nonlinear dynamical
systems. A key benefit of the EnKF method is that it requires only the
evaluation of the forward propagation but not its derivatives. Hence, in the
context of neural networks, it alleviates the need for back propagation and
reduces the memory consumption dramatically. However, the method is not a pure
&quot;black-box&quot; global optimization heuristic as it efficiently utilizes the
structure of typical learning problems. Promising first results of the EnKF for
training deep neural networks have been presented recently by Kovachki and
Stuart. We propose an important modification of the EnKF that enables us to
prove convergence of our method to the minimizer of a strongly convex function.
Our method also bears similarity with implicit filtering and we demonstrate its
potential for minimizing highly oscillatory functions using a simple example.
Further, we provide numerical examples that demonstrate the potential of our
method for training deep neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Haber_E/0/1/0/all/0/1&quot;&gt;Eldad Haber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lucka_F/0/1/0/all/0/1&quot;&gt;Felix Lucka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ruthotto_L/0/1/0/all/0/1&quot;&gt;Lars Ruthotto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12183">
<title>Context Exploitation using Hierarchical Bayesian Models. (arXiv:1805.12183v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.12183</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of how to improve automatic target recognition by
fusing the naive sensor-level classification decisions with &quot;intuition,&quot; or
context, in a mathematically principled way. This is a general approach that is
compatible with many definitions of context, but for specificity, we consider
context as co-occurrence in imagery. In particular, we consider images that
contain multiple objects identified at various confidence levels. We learn the
patterns of co-occurrence in each context, then use these patterns as
hyper-parameters for a Hierarchical Bayesian Model. The result is that
low-confidence sensor classification decisions can be dramatically improved by
fusing those readings with context. We further use hyperpriors to address the
case where multiple contexts may be appropriate. We also consider the Bayesian
Network, an alternative to the Hierarchical Bayesian Model, which is
computationally more efficient but assumes that context and sensor readings are
uncorrelated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+George_C/0/1/0/all/0/1&quot;&gt;Christopher A. George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1&quot;&gt;Pranab Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_K/0/1/0/all/0/1&quot;&gt;Kendra E. Moore&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12279">
<title>Bayesian Pose Graph Optimization via Bingham Distributions and Tempered Geodesic MCMC. (arXiv:1805.12279v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.12279</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Tempered Geodesic Markov Chain Monte Carlo (TG-MCMC) algorithm
for initializing pose graph optimization problems, arising in various scenarios
such as SFM (structure from motion) or SLAM (simultaneous localization and
mapping). TG-MCMC is first of its kind as it unites asymptotically global
non-convex optimization on the spherical manifold of quaternions with posterior
sampling, in order to provide both reliable initial poses and uncertainty
estimates that are informative about the quality of individual solutions. We
devise rigorous theoretical convergence guarantees for our method and
extensively evaluate it on synthetic and real benchmark datasets. Besides its
elegance in formulation and theory, we show that our method is robust to
missing data, noise and the estimated uncertainties capture intuitive
properties of the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1&quot;&gt;Tolga Birdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simsekli_U/0/1/0/all/0/1&quot;&gt;Umut &amp;#x15e;im&amp;#x15f;ekli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eken_M/0/1/0/all/0/1&quot;&gt;M. Onur Eken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1&quot;&gt;Slobodan Ilic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12316">
<title>Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data. (arXiv:1805.12316v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12316</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a probabilistic framework for studying adversarial attacks on
discrete data. Based on this framework, we derive a perturbation-based method,
Greedy Attack, and a scalable learning-based method, Gumbel Attack, that
illustrate various tradeoffs in the design of attacks. We demonstrate the
effectiveness of these methods using both quantitative metrics and human
evaluation on various state-of-the-art models for text classification,
including a word-based CNN, a character-based CNN and an LSTM. As as example of
our results, we show that the accuracy of character-based convolutional
networks drops to the level of random selection by modifying only five
characters through Greedy Attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1&quot;&gt;Puyudi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianbo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jane-Ling Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12387">
<title>Agents and Devices: A Relative Definition of Agency. (arXiv:1805.12387v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12387</link>
<description rdf:parseType="Literal">&lt;p&gt;According to Dennett, the same system may be described using a `physical&apos;
(mechanical) explanatory stance, or using an `intentional&apos; (belief- and
goal-based) explanatory stance. Humans tend to find the physical stance more
helpful for certain systems, such as planets orbiting a star, and the
intentional stance for others, such as living animals. We define a formal
counterpart of physical and intentional stances within computational theory: a
description of a system as either a device, or an agent, with the key
difference being that `devices&apos; are directly described in terms of an
input-output mapping, while `agents&apos; are described in terms of the function
they optimise. Bayes&apos; rule can then be applied to calculate the subjective
probability of a system being a device or an agent, based only on its
behaviour. We illustrate this using the trajectories of an object in a toy
grid-world domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orseau_L/0/1/0/all/0/1&quot;&gt;Laurent Orseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGill_S/0/1/0/all/0/1&quot;&gt;Simon McGregor McGill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legg_S/0/1/0/all/0/1&quot;&gt;Shane Legg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12495">
<title>Structural Isomprphism in Mathematical Expressions: A Simple Coding Scheme. (arXiv:1805.12495v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.12495</link>
<description rdf:parseType="Literal">&lt;p&gt;While there exist many methods in machine learning for comparison of letter
string data, most are better equipped to handle strings that represent natural
language, and their performance will not hold up when presented with strings
that correspond to mathematical expressions. Based on the graphical
representation of the expression tree, here I propose a simple method for
encoding such expressions that is only sensitive to their structural
properties, and invariant to the specifics which can vary between two seemingly
different, but semantically similar mathematical expressions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahbazi_R/0/1/0/all/0/1&quot;&gt;Reza Shahbazi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.08224">
<title>Punny Captions: Witty Wordplay in Image Descriptions. (arXiv:1704.08224v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1704.08224</link>
<description rdf:parseType="Literal">&lt;p&gt;Wit is a form of rich interaction that is often grounded in a specific
situation (e.g., a comment in response to an event). In this work, we attempt
to build computational models that can produce witty descriptions for a given
image. Inspired by a cognitive account of humor appreciation, we employ
linguistic wordplay, specifically puns, in image descriptions. We develop two
approaches which involve retrieving witty descriptions for a given image from a
large corpus of sentences, or generating them via an encoder-decoder neural
network architecture. We compare our approach against meaningful baseline
approaches via human studies and show substantial improvements. We find that
when a human is subject to similar constraints as the model regarding word
usage and style, people vote the image descriptions generated by our model to
be slightly wittier than human-written witty descriptions. Unsurprisingly,
humans are almost always wittier than the model when they are free to choose
the vocabulary, style, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1&quot;&gt;Arjun Chandrasekaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1&quot;&gt;Devi Parikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05746">
<title>Adversarial Discriminative Sim-to-real Transfer of Visuo-motor Policies. (arXiv:1709.05746v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05746</link>
<description rdf:parseType="Literal">&lt;p&gt;Various approaches have been proposed to learn visuo-motor policies for
real-world robotic applications. One solution is first learning in simulation
then transferring to the real world. In the transfer, most existing approaches
need real-world images with labels. However, the labelling process is often
expensive or even impractical in many robotic applications. In this paper, we
propose an adversarial discriminative sim-to-real transfer approach to reduce
the cost of labelling real data. The effectiveness of the approach is
demonstrated with modular networks in a table-top object reaching task where a
7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter
through visual observations. The adversarial transfer approach reduced the
labelled real data requirement by 50%. Policies can be transferred to real
environments with only 93 labelled and 186 unlabelled real images. The
transferred visuo-motor policies are robust to novel (not seen in training)
objects in clutter and even a moving target, achieving a 97.8% success rate and
1.8 cm control accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fangyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leitner_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Leitner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zongyuan Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1&quot;&gt;Michael Milford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corke_P/0/1/0/all/0/1&quot;&gt;Peter Corke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10907">
<title>Deep Reinforcement Learning for De-Novo Drug Design. (arXiv:1711.10907v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10907</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel computational strategy for de novo design of molecules
with desired properties termed ReLeaSE (Reinforcement Learning for Structural
Evolution). Based on deep and reinforcement learning approaches, ReLeaSE
integrates two deep neural networks - generative and predictive - that are
trained separately but employed jointly to generate novel targeted chemical
libraries. ReLeaSE employs simple representation of molecules by their SMILES
strings only. Generative models are trained with stack-augmented memory network
to produce chemically feasible SMILES strings, and predictive models are
derived to forecast the desired properties of the de novo generated compounds.
In the first phase of the method, generative and predictive models are trained
separately with a supervised learning algorithm. In the second phase, both
models are trained jointly with the reinforcement learning approach to bias the
generation of new chemical structures towards those with the desired physical
and/or biological properties. In the proof-of-concept study, we have employed
the ReLeaSE method to design chemical libraries with a bias toward structural
complexity or biased toward compounds with either maximal, minimal, or specific
range of physical properties such as melting point or hydrophobicity, as well
as to develop novel putative inhibitors of JAK2. The approach proposed herein
can find a general use for generating targeted chemical libraries of novel
compounds optimized for either a single desired property or multiple
properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popova_M/0/1/0/all/0/1&quot;&gt;Mariya Popova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isayev_O/0/1/0/all/0/1&quot;&gt;Olexandr Isayev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tropsha_A/0/1/0/all/0/1&quot;&gt;Alexander Tropsha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10055">
<title>Features, Projections, and Representation Change for Generalized Planning. (arXiv:1801.10055v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.10055</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized planning is concerned with the characterization and computation
of plans that solve many instances at once. In the standard formulation, a
generalized plan is a mapping from feature or observation histories into
actions, assuming that the instances share a common pool of features and
actions. This assumption, however, excludes the standard relational planning
domains where actions and objects change across instances. In this work, we
extend the standard formulation of generalized planning to such domains. This
is achieved by projecting the actions over the features, resulting in a common
set of abstract actions which can be tested for soundness and completeness, and
which can be used for generating general policies such as &quot;if the gripper is
empty, pick the clear block above x and place it on the table&quot; that achieve the
goal clear(x) in any Blocksworld instance. In this policy, &quot;pick the clear
block above x&quot; is an abstract action that may represent the action Unstack(a,
b) in one situation and the action Unstack(b, c) in another. Transformations
are also introduced for computing such policies by means of fully observable
non-deterministic (FOND) planners. The value of generalized representations for
learning general policies is also discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonet_B/0/1/0/all/0/1&quot;&gt;Blai Bonet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geffner_H/0/1/0/all/0/1&quot;&gt;Hector Geffner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09197">
<title>ASR-based Features for Emotion Recognition: A Transfer Learning Approach. (arXiv:1805.09197v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09197</link>
<description rdf:parseType="Literal">&lt;p&gt;During the last decade, the applications of signal processing have
drastically improved with deep learning. However areas of affecting computing
such as emotional speech synthesis or emotion recognition from spoken language
remains challenging. In this paper, we investigate the use of a neural
Automatic Speech Recognition (ASR) as a feature extractor for emotion
recognition. We show that these features outperform the eGeMAPS feature set to
predict the valence and arousal emotional dimensions, which means that the
audio-to-text mapping learning by the ASR system contain information related to
the emotional dimensions in spontaneous speech. We also examine the
relationship between first layers (closer to speech) and last layers (closer to
text) of the ASR and valence/arousal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tits_N/0/1/0/all/0/1&quot;&gt;No&amp;#xe9; Tits&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haddad_K/0/1/0/all/0/1&quot;&gt;Kevin El Haddad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dutoit_T/0/1/0/all/0/1&quot;&gt;Thierry Dutoit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11548">
<title>The Actor Search Tree Critic (ASTC) for Off-Policy POMDP Learning in Medical Decision Making. (arXiv:1805.11548v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11548</link>
<description rdf:parseType="Literal">&lt;p&gt;Off-policy reinforcement learning enables near-optimal policy from suboptimal
experience, thereby provisions opportunity for artificial intelligence
applications in healthcare. Previous works have mainly framed patient-clinician
interactions as Markov decision processes, while true physiological states are
not necessarily fully observable from clinical data. We capture this situation
with partially observable Markov decision process, in which an agent optimises
its actions in a belief represented as a distribution of patient states
inferred from individual history trajectories. A Gaussian mixture model is
fitted for the observed data. Moreover, we take into account the fact that
nuance in pharmaceutical dosage could presumably result in significantly
different effect by modelling a continuous policy through a Gaussian
approximator directly in the policy space, i.e. the actor. To address the
challenge of infinite number of possible belief states which renders exact
value iteration intractable, we evaluate and plan for only every encountered
belief, through heuristic search tree by tightly maintaining lower and upper
bounds of the true value of belief. We further resort to function
approximations to update value bounds estimation, i.e. the critic, so that the
tree search can be improved through more compact bounds at the fringe nodes
that will be back-propagated to the root. Both actor and critic parameters are
learned via gradient-based approaches. Our proposed policy trained from real
intensive care unit data is capable of dictating dosing on vasopressors and
intravenous fluids for sepsis patients that lead to the best patient outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Luchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komorowski_M/0/1/0/all/0/1&quot;&gt;Matthieu Komorowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faisal_A/0/1/0/all/0/1&quot;&gt;Aldo A. Faisal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12168">
<title>A Flexible Multi-Objective Bayesian Optimization Approach using Random Scalarizations. (arXiv:1805.12168v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12168</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real world applications can be framed as multi-objective optimization
problems, where we wish to simultaneously optimize for multiple criteria.
Bayesian optimization techniques for the multi-objective setting are pertinent
when the evaluation of the functions in question are expensive. Traditional
methods for multi-objective optimization, both Bayesian and otherwise, are
aimed at recovering the Pareto front of these objectives. However, we argue
that recovering the entire Pareto front may not be aligned with our goals in
practice. For example, while a practitioner might desire to identify Pareto
optimal points, she may wish to focus only on a particular region of the Pareto
front due to external considerations. In this work we propose an approach based
on random scalarizations of the objectives. We demonstrate that our approach
can focus its sampling on certain regions of the Pareto front while being
flexible enough to sample from the entire Pareto front if required.
Furthermore, our approach is less computationally demanding compared to other
existing approaches. In this paper, we also analyse a notion of regret in the
multi-objective setting and obtain sublinear regret bounds. We compare the
proposed approach to other state-of-the-art approaches on both synthetic and
real-life experiments. The results demonstrate superior performance of our
proposed algorithm in terms of flexibility, scalability and regret.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paria_B/0/1/0/all/0/1&quot;&gt;Biswajit Paria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kandasamy_K/0/1/0/all/0/1&quot;&gt;Kirthevasan Kandasamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnab&amp;#xe1;s P&amp;#xf3;czos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12218">
<title>Recurrent Deep Embedding Networks for Genotype Clustering and Ethnicity Prediction. (arXiv:1805.12218v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12218</link>
<description rdf:parseType="Literal">&lt;p&gt;The understanding of variations in genome sequences assists us in identifying
people who are predisposed to common diseases, solving rare diseases, and
finding the corresponding population group of the individuals from a larger
population group. Although classical machine learning techniques allow
researchers to identify groups (i.e. clusters) of related variables, the
accuracy, and effectiveness of these methods diminish for large and
high-dimensional datasets such as the whole human genome. On the other hand,
deep neural network architectures (the core of deep learning) can better
exploit large-scale datasets to build complex models. In this paper, we use the
K-means clustering approach for scalable genomic data analysis aiming towards
clustering genotypic variants at the population scale. Finally, we train a deep
belief network (DBN) for predicting the geographic ethnicity. We used the
genotype data from the 1000 Genomes Project, which covers the result of genome
sequencing for 2504 individuals from 26 different ethnic origins and comprises
84 million variants. Our experimental results, with a focus on accuracy and
scalability, show the effectiveness and superiority compared to the
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1&quot;&gt;Md. Rezaul Karim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1&quot;&gt;Michael Cochez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beyan_O/0/1/0/all/0/1&quot;&gt;Oya Deniz Beyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zappa_A/0/1/0/all/0/1&quot;&gt;Achille Zappa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahay_R/0/1/0/all/0/1&quot;&gt;Ratnesh Sahay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1&quot;&gt;Stefan Decker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuhmann_D/0/1/0/all/0/1&quot;&gt;Dietrich-Rebholz Schuhmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12243">
<title>Novel Video Prediction for Large-scale Scene using Optical Flow. (arXiv:1805.12243v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.12243</link>
<description rdf:parseType="Literal">&lt;p&gt;Making predictions of future frames is a critical challenge in autonomous
driving research. Most of the existing methods for video prediction attempt to
generate future frames in simple and fixed scenes. In this paper, we propose a
novel and effective optical flow conditioned method for the task of video
prediction with an application to complex urban scenes. In contrast with
previous work, the prediction model only requires video sequences and optical
flow sequences for training and testing. Our method uses the rich
spatial-temporal features in video sequences. The method takes advantage of the
motion information extracting from optical flow maps between neighbor images as
well as previous images. Empirical evaluations on the KITTI dataset and the
Cityscapes dataset demonstrate the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Henglai Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiaochuan Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1&quot;&gt;Penghong Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12244">
<title>Mining gold from implicit models to improve likelihood-free inference. (arXiv:1805.12244v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.12244</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulators often provide the best description of real-world phenomena;
however, they also lead to challenging inverse problems because the density
they implicitly define is often intractable. We present a new suite of
simulation-based inference techniques that go beyond the traditional
Approximate Bayesian Computation approach, which struggles in a
high-dimensional setting, and extend methods that use surrogate models based on
neural networks. We show that additional information, such as the joint
likelihood ratio and the joint score, can often be extracted from simulators
and used to augment the training data for these surrogate models. Finally, we
demonstrate that these new techniques are more sample efficient and provide
higher-fidelity inference than traditional methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brehmer_J/0/1/0/all/0/1&quot;&gt;Johann Brehmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Louppe_G/0/1/0/all/0/1&quot;&gt;Gilles Louppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pavez_J/0/1/0/all/0/1&quot;&gt;Juan Pavez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cranmer_K/0/1/0/all/0/1&quot;&gt;Kyle Cranmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12254">
<title>Multi-Resolution 3D Convolutional Neural Networks for Object Recognition. (arXiv:1805.12254v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.12254</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from 3D Data is a fascinating idea which is well explored and
studied in computer vision. This allows one to learn from very sparse LiDAR
data, point cloud data as well as 3D objects in terms of CAD models and
surfaces etc. Most of the approaches to learn from such data are limited to
uniform 3D volume occupancy grids or octree representations. A major challenge
in learning from 3D data is that one needs to define a proper resolution to
represent it in a voxel grid and this becomes a bottleneck for the learning
algorithms. Specifically, while we focus on learning from 3D data, a fine
resolution is very important to capture key features in the object and at the
same time the data becomes sparser as the resolution becomes finer. There are
numerous applications in computer vision where a multi-resolution
representation is used instead of a uniform grid representation in order to
make the applications memory efficient. Though such methods are difficult to
learn from, they are much more efficient in representing 3D data. In this
paper, we explore the challenges in learning from such data representation. In
particular, we use a multi-level voxel representation where we define a coarse
voxel grid that contains information of important voxels(boundary voxels) and
multiple fine voxel grids corresponding to each significant voxel of the coarse
grid. A multi-level voxel representation can capture important features in the
3D data in a memory efficient way in comparison to an octree representation.
Consequently, learning from a 3D object with high resolution, which is
paramount in feature recognition, is made efficient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghadai_S/0/1/0/all/0/1&quot;&gt;Sambit Ghadai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_X/0/1/0/all/0/1&quot;&gt;Xian Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balu_A/0/1/0/all/0/1&quot;&gt;Aditya Balu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumik Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1&quot;&gt;Adarsh Krishnamurthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12296">
<title>Root-cause Analysis for Time-series Anomalies via Spatiotemporal Graphical Modeling in Distributed Complex Systems. (arXiv:1805.12296v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.12296</link>
<description rdf:parseType="Literal">&lt;p&gt;Performance monitoring, anomaly detection, and root-cause analysis in complex
cyber-physical systems (CPSs) are often highly intractable due to widely
diverse operational modes, disparate data types, and complex fault propagation
mechanisms. This paper presents a new data-driven framework for root-cause
analysis, based on a spatiotemporal graphical modeling approach built on the
concept of symbolic dynamics for discovering and representing causal
interactions among sub-systems of complex CPSs. We formulate the root-cause
analysis problem as a minimization problem via the proposed inference based
metric and present two approximate approaches for root-cause analysis, namely
the sequential state switching ($S^3$, based on free energy concept of a
restricted Boltzmann machine, RBM) and artificial anomaly association ($A^3$, a
classification framework using deep neural networks, DNN). Synthetic data from
cases with failed pattern(s) and anomalous node(s) are simulated to validate
the proposed approaches. Real dataset based on Tennessee Eastman process (TEP)
is also used for comparison with other approaches. The results show that: (1)
$S^3$ and $A^3$ approaches can obtain high accuracy in root-cause analysis
under both pattern-based and node-based fault scenarios, in addition to
successfully handling multiple nominal operating modes, (2) the proposed
tool-chain is shown to be scalable while maintaining high accuracy, and (3) the
proposed framework is robust and adaptive in different fault conditions and
performs better in comparison with the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lore_K/0/1/0/all/0/1&quot;&gt;Kin Gwn Lore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhanhong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumik Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12298">
<title>Evaluating Reinforcement Learning Algorithms in Observational Health Settings. (arXiv:1805.12298v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12298</link>
<description rdf:parseType="Literal">&lt;p&gt;Much attention has been devoted recently to the development of machine
learning algorithms with the goal of improving treatment policies in
healthcare. Reinforcement learning (RL) is a sub-field within machine learning
that is concerned with learning how to make sequences of decisions so as to
optimize long-term effects. Already, RL algorithms have been proposed to
identify decision-making strategies for mechanical ventilation, sepsis
management and treatment of schizophrenia. However, before implementing
treatment policies learned by black-box algorithms in high-stakes clinical
decision problems, special care must be taken in the evaluation of these
policies.
&lt;/p&gt;
&lt;p&gt;In this document, our goal is to expose some of the subtleties associated
with evaluating RL algorithms in healthcare. We aim to provide a conceptual
starting point for clinical and computational researchers to ask the right
questions when designing and evaluating algorithms for new ways of treating
patients. In the following, we describe how choices about how to summarize a
history, variance of statistical estimators, and confounders in more ad-hoc
measures can result in unreliable, even misleading estimates of the quality of
a treatment policy. We also provide suggestions for mitigating these
effects---for while there is much promise for mining observational health data
to uncover better treatment policies, evaluation must be performed
thoughtfully.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottesman_O/0/1/0/all/0/1&quot;&gt;Omer Gottesman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansson_F/0/1/0/all/0/1&quot;&gt;Fredrik Johansson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_J/0/1/0/all/0/1&quot;&gt;Joshua Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dent_J/0/1/0/all/0/1&quot;&gt;Jack Dent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Donghun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1&quot;&gt;Srivatsan Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yi Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wihl_D/0/1/0/all/0/1&quot;&gt;David Wihl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xuefeng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jiayu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lage_I/0/1/0/all/0/1&quot;&gt;Isaac Lage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mosch_C/0/1/0/all/0/1&quot;&gt;Christopher Mosch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehman_L/0/1/0/all/0/1&quot;&gt;Li-wei H. Lehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komorowski_M/0/1/0/all/0/1&quot;&gt;Matthieu Komorowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komorowski_M/0/1/0/all/0/1&quot;&gt;Matthieu Komorowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faisal_A/0/1/0/all/0/1&quot;&gt;Aldo Faisal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1&quot;&gt;Leo Anthony Celi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1&quot;&gt;David Sontag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1&quot;&gt;Finale Doshi-Velez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12313">
<title>Conformation Clustering of Long MD Protein Dynamics with an Adversarial Autoencoder. (arXiv:1805.12313v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1805.12313</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments in specialized computer hardware have greatly accelerated
atomic level Molecular Dynamics (MD) simulations. A single GPU-attached cluster
is capable of producing microsecond-length trajectories in reasonable amounts
of time. Multiple protein states and a large number of microstates associated
with folding and with the function of the protein can be observed as
conformations sampled in the trajectories. Clustering those conformations,
however, is needed for identifying protein states, evaluating transition rates
and understanding protein behavior. In this paper, we propose a novel
data-driven generative conformation clustering method based on the adversarial
autoencoder (AAE) and provide the associated software implementation Cong. The
method was tested using a 208 microseconds MD simulation of the fast-folding
peptide Trp-Cage (20 residues) obtained from the D.E. Shaw Research Group. The
proposed clustering algorithm identifies many of the salient features of the
folding process by grouping a large number of conformations that share common
features not easily identifiable in the trajectory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunlong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Amzel_L/0/1/0/all/0/1&quot;&gt;L. Mario Amzel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12317">
<title>Multiaccuracy: Black-Box Post-Processing for Fairness in Classification. (arXiv:1805.12317v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12317</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning predictors are successfully deployed in applications ranging
from disease diagnosis, to predicting credit scores, to image recognition. Even
when the overall accuracy is high, the predictions often have systematic biases
that harm specific subgroups, especially for subgroups that are minorities in
the training data. We develop a rigorous framework of multiaccuracy auditing
and post-processing to improve predictor accuracies across identifiable
subgroups. Our algorithm, MultiaccuracyBoost, works in any setting where we
have black-box access to a predictor and a relatively small set of labeled data
for auditing. We prove guarantees on the convergence rate of the algorithm and
show that it improves overall accuracy at each step. Importantly, if the
initial model is accurate on an identifiable subgroup, then the post-processed
model will be also. We demonstrate the effectiveness of this approach on
diverse applications in image classification, finance, and population health.
MultiaccuracyBoost can improve subpopulation accuracy (e.g. for `black women&apos;)
even when the sensitive features (e.g. `race&apos;, `gender&apos;) are not known to the
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Michael P. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorbani_A/0/1/0/all/0/1&quot;&gt;Amirata Ghorbani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12321">
<title>Geometric Active Learning via Enclosing Ball Boundary. (arXiv:1805.12321v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12321</link>
<description rdf:parseType="Literal">&lt;p&gt;Active Learning (AL) requires learners to retrain the classifier with the
minimum human supervisions or labeling in the unlabeled data pool when the
current training set is not enough. However, general AL sampling strategies
with a few label support inevitably suffer from performance decrease. To
identify which samples determine the performance of the classification
hyperplane, Core Vector Machine (CVM) and Ball Vector Machine (BVM) use the
geometry boundary points of each Minimum Enclosing Ball (MEB) to train the
classification hypothesis. Their theoretical analysis and experimental results
show that the improved classifiers not only converge faster but also obtain
higher accuracies compared with Support Vector Machine (SVM). Inspired by this,
we formulate the cluster boundary point detection issue as the MEB boundary
problem after presenting a convincing proof of this observation. Because the
enclosing ball boundary may have a high fitting ratio when it can not enclose
the class tightly, we split the global ball problem into two kinds of small
Local Minimum Enclosing Ball (LMEB): Boundary ball (B-ball) and Core ball
(C-ball) to tackle its over-fitting problem. Through calculating the update of
radius and center when extending the local ball space, we adopt the minimum
update ball to obtain the geometric update optimization scheme of B-ball and
C-ball. After proving their update relationship, we design the LEB (Local
Enclosing Ball) algorithm using centers of B-ball of each class to detect the
enclosing ball boundary points for AL sampling. Experimental and theoretical
studies have shown that the classification accuracy, time, and space
performance of our proposed method significantly are superior than the
state-of-the-art algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1&quot;&gt;Ivor W. Tsang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jianliang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zenglin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guandong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12324">
<title>Metric on Nonlinear Dynamical Systems with Koopman Operators. (arXiv:1805.12324v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.12324</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of a metric for structural data is a long-term problem in
pattern recognition and machine learning. In this paper, we develop a general
metric for comparing nonlinear dynamical systems that is defined with Koopman
operator in reproducing kernel Hilbert spaces. Our metric includes the existing
fundamental metrics for dynamical systems, which are basically defined with
principal angles between some appropriately-chosen subspaces, as its special
cases. We also describe the estimation of our metric from finite data. We
empirically illustrate our metric with an example of rotation dynamics in a
unit disk in a complex plane, and evaluate the performance with real-world
time-series data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ishikawa_I/0/1/0/all/0/1&quot;&gt;Isao Ishikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fujii_K/0/1/0/all/0/1&quot;&gt;Keisuke Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ikeda_M/0/1/0/all/0/1&quot;&gt;Masahiro Ikeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hashimoto_Y/0/1/0/all/0/1&quot;&gt;Yuka Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kawahara_Y/0/1/0/all/0/1&quot;&gt;Yoshinobu Kawahara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12332">
<title>On representation power of neural network-based graph embedding and beyond. (arXiv:1805.12332v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.12332</link>
<description rdf:parseType="Literal">&lt;p&gt;The representation power of similarity functions used in neural network-based
graph embedding is considered. The inner product similarity (IPS) with feature
vectors computed via neural networks is commonly used for representing the
strength of association between two nodes. However, only a little work has been
done on the representation capability of IPS. A very recent work shed light on
the nature of IPS and reveals that IPS has the capability of approximating any
positive definite (PD) similarities. However, a simple example demonstrates the
fundamental limitation of IPS to approximate non-PD similarities. We then
propose a novel model named Shifted IPS (SIPS) that approximates any
Conditionally PD (CPD) similarities arbitrary well. CPD is a generalization of
PD with many examples such as negative Poincare distance and negative
Wasserstein distance, thus SIPS has a potential impact to significantly improve
the applicability of graph embedding without taking great care in configuring
the similarity function. Our numerical experiments demonstrate the SIPS&apos;s
superiority over IPS. In theory, we further extend SIPS beyond CPD by
considering the inner product in Minkowski space so that it approximates more
general similarities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Okuno_A/0/1/0/all/0/1&quot;&gt;Akifumi Okuno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shimodaira_H/0/1/0/all/0/1&quot;&gt;Hidetoshi Shimodaira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12421">
<title>HOPF: Higher Order Propagation Framework for Deep Collective Classification. (arXiv:1805.12421v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12421</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a graph wherein every node has certain attributes associated with it
and some nodes have labels associated with them, Collective Classification (CC)
is the task of assigning labels to every unlabeled node using information from
the node as well as its neighbors. It is often the case that a node is not only
influenced by its immediate neighbors but also by its higher order neighbors,
multiple hops away. Recent state-of-the-art models for CC use differentiable
variations of Weisfeiler-Lehman kernels to aggregate multi-hop neighborhood
information. However, in this work, we show that these models suffer from the
problem of Node Information Morphing wherein the information of the node is
morphed or overwhelmed by the information of its neighbors when considering
multiple hops. Further, existing models are not scalable as the memory and
computation needs grow exponentially with the number of hops considered. To
circumvent these problems, we propose a generic Higher Order Propagation
Framework (HOPF) which includes (i) a differentiable Node Information
Preserving (NIP) kernel and (ii) a scalable iterative learning and inferencing
mechanism to aggregate information over larger hops. We do an extensive
evaluation using 11 datasets from different domains and show that unlike
existing CC models, our NIP model with iterative inference is robust across all
the datasets and can handle much larger neighborhoods in a scalable manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayan_P/0/1/0/all/0/1&quot;&gt;Priyesh Vijayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandak_Y/0/1/0/all/0/1&quot;&gt;Yash Chandak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1&quot;&gt;Mitesh M. Khapra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1&quot;&gt;Balaraman Ravindran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12467">
<title>A Method Based on Convex Cone Model for Image-Set Classification with CNN Features. (arXiv:1805.12467v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.12467</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a method for image-set classification based on
convex cone models, focusing on the effectiveness of convolutional neural
network (CNN) features as inputs. CNN features have non-negative values when
using the rectified linear unit as an activation function. This naturally leads
us to model a set of CNN features by a convex cone and measure the geometric
similarity of convex cones for classification. To establish this framework, we
sequentially define multiple angles between two convex cones by repeating the
alternating least squares method and then define the geometric similarity
between the cones using the obtained angles. Moreover, to enhance our method,
we introduce a discriminant space, maximizing the between-class variance (gaps)
and minimizes the within-class variance of the projected convex cones onto the
discriminant space, similar to a Fisher discriminant analysis. Finally,
classification is based on the similarity between projected convex cones. The
effectiveness of the proposed method was demonstrated experimentally using a
private, multi-view hand shape dataset and two public databases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sogi_N/0/1/0/all/0/1&quot;&gt;Naoya Sogi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakayama_T/0/1/0/all/0/1&quot;&gt;Taku Nakayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukui_K/0/1/0/all/0/1&quot;&gt;Kazuhiro Fukui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12472">
<title>Distributed Estimation of Gaussian Correlations. (arXiv:1805.12472v1 [math.ST])</title>
<link>http://arxiv.org/abs/1805.12472</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a distributed estimation problem in which two remotely located
agents, Alice and Bob, observe an unlimited number of i.i.d. samples
corresponding to different parts of a random vector. Alice can send $k$ bits on
average to Bob, who in turn wants to estimate the cross-correlation matrix
between the two parts of the vector. In the case where the agents observe
jointly Gaussian scalar random variables with an unknown correlation $\rho$, we
obtain two constructive and simple unbiased estimators attaining a variance of
$\frac{1-\rho^2}{2k\ln 2}$, which coincides with a known but non-constructive
random coding result of Zhang and Berger. We extend our approach to the vector
Gaussian case, which has not been treated before, and construct an estimator
that is uniformly better than the scalar estimator applied separately to each
of the correlations. We then show that the Gaussian performance can essentially
be attained even when the distribution is completely unknown. This in
particular implies that in the general problem of distributed correlation
estimation, the variance can decay at least as $O(1/k)$ with the number of
transmitted bits. This behavior is however not tight: we give an example of a
rich family of distributions where a slightly modified estimator attains a
variance of $2^{-\Omega(k)}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hadar_U/0/1/0/all/0/1&quot;&gt;Uri Hadar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shayevitz_O/0/1/0/all/0/1&quot;&gt;Ofer Shayevitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12528">
<title>Fusion Graph Convolutional Networks. (arXiv:1805.12528v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12528</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised node classification involves learning to classify unlabelled
nodes given a partially labeled graph. In transductive learning, all unlabelled
nodes to be classified are observed during training and in inductive learning,
predictions are to be made for nodes not seen at training. In this paper, we
focus on both these settings for node classification in attributed graphs,
i.e., graphs in which nodes have additional features. State-of-the-art models
for node classification on such attributed graphs use differentiable recursive
functions. These differentiable recursive functions enable aggregation and
filtering of neighborhood information from multiple hops (depths). Despite
being powerful, these variants are limited in their ability to combine
information from different hops efficiently. In this work, we analyze this
limitation of recursive graph functions in terms of their representation
capacity to effectively capture multi-hop neighborhood information. Further, we
provide a simple fusion component which is mathematically motivated to address
this limitation and improve the existing models to explicitly learn the
importance of information from different hops. This proposed mechanism is shown
to improve over existing methods across 8 popular datasets from different
domains. Specifically, our model improves the Graph Convolutional Network (GCN)
and a variant of Graph SAGE by a significant margin providing highly
competitive state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayan_P/0/1/0/all/0/1&quot;&gt;Priyesh Vijayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandak_Y/0/1/0/all/0/1&quot;&gt;Yash Chandak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1&quot;&gt;Mitesh M. Khapra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1&quot;&gt;Balaraman Ravindran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12529">
<title>Analysis of Fast Structured Dictionary Learning. (arXiv:1805.12529v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12529</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparsity-based models and techniques have been exploited in many signal
processing and imaging applications. Data-driven methods based on dictionary
and transform learning enable learning rich image features from data, and can
outperform analytical models. In particular, alternating optimization
algorithms for dictionary learning have been popular. In this work, we focus on
alternating minimization for a specific structured unitary operator learning
problem, and provide a convergence analysis. While the algorithm converges to
the critical points of the problem generally, our analysis establishes under
mild assumptions, the local linear convergence of the algorithm to the
underlying generating model of the data. Analysis and numerical simulations
show that our assumptions hold well for standard probabilistic data models. In
practice, the algorithm is robust to initialization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravishankar_S/0/1/0/all/0/1&quot;&gt;Saiprasad Ravishankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1&quot;&gt;Anna Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1&quot;&gt;Deanna Needell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12547">
<title>Long-time predictive modeling of nonlinear dynamical systems using neural networks. (arXiv:1805.12547v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.12547</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the use of feedforward neural networks (FNN) to develop models of
nonlinear dynamical systems from data. Emphasis is placed on predictions at
long times, with limited data availability. Inspired by global stability
analysis, and the observation of the strong correlation between the local error
and the maximum singular value of the Jacobian of the ANN, we introduce
Jacobian regularization in the loss function. This regularization suppresses
the sensitivity of the prediction to the local error and is shown to improve
accuracy and robustness. Comparison between the proposed approach and sparse
polynomial regression is presented in numerical examples ranging from simple
ODE systems to nonlinear PDE systems including vortex shedding behind a
cylinder, and instability-driven buoyant mixing flow. Furthermore, limitations
of feedforward neural networks are highlighted, especially when the training
data does not include a low dimensional attractor. Strategies of data
augmentation are presented as remedies to address these issues to a certain
extent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shaowu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duraisamy_K/0/1/0/all/0/1&quot;&gt;Karthik Duraisamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12549">
<title>Channel Gating Neural Networks. (arXiv:1805.12549v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12549</link>
<description rdf:parseType="Literal">&lt;p&gt;Employing deep neural networks to obtain state-of-the-art performance on
computer vision tasks can consume billions of floating point operations and
several Joules of energy per evaluation. Network pruning, which statically
removes unnecessary features and weights, has emerged as a promising way to
reduce this computation cost. In this paper, we propose channel gating, a
dynamic, fine-grained, training-based computation-cost-reduction scheme.
Channel gating works by identifying the regions in the features which
contribute less to the classification result and turning off a subset of the
channels for computing the pixels within these uninteresting regions. Unlike
static network pruning, the channel gating optimizes computations exploiting
characteristics specific to each input at run-time. We show experimentally that
applying channel gating in state-of-the-art networks can achieve 66% and 60%
reduction in FLOPs with 0.22% and 0.29% accuracy loss on the CIFAR-10 and
CIFAR-100 datasets, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Weizhe Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1&quot;&gt;Christopher De Sa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_G/0/1/0/all/0/1&quot;&gt;G. Edward Suh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12573">
<title>Learning a Prior over Intent via Meta-Inverse Reinforcement Learning. (arXiv:1805.12573v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.12573</link>
<description rdf:parseType="Literal">&lt;p&gt;A significant challenge for the practical application of reinforcement
learning in the real world is the need to specify an oracle reward function
that correctly defines a task. Inverse reinforcement learning (IRL) seeks to
avoid this challenge by instead inferring a reward function from expert
behavior. While appealing, it can be impractically expensive to collect
datasets of demonstrations that cover the variation common in the real world
(e.g. opening any type of door). Thus in practice, IRL must commonly be
performed with only a limited set of demonstrations where it can be exceedingly
difficult to unambiguously recover a reward function. In this work, we exploit
the insight that demonstrations from other tasks can be used to constrain the
set of possible reward functions by learning a &quot;prior&quot; that is specifically
optimized for the ability to infer expressive reward functions from limited
numbers of demonstrations. We demonstrate that our method can efficiently
recover rewards from images for novel tasks and provide intuition as to how our
approach is analogous to learning a prior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kelvin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratner_E/0/1/0/all/0/1&quot;&gt;Ellis Ratner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1&quot;&gt;Anca Dragan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.08159">
<title>McKernel: A Library for Approximate Kernel Expansions in Log-linear Time. (arXiv:1702.08159v8 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1702.08159</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel Methods Next Generation (KMNG) introduces a framework to use kernel
approximates in the mini-batch setting with SGD Optimizer as an alternative to
Deep Learning. McKernel is a C++ library for KMNG ML Large-scale. It contains a
CPU optimized implementation of the Fastfood algorithm that allows the
computation of approximated kernel expansions in log-linear time. The algorithm
requires to compute the product of Walsh Hadamard Transform (WHT) matrices. A
cache friendly SIMD Fast Walsh Hadamard Transform (FWHT) that achieves
compelling speed and outperforms current state-of-the-art methods has been
developed. McKernel allows to obtain non-linear classification combining
Fastfood and a linear classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Curto_J/0/1/0/all/0/1&quot;&gt;Joachim D. Curt&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zarza_I/0/1/0/all/0/1&quot;&gt;Irene C. Zarza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Feng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1&quot;&gt;Alexander J. Smola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1&quot;&gt;Fernando De La Torre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1&quot;&gt;Chong-Wah Ngo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.00090">
<title>Lower Bounds on Regret for Noisy Gaussian Process Bandit Optimization. (arXiv:1706.00090v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.00090</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the problem of sequentially optimizing a black-box
function $f$ based on noisy samples and bandit feedback. We assume that $f$ is
smooth in the sense of having a bounded norm in some reproducing kernel Hilbert
space (RKHS), yielding a commonly-considered non-Bayesian form of Gaussian
process bandit optimization. We provide algorithm-independent lower bounds on
the simple regret, measuring the suboptimality of a single point reported after
$T$ rounds, and on the cumulative regret, measuring the sum of regrets over the
$T$ chosen points. For the isotropic squared-exponential kernel in $d$
dimensions, we find that an average simple regret of $\epsilon$ requires $T =
\Omega\big(\frac{1}{\epsilon^2} (\log\frac{1}{\epsilon})^{d/2}\big)$, and the
average cumulative regret is at least $\Omega\big( \sqrt{T(\log T)^{d/2}}
\big)$, thus matching existing upper bounds up to the replacement of $d/2$ by
$2d+O(1)$ in both cases. For the Mat\&apos;ern-$\nu$ kernel, we give analogous
bounds of the form $\Omega\big( (\frac{1}{\epsilon})^{2+d/\nu}\big)$ and
$\Omega\big( T^{\frac{\nu + d}{2\nu + d}} \big)$, and discuss the resulting
gaps to the existing upper bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scarlett_J/0/1/0/all/0/1&quot;&gt;Jonathan Scarlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bogunovic_I/0/1/0/all/0/1&quot;&gt;Ilijia Bogunovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.07193">
<title>Applications of Trajectory Data from the Perspective of a Road Transportation Agency: Literature Review and Maryland Case Study. (arXiv:1708.07193v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.07193</link>
<description rdf:parseType="Literal">&lt;p&gt;Transportation agencies have an opportunity to leverage
increasingly-available trajectory datasets to improve their analyses and
decision-making processes. However, this data is typically purchased from
vendors, which means agencies must understand its potential benefits beforehand
in order to properly assess its value relative to the cost of acquisition.
While the literature concerned with trajectory data is rich, it is naturally
fragmented and focused on technical contributions in niche areas, which makes
it difficult for government agencies to assess its value across different
transportation domains. To overcome this issue, the current paper explores
trajectory data from the perspective of a road transportation agency interested
in acquiring trajectories to enhance its analyses. The paper provides a
literature review illustrating applications of trajectory data in six areas of
road transportation systems analysis: demand estimation, modeling human
behavior, designing public transit, traffic performance measurement and
prediction, environment and safety. In addition, it visually explores 20
million GPS traces in Maryland, illustrating existing and suggesting new
applications of trajectory data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Markovic_N/0/1/0/all/0/1&quot;&gt;Nikola Markovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sekula_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Seku&amp;#x142;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laan_Z/0/1/0/all/0/1&quot;&gt;Zachary Vander Laan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Andrienko_G/0/1/0/all/0/1&quot;&gt;Gennady Andrienko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Andrienko_N/0/1/0/all/0/1&quot;&gt;Natalia Andrienko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01799">
<title>Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization. (arXiv:1801.01799v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01799</link>
<description rdf:parseType="Literal">&lt;p&gt;We present novel understandings of the Gamma-Poisson (GaP) model, a
probabilistic matrix factorization model for count data. We show that GaP can
be rewritten free of the score/activation matrix. This gives us new insights
about the estimation of the topic/dictionary matrix by maximum marginal
likelihood estimation. In particular, this explains the robustness of this
estimator to over-specified values of the factorization rank, especially its
ability to automatically prune irrelevant dictionary columns, as empirically
observed in previous work. The marginalization of the activation matrix leads
in turn to a new Monte Carlo Expectation-Maximization algorithm with favorable
properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Filstroff_L/0/1/0/all/0/1&quot;&gt;Louis Filstroff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lumbreras_A/0/1/0/all/0/1&quot;&gt;Alberto Lumbreras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fevotte_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric F&amp;#xe9;votte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04170">
<title>Design of Experiments for Model Discrimination Hybridising Analytical and Data-Driven Approaches. (arXiv:1802.04170v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04170</link>
<description rdf:parseType="Literal">&lt;p&gt;Healthcare companies must submit pharmaceutical drugs or medical devices to
regulatory bodies before marketing new technology. Regulatory bodies frequently
require transparent and interpretable computational modelling to justify a new
healthcare technology, but researchers may have several competing models for a
biological system and too little data to discriminate between the models. In
design of experiments for model discrimination, the goal is to design maximally
informative physical experiments in order to discriminate between rival
predictive models. Prior work has focused either on analytical approaches,
which cannot manage all functions, or on data-driven approaches, which may have
computational difficulties or lack interpretable marginal predictive
distributions. We develop a methodology introducing Gaussian process surrogates
in lieu of the original mechanistic models. We thereby extend existing design
and model discrimination methods developed for analytical models to cases of
non-analytical models in a computationally efficient manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Olofsson_S/0/1/0/all/0/1&quot;&gt;Simon Olofsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deisenroth_M/0/1/0/all/0/1&quot;&gt;Marc Peter Deisenroth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Misener_R/0/1/0/all/0/1&quot;&gt;Ruth Misener&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04846">
<title>State Space Gaussian Processes with Non-Gaussian Likelihood. (arXiv:1802.04846v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04846</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide a comprehensive overview and tooling for GP modeling with
non-Gaussian likelihoods using state space methods. The state space formulation
allows for solving one-dimensional GP models in $\mathcal{O}(n)$ time and
memory complexity. While existing literature has focused on the connection
between GP regression and state space methods, the computational primitives
allowing for inference using general likelihoods in combination with the
Laplace approximation (LA), variational Bayes (VB), and assumed density
filtering (ADF, a.k.a. single-sweep expectation propagation, EP) schemes has
been largely overlooked. We present means of combining the efficient
$\mathcal{O}(n)$ state space methodology with existing inference methods. We
extend existing methods, and provide unifying code implementing all approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nickisch_H/0/1/0/all/0/1&quot;&gt;Hannes Nickisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Solin_A/0/1/0/all/0/1&quot;&gt;Arno Solin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grigorievskiy_A/0/1/0/all/0/1&quot;&gt;Alexander Grigorievskiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02312">
<title>Dimensionality Reduction for Stationary Time Series via Stochastic Nonconvex Optimization. (arXiv:1803.02312v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02312</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic optimization naturally arises in machine learning. Efficient
algorithms with provable guarantees, however, are still largely missing, when
the objective function is nonconvex and the data points are dependent. This
paper studies this fundamental challenge through a streaming PCA problem for
stationary time series data. Specifically, our goal is to estimate the
principle component of time series data with respect to the covariance matrix
of the stationary distribution. Computationally, we propose a variant of Oja&apos;s
algorithm combined with downsampling to control the bias of the stochastic
gradient caused by the data dependency. Theoretically, we quantify the
uncertainty of our proposed stochastic algorithm based on diffusion
approximations. This allows us to prove the asymptotic rate of convergence and
further implies near optimal asymptotic sample complexity. Numerical
experiments are provided to support our analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minshuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08591">
<title>End-to-End Learning for the Deep Multivariate Probit Model. (arXiv:1803.08591v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08591</link>
<description rdf:parseType="Literal">&lt;p&gt;The multivariate probit model (MVP) is a popular classic model for studying
binary responses of multiple entities. Nevertheless, the computational
challenge of learning the MVP model, given that its likelihood involves
integrating over a multidimensional constrained space of latent variables,
significantly limits its application in practice. We propose a flexible deep
generalization of the classic MVP, the Deep Multivariate Probit Model (DMVP),
which is an end-to-end learning scheme that uses an efficient parallel sampling
process of the multivariate probit model to exploit GPU-boosted deep neural
networks. We present both theoretical and empirical analysis of the convergence
behavior of DMVP&apos;s sampling process with respect to the resolution of the
correlation structure. We provide convergence guarantees for DMVP and our
empirical analysis demonstrates the advantages of DMVP&apos;s sampling compared with
standard MCMC-based methods. We also show that when applied to multi-entity
modelling problems, which are natural DMVP applications, DMVP trains faster
than classical MVP, by at least an order of magnitude, captures rich
correlations among entities, and further improves the joint likelihood of
entities compared with several competitive models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Di Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yexiang Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1&quot;&gt;Carla P. Gomes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09202">
<title>Unsupervised Depth Estimation, 3D Face Rotation and Replacement. (arXiv:1803.09202v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09202</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an unsupervised approach for learning to estimate three
dimensional (3D) facial structure from a single image while also predicting 3D
viewpoint transformations that match a desired pose and facial geometry. We
achieve this by inferring the depth of facial keypoints of an input image in an
unsupervised manner, without using any form of ground-truth depth information.
We show how it is possible to use these depths as intermediate computations
within a new backpropable loss to predict the parameters of a 3D affine
transformation matrix that maps inferred 3D keypoints of an input face to the
corresponding 2D keypoints on a desired target facial geometry or pose. Our
resulting approach can therefore be used to infer plausible 3D transformations
from one face pose to another, allowing faces to be frontalized, transformed
into 3D models or even warped to another pose and facial geometry. Lastly, we
identify certain shortcomings with our formulation, and explore adversarial
image translation techniques as a post-processing step to re-synthesize
complete head shots for faces re-targeted to different poses or identities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1&quot;&gt;Joel Ruben Antony Moniz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beckham_C/0/1/0/all/0/1&quot;&gt;Christopher Beckham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajotte_S/0/1/0/all/0/1&quot;&gt;Simon Rajotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honari_S/0/1/0/all/0/1&quot;&gt;Sina Honari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1&quot;&gt;Christopher Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04918">
<title>Distributed Collaborative Hashing and Its Applications in Ant Financial. (arXiv:1804.04918v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04918</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative filtering, especially latent factor model, has been popularly
used in personalized recommendation. Latent factor model aims to learn user and
item latent factors from user-item historic behaviors. To apply it into real
big data scenarios, efficiency becomes the first concern, including offline
model training efficiency and online recommendation efficiency. In this paper,
we propose a Distributed Collaborative Hashing (DCH) model which can
significantly improve both efficiencies. Specifically, we first propose a
distributed learning framework, following the state-of-the-art parameter server
paradigm, to learn the offline collaborative model. Our model can be learnt
efficiently by distributedly computing subgradients in minibatches on workers
and updating model parameters on servers asynchronously. We then adopt hashing
technique to speedup the online recommendation procedure. Recommendation can be
quickly made through exploiting lookup hash tables. We conduct thorough
experiments on two real large-scale datasets. The experimental results
demonstrate that, comparing with the classic and state-of-the-art (distributed)
latent factor models, DCH has comparable performance in terms of recommendation
accuracy but has both fast convergence speed in offline model training
procedure and realtime efficiency in online recommendation procedure.
Furthermore, the encouraging performance of DCH is also shown for several
real-world applications in Ant Financial.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaochao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peilin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Longfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaolong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11221">
<title>MBA: Mini-Batch AUC Optimization. (arXiv:1805.11221v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11221</link>
<description rdf:parseType="Literal">&lt;p&gt;Area under the receiver operating characteristics curve (AUC) is an important
metric for a wide range of signal processing and machine learning problems, and
scalable methods for optimizing AUC have recently been proposed. However,
handling very large datasets remains an open challenge for this problem. This
paper proposes a novel approach to AUC maximization, based on sampling
mini-batches of positive/negative instance pairs and computing U-statistics to
approximate a global risk minimization problem. The resulting algorithm is
simple, fast, and learning-rate free. We show that the number of samples
required for good performance is independent of the number of pairs available,
which is a quadratic function of the positive and negative instances. Extensive
experiments show the practical utility of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gultekin_S/0/1/0/all/0/1&quot;&gt;San Gultekin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Avishek Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratnaparkhi_A/0/1/0/all/0/1&quot;&gt;Adwait Ratnaparkhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paisley_J/0/1/0/all/0/1&quot;&gt;John Paisley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11454">
<title>Distributed Stochastic Gradient Tracking Methods. (arXiv:1805.11454v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11454</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of distributed multi-agent optimization
over a network, where each agent possesses a local cost function that is smooth
and strongly convex. The global objective is to find a common solution that
minimizes the average of all cost functions. Assuming agents only have access
to unbiased estimates of the gradients of their local cost functions, we
consider a distributed stochastic gradient tracking method (DSGT) and a
gossip-like stochastic gradient tracking method (GSGT). We show that, in
expectation, the iterates generated by each agent are attracted to a
neighborhood of the optimal solution, where they accumulate exponentially fast
(under a constant stepsize choice). Under DSGT, the limiting (expected) error
bounds on the distance of the iterates from the optimal solution decrease with
the network size $n$, which is a comparable performance to a centralized
stochastic gradient algorithm. Moreover, we show that when the network is
well-connected, GSGT incurs lower communication cost than DSGT while
maintaining a similar computational cost. Numerical example further
demonstrates the effectiveness of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pu_S/0/1/0/all/0/1&quot;&gt;Shi Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nedic_A/0/1/0/all/0/1&quot;&gt;Angelia Nedi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11959">
<title>Algebraic Expression of Subjective Spatial and Temporal Patterns. (arXiv:1805.11959v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11959</link>
<description rdf:parseType="Literal">&lt;p&gt;Universal learning machine is a theory trying to study machine learning from
mathematical point of view. The outside world is reflected inside an universal
learning machine according to pattern of incoming data. This is subjective
pattern of learning machine. In [2,4], we discussed subjective spatial pattern,
and established a powerful tool -- X-form, which is an algebraic expression for
subjective spatial pattern. However, as the initial stage of study, there we
only discussed spatial pattern. Here, we will discuss spatial and temporal
patterns, and algebraic expression for them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Chuyu Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12111">
<title>Dynamic Advisor-Based Ensemble (dynABE): Case Study in Stock Trend Prediction of a Major Critical Metal Producer. (arXiv:1805.12111v2 [q-fin.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1805.12111</link>
<description rdf:parseType="Literal">&lt;p&gt;The demand of metals by modern technology has been shifting from common base
metals to a variety of minor metals, such as cobalt or indium. The industrial
importance and limited geological availability of some minor metals have led to
them being considered more &quot;critical,&quot; and there is a growing interest in such
critical metals and their producing companies. In this research, we create a
novel framework, Dynamic Advisor-Based Ensemble (dynABE), to predict the stock
trend of major critical metal producers. Specifically, dynABE first utilizes
domain knowledge to group the features into different &quot;advisors,&quot; each advisor
dealing with a particular economic sector. Then through ensembles of weak
classifiers, each advisor produces a prediction result, and all the advisors
are combined again in a biased online update fashion to dynamically make the
final prediction. Based on a misclassification error of 32% for Jinchuan
Group&apos;s stock (HKG: 2362), we further test a simple stock trading strategy,
which leads to a back-tested return of 296%, or an excess return of 130% within
one year. In addition, the feature set selected by dynABE also suggests
potentially influential factors to metal criticality, because stock prices of
major producers influence metal production. Therefore, not only does this
research propose a novel framework for specialized stock trend prediction, it
also provides domain insights into dynamic features that potentially influence
metal criticality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Dong&lt;/a&gt;</dc:creator>
</item></rdf:RDF>