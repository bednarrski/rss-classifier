<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-25T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08485"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06530"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02827"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08175"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08186"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08287"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08365"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08196"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08227"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08256"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08273"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08310"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08454"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1505.04343"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1607.00567"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.09641"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.09466"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.08040"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.00689"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.01688"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05424"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07644"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.08485">
<title>Sine Cosine Crow Search Algorithm: A powerful hybrid meta heuristic for global optimization. (arXiv:1801.08485v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.08485</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel hybrid algorithm named Since Cosine Crow Search
Algorithm. To propose the SCCSA, two novel algorithms are considered including
Crow Search Algorithm (CSA) and Since Cosine Algorithm (SCA). The advantages of
the two algorithms are considered and utilize to design an efficient hybrid
algorithm which can perform significantly better in various benchmark
functions. The combination of concept and operators of the two algorithms
enable the SCCSA to make an appropriate trade-off between exploration and
exploitation abilities of the algorithm. To evaluate the performance of the
proposed SCCSA, seven well-known benchmark functions are utilized. The results
indicated that the proposed hybrid algorithm is able to provide very
competitive solution comparing to other state-of-the-art meta heuristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasandideh_S/0/1/0/all/0/1&quot;&gt;Seyed Hamid Reza Pasandideh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalilpourazari_S/0/1/0/all/0/1&quot;&gt;Soheyl Khalilpourazari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06530">
<title>Dynamic Weight Alignment for Convolutional Neural Networks. (arXiv:1712.06530v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06530</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a method of improving Convolutional Neural Networks
(CNN) by determining the optimal alignment of weights and inputs using dynamic
programming. Conventional CNNs convolve learnable shared weights, or filters,
across the input data. The filters use a linear matching of weights to inputs
using an inner product between the filter and a window of the input. However,
it is possible that there exists a more optimal alignment of weights. Thus, we
propose the use of Dynamic Time Warping (DTW) to dynamically align the weights
to optimized input elements. This dynamic alignment is useful for time series
recognition due to the complexities of temporal relations and temporal
distortions. We demonstrate the effectiveness of the proposed architecture on
the Unipen online handwritten digit and character datasets, the UCI Spoken
Arabic Digit dataset, and the UCI Activities of Daily Life dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1&quot;&gt;Brian Kenji Iwana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1&quot;&gt;Seiichi Uchida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02827">
<title>Novel Methods for Enhancing the Performance of Genetic Algorithms. (arXiv:1801.02827v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02827</link>
<description rdf:parseType="Literal">&lt;p&gt;In this thesis we propose new methods for crossover operator namely: cut on
worst gene (COWGC), cut on worst L+R gene (COWLRGC) and Collision Crossovers.
And also we propose several types of mutation operator such as: worst gene with
random gene mutation (WGWRGM) , worst LR gene with random gene mutation
(WLRGWRGM), worst gene with worst gene mutation (WGWWGM), worst gene with
nearest neighbour mutation (WGWNNM), worst gene with the worst around the
nearest neighbour mutation (WGWWNNM), worst gene inserted beside nearest
neighbour mutation (WGIBNNM), random gene inserted beside nearest neighbour
mutation (RGIBNNM), Swap worst gene locally mutation (SWGLM), Insert best
random gene before worst gene mutation (IBRGBWGM) and Insert best random gene
before random gene mutation (IBRGBRGM). In addition to proposing four selection
strategies, namely: select any crossover (SAC), select any mutation (SAM),
select best crossover (SBC) and select best mutation (SBM). The first two are
based on selection of the best crossover and mutation operator respectively,
and the other two strategies randomly select any operator. So we investigate
the use of more than one crossover/mutation operator (based on the proposed
strategies) to enhance the performance of genetic algorithms. Our experiments,
conducted on several Travelling Salesman Problems (TSP), show the superiority
of some of the proposed methods in crossover and mutation over some of the
well-known crossover and mutation operators described in the literature. In
addition, using any of the four strategies (SAC, SAM, SBC and SBM), found to be
better than using one crossover/mutation operator in general, because those
allow the GA to avoid local optima, or the so-called premature convergence.
Keywords: GAs, Collision crossover, Multi crossovers, Multi mutations, TSP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkafaween_E/0/1/0/all/0/1&quot;&gt;Esra&amp;#x27;a O Alkafaween&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08175">
<title>Development and application of a machine learning supported methodology for measurement and verification (M&amp;V) 2.0. (arXiv:1801.08175v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.08175</link>
<description rdf:parseType="Literal">&lt;p&gt;The foundations of all methodologies for the measurement and verification
(M&amp;amp;V) of energy savings are based on the same five key principles: accuracy,
completeness, conservatism, consistency and transparency. The most widely
accepted methodologies tend to generalise M&amp;amp;V so as to ensure applicability
across the spectrum of energy conservation measures (ECM&apos;s). These do not
provide a rigid calculation procedure to follow. This paper aims to bridge the
gap between high-level methodologies and the practical application of modelling
algorithms, with a focus on the industrial buildings sector. This is achieved
with the development of a novel, machine learning supported methodology for M&amp;amp;V
2.0 which enables accurate quantification of savings.
&lt;/p&gt;
&lt;p&gt;A novel and computationally efficient feature selection algorithm and
powerful machine learning regression algorithms are employed to maximise the
effectiveness of available data. The baseline period energy consumption is
modelled using artificial neural networks, support vector machines, k-nearest
neighbours and multiple ordinary least squares regression. Improved knowledge
discovery and an expanded boundary of analysis allow more complex energy
systems be analysed, thus increasing the applicability of M&amp;amp;V. A case study in
a large biomedical manufacturing facility is used to demonstrate the
methodology&apos;s ability to accurately quantify the savings under real-world
conditions. The ECM was found to result in 604,527 kWh of energy savings with
57% uncertainty at a confidence interval of 68%. 20 baseline energy models are
developed using an exhaustive approach with the optimal model being used to
quantify savings. The range of savings estimated with each model are presented
and the acceptability of uncertainty is reviewed. The case study demonstrates
the ability of the methodology to perform M&amp;amp;V to an acceptable standard in
challenging circumstances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallagher_C/0/1/0/all/0/1&quot;&gt;Colm V. Gallagher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leahy_K/0/1/0/all/0/1&quot;&gt;Kevin Leahy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ODonovan_P/0/1/0/all/0/1&quot;&gt;Peter O&amp;#x27;Donovan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruton_K/0/1/0/all/0/1&quot;&gt;Ken Bruton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OSullivan_D/0/1/0/all/0/1&quot;&gt;Dominic T.J. O&amp;#x27;Sullivan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08186">
<title>MAttNet: Modular Attention Network for Referring Expression Comprehension. (arXiv:1801.08186v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.08186</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address referring expression comprehension: localizing an
image region described by a natural language expression. While most recent work
treats expressions as a single unit, we propose to decompose them into three
modular components related to subject appearance, location, and relationship to
other objects. This allows us to flexibly adapt to expressions containing
different types of information in an end-to-end framework. In our model, which
we call the Modular Attention Network (MAttNet), two types of attention are
utilized: language-based attention that learns the module weights as well as
the word/phrase attention that each module should focus on; and visual
attention that allows the subject and relationship modules to focus on relevant
image components. Module weights combine scores from all three modules
dynamically to output an overall score. Experiments show that MAttNet
outperforms previous state-of-art methods by a large margin on both
bounding-box-level and pixel-level comprehension tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Licheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhe Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaohui Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jimei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1&quot;&gt;Tamara L.Berg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08287">
<title>Directly Estimating the Variance of the {\lambda}-Return Using Temporal-Difference Methods. (arXiv:1801.08287v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.08287</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates estimating the variance of a temporal-difference
learning agent&apos;s update target. Most reinforcement learning methods use an
estimate of the value function, which captures how good it is for the agent to
be in a particular state and is mathematically expressed as the expected sum of
discounted future rewards (called the return). These values can be
straightforwardly estimated by averaging batches of returns using Monte Carlo
methods. However, if we wish to update the agent&apos;s value estimates during
learning--before terminal outcomes are observed--we must use a different
estimation target called the {\lambda}-return, which truncates the return with
the agent&apos;s own estimate of the value function. Temporal difference learning
methods estimate the expected {\lambda}-return for each state, allowing these
methods to update online and incrementally, and in most cases achieve better
generalization error and faster learning than Monte Carlo methods. Naturally
one could attempt to estimate higher-order moments of the {\lambda}-return.
This paper is about estimating the variance of the {\lambda}-return. Prior work
has shown that given estimates of the variance of the {\lambda}-return,
learning systems can be constructed to (1) mitigate risk in action selection,
and (2) automatically adapt the parameters of the learning process itself to
improve performance. Unfortunately, existing methods for estimating the
variance of the {\lambda}-return are complex and not well understood
empirically. We contribute a method for estimating the variance of the
{\lambda}-return directly using policy evaluation methods from reinforcement
learning. Our approach is significantly simpler than prior methods that
independently estimate the second moment of the {\lambda}-return. Empirically
our new approach behaves at least as well as existing approaches, but is
generally more robust.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sherstan_C/0/1/0/all/0/1&quot;&gt;Craig Sherstan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennett_B/0/1/0/all/0/1&quot;&gt;Brendan Bennett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_K/0/1/0/all/0/1&quot;&gt;Kenny Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashley_D/0/1/0/all/0/1&quot;&gt;Dylan R. Ashley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1&quot;&gt;Adam White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1&quot;&gt;Martha White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1&quot;&gt;Richard S. Sutton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08365">
<title>Probabilistic Planning by Probabilistic Programming. (arXiv:1801.08365v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.08365</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated planning is a major topic of research in artificial intelligence,
and enjoys a long and distinguished history. The classical paradigm assumes a
distinguished initial state, comprised of a set of facts, and is defined over a
set of actions which change that state in one way or another. Planning in many
real-world settings, however, is much more involved: an agent&apos;s knowledge is
almost never simply a set of facts that are true, and actions that the agent
intends to execute never operate the way they are supposed to. Thus,
probabilistic planning attempts to incorporate stochastic models directly into
the planning process. In this article, we briefly report on probabilistic
planning through the lens of probabilistic programming: a programming paradigm
that aims to ease the specification of structured probability distributions. In
particular, we provide an overview of the features of two systems, HYPE and
ALLEGRO, which emphasise different strengths of probabilistic programming that
are particularly useful for complex modelling issues raised in probabilistic
planning. Among other things, with these systems, one can instantiate planning
problems with growing and shrinking state spaces, discrete and continuous
probability distributions, and non-unique prior distributions in a first-order
setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belle_V/0/1/0/all/0/1&quot;&gt;Vaishak Belle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06431">
<title>Using KL-divergence to focus Deep Visual Explanation. (arXiv:1711.06431v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06431</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method for explaining the image classification predictions of
deep convolution neural networks, by highlighting the pixels in the image which
influence the final class prediction. Our method requires the identification of
a heuristic method to select parameters hypothesized to be most relevant in
this prediction, and here we use Kullback-Leibler divergence to provide this
focus. Overall, our approach helps in understanding and interpreting deep
network predictions and we hope contributes to a foundation for such
understanding of deep learning networks. In this brief paper, our experiments
evaluate the performance of two popular networks in this context of
interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babiker_H/0/1/0/all/0/1&quot;&gt;Housam Khalifa Bashier Babiker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1&quot;&gt;Randy Goebel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08196">
<title>Incremental Eigenpair Computation for Graph Laplacian Matrices: Theory and Applications. (arXiv:1801.08196v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.08196</link>
<description rdf:parseType="Literal">&lt;p&gt;The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs)
of a graph Laplacian matrix have been widely used in spectral clustering and
community detection. However, in real-life applications the number of clusters
or communities (say, $K$) is generally unknown a-priori. Consequently, the
majority of the existing methods either choose $K$ heuristically or they repeat
the clustering method with different choices of $K$ and accept the best
clustering result. The first option, more often, yields suboptimal result,
while the second option is computationally expensive. In this work, we propose
an incremental method for constructing the eigenspectrum of the graph Laplacian
matrix. This method leverages the eigenstructure of graph Laplacian matrix to
obtain the $K$-th smallest eigenpair of the Laplacian matrix given a collection
of all previously computed $K-1$ smallest eigenpairs. Our proposed method
adapts the Laplacian matrix such that the batch eigenvalue decomposition
problem transforms into an efficient sequential leading eigenpair computation
problem. As a practical application, we consider user-guided spectral
clustering. Specifically, we demonstrate that users can utilize the proposed
incremental method for effective eigenpair computation and for determining the
desired number of clusters based on multiple clustering metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baichuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Mohammad Al Hasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08227">
<title>Matrix Completion with Nonconvex Regularization: Spectral Operators and Scalable Algorithms. (arXiv:1801.08227v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1801.08227</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the popularly dubbed matrix completion problem, where
the task is to &quot;fill in&quot; the unobserved entries of a matrix from a small subset
of observed entries, under the assumption that the underlying matrix is of
low-rank. Our contributions herein, enhance our prior work on nuclear norm
regularized problems for matrix completion (Mazumder et al., 2010) by
incorporating a continuum of nonconvex penalty functions between the convex
nuclear norm and nonconvex rank functions. Inspired by SOFT-IMPUTE (Mazumder et
al., 2010; Hastie et al., 2016), we propose NC-IMPUTE- an EM-flavored
algorithmic framework for computing a family of nonconvex penalized matrix
completion problems with warm-starts. We present a systematic study of the
associated spectral thresholding operators, which play an important role in the
overall algorithm. We study convergence properties of the algorithm. Using
structured low-rank SVD computations, we demonstrate the computational
scalability of our proposal for problems up to the Netflix size (approximately,
a $500,000 \times 20, 000$ matrix with $10^8$ observed entries). We demonstrate
that on a wide range of synthetic and real data instances, our proposed
nonconvex regularization framework leads to low-rank solutions with better
predictive performance when compared to those obtained from nuclear norm
problems. Implementations of algorithms proposed herein, written in the R
programming language, are made available on github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mazumder_R/0/1/0/all/0/1&quot;&gt;Rahul Mazumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saldana_D/0/1/0/all/0/1&quot;&gt;Diego F. Saldana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weng_H/0/1/0/all/0/1&quot;&gt;Haolei Weng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08256">
<title>A Hilbert Space of Stationary Ergodic Processes. (arXiv:1801.08256v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.08256</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying meaningful signal buried in noise is a problem of interest
arising in diverse scenarios of data-driven modeling. We present here a
theoretical framework for exploiting intrinsic geometry in data that resists
noise corruption, and might be identifiable under severe obfuscation. Our
approach is based on uncovering a valid complete inner product on the space of
ergodic stationary finite valued processes, providing the latter with the
structure of a Hilbert space on the real field. This rigorous construction,
based on non-standard generalizations of the notions of sum and scalar
multiplication of finite dimensional probability vectors, allows us to
meaningfully talk about &quot;angles&quot; between data streams and data sources, and,
make precise the notion of orthogonal stochastic processes. In particular, the
relative angles appear to be preserved, and identifiable, under severe noise,
and will be developed in future as the underlying principle for robust
classification, clustering and unsupervised featurization algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chattopadhyay_I/0/1/0/all/0/1&quot;&gt;Ishanu Chattopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08273">
<title>Nonparametric Hawkes Processes: Online Estimation and Generalization Bounds. (arXiv:1801.08273v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.08273</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we design a nonparametric online algorithm for estimating the
triggering functions of multivariate Hawkes processes. Unlike parametric
estimation, where evolutionary dynamics can be exploited for fast computation
of the gradient, and unlike typical function learning, where representer
theorem is readily applicable upon proper regularization of the objective
function, nonparametric estimation faces the challenges of (i) inefficient
evaluation of the gradient, (ii) lack of representer theorem, and (iii)
computationally expensive projection necessary to guarantee positivity of the
triggering functions. In this paper, we offer solutions to the above
challenges, and design an online estimation algorithm named NPOLE-MHP that
outputs estimations with a $\mathcal{O}(1/T)$ regret, and a $\mathcal{O}(1/T)$
stability. Furthermore, we design an algorithm, NPOLE-MMHP, for estimation of
multivariate marked Hawkes processes. We test the performance of NPOLE-MHP on
various synthetic and real datasets, and demonstrate, under different
evaluation metrics, that NPOLE-MHP performs as good as the optimal maximum
likelihood estimation (MLE), while having a run time as little as parametric
online algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yingxiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Etesami_J/0/1/0/all/0/1&quot;&gt;Jalal Etesami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_N/0/1/0/all/0/1&quot;&gt;Niao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kiyavash_N/0/1/0/all/0/1&quot;&gt;Negar Kiyavash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08310">
<title>Information gain ratio correction: Improving prediction with more balanced decision tree splits. (arXiv:1801.08310v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.08310</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision trees algorithms use a gain function to select the best split during
the tree&apos;s induction. This function is crucial to obtain trees with high
predictive accuracy. Some gain functions can suffer from a bias when it
compares splits of different arities. Quinlan proposed a gain ratio in C4.5&apos;s
information gain function to fix this bias. In this paper, we present an
updated version of the gain ratio that performs better as it tries to fix the
gain ratio&apos;s bias for unbalanced trees and some splits with low predictive
interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Leroux_A/0/1/0/all/0/1&quot;&gt;Antonin Leroux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boussard_M/0/1/0/all/0/1&quot;&gt;Matthieu Boussard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Des_R/0/1/0/all/0/1&quot;&gt;Remi D&amp;#xe8;s&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08454">
<title>A Distributed Framework for the Construction of Transport Maps. (arXiv:1801.08454v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.08454</link>
<description rdf:parseType="Literal">&lt;p&gt;The need to reason about uncertainty in large, complex, and multi-modal
datasets has become increasingly common across modern scientific environments.
The ability to transform samples from one distribution $P$ to another
distribution $Q$ enables the solution to many problems in machine learning
(e.g. Bayesian inference, generative modeling) and has been actively pursued
from theoretical, computational, and application perspectives across the fields
of information theory, computer science, and biology. Performing such
transformations , in general, still comprises computational difficulties,
especially in high dimensions. Here, we consider the problem of computing such
&quot;measure transport maps&quot; with efficient and parallelizable methods. Under the
mild assumptions that $P$ need not be known but can be sampled from, that the
density of $Q$ is known up to a proportionality constant, and that $Q$ is
log-concave, we provide a convex optimization problem pertaining to relative
entropy minimization. We show how an empirical minimization formulation and
polynomial chaos map parameterization can allow for learning a transport map
between $P$ and $Q$ with distributed and scalable methods. We also leverage
findings from nonequilibrium thermodynamics to represent the transport map as a
composition of simpler maps, each of which is learned sequentially with a
transport cost regularized version of the aforementioned problem formulation.
We provide examples of our framework within the context of Bayesian inference
for the Boston housing dataset, active learning for optimizing human computer
interfaces, density estimation for probabilistic sleep staging with EEG, and
generative modeling for handwritten digit images from the MNIST dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mesa_D/0/1/0/all/0/1&quot;&gt;Diego A. Mesa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tantiongloc_J/0/1/0/all/0/1&quot;&gt;Justin Tantiongloc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mendoza_M/0/1/0/all/0/1&quot;&gt;Marcela Mendoza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Coleman_T/0/1/0/all/0/1&quot;&gt;Todd P. Coleman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1505.04343">
<title>Provably Correct Algorithms for Matrix Column Subset Selection with Selectively Sampled Data. (arXiv:1505.04343v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1505.04343</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of matrix column subset selection, which selects a
subset of columns from an input matrix such that the input can be well
approximated by the span of the selected columns. Column subset selection has
been applied to numerous real-world data applications such as population
genetics summarization, electronic circuits testing and recommendation systems.
In many applications the complete data matrix is unavailable and one needs to
select representative columns by inspecting only a small portion of the input
matrix. In this paper we propose the first provably correct column subset
selection algorithms for partially observed data matrices. Our proposed
algorithms exhibit different merits and limitations in terms of statistical
accuracy, computational efficiency, sample complexity and sampling schemes,
which provides a nice exploration of the tradeoff between these desired
properties for column subset selection. The proposed methods employ the idea of
feedback driven sampling and are inspired by several sampling schemes
previously introduced for low-rank matrix approximation tasks (Drineas et al.,
2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and
Singh, 2014). Our analysis shows that, under the assumption that the input data
matrix has incoherent rows but possibly coherent columns, all algorithms
provably converge to the best low-rank approximation of the original data as
number of selected columns increases. Furthermore, two of the proposed
algorithms enjoy a relative error bound, which is preferred for column subset
selection and matrix approximation purposes. We also demonstrate through both
theoretical and empirical analysis the power of feedback driven sampling
compared to uniform random sampling on input matrices with highly correlated
columns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yining Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aarti Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1607.00567">
<title>Rademacher Complexity Bounds for a Penalized Multiclass Semi-Supervised Algorithm. (arXiv:1607.00567v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1607.00567</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Rademacher complexity bounds for multiclass classifiers trained
with a two-step semi-supervised model. In the first step, the algorithm
partitions the partially labeled data and then identifies dense clusters
containing $\kappa$ predominant classes using the labeled training examples
such that the proportion of their non-predominant classes is below a fixed
threshold. In the second step, a classifier is trained by minimizing a margin
empirical loss over the labeled training set and a penalization term measuring
the disability of the learner to predict the $\kappa$ predominant classes of
the identified clusters. The resulting data-dependent generalization error
bound involves the margin distribution of the classifier, the stability of the
clustering technique used in the first step and Rademacher complexity terms
corresponding to partially labeled training data. Our theoretical result
exhibit convergence rates extending those proposed in the literature for the
binary case, and experimental results on different multiclass classification
problems show empirical evidence that supports the theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maximov_Y/0/1/0/all/0/1&quot;&gt;Yury Maximov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Amini_M/0/1/0/all/0/1&quot;&gt;Massih-Reza Amini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1&quot;&gt;Zaid Harchaoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.09641">
<title>Auxiliary gradient-based sampling algorithms. (arXiv:1610.09641v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.09641</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new family of MCMC samplers that combine auxiliary variables,
Gibbs sampling and Taylor expansions of the target density. Our approach
permits the marginalisation over the auxiliary variables yielding marginal
samplers, or the augmentation of the auxiliary variables, yielding auxiliary
samplers. The well-known Metropolis-adjusted Langevin algorithm (MALA) and
preconditioned Crank-Nicolson Langevin (pCNL) algorithm are shown to be special
cases. We prove that marginal samplers are superior in terms of asymptotic
variance and demonstrate cases where they are slower in computing time compared
to auxiliary samplers. In the context of latent Gaussian models we propose new
auxiliary and marginal samplers whose implementation requires a single tuning
parameter, which can be found automatically during the transient phase.
Extensive experimentation shows that the increase in efficiency (measured as
effective sample size per unit of computing time) relative to (optimised
implementations of) pCNL, elliptical slice sampling and MALA ranges from
10-fold in binary classification problems to 25-fold in log-Gaussian Cox
processes to 100-fold in Gaussian process regression, and it is on par with
Riemann manifold Hamiltonian Monte Carlo in an example where the latter has the
same complexity as the aforementioned algorithms. We explain this remarkable
improvement in terms of the way alternative samplers try to approximate the
eigenvalues of the target. We introduce a novel MCMC sampling scheme for
hyperparameter learning that builds upon the auxiliary samplers. The MATLAB
code for reproducing the experiments in the article is publicly available and a
Supplement to this article contains additional experiments and implementation
details.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Titsias_M/0/1/0/all/0/1&quot;&gt;Michalis K. Titsias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Papaspiliopoulos_O/0/1/0/all/0/1&quot;&gt;Omiros Papaspiliopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.09466">
<title>Double Coupled Canonical Polyadic Decomposition for Joint Blind Source Separation. (arXiv:1612.09466v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1612.09466</link>
<description rdf:parseType="Literal">&lt;p&gt;Joint blind source separation (J-BSS) is an emerging data-driven technique
for multi-set data-fusion. In this paper, J-BSS is addressed from a tensorial
perspective. We show how, by using second-order multi-set statistics in J-BSS,
a specific double coupled canonical polyadic decomposition (DC-CPD) problem can
be formulated. We propose an algebraic DC-CPD algorithm based on a coupled
rank-1 detection mapping. This algorithm converts a possibly underdetermined
DC-CPD to a set of overdetermined CPDs. The latter can be solved algebraically
via a generalized eigenvalue decomposition based scheme. Therefore, this
algorithm is deterministic and returns the exact solution in the noiseless
case. In the noisy case, it can be used to effectively initialize optimization
based DC-CPD algorithms. In addition, we obtain the determini- stic and generic
uniqueness conditions for DC-CPD, which are shown to be more relaxed than their
CPD counterpart. Experiment results are given to illustrate the superiority of
DC-CPD over standard CPD based BSS methods and several existing J-BSS methods,
with regards to uniqueness and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gong_X/0/1/0/all/0/1&quot;&gt;Xiao-Feng Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qiu-Hua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cong_F/0/1/0/all/0/1&quot;&gt;Feng-Yu Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lathauwer_L/0/1/0/all/0/1&quot;&gt;Lieven De Lathauwer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.08040">
<title>A Simple Exponential Family Framework for Zero-Shot Learning. (arXiv:1707.08040v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1707.08040</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a simple generative framework for learning to predict previously
unseen classes, based on estimating class-attribute-gated class-conditional
distributions. We model each class-conditional distribution as an exponential
family distribution and the parameters of the distribution of each seen/unseen
class are defined as functions of the respective observed class attributes.
These functions can be learned using only the seen class data and can be used
to predict the parameters of the class-conditional distribution of each unseen
class. Unlike most existing methods for zero-shot learning that represent
classes as fixed embeddings in some vector space, our generative model
naturally represents each class as a probability distribution. It is simple to
implement and also allows leveraging additional unlabeled data from unseen
classes to improve the estimates of their class-conditional distributions using
transductive/semi-supervised learning. Moreover, it extends seamlessly to
few-shot learning by easily updating these distributions when provided with a
small number of additional labelled examples from unseen classes. Through a
comprehensive set of experiments on several benchmark data sets, we demonstrate
the efficacy of our framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_V/0/1/0/all/0/1&quot;&gt;Vinay Kumar Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1&quot;&gt;Piyush Rai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.00689">
<title>Dirichlet Bayesian Network Scores and the Maximum Relative Entropy Principle. (arXiv:1708.00689v4 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1708.00689</link>
<description rdf:parseType="Literal">&lt;p&gt;A classic approach for learning Bayesian networks from data is to identify a
maximum a posteriori (MAP) network structure. In the case of discrete Bayesian
networks, MAP networks are selected by maximising one of several possible
Bayesian Dirichlet (BD) scores; the most famous is the Bayesian Dirichlet
equivalent uniform (BDeu) score from Heckerman et al (1995). The key properties
of BDeu arise from its uniform prior over the parameters of the network, which
makes structure learning computationally efficient; does not require the
elicitation of prior knowledge from experts; and satisfies score equivalence.
&lt;/p&gt;
&lt;p&gt;In this paper we will review the derivation and the properties of BD scores,
and of BDeu in particular, and we will link them to the corresponding entropy
estimates to study them from an information theoretic perspective. To this end,
we will work in the context of the foundational work of Giffin and Caticha
(2007), who showed that Bayesian inference can be framed as a particular case
of the maximum relative entropy principle. We will use this connection to show
that BDeu should not be used for structure learning from sparse data, since it
contradicts the maximum relative entropy principle; and that it is also
problematic from a more classic Bayesian model selection perspective, because
it produces Bayes factors that are very sensitive to the value of its only
hyperparameter. We will also show that these issues are in fact different
aspects of the same problem and a consequence of the distributional assumptions
of the prior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Scutari_M/0/1/0/all/0/1&quot;&gt;Marco Scutari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.01688">
<title>On the Sample Complexity of the Linear Quadratic Regulator. (arXiv:1710.01688v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1710.01688</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the optimal control problem known as the Linear
Quadratic Regulator in the case when the dynamics are unknown. We propose a
multi-stage procedure, called Coarse-ID control, that estimates a model from a
few experimental trials, estimates the error in that model with respect to the
truth, and then designs a controller using both the model and uncertainty
estimate. Our technique uses contemporary tools from random matrix theory to
bound the error in the estimation procedure. We also employ a recently
developed approach to control synthesis called System Level Synthesis that
enables robust control design by solving a convex optimization problem. We
provide end-to-end bounds on the relative error in control cost that are nearly
optimal in the number of parameters and that highlight salient properties of
the system to be controlled such as closed-loop sensitivity and optimal control
magnitude. We show experimentally that the Coarse-ID approach enables efficient
computation of a stabilizing controller in regimes where simple control schemes
that do not take the model uncertainty into account fail to stabilize the true
system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dean_S/0/1/0/all/0/1&quot;&gt;Sarah Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mania_H/0/1/0/all/0/1&quot;&gt;Horia Mania&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Matni_N/0/1/0/all/0/1&quot;&gt;Nikolai Matni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Recht_B/0/1/0/all/0/1&quot;&gt;Benjamin Recht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tu_S/0/1/0/all/0/1&quot;&gt;Stephen Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05424">
<title>The landscape of the spiked tensor model. (arXiv:1711.05424v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05424</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of estimating a large rank-one tensor ${\boldsymbol
u}^{\otimes k}\in({\mathbb R}^{n})^{\otimes k}$, $k\ge 3$ in Gaussian noise.
Earlier work characterized a critical signal-to-noise ratio $\lambda_{Bayes}=
O(1)$ above which an ideal estimator achieves strictly positive correlation
with the unknown vector of interest. Remarkably no polynomial-time algorithm is
known that achieved this goal unless $\lambda\ge C n^{(k-2)/4}$ and even
powerful semidefinite programming relaxations appear to fail for $1\ll
\lambda\ll n^{(k-2)/4}$.
&lt;/p&gt;
&lt;p&gt;In order to elucidate this behavior, we consider the maximum likelihood
estimator, which requires maximizing a degree-$k$ homogeneous polynomial over
the unit sphere in $n$ dimensions. We compute the expected number of critical
points and local maxima of this objective function and show that it is
exponential in the dimensions $n$, and give exact formulas for the exponential
growth rate. We show that (for $\lambda$ larger than a constant) critical
points are either very close to the unknown vector ${\boldsymbol u}$, or are
confined in a band of width $\Theta(\lambda^{-1/(k-1)})$ around the maximum
circle that is orthogonal to ${\boldsymbol u}$. For local maxima, this band
shrinks to be of size $\Theta(\lambda^{-1/(k-2)})$. These `uninformative&apos; local
maxima are likely to cause the failure of optimization algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Arous_G/0/1/0/all/0/1&quot;&gt;Gerard Ben Arous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mei_S/0/1/0/all/0/1&quot;&gt;Song Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Montanari_A/0/1/0/all/0/1&quot;&gt;Andrea Montanari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nica_M/0/1/0/all/0/1&quot;&gt;Mihai Nica&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07644">
<title>Non-parametric Sparse Additive Auto-regressive Network Models. (arXiv:1801.07644v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07644</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider a multi-variate time series $(X_t)_{t=0}^{T}$ where $X_t \in
\mathbb{R}^d$ which may represent spike train responses for multiple neurons in
a brain, crime event data across multiple regions, and many others. An
important challenge associated with these time series models is to estimate an
influence network between the $d$ variables, especially when the number of
variables $d$ is large meaning we are in the high-dimensional setting. Prior
work has focused on parametric vector auto-regressive models. However,
parametric approaches are somewhat restrictive in practice. In this paper, we
use the non-parametric sparse additive model (SpAM) framework to address this
challenge. Using a combination of $\beta$ and $\phi$-mixing properties of
Markov chains and empirical process techniques for reproducing kernel Hilbert
spaces (RKHSs), we provide upper bounds on mean-squared error in terms of the
sparsity $s$, logarithm of the dimension $\log d$, number of time points $T$,
and the smoothness of the RKHSs. Our rates are sharp up to logarithm factors in
many cases. We also provide numerical experiments that support our theoretical
results and display potential advantages of using our non-parametric SpAM
framework for a Chicago crime dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hao Henry Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raskutti_G/0/1/0/all/0/1&quot;&gt;Garvesh Raskutti&lt;/a&gt;</dc:creator>
</item></rdf:RDF>