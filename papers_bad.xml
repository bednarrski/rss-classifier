<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-15T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04749"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04806"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05012"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04696"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04789"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05002"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05044"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.05928"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.06196"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.08163"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01711"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06306"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04326"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04725"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04732"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04780"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04878"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04888"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04918"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04976"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05018"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05020"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1601.08057"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.00578"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09707"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09177"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11274"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.04749">
<title>Per-Corpus Configuration of Topic Modelling for GitHub and Stack Overflow Collections. (arXiv:1804.04749v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.04749</link>
<description rdf:parseType="Literal">&lt;p&gt;To make sense of large amounts of textual data, topic modelling is frequently
used as a text-mining tool for the discovery of hidden semantic structures in
text bodies. Latent Dirichlet allocation (LDA) is a commonly used topic model
that aims to explain the structure of a corpus by grouping texts. LDA requires
multiple parameters to work well, and there are only rough and sometimes
conflicting guidelines available on how these parameters should be set. In this
paper, we contribute (i) a broad study of parameters to arrive at good local
optima, (ii) an a-posteriori characterisation of text corpora related to eight
programming languages from GitHub and Stack Overflow, and (iii) an analysis of
corpus feature importance via per-corpus LDA configuration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Treude_C/0/1/0/all/0/1&quot;&gt;Christoph Treude&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1&quot;&gt;Markus Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04806">
<title>{\mu}-cuDNN: Accelerating Deep Learning Frameworks with Micro-Batching. (arXiv:1804.04806v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04806</link>
<description rdf:parseType="Literal">&lt;p&gt;NVIDIA cuDNN is a low-level library that provides GPU kernels frequently used
in deep learning. Specifically, cuDNN implements several equivalent convolution
algorithms, whose performance and memory footprint may vary considerably,
depending on the layer dimensions. When an algorithm is automatically selected
by cuDNN, the decision is performed on a per-layer basis, and thus it often
resorts to slower algorithms that fit the workspace size constraints. We
present {\mu}-cuDNN, a transparent wrapper library for cuDNN, which divides
layers&apos; mini-batch computation into several micro-batches. Based on Dynamic
Programming and Integer Linear Programming, {\mu}-cuDNN enables faster
algorithms by decreasing the workspace requirements. At the same time,
{\mu}-cuDNN keeps the computational semantics unchanged, so that it decouples
statistical efficiency from the hardware efficiency safely. We demonstrate the
effectiveness of {\mu}-cuDNN over two frameworks, Caffe and TensorFlow,
achieving speedups of 1.63x for AlexNet and 1.21x for ResNet-18 on P100-SXM2
GPU. These results indicate that using micro-batches can seamlessly increase
the performance of deep learning, while maintaining the same memory footprint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oyama_Y/0/1/0/all/0/1&quot;&gt;Yosuke Oyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Nun_T/0/1/0/all/0/1&quot;&gt;Tal Ben-Nun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1&quot;&gt;Torsten Hoefler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsuoka_S/0/1/0/all/0/1&quot;&gt;Satoshi Matsuoka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05012">
<title>Representing smooth functions as compositions of near-identity functions with implications for deep network optimization. (arXiv:1804.05012v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.05012</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that any smooth bi-Lipschitz $h$ can be represented exactly as a
composition $h_m \circ ... \circ h_1$ of functions $h_1,...,h_m$ that are close
to the identity in the sense that each $\left(h_i-\mathrm{Id}\right)$ is
Lipschitz, and the Lipschitz constant decreases inversely with the number $m$
of functions composed. This implies that $h$ can be represented to any accuracy
by a deep residual network whose nonlinear layers compute functions with a
small Lipschitz constant. Next, we consider nonlinear regression with a
composition of near-identity nonlinear maps. We show that, regarding Fr\&apos;echet
derivatives with respect to the $h_1,...,h_m$, any critical point of a
quadratic criterion in this near-identity region must be a global minimizer. In
contrast, if we consider derivatives with respect to parameters of a fixed-size
residual network with sigmoid activation functions, we show that there are
near-identity critical points that are suboptimal, even in the realizable case.
Informally, this means that functional gradient methods for residual networks
cannot get stuck at suboptimal critical points corresponding to near-identity
layers, whereas parametric gradient methods for sigmoidal residual networks
suffer from suboptimal critical points in the near-identity region.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1&quot;&gt;Peter L. Bartlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_S/0/1/0/all/0/1&quot;&gt;Steven N. Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_P/0/1/0/all/0/1&quot;&gt;Philip M. Long&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04696">
<title>Efficient Model Identification for Tensegrity Locomotion. (arXiv:1804.04696v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1804.04696</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to identify in a practical manner unknown physical
parameters, such as mechanical models of actuated robot links, which are
critical in dynamical robotic tasks. Key features include the use of an
off-the-shelf physics engine and the Bayesian optimization framework. The task
being considered is locomotion with a high-dimensional, compliant Tensegrity
robot. A key insight, in this case, is the need to project the model
identification challenge into an appropriate lower dimensional space for
efficiency. Comparisons with alternatives indicate that the proposed method can
identify the parameters more accurately within the given time budget, which
also results in more precise locomotion control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shaojun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surovik_D/0/1/0/all/0/1&quot;&gt;David Surovik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1&quot;&gt;Kostas E. Bekris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boularias_A/0/1/0/all/0/1&quot;&gt;Abdeslam Boularias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04789">
<title>Successful Nash Equilibrium Agent for a 3-Player Imperfect-Information Game. (arXiv:1804.04789v1 [cs.GT])</title>
<link>http://arxiv.org/abs/1804.04789</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating strong agents for games with more than two players is a major open
problem in AI. Common approaches are based on approximating game-theoretic
solution concepts such as Nash equilibrium, which have strong theoretical
guarantees in two-player zero-sum games, but no guarantees in non-zero-sum
games or in games with more than two players. We describe an agent that is able
to defeat a variety of realistic opponents using an exact Nash equilibrium
strategy in a 3-player imperfect-information game. This shows that, despite a
lack of theoretical guarantees, agents based on Nash equilibrium strategies can
be successful in multiplayer games after all.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganzfried_S/0/1/0/all/0/1&quot;&gt;Sam Ganzfried&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_A/0/1/0/all/0/1&quot;&gt;Austin Nowak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinales_J/0/1/0/all/0/1&quot;&gt;Joannier Pinales&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05002">
<title>Roster Evaluation Based on Classifiers for the Nurse Rostering Problem. (arXiv:1804.05002v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.05002</link>
<description rdf:parseType="Literal">&lt;p&gt;The personnel scheduling problem is a well-known NP-hard combinatorial
problem. Due to the complexity of this problem and the size of the real-world
instances, it is not possible to use exact methods, and thus heuristics,
meta-heuristics, or hyper-heuristics must be employed. The majority of
heuristic approaches are based on iterative search, where the quality of
intermediate solutions must be calculated. Unfortunately, this is
computationally highly expensive because these problems have many constraints
and some are very complex. In this study, we propose a machine learning
technique as a tool to accelerate the evaluation phase in heuristic approaches.
The solution is based on a simple classifier, which is able to determine
whether the changed solution (more precisely, the changed part of the solution)
is better than the original or not. This decision is made much faster than a
standard cost-oriented evaluation process. However, the classification process
cannot guarantee 100% correctness. Therefore, our approach, which is
illustrated using a tabu search algorithm in this study, includes a filtering
mechanism, where the classifier rejects the majority of the potentially bad
solutions and the remaining solutions are then evaluated in a standard manner.
We also show how the boosting algorithms can improve the quality of the final
solution compared with a simple classifier. We verified our proposed approach
and premises, based on standard and real-world benchmark instances, to
demonstrate the significant speedup obtained with comparable solution quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaclavik_R/0/1/0/all/0/1&quot;&gt;Roman V&amp;#xe1;clav&amp;#xed;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S%5Cr%7Bu%7Dcha_P/0/1/0/all/0/1&quot;&gt;P&amp;#x159;emysl &amp;#x160;&amp;#x16f;cha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanzalek_Z/0/1/0/all/0/1&quot;&gt;Zden&amp;#x11b;k Hanz&amp;#xe1;lek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05044">
<title>Monitoring and Executing Workflows in Linked Data Environments. (arXiv:1804.05044v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.05044</link>
<description rdf:parseType="Literal">&lt;p&gt;The W3C&apos;s Web of Things working group is aimed at addressing the
interoperability problem on the Internet of Things using Linked Data as uniform
interface. While Linked Data paves the way towards combining such devices into
integrated applications, traditional solutions for specifying the control flow
of applications do not work seamlessly with Linked Data. We therefore tackle
the problem of the specification, execution, and monitoring of applications in
the context of Linked Data. We present a novel approach that combines
workflows, semantic reasoning, and RESTful interaction into one integrated
solution. We contribute to the state of the art by (1) defining an ontology for
describing workflow models and instances, (2) providing operational semantics
for the ontology that allows for the execution and monitoring of workflow
instances, (3) presenting a benchmark to evaluate our solution. Moreover, we
showcase how we used the ontology and the operational semantics to monitor
pilots executing workflows in virtual aircraft cockpits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kafer_T/0/1/0/all/0/1&quot;&gt;Tobias K&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harth_A/0/1/0/all/0/1&quot;&gt;Andreas Harth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.05928">
<title>Modified Frank-Wolfe Algorithm for Enhanced Sparsity in Support Vector Machine Classifiers. (arXiv:1706.05928v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.05928</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes a new algorithm for training a re-weighted L2 Support
Vector Machine (SVM), inspired on the re-weighted Lasso algorithm of Cand\`es
et al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In
particular, the margin required for each training vector is set independently,
defining a new weighted SVM model. These weights are selected to be binary, and
they are automatically adapted during the training of the model, resulting in a
variation of the Frank-Wolfe optimization algorithm with essentially the same
computational complexity as the original algorithm. As shown experimentally,
this algorithm is computationally cheaper to apply since it requires less
iterations to converge, and it produces models with a sparser representation in
terms of support vectors and which are more stable with respect to the
selection of the regularization hyper-parameter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alaiz_C/0/1/0/all/0/1&quot;&gt;Carlos M. Ala&amp;#xed;z&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suykens_J/0/1/0/all/0/1&quot;&gt;Johan A. K. Suykens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.06196">
<title>Online algorithms for POMDPs with continuous state, action, and observation spaces. (arXiv:1709.06196v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.06196</link>
<description rdf:parseType="Literal">&lt;p&gt;Online solvers for partially observable Markov decision processes have been
applied to problems with large discrete state spaces, but continuous state,
action, and observation spaces remain a challenge. This paper begins by
investigating double progressive widening (DPW) as a solution to this
challenge. However, we prove that this modification alone is not sufficient
because the belief representations in the search tree collapse to a single
particle causing the algorithm to converge to a policy that is suboptimal
regardless of the computation time. This paper proposes and evaluates two new
algorithms, POMCPOW and PFT-DPW, that overcome this deficiency by using
weighted particle filtering. Simulation results show that these modifications
allow the algorithms to be successful where previous approaches fail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunberg_Z/0/1/0/all/0/1&quot;&gt;Zachary Sunberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1&quot;&gt;Mykel Kochenderfer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.08163">
<title>Intrusions in Marked Renewal Processes. (arXiv:1709.08163v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.08163</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a probabilistic model of an intrusion in a marked renewal process.
Given a process and a sequence of events, an intrusion is a subsequence of
events that is not produced by the process. Applications of the model are, for
example, online payment fraud with the fraudster taking over a user&apos;s account
and performing payments on the user&apos;s behalf, or unexpected equipment failures
due to unintended use.
&lt;/p&gt;
&lt;p&gt;We adopt Bayesian approach to infer the probability of an intrusion in a
sequence of events, a MAP subsequence of events constituting the intrusion, and
the marginal probability of each event in a sequence to belong to the
intrusion. We evaluate the model for intrusion detection on synthetic data, as
well as on anonymized data from an online payment system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolpin_D/0/1/0/all/0/1&quot;&gt;David Tolpin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01711">
<title>Coding-theorem Like Behaviour and Emergence of the Universal Distribution from Resource-bounded Algorithmic Probability. (arXiv:1711.01711v11 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01711</link>
<description rdf:parseType="Literal">&lt;p&gt;Previously referred to as `miraculous&apos; in the scientific literature because
of its powerful properties and its wide application as optimal solution to the
problem of induction/inference, (approximations to) Algorithmic Probability
(AP) and the associated Universal Distribution are (or should be) of the
greatest importance in science. Here we investigate the emergence, the rates of
emergence and convergence, and the Coding-theorem like behaviour of AP in
Turing-subuniversal models of computation. We investigate empirical
distributions of computing models in the Chomsky hierarchy. We introduce
measures of algorithmic probability and algorithmic complexity based upon
resource-bounded computation, in contrast to previously thoroughly investigated
distributions produced from the output distribution of Turing machines. This
approach allows for numerical approximations to algorithmic
(Kolmogorov-Chaitin) complexity-based estimations at each of the levels of a
computational hierarchy. We demonstrate that all these estimations are
correlated in rank and that they converge both in rank and values as a function
of computational power, despite fundamental differences between computational
models. In the context of natural processes that operate below the Turing
universal level because of finite resources and physical degradation, the
investigation of natural biases stemming from algorithmic rules may shed light
on the distribution of outcomes. We show that up to 60\% of the
simplicity/complexity bias in distributions produced even by the weakest of the
computational models can be accounted for by Algorithmic Probability in its
approximation to the Universal Distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenil_H/0/1/0/all/0/1&quot;&gt;Hector Zenil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badillo_L/0/1/0/all/0/1&quot;&gt;Liliana Badillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Orozco_S/0/1/0/all/0/1&quot;&gt;Santiago Hern&amp;#xe1;ndez-Orozco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Quiroz_F/0/1/0/all/0/1&quot;&gt;Francisco Hern&amp;#xe1;ndez-Quiroz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06306">
<title>Optimizing Interactive Systems with Data-Driven Objectives. (arXiv:1802.06306v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06306</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective optimization is essential for interactive systems to provide a
satisfactory user experience. However, it is often challenging to find an
objective to optimize for. Generally, such objectives are manually crafted and
rarely capture complex user needs accurately. Conversely, we propose an
approach that infers the objective directly from observed user interactions.
These inferences can be made regardless of prior knowledge and across different
types of user behavior. Then we introduce: Interactive System Optimizer (ISO),
a novel algorithm that uses these inferred objectives for optimization. Our
main contribution is a new general principled approach to optimizing
interactive systems using data-driven objectives. We demonstrate the high
effectiveness of ISO over several GridWorld simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grotov_A/0/1/0/all/0/1&quot;&gt;Artem Grotov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1&quot;&gt;Julia Kiseleva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1&quot;&gt;Maarten de Rijke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oosterhuis_H/0/1/0/all/0/1&quot;&gt;Harrie Oosterhuis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04326">
<title>STAIR Actions: A Video Dataset of Everyday Home Actions. (arXiv:1804.04326v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04326</link>
<description rdf:parseType="Literal">&lt;p&gt;A new large-scale video dataset for human action recognition, called STAIR
Actions is introduced. STAIR Actions contains 100 categories of action labels
representing fine-grained everyday home actions so that it can be applied to
research in various home tasks such as nursing, caring, and security. In STAIR
Actions, each video has a single action label. Moreover, for each action
category, there are around 1,000 videos that were obtained from YouTube or
produced by crowdsource workers. The duration of each video is mostly five to
six seconds. The total number of videos is 102,462. We explain how we
constructed STAIR Actions and show the characteristics of STAIR Actions
compared to existing datasets for human action recognition. Experiments with
three major models for action recognition show that STAIR Actions can train
large models and achieve good performance. STAIR Actions can be downloaded from
&lt;a href=&quot;http://actions.stair.center.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshikawa_Y/0/1/0/all/0/1&quot;&gt;Yuya Yoshikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiaqing Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeuchi_A/0/1/0/all/0/1&quot;&gt;Akikazu Takeuchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04725">
<title>Network-based protein structural classification. (arXiv:1804.04725v1 [q-bio.MN])</title>
<link>http://arxiv.org/abs/1804.04725</link>
<description rdf:parseType="Literal">&lt;p&gt;Experimental determination of protein function is resource-consuming. As an
alternative, computational prediction of protein function has received
attention. In this context, protein structural classification (PSC) can help,
by allowing for determining structural classes of currently unclassified
proteins based on their features, and then relying on the fact that proteins
with similar structures have similar functions. Existing PSC approaches rely on
sequence-based or direct (&quot;raw&quot;) 3-dimensional (3D) structure-based protein
features. Instead, we first model 3D structures as protein structure networks
(PSNs). Then, we use (&quot;processed&quot;) network-based features for PSC. We are the
first ones to do so. We propose the use of graphlets, state-of-the-art features
in many domains of network science, in the task of PSC. Moreover, because
graphlets can deal only with unweighted PSNs, and because accounting for edge
weights when constructing PSNs could improve PSC accuracy, we also propose a
deep learning framework that automatically learns network features from the
weighted PSNs. When evaluated on a large set of 9,509 CATH and 11,451 SCOP
protein domains, our proposed approaches are superior to existing PSC
approaches in terms of both accuracy and running time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rahnama_A/0/1/0/all/0/1&quot;&gt;Arash Rahnama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Newaz_K/0/1/0/all/0/1&quot;&gt;Khalique Newaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Antsaklis_P/0/1/0/all/0/1&quot;&gt;Panos J. Antsaklis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Milenkovic_T/0/1/0/all/0/1&quot;&gt;Tijana Milenkovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04732">
<title>Multimodal Unsupervised Image-to-Image Translation. (arXiv:1804.04732v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.04732</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised image-to-image translation is an important and challenging
problem in computer vision. Given an image in the source domain, the goal is to
learn the conditional distribution of corresponding images in the target
domain, without seeing any pairs of corresponding images. While this
conditional distribution is inherently multimodal, existing approaches make an
overly simplified assumption, modeling it as a deterministic one-to-one
mapping. As a result, they fail to generate diverse outputs from a given source
domain image. To address this limitation, we propose a Multimodal Unsupervised
Image-to-image Translation (MUNIT) framework. We assume that the image
representation can be decomposed into a content code that is domain-invariant,
and a style code that captures domain-specific properties. To translate an
image to another domain, we recombine its content code with a random style code
sampled from the style space of the target domain. We analyze the proposed
framework and establish several theoretical results. Extensive experiments with
comparisons to the state-of-the-art approaches further demonstrates the
advantage of the proposed framework. Moreover, our framework allows users to
control the style of translation outputs by providing an example style image.
Code and pretrained models are available at https://github.com/nvlabs/MUNIT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming-Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1&quot;&gt;Serge Belongie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1&quot;&gt;Jan Kautz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04780">
<title>Adversarial Clustering: A Grid Based Clustering Algorithm Against Active Adversaries. (arXiv:1804.04780v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.04780</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays more and more data are gathered for detecting and preventing cyber
attacks. In cyber security applications, data analytics techniques have to deal
with active adversaries that try to deceive the data analytics models and avoid
being detected. The existence of such adversarial behavior motivates the
development of robust and resilient adversarial learning techniques for various
tasks. Most of the previous work focused on adversarial classification
techniques, which assumed the existence of a reasonably large amount of
carefully labeled data instances. However, in practice, labeling the data
instances often requires costly and time-consuming human expertise and becomes
a significant bottleneck. Meanwhile, a large number of unlabeled instances can
also be used to understand the adversaries&apos; behavior. To address the above
mentioned challenges, in this paper, we develop a novel grid based adversarial
clustering algorithm. Our adversarial clustering algorithm is able to identify
the core normal regions, and to draw defensive walls around the centers of the
normal objects utilizing game theoretic ideas. Our algorithm also identifies
sub-clusters of attack objects, the overlapping areas within clusters, and
outliers which may be potential anomalies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wutao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xi_B/0/1/0/all/0/1&quot;&gt;Bowei Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kantarcioglu_M/0/1/0/all/0/1&quot;&gt;Murat Kantarcioglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04878">
<title>Learning Contracting Vector Fields For Stable Imitation Learning. (arXiv:1804.04878v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1804.04878</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new non-parametric framework for learning incrementally stable
dynamical systems x&apos; = f(x) from a set of sampled trajectories. We construct a
rich family of smooth vector fields induced by certain classes of matrix-valued
kernels, whose equilibria are placed exactly at a desired set of locations and
whose local contraction and curvature properties at various points can be
explicitly controlled using convex optimization. With curl-free kernels, our
framework may also be viewed as a mechanism to learn potential fields and
gradient flows. We develop large-scale techniques using randomized kernel
approximations in this context. We demonstrate our approach, called contracting
vector fields (CVF), on imitation learning tasks involving complex
point-to-point human handwriting motions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sindhwani_V/0/1/0/all/0/1&quot;&gt;Vikas Sindhwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1&quot;&gt;Stephen Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khansari_M/0/1/0/all/0/1&quot;&gt;Mohi Khansari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04888">
<title>Scalable and Interpretable One-class SVMs with Deep Learning and Random Fourier features. (arXiv:1804.04888v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04888</link>
<description rdf:parseType="Literal">&lt;p&gt;One-class Support Vector Machine (OC-SVM) for a long time has been one of the
most effective anomaly detection methods and widely adopted in both research as
well as industrial applications. The biggest issue for OC-SVM is, however, the
capability to operate with large and high-dimensional datasets due to
inefficient features and optimization complexity. Those problems might be
mitigated via dimensionality reduction techniques such as manifold learning or
auto-encoder. However, previous work often treats representation learning and
anomaly prediction separately. In this paper, we propose autoencoder based
one-class SVM (AE-1SVM) that brings OC-SVM, with the aid of random Fourier
features to approximate the radial basis kernel, into deep learning context by
combining it with a representation learning architecture and jointly exploit
stochastic gradient descend to obtain end-to-end training. Interestingly, this
also opens up the possible use of gradient-based attribution methods to explain
the decision making for anomaly detection, which has ever been challenging as a
result of the implicit mappings between the input space and the kernel space.
To the best of our knowledge, this is the first work to study the
interpretability of deep learning in anomaly detection. We evaluate our method
on a wide range of unsupervised anomaly detection tasks in which our end-to-end
training architecture achieves a performance significantly better than the
previous work using separate training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1&quot;&gt;Minh-Nghia Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1&quot;&gt;Ngo Anh Vien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04918">
<title>Distributed Collaborative Hashing and Its Applications in Ant Financial. (arXiv:1804.04918v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.04918</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative filtering, especially latent factor model, has been popularly
used in personalized recommendation. Latent factor model aims to learn user and
item latent factors from user-item historic behaviors. To apply it into real
big data scenarios, efficiency becomes the first concern, including offline
model training efficiency and online recommendation efficiency. In this paper,
we propose a Distributed Collaborative Hashing (DCH) model which can
significantly improve both efficiencies. Specifically, we first propose a
distributed learning framework, following the state-of-the-art parameter server
paradigm, to learn the offline collaborative model. Our model can be learnt
efficiently by distributedly computing subgradients in minibatches on workers
and updating model parameters on servers asynchronously. We then adopt hashing
technique to speedup the online recommendation procedure. Recommendation can be
quickly made through exploiting lookup hash tables. We conduct thorough
experiments on two real large-scale datasets. The experimental results
demonstrate that, comparing with the classic and state-of-the-art (distributed)
latent factor models, DCH has comparable performance in terms of recommendation
accuracy but has both fast convergence speed in offline model training
procedure and realtime efficiency in online recommendation procedure.
Furthermore, the encouraging performance of DCH is also shown for several
real-world applications in Ant Financial.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaochao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peilin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Longfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaolong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04976">
<title>Online Fall Detection using Recurrent Neural Networks. (arXiv:1804.04976v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1804.04976</link>
<description rdf:parseType="Literal">&lt;p&gt;Unintentional falls can cause severe injuries and even death, especially if
no immediate assistance is given. The aim of Fall Detection Systems (FDSs) is
to detect an occurring fall. This information can be used to trigger the
necessary assistance in case of injury. This can be done by using either
ambient-based sensors, e.g. cameras, or wearable devices. The aim of this work
is to study the technical aspects of FDSs based on wearable devices and
artificial intelligence techniques, in particular Deep Learning (DL), to
implement an effective algorithm for on-line fall detection. The proposed
classifier is based on a Recurrent Neural Network (RNN) model with underlying
Long Short-Term Memory (LSTM) blocks. The method is tested on the publicly
available SisFall dataset, with extended annotation, and compared with the
results obtained by the SisFall authors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musci_M/0/1/0/all/0/1&quot;&gt;Mirto Musci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martini_D/0/1/0/all/0/1&quot;&gt;Daniele De Martini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blago_N/0/1/0/all/0/1&quot;&gt;Nicola Blago&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Facchinetti_T/0/1/0/all/0/1&quot;&gt;Tullio Facchinetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piastra_M/0/1/0/all/0/1&quot;&gt;Marco Piastra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05018">
<title>Comparatives, Quantifiers, Proportions: A Multi-Task Model for the Learning of Quantities from Vision. (arXiv:1804.05018v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.05018</link>
<description rdf:parseType="Literal">&lt;p&gt;The present work investigates whether different quantification mechanisms
(set comparison, vague quantification, and proportional estimation) can be
jointly learned from visual scenes by a multi-task computational model. The
motivation is that, in humans, these processes underlie the same cognitive,
non-symbolic ability, which allows an automatic estimation and comparison of
set magnitudes. We show that when information about lower-complexity tasks is
available, the higher-level proportional task becomes more accurate than when
performed in isolation. Moreover, the multi-task model is able to generalize to
unseen combinations of target/non-target objects. Consistently with behavioral
evidence showing the interference of absolute number in the proportional task,
the multi-task model no longer works when asked to provide the number of target
objects in the scene.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pezzelle_S/0/1/0/all/0/1&quot;&gt;Sandro Pezzelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sorodoc_I/0/1/0/all/0/1&quot;&gt;Ionut-Teodor Sorodoc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernardi_R/0/1/0/all/0/1&quot;&gt;Raffaella Bernardi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05020">
<title>A Deep Learning Approach to Fast, Format-Agnostic Detection of Malicious Web Content. (arXiv:1804.05020v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1804.05020</link>
<description rdf:parseType="Literal">&lt;p&gt;Malicious web content is a serious problem on the Internet today. In this
paper we propose a deep learning approach to detecting malevolent web pages.
While past work on web content detection has relied on syntactic parsing or on
emulation of HTML and Javascript to extract features, our approach operates
directly on a language-agnostic stream of tokens extracted directly from static
HTML files with a simple regular expression. This makes it fast enough to
operate in high-frequency data contexts like firewalls and web proxies, and
allows it to avoid the attack surface exposure of complex parsing and emulation
code. Unlike well-known approaches such as bag-of-words models, which ignore
spatial information, our neural network examines content at hierarchical
spatial scales, allowing our model to capture locality and yielding superior
accuracy compared to bag-of-words baselines. Our proposed architecture achieves
a 97.5% detection rate at a 0.1% false positive rate, and classifies
small-batched web pages at a rate of over 100 per second on commodity hardware.
The speed and accuracy of our approach makes it appropriate for deployment to
endpoints, firewalls, and web proxies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxe_J/0/1/0/all/0/1&quot;&gt;Joshua Saxe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harang_R/0/1/0/all/0/1&quot;&gt;Richard Harang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wild_C/0/1/0/all/0/1&quot;&gt;Cody Wild&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanders_H/0/1/0/all/0/1&quot;&gt;Hillary Sanders&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1601.08057">
<title>On the Geometric Ergodicity of Hamiltonian Monte Carlo. (arXiv:1601.08057v3 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1601.08057</link>
<description rdf:parseType="Literal">&lt;p&gt;We establish general conditions under which Markov chains produced by the
Hamiltonian Monte Carlo method will and will not be geometrically ergodic. We
consider implementations with both position-independent and position-dependent
integration times. In the former case we find that the conditions for geometric
ergodicity are essentially a non-vanishing gradient of the log-density which
asymptotically points towards the centre of the space and does not grow faster
than linearly. In an idealised scenario in which the integration time is
allowed to change in different regions of the space, we show that geometric
ergodicity can be recovered for a much broader class of tail behaviours,
leading to some guidelines for the choice of this free parameter in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Livingstone_S/0/1/0/all/0/1&quot;&gt;Samuel Livingstone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Betancourt_M/0/1/0/all/0/1&quot;&gt;Michael Betancourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Byrne_S/0/1/0/all/0/1&quot;&gt;Simon Byrne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1&quot;&gt;Mark Girolami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.00578">
<title>sgmcmc: An R Package for Stochastic Gradient Markov Chain Monte Carlo. (arXiv:1710.00578v3 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1710.00578</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces the R package sgmcmc; which can be used for Bayesian
inference on problems with large datasets using stochastic gradient Markov
chain Monte Carlo (SGMCMC). Traditional Markov chain Monte Carlo (MCMC)
methods, such as Metropolis-Hastings, are known to run prohibitively slowly as
the dataset size increases. SGMCMC solves this issue by only using a subset of
data at each iteration. SGMCMC requires calculating gradients of the log
likelihood and log priors, which can be time consuming and error prone to
perform by hand. The sgmcmc package calculates these gradients itself using
automatic differentiation, making the implementation of these methods much
easier. To do this, the package uses the software library TensorFlow, which has
a variety of statistical distributions and mathematical operations as standard,
meaning a wide class of models can be built using this framework. SGMCMC has
become widely adopted in the machine learning literature, but less so in the
statistics community. We believe this may be partly due to lack of software;
this package aims to bridge this gap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baker_J/0/1/0/all/0/1&quot;&gt;Jack Baker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fearnhead_P/0/1/0/all/0/1&quot;&gt;Paul Fearnhead&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fox_E/0/1/0/all/0/1&quot;&gt;Emily B. Fox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nemeth_C/0/1/0/all/0/1&quot;&gt;Christopher Nemeth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09707">
<title>Deep learning for universal linear embeddings of nonlinear dynamics. (arXiv:1712.09707v2 [math.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1712.09707</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying coordinate transformations that make strongly nonlinear dynamics
approximately linear is a central challenge in modern dynamical systems. These
transformations have the potential to enable prediction, estimation, and
control of nonlinear systems using standard linear theory. The Koopman operator
has emerged as a leading data-driven embedding, as eigenfunctions of this
operator provide intrinsic coordinates that globally linearize the dynamics.
However, identifying and representing these eigenfunctions has proven to be
mathematically and computationally challenging. This work leverages the power
of deep learning to discover representations of Koopman eigenfunctions from
trajectory data of dynamical systems. Our network is parsimonious and
interpretable by construction, embedding the dynamics on a low-dimensional
manifold that is of the intrinsic rank of the dynamics and parameterized by the
Koopman eigenfunctions. In particular, we identify nonlinear coordinates on
which the dynamics are globally linear using a modified auto-encoder. We also
generalize Koopman representations to include a ubiquitous class of systems
that exhibit continuous spectra, ranging from the simple pendulum to nonlinear
optics and broadband turbulence. Our framework parametrizes the continuous
frequency using an auxiliary network, enabling a compact and efficient
embedding at the intrinsic rank, while connecting our models to half a century
of asymptotics. In this way, we benefit from the power and generality of deep
learning, while retaining the physical interpretability of Koopman embeddings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lusch_B/0/1/0/all/0/1&quot;&gt;Bethany Lusch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kutz_J/0/1/0/all/0/1&quot;&gt;J. Nathan Kutz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Brunton_S/0/1/0/all/0/1&quot;&gt;Steven L. Brunton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09177">
<title>Balanced Random Survival Forests for Extremely Unbalanced, Right Censored Data. (arXiv:1803.09177v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09177</link>
<description rdf:parseType="Literal">&lt;p&gt;Accuracies of survival models for life expectancy prediction as well as
critical-care applications are significantly compromised due to the sparsity of
samples and extreme imbalance between the survival (usually, the majority) and
mortality class sizes. While a recent random survival forest (RSF) model
overcomes the limitations of the proportional hazard assumption, an imbalance
in the data results in an underestimation (overestimation) of the hazard of the
mortality (survival) classes. A balanced random survival forests (BRSF) model,
based on training the RSF model with data generated from a synthetic minority
sampling scheme is presented to address this gap. Theoretical results on the
effect of balancing on prediction accuracies in BRSF are reported. Benchmarking
studies were conducted using five datasets with different levels of class
imbalance from public repositories and an imbalanced dataset of 267 acute
cardiac patients, collected at the Heart, Artery, and Vein Center of Fresno,
CA. Investigations suggest that BRSF provides an improved discriminatory
strength between the survival and the mortality classes. It outperformed both
optimized Cox (without and with balancing) and RSF with an average reduction of
55\% in the prediction error over the next best alternative.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Afrin_K/0/1/0/all/0/1&quot;&gt;Kahkashan Afrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Illangovan_G/0/1/0/all/0/1&quot;&gt;Gurudev Illangovan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srivatsa_S/0/1/0/all/0/1&quot;&gt;Sanjay S. Srivatsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bukkapatnam_S/0/1/0/all/0/1&quot;&gt;Satish T. S. Bukkapatnam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11274">
<title>PIMKL: Pathway Induced Multiple Kernel Learning. (arXiv:1803.11274v2 [q-bio.MN] UPDATED)</title>
<link>http://arxiv.org/abs/1803.11274</link>
<description rdf:parseType="Literal">&lt;p&gt;Reliable identification of molecular biomarkers is essential for accurate
patient stratification. While state-of-the-art machine learning approaches for
sample classification continue to push boundaries in terms of performance, most
of these methods are not able to integrate different data types and lack
generalization power limiting their application in a clinical setting.
Furthermore, many methods behave as black boxes, and we have very little
understanding about the mechanisms that lead to the prediction provided. While
opaqueness concerning machine behaviour might not be a problem in deterministic
domains, in health care, providing explanations about the molecular factors and
phenotypes that are driving the classification is crucial to build trust in the
performance of the predictive system. We propose Pathway Induced Multiple
Kernel Learning (PIMKL), a novel methodology to reliably classify samples that
can also help gain insights into the molecular mechanisms that underlie the
classification. PIMKL exploits prior knowledge in the form of a molecular
interaction network and annotated gene sets, by optimizing a mixture of
pathway-induced kernels using a Multiple Kernel Learning (MKL) algorithm, an
approach that has demonstrated excellent performance in different machine
learning applications. After optimizing the combination of kernels for
prediction of a specific phenotype, the model provides a stable molecular
signature that can be interpreted in the light of the ingested prior knowledge
and that can be used in transfer learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Manica_M/0/1/0/all/0/1&quot;&gt;Matteo Manica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cadow_J/0/1/0/all/0/1&quot;&gt;Joris Cadow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mathis_R/0/1/0/all/0/1&quot;&gt;Roland Mathis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Martinez_M/0/1/0/all/0/1&quot;&gt;Mar&amp;#xed;a Rodr&amp;#xed;guez Mart&amp;#xed;nez&lt;/a&gt;</dc:creator>
</item></rdf:RDF>