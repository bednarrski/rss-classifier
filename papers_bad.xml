<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-15T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.07690"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04946"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05032"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04099"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04617"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04866"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04880"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04928"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05092"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05140"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.02513"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05637"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.07519"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04732"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12111"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09762"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04855"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04523"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1610.07690">
<title>Operational Calculus for Differentiable Programming. (arXiv:1610.07690v5 [cs.FL] UPDATED)</title>
<link>http://arxiv.org/abs/1610.07690</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we present a theoretical model for differentiable programming.
We present an algebraic language that enables both implementations and analysis
of differentiable programs by way of operational calculus.
&lt;/p&gt;
&lt;p&gt;To this purpose, we develop an abstract computational model of automatically
differentiable programs of arbitrary order. In the model, programs are elements
of programming spaces and are viewed as maps from the virtual memory space to
itself. Virtual memory space is an algebra of programs, an algebraic data
structure one can calculate with.
&lt;/p&gt;
&lt;p&gt;We define the operator of differentiation ($\partial$) on programming spaces
and, using its powers, implement the general shift operator and the operator of
program composition. We provide the formula for the expansion of a
differentiable program into an infinite tensor series in terms of the powers of
$\partial$. We express the operator of program composition in terms of the
generalized shift operator and $\partial$, which implements a differentiable
composition in the language. We prove that our language enables differentiable
derivatives of programs by the use of the order reduction map. We demonstrate
our models algebraic power over analytic properties of differentiable programs
by analysing iterators, considering fractional iterations and their iterating
velocities. We than solve the special case of ReduceSum.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajovic_Z/0/1/0/all/0/1&quot;&gt;&amp;#x17d;iga Sajovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuk_M/0/1/0/all/0/1&quot;&gt;Martin Vuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04819">
<title>VizML: A Machine Learning Approach to Visualization Recommendation. (arXiv:1808.04819v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1808.04819</link>
<description rdf:parseType="Literal">&lt;p&gt;Data visualization should be accessible for all analysts with data, not just
the few with technical expertise. Visualization recommender systems aim to
lower the barrier to exploring basic visualizations by automatically generating
results for analysts to search and select, rather than manually specify. Here,
we demonstrate a novel machine learning-based approach to visualization
recommendation that learns visualization design choices from a large corpus of
datasets and associated visualizations. First, we identify five key design
choices made by analysts while creating visualizations, such as selecting a
visualization type and choosing to encode a column along the X- or Y-axis. We
train models to predict these design choices using one million
dataset-visualization pairs collected from a popular online visualization
platform. Neural networks predict these design choices with high accuracy
compared to baseline models. We report and interpret feature importances from
one of these baseline models. To evaluate the generalizability and uncertainty
of our approach, we benchmark with a crowdsourced test set, and show that the
performance of our model is comparable to human performance when predicting
consensus visualization type, and exceeds that of other ML-based systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kevin Z. Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakker_M/0/1/0/all/0/1&quot;&gt;Michiel A. Bakker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stephen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraska_T/0/1/0/all/0/1&quot;&gt;Tim Kraska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hidalgo_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;sar A. Hidalgo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04946">
<title>Automatic Derivation Of Formulas Using Reforcement Learning. (arXiv:1808.04946v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.04946</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an artificial intelligence algorithm that can be used to
derive formulas from various scientific disciplines called automatic derivation
machine. First, the formula is abstractly expressed as a multiway tree model,
and then each step of the formula derivation transformation is abstracted as a
mapping of multiway trees. Derivation steps similar can be expressed as a
reusable formula template by a multiway tree map. After that, the formula
multiway tree is eigen-encoded to feature vectors construct the feature space
of formulas, the Q-learning model using in this feature space can achieve the
derivation by making training data from derivation process. Finally, an
automatic formula derivation machine is made to choose the next derivation step
based on the current state and object. We also make an example about the
nuclear reactor physics problem to show how the automatic derivation machine
works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;MinZhong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05032">
<title>Deep RTS: A Game Environment for Deep Reinforcement Learning in Real-Time Strategy Games. (arXiv:1808.05032v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.05032</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) is an area of research that has blossomed
tremendously in recent years and has shown remarkable potential for artificial
intelligence based opponents in computer games. This success is primarily due
to the vast capabilities of convolutional neural networks, that can extract
useful features from noisy and complex data. Games are excellent tools to test
and push the boundaries of novel RL algorithms because they give valuable
insight into how well an algorithm can perform in isolated environments without
the real-life consequences. Real-time strategy games (RTS) is a genre that has
tremendous complexity and challenges the player in short and long-term
planning. There is much research that focuses on applied RL in RTS games, and
novel advances are therefore anticipated in the not too distant future.
However, there are to date few environments for testing RTS AIs. Environments
in the literature are often either overly simplistic, such as microRTS, or
complex and without the possibility for accelerated learning on consumer
hardware like StarCraft II. This paper introduces the Deep RTS game environment
for testing cutting-edge artificial intelligence algorithms for RTS games. Deep
RTS is a high-performance RTS game made specifically for artificial
intelligence research. It supports accelerated learning, meaning that it can
learn at a magnitude of 50 000 times faster compared to existing RTS games.
Deep RTS has a flexible configuration, enabling research in several different
RTS scenarios, including partially observable state-spaces and map complexity.
We show that Deep RTS lives up to our promises by comparing its performance
with microRTS, ELF, and StarCraft II on high-end consumer hardware. Using Deep
RTS, we show that a Deep Q-Network agent beats random-play agents over 70% of
the time. Deep RTS is publicly available at https://github.com/cair/DeepRTS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1&quot;&gt;Per-Arne Andersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1&quot;&gt;Morten Goodwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1&quot;&gt;Ole-Christoffer Granmo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04099">
<title>Trust-Aware Decision Making for Human-Robot Collaboration: Model Learning and Planning. (arXiv:1801.04099v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04099</link>
<description rdf:parseType="Literal">&lt;p&gt;Trust in autonomy is essential for effective human-robot collaboration and
user adoption of autonomous systems such as robot assistants. This paper
introduces a computational model which integrates trust into robot
decision-making. Specifically, we learn from data a partially observable Markov
decision process (POMDP) with human trust as a latent variable. The trust-POMDP
model provides a principled approach for the robot to (i) infer the trust of a
human teammate through interaction, (ii) reason about the effect of its own
actions on human trust, and (iii) choose actions that maximize team performance
over the long term. We validated the model through human subject experiments on
a table-clearing task in simulation (201 participants) and with a real robot
(20 participants). In our studies, the robot builds human trust by manipulating
low-risk objects first. Interestingly, the robot sometimes fails intentionally
in order to modulate human trust and achieve the best team performance. These
results show that the trust-POMDP calibrates trust to improve human-robot team
performance over the long term. Further, they highlight that maximizing trust
alone does not always lead to the best performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Min Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaidis_S/0/1/0/all/0/1&quot;&gt;Stefanos Nikolaidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soh_H/0/1/0/all/0/1&quot;&gt;Harold Soh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;David Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1&quot;&gt;Siddhartha Srinivasa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04617">
<title>Joint Ground and Aerial Package Delivery Services: A Stochastic Optimization Approach. (arXiv:1808.04617v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1808.04617</link>
<description rdf:parseType="Literal">&lt;p&gt;Unmanned aerial vehicles (UAVs), also known as drones, have emerged as a
promising mode of fast, energy-efficient, and cost-effective package delivery.
A considerable number of works have studied different aspects of drone package
delivery service by a supplier, one of which is delivery planning. However,
existing works addressing the planning issues consider a simple case of perfect
delivery without service interruption, e.g., due to accident which is common
and realistic. Therefore, this paper introduces the joint ground and aerial
delivery service optimization and planning (GADOP) framework. The framework
explicitly incorporates uncertainty of drone package delivery, i.e., takeoff
and breakdown conditions. The GADOP framework aims to minimize the total
delivery cost given practical constraints, e.g., traveling distance limit.
Specifically, we formulate the GADOP framework as a three-stage stochastic
integer programming model. To deal with the high complexity issue of the
problem, a decomposition method is adopted. Then, the performance of the GADOP
framework is evaluated by using two data sets including Solomon benchmark suite
and the real data from one of the Singapore logistics companies. The
performance evaluation clearly shows that the GADOP framework can achieve
significantly lower total payment than that of the baseline methods which do
not take uncertainty into account.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawadsitang_S/0/1/0/all/0/1&quot;&gt;Suttinee Sawadsitang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1&quot;&gt;Dusit Niyato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1&quot;&gt;Puay-Siew Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Ping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04866">
<title>Mitigating Sybils in Federated Learning Poisoning. (arXiv:1808.04866v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04866</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) over distributed data is relevant to a variety of
domains. Existing approaches, such as federated learning, compose the outputs
computed by a group of devices at a central aggregator and run multi-round
algorithms to generate a globally shared model. Unfortunately, such approaches
are susceptible to a variety of attacks, including model poisoning, which is
made substantially worse in the presence of sybils.
&lt;/p&gt;
&lt;p&gt;In this paper we first evaluate the vulnerability of federated learning to
sybil-based poisoning attacks. We then describe FoolsGold, a novel defense to
this problem that identifies poisoning sybils based on the diversity of client
contributions in the distributed learning process. Unlike prior work, our
system does not assume that the attackers are in the minority, requires no
auxiliary information outside of the learning process, and makes fewer
assumptions about clients and their data.
&lt;/p&gt;
&lt;p&gt;In our evaluation we show that FoolsGold exceeds the capabilities of existing
state of the art approaches to countering ML poisoning attacks. Our results
hold for a variety of conditions, including different distributions of data,
varying poisoning targets, and various attack strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fung_C/0/1/0/all/0/1&quot;&gt;Clement Fung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1&quot;&gt;Chris J.M. Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beschastnikh_I/0/1/0/all/0/1&quot;&gt;Ivan Beschastnikh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04875">
<title>Multi-user Communication Networks: A Coordinated Multi-armed Bandit Approach. (arXiv:1808.04875v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04875</link>
<description rdf:parseType="Literal">&lt;p&gt;Communication networks shared by many users are a widespread challenge
nowadays. In this paper we address several aspects of this challenge
simultaneously: learning unknown stochastic network characteristics, sharing
resources with other users while keeping coordination overhead to a minimum.
The proposed solution combines Multi-Armed Bandit learning with a lightweight
signalling-based coordination scheme, and ensures convergence to a stable
allocation of resources. Our work considers single-user level algorithms for
two scenarios: an unknown fixed number of users, and a dynamic number of users.
Analytic performance guarantees, proving convergence to stable marriage
configurations, are presented for both setups. The algorithms are designed
based on a system-wide perspective, rather than focusing on single user
welfare. Thus, maximal resource utilization is ensured. An extensive
experimental analysis covers convergence to a stable configuration as well as
reward maximization. Experiments are carried out over a wide range of setups,
demonstrating the advantages of our approach over existing state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avner_O/0/1/0/all/0/1&quot;&gt;Orly Avner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1&quot;&gt;Shie Mannor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04880">
<title>A Precision Environment-Wide Association Study of Hypertension via Supervised Cadre Models. (arXiv:1808.04880v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.04880</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem in precision health of grouping people into
subpopulations based on their degree of vulnerability to a risk factor. These
subpopulations cannot be discovered with traditional clustering techniques
because their quality is evaluated with a supervised metric: the ease of
modeling a response variable over observations within them. Instead, we apply
the supervised cadre model (SCM), which does use this metric. We extend the SCM
formalism so that it may be applied to multivariate regression and binary
classification problems. We also develop a way to use conditional entropy to
assess the confidence in the process by which a subject is assigned their
cadre. Using the SCM, we generalize the environment-wide association study
(EWAS) workflow to be able to model heterogeneity in population risk. In our
EWAS, we consider more than two hundred environmental exposure factors and find
their association with diastolic blood pressure, systolic blood pressure, and
hypertension. This requires adapting the SCM to be applicable to data generated
by a complex survey design. After correcting for false positives, we found 25
exposure variables that had a significant association with at least one of our
response variables. Eight of these were significant for a discovered
subpopulation but not for the overall population. Some of these associations
have been identified by previous researchers, while others appear to be novel
associations. We examine several learned subpopulations in detail, and we find
that they are interpretable and that they suggest further research questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+New_A/0/1/0/all/0/1&quot;&gt;Alexander New&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bennett_K/0/1/0/all/0/1&quot;&gt;Kristin P. Bennett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04928">
<title>Deep EHR: Chronic Disease Prediction Using Medical Notes. (arXiv:1808.04928v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04928</link>
<description rdf:parseType="Literal">&lt;p&gt;Early detection of preventable diseases is important for better disease
management, improved inter-ventions, and more efficient health-care resource
allocation. Various machine learning approacheshave been developed to utilize
information in Electronic Health Record (EHR) for this task. Majorityof
previous attempts, however, focus on structured fields and lose the vast amount
of information inthe unstructured notes. In this work we propose a general
multi-task framework for disease onsetprediction that combines both free-text
medical notes and structured information. We compareperformance of different
deep learning architectures including CNN, LSTM and hierarchical models.In
contrast to traditional text-based prediction models, our approach does not
require disease specificfeature engineering, and can handle negations and
numerical values that exist in the text. Ourresults on a cohort of about 1
million patients show that models using text outperform modelsusing just
structured data, and that models capable of using numerical values and
negations in thetext, in addition to the raw text, further improve performance.
Additionally, we compare differentvisualization methods for medical
professionals to interpret model predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingshu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zachariah Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razavian_N/0/1/0/all/0/1&quot;&gt;Narges Razavian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05092">
<title>ACVAE-VC: Non-parallel many-to-many voice conversion with auxiliary classifier variational autoencoder. (arXiv:1808.05092v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.05092</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a non-parallel many-to-many voice conversion (VC) method
using a variant of the conditional variational autoencoder (VAE) called an
auxiliary classifier VAE (ACVAE). The proposed method has three key features.
First, it adopts fully convolutional architectures to devise the encoder and
decoder networks so that the networks can learn conversion rules that capture
time dependencies in the acoustic feature sequences of source and target
speech. Second, it uses an information-theoretic regularization for the model
training to ensure that the information in the attribute class label will not
be lost in the conversion process. With regular CVAEs, the encoder and decoder
are free to ignore the attribute class label input. This can be problematic
since in such a situation, the attribute class label will have little effect on
controlling the voice characteristics of input speech at test time. Such
situations can be avoided by introducing an auxiliary classifier and training
the encoder and decoder so that the attribute classes of the decoder outputs
are correctly predicted by the classifier. Third, it avoids producing
buzzy-sounding speech at test time by simply transplanting the spectral details
of input speech into its converted version. Subjective evaluation experiments
revealed that this simple method worked reasonably well on a non-parallel
many-to-many speaker identity conversion task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kameoka_H/0/1/0/all/0/1&quot;&gt;Hirokazu Kameoka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kaneko_T/0/1/0/all/0/1&quot;&gt;Takuhiro Kaneko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tanaka_K/0/1/0/all/0/1&quot;&gt;Kou Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hojo_N/0/1/0/all/0/1&quot;&gt;Nobukatsu Hojo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05140">
<title>A Framework for Automated Cellular Network Tuning with Reinforcement Learning. (arXiv:1808.05140v1 [cs.NI])</title>
<link>http://arxiv.org/abs/1808.05140</link>
<description rdf:parseType="Literal">&lt;p&gt;Tuning cellular network performance against always occurring wireless
impairments can dramatically improve reliability to end users. In this paper,
we formulate cellular network performance tuning as a reinforcement learning
(RL) problem and provide a solution to improve the signal to
interference-plus-noise ratio (SINR) for indoor and outdoor environments. By
leveraging the ability of Q-learning to estimate future SINR improvement
rewards, we propose two algorithms: (1) voice over LTE (VoLTE) downlink closed
loop power control (PC) and (2) self-organizing network (SON) fault management.
The VoLTE PC algorithm uses RL to adjust the indoor base station transmit power
so that the effective SINR meets the target SINR. The SON fault management
algorithm uses RL to improve the performance of an outdoor cluster by resolving
faults in the network through configuration management. Both algorithms exploit
measurements from the connected users, wireless impairments, and relevant
configuration parameters to solve a non-convex SINR optimization problem using
RL. Simulation results show that our proposed RL based algorithms outperform
the industry standards today in realistic cellular communication environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mismar_F/0/1/0/all/0/1&quot;&gt;Faris B. Mismar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jinseok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1&quot;&gt;Brian L. Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.02513">
<title>Functorial Hierarchical Clustering with Overlaps. (arXiv:1609.02513v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1609.02513</link>
<description rdf:parseType="Literal">&lt;p&gt;This work draws inspiration from three important sources of research on
dissimilarity-based clustering and intertwines those three threads into a
consistent principled functorial theory of clustering. Those three are the
overlapping clustering of Jardine and Sibson, the functorial approach of
Carlsson and M\&apos;{e}moli to partition-based clustering, and the Isbell/Dress
school&apos;s study of injective envelopes. Carlsson and M\&apos;{e}moli introduce the
idea of viewing clustering methods as functors from a category of metric spaces
to a category of clusters, with functoriality subsuming many desirable
properties. Our first series of results extends their theory of functorial
clustering schemes to methods that allow overlapping clusters in the spirit of
Jardine and Sibson. This obviates some of the unpleasant effects of chaining
that occur, for example with single-linkage clustering. We prove an equivalence
between these general overlapping clustering functors and projections of weight
spaces to what we term clustering domains, by focusing on the order structure
determined by the morphisms. As a specific application of this machinery, we
are able to prove that there are no functorial projections to cut metrics, or
even to tree metrics. Finally, although we focus less on the construction of
clustering methods (clustering domains) derived from injective envelopes, we
lay out some preliminary results, that hopefully will give a feel for how the
third leg of the stool comes into play.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Culbertson_J/0/1/0/all/0/1&quot;&gt;Jared Culbertson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guralnik_D/0/1/0/all/0/1&quot;&gt;Dan P. Guralnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stiller_P/0/1/0/all/0/1&quot;&gt;Peter F. Stiller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05637">
<title>cGANs with Projection Discriminator. (arXiv:1802.05637v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05637</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel, projection based way to incorporate the conditional
information into the discriminator of GANs that respects the role of the
conditional information in the underlining probabilistic model. This approach
is in contrast with most frameworks of conditional GANs used in application
today, which use the conditional information by concatenating the (embedded)
conditional vector to the feature vectors. With this modification, we were able
to significantly improve the quality of the class conditional image generation
on ILSVRC2012 (ImageNet) 1000-class image dataset from the current
state-of-the-art result, and we achieved this with a single pair of a
discriminator and a generator. We were also able to extend the application to
super-resolution and succeeded in producing highly discriminative
super-resolution images. This new structure also enabled high quality category
transformation based on parametric functional transformation of conditional
batch normalization layers in the generator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miyato_T/0/1/0/all/0/1&quot;&gt;Takeru Miyato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyama_M/0/1/0/all/0/1&quot;&gt;Masanori Koyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.07519">
<title>DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems. (arXiv:1803.07519v4 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/1803.07519</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) defines a new data-driven programming paradigm that
constructs the internal system logic of a crafted neuron network through a set
of training data. We have seen wide adoption of DL in many safety-critical
scenarios. However, a plethora of studies have shown that the state-of-the-art
DL systems suffer from various vulnerabilities which can lead to severe
consequences when applied to real-world applications. Currently, the testing
adequacy of a DL system is usually measured by the accuracy of test data.
Considering the limitation of accessible high quality test data, good accuracy
performance on test data can hardly provide confidence to the testing adequacy
and generality of DL systems. Unlike traditional software systems that have
clear and controllable logic and functionality, the lack of interpretability in
a DL system makes system analysis and defect detection difficult, which could
potentially hinder its real-world deployment. In this paper, we propose
DeepGauge, a set of multi-granularity testing criteria for DL systems, which
aims at rendering a multi-faceted portrayal of the testbed. The in-depth
evaluation of our proposed testing criteria is demonstrated on two well-known
datasets, five DL systems, and with four state-of-the-art adversarial attack
techniques against DL. The potential usefulness of DeepGauge sheds light on the
construction of more generic and robust DL systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1&quot;&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiyuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1&quot;&gt;Minhui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chunyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_T/0/1/0/all/0/1&quot;&gt;Ting Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jianjun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yadong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04732">
<title>Multimodal Unsupervised Image-to-Image Translation. (arXiv:1804.04732v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04732</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised image-to-image translation is an important and challenging
problem in computer vision. Given an image in the source domain, the goal is to
learn the conditional distribution of corresponding images in the target
domain, without seeing any pairs of corresponding images. While this
conditional distribution is inherently multimodal, existing approaches make an
overly simplified assumption, modeling it as a deterministic one-to-one
mapping. As a result, they fail to generate diverse outputs from a given source
domain image. To address this limitation, we propose a Multimodal Unsupervised
Image-to-image Translation (MUNIT) framework. We assume that the image
representation can be decomposed into a content code that is domain-invariant,
and a style code that captures domain-specific properties. To translate an
image to another domain, we recombine its content code with a random style code
sampled from the style space of the target domain. We analyze the proposed
framework and establish several theoretical results. Extensive experiments with
comparisons to the state-of-the-art approaches further demonstrates the
advantage of the proposed framework. Moreover, our framework allows users to
control the style of translation outputs by providing an example style image.
Code and pretrained models are available at https://github.com/nvlabs/MUNIT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming-Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1&quot;&gt;Serge Belongie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1&quot;&gt;Jan Kautz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12111">
<title>Dynamic Advisor-Based Ensemble (dynABE): Case Study in Stock Trend Prediction of Critical Metal Companies. (arXiv:1805.12111v3 [q-fin.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1805.12111</link>
<description rdf:parseType="Literal">&lt;p&gt;The demand for metals by modern technology has been shifting from common base
metals to a variety of minor metals, such as cobalt or indium. The industrial
importance and limited geological availability of some minor metals have led to
them being considered more &quot;critical,&quot; and there is a growing investment
interest in such critical metals and their producing companies. In this
research, we create a novel framework, Dynamic Advisor-Based Ensemble (dynABE),
for stock prediction and use critical metal companies as case study. dynABE
uses domain knowledge to diversify the feature set by dividing them into
different &quot;advisors.&quot; creates high-level ensembles with complex base models for
each advisor, and combines the advisors together dynamically during validation
with a novel and effective online update strategy. We test dynABE on three
cobalt-related companies, and it achieves the best-case misclassification error
of 31.12% and excess return of 477% compared to the stock itself in a year and
a half. In addition to presenting an effective stock prediction model with
decent profitabilities, this research further analyzes dynABE to visualize how
it works in practice, which also yields discoveries of its interesting
behaviors when processing time-series data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09762">
<title>Boulevard: Regularized Stochastic Gradient Boosted Trees and Their Limiting Distribution. (arXiv:1806.09762v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1806.09762</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper examines a novel gradient boosting framework for regression. We
regularize gradient boosted trees by introducing subsampling and employ a
modified shrinkage algorithm so that at every boosting stage the estimate is
given by an average of trees. The resulting algorithm, titled Boulevard, is
shown to converge as the number of trees grows. We also demonstrate a central
limit theorem for this limit, allowing a characterization of uncertainty for
predictions. A simulation study and real world examples provide support for
both the predictive accuracy of the model and its limiting behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yichen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hooker_G/0/1/0/all/0/1&quot;&gt;Giles Hooker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04855">
<title>A feature agnostic approach for glaucoma detection in OCT volumes. (arXiv:1807.04855v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1807.04855</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical coherence tomography (OCT) based measurements of retinal layer
thickness, such as the retinal nerve fibre layer (RNFL) and the ganglion cell
with inner plexiform layer (GCIPL) are commonly used for the diagnosis and
monitoring of glaucoma. Previously, machine learning techniques have utilized
segmentation-based imaging features such as the peripapillary RNFL thickness
and the cup-to-disc ratio. Here, we propose a deep learning technique that
classifies eyes as healthy or glaucomatous directly from raw, unsegmented OCT
volumes of the optic nerve head (ONH) using a 3D Convolutional Neural Network
(CNN). We compared the accuracy of this technique with various feature-based
machine learning algorithms and demonstrated the superiority of the proposed
deep learning based method.
&lt;/p&gt;
&lt;p&gt;Logistic regression was found to be the best performing classical machine
learning technique with an AUC of 0.89. In direct comparison, the deep learning
approach achieved a substantially higher AUC of 0.94 with the additional
advantage of providing insight into which regions of an OCT volume are
important for glaucoma detection.
&lt;/p&gt;
&lt;p&gt;Computing Class Activation Maps (CAM), we found that the CNN identified
neuroretinal rim and optic disc cupping as well as the lamina cribrosa (LC) and
its surrounding areas as the regions significantly associated with the glaucoma
classification. These regions anatomically correspond to the well established
and commonly used clinical markers for glaucoma diagnosis such as increased cup
volume, cup diameter, and neuroretinal rim thinning at the superior and
inferior segments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maetschke_S/0/1/0/all/0/1&quot;&gt;Stefan Maetschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antony_B/0/1/0/all/0/1&quot;&gt;Bhavna Antony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishikawa_H/0/1/0/all/0/1&quot;&gt;Hiroshi Ishikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wollstein_G/0/1/0/all/0/1&quot;&gt;Gadi Wollstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuman_J/0/1/0/all/0/1&quot;&gt;Joel S. Schuman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garvani_R/0/1/0/all/0/1&quot;&gt;Rahil Garvani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04523">
<title>Adaptive Sampling for Convex Regression. (arXiv:1808.04523v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.04523</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce the first principled adaptive-sampling procedure
for learning a convex function in the $L_\infty$ norm, a problem that arises
often in economics, psychology, and the social sciences. We present a
function-specific measure of complexity and use it to prove that our algorithm
is information-theoretically near-optimal in a strong, function-specific sense.
We also corroborate our theoretical contributions with extensive numerical
experiments, finding that our method substantially outperforms passive, uniform
sampling for favorable synthetic and data-derived functions in low-noise
settings with large sampling budgets. Our results also suggest an idealized
`oracle strategy&apos;, which we use to gauge the potential for deploying the
adaptive-sampling strategy on any function in any particular setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1&quot;&gt;Max Simchowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1&quot;&gt;Kevin Jamieson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suchow_J/0/1/0/all/0/1&quot;&gt;Jordan W. Suchow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1&quot;&gt;Thomas L. Griffiths&lt;/a&gt;</dc:creator>
</item></rdf:RDF>