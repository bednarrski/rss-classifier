<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02389"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02397"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02328"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01097"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02258"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02262"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02297"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02303"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02322"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02406"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02416"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02544"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10723"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09597"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00553"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02125"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02128"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02150"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02188"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02233"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02264"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02287"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02290"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02326"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02350"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04020"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10254"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01013"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06124"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01085"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.02389">
<title>Generative models on accelerated neuromorphic hardware. (arXiv:1807.02389v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.02389</link>
<description rdf:parseType="Literal">&lt;p&gt;The traditional von Neumann computer architecture faces serious obstacles,
both in terms of miniaturization and in terms of heat production, with
increasing performance. Artificial neural (neuromorphic) substrates represent
an alternative approach to tackle this challenge. A special subset of these
systems follow the principle of &quot;physical modeling&quot; as they directly use the
physical properties of the underlying substrate to realize computation with
analog components. While these systems are potentially faster and/or more
energy efficient than conventional computers, they require robust models that
can cope with their inherent limitations in terms of controllability and range
of parameters. A natural source of inspiration for robust models is
neuroscience as the brain faces similar challenges. It has been recently
suggested that sampling with the spiking dynamics of neurons is potentially
suitable both as a generative and a discriminative model for artificial neural
substrates. In this work we present the implementation of sampling with leaky
integrate-and-fire neurons on the BrainScaleS physical model system. We prove
the sampling property of the network and demonstrate its applicability to
high-dimensional datasets. The required stochasticity is provided by a spiking
random network on the same substrate. This allows the system to run in a
self-contained fashion without external stochastic input from the host
environment. The implementation provides a basis as a building block in
large-scale biologically relevant emulations, as a fast approximate sampler or
as a framework to realize on-chip learning on (future generations of)
accelerated spiking neuromorphic hardware. Our work contributes to the
development of robust computation on physical model systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kungl_A/0/1/0/all/0/1&quot;&gt;Akos F. Kungl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmitt_S/0/1/0/all/0/1&quot;&gt;Sebastian Schmitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klahn_J/0/1/0/all/0/1&quot;&gt;Johann Kl&amp;#xe4;hn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_P/0/1/0/all/0/1&quot;&gt;Paul M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumbach_A/0/1/0/all/0/1&quot;&gt;Andreas Baumbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dold_D/0/1/0/all/0/1&quot;&gt;Dominik Dold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kugele_A/0/1/0/all/0/1&quot;&gt;Alexander Kugele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurtler_N/0/1/0/all/0/1&quot;&gt;Nico G&amp;#xfc;rtler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_E/0/1/0/all/0/1&quot;&gt;Eric M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koke_C/0/1/0/all/0/1&quot;&gt;Christoph Koke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleider_M/0/1/0/all/0/1&quot;&gt;Mitja Kleider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mauch_C/0/1/0/all/0/1&quot;&gt;Christian Mauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bretwieser_O/0/1/0/all/0/1&quot;&gt;Oliver Bretwieser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guttler_M/0/1/0/all/0/1&quot;&gt;Maurice G&amp;#xfc;ttler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Husmann_D/0/1/0/all/0/1&quot;&gt;Dan Husmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Husmann_K/0/1/0/all/0/1&quot;&gt;Kai Husmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilmberger_J/0/1/0/all/0/1&quot;&gt;Joscha Ilmberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartel_A/0/1/0/all/0/1&quot;&gt;Andreas Hartel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karasenko_V/0/1/0/all/0/1&quot;&gt;Vitali Karasenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grubl_A/0/1/0/all/0/1&quot;&gt;Andreas Gr&amp;#xfc;bl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schemmel_J/0/1/0/all/0/1&quot;&gt;Johannes Schemmel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_K/0/1/0/all/0/1&quot;&gt;Karlheinz Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrovici_M/0/1/0/all/0/1&quot;&gt;Mihai A. Petrovici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02397">
<title>Quality Diversity Through Surprise. (arXiv:1807.02397v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.02397</link>
<description rdf:parseType="Literal">&lt;p&gt;Quality diversity is a recent evolutionary computation paradigm which
maintains an appropriate balance between divergence and convergence and has
achieved promising results in complex problems. There is, however, limited
exploration on how different paradigms of divergent search may impact the
solutions found by quality diversity algorithms. Inspired by the notion of
\emph{surprise} as an effective driver of divergent search and its orthogonal
nature to novelty this paper investigates the impact of the former to quality
diversity performance. For that purpose we introduce three new quality
diversity algorithms which use surprise as a diversity measure, either on its
own or combined with novelty, and compare their performance against novelty
search with local competition, the state of the art quality diversity
algorithm. The algorithms are tested in a robot maze navigation task, in a
challenging set of 60 deceptive mazes. Our findings suggest that allowing
surprise and novelty to operate synergistically for divergence and in
combination with local competition leads to quality diversity algorithms of
significantly higher efficiency, speed and robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gravina_D/0/1/0/all/0/1&quot;&gt;Daniele Gravina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1&quot;&gt;Antonios Liapis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1&quot;&gt;Georgios N. Yannakakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02328">
<title>Generative Adversarial Perturbations. (arXiv:1712.02328v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1712.02328</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose novel generative models for creating adversarial
examples, slightly perturbed images resembling natural images but maliciously
crafted to fool pre-trained models. We present trainable deep neural networks
for transforming images to adversarial perturbations. Our proposed models can
produce image-agnostic and image-dependent perturbations for both targeted and
non-targeted attacks. We also demonstrate that similar architectures can
achieve impressive results in fooling classification and semantic segmentation
models, obviating the need for hand-crafting attack methods for each task.
Using extensive experiments on challenging high-resolution datasets such as
ImageNet and Cityscapes, we show that our perturbations achieve high fooling
rates with small perturbation norms. Moreover, our attacks are considerably
faster than current iterative methods at inference time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poursaeed_O/0/1/0/all/0/1&quot;&gt;Omid Poursaeed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsman_I/0/1/0/all/0/1&quot;&gt;Isay Katsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1&quot;&gt;Bicheng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1&quot;&gt;Serge Belongie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03744">
<title>Enhanced Optimization with Composite Objectives and Novelty Selection. (arXiv:1803.03744v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03744</link>
<description rdf:parseType="Literal">&lt;p&gt;An important benefit of multi-objective search is that it maintains a diverse
population of candidates, which helps in deceptive problems in particular. Not
all diversity is useful, however: candidates that optimize only one objective
while ignoring others are rarely helpful. This paper proposes a solution: The
original objectives are replaced by their linear combinations, thus focusing
the search on the most useful tradeoffs between objectives. To compensate for
the loss of diversity, this transformation is accompanied by a selection
mechanism that favors novelty. In the highly deceptive problem of discovering
minimal sorting networks, this approach finds better solutions, and finds them
faster and more consistently than standard methods. It is therefore a promising
approach to solving deceptive problems through multi-objective optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahrzad_H/0/1/0/all/0/1&quot;&gt;Hormoz Shahrzad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fink_D/0/1/0/all/0/1&quot;&gt;Daniel Fink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01097">
<title>A Many-Objective Evolutionary Algorithm with Two Interacting Processes: Cascade Clustering and Reference Point Incremental Learning. (arXiv:1803.01097v2 [cs.NE] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1803.01097</link>
<description rdf:parseType="Literal">&lt;p&gt;Researches have shown difficulties in obtaining proximity while maintaining
diversity for solving many-objective optimization problems (MaOPs). The
complexities of the true Pareto Front (PF) pose serious challenges for the
reference vector based algorithms for their insufficient adaptability to the
characteristics of the true PF with no priori. This paper proposes a
many-objective optimization Algorithm with two Interacting processes: cascade
Clustering and reference point incremental Learning (CLIA). In the population
selection process based on cascade clustering, using the reference vectors
provided by the incremental learning process, the non-dominated and the
dominated individuals are clustered and sorted with different manners in a
cascade style and are selected by round-robin for better proximity and
diversity. In the reference vector adaptation process based on reference point
incremental learning, using the feedbacks from the clustering process, the
proper distribution of reference points is gradually obtained by incremental
learning and the reference vectors are accordingly repositioned. The advantages
of CLIA lie not only in its effective and efficient performance, but also in
the versatility to deal with diverse characteristics of the true PF, only using
the interactions between the two processes without incurring extra evaluations.
The experimental studies on many benchmark problems show that CLIA is
competitive, efficient and versatile compared with the state-of-the-art
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_H/0/1/0/all/0/1&quot;&gt;Hongwei Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Mingde Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Liang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_G/0/1/0/all/0/1&quot;&gt;Guozhen Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;C.L. Philip Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02258">
<title>Scalable Formal Concept Analysis algorithm for large datasets using Spark. (arXiv:1807.02258v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.02258</link>
<description rdf:parseType="Literal">&lt;p&gt;In the process of knowledge discovery and representation in large datasets
using formal concept analysis, complexity plays a major role in identifying all
the formal concepts and constructing the concept lattice(digraph of the
concepts). For identifying the formal concepts and constructing the digraph
from the identified concepts in very large datasets, various distributed
algorithms are available in the literature. However, the existing distributed
algorithms are not very well suitable for concept generation because it is an
iterative process. The existing algorithms are implemented using distributed
frameworks like MapReduce and Open MP, these frameworks are not appropriate for
iterative applications. Hence, in this paper we proposed efficient distributed
algorithms for both formal concept generation and concept lattice digraph
construction in large formal contexts using Apache Spark. Various performance
metrics are considered for the evaluation of the proposed work, the results of
the evaluation proves that the proposed algorithms are efficient for concept
generation and lattice graph construction in comparison with the existing
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chunduri_R/0/1/0/all/0/1&quot;&gt;Raghavendra K Chunduri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherukuri_A/0/1/0/all/0/1&quot;&gt;Aswani Kumar Cherukuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02262">
<title>Temporal graph-based clustering for historical record linkage. (arXiv:1807.02262v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1807.02262</link>
<description rdf:parseType="Literal">&lt;p&gt;Research in the social sciences is increasingly based on large and complex
data collections, where individual data sets from different domains are linked
and integrated to allow advanced analytics. A popular type of data used in such
a context are historical censuses, as well as birth, death, and marriage
certificates. Individually, such data sets however limit the types of studies
that can be conducted. Specifically, it is impossible to track individuals,
families, or households over time. Once such data sets are linked and family
trees spanning several decades are available it is possible to, for example,
investigate how education, health, mobility, employment, and social status
influence each other and the lives of people over two or even more generations.
A major challenge is however the accurate linkage of historical data sets which
is due to data quality and commonly also the lack of ground truth data being
available. Unsupervised techniques need to be employed, which can be based on
similarity graphs generated by comparing individual records. In this paper we
present initial results from clustering birth records from Scotland where we
aim to identify all births of the same mother and group siblings into clusters.
We extend an existing clustering technique for record linkage by incorporating
temporal constraints that must hold between births by the same mother, and
propose a novel greedy temporal clustering technique. Experimental results show
improvements over non-temporary approaches, however further work is needed to
obtain links of high quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanayakkara_C/0/1/0/all/0/1&quot;&gt;Charini Nanayakkara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christen_P/0/1/0/all/0/1&quot;&gt;Peter Christen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranbaduge_T/0/1/0/all/0/1&quot;&gt;Thilina Ranbaduge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02297">
<title>Combinatorial Bandits for Incentivizing Agents with Dynamic Preferences. (arXiv:1807.02297v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02297</link>
<description rdf:parseType="Literal">&lt;p&gt;The design of personalized incentives or recommendations to improve user
engagement is gaining prominence as digital platform providers continually
emerge. We propose a multi-armed bandit framework for matching incentives to
users, whose preferences are unknown a priori and evolving dynamically in time,
in a resource constrained environment. We design an algorithm that combines
ideas from three distinct domains: (i) a greedy matching paradigm, (ii) the
upper confidence bound algorithm (UCB) for bandits, and (iii) mixing times from
the theory of Markov chains. For this algorithm, we provide theoretical bounds
on the regret and demonstrate its performance via both synthetic and realistic
(matching supply and demand in a bike-sharing platform) examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiez_T/0/1/0/all/0/1&quot;&gt;Tanner Fiez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekar_S/0/1/0/all/0/1&quot;&gt;Shreyas Sekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Liyuan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratliff_L/0/1/0/all/0/1&quot;&gt;Lillian J. Ratliff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02303">
<title>A survey on policy search algorithms for learning robot controllers in a handful of trials. (arXiv:1807.02303v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1807.02303</link>
<description rdf:parseType="Literal">&lt;p&gt;Most policy search algorithms require thousands of training episodes to find
an effective policy, which is often infeasible with a physical robot. This
survey article focuses on the extreme other end of the spectrum: how can a
robot adapt with only a handful of trials (a dozen) and a few minutes? By
analogy with the word &quot;big-data&quot;, we refer to this challenge as &quot;micro-data
reinforcement learning&quot;. We show that a first strategy is to leverage prior
knowledge on the policy structure (e.g., dynamic movement primitives), on the
policy parameters (e.g., demonstrations), or on the dynamics (e.g.,
simulators). A second strategy is to create data-driven surrogate models of the
expected reward (e.g., Bayesian optimization) or the dynamical model (e.g.,
model-based policy search), so that the policy optimizer queries the model
instead of the real system. Overall, all successful micro-data algorithms
combine these two strategies by varying the kind of model and prior knowledge.
The current scientific challenges essentially revolve around scaling up to
complex robots (e.g., humanoids), designing generic priors, and optimizing the
computing time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatzilygeroudis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Chatzilygeroudis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vassiliades_V/0/1/0/all/0/1&quot;&gt;Vassilis Vassiliades&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stulp_F/0/1/0/all/0/1&quot;&gt;Freek Stulp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calinon_S/0/1/0/all/0/1&quot;&gt;Sylvain Calinon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Mouret&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02322">
<title>Memory Augmented Policy Optimization for Program Synthesis with Generalization. (arXiv:1807.02322v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02322</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Memory Augmented Policy Optimization (MAPO): a novel
policy optimization formulation that incorporates a memory buffer of promising
trajectories to reduce the variance of policy gradient estimates for
deterministic environments with discrete actions. The formulation expresses the
expected return objective as a weighted sum of two terms: an expectation over a
memory of trajectories with high rewards, and a separate expectation over the
trajectories outside the memory. We propose 3 techniques to make an efficient
training algorithm for MAPO: (1) distributed sampling from inside and outside
memory with an actor-learner architecture; (2) a marginal likelihood constraint
over the memory to accelerate training; (3) systematic exploration to discover
high reward trajectories. MAPO improves the sample efficiency and robustness of
policy gradient, especially on tasks with a sparse reward. We evaluate MAPO on
weakly supervised program synthesis from natural language with an emphasis on
generalization. On the WikiTableQuestions benchmark we improve the
state-of-the-art by 2.5%, achieving an accuracy of 46.2%, and on the WikiSQL
benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision,
outperforming several strong baselines with full supervision. Our code is open
sourced at https://github.com/crazydonkey200/neural-symbolic-machines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1&quot;&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1&quot;&gt;Jonathan Berant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1&quot;&gt;Ni Lao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02406">
<title>Multi-atomic Annealing Heuristic for Static Dial-a-ride Problem. (arXiv:1807.02406v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.02406</link>
<description rdf:parseType="Literal">&lt;p&gt;Dial-a-ride problem (DARP) deals with the transportation of users between
pickup and drop-off locations associated with specified time windows. This
paper proposes a novel algorithm called multi-atomic annealing (MATA) to solve
static dial-a-ride problem. Two new local search operators (burn and reform), a
new construction heuristic and two request sequencing mechanisms (Sorted List
and Random List) are developed. Computational experiments conducted on various
standard DARP test instances prove that MATA is an expeditious meta-heuristic
in contrast to other existing methods. In all experiments, MATA demonstrates
the capability to obtain high quality solutions, faster convergence, and
quicker attainment of a first feasible solution. It is observed that MATA
attains a first feasible solution 29.8 to 65.1% faster, and obtains a final
solution that is 3.9 to 5.2% better, when compared to other algorithms within
60 sec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1&quot;&gt;Song Guang Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandi_R/0/1/0/all/0/1&quot;&gt;Ramesh Ramasamy Pandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagavarapu_S/0/1/0/all/0/1&quot;&gt;Sarat Chandra Nagavarapu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dauwels_J/0/1/0/all/0/1&quot;&gt;Justin Dauwels&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02416">
<title>A multidisciplinary task-based perspective for evaluating the impact of AI autonomy and generality on the future of work. (arXiv:1807.02416v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.02416</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a multidisciplinary task approach for assessing the
impact of artificial intelligence on the future of work. We provide definitions
of a task from two main perspectives: socio-economic and computational. We
propose to explore ways in which we can integrate or map these perspectives,
and link them with the skills or capabilities required by them, for humans and
AI systems. Finally, we argue that in order to understand the dynamics of
tasks, we have to explore the relevance of autonomy and generality of AI
systems for the automation or alteration of the workplace.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Macias_E/0/1/0/all/0/1&quot;&gt;Enrique Fern&amp;#xe1;ndez-Mac&amp;#xed;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_E/0/1/0/all/0/1&quot;&gt;Emilia G&amp;#xf3;mez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Orallo_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Hern&amp;#xe1;ndez-Orallo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loe_B/0/1/0/all/0/1&quot;&gt;Bao Sheng Loe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martens_B/0/1/0/all/0/1&quot;&gt;Bertin Martens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Plumed_F/0/1/0/all/0/1&quot;&gt;Fernando Mart&amp;#xed;nez-Plumed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolan_S/0/1/0/all/0/1&quot;&gt;Song&amp;#xfc;l Tolan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02544">
<title>Visual Explanations From Deep 3D Convolutional Neural Networks for Alzheimer&apos;s Disease Classification. (arXiv:1803.02544v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02544</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop three efficient approaches for generating visual explanations from
3D convolutional neural networks (3D-CNNs) for Alzheimer&apos;s disease
classification. One approach conducts sensitivity analysis on hierarchical 3D
image segmentation, and the other two visualize network activations on a
spatial map. Visual checks and a quantitative localization benchmark indicate
that all approaches identify important brain parts for Alzheimer&apos;s disease
diagnosis. Comparative analysis show that the sensitivity analysis based
approach has difficulty handling loosely distributed cerebral cortex, and
approaches based on visualization of activations are constrained by the
resolution of the convolutional layer. The complementarity of these methods
improves the understanding of 3D-CNNs in Alzheimer&apos;s disease classification
from different perspectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chengliang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1&quot;&gt;Anand Rangarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranka_S/0/1/0/all/0/1&quot;&gt;Sanjay Ranka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10723">
<title>Designing for Democratization: Introducing Novices to Artificial Intelligence Via Maker Kits. (arXiv:1805.10723v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/1805.10723</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing research highlight the myriad of benefits realized when technology
is sufficiently democratized and made accessible to non-technical or novice
users. However, democratizing complex technologies such as artificial
intelligence (AI) remains hard. In this work, we draw on theoretical
underpinnings from the democratization of innovation, in exploring the design
of maker kits that help introduce novice users to complex technologies. We
report on our work designing TJBot: an open source cardboard robot that can be
programmed using pre-built AI services. We highlight principles we adopted in
this process (approachable design, simplicity, extensibility and
accessibility), insights we learned from showing the kit at workshops (66
participants) and how users interacted with the project on GitHub over a
12-month period (Nov 2016 - Nov 2017). We find that the project succeeds in
attracting novice users (40% of users who forked the project are new to GitHub)
and a variety of demographics are interested in prototyping use cases such as
home automation, task delegation, teaching and learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dibia_V/0/1/0/all/0/1&quot;&gt;Victor Dibia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashoori_M/0/1/0/all/0/1&quot;&gt;Maryam Ashoori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cox_A/0/1/0/all/0/1&quot;&gt;Aaron Cox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weisz_J/0/1/0/all/0/1&quot;&gt;Justin Weisz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09597">
<title>Stochastic natural gradient descent draws posterior samples in function space. (arXiv:1806.09597v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.09597</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural gradient descent (NGD) minimises the cost function on a Riemannian
manifold whose metric is defined by the Fisher information. In this work, we
prove that if the model predictions on the training set approach the true
conditional distribution of labels given inputs, then the noise inherent in
minibatch gradients causes the stationary distribution of NGD to approach a
Bayesian posterior, whose temperature $T \approx \epsilon N/(2B)$ is controlled
by the learning rate $\epsilon$, training set size $N$ and batch size $B$. The
parameter-dependence of the Fisher metric introduces an implicit prior over the
parameters, which we identify as the well-known Jeffreys prior. To support our
claims, we show that the distribution of samples from NGD is close to the
Laplace approximation to the posterior when $T = 1$. Furthermore, the test loss
of ensembles drawn using NGD falls rapidly as we increase the batch size until
$B \approx \epsilon N/2$, while above this point the test loss is constant or
rises slowly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1&quot;&gt;Samuel L. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duckworth_D/0/1/0/all/0/1&quot;&gt;Daniel Duckworth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc V. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1&quot;&gt;Jascha Sohl-Dickstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00553">
<title>A Broader View on Bias in Automated Decision-Making: Reflecting on Epistemology and Dynamics. (arXiv:1807.00553v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00553</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) is increasingly deployed in real world contexts,
supplying actionable insights and forming the basis of automated
decision-making systems. While issues resulting from biases pre-existing in
training data have been at the center of the fairness debate, these systems are
also affected by technical and emergent biases, which often arise as
context-specific artifacts of implementation. This position paper interprets
technical bias as an epistemological problem and emergent bias as a dynamical
feedback phenomenon. In order to stimulate debate on how to change machine
learning practice to effectively address these issues, we explore this broader
view on bias, stress the need to reflect on epistemology, and point to
value-sensitive design methodologies to revisit the design and implementation
process of automated decision-making systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobbe_R/0/1/0/all/0/1&quot;&gt;Roel Dobbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dean_S/0/1/0/all/0/1&quot;&gt;Sarah Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_T/0/1/0/all/0/1&quot;&gt;Thomas Gilbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohli_N/0/1/0/all/0/1&quot;&gt;Nitin Kohli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02125">
<title>Scalable Gaussian Processes with Grid-Structured Eigenfunctions (GP-GRIEF). (arXiv:1807.02125v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.02125</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a kernel approximation strategy that enables computation of the
Gaussian process log marginal likelihood and all hyperparameter derivatives in
$\mathcal{O}(p)$ time. Our GRIEF kernel consists of $p$ eigenfunctions found
using a Nystr\&quot;om approximation from a dense Cartesian product grid of inducing
points. By exploiting algebraic properties of Kronecker and Khatri-Rao tensor
products, computational complexity of the training procedure can be practically
independent of the number of inducing points. This allows us to use arbitrarily
many inducing points to achieve a globally accurate kernel approximation, even
in high-dimensional problems. The fast likelihood evaluation enables type-I or
II Bayesian inference on large-scale datasets. We benchmark our algorithms on
real-world problems with up to two-million training points and $10^{33}$
inducing points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Evans_T/0/1/0/all/0/1&quot;&gt;Trefor W. Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nair_P/0/1/0/all/0/1&quot;&gt;Prasanth B. Nair&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02128">
<title>Adaptive Path-Integral Approach to Representation Learning and Planning for Dynamical Systems. (arXiv:1807.02128v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02128</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a representation learning algorithm that learns a low-dimensional
latent dynamical system from high-dimensional sequential raw data, e.g., video.
The framework builds upon recent advances in amortized inference methods that
use both an inference network and a refinement procedure to output samples from
a variational distribution given an observation sequence, and takes advantage
of the duality between control and inference to approximately solve the
intractable inference problem using the path integral control approach. The
learned dynamical model can be used to predict and plan the future states; we
also present the efficient planning method that exploits the learned
low-dimensional latent dynamics. Numerical experiments show that the proposed
path-integral control based variational inference method leads to tighter lower
bounds in statistical model learning of sequential data. The supplementary
video can be found at https://youtu.be/4jDcbuAJ7mA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1&quot;&gt;Jung-Su Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Young-Jin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chae_H/0/1/0/all/0/1&quot;&gt;Hyeok-Joo Chae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Soon-Seo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Han-Lim Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02150">
<title>Scalable Recommender Systems through Recursive Evidence Chains. (arXiv:1807.02150v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.02150</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems can be formulated as a matrix completion problem,
predicting ratings from user and item parameter vectors. Optimizing these
parameters by subsampling data becomes difficult as the number of users and
items grows. We develop a novel approach to generate all latent variables on
demand from the ratings matrix itself and a fixed pool of parameters. We
estimate missing ratings using chains of evidence that link them to a small set
of prototypical users and items. Our model automatically addresses the
cold-start and online learning problems by combining information across both
users and items. We investigate the scaling behavior of this model, and
demonstrate competitive results with respect to current matrix factorization
techniques in terms of accuracy and convergence speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tragas_E/0/1/0/all/0/1&quot;&gt;Elias Tragas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Calvin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gazeau_M/0/1/0/all/0/1&quot;&gt;Maxime Gazeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luk_K/0/1/0/all/0/1&quot;&gt;Kevin Luk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1&quot;&gt;David Duvenaud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02188">
<title>Explainable Learning: Implicit Generative Modelling during Training for Adversarial Robustness. (arXiv:1807.02188v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02188</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Explainable Learning ,ExL, an approach for training neural
networks that are intrinsically robust to adversarial attacks. We find that the
implicit generative modelling of random noise, during posterior maximization,
improves a model&apos;s understanding of the data manifold furthering adversarial
robustness. We prove our approach&apos;s efficacy and provide a simplistic
visualization tool for understanding adversarial data, using Principal
Component Analysis. Our analysis reveals that adversarial robustness, in
general, manifests in models with higher variance along the high-ranked
principal components. We show that models learnt with ExL perform remarkably
well against a wide-range of black-box attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1&quot;&gt;Priyadarshini Panda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushik Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02233">
<title>U-SLADS: Unsupervised Learning Approach for Dynamic Dendrite Sampling. (arXiv:1807.02233v1 [eess.IV])</title>
<link>http://arxiv.org/abs/1807.02233</link>
<description rdf:parseType="Literal">&lt;p&gt;Novel data acquisition schemes have been an emerging need for scanning
microscopy based imaging techniques to reduce the time in data acquisition and
to minimize probing radiation in sample exposure. Varies sparse sampling
schemes have been studied and are ideally suited for such applications where
the images can be reconstructed from a sparse set of measurements. Dynamic
sparse sampling methods, particularly supervised learning based iterative
sampling algorithms, have shown promising results for sampling pixel locations
on the edges or boundaries during imaging. However, dynamic sampling for
imaging skeleton-like objects such as metal dendrites remains difficult. Here,
we address a new unsupervised learning approach using Hierarchical Gaussian
Mixture Mod- els (HGMM) to dynamically sample metal dendrites. This technique
is very useful if the users are interested in fast imaging the primary and
secondary arms of metal dendrites in solidification process in materials
science.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ferrier_N/0/1/0/all/0/1&quot;&gt;Nicola Ferrier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gulsoy_E/0/1/0/all/0/1&quot;&gt;Emine B. Gulsoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Phatak_C/0/1/0/all/0/1&quot;&gt;Charudatta Phatak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02264">
<title>Variance Reduction for Reinforcement Learning in Input-Driven Environments. (arXiv:1807.02264v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02264</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider reinforcement learning in input-driven environments, where an
exogenous, stochastic input process affects the dynamics of the system. Input
processes arise in many applications, including queuing systems, robotics
control with disturbances, and object tracking. Since the state dynamics and
rewards depend on the input process, the state alone provides limited
information for the expected future returns. Therefore, policy gradient methods
with standard state-dependent baselines suffer high variance during training.
We derive a bias-free, input-dependent baseline to reduce this variance, and
analytically show its benefits over state-dependent baselines. We then propose
a meta-learning approach to overcome the complexity of learning a baseline that
depends on a long sequence of inputs. Our experimental results show that across
environments from queuing systems, computer networks, and MuJoCo robotic
locomotion, input-dependent baselines consistently improve training stability
and result in better eventual policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Hongzi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatakrishnan_S/0/1/0/all/0/1&quot;&gt;Shaileshh Bojja Venkatakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarzkopf_M/0/1/0/all/0/1&quot;&gt;Malte Schwarzkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alizadeh_M/0/1/0/all/0/1&quot;&gt;Mohammad Alizadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02287">
<title>Outperforming Good-Turing: Preliminary Report. (arXiv:1807.02287v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.02287</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating a large alphabet probability distribution from a limited number of
samples is a fundamental problem in machine learning and statistics. A variety
of estimation schemes have been proposed over the years, mostly inspired by the
early work of Laplace and the seminal contribution of Good and Turing. One of
the basic assumptions shared by most commonly-used estimators is the unique
correspondence between the symbol&apos;s sample frequency and its estimated
probability. In this work we tackle this paradigmatic assumption; we claim that
symbols with &quot;similar&quot; frequencies shall be assigned the same estimated
probability value. This way we regulate the number of parameters and improve
generalization. In this preliminary report we show that by applying an ensemble
of such regulated estimators, we introduce a dramatic enhancement in the
estimation accuracy (typically up to 50%), compared to currently known methods.
An implementation of our suggested method is publicly available at the first
author&apos;s web-page.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Painsky_A/0/1/0/all/0/1&quot;&gt;Amichai Painsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feder_M/0/1/0/all/0/1&quot;&gt;Meir Feder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02290">
<title>Differentially Private Online Submodular Optimization. (arXiv:1807.02290v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1807.02290</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we develop the first algorithms for online submodular
minimization that preserve differential privacy under full information feedback
and bandit feedback. A sequence of $T$ submodular functions over a collection
of $n$ elements arrive online, and at each timestep the algorithm must choose a
subset of $[n]$ before seeing the function. The algorithm incurs a cost equal
to the function evaluated on the chosen set, and seeks to choose a sequence of
sets that achieves low expected regret.
&lt;/p&gt;
&lt;p&gt;Our first result is in the full information setting, where the algorithm can
observe the entire function after making its decision at each timestep. We give
an algorithm in this setting that is $\epsilon$-differentially private and
achieves expected regret
$\tilde{O}\left(\frac{n^{3/2}\sqrt{T}}{\epsilon}\right)$. This algorithm works
by relaxing submodular function to a convex function using the Lovasz
extension, and then simulating an algorithm for differentially private online
convex optimization.
&lt;/p&gt;
&lt;p&gt;Our second result is in the bandit setting, where the algorithm can only see
the cost incurred by its chosen set, and does not have access to the entire
function. This setting is significantly more challenging because the algorithm
does not receive enough information to compute the Lovasz extension or its
subgradients. Instead, we construct an unbiased estimate using a single-point
estimation, and then simulate private online convex optimization using this
estimate. Our algorithm using bandit feedback is $\epsilon$-differentially
private and achieves expected regret
$\tilde{O}\left(\frac{n^{3/2}T^{3/4}}{\epsilon}\right)$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardoso_A/0/1/0/all/0/1&quot;&gt;Adrian Rivera Cardoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cummings_R/0/1/0/all/0/1&quot;&gt;Rachel Cummings&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02326">
<title>Causal Deep Information Bottleneck. (arXiv:1807.02326v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.02326</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating causal effects in the presence of latent confounding is a
frequently occurring problem in several tasks. In real world applications such
as medicine, accounting for the effects of latent confounding is even more
challenging as a result of high-dimensional and noisy data. In this work, we
propose estimating the causal effect from the perspective of the information
bottleneck principle by explicitly identifying a low-dimensional representation
of latent confounding. In doing so, we prove theoretically that the proposed
model can be used to recover the average causal effect. Experiments on both
synthetic data and existing causal benchmarks illustrate that our method
achieves state-of-the-art performance in terms of prediction accuracy and
sample efficiency, without sacrificing interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parbhoo_S/0/1/0/all/0/1&quot;&gt;Sonali Parbhoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wieser_M/0/1/0/all/0/1&quot;&gt;Mario Wieser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roth_V/0/1/0/all/0/1&quot;&gt;Volker Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02350">
<title>A Variational Time Series Feature Extractor for Action Prediction. (arXiv:1807.02350v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.02350</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a Variational Time Series Feature Extractor (VTSFE), inspired by
the VAE-DMP model of Chen et al., to be used for action recognition and
prediction. Our method is based on variational autoencoders. It improves
VAE-DMP in that it has a better noise inference model, a simpler transition
model constraining the acceleration in the trajectories of the latent space,
and a tighter lower bound for the variational inference. We apply the method
for classification and prediction of whole-body movements on a dataset with 7
tasks and 10 demonstrations per task, recorded with a wearable motion capture
suit. The comparison with VAE and VAE-DMP suggests the better performance of
our method for feature extraction. An open-source software implementation of
each method with TensorFlow is also provided. In addition, a more detailed
version of this work can be found in the indicated code repository. Although it
was meant to, the VTSFE hasn&apos;t been tested for action prediction, due to a lack
of time in the context of Maxime Chaveroche&apos;s Master thesis at INRIA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaveroche_M/0/1/0/all/0/1&quot;&gt;Maxime Chaveroche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malaise_A/0/1/0/all/0/1&quot;&gt;Adrien Malais&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colas_F/0/1/0/all/0/1&quot;&gt;Francis Colas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charpillet_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Charpillet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivaldi_S/0/1/0/all/0/1&quot;&gt;Serena Ivaldi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04020">
<title>Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning. (arXiv:1802.04020v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04020</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce SCAL, an algorithm designed to perform efficient
exploration-exploitation in any unknown weakly-communicating Markov decision
process (MDP) for which an upper bound $c$ on the span of the optimal bias
function is known. For an MDP with $S$ states, $A$ actions and $\Gamma \leq S$
possible next states, we prove a regret bound of $\widetilde{O}(c\sqrt{\Gamma
SAT})$, which significantly improves over existing algorithms (e.g., UCRL and
PSRL), whose regret scales linearly with the MDP diameter $D$. In fact, the
optimal bias span is finite and often much smaller than $D$ (e.g., $D=\infty$
in non-communicating MDPs). A similar result was originally derived by Bartlett
and Tewari (2009) for REGAL.C, for which no tractable algorithm is available.
In this paper, we relax the optimization problem at the core of REGAL.C, we
carefully analyze its properties, and we provide the first computationally
efficient algorithm to solve it. Finally, we report numerical simulations
supporting our theoretical findings and showing how SCAL significantly
outperforms UCRL in MDPs with large diameter and small span.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fruit_R/0/1/0/all/0/1&quot;&gt;Ronan Fruit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1&quot;&gt;Matteo Pirotta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1&quot;&gt;Alessandro Lazaric&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortner_R/0/1/0/all/0/1&quot;&gt;Ronald Ortner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10254">
<title>Disease-Atlas: Navigating Disease Trajectories with Deep Learning. (arXiv:1803.10254v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10254</link>
<description rdf:parseType="Literal">&lt;p&gt;Joint models for longitudinal and time-to-event data are commonly used in
longitudinal studies to forecast disease trajectories over time. While there
are many advantages to joint modeling, the standard forms suffer from
limitations that arise from a fixed model specification, and computational
difficulties when applied to high-dimensional datasets. In this paper, we
propose a deep learning approach to address these limitations, enhancing
existing methods with the inherent flexibility and scalability of deep neural
networks, while retaining the benefits of joint modeling. Using longitudinal
data from a real-world medical dataset, we demonstrate improvements in
performance and scalability, as well as robustness in the presence of
irregularly sampled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lim_B/0/1/0/all/0/1&quot;&gt;Bryan Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01013">
<title>Resilient Non-Submodular Maximization over Matroid Constraints. (arXiv:1804.01013v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01013</link>
<description rdf:parseType="Literal">&lt;p&gt;The control and sensing of large-scale systems results in combinatorial
problems not only for sensor and actuator placement but also for scheduling or
observability/controllability. Such combinatorial constraints in system design
and implementation can be captured using a structure known as matroids. In
particular, the algebraic structure of matroids can be exploited to develop
scalable algorithms for sensor and actuator selection, along with quantifiable
approximation bounds. However, in large-scale systems, sensors and actuators
may fail or may be (cyber-)attacked. The objective of this paper is to focus on
resilient matroid-constrained problems arising in control and sensing but in
the presence of sensor and actuator failures. In general, resilient
matroid-constrained problems are computationally hard. Contrary to the
non-resilient case (with no failures), even though they often involve objective
functions that are monotone or submodular, no scalable approximation algorithms
are known for their solution. In this paper, we provide the first algorithm,
that also has the following properties: First, it achieves system-wide
resiliency, i.e., the algorithm is valid for any number of denial-of-service
attacks or failures. Second, it is scalable, as our algorithm terminates with
the same running time as state-of-the-art algorithms for (non-resilient)
matroid-constrained optimization. Third, it provides provable approximation
bounds on the system performance, since for monotone objective functions our
algorithm guarantees a solution close to the optimal. We quantify our
algorithm&apos;s approximation performance using a notion of curvature for monotone
(not necessarily submodular) set functions. Finally, we support our theoretical
analyses with numerical experiments, by considering a control-aware sensor
selection scenario, namely, sensing-constrained robot navigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tzoumas_V/0/1/0/all/0/1&quot;&gt;Vasileios Tzoumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jadbabaie_A/0/1/0/all/0/1&quot;&gt;Ali Jadbabaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pappas_G/0/1/0/all/0/1&quot;&gt;George J. Pappas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06124">
<title>Supervised Fuzzy Partitioning. (arXiv:1806.06124v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.06124</link>
<description rdf:parseType="Literal">&lt;p&gt;Centroid-based methods including k-means and fuzzy c-means are known as
effective and easy-to-implement approaches to clustering purposes in many areas
of application. However, these algorithms cannot be directly applied to
supervised tasks. We propose a generative model extending centroid-based
clustering approaches to be applicable to classification tasks. Given an
arbitrary loss function, our approach, termed Supervised Fuzzy Partitioning
(SFP), incorporates labels information into its objective function through a
surrogate term penalizing the risk. We also fuzzify the partition and assign
weights to features alongside entropy-based regularization terms, enabling the
method to capture more complex data structure, to identify significant
features, and to yield better performance facing high-dimensional data. An
iterative algorithm based on block coordinate descent scheme was formulated to
efficiently find a local optimizer. The results show that the SFP performance
in classification of ultra high-dimensional gene expression data is competitive
with state-of-the-art algorithms such as random forest and SVM. Our method has
a major advantage over such methods in that it not only leads to a flexible
model suitable for high-dimensional cases but also uses the loss function in
training phase without compromising computational efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashtari_P/0/1/0/all/0/1&quot;&gt;Pooya Ashtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haredasht_F/0/1/0/all/0/1&quot;&gt;Fateme Nateghi Haredasht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01085">
<title>One-Class Kernel Spectral Regression for Outlier Detection. (arXiv:1807.01085v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01085</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper introduces a new efficient nonlinear one-class classifier
formulated as the Rayleigh quotient criterion. The method, operating in a
reproducing kernel Hilbert subspace, minimises the scatter of target
distribution along an optimal projection direction while at the same time
keeping projections of target observations as distant as possible from the
origin which serves as an artificial outlier with respect to the data. We
provide a graph embedding view of the problem which can then be solved
efficiently using the spectral regression approach. In this sense, unlike
previous similar methods which often require costly eigen-computations of dense
matrices, the proposed approach casts the problem under consideration into a
regression framework which avoids eigen-decomposition computations. In
particular, it is shown that the dominant complexity of the proposed method is
the complexity of computing the kernel matrix. Additional appealing
characteristics of the proposed one-class classifier are: 1-the ability to be
trained in an incremental fashion (allowing for application in streaming data
scenarios while also reducing computational complexity in the non-streaming
operation mode); 2-being unsupervised while also providing the ability for the
user to specify the expected fraction of outliers in the training set in
advance; And last but not least 3-the deployment of the kernel trick allowing
for a large class of functions by nonlinearly mapping the data into a
high-dimensional feature space. Extensive experiments conducted on several
datasets verifies the merits of the proposed approach in comparison with some
other alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kittler_S/0/1/0/all/0/1&quot;&gt;Shervin Rahimzadeh Arashloo an Josef Kittler&lt;/a&gt;</dc:creator>
</item></rdf:RDF>