<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-09-04T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00095"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00927"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02397"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.00076"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00088"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00204"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00258"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00306"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00345"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00410"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00414"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00494"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00564"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00567"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00647"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00716"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00949"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00969"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02808"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.11192"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06881"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07830"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00082"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00238"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00366"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00381"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00510"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00542"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00593"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00653"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00710"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00811"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00947"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00961"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.00999"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01000"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01015"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01022"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.01046"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1511.05174"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1604.08697"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.08306"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.02655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.01305"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04673"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.03851"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06355"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00114"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09730"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.11451"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00341"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05810"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07351"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08438"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07777"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00534"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.10728"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04427"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.05800"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.06093"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07706"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09586"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03230"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06640"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.09964"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10101"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1809.00095">
<title>Learning Low Precision Deep Neural Networks through Regularization. (arXiv:1809.00095v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.00095</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the quantization of deep neural networks (DNNs) to produce
low-precision models for efficient inference of fixed-point operations.
Compared to previous approaches to training quantized DNNs directly under the
constraints of low-precision weights and activations, we learn the quantization
of DNNs with minimal quantization loss through regularization. In particular,
we introduce the learnable regularization coefficient to find accurate
low-precision models efficiently in training. In our experiments, the proposed
scheme yields the state-of-the-art low-precision models of AlexNet and
ResNet-18, which have better accuracy than their previously available
low-precision models. We also examine our quantization method to produce
low-precision DNNs for image super resolution. We observe only $0.5$~dB peak
signal-to-noise ratio (PSNR) loss when using binary weights and 8-bit
activations. The proposed scheme can be used to train low-precision models from
scratch or to fine-tune a well-trained high-precision model to converge to a
low-precision model. Finally, we discuss how a similar regularization method
can be adopted in DNN weight pruning and compression, and show that $401\times$
compression is achieved for LeNet-5.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yoojin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Khamy_M/0/1/0/all/0/1&quot;&gt;Mostafa El-Khamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungwon Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00927">
<title>A Neural Network Model for Determining the Success or Failure of High-tech Projects Development: A Case of Pharmaceutical industry. (arXiv:1809.00927v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1809.00927</link>
<description rdf:parseType="Literal">&lt;p&gt;Financing high-tech projects always entails a great deal of risk. The lack of
a systematic method to pinpoint the risk of such projects has been recognized
as one of the most salient barriers for evaluating them. So, in order to
develop a mechanism for evaluating high-tech projects, an Artificial Neural
Network (ANN) has been developed through this study. The structure of this
paper encompasses four parts. The first part deals with introducing paper&apos;s
whole body. The second part gives a literature review. The collection process
of risk related variables and the process of developing a Risk Assessment Index
system (RAIS) through Principal Component Analysis (PCA) are those issues that
are discussed in the third part. The fourth part particularly deals with
pharmaceutical industry. Finally, the fifth part has focused on developing an
ANN for pattern recognition of failure or success of high-tech projects.
Analysis of model&apos;s results and a final conclusion are also presented in this
part.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabzian_H/0/1/0/all/0/1&quot;&gt;Hossein Sabzian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamrani_E/0/1/0/all/0/1&quot;&gt;Ehsan Kamrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashemi_S/0/1/0/all/0/1&quot;&gt;Seyyed Mostafa Seyyed Hashemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02397">
<title>Quality Diversity Through Surprise. (arXiv:1807.02397v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1807.02397</link>
<description rdf:parseType="Literal">&lt;p&gt;Quality diversity is a recent evolutionary computation paradigm which
maintains an appropriate balance between divergence and convergence and has
achieved promising results in complex problems. There is, however, limited
exploration on how different paradigms of divergent search may impact the
solutions found by quality diversity algorithms. Inspired by the notion of
surprise as an effective driver of divergent search and its orthogonal nature
to novelty this paper investigates the impact of the former to quality
diversity performance. For that purpose we introduce three new quality
diversity algorithms which use surprise as a diversity measure, either on its
own or combined with novelty, and compare their performance against novelty
search with local competition, the state of the art quality diversity
algorithm. The algorithms are tested in a robot maze navigation task, in a
challenging set of 60 deceptive mazes. Our findings suggest that allowing
surprise and novelty to operate synergistically for divergence and in
combination with local competition leads to quality diversity algorithms of
significantly higher efficiency, speed and robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gravina_D/0/1/0/all/0/1&quot;&gt;Daniele Gravina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1&quot;&gt;Antonios Liapis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1&quot;&gt;Georgios N. Yannakakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.00076">
<title>News Session-Based Recommendations using Deep Neural Networks. (arXiv:1808.00076v1 [cs.IR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1808.00076</link>
<description rdf:parseType="Literal">&lt;p&gt;News recommender systems are aimed to personalize users experiences and help
them to discover relevant articles from a large and dynamic search space.
Therefore, news domain is a challenging scenario for recommendations, due to
its sparse user profiling, fast growing number of items, accelerated item&apos;s
value decay, and users preferences dynamic shift. Some promising results have
been recently achieved by the usage of Deep Learning techniques on Recommender
Systems, specially for item&apos;s feature extraction and for session-based
recommendations with Recurrent Neural Networks. In this paper, its presented a
Deep Learning architecture for Session-Based recommendations of News articles.
This architecture is composed of two modules, the first responsible to learn
news articles representations, based on their text and metadata, and the second
module aimed to provide session-based recommendations using Recurrent Neural
Networks. The recommendation task addressed in this work is next-item
prediction for user sessions: &quot;what is the next most likely article a user
might read in a session?&quot; User session context is leveraged by the architecture
to provide additional information in such extreme cold-start scenario of news
recommendation. Users&apos; behavior and item features are both merged in an hybrid
recommendation approach. A temporal offline evaluation method is also proposed
as a complementary contribution, for a more realistic evaluation of such task,
considering dynamic factors that affect global readership interests like
popularity, recency, and seasonality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1&quot;&gt;Gabriel de Souza P. Moreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1&quot;&gt;Felipe Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunha_A/0/1/0/all/0/1&quot;&gt;Adilson Marques da Cunha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00088">
<title>Hierarchical CVAE for Fine-Grained Hate Speech Classification. (arXiv:1809.00088v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.00088</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing work on automated hate speech detection typically focuses on binary
classification or on differentiating among a small set of categories. In this
paper, we propose a novel method on a fine-grained hate speech classification
task, which focuses on differentiating among 40 hate groups of 13 different
hate group categories. We first explore the Conditional Variational Autoencoder
(CVAE) as a discriminative model and then extend it to a hierarchical
architecture to utilize the additional hate category information for more
accurate prediction. Experimentally, we show that incorporating the hate
category information for training can significantly improve the classification
performance and our proposed model outperforms commonly-used discriminative
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1&quot;&gt;Jing Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ElSherief_M/0/1/0/all/0/1&quot;&gt;Mai ElSherief&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belding_E/0/1/0/all/0/1&quot;&gt;Elizabeth Belding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00204">
<title>Improving Visual Relationship Detection using Semantic Modeling of Scene Descriptions. (arXiv:1809.00204v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.00204</link>
<description rdf:parseType="Literal">&lt;p&gt;Structured scene descriptions of images are useful for the automatic
processing and querying of large image databases. We show how the combination
of a semantic and a visual statistical model can improve on the task of mapping
images to their associated scene description. In this paper we consider scene
descriptions which are represented as a set of triples (subject, predicate,
object), where each triple consists of a pair of visual objects, which appear
in the image, and the relationship between them (e.g. man-riding-elephant,
man-wearing-hat). We combine a standard visual model for object detection,
based on convolutional neural networks, with a latent variable model for link
prediction. We apply multiple state-of-the-art link prediction methods and
compare their capability for visual relationship detection. One of the main
advantages of link prediction methods is that they can also generalize to
triples, which have never been observed in the training data. Our experimental
results on the recently published Stanford Visual Relationship dataset, a
challenging real world dataset, show that the integration of a semantic model
using link prediction methods can significantly improve the results for visual
relationship detection. Our combined approach achieves superior performance
compared to the state-of-the-art method from the Stanford computer vision
group.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baier_S/0/1/0/all/0/1&quot;&gt;Stephan Baier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunpu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00258">
<title>A Contextual-bandit-based Approach for Informed Decision-making in Clinical Trials. (arXiv:1809.00258v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.00258</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical trials involving multiple treatments utilize randomization of the
treatment assignments to enable the evaluation of treatment efficacies in an
unbiased manner. Such evaluation is performed in post hoc studies that usually
use supervised-learning methods that rely on large amounts of data collected in
a randomized fashion. That approach often proves to be suboptimal in that some
participants may suffer and even die as a result of having not received the
most appropriate treatments during the trial. Reinforcement-learning methods
improve the situation by making it possible to learn the treatment efficacies
dynamically during the course of the trial, and to adapt treatment assignments
accordingly. Recent efforts using \textit{multi-arm bandits}, a type of
reinforcement-learning methods, have focused on maximizing clinical outcomes
for a population that was assumed to be homogeneous. However, those approaches
have failed to account for the variability among participants that is becoming
increasingly evident as a result of recent clinical-trial-based studies. We
present a contextual-bandit-based online treatment optimization algorithm that,
in choosing treatments for new participants in the study, takes into account
not only the maximization of the clinical outcomes but also the patient
characteristics. We evaluated our algorithm using a real clinical trial dataset
from the International Stroke Trial. The results of our retrospective analysis
indicate that the proposed approach performs significantly better than either a
random assignment of treatments (the current gold standard) or a
multi-arm-bandit-based approach, providing substantial gains in the percentage
of participants who are assigned the most suitable treatments. The
contextual-bandit and multi-arm bandit approaches provide 72.63% and 64.34%
gains, respectively, compared to a random assignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varatharajah_Y/0/1/0/all/0/1&quot;&gt;Yogatheesan Varatharajah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berry_B/0/1/0/all/0/1&quot;&gt;Brent Berry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1&quot;&gt;Sanmi Koyejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1&quot;&gt;Ravishankar Iyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00306">
<title>Enhancing Stock Market Prediction with Extended Coupled Hidden Markov Model over Multi-Sourced Data. (arXiv:1809.00306v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.00306</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional stock market prediction methods commonly only utilize the
historical trading data, ignoring the fact that stock market fluctuations can
be impacted by various other information sources such as stock related events.
Although some recent works propose event-driven prediction approaches by
considering the event data, how to leverage the joint impacts of multiple data
sources still remains an open research problem. In this work, we study how to
explore multiple data sources to improve the performance of the stock
prediction. We introduce an Extended Coupled Hidden Markov Model incorporating
the news events with the historical trading data. To address the data sparsity
issue of news events for each single stock, we further study the fluctuation
correlations between the stocks and incorporate the correlations into the model
to facilitate the prediction task. Evaluations on China A-share market data in
2016 show the superior performance of our model against previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yixuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Senzhang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1&quot;&gt;Binxing Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00345">
<title>IntentsKB: A Knowledge Base of Entity-Oriented Search Intents. (arXiv:1809.00345v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1809.00345</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of constructing a knowledge base of entity-oriented
search intents. Search intents are defined on the level of entity types, each
comprising of a high-level intent category (property, website, service, or
other), along with a cluster of query terms used to express that intent. These
machine-readable statements can be leveraged in various applications, e.g., for
generating entity cards or query recommendations. By structuring
service-oriented search intents, we take one step towards making entities
actionable. The main contribution of this paper is a pipeline of components we
develop to construct a knowledge base of entity intents. We evaluate
performance both component-wise and end-to-end, and demonstrate that our
approach is able to generate high-quality data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garigliotti_D/0/1/0/all/0/1&quot;&gt;Dar&amp;#xed;o Garigliotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balog_K/0/1/0/all/0/1&quot;&gt;Krisztian Balog&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00410">
<title>Modeling Topical Coherence in Discourse without Supervision. (arXiv:1809.00410v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.00410</link>
<description rdf:parseType="Literal">&lt;p&gt;Coherence of text is an important attribute to be measured for both manually
and automatically generated discourse; but well-defined quantitative metrics
for it are still elusive. In this paper, we present a metric for scoring
topical coherence of an input paragraph on a real-valued scale by analyzing its
underlying topical structure. We first extract all possible topics that the
sentences of a paragraph of text are related to. Coherence of this text is then
measured by computing: (a) the degree of uncertainty of the topics with respect
to the paragraph, and (b) the relatedness between these topics. All components
of our modular framework rely only on unlabeled data and WordNet, thus making
it completely unsupervised, which is an important feature for general-purpose
usage of any metric. Experiments are conducted on two datasets - a publicly
available dataset for essay grading (representing human discourse), and a
synthetic dataset constructed by mixing content from multiple paragraphs
covering diverse topics. Our evaluation shows that the measured coherence
scores are positively correlated with the ground truth for both the datasets.
Further validation to our coherence scores is provided by conducting human
evaluation on the synthetic data, showing a significant agreement of 79.3%
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_D/0/1/0/all/0/1&quot;&gt;Disha Shrivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Abhijit Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankaranarayanan_K/0/1/0/all/0/1&quot;&gt;Karthik Sankaranarayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00414">
<title>Hypernyms Through Intra-Article Organization in Wikipedia. (arXiv:1809.00414v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1809.00414</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new measure for unsupervised hypernym detection and
directionality. The motivation is to keep the measure computationally light and
portatable across languages. We show that the relative physical location of
words in explanatory articles captures the directionality property. Further,
the phrases in section titles of articles about the word, capture the semantic
similarity needed for hypernym detection task. We experimentally show that the
combination of features coming from these two simple measures suffices to
produce results comparable with the best unsupervised measures in terms of the
average precision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_D/0/1/0/all/0/1&quot;&gt;Disha Shrivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kenkre_S/0/1/0/all/0/1&quot;&gt;Sreyash Kenkre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Penubothula_S/0/1/0/all/0/1&quot;&gt;Santosh Penubothula&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00494">
<title>Belittling the Source: Trustworthiness Indicators to Obfuscate Fake News on the Web. (arXiv:1809.00494v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1809.00494</link>
<description rdf:parseType="Literal">&lt;p&gt;With the growth of the internet, the number of fake-news online has been
proliferating every year. The consequences of such phenomena are manifold,
ranging from lousy decision-making process to bullying and violence episodes.
Therefore, fact-checking algorithms became a valuable asset. To this aim, an
important step to detect fake-news is to have access to a credibility score for
a given information source. However, most of the widely used Web indicators
have either been shut-down to the public (e.g., Google PageRank) or are not
free for use (Alexa Rank). Further existing databases are short-manually
curated lists of online sources, which do not scale. Finally, most of the
research on the topic is theoretical-based or explore confidential data in a
restricted simulation environment. In this paper we explore current research,
highlight the challenges and propose solutions to tackle the problem of
classifying websites into a credibility scale. The proposed model automatically
extracts source reputation cues and computes a credibility factor, providing
valuable insights which can help in belittling dubious and confirming trustful
unknown websites. Experimental results outperform state of the art in the
2-classes and 5-classes setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esteves_D/0/1/0/all/0/1&quot;&gt;Diego Esteves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1&quot;&gt;Aniketh Janardhan Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chawla_P/0/1/0/all/0/1&quot;&gt;Piyush Chawla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1&quot;&gt;Jens Lehmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00564">
<title>ViewpointS: towards a Collective Brain. (arXiv:1809.00564v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1809.00564</link>
<description rdf:parseType="Literal">&lt;p&gt;Tracing knowledge acquisition and linking learning events to interaction
between peers is a major challenge of our times. We have conceived, designed
and evaluated a new paradigm for constructing and using collective knowledge by
Web interactions that we called ViewpointS. By exploiting the similarity with
Edelman&apos;s Theory of Neuronal Group Selection (TNGS), we conjecture that it may
be metaphorically considered a Collective Brain, especially effective in the
case of trans-disciplinary representations. Far from being without doubts, in
the paper we present the reasons (and the limits) of our proposal that aims to
become a useful integrating tool for future quantitative explorations of
individual as well as collective learning at different degrees of granu-larity.
We are therefore challenging each of the current approaches: the logical one in
the semantic Web, the statistical one in mining and deep learning, the social
one in recommender systems based on authority and trust; not in each of their
own preferred field of operation, rather in their integration weaknesses far
from the holistic and dynamic behavior of the human brain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemoisson_P/0/1/0/all/0/1&quot;&gt;Philippe Lemoisson&lt;/a&gt; (UMR TETIS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerri_S/0/1/0/all/0/1&quot;&gt;Stefano A. Cerri&lt;/a&gt; (SMILE)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00567">
<title>PathGAN: Visual Scanpath Prediction with Generative Adversarial Networks. (arXiv:1809.00567v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.00567</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce PathGAN, a deep neural network for visual scanpath prediction
trained on adversarial examples. A visual scanpath is defined as the sequence
of fixation points over an image defined by a human observer with its gaze.
PathGAN is composed of two parts, the generator and the discriminator. Both
parts extract features from images using off-the-shelf networks, and train
recurrent layers to generate or discriminate scanpaths accordingly. In scanpath
prediction, the stochastic nature of the data makes it very difficult to
generate realistic predictions using supervised learning strategies, but we
adopt adversarial training as a suitable alternative. Our experiments prove how
PathGAN improves the state of the art of visual scanpath prediction on the iSUN
and Salient360! datasets. Source code and models are available at
https://imatge-upc.github.io/pathgan/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assens_M/0/1/0/all/0/1&quot;&gt;Marc Assens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1&quot;&gt;Xavier Giro-i-Nieto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1&quot;&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1&quot;&gt;Noel E. O&amp;#x27;Connor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00647">
<title>Automatic Event Salience Identification. (arXiv:1809.00647v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.00647</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying the salience (i.e. importance) of discourse units is an important
task in language understanding. While events play important roles in text
documents, little research exists on analyzing their saliency status. This
paper empirically studies the Event Salience task and proposes two salience
detection models based on content similarities and discourse relations. The
first is a feature based salience model that incorporates similarities among
discourse units. The second is a neural model that captures more complex
relations between discourse units. Tested on our new large-scale event salience
corpus, both methods significantly outperform the strong frequency baseline,
while our neural model further improves the feature based one by a large
margin. Our analyses demonstrate that our neural model captures interesting
connections between salience and discourse unit relations (e.g., scripts and
frame structures).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengzhong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Chenyan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1&quot;&gt;Teruko Mitamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1&quot;&gt;Eduard Hovy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00716">
<title>InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset. (arXiv:1809.00716v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.00716</link>
<description rdf:parseType="Literal">&lt;p&gt;Datasets have gained an enormous amount of popularity in the computer vision
community, from training and evaluation of Deep Learning-based methods to
benchmarking Simultaneous Localization and Mapping (SLAM). Without a doubt,
synthetic imagery bears a vast potential due to scalability in terms of amounts
of data obtainable without tedious manual ground truth annotations or
measurements. Here, we present a dataset with the aim of providing a higher
degree of photo-realism, larger scale, more variability as well as serving a
wider range of purposes compared to existing datasets. Our dataset leverages
the availability of millions of professional interior designs and millions of
production-level furniture and object assets -- all coming with fine geometric
details and high-resolution texture. We render high-resolution and high
frame-rate video sequences following realistic trajectories while supporting
various camera types as well as providing inertial measurements. Together with
the release of the dataset, we will make executable program of our interactive
simulator software as well as our renderer available at
https://interiornetdataset.github.io. To showcase the usability and uniqueness
of our dataset, we show benchmarking results of both sparse and dense SLAM
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenbin Li&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeedi_S/0/1/0/all/0/1&quot;&gt;Sajad Saeedi&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCormac_J/0/1/0/all/0/1&quot;&gt;John McCormac&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_R/0/1/0/all/0/1&quot;&gt;Ronald Clark&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzoumanikas_D/0/1/0/all/0/1&quot;&gt;Dimos Tzoumanikas&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qing Ye&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuzhong Huang&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1&quot;&gt;Rui Tang&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leutenegger_S/0/1/0/all/0/1&quot;&gt;Stefan Leutenegger&lt;/a&gt; (1) ((1) Department of Computing, Imperial College London, London UK, SW7 2AZ (2) KooLab, Kujiale.com, Hangzhou China)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00949">
<title>Automating Analysis of Construction Workers Viewing Patterns for Personalized Safety Training and Management. (arXiv:1809.00949v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1809.00949</link>
<description rdf:parseType="Literal">&lt;p&gt;Unrecognized hazards increase the likelihood of workplace fatalities and
injuries substantially. However, recent research has demonstrated that a large
proportion of hazards remain unrecognized in dynamic construction environments.
Recent studies have suggested a strong correlation between viewing patterns of
workers and their hazard recognition performance. Hence, it is important to
study and analyze the viewing patterns of workers to gain a better
understanding of their hazard recognition performance. The objective of this
exploratory research is to explore hazard recognition as a visual search
process to identifying various visual search factors that affect the process of
hazard recognition. Further, the study also proposes a framework to develop a
vision based tool capable of recording and analyzing viewing patterns of
construction workers and generate feedback for personalized training and
proactive safety management.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeelani_I/0/1/0/all/0/1&quot;&gt;Idris Jeelani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kevin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albert_A/0/1/0/all/0/1&quot;&gt;Alex Albert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00969">
<title>A Deeper Insight into the UnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation. (arXiv:1809.00969v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.00969</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an unsupervised deep learning framework called UnDEMoN
for estimating dense depth map and 6-DoF camera pose information directly from
monocular images. The proposed network is trained using unlabeled monocular
stereo image pairs and is shown to provide superior performance in depth and
ego-motion estimation compared to the existing state-of-the-art. These
improvements are achieved by introducing a new objective function that aims to
minimize spatial as well as temporal reconstruction losses simultaneously.
These losses are defined using bi-linear sampling kernel and penalized using
the Charbonnier penalty function. The objective function, thus created,
provides robustness to image gradient noises thereby improving the overall
estimation accuracy without resorting to any coarse to fine strategies which
are currently prevalent in the literature. Another novelty lies in the fact
that we combine a disparity-based depth estimation network with a pose
estimation network to obtain absolute scale-aware 6 DOF Camera pose and
superior depth map. The effectiveness of the proposed approach is demonstrated
through performance comparison with the existing supervised and unsupervised
methods on the KITTI driving dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+V_M/0/1/0/all/0/1&quot;&gt;Madhu Babu V&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumder_A/0/1/0/all/0/1&quot;&gt;Anima Majumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_K/0/1/0/all/0/1&quot;&gt;Kaushik Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Swagat Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02808">
<title>Latent Space Policies for Hierarchical Reinforcement Learning. (arXiv:1804.02808v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02808</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of learning hierarchical deep neural network policies
for reinforcement learning. In contrast to methods that explicitly restrict or
cripple lower layers of a hierarchy to force them to use higher-level
modulating signals, each layer in our framework is trained to directly solve
the task, but acquires a range of diverse strategies via a maximum entropy
reinforcement learning objective. Each layer is also augmented with latent
random variables, which are sampled from a prior distribution during the
training of that layer. The maximum entropy objective causes these latent
variables to be incorporated into the layer&apos;s policy, and the higher level
layer can directly control the behavior of the lower layer through this latent
space. Furthermore, by constraining the mapping from latent variables to
actions to be invertible, higher layers retain full expressivity: neither the
higher layers nor the lower layers are constrained in their behavior. Our
experimental evaluation demonstrates that we can improve on the performance of
single-layer policies on standard benchmark tasks simply by adding additional
layers, and that our method can solve more complex sparse-reward tasks by
learning higher-level policies on top of high-entropy skills optimized for
simple low-level objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haarnoja_T/0/1/0/all/0/1&quot;&gt;Tuomas Haarnoja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartikainen_K/0/1/0/all/0/1&quot;&gt;Kristian Hartikainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.11192">
<title>Explainable Recommendation: A Survey and New Perspectives. (arXiv:1804.11192v4 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1804.11192</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable Recommendation refers to the personalized recommendation
algorithms that address the problem of why - they not only provide users with
the recommendations, but also provide explanations to make the user or system
designer aware of why such items are recommended. In this way, it helps to
improve the effectiveness, efficiency, persuasiveness, and user satisfaction of
recommendation systems. In recent years, a large number of explainable
recommendation approaches -- especially model-based explainable recommendation
algorithms -- have been proposed and adopted in real-world systems.
&lt;/p&gt;
&lt;p&gt;In this survey, we review the work on explainable recommendation that has
been published in or before the year of 2018. We first highlight the position
of explainable recommendation in recommender system research by categorizing
recommendation problems into the 5W, i.e., what, when, who, where, and why. We
then conduct a comprehensive survey of explainable recommendation itself in
terms of three aspects: 1) We provide a chronological research line of
explanations in recommender systems, including the user study approaches in the
early years, as well as the more recent model-based approaches. 2) We provide a
taxonomy for explainable recommendation algorithms, including user-based,
item-based, model-based, and post-model explanations. 3) We summarize the
application of explainable recommendation in different recommendation tasks,
including product recommendation, social recommendation, POI recommendation,
etc. We devote a section to discuss the explanation perspectives in the broader
IR and machine learning settings, as well as their relationship with
explainable recommendation research. We end the survey by discussing potential
future research directions to promote the explainable recommendation research
area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06881">
<title>Changing Observations in Epistemic Temporal Logic. (arXiv:1805.06881v2 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/1805.06881</link>
<description rdf:parseType="Literal">&lt;p&gt;We study dynamic changes of agents&apos; observational power in logics of
knowledge and time. We consider CTL*K, the extension of CTL* with knowledge
operators, and enrich it with a new operator that models a change in an agent&apos;s
way of observing the system. We extend the classic semantics of knowledge for
perfect-recall agents to account for changes of observation, and we show that
this new operator strictly increases the expressivity of CTL*K. We reduce the
model-checking problem for our logic to that for CTL*K, which is known to be
decidable. This provides a solution to the model-checking problem for our
logic, but its complexity is not optimal. Indeed we provide a direct decision
procedure with better complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barriere_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe8;le Barri&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maubert_B/0/1/0/all/0/1&quot;&gt;Bastien Maubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murano_A/0/1/0/all/0/1&quot;&gt;Aniello Murano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubin_S/0/1/0/all/0/1&quot;&gt;Sasha Rubin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07830">
<title>Learning to Teach in Cooperative Multiagent Reinforcement Learning. (arXiv:1805.07830v4 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07830</link>
<description rdf:parseType="Literal">&lt;p&gt;Collective human knowledge has clearly benefited from the fact that
innovations by individuals are taught to others through communication. Similar
to human social groups, agents in distributed learning systems would likely
benefit from communication to share knowledge and teach skills. The problem of
teaching to improve agent learning has been investigated by prior works, but
these approaches make assumptions that prevent application of teaching to
general multiagent problems, or require domain expertise for problems they can
apply to. This learning to teach problem has inherent complexities related to
measuring long-term impacts of teaching that compound the standard multiagent
coordination challenges. In contrast to existing works, this paper presents the
first general framework and algorithm for intelligent agents to learn to teach
in a multiagent environment. Our algorithm, Learning to Coordinate and Teach
Reinforcement (LeCTR), addresses peer-to-peer teaching in cooperative
multiagent reinforcement learning. Each agent in our approach learns both when
and what to advise, then uses the received advice to improve local learning.
Importantly, these roles are not fixed; these agents learn to assume the role
of student and/or teacher at the appropriate moments, requesting and providing
advice in order to improve teamwide performance and learning. Empirical
comparisons against state-of-the-art teaching methods show that our teaching
agents not only learn significantly faster, but also learn to coordinate in
tasks where existing methods fail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omidshafiei_S/0/1/0/all/0/1&quot;&gt;Shayegan Omidshafiei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dong-Ki Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Miao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tesauro_G/0/1/0/all/0/1&quot;&gt;Gerald Tesauro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1&quot;&gt;Matthew Riemer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amato_C/0/1/0/all/0/1&quot;&gt;Christopher Amato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1&quot;&gt;Murray Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1&quot;&gt;Jonathan P. How&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00082">
<title>The NEU Meta-Algorithm for Geometric Learning with Applications in Finance. (arXiv:1809.00082v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.00082</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a meta-algorithm, called non-Euclidean upgrading (NEU), which
learns algorithm-specific geometries to improve the training and validation set
performance of a wide class of learning algorithms. Our approach is based on
iteratively performing local reconfigurations of the space in which the data
lie. These reconfigurations build universal approximation and universal
reconfiguration properties into the new algorithm being learned. This allows
any set of features to be learned by the new algorithm to arbitrary precision.
The training and validation set performance of NEU is investigated through
implementations predicting the relationship between select stock prices as well
as finding low-dimensional representations of the German Bond yield curve.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kratsios_A/0/1/0/all/0/1&quot;&gt;Anastasis Kratsios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hyndman_C/0/1/0/all/0/1&quot;&gt;Cody B. Hyndman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00238">
<title>A Machine Learning Driven IoT Solution for Noise Classification in Smart Cities. (arXiv:1809.00238v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1809.00238</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a machine learning based method for noise classification using a
low-power and inexpensive IoT unit. We use Mel-frequency cepstral coefficients
for audio feature extraction and supervised classification algorithms (that is,
support vector machine and k-nearest neighbors) for noise classification. We
evaluate our approach experimentally with a dataset of about 3000 sound samples
grouped in eight sound classes (such as, car horn, jackhammer, or street
music). We explore the parameter space of support vector machine and k-nearest
neighbors algorithms to estimate the optimal parameter values for
classification of sound samples in the dataset under study. We achieve a noise
classification accuracy in the range 85% -- 100%. Training and testing of our
k-nearest neighbors (k = 1) implementation on Raspberry Pi Zero W is less than
a second for a dataset with features of more than 3000 sound samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alsouda_Y/0/1/0/all/0/1&quot;&gt;Yasser Alsouda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pllana_S/0/1/0/all/0/1&quot;&gt;Sabri Pllana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurti_A/0/1/0/all/0/1&quot;&gt;Arianit Kurti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00366">
<title>Cold-start recommendations in Collective Matrix Factorization. (arXiv:1809.00366v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1809.00366</link>
<description rdf:parseType="Literal">&lt;p&gt;This work explores the ability of collective matrix factorization models in
recommender systems to make predictions about users and items for which there
is side information available but no feedback or interactions data, and
proposes a new formulation with a faster cold-start prediction formula that can
be used in real-time systems. While these cold-start recommendations are not as
good as warm-start ones, they were found to be of better quality than
non-personalized recommendations, and predictions about new users were found to
be more reliable than those about new items. The formulation proposed here
resulted in improved cold-start recommendations in many scenarios, at the
expense of worse warm-start ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cortes_D/0/1/0/all/0/1&quot;&gt;David Cortes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00381">
<title>Multitask Learning for Fundamental Frequency Estimation in Music. (arXiv:1809.00381v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1809.00381</link>
<description rdf:parseType="Literal">&lt;p&gt;Fundamental frequency (f0) estimation from polyphonic music includes the
tasks of multiple-f0, melody, vocal, and bass line estimation. Historically
these problems have been approached separately, and only recently, using
learning-based approaches. We present a multitask deep learning architecture
that jointly estimates outputs for various tasks including multiple-f0, melody,
vocal and bass line estimation, and is trained using a large,
semi-automatically annotated dataset. We show that the multitask model
outperforms its single-task counterparts, and explore the effect of various
design decisions in our approach, and show that it performs better or at least
competitively when compared against strong baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bittner_R/0/1/0/all/0/1&quot;&gt;Rachel M. Bittner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McFee_B/0/1/0/all/0/1&quot;&gt;Brian McFee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bello_J/0/1/0/all/0/1&quot;&gt;Juan P. Bello&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00510">
<title>Flatland: a Lightweight First-Person 2-D Environment for Reinforcement Learning. (arXiv:1809.00510v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.00510</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Flatland, a simple, lightweight environment for fast prototyping
and testing of reinforcement learning agents. It is of lower complexity
compared to similar 3D platforms, e.g. DeepMind Lab or VizDoom. At the same
time it shares some properties with the real world, such as continuity,
multi-modal partially-observable states with first-person view and coherent
physics. We propose to use it as an intermediary benchmark for problems related
to Lifelong Learning. Flatland is highly customizable and offers a wide range
of task difficulty to extensively evaluate the properties of artificial agents.
We experiment with three reinforcement learning baseline agents and show that
they can rapidly solve a navigation task in Flatland. A video of an agent
acting in Flatland is available here: https://youtu.be/I5y6Y2ZypdA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caselles_Dupre_H/0/1/0/all/0/1&quot;&gt;Hugo Caselles-Dupr&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Annabi_L/0/1/0/all/0/1&quot;&gt;Louis Annabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagen_O/0/1/0/all/0/1&quot;&gt;Oksana Hagen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Ortiz_M/0/1/0/all/0/1&quot;&gt;Michael Garcia-Ortiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1&quot;&gt;David Filliat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00542">
<title>Machine learning for predicting thermal power consumption of the Mars Express Spacecraft. (arXiv:1809.00542v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.00542</link>
<description rdf:parseType="Literal">&lt;p&gt;The thermal subsystem of the Mars Express (MEX) spacecraft keeps the on-board
equipment within its pre-defined operating temperatures range. To plan and
optimize the scientific operations of MEX, its operators need to estimate in
advance, as accurately as possible, the power consumption of the thermal
subsystem. The remaining power can then be allocated for scientific purposes.
We present a machine learning pipeline for efficiently constructing accurate
predictive models for predicting the power of the thermal subsystem on board
MEX. In particular, we employ state-of-the-art feature engineering approaches
for transforming raw telemetry data, in turn used for constructing accurate
models with different state-of-the-art machine learning methods. We show that
the proposed pipeline considerably improve our previous (competition-winning)
work in terms of time efficiency and predictive performance. Moreover, while
achieving superior predictive performance, the constructed models also provide
important insight into the spacecraft&apos;s behavior, allowing for further analyses
and optimal planning of MEX&apos;s operation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petkovic_M/0/1/0/all/0/1&quot;&gt;Matej Petkovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boumghar_R/0/1/0/all/0/1&quot;&gt;Redouane Boumghar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breskvar_M/0/1/0/all/0/1&quot;&gt;Martin Breskvar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dzeroski_S/0/1/0/all/0/1&quot;&gt;Sa&amp;#x161;o D&amp;#x17e;eroski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocev_D/0/1/0/all/0/1&quot;&gt;Dragi Kocev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levatic_J/0/1/0/all/0/1&quot;&gt;Jurica Levati&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_L/0/1/0/all/0/1&quot;&gt;Luke Lucas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osojnik_A/0/1/0/all/0/1&quot;&gt;Alja&amp;#x17e; Osojnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenko_B/0/1/0/all/0/1&quot;&gt;Bernard &amp;#x17d;enko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1&quot;&gt;Nikola Simidjievski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00593">
<title>IoU is not submodular. (arXiv:1809.00593v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.00593</link>
<description rdf:parseType="Literal">&lt;p&gt;This short article aims at demonstrate that the Intersection over Union (or
Jaccard index) is not a submodular function. This mistake has been made in an
article which is cited and used as a foundation in another article. The
Intersection of Union is widely used in machine learning as a cost function
especially for imbalance data and semantic segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerdoncuff_T/0/1/0/all/0/1&quot;&gt;Tanguy Kerdoncuff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emonet_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Emonet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00653">
<title>Towards Dynamic Computation Graphs via Sparse Latent Structure. (arXiv:1809.00653v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.00653</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep NLP models benefit from underlying structures in the data---e.g., parse
trees---typically extracted using off-the-shelf parsers. Recent attempts to
jointly learn the latent structure encounter a tradeoff: either make
factorization assumptions that limit expressiveness, or sacrifice end-to-end
differentiability. Using the recently proposed SparseMAP inference, which
retrieves a sparse distribution over latent structures, we propose a novel
approach for end-to-end learning of latent structure predictors jointly with a
downstream predictor. To the best of our knowledge, our method is the first to
enable unrestricted dynamic computation graph construction from the global
latent structure, while maintaining differentiability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1&quot;&gt;Vlad Niculae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1&quot;&gt;Claire Cardie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00710">
<title>A Dual Approach for Optimal Algorithms in Distributed Optimization over Networks. (arXiv:1809.00710v1 [math.OC])</title>
<link>http://arxiv.org/abs/1809.00710</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the optimal convergence rates for distributed convex optimization
problems over networks, where the objective is to minimize the sum
$\sum_{i=1}^{m}f_i(z)$ of local functions of the nodes in the network. We
provide optimal complexity bounds for four different cases, namely: the case
when each function $f_i$ is strongly convex and smooth, the cases when it is
either strongly convex or smooth and the case when it is convex but neither
strongly convex nor smooth. Our approach is based on the dual of an
appropriately formulated primal problem, which includes the underlying static
graph that models the communication restrictions. Our results show distributed
algorithms that achieve the same optimal rates as their centralized
counterparts (up to constant and logarithmic factors), with an additional cost
related to the spectral gap of the interaction matrix that captures the local
communications of the nodes in the network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Uribe_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;sar A. Uribe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Soomin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1&quot;&gt;Alexander Gasnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nedic_A/0/1/0/all/0/1&quot;&gt;Angelia Nedi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00811">
<title>A Deep Learning Spatiotemporal Prediction Framework for Mobile Crowdsourced Services. (arXiv:1809.00811v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.00811</link>
<description rdf:parseType="Literal">&lt;p&gt;This papers presents a deep learning-based framework to predict crowdsourced
service availability spatially and temporally. A novel two-stage prediction
model is introduced based on historical spatio-temporal traces of mobile
crowdsourced services. The prediction model first clusters mobile crowdsourced
services into regions. The availability prediction of a mobile crowdsourced
service at a certain location and time is then formulated as a classification
problem. To determine the availability duration of predicted mobile
crowdsourced services, we formulate a forecasting task of time series using the
Gramian Angular Field. We validated the effectiveness of the proposed framework
through multiple experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Said_A/0/1/0/all/0/1&quot;&gt;Ahmed Ben Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erradi_A/0/1/0/all/0/1&quot;&gt;Abdelkarim Erradi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neiat_A/0/1/0/all/0/1&quot;&gt;Azadeh Ghari Neiat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouguettaya_A/0/1/0/all/0/1&quot;&gt;Athman Bouguettaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00947">
<title>Finding Dory in the Crowd: Detecting Social Interactions using Multi-Modal Mobile Sensing. (arXiv:1809.00947v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1809.00947</link>
<description rdf:parseType="Literal">&lt;p&gt;Remembering our day-to-day social interactions is challenging even if you
aren&apos;t a blue memory challenged fish. The ability to automatically detect and
remember these types of interactions is not only beneficial for individuals
interested in their behavior in crowded situations, but also of interest to
those who analyze crowd behavior. Currently, detecting social interactions is
often performed using a variety of methods including ethnographic studies,
computer vision techniques and manual annotation-based data analysis. However,
mobile phones offer easier means for data collection that is easy to analyze
and can preserve the user&apos;s privacy. In this work, we present a system for
detecting stationary social interactions inside crowds, leveraging multi-modal
mobile sensing data such as Bluetooth Smart (BLE), accelerometer and gyroscope.
To inform the development of such system, we conducted a study with 24
participants, where we asked them to socialize with each other for 45 minutes.
We built a machine learning system based on gradient-boosted trees that
predicts both 1:1 and group interactions with 77.8% precision and 86.5% recall,
a 30.2% performance increase compared to a proximity-based approach. By
utilizing a community detection based method, we further detected the various
group formation that exist within the crowd. Using mobile phone sensors already
carried by the majority of people in a crowd makes our approach particularly
well suited to real-life analysis of crowd behaviour and influence strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katevas_K/0/1/0/all/0/1&quot;&gt;Kleomenis Katevas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansel_K/0/1/0/all/0/1&quot;&gt;Katrin H&amp;#xe4;nsel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clegg_R/0/1/0/all/0/1&quot;&gt;Richard Clegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1&quot;&gt;Ilias Leontiadis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1&quot;&gt;Hamed Haddadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tokarchuk_L/0/1/0/all/0/1&quot;&gt;Laurissa Tokarchuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00961">
<title>MSCE: An edge preserving robust loss function for improving super-resolution algorithms. (arXiv:1809.00961v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.00961</link>
<description rdf:parseType="Literal">&lt;p&gt;With the recent advancement in the deep learning technologies such as CNNs
and GANs, there is significant improvement in the quality of the images
reconstructed by deep learning based super-resolution (SR) techniques. In this
work, we propose a robust loss function based on the preservation of edges
obtained by the Canny operator. This loss function, when combined with the
existing loss function such as mean square error (MSE), gives better SR
reconstruction measured in terms of PSNR and SSIM. Our proposed loss function
guarantees improved performance on any existing algorithm using MSE loss
function, without any increase in the computational complexity during testing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1&quot;&gt;Ram Krishna Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_N/0/1/0/all/0/1&quot;&gt;Nabagata Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karmakar_S/0/1/0/all/0/1&quot;&gt;Samarjit Karmakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1&quot;&gt;A G Ramakrishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.00999">
<title>Towards Large Scale Training Of Autoencoders For Collaborative Filtering. (arXiv:1809.00999v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1809.00999</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we apply a mini-batch based negative sampling method to
efficiently train a latent factor autoencoder model on large scale and sparse
data for implicit feedback collaborative filtering. We compare our work against
a state-of-the-art baseline model on different experimental datasets and show
that this method can lead to a good and fast approximation of the baseline
model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moussawi_A/0/1/0/all/0/1&quot;&gt;Abdallah Moussawi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01000">
<title>Bayesian Outdoor Defect Detection. (arXiv:1809.01000v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.01000</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a Bayesian defect detector to facilitate the defect detection on
the motion blurred images on rough texture surfaces. To enhance the accuracy of
Bayesian detection on removing non-defect pixels, we develop a class of
reflected non-local prior distributions, which is constructed by using the mode
of a distribution to subtract its density. The reflected non-local priors
forces the Bayesian detector to approach 0 at the non-defect locations. We
conduct experiments studies to demonstrate the superior performance of the
Bayesian detector in eliminating the non-defect points. We implement the
Bayesian detector in the motion blurred drone images, in which the detector
successfully identifies the hail damages on the rough surface and substantially
enhances the accuracy of the entire defect detection pipeline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1&quot;&gt;Fei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1&quot;&gt;Guosheng Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01015">
<title>Automated segmentation on the entire cardiac cycle using a deep learning work-flow. (arXiv:1809.01015v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.01015</link>
<description rdf:parseType="Literal">&lt;p&gt;The segmentation of the left ventricle (LV) from CINE MRI images is essential
to infer important clinical parameters. Typically, machine learning algorithms
for automated LV segmentation use annotated contours from only two cardiac
phases, diastole, and systole. In this work, we present an analysis work-flow
for fully-automated LV segmentation that learns from images acquired through
the cardiac cycle. The workflow consists of three components: first, for each
image in the sequence, we perform an automated localization and subsequent
cropping of the bounding box containing the cardiac silhouette. Second, we
identify the LV contours using a Temporal Fully Convolutional Neural Network
(T-FCNN), which extends Fully Convolutional Neural Networks (FCNN) through a
recurrent mechanism enforcing temporal coherence across consecutive frames.
Finally, we further defined the boundaries using either one of two components:
fully-connected Conditional Random Fields (CRFs) with Gaussian edge potentials
and Semantic Flow. Our initial experiments suggest that significant improvement
in performance can potentially be achieved by using a recurrent neural network
component that explicitly learns cardiac motion patterns whilst performing LV
segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savioli_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xf3; Savioli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vieira_M/0/1/0/all/0/1&quot;&gt;Miguel Silva Vieira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamata_P/0/1/0/all/0/1&quot;&gt;Pablo Lamata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montana_G/0/1/0/all/0/1&quot;&gt;Giovanni Montana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01022">
<title>A Neural Network Aided Approach for LDPC Coded DCO-OFDM with Clipping Distortion. (arXiv:1809.01022v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.01022</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, a neural network-aided bit-interleaved coded modulation
(NN-BICM) receiver is designed to mitigate the nonlinear clipping distortion in
the LDPC coded direct currentbiased optical orthogonal frequency division
multiplexing (DCOOFDM) systems. Taking the cross-entropy as loss function, a
feed forward network is trained by backpropagation algorithm to output the
condition probability through the softmax activation function, thereby
assisting in a modified log-likelihood ratio (LLR) improvement. To reduce the
complexity, this feed-forward network simplifies the input layer with a single
symbol and the corresponding Gaussian variance instead of focusing on the
inter-carrier interference between multiple subcarriers. On the basis of the
neural network-aided BICM with Gray labelling, we propose a novel stacked
network architecture of the bitinterleaved coded modulation with iterative
decoding (NN-BICMID). Its performance has been improved further by calculating
the condition probability with the aid of a priori probability that derived
from the extrinsic LLRs in the LDPC decoder at the last iteration, at the
expense of customizing neural network detectors at each iteration time
separately. Utilizing the optimal DC bias as the midpoint of the dynamic
region, the simulation results demonstrate that both the NN-BICM and NN-BICM-ID
schemes achieve noticeable performance gains than other counterparts, in which
the NN-BICM-ID clearly outperforms NN-BICM with various modulation and coding
schemes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Ming Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chunming Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.01046">
<title>Group-Representative Functional Network Estimation from Multi-Subject fMRI Data via MRF-based Image Segmentation. (arXiv:1809.01046v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1809.01046</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel two-phase approach to functional network estimation of
multi-subject functional Magnetic Resonance Imaging (fMRI) data, which applies
model-based image segmentation to determine a group-representative connectivity
map. In our approach, we first improve clustering-based Independent Component
Analysis (ICA) to generate maps of components occurring consistently across
subjects, and then estimate the group-representative map through MAP-MRF
(Maximum a priori - Markov random field) labeling. For the latter, we provide a
novel and efficient variational Bayes algorithm. We study the performance of
the proposed method using synthesized data following a theoretical model, and
demonstrate its viability in blind extraction of group-representative
functional networks using simulated fMRI data. We anticipate the proposed
method will be applied in identifying common neuronal characteristics in a
population, and could be further extended to real-world clinical diagnosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Iyer_A/0/1/0/all/0/1&quot;&gt;Aditi Iyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tang_B/0/1/0/all/0/1&quot;&gt;Bingjing Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rao_V/0/1/0/all/0/1&quot;&gt;Vinayak Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kong_N/0/1/0/all/0/1&quot;&gt;Nan Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1511.05174">
<title>Cross-scale predictive dictionaries. (arXiv:1511.05174v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1511.05174</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse representations using data dictionaries provide an efficient model
particularly for signals that do not enjoy alternate analytic sparsifying
transformations. However, solving inverse problems with sparsifying
dictionaries can be computationally expensive, especially when the dictionary
under consideration has a large number of atoms. In this paper, we incorporate
additional structure on to dictionary-based sparse representations for visual
signals to enable speedups when solving sparse approximation problems. The
specific structure that we endow onto sparse models is that of a multi-scale
modeling where the sparse representation at each scale is constrained by the
sparse representation at coarser scales. We show that this cross-scale
predictive model delivers significant speedups, often in the range of
10-60$\times$, with little loss in accuracy for linear inverse problems
associated with images, videos, and light fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saragadam_V/0/1/0/all/0/1&quot;&gt;Vishwanath Saragadam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankaranarayanan_A/0/1/0/all/0/1&quot;&gt;Aswin Sankaranarayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1604.08697">
<title>Sparse Generalized Eigenvalue Problem: Optimal Statistical Rates via Truncated Rayleigh Flow. (arXiv:1604.08697v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1604.08697</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse generalized eigenvalue problem (GEP) plays a pivotal role in a large
family of high-dimensional statistical models, including sparse Fisher&apos;s
discriminant analysis, canonical correlation analysis, and sufficient dimension
reduction. Sparse GEP involves solving a non-convex optimization problem. Most
existing methods and theory in the context of specific statistical models that
are special cases of the sparse GEP require restrictive structural assumptions
on the input matrices. In this paper, we propose a two-stage computational
framework to solve the sparse GEP. At the first stage, we solve a convex
relaxation of the sparse GEP. Taking the solution as an initial value, we then
exploit a nonconvex optimization perspective and propose the truncated Rayleigh
flow method (Rifle) to estimate the leading generalized eigenvector. We show
that Rifle converges linearly to a solution with the optimal statistical rate
of convergence for many statistical models. Theoretically, our method
significantly improves upon the existing literature by eliminating structural
assumptions on the input matrices for both stages. To achieve this, our
analysis involves two key ingredients: (i) a new analysis of the gradient based
method on nonconvex objective functions, and (ii) a fine-grained
characterization of the evolution of sparsity patterns along the solution path.
Thorough numerical studies are provided to validate the theoretical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kean Ming Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.08306">
<title>Machine Learning in Downlink Coordinated Multipoint in Heterogeneous Networks. (arXiv:1608.08306v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1608.08306</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method for practical downlink coordinated multipoint (DL CoMP)
implementation in the fifth generation of wireless communications (5G) also
known as New Radio (NR). We base our method on supervised machine learning.
Contributions of this paper are to 1) demonstrate that a support vector machine
(SVM) classifier can learn improved conditions at which DL CoMP can be
dynamically triggered in a scalable realistic environment and 2) increase user
throughput in a heterogeneous network as a result of learning improved
triggering conditions of CoMP. Our simulation results show an improvement in
both the macro and pico base station peak throughputs due to the informed
triggering of the multiple DL CoMP radio streams as learned from the SVM
classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mismar_F/0/1/0/all/0/1&quot;&gt;Faris B. Mismar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Evans_B/0/1/0/all/0/1&quot;&gt;Brian L. Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.02655">
<title>Singularity structures and impacts on parameter estimation in finite mixtures of distributions. (arXiv:1609.02655v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1609.02655</link>
<description rdf:parseType="Literal">&lt;p&gt;Singularities of a statistical model are the elements of the model&apos;s
parameter space which make the corresponding Fisher information matrix
degenerate. These are the points for which estimation techniques such as the
maximum likelihood estimator and standard Bayesian procedures do not admit the
root-$n$ parametric rate of convergence. We propose a general framework for the
identification of singularity structures of the parameter space of finite
mixtures, and study the impacts of the singularity structures on minimax lower
bounds and rates of convergence for the maximum likelihood estimator over a
compact parameter space. Our study makes explicit the deep links between model
singularities, parameter estimation convergence rates and minimax lower bounds,
and the algebraic geometry of the parameter space for mixtures of continuous
distributions. The theory is applied to establish concrete convergence rates of
parameter estimation for finite mixture of skew-normal distributions. This rich
and increasingly popular mixture model is shown to exhibit a remarkably complex
range of asymptotic behaviors which have not been hitherto reported in the
literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ho_N/0/1/0/all/0/1&quot;&gt;Nhat Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nguyen_X/0/1/0/all/0/1&quot;&gt;XuanLong Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.01305">
<title>Mass Volume Curves and Anomaly Ranking. (arXiv:1705.01305v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.01305</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims at formulating the issue of ranking multivariate unlabeled
observations depending on their degree of abnormality as an unsupervised
statistical learning task. In the 1-d situation, this problem is usually
tackled by means of tail estimation techniques: univariate observations are
viewed as all the more `abnormal&apos; as they are located far in the tail(s) of the
underlying probability distribution. It would be desirable as well to dispose
of a scalar valued `scoring&apos; function allowing for comparing the degree of
abnormality of multivariate observations. Here we formulate the issue of
scoring anomalies as a M-estimation problem by means of a novel functional
performance criterion, referred to as the Mass Volume curve (MV curve in
short), whose optimal elements are strictly increasing transforms of the
density almost everywhere on the support of the density. We first study the
statistical estimation of the MV curve of a given scoring function and we
provide a strategy to build confidence regions using a smoothed bootstrap
approach. Optimization of this functional criterion over the set of piecewise
constant scoring functions is next tackled. This boils down to estimating a
sequence of empirical minimum volume sets whose levels are chosen adaptively
from the data, so as to adjust to the variations of the optimal MV curve, while
controling the bias of its approximation by a stepwise curve. Generalization
bounds are then established for the difference in sup norm between the MV curve
of the empirical scoring function thus obtained and the optimal MV curve.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Clemencon_S/0/1/0/all/0/1&quot;&gt;Stephan Cl&amp;#xe9;men&amp;#xe7;on&lt;/a&gt; (LTCI, TSI), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thomas_A/0/1/0/all/0/1&quot;&gt;Albert Thomas&lt;/a&gt; (LTCI)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04673">
<title>Analysis of Set-Valued Stochastic Approximations: Applications to Noisy Approximate Value and Fixed point Iterations. (arXiv:1709.04673v3 [cs.SY] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04673</link>
<description rdf:parseType="Literal">&lt;p&gt;The main aim of this paper is the development of Lyapunov function based
sufficient conditions for stability (almost sure boundedness) and convergence
of stochastic approximation algorithms (SAAs) with set-valued mean-fields, a
class of model-free algorithms that have become important in recent times. In
this paper we provide a complete analysis of such algorithms under three
different, yet related set of sufficient conditions, based on the existence of
an associated global/local Lyapunov function. Unlike previous Lyapunov function
based approaches, we provide a simple recipe for explicitly constructing the
Lyapunov function needed for analysis. Our work builds on the works of
Abounadi, Bertsekas and Borkar (2002), Munos (2005) and Ramaswamy and Bhatnagar
(2016). An important motivation to the flavor of our assumptions comes from the
need to understand approximate dynamic programming and reinforcement learning
algorithms, that use deep neural networks (DNNs) for function approximations
and parameterizations. These algorithms are popularly known as deep
reinforcement learning algorithms. As an important application of our theory we
provide a complete analysis of the stochastic approximation counterpart of
approximate value iteration (AVI), an important dynamic programming method
designed to tackle Bellman&apos;s curse of dimensionality. Although motivated by the
need to understand deep reinforcement learning algorithms our theory is more
generally applicable. It is further used to develop the first SAA for finding
fixed points of contractive set-valued maps and provide a comprehensive
analysis of the same.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramaswamy_A/0/1/0/all/0/1&quot;&gt;Arunselvan Ramaswamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatnagar_S/0/1/0/all/0/1&quot;&gt;Shalabh Bhatnagar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.03851">
<title>Autoencoders and Probabilistic Inference with Missing Data: An Exact Solution for The Factor Analysis Case. (arXiv:1801.03851v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.03851</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent variable models can be used to probabilistically &quot;fill-in&quot; missing
data entries. The variational autoencoder architecture (Kingma and Welling,
2014; Rezende et al., 2014) includes a &quot;recognition&quot; or &quot;encoder&quot; network that
infers the latent variables given the data variables. However, it is not clear
how to handle missing data variables in this network. The factor analysis (FA)
model is a basic autoencoder, using linear encoder and decoder networks. We
show how to calculate exactly the latent posterior distribution for the factor
analysis (FA) model in the presence of missing data, and note that this
solution exhibits a non-trivial dependence on the pattern of missingness. We
also discuss various approximations to the exact solution. Experiments compare
the effectiveness of various approaches to filling in the missing data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1&quot;&gt;Christopher K. I. Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nash_C/0/1/0/all/0/1&quot;&gt;Charlie Nash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazabal_A/0/1/0/all/0/1&quot;&gt;Alfredo Naz&amp;#xe1;bal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06355">
<title>Stochastic Chebyshev Gradient Descent for Spectral Optimization. (arXiv:1802.06355v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06355</link>
<description rdf:parseType="Literal">&lt;p&gt;A large class of machine learning techniques requires the solution of
optimization problems involving spectral functions of parametric matrices, e.g.
log-determinant and nuclear norm. Unfortunately, computing the gradient of a
spectral function is generally of cubic complexity, as such gradient descent
methods are rather expensive for optimizing objectives involving the spectral
function. Thus, one naturally turns to stochastic gradient methods in hope that
they will provide a way to reduce or altogether avoid the computation of full
gradients. However, here a new challenge appears: there is no straightforward
way to compute unbiased stochastic gradients for spectral functions. In this
paper, we develop unbiased stochastic gradients for spectral-sums, an important
subclass of spectral functions. Our unbiased stochastic gradients are based on
combining randomized trace estimators with stochastic truncation of the
Chebyshev expansions. A careful design of the truncation distribution allows us
to offer distributions that are variance-optimal, which is crucial for fast and
stable convergence of stochastic gradient methods. We further leverage our
proposed stochastic gradients to devise stochastic methods for objective
functions involving spectral-sums, and rigorously analyze their convergence
rate. The utility of our methods is demonstrated in numerical experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_I/0/1/0/all/0/1&quot;&gt;Insu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avron_H/0/1/0/all/0/1&quot;&gt;Haim Avron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jinwoo Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00114">
<title>SQL-Rank: A Listwise Approach to Collaborative Ranking. (arXiv:1803.00114v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00114</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a listwise approach for constructing user-specific
rankings in recommendation systems in a collaborative fashion. We contrast the
listwise approach to previous pointwise and pairwise approaches, which are
based on treating either each rating or each pairwise comparison as an
independent instance respectively. By extending the work of (Cao et al. 2007),
we cast listwise collaborative ranking as maximum likelihood under a
permutation model which applies probability mass to permutations based on a low
rank latent score matrix. We present a novel algorithm called SQL-Rank, which
can accommodate ties and missing data and can run in linear time. We develop a
theoretical framework for analyzing listwise ranking methods based on a novel
representation theory for the permutation model. Applying this framework to
collaborative ranking, we derive asymptotic statistical rates as the number of
users and items grow together. We conclude by demonstrating that our SQL-Rank
method often outperforms current state-of-the-art algorithms for implicit
feedback such as Weighted-MF and BPR and achieve favorable results when
compared to explicit feedback algorithms such as matrix factorization and
collaborative ranking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sharpnack_J/0/1/0/all/0/1&quot;&gt;James Sharpnack&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09730">
<title>Resilient Active Information Gathering with Mobile Robots. (arXiv:1803.09730v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09730</link>
<description rdf:parseType="Literal">&lt;p&gt;Applications of safety, security, and rescue in robotics, such as multi-robot
target tracking, involve the execution of information acquisition tasks by
teams of mobile robots. However, in failure-prone or adversarial environments,
robots get attacked, their communication channels get jammed, and their sensors
may fail, resulting in the withdrawal of robots from the collective task, and
consequently the inability of the remaining active robots to coordinate with
each other. As a result, traditional design paradigms become insufficient and,
in contrast, resilient designs against system-wide failures and attacks become
important. In general, resilient design problems are hard, and even though they
often involve objective functions that are monotone or submodular, scalable
approximation algorithms for their solution have been hitherto unknown. In this
paper, we provide the first algorithm, enabling the following capabilities:
minimal communication, i.e., the algorithm is executed by the robots based only
on minimal communication between them; system-wide resiliency, i.e., the
algorithm is valid for any number of denial-of-service attacks and failures;
and provable approximation performance, i.e., the algorithm ensures for all
monotone (and not necessarily submodular) objective functions a solution that
is finitely close to the optimal. We quantify our algorithm&apos;s approximation
performance using a notion of curvature for monotone set functions. We support
our theoretical analyses with simulated and real-world experiments, by
considering an active information gathering scenario, namely, multi-robot
target tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlotfeldt_B/0/1/0/all/0/1&quot;&gt;Brent Schlotfeldt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzoumas_V/0/1/0/all/0/1&quot;&gt;Vasileios Tzoumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakur_D/0/1/0/all/0/1&quot;&gt;Dinesh Thakur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1&quot;&gt;George J. Pappas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.11451">
<title>Minimax Estimation of Quadratic Fourier Functionals. (arXiv:1803.11451v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1803.11451</link>
<description rdf:parseType="Literal">&lt;p&gt;We study estimation of (semi-)inner products between two nonparametric
probability distributions, given IID samples from each distribution. These
products include relatively well-studied classical $\mathcal{L}^2$ and Sobolev
inner products, as well as those induced by translation-invariant reproducing
kernels, for which we believe our results are the first. We first propose
estimators for these quantities, and the induced (semi)norms and
(pseudo)metrics. We then prove non-asymptotic upper bounds on their mean
squared error, in terms of weights both of the inner product and of the two
distributions, in the Fourier basis. Finally, we prove minimax lower bounds
that imply rate-optimality of the proposed estimators over Fourier ellipsoids.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Shashank Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sriperumbudur_B/0/1/0/all/0/1&quot;&gt;Bharath K. Sriperumbudur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnab&amp;#xe1;s P&amp;#xf3;czos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00341">
<title>Sparse Principal Component Analysis via Variable Projection. (arXiv:1804.00341v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00341</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse principal component analysis (SPCA) has emerged as a powerful
technique for data analysis, providing improved interpretation of low-rank
structures by identifying localized spatial structures in the data and
disambiguating between distinct time scales. We demonstrate a robust and
scalable SPCA algorithm by formulating it as a value-function optimization
problem. This viewpoint leads to a flexible and computationally efficient
algorithm. It can further leverage randomized methods from linear algebra to
extend the approach to the large-scale (big data) setting. Our proposed
innovation also allows for a robust SPCA formulation which can obtain
meaningful sparse components in spite of grossly corrupted input data. The
proposed algorithms are demonstrated using both synthetic and real world data,
showing exceptional computational efficiency and diagnostic performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Erichson_N/0/1/0/all/0/1&quot;&gt;N. Benjamin Erichson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_P/0/1/0/all/0/1&quot;&gt;Peng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Manohar_K/0/1/0/all/0/1&quot;&gt;Krithika Manohar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brunton_S/0/1/0/all/0/1&quot;&gt;Steven L. Brunton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kutz_J/0/1/0/all/0/1&quot;&gt;J. Nathan Kutz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aravkin_A/0/1/0/all/0/1&quot;&gt;Aleksandr Y. Aravkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05810">
<title>ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detector. (arXiv:1804.05810v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05810</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the ability to directly manipulate image pixels in the digital input
space, an adversary can easily generate imperceptible perturbations to fool a
Deep Neural Network (DNN) image classifier, as demonstrated in prior work. In
this work, we propose ShapeShifter, an attack that tackles the more challenging
problem of crafting physical adversarial perturbations to fool image-based
object detectors like Faster R-CNN. Attacking an object detector is more
difficult than attacking an image classifier, as it needs to mislead the
classification results in multiple bounding boxes with different scales.
Extending the digital attack to the physical world adds another layer of
difficulty, because it requires the perturbation to be robust enough to survive
real-world distortions due to different viewing distances and angles, lighting
conditions, and camera limitations. We show that the Expectation over
Transformation technique, which was originally proposed to enhance the
robustness of adversarial perturbations in image classification, can be
successfully adapted to the object detection setting. ShapeShifter can generate
adversarially perturbed stop signs that are consistently mis-detected by Faster
R-CNN as other objects, posing a potential threat to autonomous vehicles and
other safety-critical computer vision systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shang-Tse Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornelius_C/0/1/0/all/0/1&quot;&gt;Cory Cornelius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1&quot;&gt;Jason Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07351">
<title>Sampling-free Uncertainty Estimation in Gated Recurrent Units with Exponential Families. (arXiv:1804.07351v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07351</link>
<description rdf:parseType="Literal">&lt;p&gt;There has recently been a concerted effort to derive mechanisms in vision and
machine learning systems to offer uncertainty estimates of the predictions they
make. Clearly, there are enormous benefits to a system that is not only
accurate but also has a sense for when it is not sure. Existing proposals
center around Bayesian interpretations of modern deep architectures -- these
are effective but can often be computationally demanding. We show how classical
ideas in the literature on exponential families on probabilistic networks
provide an excellent starting point to derive uncertainty estimates in Gated
Recurrent Units (GRU). Our proposal directly quantifies uncertainty
deterministically, without the need for costly sampling-based estimation. We
demonstrate how our model can be used to quantitatively and qualitatively
measure uncertainty in unsupervised image sequence prediction. To our
knowledge, this is the first result describing sampling-free uncertainty
estimation for powerful sequential models such as GRUs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Seong Jae Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1&quot;&gt;Ronak Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunwoo J. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1&quot;&gt;Vikas Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08438">
<title>A Spoofing Benchmark for the 2018 Voice Conversion Challenge: Leveraging from Spoofing Countermeasures for Speech Artifact Assessment. (arXiv:1804.08438v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08438</link>
<description rdf:parseType="Literal">&lt;p&gt;Voice conversion (VC) aims at conversion of speaker characteristic without
altering content. Due to training data limitations and modeling imperfections,
it is difficult to achieve believable speaker mimicry without introducing
processing artifacts; performance assessment of VC, therefore, usually involves
both speaker similarity and quality evaluation by a human panel. As a
time-consuming, expensive, and non-reproducible process, it hinders rapid
prototyping of new VC technology. We address artifact assessment using an
alternative, objective approach leveraging from prior work on spoofing
countermeasures (CMs) for automatic speaker verification. Therein, CMs are used
for rejecting `fake&apos; inputs such as replayed, synthetic or converted speech but
their potential for automatic speech artifact assessment remains unknown. This
study serves to fill that gap. As a supplement to subjective results for the
2018 Voice Conversion Challenge (VCC&apos;18) data, we configure a standard
constant-Q cepstral coefficient CM to quantify the extent of processing
artifacts. Equal error rate (EER) of the CM, a confusability index of VC
samples with real human speech, serves as our artifact measure. Two clusters of
VCC&apos;18 entries are identified: low-quality ones with detectable artifacts (low
EERs), and higher quality ones with less artifacts. None of the VCC&apos;18 systems,
however, is perfect: all EERs are &amp;lt; 30 % (the `ideal&apos; value would be 50 %). Our
preliminary findings suggest potential of CMs outside of their original
application, as a supplemental optimization and benchmarking tool to enhance VC
technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1&quot;&gt;Tomi Kinnunen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1&quot;&gt;Jaime Lorenzo-Trueba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Toda_T/0/1/0/all/0/1&quot;&gt;Tomoki Toda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saito_D/0/1/0/all/0/1&quot;&gt;Daisuke Saito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Villavicencio_F/0/1/0/all/0/1&quot;&gt;Fernando Villavicencio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Ling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07777">
<title>DLBI: Deep learning guided Bayesian inference for structure reconstruction of super-resolution fluorescence microscopy. (arXiv:1805.07777v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07777</link>
<description rdf:parseType="Literal">&lt;p&gt;Super-resolution fluorescence microscopy, with a resolution beyond the
diffraction limit of light, has become an indispensable tool to directly
visualize biological structures in living cells at a nanometer-scale
resolution. Despite advances in high-density super-resolution fluorescent
techniques, existing methods still have bottlenecks, including extremely long
execution time, artificial thinning and thickening of structures, and lack of
ability to capture latent structures. Here we propose a novel deep learning
guided Bayesian inference approach, DLBI, for the time-series analysis of
high-density fluorescent images. Our method combines the strength of deep
learning and statistical inference, where deep learning captures the underlying
distribution of the fluorophores that are consistent with the observed
time-series fluorescent images by exploring local features and correlation
along time-axis, and statistical inference further refines the ultrastructure
extracted by deep learning and endues physical meaning to the final image.
Comprehensive experimental results on both real and simulated datasets
demonstrate that our method provides more accurate and realistic local patch
and large-field reconstruction than the state-of-the-art method, the 3B
analysis, while our method is more than two orders of magnitude faster. The
main program is available at https://github.com/lykaust15/DLBI
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1&quot;&gt;Fan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fa Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Pingyong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingshu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1&quot;&gt;Ming Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lihua Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1&quot;&gt;Renmin Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00534">
<title>Run Procrustes, Run! On the convergence of accelerated Procrustes Flow. (arXiv:1806.00534v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00534</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present theoretical results on the convergence of non-convex
accelerated gradient descent in matrix factorization models. The technique is
applied to matrix sensing problems with squared loss, for the estimation of a
rank $r$ optimal solution $X^\star \in \mathbb{R}^{n \times n}$. We show that
the acceleration leads to linear convergence rate, even under non-convex
settings where the variable $X$ is represented as $U U^\top$ for $U \in
\mathbb{R}^{n \times r}$. Our result has the same dependence on the condition
number of the objective --and the optimal solution-- as that of the recent
results on non-accelerated algorithms. However, acceleration is observed in
practice, both in synthetic examples and in two real applications: neuronal
multi-unit activities recovery from single electrode recordings, and quantum
state tomography on quantum computing simulators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1&quot;&gt;Anastasios Kyrillidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ubaru_S/0/1/0/all/0/1&quot;&gt;Shashanka Ubaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollias_G/0/1/0/all/0/1&quot;&gt;Georgios Kollias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1&quot;&gt;Kristofer Bouchard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.10728">
<title>Deep Echo State Networks with Uncertainty Quantification for Spatio-Temporal Forecasting. (arXiv:1806.10728v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.10728</link>
<description rdf:parseType="Literal">&lt;p&gt;Long-lead forecasting for spatio-temporal systems can often entail complex
nonlinear dynamics that are difficult to specify it a priori. Current
statistical methodologies for modeling these processes are often highly
parameterized and thus, challenging to implement from a computational
perspective. One potential parsimonious solution to this problem is a method
from the dynamical systems and engineering literature referred to as an echo
state network (ESN). ESN models use so-called {\it reservoir computing} to
efficiently compute recurrent neural network (RNN) forecasts. Moreover,
so-called &quot;deep&quot; models have recently been shown to be successful at predicting
high-dimensional complex nonlinear processes, particularly those with multiple
spatial and temporal scales of variability (such as we often find in
spatio-temporal environmental data). Here we introduce a deep ensemble ESN
(D-EESN) model. We present two versions of this model for spatio-temporal
processes that both produce forecasts and associated measures of uncertainty.
The first approach utilizes a bootstrap ensemble framework and the second is
developed within a hierarchical Bayesian framework (BD-EESN). This more general
hierarchical Bayesian framework naturally accommodates non-Gaussian data types
and multiple levels of uncertainties. The methodology is first applied to a
data set simulated from a novel non-Gaussian multiscale Lorenz-96 dynamical
system simulation model and then to a long-lead United States (U.S.) soil
moisture forecasting application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McDermott_P/0/1/0/all/0/1&quot;&gt;Patrick L. McDermott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wikle_C/0/1/0/all/0/1&quot;&gt;Christopher K. Wikle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04427">
<title>Simultaneous Coherent Structure Coloring facilitates interpretable clustering of scientific data by amplifying dissimilarity. (arXiv:1807.04427v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.04427</link>
<description rdf:parseType="Literal">&lt;p&gt;The clustering of data into physically meaningful subsets often requires
assumptions regarding the number, size, or shape of the subgroups. Here, we
present a new method, simultaneous Coherent Structure Coloring (sCSC), which
accomplishes the task of unsupervised clustering without a priori guidance
regarding the underlying structure of the data. sCSC performs a sequence of
binary splittings on the dataset such that the most dissimilar data points are
required to be in separate clusters. To achieve this, we obtain a set of
orthogonal coordinates along which dissimilarity in the dataset is maximized
from a generalized eigenvalue problem based on the pairwise dissimilarity
between the data points to be clustered. This sequence of bifurcations produces
a binary tree representation of the system, from which the number of clusters
in the data and their interrelationships naturally emerge. To illustrate the
effectiveness of the method in the absence of a priori assumptions we apply it
to two exemplary problems in fluid dynamics. Then, we illustrate its capacity
for interpretability using a high-dimensional protein folding simulation
dataset. While we restrict our examples to dynamical physical systems in this
work, we anticipate straightforward translation to other fields where existing
analysis tools require ad hoc assumptions on the data structure, lack the
interpretability of the present method, or in which the underlying processes
are less accessible, such as genomics and neuroscience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Husic_B/0/1/0/all/0/1&quot;&gt;Brooke E. Husic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schlueter_Kuck_K/0/1/0/all/0/1&quot;&gt;Kristy L. Schlueter-Kuck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dabiri_J/0/1/0/all/0/1&quot;&gt;John O. Dabiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.05800">
<title>Deep Generative Model using Unregularized Score for Anomaly Detection with Heterogeneous Complexity. (arXiv:1807.05800v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.05800</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and automated detection of anomalous samples in a natural image
dataset can be accomplished with a probabilistic model for end-to-end modeling
of images. Such images have heterogeneous complexity, however, and a
probabilistic model overlooks simply shaped objects with small anomalies. This
is because the probabilistic model assigns undesirably lower likelihoods to
complexly shaped objects that are nevertheless consistent with set standards.
To overcome this difficulty, we propose an unregularized score for deep
generative models (DGMs), which are generative models leveraging deep neural
networks. We found that the regularization terms of the DGMs considerably
influence the anomaly score depending on the complexity of the samples. By
removing these terms, we obtain an unregularized score, which we evaluated on a
toy dataset and real-world manufacturing datasets. Empirical results
demonstrate that the unregularized score is robust to the inherent complexity
of samples and can be used to better detect anomalies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1&quot;&gt;Takashi Matsubara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hama_K/0/1/0/all/0/1&quot;&gt;Kenta Hama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tachibana_R/0/1/0/all/0/1&quot;&gt;Ryosuke Tachibana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1&quot;&gt;Kuniaki Uehara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.06093">
<title>Prognostics Estimations with Dynamic States. (arXiv:1807.06093v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.06093</link>
<description rdf:parseType="Literal">&lt;p&gt;The health state assessment and remaining useful life (RUL) estimation play
very important roles in prognostics and health management (PHM), owing to their
abilities to reduce the maintenance and improve the safety of machines or
equipment. However, they generally suffer from this problem of lacking prior
knowledge to pre-define the exact failure thresholds for a machinery operating
in a dynamic environment with a high level of uncertainty. In this case,
dynamic thresholds depicted by the discrete states is a very attractive way to
estimate the RUL of a dynamic machinery. Currently, there are only very few
works considering the dynamic thresholds, and these studies adopted different
algorithms to determine the discrete states and predict the continuous states
separately, which largely increases the complexity of the learning process. In
this paper, we propose a novel prognostics approach for RUL estimation of
aero-engines with self-joint prediction of continuous and discrete states,
wherein the prediction of continuous and discrete states are conducted
simultaneously and dynamically within one learning framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_R/0/1/0/all/0/1&quot;&gt;Rong-Jing Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_H/0/1/0/all/0/1&quot;&gt;Hai-Jun Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhi-Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Badong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07706">
<title>Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model. (arXiv:1807.07706v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.07706</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel framework that enables efficient probabilistic inference
in large-scale scientific models by allowing the execution of existing
domain-specific simulators as probabilistic programs, resulting in highly
interpretable posterior inference. Our framework is general purpose and
scalable, and is based on a cross-platform probabilistic execution protocol
through which an inference engine can control simulators in a language-agnostic
way. We demonstrate the technique in particle physics, on a scientifically
accurate simulation of the tau lepton decay, which is a key ingredient in
establishing the properties of the Higgs boson. High-energy physics has a rich
set of simulators based on quantum field theory and the interaction of
particles in matter. We show how to use probabilistic programming to perform
Bayesian inference in these existing simulator codebases directly, in
particular conditioning on observable outputs from a simulated particle
detector to directly produce an interpretable posterior distribution over decay
pathways. Inference efficiency is achieved via inference compilation where a
deep recurrent neural network is trained to parameterize proposal distributions
and control the stochastic simulator in a sequential importance sampling
scheme, at a fraction of the computational cost of Markov chain Monte Carlo
sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baydin_A/0/1/0/all/0/1&quot;&gt;Atilim Gunes Baydin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinrich_L/0/1/0/all/0/1&quot;&gt;Lukas Heinrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhimji_W/0/1/0/all/0/1&quot;&gt;Wahid Bhimji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gram_Hansen_B/0/1/0/all/0/1&quot;&gt;Bradley Gram-Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louppe_G/0/1/0/all/0/1&quot;&gt;Gilles Louppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Lei Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhat/0/1/0/all/0/1&quot;&gt;Prabhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cranmer_K/0/1/0/all/0/1&quot;&gt;Kyle Cranmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1&quot;&gt;Frank Wood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09586">
<title>Perturb and Combine to Identify Influential Spreaders in Real-World Networks. (arXiv:1807.09586v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1807.09586</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research has shown that graph degeneracy algorithms, which decompose a
network into a hierarchy of nested subgraphs of decreasing size and increasing
density, are very effective at detecting the good spreaders in a network.
However, it is also known that degeneracy-based decompositions of a graph are
unstable to small perturbations of the network structure. In Machine Learning,
the performance of unstable classification and regression methods, such as
fully-grown decision trees, can be greatly improved by using Perturb and
Combine (P&amp;amp;C) strategies such as bagging (bootstrap aggregating). Therefore, we
propose a P&amp;amp;C procedure for networks that (1) creates many perturbed versions
of a given graph, (2) applies a node scoring function separately to each graph
(such as a degeneracy-based one), and (3) combines the results. We conduct
real-world experiments on the tasks of identifying influential spreaders in
large social networks, and influential words (keywords) in small word
co-occurrence networks. We use the k-core, generalized k-core, and PageRank
algorithms as our vertex scoring functions. In each case, using the aggregated
scores brings significant improvements compared to using the scores computed on
the original graphs. Finally, a bias-variance analysis suggests that our P&amp;amp;C
procedure works mainly by reducing bias, and that therefore, it should be
capable of improving the performance of all vertex scoring functions, not only
unstable ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tixier_A/0/1/0/all/0/1&quot;&gt;Antoine J.-P. Tixier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_M/0/1/0/all/0/1&quot;&gt;Maria-Evgenia G. Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malliaros_F/0/1/0/all/0/1&quot;&gt;Fragkiskos D. Malliaros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1&quot;&gt;Jesse Read&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1&quot;&gt;Michalis Vazirgiannis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03230">
<title>Does Hamiltonian Monte Carlo mix faster than a random walk on multimodal densities?. (arXiv:1808.03230v2 [math.PR] UPDATED)</title>
<link>http://arxiv.org/abs/1808.03230</link>
<description rdf:parseType="Literal">&lt;p&gt;Hamiltonian Monte Carlo (HMC) is a very popular and generic collection of
Markov chain Monte Carlo (MCMC) algorithms. One explanation for the popularity
of HMC algorithms is their excellent performance as the dimension $d$ of the
target becomes large: under conditions that are satisfied for many common
statistical models, optimally-tuned HMC algorithms have a running time that
scales like $d^{0.25}$. In stark contrast, the running time of the usual
Random-Walk Metropolis (RWM) algorithm, optimally tuned, scales like $d$. This
superior scaling of the HMC algorithm with dimension is attributed to the fact
that it, unlike RWM, incorporates the gradient information in the proposal
distribution. In this paper, we investigate a different scaling question: does
HMC beat RWM for highly $\textit{multimodal}$ targets? We find that the answer
is often $\textit{no}$. We compute the spectral gaps for both the algorithms
for a specific class of multimodal target densities, and show that they are
identical. The key reason is that, within one mode, the gradient is effectively
ignorant about other modes, thus negating the advantage the HMC algorithm
enjoys in unimodal targets. We also give heuristic arguments suggesting that
the above observation may hold quite generally. Our main tool for answering
this question is a novel simple formula for the conductance of HMC using
Liouville&apos;s theorem. This result allows us to compute the spectral gap of HMC
algorithms, for both the classical HMC with isotropic momentum and the recent
Riemannian HMC, for multimodal targets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mangoubi_O/0/1/0/all/0/1&quot;&gt;Oren Mangoubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pillai_N/0/1/0/all/0/1&quot;&gt;Natesh S. Pillai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Smith_A/0/1/0/all/0/1&quot;&gt;Aaron Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06640">
<title>Adversarial Removal of Demographic Attributes from Text Data. (arXiv:1808.06640v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1808.06640</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in Representation Learning and Adversarial Training seem to
succeed in removing unwanted features from the learned representation. We show
that demographic information of authors is encoded in -- and can be recovered
from -- the intermediate representations learned by text-based neural
classifiers. The implication is that decisions of classifiers trained on
textual data are not agnostic to -- and likely condition on -- demographic
attributes. When attempting to remove such demographic information using
adversarial training, we find that while the adversarial component achieves
chance-level development-set accuracy during training, a post-hoc classifier,
trained on the encoded sentences from the first part, still manages to reach
substantially higher classification accuracies on the same data. This behavior
is consistent across several tasks, demographic properties and datasets. We
explore several techniques to improve the effectiveness of the adversarial
component. Our main conclusion is a cautionary one: do not rely on the
adversarial training to achieve invariant representation to sensitive features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1&quot;&gt;Yanai Elazar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1&quot;&gt;Yoav Goldberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.09964">
<title>Semi-Metrification of the Dynamic Time Warping Distance. (arXiv:1808.09964v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.09964</link>
<description rdf:parseType="Literal">&lt;p&gt;The dynamic time warping (dtw) distance fails to satisfy the triangle
inequality and the identity of indiscernibles. As a consequence, the
dtw-distance is not warping-invariant, which in turn results in peculiarities
in data mining applications. This article converts the dtw-distance to a
semi-metric and shows that its canonical extension is warping-invariant.
Empirical results indicate that the nearest-neighbor classifier in the proposed
semi-metric space performs comparably to the same classifier in the standard
dtw-space. To overcome the undesirable peculiarities of dtw-spaces, this result
suggests to further explore the semi-metric space for data mining applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_B/0/1/0/all/0/1&quot;&gt;Brijnesh J. Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10101">
<title>DP-ADMM: ADMM-based Distributed Learning with Differential Privacy. (arXiv:1808.10101v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1808.10101</link>
<description rdf:parseType="Literal">&lt;p&gt;Privacy-preserving distributed machine learning has become more important
than ever due to the high demand of large-scale data processing. This paper
focuses on a class of machine learning problems that can be formulated as
regularized empirical risk minimization, and develops a privacy-preserving
approach to such learning problems. We use Alternating Direction Method of
Multipliers (ADMM) to decentralize the learning algorithm, and apply Gaussian
mechanisms to provide local differential privacy guarantee. However, simply
combining ADMM and local randomization mechanisms would result in a
nonconvergent algorithm with bad performance even under moderate privacy
guarantees. Besides, this approach cannot be applied when the objective
functions of the learning problems are non-smooth. To address these concerns,
we propose an improved ADMM-based Differentially Private distributed learning
algorithm, DP-ADMM, where an approximate augmented Lagrangian function and
Gaussian mechanisms with time-varying variance are utilized. We also apply the
moment accountant method to bound the total privacy loss. Our theoretical
analysis shows that DP-ADMM can be applied to convex learning problems with
both smooth and non-smooth objectives, provides differential privacy guarantee,
and achieves a convergence rate of $O(1/\sqrt{t})$, where $t$ is the number of
iterations. Our evaluations demonstrate that our approach can achieve good
convergence and accuracy with strong privacy guarantee.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zonghao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Rui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_Tin_E/0/1/0/all/0/1&quot;&gt;Eric Chan-Tin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yanmin Gong&lt;/a&gt;</dc:creator>
</item></rdf:RDF>