<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-15T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05300"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05324"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05448"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05480"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05686"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05698"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.09232"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.07128"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04899"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05313"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05340"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05382"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05383"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05385"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05472"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05639"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.00848"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06294"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00420"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04205"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05141"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05315"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05355"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05370"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05380"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05392"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05400"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05408"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05429"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05447"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05451"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05584"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05622"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05637"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05664"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05680"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05688"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05693"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05695"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1604.04706"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.09499"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.05008"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.06176"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.06066"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.10295"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.07113"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.10433"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04826"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04956"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.05155"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.01166"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.05300">
<title>Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise. (arXiv:1802.05300v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.05300</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing importance of massive datasets with the advent of deep learning
makes robustness to label noise a critical property for classifiers to have.
Sources of label noise include automatic labeling for large datasets,
non-expert labeling, and label corruption by data poisoning adversaries. In the
latter case, corruptions may be arbitrarily bad, even so bad that a classifier
predicts the wrong labels with high confidence. To protect against such sources
of noise, we leverage the fact that a small set of clean labels is often easy
to procure. We demonstrate that robustness to label noise up to severe
strengths can be achieved by using a set of trusted data with clean labels, and
propose a loss correction that utilizes trusted examples in a data-efficient
manner to mitigate the effects of label noise on deep neural network
classifiers. Across vision and natural language processing tasks, we experiment
with various label noises at several strengths, and show that our method
significantly outperforms existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1&quot;&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1&quot;&gt;Mantas Mazeika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1&quot;&gt;Duncan Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1&quot;&gt;Kevin Gimpel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05324">
<title>Advancing System Performance with Redundancy: From Biological to Artificial Designs. (arXiv:1802.05324v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1802.05324</link>
<description rdf:parseType="Literal">&lt;p&gt;Redundancy is a fundamental characteristic of many biological processes such
as those in the genetic, visual, muscular and nervous system; yet its function
has not been fully understood. The conventional interpretation of redundancy is
that it serves as a fault-tolerance mechanism, which leads to redundancy&apos;s de
facto application in man-made systems for reliability enhancement. On the
contrary, our previous works have demonstrated an example where redundancy can
be engineered solely for enhancing other aspects of the system, namely accuracy
and precision. This design was inspired by the binocular structure of the human
vision which we believe may share a similar operation. In this paper, we
present a unified theory describing how such utilization of redundancy is
feasible through two complementary mechanisms: representational redundancy
(RPR) and entangled redundancy (ETR). Besides the previous works, we point out
two additional examples where our new understanding of redundancy can be
applied to justify a system&apos;s superior performance. One is the human
musculoskeletal system (HMS) - a biological instance, and one is the deep
residual neural network (ResNet) - an artificial counterpart. We envision that
our theory would provide a framework for the future development of bio-inspired
redundant artificial systems as well as assist the studies of the fundamental
mechanisms governing various biological processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Tuan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luu_D/0/1/0/all/0/1&quot;&gt;Diu Khue Luu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05448">
<title>Discrepancy-based Evolutionary Diversity Optimization. (arXiv:1802.05448v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.05448</link>
<description rdf:parseType="Literal">&lt;p&gt;Diversity plays a crucial role in evolutionary computation. While diversity
has been mainly used to prevent the population of an evolutionary algorithm
from premature convergence, the use of evolutionary algorithms to obtain a
diverse set of solutions has gained increasing attention in recent years.
Diversity optimization in terms of features on the underlying problem allows to
obtain a better understanding of possible solutions to the problem at hand and
can be used for algorithm selection when dealing with combinatorial
optimization problems such as the Traveling Salesperson Problem. We explore the
use of the star-discrepancy measure to guide the diversity optimization process
of an evolutionary algorithm.
&lt;/p&gt;
&lt;p&gt;In our experimental investigations, we consider our discrepancy-based
diversity optimization approaches for evolving diverse sets of images as well
as instances of the Traveling Salesperson problem where a local search is not
able to find near optimal solutions. Our experimental investigations comparing
three diversity optimization approaches show that a discrepancy-based diversity
optimization approach using a tie-breaking rule based on weighted differences
to surrounding feature points provides the best results in terms of the star
discrepancy measure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_A/0/1/0/all/0/1&quot;&gt;Aneta Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wanru Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_C/0/1/0/all/0/1&quot;&gt;Carola Doerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_F/0/1/0/all/0/1&quot;&gt;Frank Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1&quot;&gt;Markus Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05480">
<title>Evolution of Images with Diversity and Constraints Using a Generator Network. (arXiv:1802.05480v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.05480</link>
<description rdf:parseType="Literal">&lt;p&gt;Evolutionary search has been extensively used to generate artistic images.
Raw images have high dimensionality which makes a direct search for an image
challenging. In previous work this problem has been addressed by using compact
symbolic encodings or by constraining images with priors. Recent developments
in deep learning have enabled a generation of compelling artistic images using
generative networks that encode images with lower-dimensional latent spaces. To
date this work has focused on the generation of images concordant with one or
more classes and transfer of artistic styles. There is currently no work which
uses search in this latent space to generate images scoring high or low
aesthetic measures. In this paper we use evolutionary methods to search for
images in two datasets, faces and butterflies, and demonstrate the effect of
optimising aesthetic feature scores in one or two dimensions. The work gives a
preliminary indication of which feature measures promote the most interesting
images and how some of these measures interact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_A/0/1/0/all/0/1&quot;&gt;Aneta Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pyromallis_C/0/1/0/all/0/1&quot;&gt;Christo Pyromallis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_B/0/1/0/all/0/1&quot;&gt;Bradley Alexander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05686">
<title>A Bio-inspired Redundant Sensing Architecture. (arXiv:1802.05686v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.05686</link>
<description rdf:parseType="Literal">&lt;p&gt;Sensing is the process of deriving signals from the environment that allows
artificial systems to interact with the physical world. The Shannon theorem
specifies the maximum rate at which information can be acquired. However, this
upper bound is hard to achieve in many man-made systems. The biological visual
systems, on the other hand, have highly efficient signal representation and
processing mechanisms that allow precise sensing. In this work, we argue that
redundancy is one of the critical characteristics for such superior
performance. We show architectural advantages by utilizing redundant sensing,
including correction of mismatch error and significant precision enhancement.
For a proof-of-concept demonstration, we have designed a heuristic-based
analog-to-digital converter - a zero-dimensional quantizer. Through Monte Carlo
simulation with the error probabilistic distribution as a priori, the
performance approaching the Shannon limit is feasible. In actual measurements
without knowing the error distribution, we observe at least 2-bit extra
precision. The results may also help explain biological processes including the
dominance of binocular vision, the functional roles of the fixational eye
movements, and the structural mechanisms allowing hyperacuity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Tuan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05698">
<title>A Machine Learning Approach for Virtual Flow Metering and Forecasting. (arXiv:1802.05698v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.05698</link>
<description rdf:parseType="Literal">&lt;p&gt;We are concerned with robust and accurate forecasting of multiphase flow
rates in wells and pipelines during oil and gas production. In practice, the
possibility to physically measure the rates is often limited; besides, it is
desirable to estimate future values of multiphase rates based on the previous
behavior of the system. In this work, we demonstrate that a Long Short-Term
Memory (LSTM) recurrent artificial network is able not only to accurately
estimate the multiphase rates at current time (i.e., act as a virtual flow
meter), but also to forecast the rates for a sequence of future time instants.
For a synthetic severe slugging case, LSTM forecasts compare favorably with the
results of hydrodynamical modeling. LSTM results for a realistic noizy dataset
of a variable rate well test show that the model can also successfully forecast
multiphase rates for a system with changing flow patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrianov_N/0/1/0/all/0/1&quot;&gt;Nikolai Andrianov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.09232">
<title>Tradeoffs between Convergence Speed and Reconstruction Accuracy in Inverse Problems. (arXiv:1605.09232v3 [cs.NA] UPDATED)</title>
<link>http://arxiv.org/abs/1605.09232</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving inverse problems with iterative algorithms is popular, especially for
large data. Due to time constraints, the number of possible iterations is
usually limited, potentially affecting the achievable accuracy. Given an error
one is willing to tolerate, an important question is whether it is possible to
modify the original iterations to obtain faster convergence to a minimizer
achieving the allowed error without increasing the computational cost of each
iteration considerably. Relying on recent recovery techniques developed for
settings in which the desired signal belongs to some low-dimensional set, we
show that using a coarse estimate of this set may lead to faster convergence at
the cost of an additional reconstruction error related to the accuracy of the
set approximation. Our theory ties to recent advances in sparse recovery,
compressed sensing, and deep learning. Particularly, it may provide a possible
explanation to the successful approximation of the l1-minimization solution by
neural networks with layers representing iterations, as practiced in the
learned iterative shrinkage-thresholding algorithm (LISTA).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1&quot;&gt;Raja Giryes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1&quot;&gt;Yonina C. Eldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1&quot;&gt;Alex M. Bronstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapiro_G/0/1/0/all/0/1&quot;&gt;Guillermo Sapiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.07128">
<title>Hello Edge: Keyword Spotting on Microcontrollers. (arXiv:1711.07128v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/1711.07128</link>
<description rdf:parseType="Literal">&lt;p&gt;Keyword spotting (KWS) is a critical component for enabling speech based user
interactions on smart devices. It requires real-time response and high accuracy
for good user experience. Recently, neural networks have become an attractive
choice for KWS architecture because of their superior accuracy compared to
traditional speech processing algorithms. Due to its always-on nature, KWS
application has highly constrained power budget and typically runs on tiny
microcontrollers with limited memory and compute capability. The design of
neural network architecture for KWS must consider these constraints. In this
work, we perform neural network architecture evaluation and exploration for
running KWS on resource-constrained microcontrollers. We train various neural
network architectures for keyword spotting published in literature to compare
their accuracy and memory/compute requirements. We show that it is possible to
optimize these neural network architectures to fit within the memory and
compute constraints of microcontrollers without sacrificing accuracy. We
further explore the depthwise separable convolutional neural network (DS-CNN)
and compare it against other neural network architectures. DS-CNN achieves an
accuracy of 95.4%, which is ~10% higher than the DNN model with similar number
of parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yundong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suda_N/0/1/0/all/0/1&quot;&gt;Naveen Suda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1&quot;&gt;Liangzhen Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1&quot;&gt;Vikas Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04899">
<title>Field-Programmable Deep Neural Network (DNN) Learning and Inference accelerator: a concept. (arXiv:1802.04899v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04899</link>
<description rdf:parseType="Literal">&lt;p&gt;An accelerator is a specialized integrated circuit designed to perform
specific computations faster than if those were performed by CPU or GPU. A
Field-Programmable DNN learning and inference accelerator (FProg-DNN) using
hybrid systolic and non-systolic techniques, distributed information-control
and deep pipelined structure is proposed and its microarchitecture and
operation presented here. Reconfigurability attends diverse DNN designs and
allows for different number of workers to be assigned to different layers as a
function of the relative difference in computational load among layers. The
computational delay per layer is made roughly the same along pipelined
accelerator structure. VGG-16 and recently proposed Inception Modules are used
for showing the flexibility of the FProg-DNN reconfigurability. Special
structures were also added for a combination of convolution layer, map
coincidence and feedback for state of the art learning with small set of
examples, which is the focus of a companion paper by the author (Franca-Neto,
2018). The accelerator described is able to reconfigure from (1) allocating all
a DNN computations to a single worker in one extreme of sub-optimal performance
to (2) optimally allocating workers per layer according to computational load
in each DNN layer to be realized. Due the pipelined architecture, more than 50x
speedup is achieved relative to GPUs or TPUs. This speed-up is consequence of
hiding the delay in transporting activation outputs from one layer to the next
in a DNN behind the computations in the receiving layer. This FProg-DNN concept
has been simulated and validated at behavioral-functional level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franca_Neto_L/0/1/0/all/0/1&quot;&gt;Luiz M Franca-Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05313">
<title>Reinforcement Learning from Imperfect Demonstrations. (arXiv:1802.05313v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.05313</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust real-world learning should benefit from both demonstrations and
interactions with the environment. Current approaches to learning from
demonstration and reward perform supervised learning on expert demonstration
data and use reinforcement learning to further improve performance based on the
reward received from the environment. These tasks have divergent losses which
are difficult to jointly optimize and such methods can be very sensitive to
noisy demonstrations. We propose a unified reinforcement learning algorithm,
Normalized Actor-Critic (NAC), that effectively normalizes the Q-function,
reducing the Q-values of actions unseen in the demonstration data. NAC learns
an initial policy network from demonstrations and refines the policy in the
environment, surpassing the demonstrator&apos;s performance. Crucially, both
learning from demonstration and interactive refinement use the same objective,
unlike prior approaches that combine distinct supervised and reinforcement
losses. This makes NAC robust to suboptimal demonstration data since the method
is not forced to mimic all of the examples in the dataset. We show that our
unified reinforcement learning algorithm can learn robustly and outperform
existing baselines when evaluated on several realistic driving games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huazhe/0/1/0/all/0/1&quot;&gt;Huazhe&lt;/a&gt; (Harry)Xu, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Ji Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fisher Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05340">
<title>From Gameplay to Symbolic Reasoning: Learning SAT Solver Heuristics in the Style of Alpha(Go) Zero. (arXiv:1802.05340v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.05340</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent successes of deep neural networks in various fields such
as image and speech recognition, natural language processing, and reinforcement
learning, we still face big challenges in bringing the power of numeric
optimization to symbolic reasoning. Researchers have proposed different avenues
such as neural machine translation for proof synthesis, vectorization of
symbols and expressions for representing symbolic patterns, and coupling of
neural back-ends for dimensionality reduction with symbolic front-ends for
decision making. However, these initial explorations are still only point
solutions, and bear other shortcomings such as lack of correctness guarantees.
In this paper, we present our approach of casting symbolic reasoning as games,
and directly harnessing the power of deep reinforcement learning in the style
of Alpha(Go) Zero on symbolic problems. Using the Boolean Satisfiability (SAT)
problem as showcase, we demonstrate the feasibility of our method, and the
advantages of modularity, efficiency, and correctness guarantees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rompf_T/0/1/0/all/0/1&quot;&gt;Tiark Rompf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05382">
<title>Value-Aware Item Weighting for Long-Tail Recommendation. (arXiv:1802.05382v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1802.05382</link>
<description rdf:parseType="Literal">&lt;p&gt;Many recommender systems suffer from the popularity bias problem: popular
items are being recommended frequently while less popular, niche products, are
recommended rarely if not at all. However, those ignored products are exactly
the products that businesses need to find customers for and their
recommendations would be more beneficial. In this paper, we examine an item
weighting approach to improve long-tail recommendation. Our approach works as a
simple yet powerful add-on to existing recommendation algorithms for making a
tunable trade-off between accuracy and long-tail coverage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdollahpouri_H/0/1/0/all/0/1&quot;&gt;Himan Abdollahpouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burke_R/0/1/0/all/0/1&quot;&gt;Robin Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mobasher_B/0/1/0/all/0/1&quot;&gt;Bamshad Mobasher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05383">
<title>Deep Learning Based Speech Beamforming. (arXiv:1802.05383v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1802.05383</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-channel speech enhancement with ad-hoc sensors has been a challenging
task. Speech model guided beamforming algorithms are able to recover natural
sounding speech, but the speech models tend to be oversimplified or the
inference would otherwise be too complicated. On the other hand, deep learning
based enhancement approaches are able to learn complicated speech distributions
and perform efficient inference, but they are unable to deal with variable
number of input channels. Also, deep learning approaches introduce a lot of
errors, particularly in the presence of unseen noise types and settings. We
have therefore proposed an enhancement framework called DEEPBEAM, which
combines the two complementary classes of algorithms. DEEPBEAM introduces a
beamforming filter to produce natural sounding speech, but the filter
coefficients are determined with the help of a monaural speech enhancement
neural network. Experiments on synthetic and real-world data show that DEEPBEAM
is able to produce clean, dry and natural sounding speech, and is robust
against unseen noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1&quot;&gt;Kaizhi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shiyu Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xuesong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1&quot;&gt;Dinei Florencio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1&quot;&gt;Mark Hasegawa-Johnson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05385">
<title>Fooling OCR Systems with Adversarial Text Images. (arXiv:1802.05385v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.05385</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate that state-of-the-art optical character recognition (OCR)
based on deep learning is vulnerable to adversarial images. Minor modifications
to images of printed text, which do not change the meaning of the text to a
human reader, cause the OCR system to &quot;recognize&quot; a different text where
certain words chosen by the adversary are replaced by their semantic opposites.
This completely changes the meaning of the output produced by the OCR system
and by the NLP applications that use OCR for preprocessing their inputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Congzheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1&quot;&gt;Vitaly Shmatikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05472">
<title>Admissible Time Series Motif Discovery with Missing Data. (arXiv:1802.05472v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.05472</link>
<description rdf:parseType="Literal">&lt;p&gt;The discovery of time series motifs has emerged as one of the most useful
primitives in time series data mining. Researchers have shown its utility for
exploratory data mining, summarization, visualization, segmentation,
classification, clustering, and rule discovery. Although there has been more
than a decade of extensive research, there is still no technique to allow the
discovery of time series motifs in the presence of missing data, despite the
well-documented ubiquity of missing data in scientific, industrial, and medical
datasets. In this work, we introduce a technique for motif discovery in the
presence of missing data. We formally prove that our method is admissible,
producing no false negatives. We also show that our method can piggy-back off
the fastest known motif discovery method with a small constant factor
time/space overhead. We will demonstrate our approach on diverse datasets with
varying amounts of missing data
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueen_A/0/1/0/all/0/1&quot;&gt;Abdullah Mueen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keogh_E/0/1/0/all/0/1&quot;&gt;Eamonn Keogh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05639">
<title>Reliable Uncertain Evidence Modeling in Bayesian Networks by Credal Networks. (arXiv:1802.05639v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.05639</link>
<description rdf:parseType="Literal">&lt;p&gt;A reliable modeling of uncertain evidence in Bayesian networks based on a
set-valued quantification is proposed. Both soft and virtual evidences are
considered. We show that evidence propagation in this setup can be reduced to
standard updating in an augmented credal network, equivalent to a set of
consistent Bayesian networks. A characterization of the computational
complexity for this task is derived together with an efficient exact procedure
for a subclass of instances. In the case of multiple uncertain evidences over
the same variable, the proposed procedure can provide a set-valued version of
the geometric approach to opinion pooling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchetti_S/0/1/0/all/0/1&quot;&gt;Sabina Marchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antonucci_A/0/1/0/all/0/1&quot;&gt;Alessandro Antonucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.00848">
<title>Unsupervised Image-to-Image Translation Networks. (arXiv:1703.00848v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1703.00848</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised image-to-image translation aims at learning a joint distribution
of images in different domains by using images from the marginal distributions
in individual domains. Since there exists an infinite set of joint
distributions that can arrive the given marginal distributions, one could infer
nothing about the joint distribution from the marginal distributions without
additional assumptions. To address the problem, we make a shared-latent space
assumption and propose an unsupervised image-to-image translation framework
based on Coupled GANs. We compare the proposed framework with competing
approaches and present high quality image translation results on various
challenging unsupervised image translation tasks, including street scene image
translation, animal image translation, and face image translation. We also
apply the proposed framework to domain adaptation and achieve state-of-the-art
performance on benchmark datasets. Code and additional results are available in
https://github.com/mingyuliutw/unit .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming-Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1&quot;&gt;Thomas Breuel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1&quot;&gt;Jan Kautz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06294">
<title>Multi-Task Pharmacovigilance Mining from Social Media Posts. (arXiv:1801.06294v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06294</link>
<description rdf:parseType="Literal">&lt;p&gt;Social media has grown to be a crucial information source for
pharmacovigilance studies where an increasing number of people post adverse
reactions to medical drugs that are previously unreported. Aiming to
effectively monitor various aspects of Adverse Drug Reactions (ADRs) from
diversely expressed social medical posts, we propose a multi-task neural
network framework that learns several tasks associated with ADR monitoring with
different levels of supervisions collectively. Besides being able to correctly
classify ADR posts and accurately extract ADR mentions from online posts, the
proposed framework is also able to further understand reasons for which the
drug is being taken, known as &apos;indication&apos;, from the given social media post. A
coverage-based attention mechanism is adopted in our framework to help the
model properly identify &apos;phrasal&apos; ADRs and Indications that are attentive to
multiple words in a post. Our framework is applicable in situations where
limited parallel data for different pharmacovigilance tasks are available.We
evaluate the proposed framework on real-world Twitter datasets, where the
proposed model outperforms the state-of-the-art alternatives of each individual
task consistently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1&quot;&gt;Shaika Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00420">
<title>Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. (arXiv:1802.00420v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.00420</link>
<description rdf:parseType="Literal">&lt;p&gt;We identify obfuscated gradients, a kind of gradient masking, as a phenomenon
that leads to a false sense of security in defenses against adversarial
examples. While defenses that cause obfuscated gradients appear to defeat
iterative optimization-based attacks, we find defenses relying on this effect
can be circumvented. For each of the three types of obfuscated gradients we
discover, we describe characteristic behaviors of defenses exhibiting this
effect and develop attack techniques to overcome it. In a case study, examining
non-certified white-box-secure defenses at ICLR 2018, we find obfuscated
gradients are a common occurrence, with 7 of 8 defenses relying on obfuscated
gradients. Our new attacks successfully circumvent 6 completely and 1
partially.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athalye_A/0/1/0/all/0/1&quot;&gt;Anish Athalye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1&quot;&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1&quot;&gt;David Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04205">
<title>Efficient Hierarchical Robot Motion Planning Under Uncertainty and Hybrid Dynamics. (arXiv:1802.04205v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04205</link>
<description rdf:parseType="Literal">&lt;p&gt;Noisy observations coupled with nonlinear dynamics pose one of the biggest
challenges in robot motion planning. By decomposing the nonlinear dynamics into
a discrete set of local dynamics models, hybrid dynamics provide a natural way
to model nonlinear dynamics, especially in systems with sudden &quot;jumps&quot; in the
dynamics, due to factors such as contacts. We propose a hierarchical POMDP
planner that develops locally optimal motion plans for hybrid dynamics models.
The hierarchical planner first develops a high-level motion plan to sequence
the local dynamics models to be visited. The high-level plan is then converted
into a detailed cost-optimized continuous state plan. This hierarchical
planning approach results in a decomposition of the POMDP planning problem into
smaller sub-parts that can be solved with significantly lower computational
costs. The ability to sequence the visitation of local dynamics models also
provides a powerful way to leverage the hybrid dynamics to reduce state
uncertainty. We evaluate the proposed planner for two navigation and
localization tasks in simulated domains, as well as an assembly task with a
real robotic manipulator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Ajinkya Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1&quot;&gt;Scott Niekum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05141">
<title>Deep Learning and Data Assimilation for Real-Time Production Prediction in Natural Gas Wells. (arXiv:1802.05141v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05141</link>
<description rdf:parseType="Literal">&lt;p&gt;The prediction of the gas production from mature gas wells, due to their
complex end-of-life behavior, is challenging and crucial for operational
decision making. In this paper, we apply a modified deep LSTM model for
prediction of the gas flow rates in mature gas wells, including the
uncertainties in input parameters. Additionally, due to changes in the system
in time and in order to increase the accuracy and robustness of the prediction,
the Ensemble Kalman Filter (EnKF) is used to update the flow rate predictions
based on new observations. The developed approach was tested on the data from
two mature gas production wells in which their production is highly dynamic and
suffering from salt deposition. The results show that the flow predictions
using the EnKF updated model leads to better Jeffreys&apos; J-divergences than the
predictions without the EnKF model updating scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loh_K/0/1/0/all/0/1&quot;&gt;Kelvin Loh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omrani_P/0/1/0/all/0/1&quot;&gt;Pejman Shoeibi Omrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linden_R/0/1/0/all/0/1&quot;&gt;Ruud van der Linden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05315">
<title>Online Learning for Non-Stationary A/B Tests. (arXiv:1802.05315v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.05315</link>
<description rdf:parseType="Literal">&lt;p&gt;The rollout of new versions of a feature in modern applications is a manual
multi-stage process, as the feature is released to ever larger groups of users,
while its performance is carefully monitored. This kind of A/B testing is
ubiquitous, but suboptimal, as the monitoring requires heavy human
intervention, is not guaranteed to capture consistent, but short-term
fluctuations in performance, and is inefficient, as better versions take a long
time to reach the full population.
&lt;/p&gt;
&lt;p&gt;In this work we formulate this question as that of expert learning, and give
a new algorithm Follow-The-Best-Interval, FTBI, that works in dynamic,
non-stationary environments. Our approach is practical, simple, and efficient,
and has rigorous guarantees on its performance. Finally, we perform a thorough
evaluation on synthetic and real world datasets and show that our approach
outperforms current state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Medina_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Mu&amp;#xf1;oz Medina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vassilvitskii_S/0/1/0/all/0/1&quot;&gt;Sergei Vassilvitskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dong Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05355">
<title>The Role of Information Complexity and Randomization in Representation Learning. (arXiv:1802.05355v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.05355</link>
<description rdf:parseType="Literal">&lt;p&gt;A grand challenge in representation learning is to learn the different
explanatory factors of variation behind the high dimen- sional data. Encoder
models are often determined to optimize performance on training data when the
real objective is to generalize well to unseen data. Although there is enough
numerical evidence suggesting that noise injection (during training) at the
representation level might improve the generalization ability of encoders, an
information-theoretic understanding of this principle remains elusive. This
paper presents a sample-dependent bound on the generalization gap of the
cross-entropy loss that scales with the information complexity (IC) of the
representations, meaning the mutual information between inputs and their
representations. The IC is empirically investigated for standard multi-layer
neural networks with SGD on MNIST and CIFAR-10 datasets; the behaviour of the
gap and the IC appear to be in direct correlation, suggesting that SGD selects
encoders to implicitly minimize the IC. We specialize the IC to study the role
of Dropout on the generalization capacity of deep encoders which is shown to be
directly related to the encoder capacity, being a measure of the
distinguishability among samples from their representations. Our results
support some recent regularization methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vera_M/0/1/0/all/0/1&quot;&gt;Mat&amp;#xed;as Vera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Piantanida_P/0/1/0/all/0/1&quot;&gt;Pablo Piantanida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vega_L/0/1/0/all/0/1&quot;&gt;Leonardo Rey Vega&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05370">
<title>Covariance Function Pre-Training with m-Kernels for Accelerated Bayesian Optimisation. (arXiv:1802.05370v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.05370</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper presents a novel approach to direct covariance function learning
for Bayesian optimisation, with particular emphasis on experimental design
problems where an existing corpus of condensed knowledge is present. The method
presented borrows techniques from reproducing kernel Banach space theory
(specifically m-kernels) and leverages them to convert (or re-weight) existing
covariance functions into new, problem-specific covariance functions. The key
advantage of this approach is that rather than relying on the user to manually
select (with some hyperparameter tuning and experimentation) an appropriate
covariance function it constructs the covariance function to specifically match
the problem at hand. The technique is demonstrated on two real-world problems -
specifically alloy design and carbon-fibre manufacturing - as well as a
selected test function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shilton_A/0/1/0/all/0/1&quot;&gt;Alistair Shilton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Sunil Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rana_S/0/1/0/all/0/1&quot;&gt;Santu Rana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vellanki_P/0/1/0/all/0/1&quot;&gt;Pratibha Vellanki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Park_L/0/1/0/all/0/1&quot;&gt;Laurence Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1&quot;&gt;Svetha Venkatesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sutti_A/0/1/0/all/0/1&quot;&gt;Alessandra Sutti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rubin_D/0/1/0/all/0/1&quot;&gt;David Rubin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dorin_T/0/1/0/all/0/1&quot;&gt;Thomas Dorin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vahid_A/0/1/0/all/0/1&quot;&gt;Alireza Vahid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Height_M/0/1/0/all/0/1&quot;&gt;Murray Height&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05374">
<title>A Progressive Batching L-BFGS Method for Machine Learning. (arXiv:1802.05374v1 [math.OC])</title>
<link>http://arxiv.org/abs/1802.05374</link>
<description rdf:parseType="Literal">&lt;p&gt;The standard L-BFGS method relies on gradient approximations that are not
dominated by noise, so that search directions are descent directions, the line
search is reliable, and quasi-Newton updating yields useful quadratic models of
the objective function. All of this appears to call for a full batch approach,
but since small batch sizes give rise to faster algorithms with better
generalization properties, L-BFGS is currently not considered an algorithm of
choice for large-scale machine learning applications. One need not, however,
choose between the two extremes represented by the full batch or highly
stochastic regimes, and may instead follow a progressive batching approach in
which the sample size increases during the course of the optimization. In this
paper, we present a new version of the L-BFGS algorithm that combines three
basic components - progressive batching, a stochastic line search, and stable
quasi-Newton updating - and that performs well on training logistic regression
and deep neural networks. We provide supporting convergence theory for the
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bollapragada_R/0/1/0/all/0/1&quot;&gt;Raghu Bollapragada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mudigere_D/0/1/0/all/0/1&quot;&gt;Dheevatsa Mudigere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nocedal_J/0/1/0/all/0/1&quot;&gt;Jorge Nocedal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hao-Jun Michael Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tang_P/0/1/0/all/0/1&quot;&gt;Ping Tak Peter Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05380">
<title>Active Feature Acquisition with Supervised Matrix Completion. (arXiv:1802.05380v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.05380</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature missing is a serious problem in many applications, which may lead to
low quality of training data and further significantly degrade the learning
performance. While feature acquisition usually involves special devices or
complex process, it is expensive to acquire all feature values for the whole
dataset. On the other hand, features may be correlated with each other, and
some values may be recovered from the others. It is thus important to decide
which features are most informative for recovering the other features as well
as improving the learning performance. In this paper, we try to train an
effective classification model with least acquisition cost by jointly
performing active feature querying and supervised matrix completion. When
completing the feature matrix, a novel target function is proposed to
simultaneously minimize the reconstruction error on observed entries and the
supervised loss on training data. When querying the feature value, the most
uncertain entry is actively selected based on the variance of previous
iterations. In addition, a bi-objective optimization method is presented for
cost-aware active selection when features bear different acquisition costs. The
effectiveness of the proposed approach is well validated by both theoretical
analysis and experimental study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Sheng-Jun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Miao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1&quot;&gt;Ming-Kun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1&quot;&gt;Gang Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Songcan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05386">
<title>Shamap: Shape-based Manifold Learning. (arXiv:1802.05386v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.05386</link>
<description rdf:parseType="Literal">&lt;p&gt;For manifold learning, it is assumed that high-dimensional sample/data points
are on an embedded low-dimensional manifold. Usually, distances among samples
are computed to represent the underlying data structure, for a specified
distance measure such as the Euclidean distance or geodesic distance. For
manifold learning, here we propose a metric according to the angular change
along a geodesic line, thereby reflecting the underlying shape-oriented
information or the similarity between high- and low-dimensional representations
of a data cloud. Our numerical results are described to demonstrate the
feasibility and merits of the proposed dimensionality reduction scheme
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Fenglei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1&quot;&gt;Hongming Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Ge Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05392">
<title>Reducing over-clustering via the powered Chinese restaurant process. (arXiv:1802.05392v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.05392</link>
<description rdf:parseType="Literal">&lt;p&gt;Dirichlet process mixture (DPM) models tend to produce many small clusters
regardless of whether they are needed to accurately characterize the data -
this is particularly true for large data sets. However, interpretability,
parsimony, data storage and communication costs all are hampered by having
overly many clusters. We propose a powered Chinese restaurant process to limit
this kind of problem and penalize over clustering. The method is illustrated
using some simulation examples and data with large and small sample size
including MNIST and the Old Faithful Geyser data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jun Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Meng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunson_D/0/1/0/all/0/1&quot;&gt;David Dunson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05400">
<title>High Dimensional Bayesian Optimization Using Dropout. (arXiv:1802.05400v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.05400</link>
<description rdf:parseType="Literal">&lt;p&gt;Scaling Bayesian optimization to high dimensions is challenging task as the
global optimization of high-dimensional acquisition function can be expensive
and often infeasible. Existing methods depend either on limited active
variables or the additive form of the objective function. We propose a new
method for high-dimensional Bayesian optimization, that uses a dropout strategy
to optimize only a subset of variables at each iteration. We derive theoretical
bounds for the regret and show how it can inform the derivation of our
algorithm. We demonstrate the efficacy of our algorithms for optimization on
two benchmark functions and two real-world applications- training cascade
classifiers and optimizing alloy composition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Sunil Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rana_S/0/1/0/all/0/1&quot;&gt;Santu Rana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Vu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1&quot;&gt;Svetha Venkatesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shilton_A/0/1/0/all/0/1&quot;&gt;Alistair Shilton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05408">
<title>&quot;Dependency Bottleneck&quot; in Auto-encoding Architectures: an Empirical Study. (arXiv:1802.05408v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1802.05408</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works investigated the generalization properties in deep neural
networks (DNNs) by studying the Information Bottleneck in DNNs. However, the
mea- surement of the mutual information (MI) is often inaccurate due to the
density estimation. To address this issue, we propose to measure the dependency
instead of MI between layers in DNNs. Specifically, we propose to use
Hilbert-Schmidt Independence Criterion (HSIC) as the dependency measure, which
can measure the dependence of two random variables without estimating
probability densities. Moreover, HSIC is a special case of the Squared-loss
Mutual Information (SMI). In the experiment, we empirically evaluate the
generalization property using HSIC in both the reconstruction and prediction
auto-encoding (AE) architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Denny Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yixiu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1&quot;&gt;Yao-Hung Hubert Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1&quot;&gt;Makoto Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05429">
<title>Blind Source Separation with Optimal Transport Non-negative Matrix Factorization. (arXiv:1802.05429v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1802.05429</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimal transport as a loss for machine learning optimization problems has
recently gained a lot of attention. Building upon recent advances in
computational optimal transport, we develop an optimal transport non-negative
matrix factorization (NMF) algorithm for supervised speech blind source
separation (BSS). Optimal transport allows us to design and leverage a cost
between short-time Fourier transform (STFT) spectrogram frequencies, which
takes into account how humans perceive sound. We give empirical evidence that
using our proposed optimal transport NMF leads to perceptually better results
than Euclidean NMF, for both isolated voice reconstruction and BSS tasks.
Finally, we demonstrate how to use optimal transport for cross domain sound
processing tasks, where frequencies represented in the input spectrograms may
be different from one spectrogram to another.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolet_A/0/1/0/all/0/1&quot;&gt;Antoine Rolet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seguy_V/0/1/0/all/0/1&quot;&gt;Vivien Seguy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blondel_M/0/1/0/all/0/1&quot;&gt;Mathieu Blondel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawada_H/0/1/0/all/0/1&quot;&gt;Hiroshi Sawada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05431">
<title>On the Theory of Variance Reduction for Stochastic Gradient Monte Carlo. (arXiv:1802.05431v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.05431</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide convergence guarantees in Wasserstein distance for a variety of
variance-reduction methods: SAGA Langevin diffusion, SVRG Langevin diffusion
and control-variate underdamped Langevin diffusion. We analyze these methods
under a uniform set of assumptions on the log-posterior distribution, assuming
it to be smooth, strongly convex and Hessian Lipschitz. This is achieved by a
new proof technique combining ideas from finite-sum optimization and the
analysis of sampling methods. Our sharp theoretical bounds allow us to identify
regimes of interest where each method performs better than the others. Our
theory is verified with experiments on real-world and synthetic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1&quot;&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Flammarion_N/0/1/0/all/0/1&quot;&gt;Nicolas Flammarion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yi-An Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1&quot;&gt;Peter L. Bartlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05447">
<title>History PCA: A New Algorithm for Streaming PCA. (arXiv:1802.05447v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.05447</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose a new algorithm for streaming principal component
analysis. With limited memory, small devices cannot store all the samples in
the high-dimensional regime. Streaming principal component analysis aims to
find the $k$-dimensional subspace which can explain the most variation of the
$d$-dimensional data points that come into memory sequentially. In order to
deal with large $d$ and large $N$ (number of samples), most streaming PCA
algorithms update the current model using only the incoming sample and then
dump the information right away to save memory. However the information
contained in previously streamed data could be useful. Motivated by this idea,
we develop a new streaming PCA algorithm called History PCA that achieves this
goal. By using $O(Bd)$ memory with $B\approx 10$ being the block size, our
algorithm converges much faster than existing streaming PCA algorithms. By
changing the number of inner iterations, the memory usage can be further
reduced to $O(d)$ while maintaining a comparable convergence speed. We provide
theoretical guarantees for the convergence of our algorithm along with the rate
of convergence. We also demonstrate on synthetic and real world data sets that
our algorithm compares favorably with other state-of-the-art streaming PCA
methods in terms of the convergence speed and performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_P/0/1/0/all/0/1&quot;&gt;Puyudi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jane-Ling Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05451">
<title>Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction. (arXiv:1802.05451v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.05451</link>
<description rdf:parseType="Literal">&lt;p&gt;Structured prediction is concerned with predicting multiple inter-dependent
labels simultaneously. Classical methods like CRF achieve this by maximizing a
score function over the set of possible label assignments. Recent extensions
use neural networks to either implement the score function or in maximization.
The current paper takes an alternative approach, using a neural network to
generate the structured output directly, without going through a score
function. We take an axiomatic perspective to derive the desired properties and
invariances of a such network to certain input permutations, presenting a
structural characterization that is provably both necessary and sufficient. We
then discuss graph-permutation invariant (GPI) architectures that satisfy this
characterization and explain how they can be used for deep structured
prediction. We evaluate our approach on the challenging problem of inferring a
{\em scene graph} from an image, namely, predicting entities and their
relations in the image. We obtain state-of-the-art results on the challenging
Visual Genome benchmark, outperforming all recent approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Herzig_R/0/1/0/all/0/1&quot;&gt;Roei Herzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raboh_M/0/1/0/all/0/1&quot;&gt;Moshiko Raboh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chechik_G/0/1/0/all/0/1&quot;&gt;Gal Chechik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Berant_J/0/1/0/all/0/1&quot;&gt;Jonathan Berant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Globerson_A/0/1/0/all/0/1&quot;&gt;Amir Globerson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05584">
<title>Convolutional Analysis Operator Learning: Acceleration, Convergence, Application, and Neural Networks. (arXiv:1802.05584v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.05584</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional operator learning is increasingly gaining attention in many
signal processing and computer vision applications. Learning kernels has mostly
relied on so-called local approaches that extract and store many overlapping
patches across training signals. Due to memory demands, local approaches have
limitations when learning kernels from large datasets -- particularly with
multi-layered structures, e.g., convolutional neural network (CNN) -- and/or
applying the learned kernels to high-dimensional signal recovery problems. The
so-called global approach has been studied within the &quot;synthesis&quot; signal model,
e.g., convolutional dictionary learning, overcoming the memory problems by
careful algorithmic designs. This paper proposes a new convolutional analysis
operator learning (CAOL) framework in the global approach, and develops a new
convergent Block Proximal Gradient method using a Majorizer (BPG-M) to solve
the corresponding block multi-nonconvex problems. To learn diverse filters
within the CAOL framework, this paper introduces an orthogonality constraint
that enforces a tight-frame (TF) filter condition, and a regularizer that
promotes diversity between filters. Numerical experiments show that, for tight
majorizers, BPG-M significantly accelerates the CAOL convergence rate compared
to the state-of-the-art method, BPG. Numerical experiments for sparse-view
computational tomography show that CAOL using TF filters significantly improves
reconstruction quality compared to a conventional edge-preserving regularizer.
Finally, this paper shows that CAOL can be useful to mathematically model a
CNN, and the corresponding updates obtained via BPG-M coincide with core
modules of the CNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chun_I/0/1/0/all/0/1&quot;&gt;Il Yong Chun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fessler_J/0/1/0/all/0/1&quot;&gt;Jeffrey A. Fessler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05622">
<title>Conditioning of three-dimensional generative adversarial networks for pore and reservoir-scale models. (arXiv:1802.05622v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.05622</link>
<description rdf:parseType="Literal">&lt;p&gt;Geostatistical modeling of petrophysical properties is a key step in modern
integrated oil and gas reservoir studies. Recently, generative adversarial
networks (GAN) have been shown to be a successful method for generating
unconditional simulations of pore- and reservoir-scale models. This
contribution leverages the differentiable nature of neural networks to extend
GANs to the conditional simulation of three-dimensional pore- and
reservoir-scale models. Based on the previous work of Yeh et al. (2016), we use
a content loss to constrain to the conditioning data and a perceptual loss
obtained from the evaluation of the GAN discriminator network. The technique is
tested on the generation of three-dimensional micro-CT images of a Ketton
limestone constrained by two-dimensional cross-sections, and on the simulation
of the Maules Creek alluvial aquifer constrained by one-dimensional sections.
Our results show that GANs represent a powerful method for sampling conditioned
pore and reservoir samples for stochastic reservoir evaluation workflows.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mosser_L/0/1/0/all/0/1&quot;&gt;Lukas Mosser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dubrule_O/0/1/0/all/0/1&quot;&gt;Olivier Dubrule&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blunt_M/0/1/0/all/0/1&quot;&gt;Martin J. Blunt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05637">
<title>cGANs with Projection Discriminator. (arXiv:1802.05637v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.05637</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel, projection based way to incorporate the conditional
information into the discriminator of GANs that respects the role of the
conditional information in the underlining probabilistic model. This approach
is in contrast with most frameworks of conditional GANs used in application
today, which use the conditional information by concatenating the (embedded)
conditional vector to the feature vectors. With this modification, we were able
to significantly improve the quality of the class conditional image generation
on ILSVRC2012 (ImageNet) 1000-class image dataset from the current
state-of-the-art result, and we achieved this with a single pair of a
discriminator and a generator. We were also able to extend the application to
super-resolution and succeeded in producing highly discriminative
super-resolution images. This new structure also enabled high quality category
transformation based on parametric functional transformation of conditional
batch normalization layers in the generator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miyato_T/0/1/0/all/0/1&quot;&gt;Takeru Miyato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyama_M/0/1/0/all/0/1&quot;&gt;Masanori Koyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05664">
<title>DeepMatch: Balancing Deep Covariate Representations for Causal Inference Using Adversarial Training. (arXiv:1802.05664v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.05664</link>
<description rdf:parseType="Literal">&lt;p&gt;We study optimal covariate balance for causal inferences from observational
data when rich covariates and complex relationships necessitate flexible
modeling with neural networks. Standard approaches such as propensity weighting
and matching/balancing fail in such settings due to miscalibrated propensity
nets and inappropriate covariate representations, respectively. We propose a
new method based on adversarial training of a weighting and a discriminator
network that effectively addresses this methodological gap. This is
demonstrated through new theoretical characterizations of the method as well as
empirical results using both fully connected architectures to learn complex
relationships and convolutional architectures to handle image confounders,
showing how this new method can enable strong causal analyses in these
challenging settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kallus_N/0/1/0/all/0/1&quot;&gt;Nathan Kallus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05680">
<title>Constraining the Dynamics of Deep Probabilistic Models. (arXiv:1802.05680v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.05680</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel generative formulation of deep probabilistic models
implementing &quot;soft&quot; constraints on the dynamics of the functions they can
model. In particular we develop a flexible methodological framework where the
modeled functions and derivatives of a given order are subject to inequality or
equality constraints. We characterize the posterior distribution over model and
constraint parameters through stochastic variational inference techniques. As a
result, the proposed approach allows for accurate and scalable uncertainty
quantification of predictions and parameters. We demonstrate the application of
equality constraints in the challenging problem of parameter inference in
ordinary differential equation models, while we showcase the application of
inequality constraints on monotonic regression on count data. The proposed
approach is extensively tested in several experimental settings, leading to
highly competitive results in challenging modeling applications, while offering
high expressiveness, flexibility and scalability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lorenzi_M/0/1/0/all/0/1&quot;&gt;Marco Lorenzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Filippone_M/0/1/0/all/0/1&quot;&gt;Maurizio Filippone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05688">
<title>Simulation assisted machine learning. (arXiv:1802.05688v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.05688</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting how a proposed cancer treatment will affect a given tumor can be
cast as a machine learning problem, but the complexity of biological systems,
the number of potentially relevant genomic and clinical features, and the lack
of very large scale patient data repositories make this a unique challenge.
&quot;Pure data&quot; approaches to this problem are underpowered to detect
combinatorially complex interactions and are bound to uncover false
correlations despite statistical precautions taken (1). To investigate this
setting, we propose a method to integrate simulations, a strong form of prior
knowledge, into machine learning, a combination which to date has been largely
unexplored. The results of multiple simulations (under various uncertainty
scenarios) are used to compute similarity measures between every pair of
samples: sample pairs are given a high similarity score if they behave
similarly under a wide range of simulation parameters. These similarity values,
rather than the original high dimensional feature data, are used to train
kernelized machine learning algorithms such as support vector machines, thus
handling the curse-of-dimensionality that typically affects genomic machine
learning. Using four synthetic datasets of complex systems--three biological
models and one network flow optimization model--we demonstrate that when the
number of training samples is small compared to the number of features, the
simulation kernel approach dominates over no-prior-knowledge methods. In
addition to biology and medicine, this approach should be applicable to other
disciplines, such as weather forecasting, financial markets, and agricultural
management, where predictive models are sought and informative yet approximate
simulations are available. The Python SimKern software, the models (in MATLAB,
Octave, and R), and the datasets are made freely available at
https://github.com/davidcraft/SimKern.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deist_T/0/1/0/all/0/1&quot;&gt;Timo Deist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Patti_A/0/1/0/all/0/1&quot;&gt;Andrew Patti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krane_D/0/1/0/all/0/1&quot;&gt;David Krane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sorenson_T/0/1/0/all/0/1&quot;&gt;Taylor Sorenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Craft_D/0/1/0/all/0/1&quot;&gt;David Craft&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05693">
<title>Bandit Learning with Positive Externalities. (arXiv:1802.05693v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.05693</link>
<description rdf:parseType="Literal">&lt;p&gt;Many platforms are characterized by the fact that future user arrivals are
likely to have preferences similar to users who were satisfied in the past. In
other words, arrivals exhibit {\em positive externalities}. We study multiarmed
bandit (MAB) problems with positive externalities. Our model has a finite
number of arms and users are distinguished by the arm(s) they prefer. We model
positive externalities by assuming that the preferred arms of future arrivals
are self-reinforcing based on the experiences of past users. We show that
classical algorithms such as UCB which are optimal in the classical MAB setting
may even exhibit linear regret in the context of positive externalities. We
provide an algorithm which achieves optimal regret and show that such optimal
regret exhibits substantially different structure from that observed in the
standard MAB setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1&quot;&gt;Virag Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanchet_J/0/1/0/all/0/1&quot;&gt;Jose Blanchet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johari_R/0/1/0/all/0/1&quot;&gt;Ramesh Johari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05695">
<title>Explainable Prediction of Medical Codes from Clinical Text. (arXiv:1802.05695v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1802.05695</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical notes are text documents that are created by clinicians for each
patient encounter. They are typically accompanied by medical codes, which
describe the diagnosis and treatment. Annotating these codes is labor intensive
and error prone; furthermore, the connection between the codes and the text is
not annotated, obscuring the reasons and details behind specific diagnoses and
treatments. We present an attentional convolutional network that predicts
medical codes from clinical text. Our method aggregates information across the
document using a convolutional neural network, and uses an attention mechanism
to select the most relevant segments for each of the thousands of possible
codes. The method is accurate, achieving precision @ 8 of 0.7 and a Micro-F1 of
0.52, which are both significantly better than the prior state of the art.
Furthermore, through an interpretability evaluation by a physician, we show
that the attention mechanism identifies meaningful explanations for each code
assignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullenbach_J/0/1/0/all/0/1&quot;&gt;James Mullenbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1&quot;&gt;Sarah Wiegreffe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duke_J/0/1/0/all/0/1&quot;&gt;Jon Duke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1&quot;&gt;Jacob Eisenstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1604.04706">
<title>DS-MLR: Exploiting Double Separability for Scaling up Distributed Multinomial Logistic Regression. (arXiv:1604.04706v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1604.04706</link>
<description rdf:parseType="Literal">&lt;p&gt;Scaling multinomial logistic regression to datasets with very large number of
data points and classes has not been trivial. This is primarily because one
needs to compute the log-partition function on every data point. This makes
distributing the computation hard. In this paper, we present a distributed
stochastic gradient descent based optimization method (DS-MLR) for scaling up
multinomial logistic regression problems to massive scale datasets without
hitting any storage constraints on the data and model parameters. Our algorithm
exploits double-separability, an attractive property we observe in the
objective functions of several models in machine learning, that allows us to
achieve both data as well as model parallelism simultaneously. In addition to
being parallelizable, our algorithm can also easily be made non-blocking and
asynchronous. We demonstrate the effectiveness of DS-MLR empirically on several
real-world datasets, the largest being a reddit dataset created out of 1.7
billion user comments, where the data and parameter sizes are 228 GB and 358 GB
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_P/0/1/0/all/0/1&quot;&gt;Parameswaran Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1&quot;&gt;Sriram Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsushima_S/0/1/0/all/0/1&quot;&gt;Shin Matsushima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinhua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1&quot;&gt;Hyokun Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwanathan_S/0/1/0/all/0/1&quot;&gt;S.V.N. Vishwanathan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.09499">
<title>Extreme Stochastic Variational Inference: Distributed and Asynchronous. (arXiv:1605.09499v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1605.09499</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose extreme stochastic variational inference (ESVI), an asynchronous
and lock-free algorithm to perform variational inference on massive real world
datasets. Stochastic variational inference (SVI), the state-of-the-art
algorithm for scaling variational inference to large-datasets, is inherently
serial. Moreover, it requires the parameters to fit in the memory of a single
processor; this is problematic when the number of parameters is in billions.
ESVI overcomes these limitations by requiring that each processor only access a
subset of the data and a subset of the parameters, thus providing data and
model parallelism simultaneously. We demonstrate the effectiveness of ESVI by
running Latent Dirichlet Allocation (LDA) on UMBC-3B, a dataset that has a
vocabulary of 3 million and a token size of 3 billion. To best of our
knowledge, this is an order of magnitude larger than the largest dataset on
which results using variational inference have been reported in literature. In
our experiments, we found that ESVI outperforms VI and SVI, and also achieves a
better quality solution. In addition, we propose a strategy to speed up
computation and save memory when fitting large number of topics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raman_P/0/1/0/all/0/1&quot;&gt;Parameswaran Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shihao Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hsiang-Fu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vishwanathan_S/0/1/0/all/0/1&quot;&gt;S.V.N. Vishwanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhillon_I/0/1/0/all/0/1&quot;&gt;Inderjit S. Dhillon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.05008">
<title>Tree Ensembles with Rule Structured Horseshoe Regularization. (arXiv:1702.05008v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1702.05008</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new Bayesian model for flexible nonlinear regression and
classification using tree ensembles. The model is based on the RuleFit approach
in Friedman and Popescu (2008) where rules from decision trees and linear terms
are used in a L1-regularized regression. We modify RuleFit by replacing the
L1-regularization by a horseshoe prior, which is well known to give aggressive
shrinkage of noise predictor while leaving the important signal essentially
untouched. This is especially important when a large number of rules are used
as predictors as many of them only contribute noise. Our horseshoe prior has an
additional hierarchical layer that applies more shrinkage a priori to rules
with a large number of splits, and to rules that are only satisfied by a few
observations. The aggressive noise shrinkage of our prior also makes it
possible to complement the rules from boosting in Friedman and Popescu (2008)
with an additional set of trees from random forest, which brings a desirable
diversity to the ensemble. We sample from the posterior distribution using a
very efficient and easily implemented Gibbs sampler. The new model is shown to
outperform state-of-the-art methods like RuleFit, BART and random forest on 16
datasets. The model and its interpretation is demonstrated on the well known
Boston housing data, and on gene expression data for cancer classification. The
posterior sampling, prediction and graphical tools for interpreting the model
results are implemented in a publicly available R package.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nalenz_M/0/1/0/all/0/1&quot;&gt;Malte Nalenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Villani_M/0/1/0/all/0/1&quot;&gt;Mattias Villani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.06176">
<title>Segmentation of the Proximal Femur from MR Images using Deep Convolutional Neural Networks. (arXiv:1704.06176v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1704.06176</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic resonance imaging (MRI) has been proposed as a complimentary method
to measure bone quality and assess fracture risk. However, manual segmentation
of MR images of bone is time-consuming, limiting the use of MRI measurements in
the clinical practice. The purpose of this paper is to present an automatic
proximal femur segmentation method that is based on deep convolutional neural
networks (CNNs). This study had institutional review board approval and written
informed consent was obtained from all subjects. A dataset of volumetric
structural MR images of the proximal femur from 86 subject were
manually-segmented by an expert. We performed experiments by training two
different CNN architectures with multiple number of initial feature maps and
layers, and tested their segmentation performance against the gold standard of
manual segmentations using four-fold cross-validation. Automatic segmentation
of the proximal femur achieved a high dice similarity score of 0.94$\pm$0.05
with precision = 0.95$\pm$0.02, and recall = 0.94$\pm$0.08 using a CNN
architecture based on 3D convolution exceeding the performance of 2D CNNs. The
high segmentation accuracy provided by CNNs has the potential to help bring the
use of structural MRI measurements of bone quality into clinical practice for
management of osteoporosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deniz_C/0/1/0/all/0/1&quot;&gt;Cem M. Deniz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1&quot;&gt;Siyuan Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallyburton_S/0/1/0/all/0/1&quot;&gt;Spencer Hallyburton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welbeck_A/0/1/0/all/0/1&quot;&gt;Arakua Welbeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honig_S/0/1/0/all/0/1&quot;&gt;Stephen Honig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_G/0/1/0/all/0/1&quot;&gt;Gregory Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.06066">
<title>On Quadratic Convergence of DC Proximal Newton Algorithm for Nonconvex Sparse Learning in High Dimensions. (arXiv:1706.06066v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.06066</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a DC proximal Newton algorithm for solving nonconvex regularized
sparse learning problems in high dimensions. Our proposed algorithm integrates
the proximal Newton algorithm with multi-stage convex relaxation based on the
difference of convex (DC) programming, and enjoys both strong computational and
statistical guarantees. Specifically, by leveraging a sophisticated
characterization of sparse modeling structures/assumptions (i.e., local
restricted strong convexity and Hessian smoothness), we prove that within each
stage of convex relaxation, our proposed algorithm achieves (local) quadratic
convergence, and eventually obtains a sparse approximate local optimum with
optimal statistical properties after only a few convex relaxations. Numerical
experiments are provided to support our theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin F. Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ge_J/0/1/0/all/0/1&quot;&gt;Jason Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haupt_J/0/1/0/all/0/1&quot;&gt;Jarvis Haupt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.10295">
<title>Noisy Networks for Exploration. (arXiv:1706.10295v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.10295</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce NoisyNet, a deep reinforcement learning agent with parametric
noise added to its weights, and show that the induced stochasticity of the
agent&apos;s policy can be used to aid efficient exploration. The parameters of the
noise are learned with gradient descent along with the remaining network
weights. NoisyNet is straightforward to implement and adds little computational
overhead. We find that replacing the conventional exploration heuristics for
A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively)
with NoisyNet yields substantially higher scores for a wide range of Atari
games, in some cases advancing the agent from sub to super-human performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortunato_M/0/1/0/all/0/1&quot;&gt;Meire Fortunato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azar_M/0/1/0/all/0/1&quot;&gt;Mohammad Gheshlaghi Azar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piot_B/0/1/0/all/0/1&quot;&gt;Bilal Piot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1&quot;&gt;Jacob Menick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osband_I/0/1/0/all/0/1&quot;&gt;Ian Osband&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graves_A/0/1/0/all/0/1&quot;&gt;Alex Graves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mnih_V/0/1/0/all/0/1&quot;&gt;Vlad Mnih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munos_R/0/1/0/all/0/1&quot;&gt;Remi Munos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassabis_D/0/1/0/all/0/1&quot;&gt;Demis Hassabis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1&quot;&gt;Olivier Pietquin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charles Blundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legg_S/0/1/0/all/0/1&quot;&gt;Shane Legg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.07113">
<title>Adversarial Variational Optimization of Non-Differentiable Simulators. (arXiv:1707.07113v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.07113</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex computer simulators are increasingly used across fields of science as
generative models tying parameters of an underlying theory to experimental
observations. Inference in this setup is often difficult, as simulators rarely
admit a tractable density or likelihood function. We introduce Adversarial
Variational Optimization (AVO), a likelihood-free inference algorithm for
fitting a non-differentiable generative model incorporating ideas from
generative adversarial networks, variational optimization and empirical Bayes.
We adapt the training procedure of Wasserstein GANs by replacing the
differentiable generative network with a domain-specific simulator. We solve
the resulting non-differentiable minimax problem by minimizing variational
upper bounds of the two adversarial objectives. Effectively, the procedure
results in learning a proposal distribution over simulator parameters, such
that the Wasserstein distance between the marginal distribution of the
synthetic data and the empirical distribution of observed data is minimized. We
present results of the method with simulators producing both discrete and
continuous data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Louppe_G/0/1/0/all/0/1&quot;&gt;Gilles Louppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cranmer_K/0/1/0/all/0/1&quot;&gt;Kyle Cranmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.10433">
<title>On the Capacity of Face Representation. (arXiv:1709.10433v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1709.10433</link>
<description rdf:parseType="Literal">&lt;p&gt;Face recognition is a widely used technology with numerous large-scale
applications, such as surveillance, social media and law enforcement. There has
been tremendous progress in face recognition accuracy over the past few
decades, much of which can be attributed to deep learning-based approaches
during the last five years. Indeed, automated face recognition systems are now
believed to surpass human performance in some scenarios. Despite this progress,
a crucial question still remains unanswered: given a face representation, how
many identities can it resolve? In other words, what is the capacity of the
face representation? A scientific basis for estimating the capacity of a given
face representation will not only benefit the evaluation and comparison of
different face representations but will also establish an upper bound on the
scalability of an automatic face recognition system. We cast the face capacity
estimation problem under the information theoretic framework of capacity of a
Gaussian noise channel. By explicitly accounting for two sources of
representational noise: epistemic uncertainty and aleatoric variability, our
approach is able to estimate the capacity of any given face representation. To
demonstrate the efficacy of our approach, we estimate the capacity of a
128-dimensional DNN based face representation, FaceNet, and that of the
classical Eigenfaces representation of the same dimensionality. Our experiments
on unconstrained faces indicate that, (a) our proposed model yields a capacity
upper bound of 5.8x$10^{8}$ for FaceNet and 1x$10^{0}$ for Eigenfaces at a
false acceptance rate (FAR) of 1%, (b) the face representation capacity reduces
drastically as you lower the desired FAR (for FaceNet; the capacity at FAR of
0.1% and 0.001% is 2.4x$10^{6}$ and 7.0x$10^{2}$, respectively), and (c) the
empirical performance of FaceNet is significantly below the theoretical limit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1&quot;&gt;Sixue Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1&quot;&gt;Vishnu Naresh Boddeti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Anil K. Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01193">
<title>A dual framework for trace norm regularized low-rank tensor completion. (arXiv:1712.01193v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01193</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the popular approaches for low-rank tensor completion is to use the
latent trace norm as a low-rank regularizer. However, most of the existing
works learn a sparse combination of tensors. In this work, we fill this gap by
proposing a variant of the latent trace norm which helps to learn a non-sparse
combination of tensors. We develop a dual framework for solving the problem of
latent trace norm regularized low-rank tensor completion. In this framework, we
first show a novel characterization of the solution space with a novel
factorization, and then, propose two scalable optimization formulations. The
problems are shown to lie on a Cartesian product of Riemannian spectrahedron
manifolds. We exploit the versatile Riemannian optimization framework for
proposing computationally efficient trust-region algorithms. The experiments
show the good performance of the proposed algorithms on several real-world data
sets in different applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nimishakavi_M/0/1/0/all/0/1&quot;&gt;Madhav Nimishakavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawanpuria_P/0/1/0/all/0/1&quot;&gt;Pratik Jawanpuria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bamdev Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04826">
<title>Leveraging the Exact Likelihood of Deep Latent Variables Models. (arXiv:1802.04826v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04826</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep latent variable models combine the approximation abilities of deep
neural networks and the statistical foundations of generative models. The
induced data distribution is an infinite mixture model whose density is
extremely delicate to compute. Variational methods are consequently used for
inference, following the seminal work of Rezende et al. (2014) and Kingma and
Welling (2014). We study the well-posedness of the exact problem (maximum
likelihood) these techniques approximatively solve. In particular, we show that
most unconstrained models used for continuous data have an unbounded
likelihood. This ill-posedness and the problems it causes are illustrated on
real data. We also show how to insure the existence of maximum likelihood
estimates, and draw useful connections with nonparametric mixture models.
Furthermore, we describe an algorithm that allows to perform missing data
imputation using the exact conditional likelihood of a deep latent variable
model. On several real data sets, our algorithm consistently and significantly
outperforms the usual imputation scheme used within deep latent variable
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mattei_P/0/1/0/all/0/1&quot;&gt;Pierre-Alexandre Mattei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frellsen_J/0/1/0/all/0/1&quot;&gt;Jes Frellsen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04956">
<title>D2KE: From Distance to Kernel and Embedding. (arXiv:1802.04956v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04956</link>
<description rdf:parseType="Literal">&lt;p&gt;For many machine learning problem settings, particularly with structured
inputs such as sequences or sets of objects, a distance measure between inputs
can be specified more naturally than a feature representation. However, most
standard machine models are designed for inputs with a vector feature
representation. In this work, we consider the estimation of a function
$f:\mathcal{X} \rightarrow \R$ based solely on a dissimilarity measure
$d:\mathcal{X}\times\mathcal{X} \rightarrow \R$ between inputs. In particular,
we propose a general framework to derive a family of \emph{positive definite
kernels} from a given dissimilarity measure, which subsumes the widely-used
\emph{representative-set method} as a special case, and relates to the
well-known \emph{distance substitution kernel} in a limiting case. We show that
functions in the corresponding Reproducing Kernel Hilbert Space (RKHS) are
Lipschitz-continuous w.r.t. the given distance metric. We provide a tractable
algorithm to estimate a function from this RKHS, and show that it enjoys better
generalizability than Nearest-Neighbor estimates. Our approach draws from the
literature of Random Features, but instead of deriving feature maps from an
existing kernel, we construct novel kernels from a random feature map, that we
specify given the distance measure. We conduct classification experiments with
such disparate domains as strings, time series, and sets of vectors, where our
proposed framework compares favorably to existing distance-based learning
methods such as $k$-nearest-neighbors, distance-substitution kernels,
pseudo-Euclidean embedding, and the representative-set method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lingfei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yen_I/0/1/0/all/0/1&quot;&gt;Ian En-Hsu Yen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_F/0/1/0/all/0/1&quot;&gt;Fangli Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Witbrock_M/0/1/0/all/0/1&quot;&gt;Michael Witbrock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.05155">
<title>Toward Deeper Understanding of Nonconvex Stochastic Optimization with Momentum using Diffusion Approximations. (arXiv:1802.05155v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.05155</link>
<description rdf:parseType="Literal">&lt;p&gt;Momentum Stochastic Gradient Descent (MSGD) algorithm has been widely applied
to many nonconvex optimization problems in machine learning. Popular examples
include training deep neural networks, dimensionality reduction, and etc. Due
to the lack of convexity and the extra momentum term, the optimization theory
of MSGD is still largely unknown. In this paper, we study this fundamental
optimization algorithm based on the so-called &quot;strict saddle problem.&quot; By
diffusion approximation type analysis, our study shows that the momentum helps
escape from saddle points, but hurts the convergence within the neighborhood of
optima (if without the step size annealing). Our theoretical discovery
partially corroborates the empirical success of MSGD in training deep neural
networks. Moreover, our analysis applies the martingale method and
&quot;Fixed-State-Chain&quot; method from the stochastic approximation literature, which
are of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhehui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1&quot;&gt;Enlu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.01166">
<title>Maximizing the information learned from finite data selects a simple model. (arXiv:1705.01166v3 [physics.data-an] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1705.01166</link>
<description rdf:parseType="Literal">&lt;p&gt;We use the language of uninformative Bayesian prior choice to study the
selection of appropriately simple effective models. We advocate for the prior
which maximizes the mutual information between parameters and predictions,
learning as much as possible from limited data. When many parameters are poorly
constrained by the available data, we find that this prior puts weight only on
boundaries of the parameter manifold. Thus it selects a lower-dimensional
effective theory in a principled way, ignoring irrelevant parameter directions.
In the limit where there is sufficient data to tightly constrain any number of
parameters, this reduces to Jeffreys prior. But we argue that this limit is
pathological when applied to the hyper-ribbon parameter manifolds generic in
science, because it leads to dramatic dependence on effects invisible to
experiment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mattingly_H/0/1/0/all/0/1&quot;&gt;Henry H. Mattingly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Transtrum_M/0/1/0/all/0/1&quot;&gt;Mark K. Transtrum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Abbott_M/0/1/0/all/0/1&quot;&gt;Michael C. Abbott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Machta_B/0/1/0/all/0/1&quot;&gt;Benjamin B. Machta&lt;/a&gt;</dc:creator>
</item></rdf:RDF>