<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-02T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00096"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00119"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00318"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00609"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00746"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01239"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00056"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00085"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00102"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00345"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00356"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00361"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00453"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00602"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00631"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00688"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00690"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00702"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00712"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00727"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00743"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.07710"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.09393"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.02553"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10916"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10125"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.05812"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00101"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00171"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00185"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00203"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00283"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00315"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00364"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00393"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00513"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00636"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00668"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00681"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1404.4178"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1411.0589"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1603.04981"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1604.02181"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.00048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.07574"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.01968"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.01445"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.03663"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.01977"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10370"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01682"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04802"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10110"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.00096">
<title>Towards automated patient data cleaning using deep learning: A feasibility study on the standardization of organ labeling. (arXiv:1801.00096v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/1801.00096</link>
<description rdf:parseType="Literal">&lt;p&gt;Data cleaning consumes about 80% of the time spent on data analysis for
clinical research projects. This is a much bigger problem in the era of big
data and machine learning in the field of medicine where large volumes of data
are being generated. We report an initial effort towards automated patient data
cleaning using deep learning: the standardization of organ labeling in
radiation therapy. Organs are often labeled inconsistently at different
institutions (sometimes even within the same institution) and at different time
periods, which poses a problem for clinical research, especially for
multi-institutional collaborative clinical research where the acquired patient
data is not being used effectively. We developed a convolutional neural network
(CNN) to automatically identify each organ in the CT image and then label it
with the standardized nomenclature presented at AAPM Task Group 263. We tested
this model on the CT images of 54 patients with prostate and 100 patients with
head and neck cancer who previously received radiation therapy. The model
achieved 100% accuracy in detecting organs and assigning standardized labels
for the patients tested. This work shows the feasibility of using deep learning
in patient data cleaning that enables standardized datasets to be generated for
effective intra- and interinstitutional collaborative clinical research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rozario_T/0/1/0/all/0/1&quot;&gt;Timothy Rozario&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Long_T/0/1/0/all/0/1&quot;&gt;Troy Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingli Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Weiguo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Steve Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00119">
<title>Towards co-evolution of fitness predictors and Deep Neural Networks. (arXiv:1801.00119v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.00119</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks proved to be a very useful and powerful tool with many
practical applications. They especially excel at learning from large data sets
with labeled samples. However, in order to achieve good learning results, the
network architecture has to be carefully designed. Creating an optimal topology
requires a lot of experience and knowledge. Unfortunately there are no
practically applicable algorithms which could help in this situation. Using an
evolutionary process to develop new network topologies might solve this
problem. The limiting factor in this case is the speed of evaluation of a
single specimen (a single network architecture), which includes learning based
on the whole large dataset. In this paper we propose to overcome this problem
by using a fitness prediction technique: use subsets of the original training
set to conduct the training process and use its results as an approximation of
specimen&apos;s fitness. We discuss the feasibility of this approach in context of
the desired fitness predictor features and analyze whether subsets obtained in
an evolutionary process can be used to estimate the fitness of the network
topology. Finally we draw conclusions from our experiments and outline plans
for future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Funika_W/0/1/0/all/0/1&quot;&gt;W&amp;#x142;odzimierz Funika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koperek_P/0/1/0/all/0/1&quot;&gt;Pawe&amp;#x142; Koperek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00318">
<title>Towards Building an Intelligent Anti-Malware System: A Deep Learning Approach using Support Vector Machine (SVM) for Malware Classification. (arXiv:1801.00318v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.00318</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective and efficient mitigation of malware is a long-time endeavor in the
information security community. The development of an anti-malware system that
can counteract an unknown malware is a prolific activity that may benefit
several sectors. We envision an intelligent anti-malware system that utilizes
the power of deep learning (DL) models. Using such models would enable the
detection of newly-released malware through mathematical generalization. That
is, finding the relationship between a given malware $x$ and its corresponding
malware family $y$, $f: x \mapsto y$. To accomplish this feat, we used the
Malimg dataset (Nataraj et al., 2011) which consists of malware images that
were processed from malware binaries, and then we trained the following DL
models 1 to classify each malware family: CNN-SVM (Tang, 2013), GRU-SVM
(Agarap, 2017), and MLP-SVM. Empirical evidence has shown that the GRU-SVM
stands out among the DL models with a predictive accuracy of ~84.92%. This
stands to reason for the mentioned model had the relatively most sophisticated
architecture design among the presented models. The exploration of an even more
optimal DL-SVM model is the next stage towards the engineering of an
intelligent anti-malware system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarap_A/0/1/0/all/0/1&quot;&gt;Abien Fred Agarap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pepito_F/0/1/0/all/0/1&quot;&gt;Francis John Hill Pepito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00609">
<title>Interactive Decomposition Multi-Objective Optimization via Progressively Learned Value Functions. (arXiv:1801.00609v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.00609</link>
<description rdf:parseType="Literal">&lt;p&gt;Decomposition has become an increasingly popular technique for evolutionary
multi-objective optimization (EMO). A decomposition-based EMO algorithm is
usually designed to approximate a whole Pareto-optimal front (PF). However, in
practice, the decision maker (DM) might only be interested in her/his region of
interest (ROI), i.e., a part of the PF. Solutions outside that might be useless
or even noisy to the decision-making procedure. Furthermore, there is no
guarantee to find the preferred solutions when tackling many-objective
problems. This paper develops an interactive framework for the
decomposition-based EMO algorithm to lead a DM to the preferred solutions of
her/his choice. It consists of three modules, i.e., consultation, preference
elicitation and optimization. Specifically, after every several generations,
the DM is asked to score a few candidate solutions in a consultation session.
Thereafter, an approximated value function, which models the DM&apos;s preference
information, is progressively learned from the DM&apos;s behavior. In the preference
elicitation session, the preference information learned in the consultation
module is translated into the form that can be used in a decomposition-based
EMO algorithm, i.e., a set of reference points that are biased toward to the
ROI. The optimization module, which can be any decomposition-based EMO
algorithm in principle, utilizes the biased reference points to direct its
search process. Extensive experiments on benchmark problems with three to ten
objectives fully demonstrate the effectiveness of our proposed method for
finding the DM&apos;s preferred solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Renzhi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savic_D/0/1/0/all/0/1&quot;&gt;Dragan Savic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1&quot;&gt;Xin Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00746">
<title>Bridging the Gap Between Neural Networks and Neuromorphic Hardware with A Neural Network Compiler. (arXiv:1801.00746v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.00746</link>
<description rdf:parseType="Literal">&lt;p&gt;Different from training common neural networks (NNs) for inference on
general-purpose processors, the development of NNs for neuromorphic chips is
usually faced with a number of hardware-specific restrictions, including the
limited precision of network signals and parameters, constrained computation
scale and limited types of non-linear functions. This paper proposes a general
methodology to address the challenge. It can transform an existing trained,
unrestricted NN (usually for software execution substrate) into an equivalent
network that meets the given hardware constraints, which decouples NN
applications from target hardware. Formally, the original NN is expressed as a
computational graph (CG) that would be fine-tuned part by part according to a
topological ordering to become the target CG. Quite a few techniques, including
the multilayer-perceptron(MLP)-based universal approximator, a data re-encoding
method, a split-and-merge network reconstruction method and a multi-phase
weight-tuning algorithm, are proposed to conquer the above restrictions
respectively. We have built such a software tool that supports both spiking
neural networks (SNNs) and traditional artificial neural networks (ANNs). Its
effectiveness has been demonstrated with a real neuromorphic chip and a
processing-in-memory (PIM) design. Tests show that the extra inference error
caused by this solution is very limited and the transformation time is much
less than the retraining time. In addition, quite a few parameter-sensitivity
evaluations have been completed to explore the tradeoff between network error,
resource consumption and different transformation strategies, which could
provide insights for co-design optimization of neuromorphic hardware and
software.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yu Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;YouHui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;WenGuang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuan Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01239">
<title>Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning. (arXiv:1711.01239v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01239</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-task learning (MTL) with neural networks leverages commonalities in
tasks to improve performance, but often suffers from task interference which
reduces the benefits of transfer. To address this issue we introduce the
routing network paradigm, a novel neural network and training algorithm. A
routing network is a kind of self-organizing neural network consisting of two
components: a router and a set of one or more function blocks. A function block
may be any neural network - for example a fully-connected or a convolutional
layer. Given an input the router makes a routing decision, choosing a function
block to apply and passing the output back to the router recursively,
terminating when a fixed recursion depth is reached. In this way the routing
network dynamically composes different function blocks for each input. We
employ a collaborative multi-agent reinforcement learning (MARL) approach to
jointly train the router and function blocks. We evaluate our model against
cross-stitch networks and shared-layer baselines on multi-task settings of the
MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a
significant improvement in accuracy, with sharper convergence. In addition,
routing networks have nearly constant per-task training cost while cross-stitch
networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we
obtain cross-stitch performance levels with an 85% reduction in training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenbaum_C/0/1/0/all/0/1&quot;&gt;Clemens Rosenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klinger_T/0/1/0/all/0/1&quot;&gt;Tim Klinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1&quot;&gt;Matthew Riemer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00048">
<title>Characterizing optimal hierarchical policy inference on graphs via non-equilibrium thermodynamics. (arXiv:1801.00048v1 [cs.SY])</title>
<link>http://arxiv.org/abs/1801.00048</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchies are of fundamental interest in both stochastic optimal control
and biological control due to their facilitation of a range of desirable
computational traits in a control algorithm and the possibility that they may
form a core principle of sensorimotor and cognitive control systems. However, a
theoretically justified construction of state-space hierarchies over all
spatial resolutions and their evolution through a policy inference process
remains elusive. Here, a formalism for deriving such normative representations
of discrete Markov decision processes is introduced in the context of graphs.
The resulting hierarchies correspond to a hierarchical policy inference
algorithm approximating a discrete gradient flow between state-space trajectory
densities generated by the prior and optimal policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McNamee_D/0/1/0/all/0/1&quot;&gt;Daniel McNamee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00056">
<title>f-Divergence constrained policy improvement. (arXiv:1801.00056v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.00056</link>
<description rdf:parseType="Literal">&lt;p&gt;To ensure stability of learning, state-of-the-art generalized policy
iteration algorithms augment the policy improvement step with a trust region
constraint bounding the information loss. The size of the trust region is
commonly determined by the Kullback-Leibler (KL) divergence, which not only
captures the notion of distance well but also yields closed-form solutions. In
this paper, we consider a more general class of f-divergences and derive the
corresponding policy update rules. The generic solution is expressed through
the derivative of the convex conjugate function to f and includes the KL
solution as a special case. Within the class of f-divergences, we further focus
on a one-parameter family of {\alpha}-divergences to study effects of the
choice of divergence on policy improvement. Previously known as well as new
policy updates emerge for different values of {\alpha}. We show that every type
of policy update comes with a compatible policy evaluation resulting from the
chosen f-divergence. Interestingly, the mean-squared Bellman error minimization
is closely related to policy evaluation with the Pearson $\chi^2$-divergence
penalty, while the KL divergence results in the soft-max policy update and a
log-sum-exp critic. We carry out asymptotic analysis of the solutions for
different values of {\alpha} and demonstrate the effects of using different
divergence functions on a multi-armed bandit problem and on common standard
reinforcement learning problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belousov_B/0/1/0/all/0/1&quot;&gt;Boris Belousov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1&quot;&gt;Jan Peters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00085">
<title>Learning Structural Weight Uncertainty for Sequential Decision-Making. (arXiv:1801.00085v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.00085</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning probability distributions on the weights of neural networks (NNs)
has recently proven beneficial in many applications. Bayesian methods, such as
Stein variational gradient descent (SVGD), offer an elegant framework to reason
about NN model uncertainty. However, by assuming independent Gaussian priors
for the individual NN weights (as often applied), SVGD does not impose prior
knowledge that there is often structural information (dependence) among
weights. We propose efficient posterior learning of structural weight
uncertainty, within an SVGD framework, by employing matrix variate Gaussian
priors on NN parameters. We further investigate the learned structural
uncertainty in sequential decision-making problems, including contextual
bandits and reinforcement learning. Experiments on several synthetic and real
datasets indicate the superiority of our model, compared with state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changyou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1&quot;&gt;Lawrence Carin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00102">
<title>A Compare-Propagate Architecture with Alignment Factorization for Natural Language Inference. (arXiv:1801.00102v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1801.00102</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new deep learning architecture for Natural Language
Inference (NLI). Firstly, we introduce a new compare-propagate architecture
where alignments pairs are compared and then propagated to upper layers for
enhanced representation learning. Secondly, we adopt novel factorization layers
for efficient compression of alignment vectors into scalar valued features,
which are then be used to augment the base word representations. The design of
our approach is aimed to be conceptually simple, compact and yet powerful. We
conduct experiments on three popular benchmarks, SNLI, MultiNLI and SciTail,
achieving state-of-the-art performance on all. A lightweight parameterization
of our model enjoys a $\approx 300\%$ reduction in parameter size compared to
the ESIM and DIIN, while maintaining competitive performance. Visual analysis
shows that our propagated features are highly interpretable, opening new
avenues to explainability in neural NLI models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1&quot;&gt;Yi Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1&quot;&gt;Luu Anh Tuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1&quot;&gt;Siu Cheung Hui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00345">
<title>Users Constraints in Itemset Mining. (arXiv:1801.00345v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.00345</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering significant itemsets is one of the fundamental problems in data
mining. It has recently been shown that constraint programming is a flexible
way to tackle data mining tasks. With a constraint programming approach, we can
easily express and efficiently answer queries with users constraints on items.
However, in many practical cases it is possible that queries also express users
constraints on the dataset itself. For instance, asking for a particular
itemset in a particular part of the dataset. This paper presents a general
constraint programming model able to handle any kind of query on the items or
the dataset for itemset mining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bessiere_C/0/1/0/all/0/1&quot;&gt;Christian Bessiere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazaar_N/0/1/0/all/0/1&quot;&gt;Nadjib Lazaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lebbah_Y/0/1/0/all/0/1&quot;&gt;Yahia Lebbah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maamar_M/0/1/0/all/0/1&quot;&gt;Mehdi Maamar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00356">
<title>How will the Internet of Things enable Augmented Personalized Health?. (arXiv:1801.00356v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1801.00356</link>
<description rdf:parseType="Literal">&lt;p&gt;Internet-of-Things (IoT) is profoundly redefining the way we create, consume,
and share information. Health aficionados and citizens are increasingly using
IoT technologies to track their sleep, food intake, activity, vital body
signals, and other physiological observations. This is complemented by IoT
systems that continuously collect health-related data from the environment and
inside the living quarters. Together, these have created an opportunity for a
new generation of healthcare solutions. However, interpreting data to
understand an individual&apos;s health is challenging. It is usually necessary to
look at that individual&apos;s clinical record and behavioral information, as well
as social and environmental information affecting that individual. Interpreting
how well a patient is doing also requires looking at his adherence to
respective health objectives, application of relevant clinical knowledge and
the desired outcomes.
&lt;/p&gt;
&lt;p&gt;We resort to the vision of Augmented Personalized Healthcare (APH) to exploit
the extensive variety of relevant data and medical knowledge using Artificial
Intelligence (AI) techniques to extend and enhance human health to presents
various stages of augmented health management strategies: self-monitoring,
self-appraisal, self-management, intervention, and disease progress tracking
and prediction. kHealth technology, a specific incarnation of APH, and its
application to Asthma and other diseases are used to provide illustrations and
discuss alternatives for technology-assisted health management. Several
prominent efforts involving IoT and patient-generated health data (PGHD) with
respect converting multimodal data into actionable information (big data to
smart data) are also identified. Roles of three components in an evidence-based
semantic perception approach- Contextualization, Abstraction, and
Personalization are discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1&quot;&gt;Amit Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaimini_U/0/1/0/all/0/1&quot;&gt;Utkarshani Jaimini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yip_H/0/1/0/all/0/1&quot;&gt;Hong Yung Yip&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00361">
<title>SenseNet: 3D Objects Database and Tactile Simulator. (arXiv:1801.00361v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.00361</link>
<description rdf:parseType="Literal">&lt;p&gt;The majority of artificial intelligence research, as it relates from which to
biological senses has been focused on vision. The recent explosion of machine
learning and in particular, dee p learning, can be partially attributed to the
release of high quality data sets for algorithm s from which to model the world
on. Thus, most of these datasets are comprised of images. We believe that
focusing on sensorimotor systems and tactile feedback will create algorithms
that better mimic human intelligence. Here we present SenseNet: a collection of
tactile simulators and a large scale dataset of 3D objects for manipulation.
SenseNet was created for the purpose of researching and training Artificial
Intelligences (AIs) to interact with the environment via sensorimotor neural
systems and tactile feedback. We aim to accelerate that same explosion in image
processing, but for the domain of tactile feedback and sensorimotor research.
We hope that SenseNet can offer researchers in both the machine learning and
computational neuroscience communities brand new opportunities and avenues to
explore.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toy_J/0/1/0/all/0/1&quot;&gt;Jason Toy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00453">
<title>Automated rating of recorded classroom presentations using speech analysis in kazakh. (arXiv:1801.00453v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1801.00453</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective presentation skills can help to succeed in business, career and
academy. This paper presents the design of speech assessment during the oral
presentation and the algorithm for speech evaluation based on criteria of
optimal intonation. As the pace of the speech and its optimal intonation varies
from language to language, developing an automatic identification of language
during the presentation is required. Proposed algorithm was tested with
presentations delivered in Kazakh language. For testing purposes the features
of Kazakh phonemes were extracted using MFCC and PLP methods and created a
Hidden Markov Model (HMM) [5], [5] of Kazakh phonemes. Kazakh vowel formants
were defined and the correlation between the deviation rate in fundamental
frequency and the liveliness of the speech to evaluate intonation of the
presentation was analyzed. It was established that the threshold value between
monotone and dynamic speech is 0.16 and the error for intonation evaluation is
19%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izbassarova_A/0/1/0/all/0/1&quot;&gt;Akzharkyn Izbassarova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irmanova_A/0/1/0/all/0/1&quot;&gt;Aidana Irmanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_A/0/1/0/all/0/1&quot;&gt;A. P. James&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00602">
<title>Accurate reconstruction of image stimuli from human fMRI based on the decoding model with capsule network architecture. (arXiv:1801.00602v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.00602</link>
<description rdf:parseType="Literal">&lt;p&gt;In neuroscience, all kinds of computation models were designed to answer the
open question of how sensory stimuli are encoded by neurons and conversely, how
sensory stimuli can be decoded from neuronal activities. Especially, functional
Magnetic Resonance Imaging (fMRI) studies have made many great achievements
with the rapid development of the deep network computation. However, comparing
with the goal of decoding orientation, position and object category from
activities in visual cortex, accurate reconstruction of image stimuli from
human fMRI is a still challenging work. In this paper, the capsule network
(CapsNet) architecture based visual reconstruction (CNAVR) method is developed
to reconstruct image stimuli. The capsule means containing a group of neurons
to perform the better organization of feature structure and representation,
inspired by the structure of cortical mini column including several hundred
neurons in primates. The high-level capsule features in the CapsNet includes
diverse features of image stimuli such as semantic class, orientation, location
and so on. We used these features to bridge between human fMRI and image
stimuli. We firstly employed the CapsNet to train the nonlinear mapping from
image stimuli to high-level capsule features, and from high-level capsule
features to image stimuli again in an end-to-end manner. After estimating the
serviceability of each voxel by encoding performance to accomplish the
selecting of voxels, we secondly trained the nonlinear mapping from
dimension-decreasing fMRI data to high-level capsule features. Finally, we can
predict the high-level capsule features with fMRI data, and reconstruct image
stimuli with the CapsNet. We evaluated the proposed CNAVR method on the dataset
of handwritten digital images, and exceeded about 10% than the accuracy of all
existing state-of-the-art methods on the structural similarity index (SSIM).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1&quot;&gt;Kai Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Linyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bin Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1&quot;&gt;Lei Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1&quot;&gt;Li Tong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00631">
<title>Deep Learning: A Critical Appraisal. (arXiv:1801.00631v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.00631</link>
<description rdf:parseType="Literal">&lt;p&gt;Although deep learning has historical roots going back decades, neither the
term &quot;deep learning&quot; nor the approach was popular just over five years ago,
when the field was reignited by papers such as Krizhevsky, Sutskever and
Hinton&apos;s now classic (2012) deep network model of Imagenet. What has the field
discovered in the five subsequent years? Against a background of considerable
progress in areas such as speech recognition, image recognition, and game
playing, and considerable enthusiasm in the popular press, I present ten
concerns for deep learning, and suggest that deep learning must be supplemented
by other techniques if we are to reach artificial general intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcus_G/0/1/0/all/0/1&quot;&gt;Gary Marcus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00688">
<title>Learning audio and image representations with bio-inspired trainable feature extractors. (arXiv:1801.00688v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.00688</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in pattern recognition and signal processing concern the
automatic learning of data representations from labeled training samples.
Typical approaches are based on deep learning and convolutional neural
networks, which require large amount of labeled training samples. In this work,
we propose novel feature extractors that can be used to learn the
representation of single prototype samples in an automatic configuration
process. We employ the proposed feature extractors in applications of audio and
image processing, and show their effectiveness on benchmark data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strisciuglio_N/0/1/0/all/0/1&quot;&gt;Nicola Strisciuglio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00690">
<title>DeepMind Control Suite. (arXiv:1801.00690v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.00690</link>
<description rdf:parseType="Literal">&lt;p&gt;The DeepMind Control Suite is a set of continuous control tasks with a
standardised structure and interpretable rewards, intended to serve as
performance benchmarks for reinforcement learning agents. The tasks are written
in Python and powered by the MuJoCo physics engine, making them easy to use and
modify. We include benchmarks for several learning algorithms. The Control
Suite is publicly available at https://www.github.com/deepmind/dm_control. A
video summary of all tasks is available at &lt;a href=&quot;http://youtu.be/rAai4QzcYbs&quot;&gt;this http URL&lt;/a&gt; .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tassa_Y/0/1/0/all/0/1&quot;&gt;Yuval Tassa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doron_Y/0/1/0/all/0/1&quot;&gt;Yotam Doron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muldal_A/0/1/0/all/0/1&quot;&gt;Alistair Muldal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erez_T/0/1/0/all/0/1&quot;&gt;Tom Erez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yazhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1&quot;&gt;Diego de Las Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budden_D/0/1/0/all/0/1&quot;&gt;David Budden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdolmaleki_A/0/1/0/all/0/1&quot;&gt;Abbas Abdolmaleki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merel_J/0/1/0/all/0/1&quot;&gt;Josh Merel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lefrancq_A/0/1/0/all/0/1&quot;&gt;Andrew Lefrancq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lillicrap_T/0/1/0/all/0/1&quot;&gt;Timothy Lillicrap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1&quot;&gt;Martin Riedmiller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00702">
<title>A total uncertainty measure for D numbers based on belief intervals. (arXiv:1801.00702v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.00702</link>
<description rdf:parseType="Literal">&lt;p&gt;As a generalization of Dempster-Shafer theory, the theory of D numbers is a
new theoretical framework for uncertainty reasoning. Measuring the uncertainty
of knowledge or information represented by D numbers is an unsolved issue in
that theory. In this paper, inspired by distance based uncertainty measures for
Dempster-Shafer theory, a total uncertainty measure for a D number is proposed
based on its belief intervals. The proposed total uncertainty measure can
simultaneously capture the discord, and non-specificity, and non-exclusiveness
involved in D numbers. And some basic properties of this total uncertainty
measure, including range, monotonicity, generalized set consistency, are also
presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xinyang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wen Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00712">
<title>Multi-Objective Vehicle Routing Problem Applied to Large Scale Post Office Deliveries. (arXiv:1801.00712v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.00712</link>
<description rdf:parseType="Literal">&lt;p&gt;The number of optimization techniques in the combinatorial domain is large
and diversified. Nevertheless, real-world based benchmarks for testing
algorithms are few. This work creates an extensible real-world mail delivery
benchmark to the Vehicle Routing Problem (VRP) in a planar graph embedded in
the 2D Euclidean space. Such problem is multi-objective on a roadmap with up to
25 vehicles and 30,000 deliveries per day. Each instance models one generic day
of mail delivery, allowing both comparison and validation of optimization
algorithms for routing problems. The benchmark may be extended to model other
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meira_L/0/1/0/all/0/1&quot;&gt;Luis A. A. Meira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1&quot;&gt;Paulo S. Martins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menzori_M/0/1/0/all/0/1&quot;&gt;Mauro Menzori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeni_G/0/1/0/all/0/1&quot;&gt;Guilherme A. Zeni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00727">
<title>Accounting for hidden common causes when infering cause and effect from observational data. (arXiv:1801.00727v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.00727</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying causal relationships from observation data is difficult, in large
part, due to the presence of hidden common causes. In some cases, where just
the right patterns of conditional independence and dependence lie in the
data---for example, Y-structures---it is possible to identify cause and effect.
In other cases, the analyst deliberately makes an uncertain assumption that
hidden common causes are absent, and infers putative causal relationships to be
tested in a randomized trial. Here, we consider a third approach, where there
are sufficient clues in the data such that hidden common causes can be
inferred.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1&quot;&gt;David Heckerman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00743">
<title>Um Sistema Multiagente no Combate ao Braqueamento de Capitais. (arXiv:1801.00743v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1801.00743</link>
<description rdf:parseType="Literal">&lt;p&gt;Money laundering is a crime that makes it possible to finance other crimes,
for this reason, it is important for criminal organizations and their combat is
prioritized by nations around the world. The anti-money laundering process has
not evolved as expected because it has prioritized only the signaling of
suspicious transactions. The constant increasing in the volume of transactions
has overloaded the indispensable human work of final evaluation of the
suspicions. This article presents a multiagent system that aims to go beyond
the capture of suspicious transactions, seeking to assist the human expert in
the analysis of suspicions. The agents created use data mining techniques to
create transactional behavioral profiles; apply rules generated in learning
process in conjunction with specific rules based on legal aspects and profiles
created to capture suspicious transactions; and analyze these suspicious
transactions indicating to the human expert those that require more detailed
analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexandre_C/0/1/0/all/0/1&quot;&gt;Claudio Alexandre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balsa_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Balsa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.07710">
<title>Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning. (arXiv:1703.07710v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.07710</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistical performance bounds for reinforcement learning (RL) algorithms can
be critical for high-stakes applications like healthcare. This paper introduces
a new framework for theoretically measuring the performance of such algorithms
called Uniform-PAC, which is a strengthening of the classical Probably
Approximately Correct (PAC) framework. In contrast to the PAC framework, the
uniform version may be used to derive high probability regret guarantees and so
forms a bridge between the two setups that has been missing in the literature.
We demonstrate the benefits of the new framework for finite-state episodic MDPs
with a new algorithm that is Uniform-PAC and simultaneously achieves optimal
regret and PAC guarantees except for a factor of the horizon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dann_C/0/1/0/all/0/1&quot;&gt;Christoph Dann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lattimore_T/0/1/0/all/0/1&quot;&gt;Tor Lattimore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1&quot;&gt;Emma Brunskill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.09393">
<title>Default Logic and Bounded Treewidth. (arXiv:1706.09393v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1706.09393</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study Reiter&apos;s propositional default logic when the
treewidth of a certain graph representation (semi-primal graph) of the input
theory is bounded. We establish a dynamic programming algorithm on tree
decompositions that decides whether a theory has a consistent stable extension
(Ext). Our algorithm can even be used to enumerate all generating defaults
(ExtEnum) that lead to stable extensions.
&lt;/p&gt;
&lt;p&gt;We show that our algorithm decides Ext in linear time in the input theory and
triple exponential time in the treewidth (so-called fixed-parameter linear
algorithm).
&lt;/p&gt;
&lt;p&gt;Further, our algorithm solves ExtEnum with a pre-computation step that is
linear in the input theory and triple exponential in the treewidth followed by
a linear delay to output solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fichte_J/0/1/0/all/0/1&quot;&gt;Johannes K. Fichte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hecher_M/0/1/0/all/0/1&quot;&gt;Markus Hecher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schindler_I/0/1/0/all/0/1&quot;&gt;Irina Schindler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.02553">
<title>Robust Computer Algebra, Theorem Proving, and Oracle AI. (arXiv:1708.02553v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1708.02553</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of superintelligent AI systems, the term &quot;oracle&quot; has two
meanings. One refers to modular systems queried for domain-specific tasks.
Another usage, referring to a class of systems which may be useful for
addressing the value alignment and AI control problems, is a superintelligent
AI system that only answers questions. The aim of this manuscript is to survey
contemporary research problems related to oracles which align with long-term
research goals of AI safety. We examine existing question answering systems and
argue that their high degree of architectural heterogeneity makes them poor
candidates for rigorous analysis as oracles. On the other hand, we identify
computer algebra systems (CASs) as being primitive examples of domain-specific
oracles for mathematics and argue that efforts to integrate computer algebra
systems with theorem provers, systems which have largely been developed
independent of one another, provide a concrete set of problems related to the
notion of provable safety that has emerged in the AI safety community. We
review approaches to interfacing CASs with theorem provers, describe
well-defined architectural deficiencies that have been identified with CASs,
and suggest possible lines of research and practical software projects for
scientists interested in AI safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarma_G/0/1/0/all/0/1&quot;&gt;Gopal P. Sarma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hay_N/0/1/0/all/0/1&quot;&gt;Nick J. Hay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10916">
<title>StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks. (arXiv:1710.10916v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10916</link>
<description rdf:parseType="Literal">&lt;p&gt;Although Generative Adversarial Networks (GANs) have shown remarkable success
in various tasks, they still face challenges in generating high quality images.
In this paper, we propose Stacked Generative Adversarial Networks (StackGAN)
aiming at generating high-resolution photo-realistic images. First, we propose
a two-stage generative adversarial network architecture, StackGAN-v1, for
text-to-image synthesis. The Stage-I GAN sketches the primitive shape and
colors of the object based on given text description, yielding low-resolution
images. The Stage-II GAN takes Stage-I results and text descriptions as inputs,
and generates high-resolution images with photo-realistic details. Second, an
advanced multi-stage generative adversarial network architecture, StackGAN-v2,
is proposed for both conditional and unconditional generative tasks. Our
StackGAN-v2 consists of multiple generators and discriminators in a tree-like
structure; images at multiple scales corresponding to the same scene are
generated from different branches of the tree. StackGAN-v2 shows more stable
training behavior than StackGAN-v1 by jointly approximating multiple
distributions. Extensive experiments demonstrate that the proposed stacked
generative adversarial networks significantly outperform other state-of-the-art
methods in generating photo-realistic images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Han Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaoting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaolei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1&quot;&gt;Dimitris Metaxas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10125">
<title>Learning to cluster in order to transfer across domains and tasks. (arXiv:1711.10125v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10125</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel method to perform transfer learning across
domains and tasks, formulating it as a problem of learning to cluster. The key
insight is that, in addition to features, we can transfer similarity
information and this is sufficient to learn a similarity function and
clustering network to perform both domain adaptation and cross-task transfer
learning. We begin by reducing categorical information to pairwise constraints,
which only considers whether two instances belong to the same class or not.
This similarity is category-agnostic and can be learned from data in the source
domain using a similarity network. We then present two novel approaches for
performing transfer learning using this similarity function. First, for
unsupervised domain adaptation, we design a new loss function to regularize
classification with a constrained clustering loss, hence learning a clustering
network with the transferred similarity metric generating the training inputs.
Second, for cross-task learning (i.e., unsupervised clustering with unseen
categories), we propose a framework to reconstruct and estimate the number of
semantic clusters, again using the clustering network. Since the similarity
network is noisy, the key is to use a robust clustering algorithm, and we show
that our formulation is more robust than the alternative constrained and
unconstrained clustering approaches. Using this method, we first show state of
the art results for the challenging cross-task problem, applied on Omniglot and
ImageNet. Our results show that we can reconstruct semantic clusters with high
accuracy. We then evaluate the performance of cross-domain transfer using
images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both
datasets. Our approach doesn&apos;t explicitly deal with domain discrepancy. If we
combine with a domain adaptation loss, it shows further improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1&quot;&gt;Yen-Chang Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1&quot;&gt;Zsolt Kira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.05812">
<title>Impossibility of deducing preferences and rationality from human policy. (arXiv:1712.05812v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.05812</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse reinforcement learning (IRL) attempts to infer human rewards or
preferences from observed behavior. However, human planning systematically
deviates from rationality. Though there has been some IRL work which assumes
humans are noisily rational, there has been little analysis of the general
problem of inferring the reward of a human of unknown rationality. The observed
behavior can, in principle, be decomposed into two components: a reward
function and a planning algorithm that maps reward function to policy. Both of
these variables have to be inferred from behaviour. This paper presents a &quot;No
Free Lunch&quot; theorem in this area, showing that, without making `normative&apos;
assumptions beyond the data, nothing about the human reward function can be
deduced from human behaviour. Unlike most No Free Lunch theorems, this cannot
be alleviated by regularising with simplicity assumptions. The simplest
hypotheses are generally degenerate. The paper will then sketch how one might
begin to use normative assumptions to get around the problem, without which
solving the general IRL problem is impossible. The reward function-planning
algorithm formalism can also be used to encode what it means for an agent to
manipulate or override human preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armstrong_S/0/1/0/all/0/1&quot;&gt;Stuart Armstrong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Mindermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06924">
<title>Safe Policy Improvement with Baseline Bootstrapping. (arXiv:1712.06924v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06924</link>
<description rdf:parseType="Literal">&lt;p&gt;A common goal in Reinforcement Learning is to derive a good strategy given a
limited batch of data. In this paper, we adopt the safe policy improvement
(SPI) approach: we compute a target policy guaranteed to perform at least as
well as a given baseline policy. Our SPI strategy, inspired by the
knows-what-it-knows paradigms, consists in bootstrapping the target policy with
the baseline policy when it does not know. We develop two computationally
efficient bootstrapping algorithms, a value-based and a policy-based, both
accompanied with theoretical SPI bounds. Three algorithm variants are proposed.
We empirically show the literature algorithms limits on a small stochastic
gridworld problem, and then demonstrate that our five algorithms not only
improve the worst case scenarios, but also the mean performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1&quot;&gt;Romain Laroche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trichelair_P/0/1/0/all/0/1&quot;&gt;Paul Trichelair&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00101">
<title>Parameter-free online learning via model selection. (arXiv:1801.00101v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.00101</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an efficient algorithmic framework for model selection in online
learning, also known as parameter-free online learning. Departing from previous
work, which has focused on highly structured function classes such as nested
balls in Hilbert space, we propose a generic meta-algorithm framework that
achieves online model selection oracle inequalities under minimal structural
assumptions. We give the first computationally efficient parameter-free
algorithms that work in arbitrary Banach spaces under mild smoothness
assumptions; previous results applied only to Hilbert spaces. We further derive
new oracle inequalities for matrix classes, non-nested convex sets, and
$\mathbb{R}^{d}$ with generic regularizers. Finally, we generalize these
results by providing oracle inequalities for arbitrary non-linear classes in
the online supervised learning model. These results are all derived through a
unified meta-algorithm scheme using a novel &quot;multi-scale&quot; algorithm for
prediction with expert advice based on random playout, which may be of
independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1&quot;&gt;Dylan J. Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1&quot;&gt;Satyen Kale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1&quot;&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhran_K/0/1/0/all/0/1&quot;&gt;Karthik Sridhran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00171">
<title>PAC-Bayesian Margin Bounds for Convolutional Neural Networks - Technical Report. (arXiv:1801.00171v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.00171</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently the generalisation error of deep neural networks has been analysed
through the PAC-Bayesian framework, for the case of fully connected layers. We
adapt this approach to the convolutional setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konstantinos_P/0/1/0/all/0/1&quot;&gt;Pitas Konstantinos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_M/0/1/0/all/0/1&quot;&gt;Mike Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vandergheynst_P/0/1/0/all/0/1&quot;&gt;Pierre Vandergheynst&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00185">
<title>A dynamic network model with persistent links and node-specific latent variables, with an application to the interbank market. (arXiv:1801.00185v1 [cs.SI])</title>
<link>http://arxiv.org/abs/1801.00185</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a dynamic network model where two mechanisms control the
probability of a link between two nodes: (i) the existence or absence of this
link in the past, and (ii) node-specific latent variables (dynamic fitnesses)
describing the propensity of each node to create links. Assuming a Markov
dynamics for both mechanisms, we propose an Expectation-Maximization algorithm
for model estimation and inference of the latent variables. The estimated
parameters and fitnesses can be used to forecast the presence of a link in the
future. We apply our methodology to the e-MID interbank network for which the
two linkage mechanisms are associated with two different trading behaviors in
the process of network formation, namely preferential trading and trading
driven by node-specific characteristics. The empirical results allow to
recognise preferential lending in the interbank market and indicate how a
method that does not account for time-varying network topologies tends to
overestimate preferential linkage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazzarisi_P/0/1/0/all/0/1&quot;&gt;Piero Mazzarisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barucca_P/0/1/0/all/0/1&quot;&gt;Paolo Barucca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lillo_F/0/1/0/all/0/1&quot;&gt;Fabrizio Lillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tantari_D/0/1/0/all/0/1&quot;&gt;Daniele Tantari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00203">
<title>Molecular enhanced sampling with autoencoders: On-the-fly collective variable discovery and accelerated free energy landscape exploration. (arXiv:1801.00203v1 [physics.bio-ph])</title>
<link>http://arxiv.org/abs/1801.00203</link>
<description rdf:parseType="Literal">&lt;p&gt;Macromolecular and biomolecular folding landscapes typically contain high
free energy barriers that impede efficient sampling of configurational space by
standard molecular dynamics simulation. Biased sampling can artificially drive
the simulation along pre-specified collective variables (CVs), but success
depends critically on the availability of good CVs associated with the
important collective dynamical motions. Nonlinear machine learning techniques
can identify such CVs but typically do not furnish an explicit relationship
with the atomic coordinates necessary to perform biased sampling. In this work,
we employ auto-associative artificial neural networks (&quot;autoencoders&quot;) to learn
nonlinear CVs that are explicit and differentiable functions of the atomic
coordinates. Our approach offers substantial speedups in exploration of
configurational space, and is distinguished from exiting approaches by its
capacity to simultaneously discover and directly accelerate along data-driven
CVs. We demonstrate the approach in simulations of alanine dipeptide and
Trp-cage, and have developed an open-source and freely-available implementation
within OpenMM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ferguson_A/0/1/0/all/0/1&quot;&gt;Andrew L Ferguson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00283">
<title>Restricted Boltzmann Machines for Robust and Fast Latent Truth Discovery. (arXiv:1801.00283v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.00283</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of latent truth discovery, LTD for short, where the
goal is to discover the underlying true values of entity attributes in the
presence of noisy, conflicting or incomplete information. Despite a multitude
of algorithms to address the LTD problem that can be found in literature, only
little is known about their overall performance with respect to effectiveness
(in terms of truth discovery capabilities), efficiency and robustness. A
practical LTD approach should satisfy all these characteristics so that it can
be applied to heterogeneous datasets of varying quality and degrees of
cleanliness.
&lt;/p&gt;
&lt;p&gt;We propose a novel algorithm for LTD that satisfies the above requirements.
The proposed model is based on Restricted Boltzmann Machines, thus coined
LTD-RBM. In extensive experiments on various heterogeneous and publicly
available datasets, LTD-RBM is superior to state-of-the-art LTD techniques in
terms of an overall consideration of effectiveness, efficiency and robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Broelemann_K/0/1/0/all/0/1&quot;&gt;Klaus Broelemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottron_T/0/1/0/all/0/1&quot;&gt;Thomas Gottron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1&quot;&gt;Gjergji Kasneci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00315">
<title>Learning Relevant Features of Data with Multi-scale Tensor Networks. (arXiv:1801.00315v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.00315</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by coarse-graining approaches used in physics, we show how similar
algorithms can be adapted for data. The resulting algorithms are based on
layered tree tensor networks and scale linearly with both the dimension of the
input and the training set size. Computing most of the layers with an
unsupervised algorithm, then optimizing just the top layer for supervised
classification of the MNIST and fashion-MNIST data sets gives very good
results. We also discuss mixing a prior guess for supervised weights together
with an unsupervised representation of the data, yielding a smaller number of
features nevertheless able to give good performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stoudenmire_E/0/1/0/all/0/1&quot;&gt;E.M. Stoudenmire&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00329">
<title>ZOOpt/ZOOjl: Toolbox for Derivative-Free Optimization. (arXiv:1801.00329v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.00329</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances of derivative-free optimization allow efficient approximating
the global optimal solutions of sophisticated functions, such as functions with
many local optima, non-differentiable and non-continuous functions. This
article describes the ZOOpt/ZOOjl toolbox that provides efficient
derivative-free solvers and are designed easy to use. ZOOpt provides a Python
package for single-thread optimization, and ZOOjl provides a distributed
version with the help of the Julia language for Python described functions.
ZOOpt/ZOOjl toolbox particularly focuses on optimization problems in machine
learning, addressing high-dimensional, noisy, and large-scale problems. The
toolbox is being maintained toward ready-to-use tools in real-world machine
learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu-Ren Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yi-Qi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1&quot;&gt;Hong Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chao Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00364">
<title>Estimation and Inference of Treatment Effects with $L_2$-Boosting in High-Dimensional Settings. (arXiv:1801.00364v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.00364</link>
<description rdf:parseType="Literal">&lt;p&gt;Boosting algorithms are very popular in Machine Learning and have proven very
useful for prediction and variable selection. Nevertheless in many applications
the researcher is interested in inference on treatment effects or policy
variables in a high-dimensional setting. Empirical researchers are more and
more faced with rich datasets containing very many controls or instrumental
variables, where variable selection is challenging. In this paper we give
results for the valid inference of a treatment effect after selecting from
among very many control variables and the estimation of instrumental variables
with potentially very many instruments when post- or orthogonal $L_2$-Boosting
is used for the variable selection. This setting allows for valid inference on
low-dimensional components in a regression estimated with $L_2$-Boosting. We
give simulation results for the proposed methods and an empirical application,
in which we analyze the effectiveness of a pulmonary artery catheter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Ye Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spindler_M/0/1/0/all/0/1&quot;&gt;Martin Spindler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00393">
<title>Theoretical Analysis of Sparse Subspace Clustering with Missing Entries. (arXiv:1801.00393v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.00393</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse Subspace Clustering (SSC) is a popular unsupervised machine learning
method for clustering data lying close to a union of low-dimensional linear
subspaces; a problem with numerous applications in pattern recognition and
computer vision. Even though the behavior of SSC for uncorrupted data is by now
well-understood, little is known about its theoretical properties when applied
to data with missing entries. In this paper we give the first interpretable
theoretical guarantees for SSC with incomplete data, and analytically establish
that projecting the zero-filled data onto the observation patten of the point
being expressed leads to a substantial improvement in performance. Since the
projection induces further missing entries, this is a remarkable phenomenon,
whose significance potentially extends to the entire class of self-expressive
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsakiris_M/0/1/0/all/0/1&quot;&gt;Manolis C. Tsakiris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1&quot;&gt;Rene Vidal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00513">
<title>An elementary derivation of the Chinese restaurant process from Sethuraman&apos;s stick-breaking process. (arXiv:1801.00513v1 [math.ST])</title>
<link>http://arxiv.org/abs/1801.00513</link>
<description rdf:parseType="Literal">&lt;p&gt;The Chinese restaurant process and the stick-breaking process are the two
most commonly used representations of the Dirichlet process. However, the usual
proof of the connection between them is indirect, relying on abstract
properties of the Dirichlet process that are difficult for nonexperts to
verify. This short note provides a direct proof that the stick-breaking process
gives rise to the Chinese restaurant process, without using any measure theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Miller_J/0/1/0/all/0/1&quot;&gt;Jeffrey W. Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00636">
<title>Transferable neural networks for enhanced sampling of protein dynamics. (arXiv:1801.00636v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.00636</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational auto-encoder frameworks have demonstrated success in reducing
complex nonlinear dynamics in molecular simulation to a single non-linear
embedding. In this work, we illustrate how this non-linear latent embedding can
be used as a collective variable for enhanced sampling, and present a simple
modification that allows us to rapidly perform sampling in multiple related
systems. We first demonstrate our method is able to describe the effects of
force field changes in capped alanine dipeptide after learning a model using
AMBER99. We further provide a simple extension to variational dynamics encoders
that allows the model to be trained in a more efficient manner on larger
systems by encoding the outputs of a linear transformation using time-structure
based independent component analysis (tICA). Using this technique, we show how
such a model trained for one protein, the WW domain, can efficiently be
transferred to perform enhanced sampling on a related mutant protein, the GTT
mutation. This method shows promise for its ability to rapidly sample related
systems using a single transferable collective variable and is generally
applicable to sets of related simulations, enabling us to probe the effects of
variation in increasingly large systems of biophysical interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sultan_M/0/1/0/all/0/1&quot;&gt;Mohammad M. Sultan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wayment_Steele_H/0/1/0/all/0/1&quot;&gt;Hannah K. Wayment-Steele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pande_V/0/1/0/all/0/1&quot;&gt;Vijay S. Pande&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00668">
<title>Random Euler Complex-Valued Nonlinear Filters. (arXiv:1801.00668v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.00668</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the last decade, both the neural network and kernel adaptive filter have
successfully been used for nonlinear signal processing. However, they suffer
from high computational cost caused by their complex/growing network
structures. In this paper, we propose two random Euler filters for
complex-valued nonlinear filtering problem, i.e., linear random Euler
complex-valued filter (LRECF) and its widely-linear version (WLRECF), which
possess a simple and fixed network structure. The transient and steady-state
performances are studied in a non-stationary environment. The analytical
minimum mean square error (MSE) and optimum step-size are derived. Finally,
numerical simulations on complex-valued nonlinear system identification and
nonlinear channel equalization are presented to show the effectiveness of the
proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiashu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Defang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00681">
<title>A novel improved fuzzy support vector machine based stock price trend forecast model. (arXiv:1801.00681v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.00681</link>
<description rdf:parseType="Literal">&lt;p&gt;Application of fuzzy support vector machine in stock price forecast. Support
vector machine is a new type of machine learning method proposed in 1990s. It
can deal with classification and regression problems very successfully. Due to
the excellent learning performance of support vector machine, the technology
has become a hot research topic in the field of machine learning, and it has
been successfully applied in many fields. However, as a new technology, there
are many limitations to support vector machines. There is a large amount of
fuzzy information in the objective world. If the training of support vector
machine contains noise and fuzzy information, the performance of the support
vector machine will become very weak and powerless. As the complexity of many
factors influence the stock price prediction, the prediction results of
traditional support vector machine cannot meet people with precision, this
study improved the traditional support vector machine fuzzy prediction
algorithm is proposed to improve the new model precision. NASDAQ Stock Market,
Standard &amp;amp; Poor&apos;s (S&amp;amp;P) Stock market are considered. Novel advanced- fuzzy
support vector machine (NA-FSVM) is the proposed methodology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guohao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bao_Y/0/1/0/all/0/1&quot;&gt;Yifan Bao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1404.4178">
<title>Speeding Up MCMC by Efficient Data Subsampling. (arXiv:1404.4178v6 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1404.4178</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Subsampling MCMC, a Markov Chain Monte Carlo (MCMC) framework
where the likelihood function for $n$ observations is estimated from a random
subset of $m$ observations. We introduce a highly efficient unbiased estimator
of the log-likelihood based on control variates, such that the computing cost
is much smaller than that of the full log-likelihood in standard MCMC. The
likelihood estimate is bias-corrected and used in two dependent pseudo-marginal
algorithms to sample from a perturbed posterior, for which we derive the
asymptotic error with respect to $n$ and $m$, respectively. We propose a
practical estimator of the error and show that the error is negligible even for
a very small $m$ in our applications. We demonstrate that Subsampling MCMC is
substantially more efficient than standard MCMC in terms of sampling efficiency
for a given computational budget, and that it outperforms other subsampling
methods for MCMC proposed in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Quiroz_M/0/1/0/all/0/1&quot;&gt;Matias Quiroz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohn_R/0/1/0/all/0/1&quot;&gt;Robert Kohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Villani_M/0/1/0/all/0/1&quot;&gt;Mattias Villani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Minh-Ngoc Tran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1411.0589">
<title>Modular proximal optimization for multidimensional total-variation regularization. (arXiv:1411.0589v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1411.0589</link>
<description rdf:parseType="Literal">&lt;p&gt;We study \emph{TV regularization}, a widely used technique for eliciting
structured sparsity. In particular, we propose efficient algorithms for
computing prox-operators for $\ell_p$-norm TV. The most important among these
is $\ell_1$-norm TV, for whose prox-operator we present a new geometric
analysis which unveils a hitherto unknown connection to taut-string methods.
This connection turns out to be remarkably useful as it shows how our geometry
guided implementation results in efficient weighted and unweighted 1D-TV
solvers, surpassing state-of-the-art methods. Our 1D-TV solvers provide the
backbone for building more complex (two or higher-dimensional) TV solvers
within a modular proximal optimization approach. We review the literature for
an array of methods exploiting this strategy, and illustrate the benefits of
our modular design through extensive suite of experiments on (i) image
denoising, (ii) image deconvolution, (iii) four variants of fused-lasso, and
(iv) video denoising. To underscore our claims and permit easy reproducibility,
we provide all the reviewed and our new TV solvers in an easy to use
multi-threaded C++, Matlab and Python library.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barbero_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;lvaro Barbero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sra_S/0/1/0/all/0/1&quot;&gt;Suvrit Sra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1603.04981">
<title>Repeated Games with Vector Losses: A Set-valued Dynamic Programming Approach. (arXiv:1603.04981v3 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/1603.04981</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider infinitely repeated games with vector losses discounted over
time. We characterize the set of minimal upper bounds on expected losses that a
player can simultaneously guarantee across the different dimensions.
Specifically, we show that this set is the fixed point of a set-valued dynamic
programming operator. This approach also characterizes the strategies that
achieve these bounds. These optimal strategies are shown to be independent of
the player&apos;s own past actions and stationary relative to a compact state space
obtained by parameterizing the set of the minimal bounds. We also present a
computational procedure to approximate this set and the optimal strategies.
&lt;/p&gt;
&lt;p&gt;We discuss two applications of our results: 1) characterization of the
optimal strategy of the uninformed player in zero-sum discounted repeated games
with incomplete information on one side; 2) characterization of the minmax
optimal regret and the regret-optimal strategy in repeated games with
discounted losses. Our approximation procedure can be used to compute
approximately optimal strategies in both these applications. We illustrate this
procedure by computing approximately regret-optimal strategies for the problem
of prediction using expert advice from two and three experts under
$\{0,1\}-$losses. Our numerical evaluations demonstrate improved performance
over existing algorithms for this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamble_V/0/1/0/all/0/1&quot;&gt;Vijay Kamble&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loiseau_P/0/1/0/all/0/1&quot;&gt;Patrick Loiseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walrand_J/0/1/0/all/0/1&quot;&gt;Jean Walrand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1604.02181">
<title>A Unified Framework for Sparse Non-Negative Least Squares using Multiplicative Updates and the Non-Negative Matrix Factorization Problem. (arXiv:1604.02181v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1604.02181</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the sparse non-negative least squares (S-NNLS) problem. S-NNLS
occurs naturally in a wide variety of applications where an unknown,
non-negative quantity must be recovered from linear measurements. We present a
unified framework for S-NNLS based on a rectified power exponential scale
mixture prior on the sparse codes. We show that the proposed framework
encompasses a large class of S-NNLS algorithms and provide a computationally
efficient inference procedure based on multiplicative update rules. Such update
rules are convenient for solving large sets of S-NNLS problems simultaneously,
which is required in contexts like sparse non-negative matrix factorization
(S-NMF). We provide theoretical justification for the proposed approach by
showing that the local minima of the objective function being optimized are
sparse and the S-NNLS algorithms presented are guaranteed to converge to a set
of stationary points of the objective function. We then extend our framework to
S-NMF, showing that our framework leads to many well known S-NMF algorithms
under specific choices of prior and providing a guarantee that a popular
subclass of the proposed algorithms converges to a set of stationary points of
the objective function. Finally, we study the performance of the proposed
approaches on synthetic and real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fedorov_I/0/1/0/all/0/1&quot;&gt;Igor Fedorov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nalci_A/0/1/0/all/0/1&quot;&gt;Alican Nalci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Giri_R/0/1/0/all/0/1&quot;&gt;Ritwik Giri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rao_B/0/1/0/all/0/1&quot;&gt;Bhaskar D. Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Truong Q. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garudadri_H/0/1/0/all/0/1&quot;&gt;Harinath Garudadri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.00048">
<title>Practical sketching algorithms for low-rank matrix approximation. (arXiv:1609.00048v2 [cs.NA] UPDATED)</title>
<link>http://arxiv.org/abs/1609.00048</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes a suite of algorithms for constructing low-rank
approximations of an input matrix from a random linear image of the matrix,
called a sketch. These methods can preserve structural properties of the input
matrix, such as positive-semidefiniteness, and they can produce approximations
with a user-specified rank. The algorithms are simple, accurate, numerically
stable, and provably correct. Moreover, each method is accompanied by an
informative error bound that allows users to select parameters a priori to
achieve a given approximation quality. These claims are supported by numerical
experiments with real and synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tropp_J/0/1/0/all/0/1&quot;&gt;Joel A. Tropp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yurtsever_A/0/1/0/all/0/1&quot;&gt;Alp Yurtsever&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1&quot;&gt;Madeleine Udell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.07574">
<title>Dynamic Pricing in High-dimensions. (arXiv:1609.07574v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1609.07574</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the pricing problem faced by a firm that sells a large number of
products, described via a wide range of features, to customers that arrive over
time. Customers independently make purchasing decisions according to a general
choice model that includes products features and customers&apos; characteristics,
encoded as $d$-dimensional numerical vectors, as well as the price offered. The
parameters of the choice model are a priori unknown to the firm, but can be
learned as the (binary-valued) sales data accrues over time. The firm&apos;s
objective is to minimize the regret, i.e., the expected revenue loss against a
clairvoyant policy that knows the parameters of the choice model in advance,
and always offers the revenue-maximizing price. This setting is motivated in
part by the prevalence of online marketplaces that allow for real-time pricing.
We assume a structured choice model, parameters of which depend on $s_0$ out of
the $d$ product features. We propose a dynamic policy, called Regularized
Maximum Likelihood Pricing (RMLP) that leverages the (sparsity) structure of
the high-dimensional model and obtains a logarithmic regret in $T$. More
specifically, the regret of our algorithm is of $O(s_0 \log d \cdot \log T)$.
Furthermore, we show that no policy can obtain regret better than $O(s_0 (\log
d + \log T))$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Javanmard_A/0/1/0/all/0/1&quot;&gt;Adel Javanmard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nazerzadeh_H/0/1/0/all/0/1&quot;&gt;Hamid Nazerzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.01968">
<title>Max-value Entropy Search for Efficient Bayesian Optimization. (arXiv:1703.01968v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.01968</link>
<description rdf:parseType="Literal">&lt;p&gt;Entropy Search (ES) and Predictive Entropy Search (PES) are popular and
empirically successful Bayesian Optimization techniques. Both rely on a
compelling information-theoretic motivation, and maximize the information
gained about the $\arg\max$ of the unknown function; yet, both are plagued by
the expensive computation for estimating entropies. We propose a new criterion,
Max-value Entropy Search (MES), that instead uses the information about the
maximum function value. We show relations of MES to other Bayesian optimization
methods, and establish a regret bound. We observe that MES maintains or
improves the good empirical performance of ES/PES, while tremendously
lightening the computational burden. In particular, MES is much more robust to
the number of samples used for computing the entropy, and hence more efficient
for higher dimensional problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.01445">
<title>Batched Large-scale Bayesian Optimization in High-dimensional Spaces. (arXiv:1706.01445v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.01445</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization (BO) has become an effective approach for black-box
function optimization problems when function evaluations are expensive and the
optimum can be achieved within a relatively small number of queries. However,
many cases, such as the ones with high-dimensional inputs, may require a much
larger number of observations for optimization. Despite an abundance of
observations thanks to parallel experiments, current BO techniques have been
limited to merely a few thousand observations. In this paper, we propose
ensemble Bayesian optimization (EBO) to address three current challenges in BO
simultaneously: (1) large-scale observations; (2) high dimensional input
spaces; and (3) selections of batch queries that balance quality and diversity.
The key idea of EBO is to operate on an ensemble of additive Gaussian process
models, each of which possesses a randomized strategy to divide and conquer. We
show unprecedented, previously impossible results of scaling up BO to tens of
thousands of observations within minutes of computation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gehring_C/0/1/0/all/0/1&quot;&gt;Clement Gehring&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohli_P/0/1/0/all/0/1&quot;&gt;Pushmeet Kohli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jegelka_S/0/1/0/all/0/1&quot;&gt;Stefanie Jegelka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.03663">
<title>Underdamped Langevin MCMC: A non-asymptotic analysis. (arXiv:1707.03663v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.03663</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the underdamped Langevin diffusion when the log of the target
distribution is smooth and strongly concave. We present a MCMC algorithm based
on its discretization and show that it achieves $\varepsilon$ error (in
2-Wasserstein distance) in $\mathcal{O}(\sqrt{d}/\varepsilon)$ steps. This is a
significant improvement over the best known rate for overdamped Langevin MCMC,
which is $\mathcal{O}(d/\varepsilon^2)$ steps under the same
smoothness/concavity assumptions.
&lt;/p&gt;
&lt;p&gt;The underdamped Langevin MCMC scheme can be viewed as a version of
Hamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped
Langevin MCMC methods in a number of application areas. We provide quantitative
rates that support this empirical wisdom.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1&quot;&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1&quot;&gt;Peter L. Bartlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.01977">
<title>Why Adaptively Collected Data Have Negative Bias and How to Correct for It. (arXiv:1708.01977v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.01977</link>
<description rdf:parseType="Literal">&lt;p&gt;From scientific experiments to online A/B testing, the previously observed
data often affects how future experiments are performed, which in turn affects
which data will be collected. Such adaptivity introduces complex correlations
between the data and the collection procedure. In this paper, we prove that
when the data collection procedure satisfies natural conditions, then sample
means of the data have systematic \emph{negative} biases. As an example,
consider an adaptive clinical trial where additional data points are more
likely to be tested for treatments that show initial promise. Our surprising
result implies that the average observed treatment effects would underestimate
the true effects of each treatment. We quantitatively analyze the magnitude and
behavior of this negative bias in a variety of settings. We also propose a
novel debiasing algorithm based on selective inference techniques. In
experiments, our method can effectively reduce bias and estimation error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nie_X/0/1/0/all/0/1&quot;&gt;Xinkun Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tian_X/0/1/0/all/0/1&quot;&gt;Xiaoying Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Taylor_J/0/1/0/all/0/1&quot;&gt;Jonathan Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10370">
<title>Topology Adaptive Graph Convolutional Networks. (arXiv:1710.10370v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10370</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolution acts as a local feature extractor in convolutional neural
networks (CNNs). However, the convolution operation is not applicable when the
input data is supported on an irregular graph such as with social networks,
citation networks, or knowledge graphs. This paper proposes the topology
adaptive graph convolutional network (TAGCN), a novel graph convolutional
network that generalizes CNN architectures to graph-structured data and
provides a systematic way to design a set of fixed-size learnable filters to
perform convolutions on graphs. The topologies of these filters are adaptive to
the topology of the graph when they scan the graph to perform convolution,
replacing the square filter for the grid-structured data in traditional CNNs.
The outputs are the weighted sum of these filters&apos; outputs, extraction of both
vertex features and strength of correlation between vertices. It can be used
with both directed and undirected graphs. The proposed TAGCN not only inherits
the properties of convolutions in CNN for grid-structured data, but it is also
consistent with convolution as defined in graph signal processing. Further, as
no approximation to the convolution is needed, TAGCN exhibits better
performance than existing graph-convolution-approximation methods on a number
of data sets. As only the polynomials of degree two of the adjacency matrix are
used, TAGCN is also computationally simpler than other recent methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jian Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shanghang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Guanhang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moura_J/0/1/0/all/0/1&quot;&gt;Jose M. F. Moura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kar_S/0/1/0/all/0/1&quot;&gt;Soummya Kar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01682">
<title>Estimation of Low-Rank Matrices via Approximate Message Passing. (arXiv:1711.01682v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01682</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider the problem of estimating a low-rank symmetric matrix when its
entries are perturbed by Gaussian noise, a setting that is also known as
`spiked model&apos; or `deformed Wigner matrix.&apos; If the empirical distribution of
the entries of the spikes is known, optimal estimators that exploit this
knowledge can substantially outperform simple spectral approaches. Recent work
characterizes the asymptotic accuracy of Bayes-optimal estimators in the
high-dimensional limit. In this paper we present a practical algorithm that can
achieve Bayes-optimal accuracy above the spectral threshold. A bold conjecture
from statistical physics posits that no polynomial-time algorithm achieves
optimal error below the same threshold (unless the best estimator is trivial).
&lt;/p&gt;
&lt;p&gt;Our approach uses Approximate Message Passing (AMP) in conjunction with a
spectral initialization. AMP algorithms have proved successful in a variety of
statistical estimation tasks, and are amenable to exact asymptotic analysis via
state evolution. Unfortunately, state evolution is uninformative when the
algorithm is initialized near an unstable fixed point, as is often happens in
low-rank matrix estimation problems. We develop a a new analysis of AMP that
allows for spectral initializations, and builds on a decoupling between the
outlier eigenvectors and the bulk in the spiked random matrix model.
&lt;/p&gt;
&lt;p&gt;Our main theorem is general and applies beyond matrix estimation. However, we
use it to derive detailed predictions for the problem of estimating a rank-one
matrix in noise. Special cases of these problem are closely related -- via
universality arguments -- to the network community detection problem for two
asymmetric communities. As a further illustration, we consider the example of a
block-constant low-rank matrix with symmetric blocks, which we refer to as
`Gaussian Block Model&apos;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Montanari_A/0/1/0/all/0/1&quot;&gt;Andrea Montanari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Venkataramanan_R/0/1/0/all/0/1&quot;&gt;Ramji Venkataramanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04802">
<title>Generic Machine Learning Inference on Heterogenous Treatment Effects in Randomized Experiments. (arXiv:1712.04802v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04802</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose strategies to estimate and make inference on key features of
heterogeneous effects in randomized experiments. These key features include
best linear predictors of the effects using machine learning proxies, average
effects sorted by impact groups, and average characteristics of most and least
impacted units. The approach is valid in high dimensional settings, where the
effects are proxied by machine learning methods. We post-process these proxies
into the estimates of the key features. Our approach is generic, it can be used
in conjunction with penalized methods, deep and shallow neural networks,
canonical and new random forests, boosted trees, and ensemble methods. Our
approach is agnostic and does not make unrealistic or hard-to-check
assumptions; we don&apos;t require conditions for consistency of the ML methods.
Estimation and inference relies on repeated data splitting to avoid overfitting
and achieve validity. For inference, we take medians of p-values and medians of
confidence intervals, resulting from many different data splits, and then
adjust their nominal level to guarantee uniform validity. This variational
inference method is shown to be uniformly valid and quantifies the uncertainty
coming from both parameter estimation and data splitting. The inference method
could be of substantial independent interest in many machine learning
applications. An empirical application to the impact of micro-credit on
economic development illustrates the use of the approach in randomized
experiments. An additional application to the impact of the gender
discrimination on wages illustrates the potential use of the approach in
observational studies, where machine learning methods can be used to condition
flexibly on very high-dimensional controls.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chernozhukov_V/0/1/0/all/0/1&quot;&gt;Victor Chernozhukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Demirer_M/0/1/0/all/0/1&quot;&gt;Mert Demirer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duflo_E/0/1/0/all/0/1&quot;&gt;Esther Duflo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fernandez_Val_I/0/1/0/all/0/1&quot;&gt;Ivan Fernandez-Val&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10110">
<title>Beyond Keywords and Relevance: A Personalized Ad Retrieval Framework in E-Commerce Sponsored Search. (arXiv:1712.10110v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1712.10110</link>
<description rdf:parseType="Literal">&lt;p&gt;On most sponsored search platforms, advertisers bid on some keywords for
their advertisements (ads). Given a search request, ad retrieval module
rewrites the query into bidding keywords, and uses these keywords as keys to
select Top N ads through inverted indexes. In this way, an ad will not be
retrieved even if queries are related when the advertiser does not bid on
corresponding keywords. Moreover, most ad retrieval approaches regard rewriting
and ad-selecting as two separated tasks, and focus on boosting relevance
between search queries and ads. Recently, in e-commerce sponsored search more
and more personalized information has been introduced, such as user profiles,
long-time and real-time clicks. Personalized information makes ad retrieval
able to employ more elements (e.g. real-time clicks) as search signals and
retrieval keys, however it makes ad retrieval more difficult to measure ads
retrieved through different signals. To address these problems, we propose a
novel ad retrieval framework beyond keywords and relevance in e-commerce
sponsored search. Firstly, we employ historical ad click data to initialize a
hierarchical network representing signals, keys and ads, in which personalized
information is introduced. Then we train a model on top of the hierarchical
network by learning the weights of edges. Finally we select the best edges
according to the model, boosting RPM/CTR. Experimental results on our
e-commerce platform demonstrate that our ad retrieval framework achieves good
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Su Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianshu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1&quot;&gt;Daorui Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kaipeng Liu&lt;/a&gt;</dc:creator>
</item></rdf:RDF>