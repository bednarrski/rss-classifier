<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-09-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03646"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03721"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03864"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05488"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03008"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03541"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03609"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03656"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03695"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03928"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03979"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01700"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09756"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.10139"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03534"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03538"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03548"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03561"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03566"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03576"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03659"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03776"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03779"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03817"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03839"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.03986"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.03528"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08881"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.09318"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.05409"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11259"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04119"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.07379"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1809.02519"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1809.03646">
<title>Tuning metaheuristics by sequential optimization of regression models. (arXiv:1809.03646v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1809.03646</link>
<description rdf:parseType="Literal">&lt;p&gt;Tuning parameters is an important step for the application of metaheuristics
to problem classes of interest. In this work we present a tuning framework
based on the sequential optimization of perturbed regression models. Besides
providing algorithm configurations with good expected performance, the proposed
methodology can also provide insights on the relevance of each parameter and
their interactions, as well as models of expected algorithm performance for a
given problem class, conditional on the parameter values. A test case is
presented for the tuning of six parameters of a decomposition-based
multiobjective optimization algorithm, in which an instantiation of the
proposed framework is compared against the results obtained by the most recent
version the Iterated Racing (Irace) procedure. The results suggest that the
proposed approach returns solutions that are as good as those of Irace in terms
of mean performance, with the advantage of providing more information on the
relevance and effect of each parameter on the expected performance of the
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trindade_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;thila R. Trindade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campelo_F/0/1/0/all/0/1&quot;&gt;Felipe Campelo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03721">
<title>Deep Asymmetric Networks with a Set of Node-wise Variant Activation Functions. (arXiv:1809.03721v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03721</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents deep asymmetric networks with a set of node-wise variant
activation functions. The nodes&apos; sensitivities are affected by activation
function selections such that the nodes with smaller indices become
increasingly more sensitive. As a result, features learned by the nodes are
sorted by the node indices in the order of their importance. Asymmetric
networks not only learn input features but also the importance of those
features. Nodes of lesser importance in asymmetric networks can be pruned to
reduce the complexity of the networks, and the pruned networks can be retrained
without incurring performance losses. We validate the feature-sorting property
using both shallow and deep asymmetric networks as well as deep asymmetric
networks transferred from famous networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jinhyeok Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hyunjoong Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaehong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaeyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Seungjoon Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03864">
<title>Response Characterization for Auditing Cell Dynamics in Long Short-term Memory Networks. (arXiv:1809.03864v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03864</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel method to interpret recurrent neural
networks (RNNs), particularly long short-term memory networks (LSTMs) at the
cellular level. We propose a systematic pipeline for interpreting individual
hidden state dynamics within the network using response characterization
methods. The ranked contribution of individual cells to the network&apos;s output is
computed by analyzing a set of interpretable metrics of their decoupled step
and sinusoidal responses. As a result, our method is able to uniquely identify
neurons with insightful dynamics, quantify relationships between dynamical
properties and test accuracy through ablation analysis, and interpret the
impact of network capacity on a network&apos;s dynamical distribution. Finally, we
demonstrate generalizability and scalability of our method by evaluating a
series of different benchmark sequential datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1&quot;&gt;Ramin M. Hasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1&quot;&gt;Alexander Amini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1&quot;&gt;Mathias Lechner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naser_F/0/1/0/all/0/1&quot;&gt;Felix Naser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1&quot;&gt;Radu Grosu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1&quot;&gt;Daniela Rus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05488">
<title>CBinfer: Exploiting Frame-to-Frame Locality for Faster Convolutional Network Inference on Video Streams. (arXiv:1808.05488v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1808.05488</link>
<description rdf:parseType="Literal">&lt;p&gt;The last few years have brought advances in computer vision at an amazing
pace, grounded on new findings in deep neural network construction and training
as well as the availability of large labeled datasets. Applying these networks
to images demands a high computational effort and pushes the use of
state-of-the-art networks on real-time video data out of reach of embedded
platforms. Many recent works focus on reducing network complexity for real-time
inference on embedded computing platforms. We adopt an orthogonal viewpoint and
propose a novel algorithm exploiting the spatio-temporal sparsity of pixel
changes. This optimized inference procedure resulted in an average speed-up of
9.1x over cuDNN on the Tegra X2 platform at a negligible accuracy loss of &amp;lt;0.1%
and no retraining of the network for a semantic segmentation application.
Similarly, an average speed-up of 7.0x has been achieved for a pose detection
DNN on static camera video surveillance data. These throughput gains combined
with a lower power consumption result in an energy efficiency of 511 GOp/s/W
compared to 70 GOp/s/W for the baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavigelli_L/0/1/0/all/0/1&quot;&gt;Lukas Cavigelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03008">
<title>Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability. (arXiv:1809.03008v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1809.03008</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the concept of co-design in the context of neural network
verification. Specifically, we aim to train deep neural networks that not only
are robust to adversarial perturbations but also whose robustness can be
verified more easily. To this end, we identify two properties of network models
- weight sparsity and so-called ReLU stability - that turn out to significantly
impact the complexity of the corresponding verification task. We demonstrate
that improving weight sparsity alone already enables us to turn computationally
intractable verification problems into tractable ones. Then, improving ReLU
stability leads to an additional 4-13x speedup in verification times. An
important feature of our methodology is its &quot;universality,&quot; in the sense that
it can be used with a broad range of training procedures and verification
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1&quot;&gt;Kai Y. Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tjeng_V/0/1/0/all/0/1&quot;&gt;Vincent Tjeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafiullah_N/0/1/0/all/0/1&quot;&gt;Nur Muhammad Shafiullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1&quot;&gt;Aleksander Madry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03541">
<title>Bayesian Patchworks: An Approach to Case-Based Reasoning. (arXiv:1809.03541v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.03541</link>
<description rdf:parseType="Literal">&lt;p&gt;Doctors often rely on their past experience in order to diagnose patients.
For a doctor with enough experience, almost every patient would have
similarities to key cases seen in the past, and each new patient could be
viewed as a mixture of these key past cases. Because doctors often tend to
reason this way, an efficient computationally aided diagnostic tool that thinks
in the same way might be helpful in locating key past cases of interest that
could assist with diagnosis. This article develops a novel mathematical model
to mimic the type of logical thinking that physicians use when considering past
cases. The proposed model can also provide physicians with explanations that
would be similar to the way they would naturally reason about cases. The
proposed method is designed to yield predictive accuracy, computational
efficiency, and insight into medical data; the key element is the insight into
medical data, in some sense we are automating a complicated process that
physicians might perform manually. We finally implemented the result of this
work on two publicly available healthcare datasets, for heart disease
prediction and breast cancer prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghaddass_R/0/1/0/all/0/1&quot;&gt;Ramin Moghaddass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03609">
<title>URBAN-i: From urban scenes to mapping slums, transport modes, and pedestrians in cities using deep learning and computer vision. (arXiv:1809.03609v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.03609</link>
<description rdf:parseType="Literal">&lt;p&gt;Within the burgeoning expansion of deep learning and computer vision across
the different fields of science, when it comes to urban development, deep
learning and computer vision applications are still limited towards the notions
of smart cities and autonomous vehicles. Indeed, a wide gap of knowledge
appears when it comes to cities and urban regions in less developed countries
where the chaos of informality is the dominant scheme. How can deep learning
and Artificial Intelligence (AI) untangle the complexities of informality to
advance urban modelling and our understanding of cities? Various questions and
debates can be raised concerning the future of cities of the North and the
South in the paradigm of AI and computer vision. In this paper, we introduce a
new method for multipurpose realistic-dynamic urban modelling relying on deep
learning and computer vision, using deep Convolutional Neural Networks (CNN),
to sense and detect informality and slums in urban scenes from aerial and
street view images in addition to detection of pedestrian and transport modes.
The model has been trained on images of urban scenes in cities across the
globe. The model shows a good validation of understanding a wide spectrum of
nuances among the planned and the unplanned regions, including informal and
slum areas. We attempt to advance urban modelling for better understanding the
dynamics of city developments. We also aim to exemplify the significant impacts
of AI in cities beyond how smart cities are discussed and perceived in the
mainstream. The algorithms of the URBAN-i model are fully-coded in Python
programming with the pre-trained deep learning models to be used as a tool for
mapping and city modelling in the various corner of the globe, including
informal settlements and slum regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1&quot;&gt;Mohamed R. Ibrahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haworth_J/0/1/0/all/0/1&quot;&gt;James Haworth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1&quot;&gt;Tao Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03656">
<title>Resource-driven Substructural Defeasible Logic. (arXiv:1809.03656v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.03656</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear Logic and Defeasible Logic have been adopted to formalise different
features relevant to agents: consumption of resources, and reasoning with
exceptions. We propose a framework to combine sub-structural features,
corresponding to the consumption of resources, with defeasibility aspects, and
we discuss the design choices for the framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olivieri_F/0/1/0/all/0/1&quot;&gt;Francesco Olivieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Governatori_G/0/1/0/all/0/1&quot;&gt;Guido Governatori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1&quot;&gt;Matteo Cristani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beest_N/0/1/0/all/0/1&quot;&gt;Nick van Beest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colombo_Tosatto_S/0/1/0/all/0/1&quot;&gt;Silvano Colombo-Tosatto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03695">
<title>Evaluating Multimodal Representations on Sentence Similarity: vSTS, Visual Semantic Textual Similarity Dataset. (arXiv:1809.03695v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1809.03695</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we introduce vSTS, a new dataset for measuring textual
similarity of sentences using multimodal information. The dataset is comprised
by images along with its respectively textual captions. We describe the dataset
both quantitatively and qualitatively, and claim that it is a valid gold
standard for measuring automatic multimodal textual similarity systems. We also
describe the initial experiments combining the multimodal information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1&quot;&gt;Oier Lopez de Lacalle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1&quot;&gt;Aitor Soroa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1&quot;&gt;Eneko Agirre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03928">
<title>SAI, a Sensible Artificial Intelligence that plays Go. (arXiv:1809.03928v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1809.03928</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a multiple-komi modification of the AlphaGo Zero/Leela Zero
paradigm. The winrate as a function of the komi is modeled with a
two-parameters sigmoid function, so that the neural network must predict just
one more variable to assess the winrate for all komi values. A second novel
feature is that training is based on self-play games that occasionaly branch
-with changed komi- when the position is uneven. With this setting,
reinforcement learning is showed to work on 7x7 Go, obtaining very strong
playing agents. As a useful byproduct, the sigmoid parameters given by the
network allow to estimate the score difference on the board, and to evaluate
how much the game is decided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morandin_F/0/1/0/all/0/1&quot;&gt;Francesco Morandin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1&quot;&gt;Gianluca Amato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gini_R/0/1/0/all/0/1&quot;&gt;Rosa Gini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metta_C/0/1/0/all/0/1&quot;&gt;Carlo Metta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parton_M/0/1/0/all/0/1&quot;&gt;Maurizio Parton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascutto_G/0/1/0/all/0/1&quot;&gt;Gian-Carlo Pascutto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03979">
<title>Endowing Robots with Longer-term Autonomy by Recovering from External Disturbances in Manipulation through Grounded Anomaly Classification and Recovery Policies. (arXiv:1809.03979v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1809.03979</link>
<description rdf:parseType="Literal">&lt;p&gt;Robot manipulation is increasingly poised to interact with humans in
co-shared workspaces. Despite increasingly robust manipulation and control
algorithms, failure modes continue to exist whenever models do not capture the
dynamics of the unstructured environment. To obtain longer-term horizons in
robot automation, robots must develop introspection and recovery abilities. We
contribute a set of recovery policies to deal with anomalies produced by
external disturbances as well as anomaly classification through the use of
non-parametric statistics with memoized variational inference with scalable
adaptation. A recovery critic stands atop of a tightly-integrated, graph-based
online motion-generation and introspection system that resolves a wide range of
anomalous situations. Policies, skills, and introspection models are learned
incrementally and contextually in a task. Two task-level recovery policies:
re-enactment and adaptation resolve accidental and persistent anomalies
respectively. The introspection system uses non-parametric priors along with
Markov jump linear systems and memoized variational inference with scalable
adaptation to learn a model from the data. Extensive real-robot experimentation
with various strenuous anomalous conditions is induced and resolved at
different phases of a task and in different combinations. The system executes
around-the-clock introspection and recovery and even elicited self-recovery
when misclassifications occurred.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hongmin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Shuangqi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Longxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1&quot;&gt;Shuangda Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chumkamon_S/0/1/0/all/0/1&quot;&gt;Sakmongkon Chumkamon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1&quot;&gt;Yisheng Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojas_J/0/1/0/all/0/1&quot;&gt;Juan Rojas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01700">
<title>CNN features are also great at unsupervised classification. (arXiv:1707.01700v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01700</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims at providing insight on the transferability of deep CNN
features to unsupervised problems. We study the impact of different pretrained
CNN feature extractors on the problem of image set clustering for object
classification as well as fine-grained classification. We propose a rather
straightforward pipeline combining deep-feature extraction using a CNN
pretrained on ImageNet and a classic clustering algorithm to classify sets of
images. This approach is compared to state-of-the-art algorithms in
image-clustering and provides better results. These results strengthen the
belief that supervised training of deep CNN on large datasets, with a large
variability of classes, extracts better features than most carefully designed
engineering approaches, even for unsupervised tasks. We also validate our
approach on a robotic application, consisting in sorting and storing objects
smartly based on clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerin_J/0/1/0/all/0/1&quot;&gt;Joris Gu&amp;#xe9;rin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gibaru_O/0/1/0/all/0/1&quot;&gt;Olivier Gibaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiery_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Thiery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nyiri_E/0/1/0/all/0/1&quot;&gt;Eric Nyiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09756">
<title>Real-Time Bidding with Multi-Agent Reinforcement Learning in Display Advertising. (arXiv:1802.09756v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09756</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time advertising allows advertisers to bid for each impression for a
visiting user. To optimize specific goals such as maximizing revenue and return
on investment (ROI) led by ad placements, advertisers not only need to estimate
the relevance between the ads and user&apos;s interests, but most importantly
require a strategic response with respect to other advertisers bidding in the
market. In this paper, we formulate bidding optimization with multi-agent
reinforcement learning. To deal with a large number of advertisers, we propose
a clustering method and assign each cluster with a strategic bidding agent. A
practical Distributed Coordinated Multi-Agent Bidding (DCMAB) has been proposed
and implemented to balance the tradeoff between the competition and cooperation
among advertisers. The empirical study on our industry-scaled real-world data
has demonstrated the effectiveness of our methods. Our results show
cluster-based bidding would largely outperform single-agent and bandit
approaches, and the coordinated bidding achieves better overall objectives than
purely self-interested bidding agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Junqi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Chengru Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gai_K/0/1/0/all/0/1&quot;&gt;Kun Gai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.10139">
<title>A dynamic approach to surgical scheduling. (arXiv:1808.10139v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1808.10139</link>
<description rdf:parseType="Literal">&lt;p&gt;A mixed integer programming (MIP) formulation is presented that
simultaneously tackles operating theatre (OT) planning and scheduling problems
through the combined Master Surgical Scheduling (MSS) and Surgical Case
Assignment (SCA) problems. We consider stochastic surgical durations and
non-elective arrivals whilst applying a dynamic approach to adjust the schedule
after cancellations, equipment failure, or new arrivals on the waiting list.
The model is based on an Australian public hospital with a large surgical
department. Significant detail is included in the formulation to provide
practitioners with a model that can be implemented in hospitals. We show that
good feasible solutions can be provided in short amounts of computational time
with hyper metaheuristics. A dynamic approach is used to show how schedule
predictability affects patient throughput. It was found that the use of a
two-week schedule increases patient throughput and can help reduce waiting
lists.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Spratt_B/0/1/0/all/0/1&quot;&gt;Belinda Spratt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kozan_E/0/1/0/all/0/1&quot;&gt;Erhan Kozan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02193">
<title>Logical Rule Induction and Theory Learning Using Neural Theorem Proving. (arXiv:1809.02193v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1809.02193</link>
<description rdf:parseType="Literal">&lt;p&gt;A hallmark of human cognition is the ability to continually acquire and
distill observations of the world into meaningful, predictive theories. In this
paper we present a new mechanism for logical theory acquisition which takes a
set of observed facts and learns to extract from them a set of logical rules
and a small set of core facts which together entail the observations. Our
approach is neuro-symbolic in the sense that the rule pred- icates and core
facts are given dense vector representations. The rules are applied to the core
facts using a soft unification procedure to infer additional facts. After k
steps of forward inference, the consequences are compared to the initial
observations and the rules and core facts are then encouraged towards
representations that more faithfully generate the observations through
inference. Our approach is based on a novel neural forward-chaining
differentiable rule induction network. The rules are interpretable and learned
compositionally from their predicates, which may be invented. We demonstrate
the efficacy of our approach on a variety of ILP rule induction and domain
theory learning datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campero_A/0/1/0/all/0/1&quot;&gt;Andres Campero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pareja_A/0/1/0/all/0/1&quot;&gt;Aldo Pareja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klinger_T/0/1/0/all/0/1&quot;&gt;Tim Klinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Josh Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1&quot;&gt;Sebastian Riedel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03497">
<title>A Correlation Maximization Approach for Cross Domain Co-Embeddings. (arXiv:1809.03497v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1809.03497</link>
<description rdf:parseType="Literal">&lt;p&gt;Although modern recommendation systems can exploit the structure in users&apos;
item feedback, most are powerless in the face of new users who provide no
structure for them to exploit. In this paper we introduce ImplicitCE, an
algorithm for recommending items to new users during their sign-up flow.
ImplicitCE works by transforming users&apos; implicit feedback towards auxiliary
domain items into an embedding in the target domain item embedding space.
ImplicitCE learns these embedding spaces and transformation function in an
end-to-end fashion and can co-embed users and items with any differentiable
similarity function. To train ImplicitCE we explore methods for maximizing the
correlations between model predictions and users&apos; affinities and introduce
Sample Correlation Update, a novel and extremely simple training strategy.
Finally, we show that ImplicitCE trained with Sample Correlation Update
outperforms a variety of state of the art algorithms and loss functions on both
a large scale Twitter dataset and the DBLP dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiebler_D/0/1/0/all/0/1&quot;&gt;Dan Shiebler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03534">
<title>Energy Disaggregation via Deep Temporal Dictionary Learning. (arXiv:1809.03534v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03534</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the energy disaggregation problem, i.e. decomposing the
electricity signal of a whole home to its operating devices. First, we cast the
problem as a dictionary learning (DL) problem where the key electricity
patterns representing consumption behaviors are extracted for each device and
stored in a dictionary matrix. The electricity signal of each device is then
modeled by a linear combination of such patterns with sparse coefficients that
determine the contribution of each device in the total electricity. Although
popular, the classic DL approach is prone to high error in real-world
applications including energy disaggregation, as it merely finds linear
dictionaries. Moreover, this method lacks a recurrent structure; thus, it is
unable to leverage the temporal structure of energy signals. Motivated by such
shortcomings, we propose a novel optimization program where the dictionary and
its sparse coefficients are optimized simultaneously with a deep neural model
extracting powerful nonlinear features from the energy signals. A long
short-term memory auto-encoder (LSTM-AE) is proposed with tunable time
dependent states to capture the temporal behavior of energy signals for each
device. We learn the dictionary in the space of temporal features captured by
the LSTM-AE rather than the original space of the energy signals; hence, in
contrast to the traditional DL, here, a nonlinear dictionary is learned using
powerful temporal features extracted from our deep model. Real experiments on
the publicly available Reference Energy Disaggregation Dataset (REDD) show
significant improvement compared to the state-of-the-art methodologies in terms
of the disaggregation accuracy and F-score metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khodayar_M/0/1/0/all/0/1&quot;&gt;Mahdi Khodayar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03538">
<title>Convolutional Graph Auto-encoder: A Deep Generative Neural Architecture for Probabilistic Spatio-temporal Solar Irradiance Forecasting. (arXiv:1809.03538v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03538</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Learning on graph-structured data is an important and omnipresent
task for a vast variety of applications including anomaly detection and dynamic
network analysis. In this paper, a deep generative model is introduced to
capture continuous probability densities corresponding to the nodes of an
arbitrary graph. In contrast to all learning formulations in the area of
discriminative pattern recognition, we propose a scalable generative
optimization/algorithm theoretically proved to capture distributions at the
nodes of a graph. Our model is able to generate samples from the probability
densities learned at each node. This probabilistic data generation model, i.e.
convolutional graph auto-encoder (CGAE), is devised based on the localized
first-order approximation of spectral graph convolutions, deep learning, and
the variational Bayesian inference. We apply our CGAE to a new problem, the
spatio-temporal probabilistic solar irradiance prediction. Multiple solar
radiation measurement sites in a wide area in northern states of the US are
modeled as an undirected graph. Using our proposed model, the distribution of
future irradiance given historical radiation observations is estimated for
every site/node. Numerical results on the National Solar Radiation Database
show state-of-the-art performance for probabilistic radiation prediction on
geographically distributed irradiance data in terms of reliability, sharpness,
and continuous ranked probability score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khodayar_M/0/1/0/all/0/1&quot;&gt;Mahdi Khodayar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1&quot;&gt;Saeed Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khodayar_M/0/1/0/all/0/1&quot;&gt;Mohammad Khodayar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guangyi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03548">
<title>VPE: Variational Policy Embedding for Transfer Reinforcement Learning. (arXiv:1809.03548v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03548</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning methods are capable of solving complex problems, but
resulting policies might perform poorly in environments that are even slightly
different. In robotics especially, training and deployment conditions often
vary and data collection is expensive, making retraining undesirable.
Simulation training allows for feasible training times, but on the other hand
suffers from a reality-gap when applied in real-world settings. This raises the
need of efficient adaptation of policies acting in new environments. We
consider this as a problem of transferring knowledge within a family of similar
Markov decision processes.
&lt;/p&gt;
&lt;p&gt;For this purpose we assume that Q-functions are generated by some
low-dimensional latent variable. Given such a Q-function, we can find a master
policy that can adapt given different values of this latent variable. Our
method learns both the generative mapping and an approximate posterior of the
latent variables, enabling identification of policies for new tasks by
searching only in the latent space, rather than the space of all policies. The
low-dimensional space, and master policy found by our method enables policies
to quickly adapt to new environments. We demonstrate the method on both a
pendulum swing-up task in simulation, and for simulation-to-real transfer on a
pushing task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnekvist_I/0/1/0/all/0/1&quot;&gt;Isac Arnekvist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1&quot;&gt;Danica Kragic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stork_J/0/1/0/all/0/1&quot;&gt;Johannes A. Stork&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03561">
<title>Quantile Regression for Qualifying Match of GEFCom2017 Probabilistic Load Forecasting. (arXiv:1809.03561v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1809.03561</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a simple quantile regression-based forecasting method that was
applied in a probabilistic load forecasting framework of the Global Energy
Forecasting Competition 2017 (GEFCom2017). The hourly load data is log
transformed and split into a long-term trend component and a remainder term.
The key forecasting element is the quantile regression approach for the
remainder term that takes into account weekly and annual seasonalities such as
their interactions. Temperature information is only used to stabilize the
forecast of the long-term trend component. Public holidays information is
ignored. Still, the forecasting method placed second in the open data track and
fourth in the definite data track with our forecasting method, which is
remarkable given simplicity of the model. The method also outperforms the
Vanilla benchmark consistently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ziel_F/0/1/0/all/0/1&quot;&gt;Florian Ziel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03566">
<title>Collapsed Variational Inference for Nonparametric Bayesian Group Factor Analysis. (arXiv:1809.03566v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03566</link>
<description rdf:parseType="Literal">&lt;p&gt;Group factor analysis (GFA) methods have been widely used to infer the common
structure and the group-specific signals from multiple related datasets in
various fields including systems biology and neuroimaging. To date, most
available GFA models require Gibbs sampling or slice sampling to perform
inference, which prevents the practical application of GFA to large-scale data.
In this paper we present an efficient collapsed variational inference (CVI)
algorithm for the nonparametric Bayesian group factor analysis (NGFA) model
built upon an hierarchical beta Bernoulli process. Our CVI algorithm proceeds
by marginalizing out the group-specific beta process parameters, and then
approximating the true posterior in the collapsed space using mean field
methods. Experimental results on both synthetic and real-world data demonstrate
the effectiveness of our CVI algorithm for the NGFA compared with
state-of-the-art GFA methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sikun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1&quot;&gt;Heinz Koeppl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03576">
<title>Out-of-Distribution Detection Using an Ensemble of Self Supervised Leave-out Classifiers. (arXiv:1809.03576v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03576</link>
<description rdf:parseType="Literal">&lt;p&gt;As deep learning methods form a critical part in commercially important
applications such as autonomous driving and medical diagnostics, it is
important to reliably detect out-of-distribution (OOD) inputs while employing
these algorithms. In this work, we propose an OOD detection algorithm which
comprises of an ensemble of classifiers. We train each classifier in a
self-supervised manner by leaving out a random subset of training data as OOD
data and the rest as in-distribution (ID) data. We propose a novel margin-based
loss over the softmax output which seeks to maintain at least a margin $m$
between the average entropy of the OOD and in-distribution samples. In
conjunction with the standard cross-entropy loss, we minimize the novel loss to
train an ensemble of classifiers. We also propose a novel method to combine the
outputs of the ensemble of classifiers to obtain OOD detection score and class
prediction. Overall, our method convincingly outperforms Hendrycks et al.[7]
and the current state-of-the-art ODIN[13] on several OOD detection benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyas_A/0/1/0/all/0/1&quot;&gt;Apoorv Vyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jammalamadaka_N/0/1/0/all/0/1&quot;&gt;Nataraj Jammalamadaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xia Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1&quot;&gt;Dipankar Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaul_B/0/1/0/all/0/1&quot;&gt;Bharat Kaul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willke_T/0/1/0/all/0/1&quot;&gt;Theodore L. Willke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03655">
<title>An Efficient ADMM-Based Algorithm to Nonconvex Penalized Support Vector Machines. (arXiv:1809.03655v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1809.03655</link>
<description rdf:parseType="Literal">&lt;p&gt;Support vector machines (SVMs) with sparsity-inducing nonconvex penalties
have received considerable attentions for the characteristics of automatic
classification and variable selection. However, it is quite challenging to
solve the nonconvex penalized SVMs due to their nondifferentiability,
nonsmoothness and nonconvexity. In this paper, we propose an efficient
ADMM-based algorithm to the nonconvex penalized SVMs. The proposed algorithm
covers a large class of commonly used nonconvex regularization terms including
the smooth clipped absolute deviation (SCAD) penalty, minimax concave penalty
(MCP), log-sum penalty (LSP) and capped-$\ell_1$ penalty. The computational
complexity analysis shows that the proposed algorithm enjoys low computational
cost. Moreover, the convergence of the proposed algorithm is guaranteed.
Extensive experimental evaluations on five benchmark datasets demonstrate the
superior performance of the proposed algorithm to other three state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guan_L/0/1/0/all/0/1&quot;&gt;Lei Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Linbo Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ge_K/0/1/0/all/0/1&quot;&gt;Keshi Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xicheng Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03659">
<title>New models for symbolic data analysis. (arXiv:1809.03659v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1809.03659</link>
<description rdf:parseType="Literal">&lt;p&gt;Symbolic data analysis (SDA) is an emerging area of statistics based on
aggregating individual level data into group-based distributional summaries
(symbols), and then developing statistical methods to analyse them. It is ideal
for analysing large and complex datasets, and has immense potential to become a
standard inferential technique in the near future. However, existing SDA
techniques are either non-inferential, do not easily permit meaningful
statistical models, are unable to distinguish between competing models, and are
based on simplifying assumptions that are known to be false. Further, the
procedure for constructing symbols from the underlying data is erroneously not
considered relevant to the resulting statistical analysis. In this paper we
introduce a new general method for constructing likelihood functions for
symbolic data based on a desired probability model for the underlying classical
data, while only observing the distributional summaries. This approach resolves
many of the conceptual and practical issues with current SDA methods, opens the
door for new classes of symbol design and construction, in addition to
developing SDA as a viable tool to enable and improve upon classical data
analyses, particularly for very large and complex datasets. This work creates a
new direction for SDA research, which we illustrate through several real and
simulated data analyses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Beranger_B/0/1/0/all/0/1&quot;&gt;Boris Beranger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Huan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sisson_S/0/1/0/all/0/1&quot;&gt;Scott A. Sisson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03776">
<title>Solving Non-identifiable Latent Feature Models. (arXiv:1809.03776v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03776</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent feature models (LFM)s are widely employed for extracting latent
structures of data. While offering high, parameter estimation is difficult with
LFMs because of the combinational nature of latent features, and
non-identifiability is a particularly difficult problem when parameter
estimation is not unique and there exists equivalent solutions. In this paper,
a necessary and sufficient condition for non-identifiability is shown. The
condition is significantly related to dependency of features, and this implies
that non-identifiability may often occur in real-world applications. A novel
method for parameter estimation that solves the non-identifiability problem is
also proposed. This method can be combined as a post-process with existing
methods and can find an appropriate solution by hopping efficiently through
equivalent solutions. We have evaluated the effectiveness of the method on both
synthetic and real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suzuki_R/0/1/0/all/0/1&quot;&gt;Ryota Suzuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takahashi_S/0/1/0/all/0/1&quot;&gt;Shingo Takahashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petradwala_M/0/1/0/all/0/1&quot;&gt;Murtuza Petradwala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohmoto_S/0/1/0/all/0/1&quot;&gt;Shigeru Kohmoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03779">
<title>Probabilistic approach to limited-data computed tomography reconstruction. (arXiv:1809.03779v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1809.03779</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of reconstructing the internal structure of an object
from limited x-ray projections. In this work, we use a Gaussian process to
model the target function. In contrast to other established methods, this comes
with the advantage of not requiring any manual parameter tuning, which usually
arises in classical regularization strategies. The Gaussian process is
well-known in a heavy computation for the inversion of a covariance matrix, and
in this work, by employing an approximative spectral-based technique, we reduce
the computational complexity and avoid the need of numerical integration.
Results from simulated and real data indicate that this approach is less
sensitive to streak artifacts as compared to the commonly used method of
filteredback projection, an analytic reconstruction algorithm using Radon
inversion formula.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purisha_Z/0/1/0/all/0/1&quot;&gt;Zenith Purisha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jidling_C/0/1/0/all/0/1&quot;&gt;Carl Jidling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahlstrom_N/0/1/0/all/0/1&quot;&gt;Niklas Wahlstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkka_S/0/1/0/all/0/1&quot;&gt;Simo S&amp;#xe4;rkk&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1&quot;&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03817">
<title>Predicting Blood Glucose with an LSTM and Bi-LSTM Based Deep Neural Network. (arXiv:1809.03817v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03817</link>
<description rdf:parseType="Literal">&lt;p&gt;A deep learning network was used to predict future blood glucose levels, as
this can permit diabetes patients to take action before imminent hyperglycaemia
and hypoglycaemia. A sequential model with one long-short-term memory (LSTM)
layer, one bidirectional LSTM layer and several fully connected layers was used
to predict blood glucose levels for different prediction horizons. The method
was trained and tested on 26 datasets from 20 real patients. The proposed
network outperforms the baseline methods in terms of all evaluation criteria.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qingnan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jankovic_M/0/1/0/all/0/1&quot;&gt;Marko V. Jankovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bally_L/0/1/0/all/0/1&quot;&gt;Lia Bally&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mougiakakou_S/0/1/0/all/0/1&quot;&gt;Stavroula G. Mougiakakou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03839">
<title>Unsupervised Domain Adaptation Based on Source-guided Discrepancy. (arXiv:1809.03839v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1809.03839</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation is the problem setting where data generating
distributions in the source and target domains are different, and labels in the
target domain are unavailable. One important question in unsupervised domain
adaptation is how to measure the difference between the source and target
domains. A previously proposed discrepancy that does not use the source domain
labels requires high computational cost to estimate and may lead to a loose
generalization error bound in the target domain.To mitigate these problems, we
propose a novel discrepancy called source-guided discrepancy ($S$-disc), which
exploits labels in the source domain. As a consequence, $S$-disc can be
computed efficiently with a finite sample convergence guarantee. In addition,
we show that $S$-disc can provide a tighter generalization error bound than the
one based on an existing discrepancy. Finally, we report experimental results
that demonstrate the advantages of $S$-disc over the existing discrepancies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuroki_S/0/1/0/all/0/1&quot;&gt;Seiichi Kuroki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charonenphakdee_N/0/1/0/all/0/1&quot;&gt;Nontawat Charonenphakdee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1&quot;&gt;Han Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honda_J/0/1/0/all/0/1&quot;&gt;Junya Honda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1&quot;&gt;Issei Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.03986">
<title>Efficient Statistics, in High Dimensions, from Truncated Samples. (arXiv:1809.03986v1 [math.ST])</title>
<link>http://arxiv.org/abs/1809.03986</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide an efficient algorithm for the classical problem, going back to
Galton, Pearson, and Fisher, of estimating, with arbitrary accuracy the
parameters of a multivariate normal distribution from truncated samples.
Truncated samples from a $d$-variate normal ${\cal
N}(\mathbf{\mu},\mathbf{\Sigma})$ means a samples is only revealed if it falls
in some subset $S \subseteq \mathbb{R}^d$; otherwise the samples are hidden and
their count in proportion to the revealed samples is also hidden. We show that
the mean $\mathbf{\mu}$ and covariance matrix $\mathbf{\Sigma}$ can be
estimated with arbitrary accuracy in polynomial-time, as long as we have oracle
access to $S$, and $S$ has non-trivial measure under the unknown $d$-variate
normal distribution. Additionally we show that without oracle access to $S$,
any non-trivial estimation is impossible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Daskalakis_C/0/1/0/all/0/1&quot;&gt;Constantinos Daskalakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gouleakis_T/0/1/0/all/0/1&quot;&gt;Themis Gouleakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tzamos_C/0/1/0/all/0/1&quot;&gt;Christos Tzamos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zampetakis_M/0/1/0/all/0/1&quot;&gt;Manolis Zampetakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.03528">
<title>GIANT: Globally Improved Approximate Newton Method for Distributed Optimization. (arXiv:1709.03528v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.03528</link>
<description rdf:parseType="Literal">&lt;p&gt;For distributed computing environment, we consider the empirical risk
minimization problem and propose a distributed and communication-efficient
Newton-type optimization method. At every iteration, each worker locally finds
an Approximate NewTon (ANT) direction, which is sent to the main driver. The
main driver, then, averages all the ANT directions received from workers to
form a {\it Globally Improved ANT} (GIANT) direction. GIANT is highly
communication efficient and naturally exploits the trade-offs between local
computations and global communications in that more local computations result
in fewer overall rounds of communications. Theoretically, we show that GIANT
enjoys an improved convergence rate as compared with first-order methods and
existing distributed Newton-type methods. Further, and in sharp contrast with
many existing distributed Newton-type methods, as well as popular first-order
methods, a highly advantageous practical feature of GIANT is that it only
involves one tuning parameter. We conduct large-scale experiments on a computer
cluster and, empirically, demonstrate the superior performance of GIANT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shusen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roosta_Khorasani_F/0/1/0/all/0/1&quot;&gt;Farbod Roosta-Khorasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1&quot;&gt;Michael W. Mahoney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08881">
<title>Correlated Components Analysis - Extracting Reliable Dimensions in Multivariate Data. (arXiv:1801.08881v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.08881</link>
<description rdf:parseType="Literal">&lt;p&gt;How does one find dimensions in multivariate data that are reliably expressed
across repetitions? For example, in a brain imaging study one may want to
identify combinations of neural signals that are reliably expressed across
multiple trials or subjects. For a behavioral assessment with multiple ratings,
one may want to identify an aggregate score that is reliably reproduced across
raters. Correlated Components Analysis (CorrCA) addresses this problem by
identifying components that are maximally correlated between repetitions (e.g.
trials, subjects, raters). Here we formalize this as the maximization of the
ratio of between-repetition to within-repetition covariance. We show that this
criterion maximizes repeat-reliability, defined as mean over variance across
repeats, and that it leads to CorrCA or to multi-set Canonical Correlation
Analysis, depending on the constraints. Surprisingly, we also find that CorrCA
is equivalent to Linear Discriminant Analysis for equal-mean signals, which
provides an unexpected link between classic concepts of multivariate analysis.
We provided an exact parametric test for statistical significance based on the
F-statistic for normally distributed independent samples, and present and
validate shuffle statistics for the case of dependent samples. Regularization
and extension to non-linear mappings using kernels are also presented. The
algorithms are demonstrated on a series of data analysis applications, and we
provide all code and data required to reproduce the results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parra_L/0/1/0/all/0/1&quot;&gt;Lucas C. Parra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haufe_S/0/1/0/all/0/1&quot;&gt;Stefan Haufe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dmochowski_J/0/1/0/all/0/1&quot;&gt;Jacek P. Dmochowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.09318">
<title>Data-driven Discovery of Closure Models. (arXiv:1803.09318v3 [math.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1803.09318</link>
<description rdf:parseType="Literal">&lt;p&gt;Derivation of reduced order representations of dynamical systems requires the
modeling of the truncated dynamics on the retained dynamics. In its most
general form, this so-called closure model has to account for memory effects.
In this work, we present a framework of operator inference to extract the
governing dynamics of closure from data in a compact, non-Markovian form. We
employ sparse polynomial regression and artificial neural networks to extract
the underlying operator. For a special class of non-linear systems,
observability of the closure in terms of the resolved dynamics is analyzed and
theoretical results are presented on the compactness of the memory. The
proposed framework is evaluated on examples consisting of linear to nonlinear
systems with and without chaotic dynamics, with an emphasis on predictive
performance on unseen data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shaowu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Duraisamy_K/0/1/0/all/0/1&quot;&gt;Karthik Duraisamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.05409">
<title>Machine Learning for Public Administration Research, with Application to Organizational Reputation. (arXiv:1805.05409v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1805.05409</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning methods have gained a great deal of popularity in recent
years among public administration scholars and practitioners. These techniques
open the door to the analysis of text, image and other types of data that allow
us to test foundational theories of public administration and to develop new
theories. Despite the excitement surrounding machine learning methods, clarity
regarding their proper use and potential pitfalls is lacking. This paper
attempts to fill this gap in the literature through providing a machine
learning &quot;guide to practice&quot; for public administration scholars and
practitioners. Here, we take a foundational view of machine learning and
describe how these methods can enrich public administration research and
practice through their ability develop new measures, tap into new sources of
data and conduct statistical inference and causal inference in a principled
manner. We then turn our attention to the pitfalls of using these methods such
as unvalidated measures and lack of interpretability. Finally, we demonstrate
how machine learning techniques can help us learn about organizational
reputation in federal agencies through an illustrated example using tweets from
13 executive federal agencies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anastasopoulos_L/0/1/0/all/0/1&quot;&gt;L. Jason Anastasopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitford_A/0/1/0/all/0/1&quot;&gt;Andrew B. Whitford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11259">
<title>Statistical mechanical analysis of sparse linear regression as a variable selection problem. (arXiv:1805.11259v2 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11259</link>
<description rdf:parseType="Literal">&lt;p&gt;An algorithmic limit of compressed sensing or related variable-selection
problems is analytically evaluated when a design matrix is given by an
overcomplete random matrix. The replica method from statistical mechanics is
employed to derive the result. The analysis is conducted through evaluation of
the entropy, an exponential rate of the number of combinations of variables
giving a specific value of fit error to given data which is assumed to be
generated from a linear process using the design matrix. This yields the
typical achievable limit of the fit error when solving a representative
$\ell_0$ problem and includes the presence of unfavourable phase transitions
preventing local search algorithms from reaching the minimum-error
configuration. The associated phase diagrams are presented. A noteworthy
outcome of the phase diagrams is that there exists a wide parameter region
where any phase transition is absent from the high temperature to the lowest
temperature at which the minimum-error configuration or the ground state is
reached. This implies that certain local search algorithms can find the ground
state with moderate computational costs in that region. Another noteworthy
result is the presence of the random first-order transition in the strong noise
case. The theoretical evaluation of the entropy is confirmed by extensive
numerical methods using the exchange Monte Carlo and the multi-histogram
methods. Another numerical test based on a metaheuristic optimisation algorithm
called simulated annealing is conducted, which well supports the theoretical
predictions on the local search algorithms. In the successful region with no
phase transition, the computational cost of the simulated annealing to reach
the ground state is estimated as the third order polynomial of the model
dimensionality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Obuchi_T/0/1/0/all/0/1&quot;&gt;Tomoyuki Obuchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Nakanishi_Ohno_Y/0/1/0/all/0/1&quot;&gt;Yoshinori Nakanishi-Ohno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Okada_M/0/1/0/all/0/1&quot;&gt;Masato Okada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Kabashima_Y/0/1/0/all/0/1&quot;&gt;Yoshiyuki Kabashima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04819">
<title>Integral Privacy for Sampling from Mollifier Densities with Approximation Guarantees. (arXiv:1806.04819v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04819</link>
<description rdf:parseType="Literal">&lt;p&gt;Sampling encompasses old and central problems in statistics and machine
learning. There exists several approaches to cast this problem in a
differential privacy framework but little is still comparatively known about
the approximation guarantees of the unknown density by the private one learned.
In this paper, we first introduce a general condition for a set of densities,
called an $\varepsilon$-mollifier, to grant privacy for sampling in the
$\varepsilon$-differential privacy model, and even in a stronger model where we
remove the famed adjacency condition of inputs. We then show how to exploit the
boosting toolkit to learn a density within an $\varepsilon$-mollifier with
guaranteed approximation of the target density that degrade gracefully with the
privacy budget. Approximation guarantees cover the mode capture problem, a
problem which is receiving a lot of attention in the generative models
literature. To our knowledge, the way we exploit the boosting toolkit has never
been done before in the context of density estimation or sampling: we require
access to a weak learner in the original boosting sense, so we learn a density
out of \textit{classifiers}. Experimental results against a state of the art
implementation of private kernel density estimation display that our technique
consistently obtains improved results, managing in particular to get similar
outputs for a privacy budget $\epsilon$ which is however orders of magnitude
smaller.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Husain_H/0/1/0/all/0/1&quot;&gt;Hisham Husain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cranko_Z/0/1/0/all/0/1&quot;&gt;Zac Cranko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nock_R/0/1/0/all/0/1&quot;&gt;Richard Nock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04119">
<title>Exploiting statistical dependencies of time series with hierarchical correlation reconstruction. (arXiv:1807.04119v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.04119</link>
<description rdf:parseType="Literal">&lt;p&gt;While we are usually focused on forecasting future values of time series, it
is often valuable to additionally predict their entire probability
distributions, e.g. to evaluate risk, Monte Carlo simulations. On example of
time series of $\approx$ 30000 Dow Jones Industrial Averages, there will be
presented application of hierarchical correlation reconstruction for this
purpose: mean-square estimating polynomial as joint density for (current value,
context), where context is for example a few previous values. Then substituting
the currently observed context and normalizing density to 1, we get predicted
probability distribution for the current value. In contrast to standard machine
learning approaches like neural networks, optimal polynomial coefficients here
can be inexpensively directly calculated, have controllable accuracy, are
unique and independent, each has a specific cumulant-like interpretation, and
such approximation using can approach complete description of any real joint
distribution - providing a perfect tool to quantitatively describe and exploit
statistical dependencies in time series. There is also discussed application
for non-stationary time series like calculating linear time trend, or adapting
coefficients to local statistical behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duda_J/0/1/0/all/0/1&quot;&gt;Jarek Duda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.07379">
<title>Privacy Mining from IoT-based Smart Homes. (arXiv:1808.07379v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1808.07379</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a wide range of smart devices are deployed in a variety of
environments to improve the quality of human life. One of the important
IoT-based applications is smart homes for healthcare, especially for elders.
IoT-based smart homes enable elders&apos; health to be properly monitored and taken
care of. However, elders&apos; privacy might be disclosed from smart homes due to
non-fully protected network communication or other reasons. To demonstrate how
serious this issue is, we introduce in this paper a Privacy Mining Approach
(PMA) to mine privacy from smart homes by conducting a series of deductions and
analyses on sensor datasets generated by smart homes. The experimental results
demonstrate that PMA is able to deduce a global sensor topology for a smart
home and disclose elders&apos; privacy in terms of their house layouts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Ming-Chang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jia-Chun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owe_O/0/1/0/all/0/1&quot;&gt;Olaf Owe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1809.02519">
<title>Fairness Through Causal Awareness: Learning Latent-Variable Models for Biased Data. (arXiv:1809.02519v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1809.02519</link>
<description rdf:parseType="Literal">&lt;p&gt;How do we learn from biased data? Historical datasets often reflect
historical prejudices; sensitive or protected attributes may affect the
observed treatments and outcomes. Classification algorithms tasked with
predicting outcomes accurately from these datasets tend to replicate these
biases. We advocate a causal modeling approach to learning from biased data and
reframe fair classification as an intervention problem. We propose a causal
model in which the sensitive attribute confounds both the treatment and the
outcome. Building on prior work in deep learning and generative modeling, we
describe how to learn the parameters of this causal model from observational
data alone, even in the presence of unobserved confounders. We show
experimentally that fairness-aware causal modeling provides better estimates of
the causal effects between the sensitive attribute, the treatment, and the
outcome. We further present evidence that estimating these causal effects can
help us to learn policies which are both more accurate and fair, when presented
with a historically biased dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madras_D/0/1/0/all/0/1&quot;&gt;David Madras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1&quot;&gt;Elliot Creager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pitassi_T/0/1/0/all/0/1&quot;&gt;Toniann Pitassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;</dc:creator>
</item></rdf:RDF>