<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-05-10T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03687"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03886"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03908"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03696"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03876"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03887"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04032"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04051"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04155"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02861"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03364"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03644"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03777"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03779"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03901"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03911"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04018"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.06997"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.06619"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.03528"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05398"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01113"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05345"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07891"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.03444"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1805.03687">
<title>Statistical Analysis on E-Commerce Reviews, with Sentiment Classification using Bidirectional Recurrent Neural Network (RNN). (arXiv:1805.03687v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.03687</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding customer sentiments is of paramount importance in marketing
strategies today. Not only will it give companies an insight as to how
customers perceive their products and/or services, but it will also give them
an idea on how to improve their offers. This paper attempts to understand the
correlation of different variables in customer reviews on a women clothing
e-commerce, and to classify each review whether it recommends the reviewed
product or not and whether it consists of positive, negative, or neutral
sentiment. To achieve these goals, we employed univariate and multivariate
analyses on dataset features except for review titles and review texts, and we
implemented a bidirectional recurrent neural network (RNN) with long-short term
memory unit (LSTM) for recommendation and sentiment classification. Results
have shown that a recommendation is a strong indicator of a positive sentiment
score, and vice-versa. On the other hand, ratings in product reviews are fuzzy
indicators of sentiment scores. We also found out that the bidirectional LSTM
was able to reach an F1-score of 0.88 for recommendation classification, and
0.93 for sentiment classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarap_A/0/1/0/all/0/1&quot;&gt;Abien Fred Agarap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grafilon_P/0/1/0/all/0/1&quot;&gt;Paul Grafilon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03886">
<title>Effect of dilution in asymmetric recurrent neural networks. (arXiv:1805.03886v1 [cond-mat.dis-nn])</title>
<link>http://arxiv.org/abs/1805.03886</link>
<description rdf:parseType="Literal">&lt;p&gt;We study with numerical simulation the possible limit behaviors of
synchronous discrete-time deterministic recurrent neural networks composed of N
binary neurons as a function of a network&apos;s level of dilution and asymmetry.
The network dilution measures the fraction of neuron couples that are
connected, and the network asymmetry measures to what extent the underlying
connectivity matrix is asymmetric. For each given neural network, we study the
dynamical evolution of all the different initial conditions, thus
characterizing the full dynamical landscape without imposing any learning rule.
Because of the deterministic dynamics, each trajectory converges to an
attractor, that can be either a fixed point or a limit cycle. These attractors
form the set of all the possible limit behaviors of the neural network. For
each network, we then determine the convergence times, the limit cycles&apos;
length, the number of attractors, and the sizes of the attractors&apos; basin. We
show that there are two network structures that maximize the number of possible
limit behaviors. The first optimal network structure is fully-connected and
symmetric. On the contrary, the second optimal network structure is highly
sparse and asymmetric. The latter optimal is similar to what observed in
different biological neuronal circuits. These observations lead us to
hypothesize that independently from any given learning model, an efficient and
effective biologic network that stores a number of limit behaviors close to its
maximum capacity tends to develop a connectivity structure similar to one of
the optimal networks we found.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Folli_V/0/1/0/all/0/1&quot;&gt;Viola Folli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Gosti_G/0/1/0/all/0/1&quot;&gt;Giorgio Gosti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Leonetti_M/0/1/0/all/0/1&quot;&gt;Marco Leonetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ruocco_G/0/1/0/all/0/1&quot;&gt;Giancarlo Ruocco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03908">
<title>Towards a universal neural network encoder for time series. (arXiv:1805.03908v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.03908</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the use of a time series encoder to learn representations that are
useful on data set types with which it has not been trained on. The encoder is
formed of a convolutional neural network whose temporal output is summarized by
a convolutional attention mechanism. This way, we obtain a compact,
fixed-length representation from longer, variable-length time series. We
evaluate the performance of the proposed approach on a well-known time series
classification benchmark, considering full adaptation, partial adaptation, and
no adaptation of the encoder to the new data type. Results show that such
strategies are competitive with the state-of-the-art, often outperforming
conceptually-matching approaches. Besides accuracy scores, the facility of
adaptation and the efficiency of pre-trained encoders make them an appealing
option for the processing of scarcely- or non-labeled time series.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serra_J/0/1/0/all/0/1&quot;&gt;Joan Serr&amp;#xe0;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascual_S/0/1/0/all/0/1&quot;&gt;Santiago Pascual&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karatzoglou_A/0/1/0/all/0/1&quot;&gt;Alexandros Karatzoglou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03696">
<title>BayesLands: A Bayesian inference approach for parameter uncertainty quantification in Badlands. (arXiv:1805.03696v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/1805.03696</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian inference provides a principled approach towards uncertainty
quantification of free parameters in geophysical forward models. This provides
advantages over optimization methods that provide single point estimates as
solutions, which lack uncertainty quantification. Badlands (basin and landscape
dynamics model) is geophysical forward model that simulates topography
development at various space and time scales. Badlands consists of a number of
geophysical parameters that need to be estimated with appropriate uncertainty
quantification, given the observed ground truth such as surface topography,
sediment thickness and stratigraphy through time. This is challenging due to
the scarcity of data, sensitivity of the parameters and complexity of the
Badlands model. In this paper, we take a Bayesian approach to provide inference
using Markov chain Monte Carlo sampling (MCMC). Hence, we present
\textit{BayesLands}, a Bayesian framework for Badlands that fuses information
obtained from complex forward models with observational data and prior
knowledge. As a proof-of-concept, we consider a synthetic and real-world
topography with two free parameters, namely precipitation and erodibility, that
we need to estimate through BayesLands. The results of the experiments shows
that BayesLands yields a promising distribution of the parameters. Moreover,
the challenge in sampling due to multi-modality is presented through
visualizing a likelihood surface that has a range of suboptimal modes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chandra_R/0/1/0/all/0/1&quot;&gt;Rohitash Chandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Azam_D/0/1/0/all/0/1&quot;&gt;Danial Azam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Muller_R/0/1/0/all/0/1&quot;&gt;R. Dietmar M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Salles_T/0/1/0/all/0/1&quot;&gt;Tristan Salles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cripps_S/0/1/0/all/0/1&quot;&gt;Sally Cripps&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03876">
<title>Learning Robust Search Strategies Using a Bandit-Based Approach. (arXiv:1805.03876v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1805.03876</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective solving of constraint problems often requires choosing good or
specific search heuristics. However, choosing or designing a good search
heuristic is non-trivial and is often a manual process. In this paper, rather
than manually choosing/designing search heuristics, we propose the use of
bandit-based learning techniques to automatically select search heuristics. Our
approach is online where the solver learns and selects from a set of heuristics
during search. The goal is to obtain automatic search heuristics which give
robust performance. Preliminary experiments show that our adaptive technique is
more robust than the original search heuristics. It can also outperform the
original heuristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1&quot;&gt;Wei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yap_R/0/1/0/all/0/1&quot;&gt;Roland H. C. Yap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03887">
<title>Scaling associative classification for very large datasets. (arXiv:1805.03887v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.03887</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised learning algorithms are nowadays successfully scaling up to
datasets that are very large in volume, leveraging the potential of in-memory
cluster-computing Big Data frameworks. Still, massive datasets with a number of
large-domain categorical features are a difficult challenge for any classifier.
Most off-the-shelf solutions cannot cope with this problem. In this work we
introduce DAC, a Distributed Associative Classifier. DAC exploits ensemble
learning to distribute the training of an associative classifier among parallel
workers and improve the final quality of the model. Furthermore, it adopts
several novel techniques to reach high scalability without sacrificing quality,
among which a preventive pruning of classification rules in the extraction
phase based on Gini impurity. We ran experiments on Apache Spark, on a real
large-scale dataset with more than 4 billion records and 800 million distinct
categories. The results showed that DAC improves on a state-of-the-art solution
in both prediction quality and execution time. Since the generated model is
human-readable, it can not only classify new records, but also allow
understanding both the logic behind the prediction and the properties of the
model, becoming a useful aid for decision makers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venturini_L/0/1/0/all/0/1&quot;&gt;Luca Venturini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baralis_E/0/1/0/all/0/1&quot;&gt;Elena Baralis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garza_P/0/1/0/all/0/1&quot;&gt;Paolo Garza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04032">
<title>From Word to Sense Embeddings: A Survey on Vector Representations of Meaning. (arXiv:1805.04032v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1805.04032</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past years, distributed representations have proven effective and
flexible keepers of prior knowledge to be integrated into downstream
applications. This survey is focused on semantic representation of meaning. We
start from the theoretical background behind word vector space models and
highlight one of its main limitations: the meaning conflation deficiency. Then,
we explain how this deficiency can be addressed through a transition from word
level to the more fine-grained level of word senses (in its broader
acceptation) as a method for modelling unambiguous lexical meaning. We present
a comprehensive overview of the wide range of techniques in the two main
branches of sense representation, i.e., unsupervised and knowledge-based.
Finally, this survey covers the main evaluation procedures and an analysis of
five important aspects: interpretability, sense granularity, adaptability to
different domains, compositionality and integration into downstream
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1&quot;&gt;Jose Camacho-Collados&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilehvar_T/0/1/0/all/0/1&quot;&gt;Taher Pilehvar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04051">
<title>Classification of Household Materials via Spectroscopy. (arXiv:1805.04051v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1805.04051</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognizing an object&apos;s material can inform a robot on how hard it may grasp
the object during manipulation, or if the object may be safely heated up. To
estimate an object&apos;s material during manipulation, many prior works have
explored the use of haptic sensing. In this paper, we explore a technique for
robots to estimate the materials of objects using spectroscopy. We demonstrate
that spectrometers provide several benefits for material recognition, including
fast sensing times and accurate measurements with low noise. Furthermore,
spectrometers do not require direct contact with an object. To illustrate this,
we collected a dataset of spectral measurements from two commercially available
spectrometers during which a robotic platform interacted with 50 distinct
objects, and we show that a residual neural network can accurately analyze
these measurements. Due to the low variance in consecutive spectral
measurements, our model achieved a material classification accuracy of 97.7%
when given only one spectral sample per object. Similar to prior works with
haptic sensors, we found that generalizing material recognition to new objects
posed a greater challenge, for which we achieved an accuracy of 81.4% via
leave-one-object-out cross-validation. From this work, we find that
spectroscopy poses a promising approach for further research in material
classification during robotic manipulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erickson_Z/0/1/0/all/0/1&quot;&gt;Zackory Erickson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luskey_N/0/1/0/all/0/1&quot;&gt;Nathan Luskey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chernova_S/0/1/0/all/0/1&quot;&gt;Sonia Chernova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1&quot;&gt;Charles C. Kemp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04155">
<title>Toward `verifying&apos; a Water Treatment System. (arXiv:1712.04155v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04155</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling and verifying real-world cyber-physical systems is challenging,
which is especially so for complex systems where manually modeling is
infeasible. In this work, we report our experience on combining model learning
and abstraction refinement to analyze a challenging system, i.e., a real-world
Secure Water Treatment system (SWaT). Given a set of safety requirements, the
objective is to either show that the system is safe with a high probability (so
that a system shutdown is rarely triggered due to safety violation) or not. As
the system is too complicated to be manually modeled, we apply latest automatic
model learning techniques to construct a set of Markov chains through
abstraction and refinement, based on two long system execution logs (one for
training and the other for testing). For each probabilistic safety property, we
either report it does not hold with a certain level of probabilistic
confidence, or report that it holds by showing the evidence in the form of an
abstract Markov chain. The Markov chains can subsequently be implemented as
runtime monitors in SWaT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yifan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1&quot;&gt;Shengchao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwu Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02861">
<title>Synthesizing Efficient Solutions for Patrolling Problems in the Internet Environment. (arXiv:1805.02861v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.02861</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an algorithm for constructing efficient patrolling strategies in
the Internet environment, where the protected targets are nodes connected to
the network and the patrollers are software agents capable of
detecting/preventing undesirable activities on the nodes. The algorithm is
based on a novel compositional principle designed for a special class of
strategies, and it can quickly construct (sub)optimal solutions even if the
number of targets reaches hundreds of millions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brazdil_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Br&amp;#xe1;zdil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kucera_A/0/1/0/all/0/1&quot;&gt;Anton&amp;#xed;n Ku&amp;#x10d;era&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rehak_V/0/1/0/all/0/1&quot;&gt;Vojt&amp;#x11b;ch &amp;#x158;eh&amp;#xe1;k&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03364">
<title>A Symbolic Approach to Explaining Bayesian Network Classifiers. (arXiv:1805.03364v1 [cs.AI] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1805.03364</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an approach for explaining Bayesian network classifiers, which is
based on compiling such classifiers into decision functions that have a
tractable and symbolic form. We introduce two types of explanations for why a
classifier may have classified an instance positively or negatively and suggest
algorithms for computing these explanations. The first type of explanation
identifies a minimal set of the currently active features that is responsible
for the current classification, while the second type of explanation identifies
a minimal set of features whose current state (active or not) is sufficient for
the classification. We consider in particular the compilation of Naive and
Latent-Tree Bayesian network classifiers into Ordered Decision Diagrams (ODDs),
providing a context for evaluating our proposal using case studies and
experiments based on classifiers from the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shih_A/0/1/0/all/0/1&quot;&gt;Andy Shih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_A/0/1/0/all/0/1&quot;&gt;Arthur Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darwiche_A/0/1/0/all/0/1&quot;&gt;Adnan Darwiche&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03644">
<title>Improving GAN Training via Binarized Representation Entropy (BRE) Regularization. (arXiv:1805.03644v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.03644</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel regularizer to improve the training of Generative
Adversarial Networks (GANs). The motivation is that when the discriminator D
spreads out its model capacity in the right way, the learning signals given to
the generator G are more informative and diverse. These in turn help G to
explore better and discover the real data manifold while avoiding large
unstable jumps due to the erroneous extrapolation made by D. Our regularizer
guides the rectifier discriminator D to better allocate its model capacity, by
encouraging the binary activation patterns on selected internal layers of D to
have a high joint entropy. Experimental results on both synthetic data and real
datasets demonstrate improvements in stability and convergence speed of the GAN
training, as well as higher sample quality. The approach also leads to higher
classification accuracies in semi-supervised learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yanshuai Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1&quot;&gt;Gavin Weiguang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lui_K/0/1/0/all/0/1&quot;&gt;Kry Yik-Chau Lui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ruitong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03777">
<title>Deep Reinforcement Learning for Optimal Control of Space Heating. (arXiv:1805.03777v1 [stat.AP])</title>
<link>http://arxiv.org/abs/1805.03777</link>
<description rdf:parseType="Literal">&lt;p&gt;Classical methods to control heating systems are often marred by suboptimal
performance, inability to adapt to dynamic conditions and unreasonable
assumptions e.g. existence of building models. This paper presents a novel deep
reinforcement learning algorithm which can control space heating in buildings
in a computationally efficient manner, and benchmarks it against other known
techniques. The proposed algorithm outperforms rule based control by between
5-10% in a simulation environment for a number of price signals. We conclude
that, while not optimal, the proposed algorithm offers additional practical
advantages such as faster computation times and increased robustness to
non-stationarities in building dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nagy_A/0/1/0/all/0/1&quot;&gt;Adam Nagy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kazmi_H/0/1/0/all/0/1&quot;&gt;Hussain Kazmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheaib_F/0/1/0/all/0/1&quot;&gt;Farah Cheaib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Driesen_J/0/1/0/all/0/1&quot;&gt;Johan Driesen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03779">
<title>k-Space Deep Learning for Accelerated MRI. (arXiv:1805.03779v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1805.03779</link>
<description rdf:parseType="Literal">&lt;p&gt;The annihilating filter-based low-rank Hanel matrix approach (ALOHA) is one
of the state-of-the-art compressed sensing approaches that directly
interpolates the missing k-space data using low-rank Hankel matrix completion.
Inspired by the recent mathematical discovery that links deep neural networks
to Hankel matrix decomposition using data-driven framelet basis, here we
propose a fully data-driven deep learning algorithm for k-space interpolation.
Our network can be also easily applied to non-Cartesian k-space trajectories by
simply adding an additional re-gridding layer. Extensive numerical experiments
show that the proposed deep learning method significantly outperforms the
existing image-domain deep learning approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yoseob Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03901">
<title>Loss-Calibrated Approximate Inference in Bayesian Neural Networks. (arXiv:1805.03901v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.03901</link>
<description rdf:parseType="Literal">&lt;p&gt;Current approaches in approximate inference for Bayesian neural networks
minimise the Kullback-Leibler divergence to approximate the true posterior over
the weights. However, this approximation is without knowledge of the final
application, and therefore cannot guarantee optimal predictions for a given
task. To make more suitable task-specific approximations, we introduce a new
loss-calibrated evidence lower bound for Bayesian neural networks in the
context of supervised learning, informed by Bayesian decision theory. By
introducing a lower bound that depends on a utility function, we ensure that
our approximation achieves higher utility than traditional methods for
applications that have asymmetric utility functions. Furthermore, in using
dropout inference, we highlight that our new objective is identical to that of
standard dropout neural networks, with an additional utility-dependent penalty
term. We demonstrate our new loss-calibrated model with an illustrative medical
example and a restricted model capacity experiment, and highlight failure modes
of the comparable weighted cross entropy approach. Lastly, we demonstrate the
scalability of our method to real world applications with per-pixel semantic
segmentation on an autonomous driving data set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cobb_A/0/1/0/all/0/1&quot;&gt;Adam D. Cobb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen J. Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gal_Y/0/1/0/all/0/1&quot;&gt;Yarin Gal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03911">
<title>Labelling as an unsupervised learning problem. (arXiv:1805.03911v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1805.03911</link>
<description rdf:parseType="Literal">&lt;p&gt;Unravelling hidden patterns in datasets is a classical problem with many
potential applications. In this paper, we present a challenge whose objective
is to discover nonlinear relationships in noisy cloud of points. If a set of
point satisfies a nonlinear relationship that is unlikely to be due to
randomness, we will label the set with this relationship. Since points can
satisfy one, many or no such nonlinear relationships, cloud of points will
typically have one, multiple or no labels at all. This introduces the labelling
problem that will be studied in this paper.
&lt;/p&gt;
&lt;p&gt;The objective of this paper is to develop a framework for the labelling
problem. We introduce a precise notion of a label, and we propose an algorithm
to discover such labels in a given dataset, which is then tested in synthetic
datasets. We also analyse, using tools from random matrix theory, the problem
of discovering false labels in the dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lyons_T/0/1/0/all/0/1&quot;&gt;Terry Lyons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arribas_I/0/1/0/all/0/1&quot;&gt;Imanol Perez Arribas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04018">
<title>Supervising Nystr\&quot;om Methods via Negative Margin Support Vector Selection. (arXiv:1805.04018v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1805.04018</link>
<description rdf:parseType="Literal">&lt;p&gt;Pattern recognition on big data can be challenging for kernel machines as the
complexity grows at least with the squared number of training samples.
Recently, methods to approximate explicit, low-dimensional feature mappings for
kernel functions have been applied to overcome this hurdle, such as the
Nystr\&quot;om methods and Random Fourier Features. The Nystr\&quot;om methods, in
particular, create the feature mappings from pairwise comparisons with the
training data. However, Nystr\&quot;om methods in previous works are generally
applied without the supervision provided by the training labels in the
classification/regression problems. This leads to the pairwise comparisons with
randomly chosen training samples in the model, instead of important ones.
Conversely, this work studies a supervised Nystr\&quot;om method, which chooses the
subsets of samples that are critical for the success of the Machine Learning
model. Particularly, we select the Nystr\&quot;om support vectors via the negative
margin criterion, and create explicit feature maps that are more suitable for
the classification task on the data. Experimental results on small to large
scale data sets show that our methods can significantly improve the
classification performance achieved via kernel approximation methods, and at
times, even exceed the performance of the full-dimensional kernel machines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_M/0/1/0/all/0/1&quot;&gt;Mert Al&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chanyaswad_T/0/1/0/all/0/1&quot;&gt;Thee Chanyaswad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kung_S/0/1/0/all/0/1&quot;&gt;Sun-Yuan Kung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.06997">
<title>On the uniqueness and stability of dictionaries for sparse representation of noisy signals. (arXiv:1606.06997v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1606.06997</link>
<description rdf:parseType="Literal">&lt;p&gt;Dictionary learning for sparse linear coding has exposed characteristic
properties of natural signals. However, a universal theorem guaranteeing the
consistency of estimation in this model is lacking. Here, we prove that for all
diverse enough datasets generated from the sparse coding model, latent
dictionaries and codes are uniquely and stably determined up to measurement
error. Applications are given to data analysis, engineering, and neuroscience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garfinkle_C/0/1/0/all/0/1&quot;&gt;Charles J. Garfinkle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hillar_C/0/1/0/all/0/1&quot;&gt;Christopher J. Hillar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.06619">
<title>Most Ligand-Based Classification Benchmarks Reward Memorization Rather than Generalization. (arXiv:1706.06619v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/1706.06619</link>
<description rdf:parseType="Literal">&lt;p&gt;Undetected overfitting can occur when there are significant redundancies
between training and validation data. We describe AVE, a new measure of
training-validation redundancy for ligand-based classification problems that
accounts for the similarity amongst inactive molecules as well as active. We
investigated seven widely-used benchmarks for virtual screening and
classification, and show that the amount of AVE bias strongly correlates with
the performance of ligand-based predictive methods irrespective of the
predicted property, chemical fingerprint, similarity measure, or
previously-applied unbiasing techniques. Therefore, it may be that the
previously-reported performance of most ligand-based methods can be explained
by overfitting to benchmarks rather than good prospective accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wallach_I/0/1/0/all/0/1&quot;&gt;Izhar Wallach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Heifets_A/0/1/0/all/0/1&quot;&gt;Abraham Heifets&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.03528">
<title>GIANT: Globally Improved Approximate Newton Method for Distributed Optimization. (arXiv:1709.03528v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.03528</link>
<description rdf:parseType="Literal">&lt;p&gt;For distributed computing environments, we consider the canonical machine
learning problem of empirical risk minimization (ERM) with quadratic
regularization, and we propose a distributed and communication-efficient
Newton-type optimization method. At every iteration, each worker locally finds
an Approximate NewTon (ANT) direction, and then it sends this direction to the
main driver. The driver, then, averages all the ANT directions received from
workers to form a Globally Improved ANT (GIANT) direction. GIANT naturally
exploits the trade-offs between local computations and global communications in
that more local computations result in fewer overall rounds of communications.
GIANT is highly communication efficient in that, for $d$-dimensional data
uniformly distributed across $m$ workers, it has $4$ or $6$ rounds of
communication and $O (d \log m)$ communication complexity per iteration.
Theoretically, we show that GIANT&apos;s convergence rate is faster than first-order
methods and existing distributed Newton-type methods. From a practical
point-of-view, a highly beneficial feature of GIANT is that it has only one
tuning parameter---the iterations of the local solver for computing an ANT
direction. This is indeed in sharp contrast with many existing distributed
Newton-type methods, as well as popular first-order methods, which have several
tuning parameters, and whose performance can be greatly affected by the
specific choices of such parameters. In this light, we empirically demonstrate
the superior performance of GIANT compared with other competing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shusen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roosta_Khorasani_F/0/1/0/all/0/1&quot;&gt;Farbod Roosta-Khorasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1&quot;&gt;Michael W. Mahoney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05398">
<title>On the Direction of Discrimination: An Information-Theoretic Analysis of Disparate Impact in Machine Learning. (arXiv:1801.05398v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1801.05398</link>
<description rdf:parseType="Literal">&lt;p&gt;Disparate impact in machine learning is a critical issue with important
societal implications. In this paper, we proposed an information-theoretic
framework to study disparate impact. We derived a correction function that
reflects how components of the input variables $X$ affect the disparity in the
output distributions. We then demonstrated how our framework could be used on a
recidivism prediction application derived from a real-world dataset.
Interesting directions for future work include extending our analysis to a
broader class of predictive models, and using correction functions to design
machine learning algorithms that mitigate disparate impact. We are confident
that information-theoretic tools can inspire exciting new solutions to the
problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ustun_B/0/1/0/all/0/1&quot;&gt;Berk Ustun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calmon_F/0/1/0/all/0/1&quot;&gt;Flavio P. Calmon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01113">
<title>Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed SGD. (arXiv:1803.01113v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01113</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed Stochastic Gradient Descent (SGD) when run in a synchronous
manner, suffers from delays in waiting for the slowest learners (stragglers).
Asynchronous methods can alleviate stragglers, but cause gradient staleness
that can adversely affect convergence. In this work we present a novel
theoretical characterization of the speed-up offered by asynchronous methods by
analyzing the trade-off between the error in the trained model and the actual
training runtime (wallclock time). The novelty in our work is that our runtime
analysis considers random straggler delays, which helps us design and compare
distributed SGD algorithms that strike a balance between stragglers and
staleness. We also present a new convergence analysis of asynchronous SGD
variants without bounded or exponential delay assumptions, and a novel learning
rate schedule to compensate for gradient staleness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Sanghamitra Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Joshi_G/0/1/0/all/0/1&quot;&gt;Gauri Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Soumyadip Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dube_P/0/1/0/all/0/1&quot;&gt;Parijat Dube&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nagpurkar_P/0/1/0/all/0/1&quot;&gt;Priya Nagpurkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05345">
<title>Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds. (arXiv:1804.05345v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05345</link>
<description rdf:parseType="Literal">&lt;p&gt;The deployment of state-of-the-art neural networks containing millions of
parameters to resource-constrained platforms may be prohibitive in terms of
both time and space. We present an efficient coresets-based neural network
compression algorithm that provably sparsifies the parameters of a trained
fully-connected neural network in a manner that approximately preserves the
network&apos;s output. Our approach is based on an importance sampling scheme that
judiciously defines a sampling distribution over the neural network parameters,
and as a result, retains parameters of high importance while discarding
redundant ones. We leverage a novel, empirical notion of sensitivity and extend
traditional coreset constructions to the application of compressing parameters.
Our theoretical analysis establishes guarantees on the size and accuracy of the
resulting compressed neural network and gives rise to new generalization bounds
that may provide novel insights on the generalization properties of neural
networks. We demonstrate the practical effectiveness of our algorithm on a
variety of neural network configurations and real-world data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baykal_C/0/1/0/all/0/1&quot;&gt;Cenk Baykal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1&quot;&gt;Lucas Liebenwein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilitschenski_I/0/1/0/all/0/1&quot;&gt;Igor Gilitschenski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_D/0/1/0/all/0/1&quot;&gt;Dan Feldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1&quot;&gt;Daniela Rus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07891">
<title>A Deep Learning Approach for Forecasting Air Pollution in South Korea Using LSTM. (arXiv:1804.07891v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07891</link>
<description rdf:parseType="Literal">&lt;p&gt;Tackling air pollution is an imperative problem in South Korea, especially in
urban areas, over the last few years. More specially, South Korea has joined
the ranks of the world&apos;s most polluted countries alongside with other Asian
capitals, such as Beijing or Delhi. Much research is being conducted in
environmental science to evaluate the dangerous impact of particulate matters
on public health. Besides that, deterministic models of air pollutant behavior
are also generated; however, this is both complex and often inaccurate. On the
contrary, deep recurrent neural network reveals potent potential on forecasting
out-comes of time-series data and has become more prevalent. This paper uses
Recurrent Neural Network (RNN) with Long Short-Term Memory units as a framework
for leveraging knowledge from time-series data of air pollution and
meteorological information in Daegu, Seoul, Beijing, and Shenyang.
Additionally, we use encoder-decoder model, which is similar to machine
comprehension problems, as a crucial part of our prediction machine. Finally,
we investigate the prediction accuracy of various configurations. Our
experiments prevent the efficiency of integrating multiple layers of RNN on
prediction model when forecasting far timesteps ahead. This research is a
significant motivation for not only continuing researching on urban air quality
but also help the government leverage that insight to enact beneficial policies
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1&quot;&gt;Tien-Cuong Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1&quot;&gt;Van-Duc Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1&quot;&gt;Sang-Kyun Cha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.03444">
<title>Controlling the privacy loss with the input feature maps of the layers in convolutional neural networks. (arXiv:1805.03444v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.03444</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the method to sanitize the privacy of the IFM(Input Feature Map)s
that are fed into the layers of CNN(Convolutional Neural Network)s. The method
introduces the degree of the sanitization that makes the application using a
CNN be able to control the privacy loss represented as the ratio of the
probabilistic accuracies for original IFM(Input Feature Map) and sanitized IFM.
For the sanitization of an IFM, the sample-and-hold based approximation scheme
is devised to satisfy an application-specific degree of the sanitization. The
scheme approximates an IFM by replacing all the samples in a window with the
non-zero sample closest to the mean of the sampling window. It also removes the
dependency on CNN configuration by unfolding multi-dimensional IFM tensors into
one-dimensional streams to be approximated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chun_W/0/1/0/all/0/1&quot;&gt;Woohyung Chun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Sung-Min Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huh_J/0/1/0/all/0/1&quot;&gt;Junho Huh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_I/0/1/0/all/0/1&quot;&gt;Inyup Kang&lt;/a&gt;</dc:creator>
</item></rdf:RDF>