<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-14T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04752"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04440"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04449"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04527"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04617"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04620"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04679"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04750"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04758"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04803"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.03875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.07178"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08033"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11298"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.11622"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.06689"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.02238"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04433"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04439"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04441"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04447"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04523"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04550"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04580"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04759"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04760"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.04768"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.07268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05409"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07572"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.02483"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12528"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04900"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03298"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09958"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.01857"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.04752">
<title>A Survey on Methods and Theories of Quantized Neural Networks. (arXiv:1808.04752v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04752</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are the state-of-the-art methods for many real-world
tasks, such as computer vision, natural language processing and speech
recognition. For all its popularity, deep neural networks are also criticized
for consuming a lot of memory and draining battery life of devices during
training and inference. This makes it hard to deploy these models on mobile or
embedded devices which have tight resource constraints. Quantization is
recognized as one of the most effective approaches to satisfy the extreme
memory requirements that deep neural network models demand. Instead of adopting
32-bit floating point format to represent weights, quantized representations
store weights using more compact formats such as integers or even binary
numbers. Despite a possible degradation in predictive performance, quantization
provides a potential solution to greatly reduce the model size and the energy
consumption. In this survey, we give a thorough review of different aspects of
quantized neural networks. Current challenges and trends of quantized neural
networks are also discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yunhui Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04440">
<title>FaceOff: Anonymizing Videos in the Operating Rooms. (arXiv:1808.04440v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.04440</link>
<description rdf:parseType="Literal">&lt;p&gt;Video capture in the surgical operating room (OR) is increasingly possible
and has potential for use with computer assisted interventions (CAI), surgical
data science and within smart OR integration. Captured video innately carries
sensitive information that should not be completely visible in order to
preserve the patient&apos;s and the clinical teams&apos; identities. When surgical video
streams are stored on a server, the videos must be anonymized prior to storage
if taken outside of the hospital. In this article, we describe how a deep
learning model, Faster R-CNN, can be used for this purpose and help to
anonymize video data captured in the OR. The model detects and blurs faces in
an effort to preserve anonymity. After testing an existing face detection
trained model, a new dataset tailored to the surgical environment, with faces
obstructed by surgical masks and caps, was collected for fine-tuning to achieve
higher face-detection rates in the OR. We also propose a temporal
regularisation kernel to improve recall rates. The fine-tuned model achieves a
face detection recall of 88.05 % and 93.45 % before and after applying
temporal-smoothing respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flouty_E/0/1/0/all/0/1&quot;&gt;Evangello Flouty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisimopoulos_O/0/1/0/all/0/1&quot;&gt;Odysseas Zisimopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1&quot;&gt;Danail Stoyanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04449">
<title>Starting Movement Detection of Cyclists Using Smart Devices. (arXiv:1808.04449v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.04449</link>
<description rdf:parseType="Literal">&lt;p&gt;In near future, vulnerable road users (VRUs) such as cyclists and pedestrians
will be equipped with smart devices and wearables which are capable to
communicate with intelligent vehicles and other traffic participants. Road
users are then able to cooperate on different levels, such as in cooperative
intention detection for advanced VRU protection. Smart devices can be used to
detect intentions, e.g., an occluded cyclist intending to cross the road, to
warn vehicles of VRUs, and prevent potential collisions. This article presents
a human activity recognition approach to detect the starting movement of
cyclists wearing smart devices. We propose a novel two-stage feature selection
procedure using a score specialized for robust starting detection reducing the
false positive detections and leading to understandable and interpretable
features. The detection is modelled as a classification problem and realized by
means of a machine learning classifier. We introduce an auxiliary class, that
models starting movements and allows to integrate early movement indicators,
i.e., body part movements indicating future behaviour. In this way we improve
the robustness and reduce the detection time of the classifier. Our empirical
studies with real-world data originating from experiments which involve 49 test
subjects and consists of 84 starting motions show that we are able to detect
the starting movements early. Our approach reaches an F1-score of 67 % within
0.33 s after the first movement of the bicycle wheel. Investigations concerning
the device wearing location show that for devices worn in the trouser pocket
the detector has less false detections and detects starting movements faster on
average. We found that we can further improve the results when we train
distinct classifiers for different wearing locations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bieshaar_M/0/1/0/all/0/1&quot;&gt;Maarten Bieshaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Depping_M/0/1/0/all/0/1&quot;&gt;Malte Depping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneegans_J/0/1/0/all/0/1&quot;&gt;Jan Schneegans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1&quot;&gt;Bernhard Sick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04527">
<title>Weight Learning in a Probabilistic Extension of Answer Set Programs. (arXiv:1808.04527v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.04527</link>
<description rdf:parseType="Literal">&lt;p&gt;LPMLN is a probabilistic extension of answer set programs with the weight
scheme derived from that of Markov Logic. Previous work has shown how inference
in LPMLN can be achieved. In this paper, we present the concept of weight
learning in LPMLN and learning algorithms for LPMLN derived from those for
Markov Logic. We also present a prototype implementation that uses answer set
solvers for learning as well as some example domains that illustrate distinct
features of LPMLN learning. Learning in LPMLN is in accordance with the stable
model semantics, thereby it learns parameters for probabilistic extensions of
knowledge-rich domains where answer set programming has shown to be useful but
limited to the deterministic case, such as reachability analysis and reasoning
about actions in dynamic domains. We also apply the method to learn the
parameters for probabilistic abductive reasoning about actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joohyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04617">
<title>Joint Ground and Aerial Package Delivery Services: A Stochastic Optimization Approach. (arXiv:1808.04617v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.04617</link>
<description rdf:parseType="Literal">&lt;p&gt;Unmanned aerial vehicles (UAVs), also known as drones, have emerged as a
promising mode of fast, energy-efficient, and cost-effective package delivery.
A considerable number of works have studied different aspects of drone package
delivery service by a supplier, one of which is delivery planning. However,
existing works addressing the planning issues consider a simple case of perfect
delivery without service interruption, e.g., due to accident which is common
and realistic. Therefore, this paper introduces the joint ground and aerial
delivery service optimization and planning (GADOP) framework. The framework
explicitly incorporates uncertainty of drone package delivery, i.e., takeoff
and breakdown conditions. The GADOP framework aims to minimize the total
delivery cost given practical constraints, e.g., traveling distance limit.
Specifically, we formulate the GADOP framework as a three-stage stochastic
integer programming model. To deal with the high complexity issue of the
problem, a decomposition method is adopted. Then, the performance of the GADOP
framework is evaluated by using two data sets including Solomon benchmark suite
and the real data from one of the Singapore logistics companies. The
performance evaluation clearly shows that the GADOP framework can achieve
significantly lower total payment than that of the baseline methods which do
not take uncertainty into account.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawadsitang_S/0/1/0/all/0/1&quot;&gt;Suttinee Sawadsitang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1&quot;&gt;Dusit Niyato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1&quot;&gt;Puay-Siew Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Ping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04620">
<title>Applying the Closed World Assumption to SUMO-based Ontologies. (arXiv:1808.04620v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.04620</link>
<description rdf:parseType="Literal">&lt;p&gt;In commonsense knowledge representation, the Open World Assumption is adopted
as a general standard strategy for the design, construction and use of
ontologies, e.g. in OWL. This strategy limits the inferencing capabilities of
any system using these ontologies because non-asserted statements could be
assumed to be alternatively true or false in different interpretations. In this
paper, we investigate the application of the Closed World Assumption to enable
a better exploitation of the structural knowledge encoded in a SUMO-based
ontology. To that end, we explore three different Closed World Assumption
formulations for subclass and disjoint relations in order to reduce the
ambiguity of the knowledge encoded in first-order logic ontologies. We evaluate
these formulations on a practical experimentation using a very large
commonsense benchmark automatically obtained from the knowledge encoded in
WordNet through its mapping to SUMO. The results show that the competency of
the ontology improves more than 47 % when reasoning under the Closed World
Assumption. As conclusion, applying the Closed World Assumption automatically
to first-order logic ontologies reduces their expressed ambiguity and more
commonsense questions can be answered.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvez_J/0/1/0/all/0/1&quot;&gt;Javier &amp;#xc1;lvez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_Dios_I/0/1/0/all/0/1&quot;&gt;Itziar Gonzalez-Dios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rigau_G/0/1/0/all/0/1&quot;&gt;German Rigau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04679">
<title>An Optimal Policy for Patient Laboratory Tests in Intensive Care Units. (arXiv:1808.04679v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.04679</link>
<description rdf:parseType="Literal">&lt;p&gt;Laboratory testing is an integral tool in the management of patient care in
hospitals, particularly in intensive care units (ICUs). There exists an
inherent trade-off in the selection and timing of lab tests between
considerations of the expected utility in clinical decision-making of a given
test at a specific time, and the associated cost or risk it poses to the
patient. In this work, we introduce a framework that learns policies for
ordering lab tests which optimizes for this trade-off. Our approach uses batch
off-policy reinforcement learning with a composite reward function based on
clinical imperatives, applied to data that include examples of clinicians
ordering labs for patients. To this end, we develop and extend principles of
Pareto optimality to improve the selection of actions based on multiple reward
function components while respecting typical procedural considerations and
prioritization of clinical goals in the ICU. Our experiments show that we can
estimate a policy that reduces the frequency of lab tests and optimizes timing
to minimize information redundancy. We also find that the estimated policies
typically suggest ordering lab tests well ahead of critical onsets--such as
mechanical ventilation or dialysis--that depend on the lab results. We evaluate
our approach by quantifying how these policies may initiate earlier onset of
treatment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Li-Fang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_N/0/1/0/all/0/1&quot;&gt;Niranjani Prasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engelhardt_B/0/1/0/all/0/1&quot;&gt;Barbara E Engelhardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04750">
<title>Quantifying the Influences on Probabilistic Wind Power Forecasts. (arXiv:1808.04750v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.04750</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, probabilistic forecasts techniques were proposed in research
as well as in applications to integrate volatile renewable energy resources
into the electrical grid. These techniques allow decision makers to take the
uncertainty of the prediction into account and, therefore, to devise optimal
decisions, e.g., related to costs and risks in the electrical grid. However, it
was yet not studied how the input, such as numerical weather predictions,
affects the model output of forecasting models in detail. Therefore, we examine
the potential influences with techniques from the field of sensitivity analysis
on three different black-box models to obtain insights into differences and
similarities of these probabilistic models. The analysis shows a considerable
number of potential influences in those models depending on, e.g., the
predicted probability and the type of model. These effects motivate the need to
take various influences into account when models are tested, analyzed, or
compared. Nevertheless, results of the sensitivity analysis will allow us to
select a model with advantages in the practical application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schreiber_J/0/1/0/all/0/1&quot;&gt;Jens Schreiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sick_B/0/1/0/all/0/1&quot;&gt;Bernhard Sick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04758">
<title>Finding Minimal Cost Herbrand Models with Branch-Cut-and-Price. (arXiv:1808.04758v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.04758</link>
<description rdf:parseType="Literal">&lt;p&gt;Given (1) a set of clauses $T$ in some first-order language $\cal L$ and (2)
a cost function $c : B_{{\cal L}} \rightarrow \mathbb{R}_{+}$, mapping each
ground atom in the Herbrand base $B_{{\cal L}}$ to a non-negative real, then
the problem of finding a minimal cost Herbrand model is to either find a
Herbrand model $\cal I$ of $T$ which is guaranteed to minimise the sum of the
costs of true ground atoms, or establish that there is no Herbrand model for
$T$. A branch-cut-and-price integer programming (IP) approach to solving this
problem is presented. Since the number of ground instantiations of clauses and
the size of the Herbrand base are both infinite in general, we add the
corresponding IP constraints and IP variables `on the fly&apos; via `cutting&apos; and
`pricing&apos; respectively. In the special case of a finite Herbrand base we show
that adding all IP variables and constraints from the outset can be
advantageous, showing that a challenging Markov logic network MAP problem can
be solved in this way if encoded appropriately.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cussens_J/0/1/0/all/0/1&quot;&gt;James Cussens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04794">
<title>Improving Hearthstone AI by Combining MCTS and Supervised Learning Algorithms. (arXiv:1808.04794v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.04794</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the impact of supervised prediction models on the strength and
efficiency of artificial agents that use the Monte-Carlo Tree Search (MCTS)
algorithm to play a popular video game Hearthstone: Heroes of Warcraft. We
overview our custom implementation of the MCTS that is well-suited for games
with partially hidden information and random effects. We also describe
experiments which we designed to quantify the performance of our Hearthstone
agent&apos;s decision making. We show that even simple neural networks can be
trained and successfully used for the evaluation of game states. Moreover, we
demonstrate that by providing a guidance to the game state search heuristic, it
is possible to substantially improve the win rate, and at the same time reduce
the required computations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swiechowski_M/0/1/0/all/0/1&quot;&gt;Maciej &amp;#x15a;wiechowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tajmajer_T/0/1/0/all/0/1&quot;&gt;Tomasz Tajmajer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janusz_A/0/1/0/all/0/1&quot;&gt;Andrzej Janusz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04803">
<title>Hierarchical binary CNNs for landmark localization with limited resources. (arXiv:1808.04803v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.04803</link>
<description rdf:parseType="Literal">&lt;p&gt;Our goal is to design architectures that retain the groundbreaking
performance of Convolutional Neural Networks (CNNs) for landmark localization
and at the same time are lightweight, compact and suitable for applications
with limited computational resources. To this end, we make the following
contributions: (a) we are the first to study the effect of neural network
binarization on localization tasks, namely human pose estimation and face
alignment. We exhaustively evaluate various design choices, identify
performance bottlenecks, and more importantly propose multiple orthogonal ways
to boost performance. (b) Based on our analysis, we propose a novel
hierarchical, parallel and multi-scale residual architecture that yields large
performance improvement over the standard bottleneck block while having the
same number of parameters, thus bridging the gap between the original network
and its binarized counterpart. (c) We perform a large number of ablation
studies that shed light on the properties and the performance of the proposed
block. (d) We present results for experiments on the most challenging datasets
for human pose estimation and face alignment, reporting in many cases
state-of-the-art performance. (e) We further provide additional results for the
problem of facial part segmentation. Code can be downloaded from
https://www.adrianbulat.com/binary-cnn-landmark
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulat_A/0/1/0/all/0/1&quot;&gt;Adrian Bulat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1&quot;&gt;Georgios Tzimiropoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.03875">
<title>Learning Task Specifications from Demonstrations. (arXiv:1710.03875v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.03875</link>
<description rdf:parseType="Literal">&lt;p&gt;Real world applications often naturally decompose into several sub-tasks. In
many settings (e.g., robotics) demonstrations provide a natural way to specify
the sub-tasks. However, most methods for learning from demonstrations either do
not provide guarantees that the artifacts learned for the subtasks can be
safely recombined or limit the types of composition available. Motivated by
this deficit, we consider the problem of inferring binary non-Markovian
rewards, also known as logical trace properties or \emph{specifications}, from
demonstrations provided by an agent operating in an uncertain, stochastic
environment. Crucially, specifications admit well-defined composition rules
that are typically easy to interpret. In this paper, we formulate the
specification inference task as a maximum a posteriori (MAP) probability
inference problem, apply the principle of maximum entropy to derive an analytic
demonstration likelihood model and give an efficient approach to search for the
most likely specification in a large candidate pool of specifications. In our
experiments, we demonstrate how learning specifications can help avoid common
bugs that often occur due to ad-hoc reward composition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazquez_Chanlatte_M/0/1/0/all/0/1&quot;&gt;Marcell Vazquez-Chanlatte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1&quot;&gt;Susmit Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1&quot;&gt;Ashish Tiwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1&quot;&gt;Mark K. Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1&quot;&gt;Sanjit A. Seshia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.07178">
<title>Vehicle Communication Strategies for Simulated Highway Driving. (arXiv:1804.07178v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/1804.07178</link>
<description rdf:parseType="Literal">&lt;p&gt;Interest in emergent communication has recently surged in Machine Learning.
The focus of this interest has largely been either on investigating the
properties of the learned protocol or on utilizing emergent communication to
better solve problems that already have a viable solution. Here, we consider
self-driving cars coordinating with each other and focus on how communication
influences the agents&apos; collective behavior. Our main result is that
communication helps (most) with adverse conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Resnick_C/0/1/0/all/0/1&quot;&gt;Cinjon Resnick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulikov_I/0/1/0/all/0/1&quot;&gt;Ilya Kulikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1&quot;&gt;Jason Weston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08033">
<title>Learning from the experts: From expert systems to machine-learned diagnosis models. (arXiv:1804.08033v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08033</link>
<description rdf:parseType="Literal">&lt;p&gt;Expert diagnostic support systems have been extensively studied. The
practical applications of these systems in real-world scenarios have been
somewhat limited due to well-understood shortcomings, such as lack of
extensibility. More recently, machine-learned models for medical diagnosis have
gained momentum, since they can learn and generalize patterns found in very
large datasets like electronic health records. These models also have
shortcomings - in particular, there is no easy way to incorporate prior
knowledge from existing literature or experts. In this paper, we present a
method to merge both approaches by using expert systems as generative models
that create simulated data on which models can be learned. We demonstrate that
such a learned model not only preserves the original properties of the expert
systems but also addresses some of their limitations. Furthermore, we show how
this approach can also be used as the starting point to combine expert
knowledge with knowledge extracted from other data sources, such as electronic
health records.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravuri_M/0/1/0/all/0/1&quot;&gt;Murali Ravuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1&quot;&gt;Anitha Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tso_G/0/1/0/all/0/1&quot;&gt;Geoffrey J. Tso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amatriain_X/0/1/0/all/0/1&quot;&gt;Xavier Amatriain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11298">
<title>A General Multi-agent Epistemic Planner Based on Higher-order Belief Change. (arXiv:1806.11298v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1806.11298</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, multi-agent epistemic planning has received attention from
both dynamic logic and planning communities. Existing implementations of
multi-agent epistemic planning are based on compilation into classical planning
and suffer from various limitations, such as generating only linear plans,
restriction to public actions, and incapability to handle disjunctive beliefs.
In this paper, we propose a general representation language for multi-agent
epistemic planning where the initial KB and the goal, the preconditions and
effects of actions can be arbitrary multi-agent epistemic formulas, and the
solution is an action tree branching on sensing results. To support efficient
reasoning in the multi-agent KD45 logic, we make use of a normal form called
alternating cover disjunctive formulas (ACDFs). We propose basic revision and
update algorithms for ACDFs. We also handle static propositional common
knowledge, which we call constraints. Based on our reasoning, revision and
update algorithms, adapting the PrAO algorithm for contingent planning from the
literature, we implemented a multi-agent epistemic planner called MEPK. Our
experimental results show the viability of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1&quot;&gt;Biqing Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1&quot;&gt;Hai Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongmei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.11622">
<title>Count-Based Exploration with the Successor Representation. (arXiv:1807.11622v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.11622</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of exploration in reinforcement learning is well-understood in
the tabular case and many sample-efficient algorithms are known. Nevertheless,
it is often unclear how the algorithms in the tabular setting can be extended
to tasks with large state-spaces where generalization is required. Recent
promising developments generally depend on problem-specific density models or
handcrafted features. In this paper we introduce a simple approach for
exploration that allows us to develop theoretically justified algorithms in the
tabular case but that also give us intuitions for new algorithms applicable to
settings where function approximation is required. Our approach and its
underlying theory is based on the substochastic successor representation, a
concept we develop here. While the traditional successor representation is a
representation that defines state generalization by the similarity of successor
states, the substochastic successor representation is also able to implicitly
count the number of times each state (or feature) has been observed. This
extension connects two until now disjoint areas of research. We show in
traditional tabular domains (RiverSwim and SixArms) that our algorithm
empirically performs as well as other sample-efficient algorithms. We then
describe a deep reinforcement learning algorithm inspired by these ideas and
show that it matches the performance of recent pseudo-count-based methods in
hard exploration Atari 2600 games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1&quot;&gt;Marlos C. Machado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellemare_M/0/1/0/all/0/1&quot;&gt;Marc G. Bellemare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowling_M/0/1/0/all/0/1&quot;&gt;Michael Bowling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.06689">
<title>Chemception: A Deep Neural Network with Minimal Chemistry Knowledge Matches the Performance of Expert-developed QSAR/QSPR Models. (arXiv:1706.06689v1 [stat.ML] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1706.06689</link>
<description rdf:parseType="Literal">&lt;p&gt;In the last few years, we have seen the transformative impact of deep
learning in many applications, particularly in speech recognition and computer
vision. Inspired by Google&apos;s Inception-ResNet deep convolutional neural network
(CNN) for image classification, we have developed &quot;Chemception&quot;, a deep CNN for
the prediction of chemical properties, using just the images of 2D drawings of
molecules. We develop Chemception without providing any additional explicit
chemistry knowledge, such as basic concepts like periodicity, or advanced
features like molecular descriptors and fingerprints. We then show how
Chemception can serve as a general-purpose neural network architecture for
predicting toxicity, activity, and solvation properties when trained on a
modest database of 600 to 40,000 compounds. When compared to multi-layer
perceptron (MLP) deep neural networks trained with ECFP fingerprints,
Chemception slightly outperforms in activity and solvation prediction and
slightly underperforms in toxicity prediction. Having matched the performance
of expert-developed QSAR/QSPR deep learning models, our work demonstrates the
plausibility of using deep neural networks to assist in computational chemistry
research, where the feature engineering process is performed primarily by a
deep learning algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goh_G/0/1/0/all/0/1&quot;&gt;Garrett B. Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siegel_C/0/1/0/all/0/1&quot;&gt;Charles Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vishnu_A/0/1/0/all/0/1&quot;&gt;Abhinav Vishnu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hodas_N/0/1/0/all/0/1&quot;&gt;Nathan O. Hodas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baker_N/0/1/0/all/0/1&quot;&gt;Nathan Baker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.02238">
<title>How Much Chemistry Does a Deep Neural Network Need to Know to Make Accurate Predictions?. (arXiv:1710.02238v2 [stat.ML] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1710.02238</link>
<description rdf:parseType="Literal">&lt;p&gt;The meteoric rise of deep learning models in computer vision research, having
achieved human-level accuracy in image recognition tasks is firm evidence of
the impact of representation learning of deep neural networks. In the chemistry
domain, recent advances have also led to the development of similar CNN models,
such as Chemception, that is trained to predict chemical properties using
images of molecular drawings. In this work, we investigate the effects of
systematically removing and adding localized domain-specific information to the
image channels of the training data. By augmenting images with only 3
additional basic information, and without introducing any architectural
changes, we demonstrate that an augmented Chemception (AugChemception)
outperforms the original model in the prediction of toxicity, activity, and
solvation free energy. Then, by altering the information content in the images,
and examining the resulting model&apos;s performance, we also identify two distinct
learning patterns in predicting toxicity/activity as compared to solvation free
energy. These patterns suggest that Chemception is learning about its tasks in
the manner that is consistent with established knowledge. Thus, our work
demonstrates that advanced chemical knowledge is not a pre-requisite for deep
learning models to accurately predict complex chemical properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goh_G/0/1/0/all/0/1&quot;&gt;Garrett B. Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siegel_C/0/1/0/all/0/1&quot;&gt;Charles Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vishnu_A/0/1/0/all/0/1&quot;&gt;Abhinav Vishnu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hodas_N/0/1/0/all/0/1&quot;&gt;Nathan O. Hodas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baker_N/0/1/0/all/0/1&quot;&gt;Nathan Baker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02034">
<title>SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for Predicting Chemical Properties. (arXiv:1712.02034v2 [stat.ML] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.02034</link>
<description rdf:parseType="Literal">&lt;p&gt;Chemical databases store information in text representations, and the SMILES
format is a universal standard used in many cheminformatics software. Encoded
in each SMILES string is structural information that can be used to predict
complex chemical properties. In this work, we develop SMILES2vec, a deep RNN
that automatically learns features from SMILES to predict chemical properties,
without the need for additional explicit feature engineering. Using Bayesian
optimization methods to tune the network architecture, we show that an
optimized SMILES2vec model can serve as a general-purpose neural network for
predicting distinct chemical properties including toxicity, activity,
solubility and solvation energy, while also outperforming contemporary MLP
neural networks that uses engineered features. Furthermore, we demonstrate
proof-of-concept of interpretability by developing an explanation mask that
localizes on the most important characters used in making a prediction. When
tested on the solubility dataset, it identified specific parts of a chemical
that is consistent with established first-principles knowledge with an accuracy
of 88%. Our work demonstrates that neural networks can learn technically
accurate chemical concept and provide state-of-the-art accuracy, making
interpretable deep neural networks a useful tool of relevance to the chemical
industry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goh_G/0/1/0/all/0/1&quot;&gt;Garrett B. Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hodas_N/0/1/0/all/0/1&quot;&gt;Nathan O. Hodas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siegel_C/0/1/0/all/0/1&quot;&gt;Charles Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vishnu_A/0/1/0/all/0/1&quot;&gt;Abhinav Vishnu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04433">
<title>Out of the Black Box: Properties of deep neural networks and their applications. (arXiv:1808.04433v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.04433</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are powerful machine learning approaches that have
exhibited excellent results on many classification tasks. However, they are
considered as black boxes and some of their properties remain to be formalized.
In the context of image recognition, it is still an arduous task to understand
why an image is recognized or not. In this study, we formalize some properties
shared by eight state-of-the-art deep neural networks in order to grasp the
principles allowing a given deep neural network to classify an image. Our
results, tested on these eight networks, show that an image can be sub-divided
into several regions (patches) responding at different degrees of probability
(local property). With the same patch, some locations in the image can answer
two (or three) orders of magnitude higher than other locations (spatial
property). Some locations are activators and others inhibitors
(activation-inhibition property). The repetition of the same patch can increase
(or decrease) the probability of recognition of an object (cumulative
property). Furthermore, we propose a new approach called Deepception that
exploits these properties to deceive a deep neural network. We obtain for the
VGG-VDD-19 neural network a fooling ratio of 88\%. Thanks to our
&quot;Psychophysics&quot; approach, no prior knowledge on the networks architectures is
required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouarti_N/0/1/0/all/0/1&quot;&gt;Nizar Ouarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmona_D/0/1/0/all/0/1&quot;&gt;David Carmona&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04439">
<title>Image Registration and Predictive Modeling: Learning the Metric on the Space of Diffeomorphisms. (arXiv:1808.04439v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.04439</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method for metric optimization in the Large Deformation
Diffeomorphic Metric Mapping (LDDMM) framework, by treating the induced
Riemannian metric on the space of diffeomorphisms as a kernel in a machine
learning context. For simplicity, we choose the kernel Fischer Linear
Discriminant Analysis (KLDA) as the framework. Optimizing the kernel parameters
in an Expectation-Maximization framework, we define model fidelity via the
hinge loss of the decision function. The resulting algorithm optimizes the
parameters of the LDDMM norm-inducing differential operator as a solution to a
group-wise registration and classification problem. In practice, this may lead
to a biology-aware registration, focusing its attention on the predictive task
at hand such as identifying the effects of disease. We first tested our
algorithm on a synthetic dataset, showing that our parameter selection improves
registration quality and classification accuracy. We then tested the algorithm
on 3D subcortical shapes from the Schizophrenia cohort Schizconnect. Our
Schizpohrenia-Control predictive model showed significant improvement in ROC
AUC compared to baseline parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mussabayeva_A/0/1/0/all/0/1&quot;&gt;Ayagoz Mussabayeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kroshnin_A/0/1/0/all/0/1&quot;&gt;Alexey Kroshnin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurmukov_A/0/1/0/all/0/1&quot;&gt;Anvar Kurmukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dodonova_Y/0/1/0/all/0/1&quot;&gt;Yulia Dodonova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_S/0/1/0/all/0/1&quot;&gt;Shan Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutman_B/0/1/0/all/0/1&quot;&gt;Boris A. Gutman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04441">
<title>Deep Morphing: Detecting bone structures in fluoroscopic X-ray images with prior knowledge. (arXiv:1808.04441v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.04441</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose approaches based on deep learning to localize objects in images
when only a small training dataset is available and the images have low
quality. That applies to many problems in medical image processing, and in
particular to the analysis of fluoroscopic (low-dose) X-ray images, where the
images have low contrast. We solve the problem by incorporating high-level
information about the objects, which could be a simple geometrical model, like
a circular outline, or a more complex statistical model. A simple geometrical
representation can sufficiently describe some objects and only requires minimal
labeling. Statistical shape models can be used to represent more complex
objects. We propose computationally efficient two-stage approaches, which we
call deep morphing, for both representations by fitting the representation to
the output of a deep segmentation network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pries_A/0/1/0/all/0/1&quot;&gt;Aaron Pries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schreier_P/0/1/0/all/0/1&quot;&gt;Peter J. Schreier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamm_A/0/1/0/all/0/1&quot;&gt;Artur Lamm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pede_S/0/1/0/all/0/1&quot;&gt;Stefan Pede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04447">
<title>Deep Learning Super-Resolution Enables Rapid Simultaneous Morphological and Quantitative Magnetic Resonance Imaging. (arXiv:1808.04447v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1808.04447</link>
<description rdf:parseType="Literal">&lt;p&gt;Obtaining magnetic resonance images (MRI) with high resolution and generating
quantitative image-based biomarkers for assessing tissue biochemistry is
crucial in clinical and research applications. How- ever, acquiring
quantitative biomarkers requires high signal-to-noise ratio (SNR), which is at
odds with high-resolution in MRI, especially in a single rapid sequence. In
this paper, we demonstrate how super-resolution can be utilized to maintain
adequate SNR for accurate quantification of the T2 relaxation time biomarker,
while simultaneously generating high- resolution images. We compare the
efficacy of resolution enhancement using metrics such as peak SNR and
structural similarity. We assess accuracy of cartilage T2 relaxation times by
comparing against a standard reference method. Our evaluation suggests that SR
can successfully maintain high-resolution and generate accurate biomarkers for
accelerating MRI scans and enhancing the value of clinical and research MRI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_A/0/1/0/all/0/1&quot;&gt;Akshay Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhongnan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jin Hyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gold_G/0/1/0/all/0/1&quot;&gt;Garry Gold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hargreaves_B/0/1/0/all/0/1&quot;&gt;Brian Hargreaves&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04523">
<title>Adaptive Sampling for Convex Regression. (arXiv:1808.04523v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04523</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce the first principled adaptive-sampling procedure
for learning a convex function in the $L_\infty$ norm, a problem that arises
often in economics, psychology, and the social sciences. We present a
function-specific measure of complexity and use it to prove that our algorithm
is information-theoretically near-optimal in a strong, function-specific sense.
We also corroborate our theoretical contributions with extensive numerical
experiments, finding that our method substantially outperforms passive, uniform
sampling for favorable synthetic and data-derived functions in low-noise
settings with large sampling budgets. Our results also suggest an idealized
`oracle strategy&apos;, which we use to gauge the potential for deploying the
adaptive-sampling strategy on any function in any particular setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1&quot;&gt;Max Simchowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1&quot;&gt;Kevin Jamieson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suchow_J/0/1/0/all/0/1&quot;&gt;Jordan Suchow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1&quot;&gt;Tom Griffiths&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04550">
<title>SciSports: Learning football kinematics through two-dimensional tracking data. (arXiv:1808.04550v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04550</link>
<description rdf:parseType="Literal">&lt;p&gt;SciSports is a Dutch startup company specializing in football analytics. This
paper describes a joint research effort with SciSports, during the Study Group
Mathematics with Industry 2018 at Eindhoven, the Netherlands. The main
challenge that we addressed was to automatically process empirical football
players&apos; trajectories, in order to extract useful information from them. The
data provided to us was two-dimensional positional data during entire matches.
We developed methods based on Newtonian mechanics and the Kalman filter,
Generative Adversarial Nets and Variational Autoencoders. In addition, we
trained a discriminator network to recognize and discern different movement
patterns of players. The Kalman-filter approach yields an interpretable model,
in which a small number of player-dependent parameters can be fit; in theory
this could be used to distinguish among players. The
Generative-Adversarial-Nets approach appears promising in theory, and some
initial tests showed an improvement with respect to the baseline, but the
limits in time and computational power meant that we could not fully explore
it. We also trained a Discriminator network to distinguish between two players
based on their trajectories; after training, the network managed to distinguish
between some pairs of players, but not between others. After training, the
Variational Autoencoders generated trajectories that are difficult to
distinguish, visually, from the data. These experiments provide an indication
that deep generative models can learn the underlying structure and statistics
of football players&apos; trajectories. This can serve as a starting point for
determining player qualities based on such trajectory data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babic_A/0/1/0/all/0/1&quot;&gt;Anatoliy Babic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Harshit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finocchio_G/0/1/0/all/0/1&quot;&gt;Gianluca Finocchio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golak_J/0/1/0/all/0/1&quot;&gt;Julian Golak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peletier_M/0/1/0/all/0/1&quot;&gt;Mark Peletier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1&quot;&gt;Jim Portegies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stegehuis_C/0/1/0/all/0/1&quot;&gt;Clara Stegehuis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1&quot;&gt;Anuj Tyagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincze_R/0/1/0/all/0/1&quot;&gt;Roland Vincze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_W/0/1/0/all/0/1&quot;&gt;William Weimin Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04580">
<title>NFFT meets Krylov methods: Fast matrix-vector products for the graph Laplacian of fully connected networks. (arXiv:1808.04580v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04580</link>
<description rdf:parseType="Literal">&lt;p&gt;The graph Laplacian is a standard tool in data science, machine learning, and
image processing. The corresponding matrix inherits the complex structure of
the underlying network and is in certain applications densely populated. This
makes computations, in particular matrix-vector products, with the graph
Laplacian a hard task. A typical application is the computation of a number of
its eigenvalues and eigenvectors. Standard methods become infeasible as the
number of nodes in the graph is too large. We propose the use of the fast
summation based on the nonequispaced fast Fourier transform (NFFT) to perform
the dense matrix-vector product with the graph Laplacian fast without ever
forming the whole matrix. The enormous flexibility of the NFFT algorithm allows
us to embed the accelerated multiplication into Lanczos-based eigenvalues
routines or iterative linear system solvers. We illustrate the feasibility of
our approach on a number of test problems from image segmentation to
semi-supervised learning based on graph-based PDEs. In particular, we compare
our approach with the Nystr\&quot;om method. Moreover, we present and test an
enhanced, hybrid version of the Nystr\&quot;om method, which internally uses the
NFFT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alfke_D/0/1/0/all/0/1&quot;&gt;Dominik Alfke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potts_D/0/1/0/all/0/1&quot;&gt;Daniel Potts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoll_M/0/1/0/all/0/1&quot;&gt;Martin Stoll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volkmer_T/0/1/0/all/0/1&quot;&gt;Toni Volkmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04759">
<title>An Overview and a Benchmark of Active Learning for One-Class Classification. (arXiv:1808.04759v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04759</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning stands for methods which increase classification quality by
means of user feedback. An important subcategory is active learning for
one-class classifiers, i.e., for imbalanced class distributions. While various
methods in this category exist, selecting one for a given application scenario
is difficult. This is because existing methods rely on different assumptions,
have different objectives, and often are tailored to a specific use case. All
this calls for a comprehensive comparison, the topic of this article. This
article starts with a categorization of the various methods. We then propose
ways to evaluate active learning results. Next, we run extensive experiments to
compare existing methods, for a broad variety of scenarios. One result is that
the practicality and the performance of an active learning method strongly
depend on its category and on the assumptions behind it. Another observation is
that there only is a small subset of our experiments where existing approaches
outperform random baselines. Finally, we show that a well-laid-out
categorization and a rigorous specification of assumptions can facilitate the
selection of a good method for one-class classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trittenbach_H/0/1/0/all/0/1&quot;&gt;Holger Trittenbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Englhardt_A/0/1/0/all/0/1&quot;&gt;Adrian Englhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohm_K/0/1/0/all/0/1&quot;&gt;Klemens B&amp;#xf6;hm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04760">
<title>Parallel Statistical and Machine Learning Methods for Estimation of Physical Load. (arXiv:1808.04760v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04760</link>
<description rdf:parseType="Literal">&lt;p&gt;Several statistical and machine learning methods are proposed to estimate the
type and intensity of physical load and accumulated fatigue . They are based on
the statistical analysis of accumulated and moving window data subsets with
construction of a kurtosis-skewness diagram. This approach was applied to the
data gathered by the wearable heart monitor for various types and levels of
physical activities, and for people with various physical conditions. The
different levels of physical activities, loads, and fitness can be
distinguished from the kurtosis-skewness diagram, and their evolution can be
monitored. Several metrics for estimation of the instant effect and accumulated
effect (physical fatigue) of physical loads were proposed. The data and results
presented allow to extend application of these methods for modeling and
characterization of complex human activity patterns, for example, to estimate
the actual and accumulated physical load and fatigue, model the potential
dangerous development, and give cautions and advice in real time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stirenko_S/0/1/0/all/0/1&quot;&gt;Sergii Stirenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_G/0/1/0/all/0/1&quot;&gt;Gang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wei Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordienko_Y/0/1/0/all/0/1&quot;&gt;Yuri Gordienko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alienin_O/0/1/0/all/0/1&quot;&gt;Oleg Alienin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rokovyi_O/0/1/0/all/0/1&quot;&gt;Oleksandr Rokovyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordienko_N/0/1/0/all/0/1&quot;&gt;Nikita Gordienko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.04768">
<title>Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical Models. (arXiv:1808.04768v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.04768</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a method which enables a recurrent dynamics model to be
temporally abstract. Our approach, which we call Adaptive Skip Intervals (ASI),
is based on the observation that in many sequential prediction tasks, the exact
time at which events occur is irrelevant to the underlying objective. Moreover,
in many situations, there exist prediction intervals which result in
particularly easy-to-predict transitions. We show that there are prediction
tasks for which we gain both computational efficiency and prediction accuracy
by allowing the model to make predictions at a sampling rate which it can
choose itself.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neitz_A/0/1/0/all/0/1&quot;&gt;Alexander Neitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parascandolo_G/0/1/0/all/0/1&quot;&gt;Giambattista Parascandolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1&quot;&gt;Stefan Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.07268">
<title>Semi-supervised Inference: General Theory and Estimation of Means. (arXiv:1606.07268v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1606.07268</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a general semi-supervised inference framework focused on the
estimation of the population mean. As usual in semi-supervised settings, there
exists an unlabeled sample of covariate vectors and a labeled sample consisting
of covariate vectors along with real-valued responses (&quot;labels&quot;). Otherwise,
the formulation is &quot;assumption-lean&quot; in that no major conditions are imposed on
the statistical or functional form of the data. We consider both the ideal
semi-supervised setting where infinitely many unlabeled samples are available,
as well as the ordinary semi-supervised setting in which only a finite number
of unlabeled samples is available.
&lt;/p&gt;
&lt;p&gt;Estimators are proposed along with corresponding confidence intervals for the
population mean. Theoretical analysis on both the asymptotic distribution and
$\ell_2$-risk for the proposed procedures are given. Surprisingly, the proposed
estimators, based on a simple form of the least squares method, outperform the
ordinary sample mean. The simple, transparent form of the estimator lends
confidence to the perception that its asymptotic improvement over the ordinary
sample mean also nearly holds even for moderate size samples. The method is
further extended to a nonparametric setting, in which the oracle rate can be
achieved asymptotically. The proposed estimators are further illustrated by
simulation studies and a real data example involving estimation of the homeless
population.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brown_L/0/1/0/all/0/1&quot;&gt;Lawrence D. Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1&quot;&gt;T. Tony Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05409">
<title>Gaussian Process Latent Force Models for Learning and Stochastic Control of Physical Systems. (arXiv:1709.05409v2 [cs.SY] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05409</link>
<description rdf:parseType="Literal">&lt;p&gt;This article is concerned with learning and stochastic control in physical
systems which contain unknown input signals. These unknown signals are modeled
as Gaussian processes (GP) with certain parametrized covariance structures. The
resulting latent force models (LFMs) can be seen as hybrid models that contain
a first-principles physical model part and a non-parametric GP model part. We
briefly review the statistical inference and learning methods for this kind of
models, introduce stochastic control methodology for the models, and provide
new theoretical observability and controllability results for them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkka_S/0/1/0/all/0/1&quot;&gt;Simo S&amp;#xe4;rkk&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_M/0/1/0/all/0/1&quot;&gt;Mauricio A. &amp;#xc1;lvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawrence_N/0/1/0/all/0/1&quot;&gt;Neil D. Lawrence&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07572">
<title>Information Theoretic Co-Training. (arXiv:1802.07572v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07572</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces an information theoretic co-training objective for
unsupervised learning. We consider the problem of predicting the future. Rather
than predict future sensations (image pixels or sound waves) we predict
&quot;hypotheses&quot; to be confirmed by future sensations. More formally, we assume a
population distribution on pairs $(x,y)$ where we can think of $x$ as a past
sensation and $y$ as a future sensation. We train both a predictor model
$P_\Phi(z|x)$ and a confirmation model $P_\Psi(z|y)$ where we view $z$ as
hypotheses (when predicted) or facts (when confirmed). For a population
distribution on pairs $(x,y)$ we focus on the problem of measuring the mutual
information between $x$ and $y$. By the data processing inequality this mutual
information is at least as large as the mutual information between $x$ and $z$
under the distribution on triples $(x,z,y)$ defined by the confirmation model
$P_\Psi(z|y)$. The information theoretic training objective for $P_\Phi(z|x)$
and $P_\Psi(z|y)$ can be viewed as a form of co-training where we want the
prediction from $x$ to match the confirmation from $y$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAllester_D/0/1/0/all/0/1&quot;&gt;David McAllester&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.02483">
<title>The Logistic Network Lasso. (arXiv:1805.02483v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.02483</link>
<description rdf:parseType="Literal">&lt;p&gt;We apply the network Lasso to solve binary classification and clustering
problems for network-structured data. To this end, we generalize ordinary
logistic regression to non-Euclidean data with an intrinsic network structure.
The resulting &quot;logistic network Lasso&quot; amounts to solving a non-smooth convex
regularized empirical risk minimization. The risk is measured using the
logistic loss incurred over a small set of labeled nodes. For the
regularization, we propose to use the total variation of the classifier
requiring it to conform to the underlying network structure. A scalable
implementation of the learning method is obtained using an inexact variant of
the alternating direction methods of multipliers which results in a scalable
learning algorithm
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ambos_H/0/1/0/all/0/1&quot;&gt;Henrik Ambos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1&quot;&gt;Nguyen Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_A/0/1/0/all/0/1&quot;&gt;Alexander Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12528">
<title>Fusion Graph Convolutional Networks. (arXiv:1805.12528v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.12528</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised node classification involves learning to classify unlabelled
nodes given a partially labeled graph. In transductive learning, all unlabelled
nodes to be classified are observed during training and in inductive learning,
predictions are to be made for nodes not seen at training. In this paper, we
focus on both these settings for node classification in attributed graphs,
i.e., graphs in which nodes have additional features. State-of-the-art models
for node classification on such attributed graphs use differentiable recursive
functions. These differentiable recursive functions enable aggregation and
filtering of neighborhood information from multiple hops (depths). Despite
being powerful, these variants are limited in their ability to combine
information from different hops efficiently. In this work, we analyze this
limitation of recursive graph functions in terms of their representation
capacity to effectively capture multi-hop neighborhood information. Further, we
provide a simple fusion component which is mathematically motivated to address
this limitation and improve the existing models to explicitly learn the
importance of information from different hops. This proposed mechanism is shown
to improve over existing methods across 8 popular datasets from different
domains. Specifically, our model improves the Graph Convolutional Network (GCN)
and a variant of Graph SAGE by a significant margin providing highly
competitive state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayan_P/0/1/0/all/0/1&quot;&gt;Priyesh Vijayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandak_Y/0/1/0/all/0/1&quot;&gt;Yash Chandak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1&quot;&gt;Mitesh M. Khapra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1&quot;&gt;Balaraman Ravindran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04900">
<title>A Machine-Learning Item Recommendation System for Video Games. (arXiv:1806.04900v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04900</link>
<description rdf:parseType="Literal">&lt;p&gt;Video-game players generate huge amounts of data, as everything they do
within a game is recorded. In particular, among all the stored actions and
behaviors, there is information on the in-game purchases of virtual products.
Such information is of critical importance in modern free-to-play titles, where
gamers can select or buy a profusion of items during the game in order to
progress and fully enjoy their experience.
&lt;/p&gt;
&lt;p&gt;To try to maximize these kind of purchases, one can use a recommendation
system so as to present players with items that might be interesting for them.
Such systems can better achieve their goal by employing machine learning
algorithms that are able to predict the rating of an item or product by a
particular user. In this paper we evaluate and compare two of these algorithms,
an ensemble-based model (extremely randomized trees) and a deep neural network,
both of which are promising candidates for operational video-game recommender
engines.
&lt;/p&gt;
&lt;p&gt;Item recommenders can help developers improve the game. But, more
importantly, it should be possible to integrate them into the game, so that
users automatically get personalized recommendations while playing. The
presented models are not only able to meet this challenge, providing accurate
predictions of the items that a particular player will find attractive, but
also sufficiently fast and robust to be used in operational settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bertens_P/0/1/0/all/0/1&quot;&gt;Paul Bertens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guitart_A/0/1/0/all/0/1&quot;&gt;Anna Guitart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pei Pei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perianez_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;frica Peri&amp;#xe1;&amp;#xf1;ez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03298">
<title>Probabilistic Ensemble of Collaborative Filters. (arXiv:1808.03298v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1808.03298</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative filtering is an important technique for recommendation. Whereas
it has been repeatedly shown to be effective in previous work, its performance
remains unsatisfactory in many real-world applications, especially those where
the items or users are highly diverse. In this paper, we explore an
ensemble-based framework to enhance the capability of a recommender in handling
diverse data. Specifically, we formulate a probabilistic model which integrates
the items, the users, as well as the associations between them into a
generative process. On top of this formulation, we further derive a progressive
algorithm to construct an ensemble of collaborative filters. In each iteration,
a new filter is derived from re-weighted entries and incorporated into the
ensemble. It is noteworthy that while the algorithmic procedure of our
algorithm is apparently similar to boosting, it is derived from an essentially
different formulation and thus differs in several key technical aspects. We
tested the proposed method on three large datasets, and observed substantial
improvement over the state of the art, including L2Boost, an effective method
based on boosting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09958">
<title>Rethinking the Form of Latent States in Image Captioning. (arXiv:1807.09958v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1807.09958</link>
<description rdf:parseType="Literal">&lt;p&gt;RNNs and their variants have been widely adopted for image captioning. In
RNNs, the production of a caption is driven by a sequence of latent states.
Existing captioning models usually represent latent states as vectors, taking
this practice for granted. We rethink this choice and study an alternative
formulation, namely using two-dimensional maps to encode latent states. This is
motivated by the curiosity about a question: how the spatial structures in the
latent states affect the resultant captions? Our study on MSCOCO and Flickr30k
leads to two significant observations. First, the formulation with 2D states is
generally more effective in captioning, consistently achieving higher
performance with comparable parameter sizes. Second, 2D states preserve spatial
locality. Taking advantage of this, we visually reveal the internal dynamics in
the process of caption generation, as well as the connections between input
visual domain and output linguistic domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1&quot;&gt;Deming Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.01857">
<title>Statistical Windows in Testing for the Initial Distribution of a Reversible Markov Chain. (arXiv:1808.01857v1 [math.ST] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1808.01857</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of hypothesis testing between two discrete
distributions, where we only have access to samples after the action of a known
reversible Markov chain, playing the role of noise. We derive
instance-dependent minimax rates for the sample complexity of this problem, and
show how its dependence in time is related to the spectral properties of the
Markov chain. We show that there exists a wide statistical window, in terms of
sample complexity for hypothesis testing between different pairs of initial
distributions. We illustrate these results in several concrete examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Berthet_Q/0/1/0/all/0/1&quot;&gt;Quentin Berthet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kanade_V/0/1/0/all/0/1&quot;&gt;Varun Kanade&lt;/a&gt;</dc:creator>
</item></rdf:RDF>