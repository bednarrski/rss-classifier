<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-26T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08969"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09014"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09206"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.05468"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.09288"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08626"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08697"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08858"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08878"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08883"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08996"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09131"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1503.01327"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1604.07981"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1608.01716"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.05233"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00399"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08642"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08664"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08708"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08754"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08773"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08786"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08880"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08914"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09005"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09007"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09150"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09196"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.09277"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1308.0642"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.02711"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.03113"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.05024"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.07352"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.02355"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.10934"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.08457"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.01384"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10646"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10737"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11581"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06050"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07788"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.08969">
<title>Mean Field Residual Networks: On the Edge of Chaos. (arXiv:1712.08969v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.08969</link>
<description rdf:parseType="Literal">&lt;p&gt;We study randomly initialized residual networks using mean field theory and
the theory of difference equations. Classical feedforward neural networks, such
as those with tanh activations, exhibit exponential behavior on the average
when propagating inputs forward or gradients backward. The exponential forward
dynamics causes rapid collapsing of the input space geometry, while the
exponential backward dynamics causes drastic vanishing or exploding gradients.
We show, in contrast, that by adding skip connections, the network will,
depending on the nonlinearity, adopt subexponential forward and backward
dynamics, and in many cases in fact polynomial. The exponents of these
polynomials are obtained through analytic methods and proved and verified
empirically to be correct. In terms of the &quot;edge of chaos&quot; hypothesis, these
subexponential and polynomial laws allow residual networks to &quot;hover over the
boundary between stability and chaos,&quot; thus preserving the geometry of the
input space and the gradient information flow. In our experiments, for each
activation function we study here, we initialize residual networks with
different hyperparameters and train them on MNIST. Remarkably, our
initialization time theory can accurately predict test time performance of
these networks, by tracking either the expected amount of gradient explosion or
the expected squared distance between the images of two input vectors.
Importantly, we show, theoretically as well as empirically, that common
initializations such as the Xavier or the He schemes are not optimal for
residual networks, because the optimal initialization variances depend on the
depth. Finally, we have made mathematical contributions by deriving several new
identities for the kernels of powers of ReLU functions by relating them to the
zeroth Bessel function of the second kind.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Greg Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoenholz_S/0/1/0/all/0/1&quot;&gt;Samuel S. Schoenholz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09014">
<title>Null Dynamical State Models of Human Cognitive Dysfunction. (arXiv:1712.09014v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.09014</link>
<description rdf:parseType="Literal">&lt;p&gt;The hard problem in artificial intelligence asks how the shuffling of
syntactical symbols in a program can lead to systems which experience semantics
and qualia. We address this question in three stages. First, we introduce a new
class of human semantic symbols which appears when unexpected and drastic
environmental change causes humans to become surprised, confused, uncertain,
and in extreme cases, unresponsive, passive and dysfunctional. For this class
of symbols, pre-learned programs become inoperative so these syntactical
programs cannot be the source of experienced qualia. Second, we model the
dysfunctional human response to a radically changed environment as being the
natural response of any learning machine facing novel inputs from well outside
its previous training set. In this situation, learning machines are unable to
extract information from their input and will typically enter a dynamical state
characterized by null outputs and a lack of response. This state immediately
predicts and explains the characteristics of the semantic experiences of humans
in similar circumstances. In the third stage, we consider learning machines
trained to implement multiple functions in simple sequential programs using
environmental data to specify subroutine names, control flow instructions,
memory calls, and so on. Drastic change in any of these environmental inputs
can again lead to inoperative programs. By examining changes specific to people
or locations we can model human cognitive symbols featuring these dependencies,
such as attachment and grief. Our approach links known dynamical machines
states with human qualia and thus offers new insight into the hard problem of
artificial intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagen_M/0/1/0/all/0/1&quot;&gt;M. J. Gagen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09206">
<title>Chaos-guided Input Structuring for Improved Learning in Recurrent Neural Networks. (arXiv:1712.09206v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1712.09206</link>
<description rdf:parseType="Literal">&lt;p&gt;Anatomical studies demonstrate that brain reformats input information to
generate reliable responses for performing computations. However, it remains
unclear how neural circuits encode complex spatio-temporal patterns. We show
that neural dynamics are strongly influenced by the phase alignment between the
input and the spontaneous chaotic activity. Input structuring along the
dominant chaotic projections causes the chaotic trajectories to become stable
channels (or attractors), hence, improving the computational capability of a
recurrent network. Using mean field analysis, we derive the impact of input
structuring on the overall stability of attractors formed. Our results indicate
that input alignment determines the extent of intrinsic noise suppression and
hence, alters the attractor state stability, thereby controlling the network&apos;s
inference ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Panda_P/0/1/0/all/0/1&quot;&gt;Priyadarshini Panda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushik Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.05468">
<title>Generalization in Deep Learning. (arXiv:1710.05468v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.05468</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explains why deep learning can generalize well, despite large
capacity and possible algorithmic instability, nonrobustness, and sharp minima,
effectively addressing an open problem in the literature. Based on our
theoretical insight, this paper also proposes a family of new regularization
methods. Its simplest member was empirically shown to improve base models and
achieve competitive performance on MNIST and CIFAR-10 benchmarks. Moreover,
this paper presents both data-dependent and data-independent generalization
guarantees with improved convergence rates. Our results suggest several new
open areas of research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie Pack Kaelbling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.09288">
<title>Adversarial Deep Structured Nets for Mass Segmentation from Mammograms. (arXiv:1710.09288v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1710.09288</link>
<description rdf:parseType="Literal">&lt;p&gt;Mass segmentation provides effective morphological features which are
important for mass diagnosis. In this work, we propose a novel end-to-end
network for mammographic mass segmentation which employs a fully convolutional
network (FCN) to model a potential function, followed by a CRF to perform
structured learning. Because the mass distribution varies greatly with pixel
position, the FCN is combined with a position priori. Further, we employ
adversarial training to eliminate over-fitting due to the small sizes of
mammogram datasets. Multi-scale FCN is employed to improve the segmentation
performance. Experimental results on two public datasets, INbreast and
DDSM-BCRP, demonstrate that our end-to-end network achieves better performance
than state-of-the-art approaches.
\footnote{https://github.com/wentaozhu/adversarial-deep-structural-networks.git}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1&quot;&gt;Xiang Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Trac D. Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1&quot;&gt;Gregory D. Hager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08626">
<title>Obtaining Accurate Probabilistic Causal Inference by Post-Processing Calibration. (arXiv:1712.08626v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.08626</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovery of an accurate causal Bayesian network structure from observational
data can be useful in many areas of science. Often the discoveries are made
under uncertainty, which can be expressed as probabilities. To guide the use of
such discoveries, including directing further investigation, it is important
that those probabilities be well-calibrated. In this paper, we introduce a
novel framework to derive calibrated probabilities of causal relationships from
observational data. The framework consists of three components: (1) an
approximate method for generating initial probability estimates of the edge
types for each pair of variables, (2) the availability of a relatively small
number of the causal relationships in the network for which the truth status is
known, which we call a calibration training set, and (3) a calibration method
for using the approximate probability estimates and the calibration training
set to generate calibrated probabilities for the many remaining pairs of
variables. We also introduce a new calibration method based on a shallow neural
network. Our experiments on simulated data support that the proposed approach
improves the calibration of causal edge predictions. The results also support
that the approach often improves the precision and recall of predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jabbari_F/0/1/0/all/0/1&quot;&gt;Fattaneh Jabbari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naeini_M/0/1/0/all/0/1&quot;&gt;Mahdi Pakdaman Naeini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooper_G/0/1/0/all/0/1&quot;&gt;Gregory F. Cooper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08697">
<title>Interpretable Counting for Visual Question Answering. (arXiv:1712.08697v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.08697</link>
<description rdf:parseType="Literal">&lt;p&gt;Questions that require counting a variety of objects in images remain a major
challenge in visual question answering (VQA). The most common approaches to VQA
involve either classifying answers based on fixed length representations of
both the image and question or summing fractional counts estimated from each
section of the image. In contrast, we treat counting as a sequential decision
process and force our model to make discrete choices of what to count.
Specifically, the model sequentially selects from detected objects and learns
interactions between objects that influence subsequent selections. A
distinction of our approach is its intuitive and interpretable output, as
discrete counts are automatically grounded in the image. Furthermore, our
method outperforms the state of the art architecture for VQA on multiple
metrics that evaluate counting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1&quot;&gt;Alexander Trott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08858">
<title>Towards Collaborative Conceptual Exploration. (arXiv:1712.08858v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.08858</link>
<description rdf:parseType="Literal">&lt;p&gt;In domains with high knowledge distribution a natural objective is to create
principle foundations for collaborative interactive learning environments. We
present a first mathematical characterization of a collaborative learning
group, a consortium, based on closure systems of attribute sets and the
well-known attribute exploration algorithm from formal concept analysis. To
this end, we introduce (weak) local experts for subdomains of a given knowledge
domain. These entities are able to refute and potentially accept a given
(implicational) query for some closure system that is a restriction of the
whole domain. On this we build up a consortial expert and show first insights
about the ability of such an expert to answer queries. Furthermore, we depict
techniques on how to cope with falsely accepted implications and on combining
counterexamples. Using notions from combinatorial design theory we further
expand those insights as far as providing first results on the decidability
problem if a given consortium is able to explore some target domain.
Applications in conceptual knowledge acquisition as well as in collaborative
interactive ontology learning are at hand.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanika_T/0/1/0/all/0/1&quot;&gt;Tom Hanika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zumbragel_J/0/1/0/all/0/1&quot;&gt;Jens Zumbr&amp;#xe4;gel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08875">
<title>Predicting Rich Drug-Drug Interactions via Biomedical Knowledge Graphs and Text Jointly Embedding. (arXiv:1712.08875v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.08875</link>
<description rdf:parseType="Literal">&lt;p&gt;Minimizing adverse reactions caused by drug-drug interactions has always been
a momentous research topic in clinical pharmacology. Detecting all possible
interactions through clinical studies before a drug is released to the market
is a demanding task. The power of big data is opening up new approaches to
discover various drug-drug interactions. However, these discoveries contain a
huge amount of noise and provide knowledge bases far from complete and
trustworthy ones to be utilized. Most existing studies focus on predicting
binary drug-drug interactions between drug pairs but ignore other interactions.
In this paper, we propose a novel framework, called PRD, to predict drug-drug
interactions. The framework uses the graph embedding that can overcome data
incompleteness and sparsity issues to achieve multiple DDI label prediction.
First, a large-scale drug knowledge graph is generated from different sources.
Then, the knowledge graph is embedded with comprehensive biomedical text into a
common low dimensional space. Finally, the learned embeddings are used to
efficiently compute rich DDI information through a link prediction process. To
validate the effectiveness of the proposed framework, extensive experiments
were conducted on real-world datasets. The results demonstrate that our model
outperforms several state-of-the-art baseline methods in terms of capability
and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yihe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1&quot;&gt;Buyue Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1&quot;&gt;Guodong Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08878">
<title>How Intelligent is your Intelligent Robot?. (arXiv:1712.08878v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1712.08878</link>
<description rdf:parseType="Literal">&lt;p&gt;How intelligent is robot A compared with robot B? And how intelligent are
robots A and B compared with animals (or plants) X and Y? These are both
interesting and deeply challenging questions. In this paper we address the
question &quot;how intelligent is your intelligent robot?&quot; by proposing that
embodied intelligence emerges from the interaction and integration of four
different and distinct kinds of intelligence. We then suggest a simple
diagrammatic representation on which these kinds of intelligence are shown as
four axes in a star diagram. A crude qualitative comparison of the intelligence
graphs of animals and robots both exposes and helps to explain the chronic
intelligence deficit of intelligent robots. Finally we examine the options for
determining numerical values for the four kinds of intelligence in an effort to
move toward a quantifiable intelligence vector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winfield_A/0/1/0/all/0/1&quot;&gt;Alan F. T. Winfield&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08883">
<title>Traffic Flow Forecasting Using a Spatio-Temporal Bayesian Network Predictor. (arXiv:1712.08883v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.08883</link>
<description rdf:parseType="Literal">&lt;p&gt;A novel predictor for traffic flow forecasting, namely spatio-temporal
Bayesian network predictor, is proposed. Unlike existing methods, our approach
incorporates all the spatial and temporal information available in a
transportation network to carry our traffic flow forecasting of the current
site. The Pearson correlation coefficient is adopted to rank the input
variables (traffic flows) for prediction, and the best-first strategy is
employed to select a subset as the cause nodes of a Bayesian network. Given the
derived cause nodes and the corresponding effect node in the spatio-temporal
Bayesian network, a Gaussian Mixture Model is applied to describe the
statistical relationship between the input and output. Finally, traffic flow
forecasting is performed under the criterion of Minimum Mean Square Error
(M.M.S.E.). Experimental results with the urban vehicular flow data of Beijing
demonstrate the effectiveness of our presented spatio-temporal Bayesian network
predictor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shiliang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changshui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08996">
<title>Android Malware Detection using Deep Learning on API Method Sequences. (arXiv:1712.08996v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1712.08996</link>
<description rdf:parseType="Literal">&lt;p&gt;Android OS experiences a blazing popularity since the last few years. This
predominant platform has established itself not only in the mobile world but
also in the Internet of Things (IoT) devices. This popularity, however, comes
at the expense of security, as it has become a tempting target of malicious
apps. Hence, there is an increasing need for sophisticated, automatic, and
portable malware detection solutions. In this paper, we propose MalDozer, an
automatic Android malware detection and family attribution framework that
relies on sequences classification using deep learning techniques. Starting
from the raw sequence of the app&apos;s API method calls, MalDozer automatically
extracts and learns the malicious and the benign patterns from the actual
samples to detect Android malware. MalDozer can serve as a ubiquitous malware
detection system that is not only deployed on servers, but also on mobile and
even IoT devices. We evaluate MalDozer on multiple Android malware datasets
ranging from 1K to 33K malware apps, and 38K benign apps. The results show that
MalDozer can correctly detect malware and attribute them to their actual
families with an F1-Score of 96%-99% and a false positive rate of 0.06%-2%,
under all tested datasets and settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karbab_E/0/1/0/all/0/1&quot;&gt;ElMouatez Billah Karbab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Debbabi_M/0/1/0/all/0/1&quot;&gt;Mourad Debbabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derhab_A/0/1/0/all/0/1&quot;&gt;Abdelouahid Derhab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouheb_D/0/1/0/all/0/1&quot;&gt;Djedjiga Mouheb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09131">
<title>A Random Block-Coordinate Douglas-Rachford Splitting Method with Low Computational Complexity for Binary Logistic Regression. (arXiv:1712.09131v1 [math.OC])</title>
<link>http://arxiv.org/abs/1712.09131</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a new optimization algorithm for sparse logistic
regression based on a stochastic version of the Douglas-Rachford splitting
method. Our algorithm sweeps the training set by randomly selecting a
mini-batch of data at each iteration, and it allows us to update the variables
in a block coordinate manner. Our approach leverages the proximity operator of
the logistic loss, which is expressed with the generalized Lambert W function.
Experiments carried out on standard datasets demonstrate the efficiency of our
approach w.r.t. stochastic gradient-like methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Briceno_Arias_L/0/1/0/all/0/1&quot;&gt;Luis M. Briceno-Arias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chierchia_G/0/1/0/all/0/1&quot;&gt;Giovanni Chierchia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chouzenoux_E/0/1/0/all/0/1&quot;&gt;Emilie Chouzenoux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pesquet_J/0/1/0/all/0/1&quot;&gt;Jean-Christophe Pesquet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1503.01327">
<title>Estimating the Probability of Meeting a Deadline in Hierarchical Plans. (arXiv:1503.01327v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1503.01327</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a hierarchical plan (or schedule) with uncertain task times, we propose
a deterministic polynomial (time and memory) algorithm for estimating the
probability that its meets a deadline, or, alternately, that its {\em makespan}
is less than a given duration. Approximation is needed as it is known that this
problem is NP-hard even for sequential plans (just, a sum of random variables).
In addition, we show two new complexity results: (1) Counting the number of
events that do not cross deadline is \#P-hard; (2)~Computing the expected
makespan of a hierarchical plan is NP-hard. For the proposed approximation
algorithm, we establish formal approximation bounds and show that the time and
memory complexities grow polynomially with the required accuracy, the number of
nodes in the plan, and with the size of the support of the random variables
that represent the durations of the primitive tasks. We examine these
approximation bounds empirically and demonstrate, using task networks taken
from the literature, how our scheme outperforms sampling techniques and exact
computation in terms of accuracy and run-time. As the empirical data shows much
better error bounds than guaranteed, we also suggest a method for tightening
the bounds in some cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_L/0/1/0/all/0/1&quot;&gt;Liat Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimony_S/0/1/0/all/0/1&quot;&gt;Solomon Eyal Shimony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1&quot;&gt;Gera Weiss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1604.07981">
<title>The Power of Arc Consistency for CSPs Defined by Partially-Ordered Forbidden Patterns. (arXiv:1604.07981v4 [cs.CC] UPDATED)</title>
<link>http://arxiv.org/abs/1604.07981</link>
<description rdf:parseType="Literal">&lt;p&gt;Characterising tractable fragments of the constraint satisfaction problem
(CSP) is an important challenge in theoretical computer science and artificial
intelligence. Forbidding patterns (generic sub-instances) provides a means of
defining CSP fragments which are neither exclusively language-based nor
exclusively structure-based. It is known that the class of binary CSP instances
in which the broken-triangle pattern (BTP) does not occur, a class which
includes all tree-structured instances, are decided by arc consistency (AC), a
ubiquitous reduction operation in constraint solvers. We provide a
characterisation of simple partially-ordered forbidden patterns which have this
AC-solvability property. It turns out that BTP is just one of five such
AC-solvable patterns. The four other patterns allow us to exhibit new tractable
classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooper_M/0/1/0/all/0/1&quot;&gt;Martin C. Cooper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zivny_S/0/1/0/all/0/1&quot;&gt;Stanislav &amp;#x17d;ivn&amp;#xfd;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1608.01716">
<title>A Polynomial-Time Deterministic Approach to the Traveling Salesperson Problem. (arXiv:1608.01716v3 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1608.01716</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new polynomial-time deterministic algorithm that produces an
approximated solution for the traveling salesperson problem. The proposed
algorithm ranks cities based on their priorities calculated using a power
function of means and standard deviations of their distances from other cities
and then connects the cities to their neighbors in the order of their
priorities. When connecting a city, a neighbor is selected based on their
neighbors&apos; priorities calculated as another power function that additionally
includes their distance from the focal city to be connected. This repeats until
all the cities are connected into a single loop. The time complexity of the
proposed algorithm is $O(n^2)$, where $n$ is the number of cities. Numerical
evaluation shows that, despite its simplicity, the proposed algorithm produces
shorter tours with less time complexity than other conventional tour
construction heuristics. The proposed algorithm can be used by itself or as an
initial tour generator for other more complex heuristic optimization
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jazayeri_A/0/1/0/all/0/1&quot;&gt;Ali Jazayeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayama_H/0/1/0/all/0/1&quot;&gt;Hiroki Sayama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.05233">
<title>Learners that Leak Little Information. (arXiv:1710.05233v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.05233</link>
<description rdf:parseType="Literal">&lt;p&gt;We study learning algorithms that are restricted to using a small amount of
information from their input sample. We introduce a category of learning
algorithms we term d-bit information learners, which are algorithms whose
output conveys at most d bits of information on their input. A central theme in
this work is that such algorithms generalize.
&lt;/p&gt;
&lt;p&gt;We focus on the learning capacity of these algorithms, and prove sample
complexity bounds with tight dependencies on the confidence and error
parameters. We also observe connections with well studied notions such as
sample compression schemes, Occam&apos;s razor, PAC-Bayes and differential privacy.
&lt;/p&gt;
&lt;p&gt;We discuss an approach that allows us to prove upper bounds on the amount of
information that algorithms reveal about their inputs, and also provide a lower
bound by showing a simple concept class for which every (possibly randomized)
empirical risk minimizer must reveal a lot of information. On the other hand,
we show that in the distribution-dependent setting every VC class has empirical
risk minimizers that do not reveal a lot of information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassily_R/0/1/0/all/0/1&quot;&gt;Raef Bassily&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1&quot;&gt;Shay Moran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachum_I/0/1/0/all/0/1&quot;&gt;Ido Nachum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafer_J/0/1/0/all/0/1&quot;&gt;Jonathan Shafer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yehudayoff_A/0/1/0/all/0/1&quot;&gt;Amir Yehudayoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00399">
<title>Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR. (arXiv:1711.00399v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00399</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been much discussion of the right to explanation in the EU General
Data Protection Regulation, and its existence, merits, and disadvantages.
Implementing a right to explanation that opens the black box of algorithmic
decision-making faces major legal and technical barriers. Explaining the
functionality of complex algorithmic decision-making systems and their
rationale in specific cases is a technically challenging problem. Some
explanations may offer little meaningful information to data subjects, raising
questions around their value. Explanations of automated decisions need not
hinge on the general public understanding how algorithmic systems function.
Even though such interpretability is of great importance and should be pursued,
explanations can, in principle, be offered without opening the black box.
Looking at explanations as a means to help a data subject act rather than
merely understand, one could gauge the scope and content of explanations
according to the specific goal or action they are intended to support. From the
perspective of individuals affected by automated decision-making, we propose
three aims for explanations: (1) to inform and help the individual understand
why a particular decision was reached, (2) to provide grounds to contest the
decision if the outcome is undesired, and (3) to understand what would need to
change in order to receive a desired result in the future, based on the current
decision-making model. We assess how each of these goals finds support in the
GDPR. We suggest data controllers should offer a particular type of
explanation, unconditional counterfactual explanations, to support these three
aims. These counterfactual explanations describe the smallest change to the
world that can be made to obtain a desirable outcome, or to arrive at the
closest possible world, without needing to explain the internal logic of the
system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wachter_S/0/1/0/all/0/1&quot;&gt;Sandra Wachter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittelstadt_B/0/1/0/all/0/1&quot;&gt;Brent Mittelstadt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1&quot;&gt;Chris Russell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08642">
<title>Least-Squares Temporal Difference Learning for the Linear Quadratic Regulator. (arXiv:1712.08642v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.08642</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) has been successfully used to solve many
continuous control tasks. Despite its impressive results however, fundamental
questions regarding the sample complexity of RL on continuous problems remain
open. We study the performance of RL in this setting by considering the
behavior of the Least-Squares Temporal Difference (LSTD) estimator on the
classic Linear Quadratic Regulator (LQR) problem from optimal control. We give
the first finite-time analysis of the number of samples needed to estimate the
value function for a fixed static state-feedback policy to within
$\varepsilon$-relative error. In the process of deriving our result, we give a
general characterization for when the minimum eigenvalue of the empirical
covariance matrix formed along the sample path of a fast-mixing stochastic
process concentrates above zero, extending a result by Koltchinskii and
Mendelson in the independent covariates setting. Finally, we provide
experimental evidence indicating that our analysis correctly captures the
qualitative behavior of LSTD on several LQR instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1&quot;&gt;Stephen Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Recht_B/0/1/0/all/0/1&quot;&gt;Benjamin Recht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08655">
<title>Sparse travel time tomography with adaptive dictionaries. (arXiv:1712.08655v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/1712.08655</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a 2D travel time tomography method which regularizes the inversion
by modeling groups of slowness pixels from discrete slowness maps, called
patches, as sparse linear combinations of atoms from a dictionary. We further
propose to learn optimal slowness dictionaries using dictionary learning, in
parallel with the inversion. This patch regularization, which we call the local
model, is integrated into the overall slowness map, called the global model.
Where the local model considers small-scale variations using a sparsity
constraint, the global model considers larger-scale features which are
constrained using $\ell_2$-norm regularization. This local-global modeling
strategy with dictionary learning has been successful for image restoration
tasks such as denoising and inpainting, where diverse image content is
recovered from noisy or incomplete measurements. We use this strategy in our
locally-sparse travel time tomography (LST) approach to model simultaneously
smooth and discontinuous slowness features. This is in contrast to conventional
tomography methods, which constrain models to be exclusively smooth or
discontinuous. We develop a $\textit{maximum a posteriori}$ formulation for LST
and exploit the sparsity of slowness patches using dictionary learning. We
demonstrate the LST approach on densely, but irregularly sampled synthetic
slowness maps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bianco_M/0/1/0/all/0/1&quot;&gt;Michael Bianco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gertoft_P/0/1/0/all/0/1&quot;&gt;Peter Gertoft&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08664">
<title>Mixtures of Matrix Variate Bilinear Factor Analyzers. (arXiv:1712.08664v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1712.08664</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the years data is becoming increasingly higher dimensional, which has
prompted an increased need for dimension reduction techniques, in particular
for clustering and classification. Although dimension reduction in the area of
clustering for multivariate data has been thoroughly discussed in the
literature there is relatively little work in the area of three way (matrix
variate) data. Herein, we develop a mixture of matrix variate bilinear factor
analyzers (MMVBFA) model for use in clustering high dimensional matrix variate
data. Parameter estimation is discussed, and the MMVBFA model is illustrated
using simulated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gallaugher_M/0/1/0/all/0/1&quot;&gt;Michael P.B. Gallaugher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McNicholas_P/0/1/0/all/0/1&quot;&gt;Paul D. McNicholas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08708">
<title>Variational Autoencoders for Learning Latent Representations of Speech Emotion. (arXiv:1712.08708v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1712.08708</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent representation of data in unsupervised fashion is a very interesting
process. It provides more relevant features that can enhance the performance of
a classifier. For speech emotion recognition tasks generating effective
features is very crucial. Recently, deep generative models such as Variational
Autoencoders (VAEs) have gained enormous success to model natural images. Being
inspired by that in this paper, we use VAE for the modeling of emotions in
human speech. We derive the latent representation of speech signal and use this
for classification of emotions. We demonstrate that features learned by VAEs
can achieve state-of-the-art emotion recognition results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latif_S/0/1/0/all/0/1&quot;&gt;Siddique Latif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rana_R/0/1/0/all/0/1&quot;&gt;Rajib Rana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qadir_J/0/1/0/all/0/1&quot;&gt;Junaid Qadir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epps_J/0/1/0/all/0/1&quot;&gt;Julien Epps&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08754">
<title>On Estimation of Conditional Modes Using Multiple Quantile Regressions. (arXiv:1712.08754v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.08754</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an estimation method for the conditional mode when the
conditioning variable is high-dimensional. In the proposed method, we first
estimate the conditional density by solving quantile regressions multiple
times. We then estimate the conditional mode by finding the maximum of the
estimated conditional density. The proposed method has two advantages in that
it is computationally stable because it has no initial parameter dependencies,
and it is statistically efficient with a fast convergence rate. Synthetic and
real-world data experiments demonstrate the better performance of the proposed
method compared to other existing ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ohta_H/0/1/0/all/0/1&quot;&gt;Hirofumi Ohta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hara_S/0/1/0/all/0/1&quot;&gt;Satoshi Hara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08773">
<title>An Approximate Bayesian Long Short-Term Memory Algorithm for Outlier Detection. (arXiv:1712.08773v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.08773</link>
<description rdf:parseType="Literal">&lt;p&gt;Long Short-Term Memory networks trained with gradient descent and
back-propagation have received great success in various applications. However,
point estimation of the weights of the networks is prone to over-fitting
problems and lacks important uncertainty information associated with the
estimation. However, exact Bayesian neural network methods are intractable and
non-applicable for real-world applications. In this study, we propose an
approximate estimation of the weights uncertainty using Ensemble Kalman Filter,
which is easily scalable to a large number of weights. Furthermore, we optimize
the covariance of the noise distribution in the ensemble update step using
maximum likelihood estimation. To assess the proposed algorithm, we apply it to
outlier detection in five real-world events retrieved from the Twitter
platform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terejanu_G/0/1/0/all/0/1&quot;&gt;Gabriel Terejanu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08786">
<title>Merging $K$-means with hierarchical clustering for identifying general-shaped groups. (arXiv:1712.08786v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.08786</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering partitions a dataset such that observations placed together in a
group are similar but different from those in other groups. Hierarchical and
$K$-means clustering are two approaches but have different strengths and
weaknesses. For instance, hierarchical clustering identifies groups in a
tree-like structure but suffers from computational complexity in large datasets
while $K$-means clustering is efficient but designed to identify homogeneous
spherically-shaped clusters. We present a hybrid non-parametric clustering
approach that amalgamates the two methods to identify general-shaped clusters
and that can be applied to larger datasets. Specifically, we first partition
the dataset into spherical groups using $K$-means. We next merge these groups
using hierarchical methods with a data-driven distance measure as a stopping
criterion. Our proposal has the potential to reveal groups with general shapes
and structure in a dataset. We demonstrate good performance on several
simulated and real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peterson_A/0/1/0/all/0/1&quot;&gt;Anna D. Peterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Arka P. Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1&quot;&gt;Ranjan Maitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08880">
<title>Lectures on Randomized Numerical Linear Algebra. (arXiv:1712.08880v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1712.08880</link>
<description rdf:parseType="Literal">&lt;p&gt;This chapter is based on lectures on Randomized Numerical Linear Algebra from
the 2016 Park City Mathematics Institute summer school on The Mathematics of
Data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drineas_P/0/1/0/all/0/1&quot;&gt;Petros Drineas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1&quot;&gt;Michael W. Mahoney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08914">
<title>Bayesian Nonparametric Causal Inference: Information Rates and Learning Algorithms. (arXiv:1712.08914v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1712.08914</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the problem of estimating the causal effect of a treatment on
individual subjects from observational data, this is a central problem in
various application domains, including healthcare, social sciences, and online
advertising. Within the Neyman Rubin potential outcomes model, we use the
Kullback Leibler (KL) divergence between the estimated and true distributions
as a measure of accuracy of the estimate, and we define the information rate of
the Bayesian causal inference procedure as the (asymptotic equivalence class of
the) expected value of the KL divergence between the estimated and true
distributions as a function of the number of samples. Using Fano method, we
establish a fundamental limit on the information rate that can be achieved by
any Bayesian estimator, and show that this fundamental limit is independent of
the selection bias in the observational data. We characterize the Bayesian
priors on the potential (factual and counterfactual) outcomes that achieve the
optimal information rate. As a consequence, we show that a particular class of
priors that have been widely used in the causal inference literature cannot
achieve the optimal information rate. On the other hand, a broader class of
priors can achieve the optimal information rate. We go on to propose a prior
adaptation procedure (which we call the information based empirical Bayes
procedure) that optimizes the Bayesian prior by maximizing an information
theoretic criterion on the recovered causal effects rather than maximizing the
marginal likelihood of the observed (factual) data. Building on our analysis,
we construct an information optimal Bayesian causal inference algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alaa_A/0/1/0/all/0/1&quot;&gt;Ahmed M. Alaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09005">
<title>Efficient Algorithms for t-distributed Stochastic Neighborhood Embedding. (arXiv:1712.09005v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.09005</link>
<description rdf:parseType="Literal">&lt;p&gt;t-distributed Stochastic Neighborhood Embedding (t-SNE) is a method for
dimensionality reduction and visualization that has become widely popular in
recent years. Efficient implementations of t-SNE are available, but they scale
poorly to datasets with hundreds of thousands to millions of high dimensional
data-points. We present Fast Fourier Transform-accelerated Interpolation-based
t-SNE (FIt-SNE), which dramatically accelerates the computation of t-SNE. The
most time-consuming step of t-SNE is a convolution that we accelerate by
interpolating onto an equispaced grid and subsequently using the fast Fourier
transform to perform the convolution. We also optimize the computation of input
similarities in high dimensions using multi-threaded approximate nearest
neighbors. We further present a modification to t-SNE called &quot;late
exaggeration,&quot; which allows for easier identification of clusters in t-SNE
embeddings. Finally, for datasets that cannot be loaded into the memory, we
present out-of-core randomized principal component analysis (oocPCA), so that
the top principal components of a dataset can be computed without ever fully
loading the matrix, hence allowing for t-SNE of large datasets to be computed
on resource-limited machines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linderman_G/0/1/0/all/0/1&quot;&gt;George C. Linderman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rachh_M/0/1/0/all/0/1&quot;&gt;Manas Rachh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoskins_J/0/1/0/all/0/1&quot;&gt;Jeremy G. Hoskins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinerberger_S/0/1/0/all/0/1&quot;&gt;Stefan Steinerberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kluger_Y/0/1/0/all/0/1&quot;&gt;Yuval Kluger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09007">
<title>Stochastic Multi-armed Bandits in Constant Space. (arXiv:1712.09007v1 [cs.DS])</title>
<link>http://arxiv.org/abs/1712.09007</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the stochastic bandit problem in the sublinear space setting,
where one cannot record the win-loss record for all $K$ arms. We give an
algorithm using $O(1)$ words of space with regret \[
&lt;/p&gt;
&lt;p&gt;\sum_{i=1}^{K}\frac{1}{\Delta_i}\log \frac{\Delta_i}{\Delta}\log T \] where
$\Delta_i$ is the gap between the best arm and arm $i$ and $\Delta$ is the gap
between the best and the second-best arms. If the rewards are bounded away from
$0$ and $1$, this is within an $O(\log 1/\Delta)$ factor of the optimum regret
possible without space constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liau_D/0/1/0/all/0/1&quot;&gt;David Liau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1&quot;&gt;Eric Price&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Ger Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09150">
<title>Variational Bayes Estimation of Time Series Copulas for Multivariate Ordinal and Mixed Data. (arXiv:1712.09150v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1712.09150</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new variational Bayes method for estimating high-dimensional
copulas with discrete, or discrete and continuous, margins. The method is based
on a variational approximation to a tractable augmented posterior, and is
substantially faster than previous likelihood-based approaches. We use it to
estimate drawable vine copulas for univariate and multivariate Markov ordinal
and mixed time series. These have dimension $rT$, where $T$ is the number of
observations and $r$ is the number of series, and are difficult to estimate
using previous methods. The vine pair-copulas are carefully selected to allow
for heteroskedasticity, which is a common feature of ordinal time series data.
When combined with flexible margins, the resulting time series models also
allow for other common features of ordinal data, such as zero inflation,
multiple modes and under- or over-dispersion. Using data on homicides in New
South Wales, and also U.S bankruptcies, we illustrate both the flexibility of
the time series copula models, and the efficacy of the variational Bayes
estimator for copulas of up to 792 dimensions and 60 parameters. This far
exceeds the size and complexity of copula models for discrete data that can be
estimated using previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loaiza_Maya_R/0/1/0/all/0/1&quot;&gt;Ruben Loaiza-Maya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smith_M/0/1/0/all/0/1&quot;&gt;Michael Stanley Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09196">
<title>The Robust Manifold Defense: Adversarial Training using Generative Models. (arXiv:1712.09196v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.09196</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are demonstrating excellent performance on several
classical vision problems. However, these networks are vulnerable to
adversarial examples, minutely modified images that induce arbitrary
attacker-chosen output from the network. We propose a mechanism to protect
against these adversarial inputs based on a generative model of the data. We
introduce a pre-processing step that projects on the range of a generative
model using gradient descent before feeding an input into a classifier. We show
that this step provides the classifier with robustness against first-order,
substitute model, and combined adversarial attacks. Using a min-max
formulation, we show that there may exist adversarial examples even in the
range of the generator, natural-looking images extremely close to the decision
boundary for which the classifier has unjustifiedly high confidence. We show
that adversarial training on the generative manifold can be used to make a
classifier that is robust to these attacks.
&lt;/p&gt;
&lt;p&gt;Finally, we show how our method can be applied even without a pre-trained
generative model using a recent method called the deep image prior. We evaluate
our method on MNIST, CelebA and Imagenet and show robustness against the
current state of the art attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilyas_A/0/1/0/all/0/1&quot;&gt;Andrew Ilyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1&quot;&gt;Ajil Jalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asteri_E/0/1/0/all/0/1&quot;&gt;Eirini Asteri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1&quot;&gt;Constantinos Daskalakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1&quot;&gt;Alexandros G. Dimakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.09277">
<title>Scalable Prototype Selection by Genetic Algorithms and Hashing. (arXiv:1712.09277v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.09277</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification in the dissimilarity space has become a very active research
area since it provides a possibility to learn from data given in the form of
pairwise non-metric dissimilarities, which otherwise would be difficult to cope
with. The selection of prototypes is a key step for the further creation of the
space. However, despite previous efforts to find good prototypes, how to select
the best representation set remains an open issue. In this paper we proposed
scalable methods to select the set of prototypes out of very large datasets.
The methods are based on genetic algorithms, dissimilarity-based hashing, and
two different unsupervised and supervised scalable criteria. The unsupervised
criterion is based on the Minimum Spanning Tree of the graph created by the
prototypes as nodes and the dissimilarities as edges. The supervised criterion
is based on counting matching labels of objects and their closest prototypes.
The suitability of these type of algorithms is analyzed for the specific case
of dissimilarity representations. The experimental results showed that the
methods select good prototypes taking advantage of the large datasets, and they
do so at low runtimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Plasencia_Calana_Y/0/1/0/all/0/1&quot;&gt;Yenisel Plasencia-Cala&amp;#xf1;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Orozco_Alzate_M/0/1/0/all/0/1&quot;&gt;Mauricio Orozco-Alzate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mendez_Vazquez_H/0/1/0/all/0/1&quot;&gt;Heydi M&amp;#xe9;ndez-V&amp;#xe1;zquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garcia_Reyes_E/0/1/0/all/0/1&quot;&gt;Edel Garc&amp;#xed;a-Reyes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duin_R/0/1/0/all/0/1&quot;&gt;Robert P.W. Duin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1308.0642">
<title>Nonlinear Time Series Modeling: A Unified Perspective, Algorithm, and Application. (arXiv:1308.0642v4 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1308.0642</link>
<description rdf:parseType="Literal">&lt;p&gt;A new comprehensive approach to nonlinear time series analysis and modeling
is developed in the present paper. We introduce novel data-specific
mid-distribution based Legendre Polynomial (LP) like nonlinear transformations
of the original time series Y(t) that enables us to adapt all the existing
stationary linear Gaussian time series modeling strategy and made it applicable
for non-Gaussian and nonlinear processes in a robust fashion. The emphasis of
the present paper is on empirical time series modeling via the algorithm
LPTime. We demonstrate the effectiveness of our theoretical framework using
daily S&amp;amp;P 500 return data between Jan/2/1963 - Dec/31/2009. Our proposed LPTime
algorithm systematically discovers all the `stylized facts&apos; of the financial
time series automatically all at once, which were previously noted by many
researchers one at a time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mukhopadhyay_S/0/1/0/all/0/1&quot;&gt;Subhadeep Mukhopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Parzen_E/0/1/0/all/0/1&quot;&gt;Emanuel Parzen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.02711">
<title>Nonconvex Sparse Learning via Stochastic Optimization with Progressive Variance Reduction. (arXiv:1605.02711v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1605.02711</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a stochastic variance reduced optimization algorithm for solving
sparse learning problems with cardinality constraints. Sufficient conditions
are provided, under which the proposed algorithm enjoys strong linear
convergence guarantees and optimal estimation accuracy in high dimensions. We
further extend the proposed algorithm to an asynchronous parallel variant with
a near linear speedup. Numerical experiments demonstrate the efficiency of our
algorithm in terms of both parameter estimation and computational performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_R/0/1/0/all/0/1&quot;&gt;Raman Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haupt_J/0/1/0/all/0/1&quot;&gt;Jarvis Haupt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.03113">
<title>Truncated Variational Expectation Maximization. (arXiv:1610.03113v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.03113</link>
<description rdf:parseType="Literal">&lt;p&gt;We derive a novel variational expectation maximization approach based on
truncated variational distributions. Truncated distributions are proportional
to exact posteriors within a subset of a discrete state space and equal zero
otherwise. The novel variational approach is realized by first generalizing the
standard variational EM framework to include variational distributions with
exact (`hard&apos;) zeros. A fully variational treatment of truncated distributions
then allows for deriving novel and mathematically grounded results, which in
turn can be used to formulate novel efficient algorithms to optimize the
parameters of probabilistic generative models. We find the free energies which
correspond to truncated distributions to be given by concise and efficiently
computable expressions, while update equations for model parameters (M-steps)
remain in their standard form. Furthermore, we obtain generic expressions for
expectation values w.r.t. truncated distributions. Based on these observations,
we show how efficient and easily applicable meta-algorithms can be formulated
that guarantee a monotonic increase of the free energy. Example applications of
the here derived framework provide novel theoretical results and learning
procedures for latent variable models as well as mixture models including
procedures to tightly couple sampling and variational optimization approaches.
Furthermore, by considering a special case of truncated variational
distributions, we can cleanly and fully embed the well-known `hard EM&apos;
approaches into the variational EM framework, and we show that `hard EM&apos; (for
models with discrete latents) provably optimizes a lower free energy bound of
the data log-likelihood.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lucke_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg L&amp;#xfc;cke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.05024">
<title>Optimal structure and parameter learning of Ising models. (arXiv:1612.05024v2 [cond-mat.stat-mech] UPDATED)</title>
<link>http://arxiv.org/abs/1612.05024</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstruction of structure and parameters of an Ising model from binary
samples is a problem of practical importance in a variety of disciplines,
ranging from statistical physics and computational biology to image processing
and machine learning. The focus of the research community shifted towards
developing universal reconstruction algorithms which are both computationally
efficient and require the minimal amount of expensive data. We introduce a new
method, Interaction Screening, which accurately estimates the model parameters
using local optimization problems. The algorithm provably achieves perfect
graph structure recovery with an information-theoretically optimal number of
samples, notably in the low-temperature regime which is known to be the hardest
for learning. The efficacy of Interaction Screening is assessed through
extensive numerical tests on synthetic Ising models of various topologies with
different types of interactions, as well as on a real data produced by a D-Wave
quantum computer. This study shows that the Interaction Screening method is an
exact, tractable and optimal technique universally solving the inverse Ising
problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Lokhov_A/0/1/0/all/0/1&quot;&gt;Andrey Y. Lokhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Vuffray_M/0/1/0/all/0/1&quot;&gt;Marc Vuffray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Misra_S/0/1/0/all/0/1&quot;&gt;Sidhant Misra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Chertkov_M/0/1/0/all/0/1&quot;&gt;Michael Chertkov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.07352">
<title>A unified framework for structured low-rank matrix learning. (arXiv:1704.07352v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1704.07352</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel optimization framework for learning a low-rank matrix
which is also constrained to lie in a linear subspace. Exploiting the duality
theory, we present a factorization that decouples the low-rank and structural
constraints onto separate factors. The optimization problem is formulated on
the Riemannian spectrahedron manifold, where the Riemannian framework allows to
develop computationally efficient conjugate gradient and trust-region
algorithms. Our approach easily accommodates popular non-smooth loss functions,
e.g., L1-loss, and our algorithms are scalable to large-scale problem
instances. The numerical comparisons show that our algorithms outperform
state-of-the-art in standard, robust, and non-negative matrix completion,
Hankel matrix learning, and multi-task feature learning problems on various
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jawanpuria_P/0/1/0/all/0/1&quot;&gt;Pratik Jawanpuria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mishra_B/0/1/0/all/0/1&quot;&gt;Bamdev Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.02355">
<title>Accelerating Science with Generative Adversarial Networks: An Application to 3D Particle Showers in Multi-Layer Calorimeters. (arXiv:1705.02355v2 [hep-ex] UPDATED)</title>
<link>http://arxiv.org/abs/1705.02355</link>
<description rdf:parseType="Literal">&lt;p&gt;Physicists at the Large Hadron Collider (LHC) rely on detailed simulations of
particle collisions to build expectations of what experimental data may look
like under different theory modeling assumptions. Petabytes of simulated data
are needed to develop analysis techniques, though they are expensive to
generate using existing algorithms and computing resources. The modeling of
detectors and the precise description of particle cascades as they interact
with the material in the calorimeter are the most computationally demanding
steps in the simulation pipeline. We therefore introduce a deep neural
network-based generative model to enable high-fidelity, fast, electromagnetic
calorimeter simulation. There are still challenges for achieving precision
across the entire phase space, but our current solution can reproduce a variety
of particle shower properties while achieving speed-up factors of up to
100,000$\times$. This opens the door to a new era of fast simulation that could
save significant computing time and disk space, while extending the reach of
physics searches and precision measurements at the LHC and beyond.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Paganini_M/0/1/0/all/0/1&quot;&gt;Michela Paganini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Oliveira_L/0/1/0/all/0/1&quot;&gt;Luke de Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Nachman_B/0/1/0/all/0/1&quot;&gt;Benjamin Nachman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.10934">
<title>Learning Graphs with Monotone Topology Properties and Multiple Connected Components. (arXiv:1705.10934v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.10934</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent papers have formulated the problem of learning graphs from data as an
inverse covariance estimation with graph Laplacian constraints. While such
problems are convex, existing methods cannot guarantee that solutions will have
specific graph topology properties (e.g., being a tree or k-partite), which are
desirable for some applications. In fact, the problem of learning a graph with
given topology properties, e.g., finding the k-partite graph that best matches
the data, is in general non-convex. In this paper, we develop novel results
that provide theoretical guarantees for an approach to solve these problems by
decomposing them into two sub-problems, for which efficient solutions are
known. Specifically, a graph topology inference (GTI) step is employed to
select a feasible graph topology, i.e., one having the desired topology
property. Then, a graph weight estimation (GWE) step is performed by solving a
generalized graph Laplacian estimation problem, where edges are constrained by
the topology found in the GTI step. Our main result is a bound on the error of
the GWE step as a function of the error in the GTI step. This error bound
indicates that the GTI step should be solved using an algorithm that
approximates the similarity matrix (which in general corresponds to a complete
weighted graph) by another matrix whose entries have been thresholded to zero
to have the desired type of graph topology. The GTI stage can leverage existing
methods (e.g., state of the art approaches for graph coloring) which are
typically based on minimizing the total weight of removed edges. Since the GWE
stage is formulated as an inverse covariance estimation problem with linear
constraints, it can be solved using existing convex optimization methods. We
demonstrate that our two step approach can achieve good results for both
synthetic and texture image data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pavez_E/0/1/0/all/0/1&quot;&gt;Eduardo Pavez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Egilmez_H/0/1/0/all/0/1&quot;&gt;Hilmi E. Egilmez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ortega_A/0/1/0/all/0/1&quot;&gt;Antonio Ortega&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.08457">
<title>Iterative Random Forests to detect predictive and stable high-order interactions. (arXiv:1706.08457v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.08457</link>
<description rdf:parseType="Literal">&lt;p&gt;Genomics has revolutionized biology, enabling the interrogation of whole
transcriptomes, genome-wide binding sites for proteins, and many other
molecular processes. However, individual genomic assays measure elements that
interact in vivo as components of larger molecular machines. Understanding how
these high-order interactions drive gene expression presents a substantial
statistical challenge. Building on Random Forests (RF), Random Intersection
Trees (RITs), and through extensive, biologically inspired simulations, we
developed the iterative Random Forest algorithm (iRF). iRF trains a
feature-weighted ensemble of decision trees to detect stable, high-order
interactions with same order of computational cost as RF. We demonstrate the
utility of iRF for high-order interaction discovery in two prediction problems:
enhancer activity in the early Drosophila embryo and alternative splicing of
primary transcripts in human derived cell lines. In Drosophila, among the 20
pairwise transcription factor interactions iRF identifies as stable (returned
in more than half of bootstrap replicates), 80% have been previously reported
as physical interactions. Moreover, novel third-order interactions, e.g.
between Zelda (Zld), Giant (Gt), and Twist (Twi), suggest high-order
relationships that are candidates for follow-up experiments. In human-derived
cells, iRF re-discovered a central role of H3K36me3 in chromatin-mediated
splicing regulation, and identified novel 5th and 6th order interactions,
indicative of multi-valent nucleosomes with specific roles in splicing
regulation. By decoupling the order of interactions from the computational cost
of identification, iRF opens new avenues of inquiry into the molecular
mechanisms underlying genome biology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Basu_S/0/1/0/all/0/1&quot;&gt;Sumanta Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kumbier_K/0/1/0/all/0/1&quot;&gt;Karl Kumbier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brown_J/0/1/0/all/0/1&quot;&gt;James B. Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.01384">
<title>Variance-Reduced Stochastic Learning by Networked Agents under Random Reshuffling. (arXiv:1708.01384v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.01384</link>
<description rdf:parseType="Literal">&lt;p&gt;A new amortized variance-reduced gradient (AVRG) algorithm was developed in
[1], which has constant storage requirement in comparison to SAGA and balanced
gradient computations in comparison to SVRG. One key advantage of the AVRG
strategy is its amenability to decentralized implementations. In this work, we
show how AVRG can be extended to the network case where multiple learning
agents are assumed to be connected by a graph topology. In this scenario, each
agent observes data that is spatially distributed and all agents are only
allowed to communicate with direct neighbors. Moreover, the amount of data
observed by the individual agents may differ drastically. For such situations,
the balanced gradient computation property of AVRG becomes a real advantage in
reducing idle time caused by unbalanced local data storage requirements, which
is characteristic of other reduced-variance gradient algorithms. The resulting
diffusion-AVRG algorithm is shown to have linear convergence to the exact
solution, and is much more memory efficient than other alternative algorithms.
In addition, by using a mini-batch strategy, it is shown that diffusion-AVRG is
more computationally efficient than exact diffusion or EXTRA while maintaining
almost the same amount of communications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Kun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_B/0/1/0/all/0/1&quot;&gt;Bicheng Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiageng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayed_A/0/1/0/all/0/1&quot;&gt;Ali H. Sayed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10646">
<title>On the Consistency of Quick Shift. (arXiv:1710.10646v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10646</link>
<description rdf:parseType="Literal">&lt;p&gt;Quick Shift is a popular mode-seeking and clustering algorithm. We present
finite sample statistical consistency guarantees for Quick Shift on mode and
cluster recovery under mild distributional assumptions. We then apply our
results to construct a consistent modal regression algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Heinrich Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10737">
<title>Linearly convergent stochastic heavy ball method for minimizing generalization error. (arXiv:1710.10737v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10737</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we establish the first linear convergence result for the
stochastic heavy ball method. The method performs SGD steps with a fixed
stepsize, amended by a heavy ball momentum term. In the analysis, we focus on
minimizing the expected loss and not on finite-sum minimization, which is
typically a much harder problem. While in the analysis we constrain ourselves
to quadratic loss, the overall objective is not necessarily strongly convex.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Loizou_N/0/1/0/all/0/1&quot;&gt;Nicolas Loizou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1&quot;&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05225">
<title>CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning. (arXiv:1711.05225v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05225</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop an algorithm that can detect pneumonia from chest X-rays at a
level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer
convolutional neural network trained on ChestX-ray14, currently the largest
publicly available chest X-ray dataset, containing over 100,000 frontal-view
X-ray images with 14 diseases. Four practicing academic radiologists annotate a
test set, on which we compare the performance of CheXNet to that of
radiologists. We find that CheXNet exceeds average radiologist performance on
the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and
achieve state of the art results on all 14 diseases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1&quot;&gt;Pranav Rajpurkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irvin_J/0/1/0/all/0/1&quot;&gt;Jeremy Irvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kaylie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Brandon Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1&quot;&gt;Hershel Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_T/0/1/0/all/0/1&quot;&gt;Tony Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1&quot;&gt;Daisy Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagul_A/0/1/0/all/0/1&quot;&gt;Aarti Bagul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1&quot;&gt;Curtis Langlotz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shpanskaya_K/0/1/0/all/0/1&quot;&gt;Katie Shpanskaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1&quot;&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1&quot;&gt;Andrew Y. Ng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11581">
<title>Outlier-robust moment-estimation via sum-of-squares. (arXiv:1711.11581v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11581</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop efficient algorithms for estimating low-degree moments of unknown
distributions in the presence of adversarial outliers. The guarantees of our
algorithms improve in many cases significantly over the best previous ones,
obtained in recent works of Diakonikolas et al, Lai et al, and Charikar et al.
We also show that the guarantees of our algorithms match information-theoretic
lower-bounds for the class of distributions we consider. These improved
guarantees allow us to give improved algorithms for independent component
analysis and learning mixtures of Gaussians in the presence of outliers.
&lt;/p&gt;
&lt;p&gt;Our algorithms are based on a standard sum-of-squares relaxation of the
following conceptually-simple optimization problem: Among all distributions
whose moments are bounded in the same way as for the unknown distribution, find
the one that is closest in statistical distance to the empirical distribution
of the adversarially-corrupted sample.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kothari_P/0/1/0/all/0/1&quot;&gt;Pravesh K. Kothari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steurer_D/0/1/0/all/0/1&quot;&gt;David Steurer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06050">
<title>Wasserstein Distributional Robustness and Regularization in Statistical Learning. (arXiv:1712.06050v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.06050</link>
<description rdf:parseType="Literal">&lt;p&gt;A central question in statistical learning is to design algorithms that not
only perform well on training data, but also generalize to new and unseen data.
In this paper, we tackle this question by formulating a distributionally robust
stochastic optimization (DRSO) problem, which seeks a solution that minimizes
the worst-case expected loss over a family of distributions that are close to
the empirical distribution in Wasserstein distances. We establish a connection
between such Wasserstein DRSO and regularization. More precisely, we identify a
broad class of loss functions, for which the Wasserstein DRSO is asymptotically
equivalent to a regularization problem with a gradient-norm penalty. Such
relation provides new interpretations for problems involving regularization,
including a great number of statistical learning problems and discrete choice
models (e.g. multinomial logit). The connection suggests a principled way to
regularize high-dimensional, non-convex problems. This is demonstrated through
the training of Wasserstein generative adversarial networks in deep learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Rui Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleywegt_A/0/1/0/all/0/1&quot;&gt;Anton J. Kleywegt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07788">
<title>Deep Unsupervised Clustering Using Mixture of Autoencoders. (arXiv:1712.07788v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07788</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised clustering is one of the most fundamental challenges in machine
learning. A popular hypothesis is that data are generated from a union of
low-dimensional nonlinear manifolds; thus an approach to clustering is
identifying and separating these manifolds. In this paper, we present a novel
approach to solve this problem by using a mixture of autoencoders. Our model
consists of two parts: 1) a collection of autoencoders where each autoencoder
learns the underlying manifold of a group of similar objects, and 2) a mixture
assignment neural network, which takes the concatenated latent vectors from the
autoencoders as input and infers the distribution over clusters. By jointly
optimizing the two parts, we simultaneously assign data to clusters and learn
the underlying manifolds of each cluster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dejiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eriksson_B/0/1/0/all/0/1&quot;&gt;Brian Eriksson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balzano_L/0/1/0/all/0/1&quot;&gt;Laura Balzano&lt;/a&gt;</dc:creator>
</item></rdf:RDF>