<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03327"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03387"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03519"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03598"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03611"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.06215"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06677"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00587"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03265"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03298"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03300"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03319"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03331"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03333"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03350"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03351"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03388"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03504"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03566"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03591"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03601"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03604"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.00672"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06501"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08898"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03666"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.10070"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09917"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.03064"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.03327">
<title>Fuzzy Clustering to Identify Clusters at Different Levels of Fuzziness: An Evolutionary Multi-Objective Optimization Approach. (arXiv:1808.03327v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.03327</link>
<description rdf:parseType="Literal">&lt;p&gt;Fuzzy clustering methods identify naturally occurring clusters in a dataset,
where the extent to which different clusters are overlapped can differ. Most
methods have a parameter to fix the level of fuzziness. However, the
appropriate level of fuzziness depends on the application at hand. This paper
presents Entropy $c$-Means (ECM), a method of fuzzy clustering that
simultaneously optimizes two contradictory objective functions, resulting in
the creation of fuzzy clusters with different levels of fuzziness. This allows
ECM to identify clusters with different degrees of overlap. ECM optimizes the
two objective functions using two multi-objective optimization methods,
Non-dominated Sorting Genetic Algorithm II (NSGA-II), and Multiobjective
Evolutionary Algorithm based on Decomposition (MOEA/D). We also propose a
method to select a suitable trade-off clustering from the Pareto front.
Experiments on challenging synthetic datasets as well as real-world datasets
show that ECM leads to better cluster detection compared to the conventional
fuzzy clustering methods as well as previously used multi-objective methods for
fuzzy clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Avisek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1&quot;&gt;Shounak Datta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Swagatam Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03387">
<title>Computational Complexity of Observing Evolution in Artificial-Life Forms. (arXiv:1808.03387v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1808.03387</link>
<description rdf:parseType="Literal">&lt;p&gt;Observations are an essential component of the simulation based studies on
artificial-evolutionary systems (AES) by which entities are identified and
their behavior is observed to uncover higher-level &quot;emergent&quot; phenomena.
Because of the heterogeneity of AES models and implicit nature of observations,
precise characterization of the observation process, independent of the
underlying micro-level reaction semantics of the model, is a difficult problem.
Building upon the multiset based algebraic framework to characterize
state-space trajectory of AES model simulations, we estimate bounds on
computational resource requirements of the process of automatically discovering
life-like evolutionary behavior in AES models during simulations. For
illustration, we consider the case of Langton&apos;s Cellular Automata model and
characterize the worst case computational complexity bounds for identifying
entity and population level reproduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_J/0/1/0/all/0/1&quot;&gt;Janardan Misra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03519">
<title>Self-Adaptive Systems in Organic Computing: Strategies for Self-Improvement. (arXiv:1808.03519v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.03519</link>
<description rdf:parseType="Literal">&lt;p&gt;With the intensified use of intelligent things, the demands on the
technological systems are increasing permanently. A possible approach to meet
the continuously changing challenges is to shift the system integration from
design to run-time by using adaptive systems. Diverse adaptivity properties,
so-called self-* properties, form the basis of these systems and one of the
properties is self-improvement. It describes the ability of a system not only
to adapt to a changing environment according to a predefined model, but also
the capability to adapt the adaptation logic of the whole system. In this
paper, a closer look is taken at the structure of self-adaptive systems.
Additionally, the systems&apos; ability to improve themselves during run-time is
described from the perspective of Organic Computing. Furthermore, four
different strategies for self-improvement are presented, following the taxonomy
of self-adaptation suggested by Christian Krupitzer et al.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niederquell_A/0/1/0/all/0/1&quot;&gt;Andreas Niederquell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03598">
<title>Overarching Computation Model (OCM). (arXiv:1808.03598v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.03598</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing models of computation, such as a Turing machine (hereafter, TM), do
not consider the agent involved in interpreting the outcome of the computation.
We argue that a TM, or any other computation model, has no significance if its
output is not interpreted by some agent. Furthermore, we argue that including
the interpreter in the model definition sheds light on some of the difficult
problems faced in computation and mathematics. We provide an analytic process
framework to address this limitation. The framework can be overlaid on existing
concepts of computation to address many practical and philosophical concerns
such as the P vs NP problem. In addition, we provide constructive proof for the
P vs NP problem under the assumption that the class NP comprises of problems
solvable by non-deterministic algorithms. We utilize the observation that
deterministic computational procedures lack fundamental capacity to fully
simulate their non-deterministic variant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghebrechristos_H/0/1/0/all/0/1&quot;&gt;Henok Ghebrechristos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1&quot;&gt;Drew Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03611">
<title>An Iterative Path-Breaking Approach with Mutation and Restart Strategies for the MAX-SAT Problem. (arXiv:1808.03611v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1808.03611</link>
<description rdf:parseType="Literal">&lt;p&gt;Although Path-Relinking is an effective local search method for many
combinatorial optimization problems, its application is not straightforward in
solving the MAX-SAT, an optimization variant of the satisfiability problem
(SAT) that has many real-world applications and has gained more and more
attention in academy and industry. Indeed, it was not used in any recent
competitive MAX-SAT algorithms in our knowledge. In this paper, we propose a
new local search algorithm called IPBMR for the MAX-SAT, that remedies the
drawbacks of the Path-Relinking method by using a careful combination of three
components: a new strategy named Path-Breaking to avoid unpromising regions of
the search space when generating trajectories between two elite solutions; a
weak and a strong mutation strategies, together with restarts, to diversify the
search; and stochastic path generating steps to avoid premature local optimum
solutions. We then present experimental results to show that IPBMR outperforms
two of the best state-of-the-art MAX-SAT solvers, and an empirical
investigation to identify and explain the effect of the three components in
IPBMR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhen-Xing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kun He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chu-Min Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.06215">
<title>On Singleton Arc Consistency for CSPs Defined by Monotone Patterns. (arXiv:1704.06215v4 [cs.CC] UPDATED)</title>
<link>http://arxiv.org/abs/1704.06215</link>
<description rdf:parseType="Literal">&lt;p&gt;Singleton arc consistency is an important type of local consistency which has
been recently shown to solve all constraint satisfaction problems (CSPs) over
constraint languages of bounded width. We aim to characterise all classes of
CSPs defined by a forbidden pattern that are solved by singleton arc
consistency and closed under removing constraints. We identify five new
patterns whose absence ensures solvability by singleton arc consistency, four
of which are provably maximal and three of which generalise 2-SAT. Combined
with simple counter-examples for other patterns, we make significant progress
towards a complete classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbonnel_C/0/1/0/all/0/1&quot;&gt;Clement Carbonnel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1&quot;&gt;David A. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooper_M/0/1/0/all/0/1&quot;&gt;Martin C. Cooper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zivny_S/0/1/0/all/0/1&quot;&gt;Stanislav Zivny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06677">
<title>Is prioritized sweeping the better episodic control?. (arXiv:1711.06677v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06677</link>
<description rdf:parseType="Literal">&lt;p&gt;Episodic control has been proposed as a third approach to reinforcement
learning, besides model-free and model-based control, by analogy with the three
types of human memory. i.e. episodic, procedural and semantic memory. But the
theoretical properties of episodic control are not well investigated. Here I
show that in deterministic tree Markov decision processes, episodic control is
equivalent to a form of prioritized sweeping in terms of sample efficiency as
well as memory and computation demands. For general deterministic and
stochastic environments, prioritized sweeping performs better even when memory
and computation demands are restricted to be equal to those of episodic
control. These results suggest generalizations of prioritized sweeping to
partially observable environments, its combined use with function approximation
and the search for possible implementations of prioritized sweeping in brains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brea_J/0/1/0/all/0/1&quot;&gt;Johanni Brea&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00587">
<title>Structure-sensitive Multi-scale Deep Neural Network for Low-Dose CT Denoising. (arXiv:1805.00587v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00587</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed tomography (CT) is a popular medical imaging modality in clinical
applications. At the same time, the x-ray radiation dose associated with CT
scans raises public concerns due to its potential risks to the patients. Over
the past years, major efforts have been dedicated to the development of
Low-Dose CT (LDCT) methods. However, the radiation dose reduction compromises
the signal-to-noise ratio (SNR), leading to strong noise and artifacts that
down-grade CT image quality. In this paper, we propose a novel 3D noise
reduction method, called Structure-sensitive Multi-scale Generative Adversarial
Net (SMGAN), to improve the LDCT image quality. Specifically, we incorporate
three-dimensional (3D) volumetric information to improve the image quality.
Also, different loss functions for training denoising models are investigated.
Experiments show that the proposed method can effectively preserve structural
and texture information from normal-dose CT (NDCT) images, and significantly
suppress noise and artifacts. Qualitative visual assessments by three
experienced radiologists demonstrate that the proposed method retrieves more
detailed information, and outperforms competing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenyu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qingsong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1&quot;&gt;Hongming Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gjesteby_L/0/1/0/all/0/1&quot;&gt;Lars Gjesteby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_S/0/1/0/all/0/1&quot;&gt;Shenghong Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuiyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1&quot;&gt;Wenxiang Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Ge Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03265">
<title>A Hybrid Recommender System for Patient-Doctor Matchmaking in Primary Care. (arXiv:1808.03265v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.03265</link>
<description rdf:parseType="Literal">&lt;p&gt;We partner with a leading European healthcare provider and design a mechanism
to match patients with family doctors in primary care. We define the
matchmaking process for several distinct use cases given different levels of
available information about patients. Then, we adopt a hybrid recommender
system to present each patient a list of family doctor recommendations. In
particular, we model patient trust of family doctors using a large-scale
dataset of consultation histories, while accounting for the temporal dynamics
of their relationships. Our proposed approach shows higher predictive accuracy
than both a heuristic baseline and a collaborative filtering approach, and the
proposed trust measure further improves model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1&quot;&gt;Qiwei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1&quot;&gt;Mengxin Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Troya_I/0/1/0/all/0/1&quot;&gt;Inigo Martinez de Rituerto de Troya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1&quot;&gt;Manas Gaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zejnilovic_L/0/1/0/all/0/1&quot;&gt;Leid Zejnilovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03298">
<title>Probabilistic Ensemble of Collaborative Filters. (arXiv:1808.03298v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1808.03298</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative filtering is an important technique for recommendation. Whereas
it has been repeatedly shown to be effective in previous work, its performance
remains unsatisfactory in many real-world applications, especially those where
the items or users are highly diverse. In this paper, we explore an
ensemble-based framework to enhance the capability of a recommender in handling
diverse data. Specifically, we formulate a probabilistic model which integrates
the items, the users, as well as the associations between them into a
generative process. On top of this formulation, we further derive a progressive
algorithm to construct an ensemble of collaborative filters. In each iteration,
a new filter is derived from re-weighted entries and incorporated into the
ensemble. It is noteworthy that while the algorithmic procedure of our
algorithm is apparently similar to boosting, it is derived from an essentially
different formulation and thus differs in several key technical aspects. We
tested the proposed method on three large datasets, and observed substantial
improvement over the state of the art, including L2Boost, an effective method
based on boosting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03300">
<title>$\alpha$-Approximation Density-based Clustering of Multi-valued Objects. (arXiv:1808.03300v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.03300</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-valued data are commonly found in many real applications. During the
process of clustering multi-valued data, most existing methods use sampling or
aggregation mechanisms that cannot reflect the real distribution of objects and
their instances and thus fail to obtain high-quality clusters. In this paper, a
concept of $\alpha$-approximation distance is introduced to measure the
connectivity between multi-valued objects by taking account of the distribution
of the instances. An $\alpha$-approximation density-based clustering algorithm
(DBCMO) is proposed to efficiently cluster the multi-valued objects by using
global and local R* tree structures. To speed up the algorithm, four pruning
rules on the tree structures are implemented. Empirical studies on synthetic
and real datasets demonstrate that DBCMO can efficiently and effectively
discover the multi-valued object clusters. A comparison with two existing
methods further shows that DBCMO can better handle a continuous decrease in the
cluster density and detect clusters of varying density.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhilin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03319">
<title>Continuous Authentication of Smartphones Based on Application Usage. (arXiv:1808.03319v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1808.03319</link>
<description rdf:parseType="Literal">&lt;p&gt;An empirical investigation of active/continuous authentication for
smartphones is presented in this paper by exploiting users&apos; unique application
usage data, i.e., distinct patterns of use, modeled by a Markovian process.
Variations of Hidden Markov Models (HMMs) are evaluated for continuous user
verification, and challenges due to the sparsity of session-wise data, an
explosion of states, and handling unforeseen events in the test data are
tackled. Unlike traditional approaches, the proposed formulation does not
depend on the top N-apps, rather uses the complete app-usage information to
achieve low latency. Through experimentation, empirical assessment of the
impact of unforeseen events, i.e., unknown applications and unforeseen
observations, on user verification is done via a modified edit-distance
algorithm for simple sequence matching. It is found that for enhanced
verification performance, unforeseen events should be incorporated in the
models by adopting smoothing techniques with HMMs. For validation, extensive
experiments on two distinct datasets are performed. The marginal smoothing
technique is the most effective for user verification in terms of equal error
rate (EER) and with a sampling rate of 1/30s^{-1} and 30 minutes of historical
data, and the method is capable of detecting an intrusion within ~2.5 minutes
of application use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahbub_U/0/1/0/all/0/1&quot;&gt;Upal Mahbub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komulainen_J/0/1/0/all/0/1&quot;&gt;Jukka Komulainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1&quot;&gt;Denzil Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1&quot;&gt;Rama Chellappa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03331">
<title>The Effectiveness of Multitask Learning for Phenotyping with Electronic Health Records Data. (arXiv:1808.03331v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.03331</link>
<description rdf:parseType="Literal">&lt;p&gt;Electronic phenotyping, which is the task of ascertaining whether an
individual has a medical condition of interest by analyzing their medical
records, is a foundational task in clinical informatics. Increasingly,
electronic phenotyping is performed via supervised learning. We investigate the
effectiveness of multitask learning for phenotyping using electronic health
records (EHR) data. Multitask learning aims to improve model performance on a
target task by jointly learning additional auxiliary tasks, and has been used
to good effect in disparate areas of machine learning. However, its utility
when applied to EHR data has not been established, and prior work suggests that
its benefits are inconsistent. Here we present experiments that elucidate when
multitask learning with neural networks can improve performance for electronic
phenotyping using EHR data relative to well-tuned single task neural networks.
We find that multitask networks consistently outperform single task networks
for rare phenotypes but underperform for more common phenotypes. The effect
size increases as more auxiliary tasks are added.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ding_D/0/1/0/all/0/1&quot;&gt;Daisy Yi Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Simpson_C/0/1/0/all/0/1&quot;&gt;Chlo&amp;#xe9; Simpson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pfohl_S/0/1/0/all/0/1&quot;&gt;Stephen Pfohl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kale_D/0/1/0/all/0/1&quot;&gt;Dave C. Kale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jung_K/0/1/0/all/0/1&quot;&gt;Kenneth Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nigam H. Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03333">
<title>Linked Causal Variational Autoencoder for Inferring Paired Spillover Effects. (arXiv:1808.03333v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.03333</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling spillover effects from observational data is an important problem in
economics, business, and other fields of research. % It helps us infer the
causality between two seemingly unrelated set of events. For example, if
consumer spending in the United States declines, it has spillover effects on
economies that depend on the U.S. as their largest export market. In this
paper, we aim to infer the causation that results in spillover effects between
pairs of entities (or units), we call this effect as \textit{paired spillover}.
To achieve this, we leverage the recent developments in variational inference
and deep learning techniques to propose a generative model called Linked Causal
Variational Autoencoder (LCVA). Similar to variational autoencoders (VAE), LCVA
incorporates an encoder neural network to learn the latent attributes and a
decoder network to reconstruct the inputs. However, unlike VAE, LCVA treats the
\textit{latent attributes as confounders that are assumed to affect both the
treatment and the outcome of units}. Specifically, given a pair of units $u$
and $\bar{u}$, their individual treatment and outcomes, the encoder network of
LCVA samples the confounders by conditioning on the observed covariates of $u$,
the treatments of both $u$ and $\bar{u}$ and the outcome of $u$. Once inferred,
the latent attributes (or confounders) of $u$ captures the spillover effect of
$\bar{u}$ on $u$. Using a network of users from job training dataset (LaLonde
(1986)) and co-purchase dataset from Amazon e-commerce domain, we show that
LCVA is significantly more robust than existing methods in capturing spillover
effects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakesh_V/0/1/0/all/0/1&quot;&gt;Vineeth Rakesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1&quot;&gt;Ruocheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1&quot;&gt;Raha Moraffah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_N/0/1/0/all/0/1&quot;&gt;Nitin Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03350">
<title>Uncovering the Spread of Chagas Disease in Argentina and Mexico. (arXiv:1808.03350v1 [cs.CY])</title>
<link>http://arxiv.org/abs/1808.03350</link>
<description rdf:parseType="Literal">&lt;p&gt;Chagas disease is a neglected disease, and information about its geographical
spread is very scarse. We analyze here mobility and calling patterns in order
to identify potential risk zones for the disease, by using public health
information and mobile phone records. Geolocalized call records are rich in
social and mobility information, which can be used to infer whether an
individual has lived in an endemic area. We present two case studies in Latin
American countries. Our objective is to generate risk maps which can be used by
public health campaign managers to prioritize detection campaigns and target
specific areas. Finally, we analyze the value of mobile phone data to infer
long-term migrations, which play a crucial role in the geographical spread of
Chagas disease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monasterio_J/0/1/0/all/0/1&quot;&gt;Juan de Monasterio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salles_A/0/1/0/all/0/1&quot;&gt;Alejo Salles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1&quot;&gt;Carolina Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinberg_D/0/1/0/all/0/1&quot;&gt;Diego Weinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minnoni_M/0/1/0/all/0/1&quot;&gt;Martin Minnoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Travizano_M/0/1/0/all/0/1&quot;&gt;Matias Travizano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarraute_C/0/1/0/all/0/1&quot;&gt;Carlos Sarraute&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03351">
<title>Exploiting Structure for Fast Kernel Learning. (arXiv:1808.03351v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.03351</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose two methods for exact Gaussian process (GP) inference and learning
on massive image, video, spatial-temporal, or multi-output datasets with
missing values (or &quot;gaps&quot;) in the observed responses. The first method ignores
the gaps using sparse selection matrices and a highly effective low-rank
preconditioner is introduced to accelerate computations. The second method
introduces a novel approach to GP training whereby response values are inferred
on the gaps before explicitly training the model. We find this second approach
to be greatly advantageous for the class of problems considered. Both of these
novel approaches make extensive use of Kronecker matrix algebra to design
massively scalable algorithms which have low memory requirements. We
demonstrate exact GP inference for a spatial-temporal climate modelling problem
with 3.7 million training points as well as a video reconstruction problem with
1 billion points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Evans_T/0/1/0/all/0/1&quot;&gt;Trefor W. Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nair_P/0/1/0/all/0/1&quot;&gt;Prasanth B. Nair&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03388">
<title>Code-division multiplexed resistive pulse sensor networks for spatio-temporal detection of particles in microfluidic devices. (arXiv:1808.03388v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1808.03388</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatial separation of suspended particles based on contrast in their physical
or chemical properties forms the basis of various biological assays performed
on lab-on-achip devices. To electronically acquire this information, we have
recently introduced a microfluidic sensing platform, called Microfluidic CODES,
which combines the resistive pulse sensing with the code division multiple
access in multiplexing a network of integrated electrical sensors. In this
paper, we enhance the multiplexing capacity of the Microfluidic CODES by
employing sensors that generate non-orthogonal code waveforms and a new
decoding algorithm that combines machine learning techniques with minimum
mean-squared error estimation. As a proof of principle, we fabricated a
microfluidic device with a network of 10 code-multiplexed sensors and
characterized it using cells suspended in phosphate buffer saline solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Ningquan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruxiu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khodambashi_R/0/1/0/all/0/1&quot;&gt;Roozbeh Khodambashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asmare_N/0/1/0/all/0/1&quot;&gt;Norh Asmare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarioglu_A/0/1/0/all/0/1&quot;&gt;A. Fatih Sarioglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03504">
<title>Model Approximation Using Cascade of Tree Decompositions. (arXiv:1808.03504v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1808.03504</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a general, multistage framework for graphical model
approximation using a cascade of models such as trees. In particular, we look
at the problem of covariance matrix approximation for Gaussian distributions as
linear transformations of tree models. This is a new way to decompose the
covariance matrix. Here, we propose an algorithm which incorporates the
Cholesky factorization method to compute the decomposition matrix and thus can
approximate a simple graphical model using a cascade of the Cholesky
factorization of the tree approximation transformations. The Cholesky
decomposition enables us to achieve a tree structure factor graph at each
cascade stage of the algorithm which facilitates the use of the message passing
algorithm since the approximated graph has less loops compared to the original
graph. The overall graph is a cascade of factor graphs with each factor graph
being a tree. This is a different perspective on the approximation model, and
algorithms such as Gaussian belief propagation can be used on this overall
graph. Here, we present theoretical result that guarantees the convergence of
the proposed model approximation using the cascade of tree decompositions. In
the simulations, we look at synthetic and real data and measure the performance
of the proposed framework by comparing the KL divergences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khajavi_N/0/1/0/all/0/1&quot;&gt;Navid Tafaghodi Khajavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuh_A/0/1/0/all/0/1&quot;&gt;Anthony Kuh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03566">
<title>Greedy Algorithms for Approximating the Diameter of Machine Learning Datasets in Multidimensional Euclidean Space. (arXiv:1808.03566v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.03566</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding the diameter of a dataset in multidimensional Euclidean space is a
well-established problem, with well-known algorithms. However, most of the
algorithms found in the literature do not scale well with large values of data
dimension, so the time complexity grows exponentially in most cases, which
makes these algorithms impractical. Therefore, we implemented 4 simple greedy
algorithms to be used for approximating the diameter of a multidimensional
dataset; these are based on minimum/maximum l2 norms, hill climbing search,
Tabu search and Beam search approaches, respectively. The time complexity of
the implemented algorithms is near-linear, as they scale near-linearly with
data size and its dimensions. The results of the experiments (conducted on
different machine learning data sets) prove the efficiency of the implemented
algorithms and can therefore be recommended for finding the diameter to be used
by different machine learning applications when needed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanat_A/0/1/0/all/0/1&quot;&gt;Ahmad B. Hassanat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03591">
<title>How Complex is your classification problem? A survey on measuring classification complexity. (arXiv:1808.03591v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.03591</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting characteristics from the training datasets of classification
problems has proven effective in a number of meta-analyses. Among them,
measures of classification complexity can estimate the difficulty in separating
the data points into their expected classes. Descriptors of the spatial
distribution of the data and estimates of the shape and size of the decision
boundary are among the existent measures for this characterization. This
information can support the formulation of new data-driven pre-processing and
pattern recognition techniques, which can in turn be focused on challenging
characteristics of the problems. This paper surveys and analyzes measures which
can be extracted from the training datasets in order to characterize the
complexity of the respective classification problems. Their use in recent
literature is also reviewed and discussed, allowing to prospect opportunities
for future work in the area. Finally, descriptions are given on an R package
named Extended Complexity Library (ECoL) that implements a set of complexity
measures and is made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorena_A/0/1/0/all/0/1&quot;&gt;Ana C. Lorena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_L/0/1/0/all/0/1&quot;&gt;Lu&amp;#xed;s P. F. Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1&quot;&gt;Jens Lehmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souto_M/0/1/0/all/0/1&quot;&gt;Marcilio C. P. Souto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Tin K. Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03601">
<title>Using Randomness to Improve Robustness of Machine-Learning Models Against Evasion Attacks. (arXiv:1808.03601v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1808.03601</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models have been widely used in security applications such
as intrusion detection, spam filtering, and virus or malware detection.
However, it is well-known that adversaries are always trying to adapt their
attacks to evade detection. For example, an email spammer may guess what
features spam detection models use and modify or remove those features to avoid
detection. There has been some work on making machine learning models more
robust to such attacks. However, one simple but promising approach called {\em
randomization} is underexplored. This paper proposes a novel
randomization-based approach to improve robustness of machine learning models
against evasion attacks. The proposed approach incorporates randomization into
both model training time and model application time (meaning when the model is
used to detect attacks). We also apply this approach to random forest, an
existing ML method which already has some degree of randomness. Experiments on
intrusion detection and spam filtering data show that our approach further
improves robustness of random-forest method. We also discuss how this approach
can be applied to other ML models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03604">
<title>Disease Progression Timeline Estimation for Alzheimer&apos;s Disease using Discriminative Event Based Modeling. (arXiv:1808.03604v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.03604</link>
<description rdf:parseType="Literal">&lt;p&gt;Alzheimer&apos;s Disease (AD) is characterized by a cascade of biomarkers becoming
abnormal, the pathophysiology of which is very complex and largely unknown.
Event-based modeling (EBM) is a data-driven technique to estimate the sequence
in which biomarkers for a disease become abnormal based on cross-sectional
data. It can help in understanding the dynamics of disease progression and
facilitate early diagnosis and prognosis. In this work we propose a novel
discriminative approach to EBM, which is shown to be more accurate than
existing state-of-the-art EBM methods. The method first estimates for each
subject an approximate ordering of events. Subsequently, the central ordering
over all subjects is estimated by fitting a generalized Mallows model to these
approximate subject-specific orderings. We also introduce the concept of
relative distance between events which helps in creating a disease progression
timeline. Subsequently, we propose a method to stage subjects by placing them
on the estimated disease progression timeline. We evaluated the proposed method
on Alzheimer&apos;s Disease Neuroimaging Initiative (ADNI) data and compared the
results with existing state-of-the-art EBM methods. We also performed extensive
experiments on synthetic data simulating the progression of Alzheimer&apos;s
disease. The event orderings obtained on ADNI data seem plausible and are in
agreement with the current understanding of progression of AD. The proposed
patient staging algorithm performed consistently better than that of
state-of-the-art EBM methods. Event orderings obtained in simulation
experiments were more accurate than those of other EBM methods and the
estimated disease progression timeline was observed to correlate with the
timeline of actual disease progression. The results of these experiments are
encouraging and suggest that discriminative EBM is a promising approach to
disease progression modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatraghavan_V/0/1/0/all/0/1&quot;&gt;Vikram Venkatraghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bron_E/0/1/0/all/0/1&quot;&gt;Esther E. Bron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessen_W/0/1/0/all/0/1&quot;&gt;Wiro J. Niessen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_S/0/1/0/all/0/1&quot;&gt;Stefan Klein&lt;/a&gt;, for the &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Initiative_A/0/1/0/all/0/1&quot;&gt;Alzheimer&amp;#x27;s Disease Neuroimaging Initiative&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.00672">
<title>The Inflation Technique for Causal Inference with Latent Variables. (arXiv:1609.00672v4 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1609.00672</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of causal inference is to determine if a given probability
distribution on observed variables is compatible with some causal structure.
The difficult case is when the causal structure includes latent variables. We
here introduce the $\textit{inflation technique}$ for tackling this problem. An
inflation of a causal structure is a new causal structure that can contain
multiple copies of each of the original variables, but where the ancestry of
each copy mirrors that of the original. To every distribution of the observed
variables that is compatible with the original causal structure, we assign a
family of marginal distributions on certain subsets of the copies that are
compatible with the inflated causal structure. It follows that compatibility
constraints for the inflation can be translated into compatibility constraints
for the original causal structure. Even if the constraints at the level of
inflation are weak, such as observable statistical independences implied by
disjoint causal ancestry, the translated constraints can be strong. We apply
this method to derive new inequalities whose violation by a distribution
witnesses that distribution&apos;s incompatibility with the causal structure (of
which Bell inequalities and Pearl&apos;s instrumental inequality are prominent
examples). We describe an algorithm for deriving all such inequalities for the
original causal structure that follow from ancestral independences in the
inflation. For three observed binary variables with pairwise common causes, it
yields inequalities that are stronger in at least some aspects than those
obtainable by existing methods. We also describe an algorithm that derives a
weaker set of inequalities but is more efficient. Finally, we discuss which
inflations are such that the inequalities one obtains from them remain valid
even for quantum (and post-quantum) generalizations of the notion of a causal
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wolfe_E/0/1/0/all/0/1&quot;&gt;Elie Wolfe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Spekkens_R/0/1/0/all/0/1&quot;&gt;Robert W. Spekkens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Fritz_T/0/1/0/all/0/1&quot;&gt;Tobias Fritz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06501">
<title>Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning. (arXiv:1802.06501v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06501</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems play a crucial role in mitigating the problem of
information overload by suggesting users&apos; personalized items or services. The
vast majority of traditional recommender systems consider the recommendation
procedure as a static process and make recommendations following a fixed
strategy. In this paper, we propose a novel recommender system with the
capability of continuously improving its strategies during the interactions
with users. We model the sequential interactions between users and a
recommender system as a Markov Decision Process (MDP) and leverage
Reinforcement Learning (RL) to automatically learn the optimal strategies via
recommending trial-and-error items and receiving reinforcements of these items
from users&apos; feedback. Users&apos; feedback can be positive and negative and both
types of feedback have great potentials to boost recommendations. However, the
number of negative feedback is much larger than that of positive one; thus
incorporating them simultaneously is challenging since positive feedback could
be buried by negative one. In this paper, we develop a novel approach to
incorporate them into the proposed deep recommender system (DEERS) framework.
The experimental results based on real-world e-commerce data demonstrate the
effectiveness of the proposed framework. Further experiments have been
conducted to understand the importance of both positive and negative feedback
in recommendations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zhuoye Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Long Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08898">
<title>Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo. (arXiv:1802.08898v5 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08898</link>
<description rdf:parseType="Literal">&lt;p&gt;Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from
high-dimensional distributions in Statistics and Machine learning. HMC is known
to run very efficiently in practice and its popular second-order &quot;leapfrog&quot;
implementation has long been conjectured to run in $d^{1/4}$ gradient
evaluations. Here we show that this conjecture is true when sampling from
strongly log-concave target distributions that satisfy a weak third-order
regularity property associated with the input data. Our regularity condition is
weaker than the Lipschitz Hessian property and allows us to show faster
convergence bounds for a much larger class of distributions than would be
possible with the usual Lipschitz Hessian constant alone. Important
distributions that satisfy our regularity condition include posterior
distributions used in Bayesian logistic regression for which the data satisfies
an &quot;incoherence&quot; property. Our result compares favorably with the best
available bounds for the class of strongly log-concave distributions, which
grow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, our
simulations on synthetic data suggest that, when our regularity condition is
satisfied, leapfrog HMC performs better than its competitors -- both in terms
of accuracy and in terms of the number of gradient evaluations it requires.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mangoubi_O/0/1/0/all/0/1&quot;&gt;Oren Mangoubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1&quot;&gt;Nisheeth K. Vishnoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03666">
<title>Standing Wave Decomposition Gaussian Process. (arXiv:1803.03666v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.03666</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a Standing Wave Decomposition (SWD) approximation to Gaussian
Process regression (GP). GP involves a costly matrix inversion operation, which
limits applicability to large data analysis. For an input space that can be
approximated by a grid and when correlations among data are short-ranged, the
kernel matrix inversion can be replaced by analytic diagonalization using the
SWD. We show that this approach applies to uni- and multi-dimensional input
data, extends to include longer-range correlations, and the grid can be in a
latent space and used as inducing points. Through simulations, we show that our
approximate method applied to the squared exponential kernel outperforms
existing methods in predictive accuracy per unit time in the regime where data
are plentiful. Our SWD-GP is recommended for regression analyses where there is
a relatively large amount of data and/or there are constraints on computation
time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chi-Ken Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Scott Cheng-Hsin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shafto_P/0/1/0/all/0/1&quot;&gt;Patrick Shafto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.10070">
<title>Adaptive pooling operators for weakly labeled sound event detection. (arXiv:1804.10070v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/1804.10070</link>
<description rdf:parseType="Literal">&lt;p&gt;Sound event detection (SED) methods are tasked with labeling segments of
audio recordings by the presence of active sound sources. SED is typically
posed as a supervised machine learning problem, requiring strong annotations
for the presence or absence of each sound source at every time instant within
the recording. However, strong annotations of this type are both labor- and
cost-intensive for human annotators to produce, which limits the practical
scalability of SED methods.
&lt;/p&gt;
&lt;p&gt;In this work, we treat SED as a multiple instance learning (MIL) problem,
where training labels are static over a short excerpt, indicating the presence
or absence of sound sources but not their temporal locality. The models,
however, must still produce temporally dynamic predictions, which must be
aggregated (pooled) when comparing against static labels during training. To
facilitate this aggregation, we develop a family of adaptive pooling
operators---referred to as auto-pool---which smoothly interpolate between
common pooling operators, such as min-, max-, or average-pooling, and
automatically adapt to the characteristics of the sound sources in question. We
evaluate the proposed pooling operators on three datasets, and demonstrate that
in each case, the proposed methods outperform non-adaptive pooling operators
for static prediction, and nearly match the performance of models trained with
strong, dynamic annotations. The proposed method is evaluated in conjunction
with convolutional neural networks, but can be readily applied to any
differentiable model for time-series label prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McFee_B/0/1/0/all/0/1&quot;&gt;Brian McFee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salamon_J/0/1/0/all/0/1&quot;&gt;Justin Salamon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bello_J/0/1/0/all/0/1&quot;&gt;Juan Pablo Bello&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09917">
<title>Machine-learning inference of fluid variables from data using reservoir computing. (arXiv:1805.09917v3 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09917</link>
<description rdf:parseType="Literal">&lt;p&gt;We infer both microscopic and macroscopic behaviors of a three-dimensional
chaotic fluid flow using reservoir computing. In our procedure of the
inference, we assume no prior knowledge of a physical process of a fluid flow
except that its behavior is complex but deterministic. We present two ways of
inference of the complex behavior; the first called partial-inference requires
continued knowledge of partial time-series data during the inference as well as
past time-series data, while the second called full-inference requires only
past time-series data as training data. For the first case, we are able to
infer long-time motion of microscopic fluid variables. For the second case, we
show that the reservoir dynamics constructed from only past data of energy
functions can infer the future behavior of energy functions and reproduce the
energy spectrum. It is also shown that we can infer a time-series data from
only one measurement by using the delay coordinates. These implies that the
obtained two reservoir systems constructed without the knowledge of microscopic
data are equivalent to the dynamical systems describing macroscopic behavior of
energy functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nakai_K/0/1/0/all/0/1&quot;&gt;Kengo Nakai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Saiki_Y/0/1/0/all/0/1&quot;&gt;Yoshitaka Saiki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.03064">
<title>Gradient and Newton Boosting for Classification and Regression. (arXiv:1808.03064v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1808.03064</link>
<description rdf:parseType="Literal">&lt;p&gt;Boosting algorithms enjoy large popularity due to their high predictive
accuracy on a wide array of datasets. In this article, we argue that it is
important to distinguish between three types of statistical boosting
algorithms: gradient and Newton boosting as well as a hybrid variant of the
two. To date, both researchers and practitioners often do not discriminate
between these boosting variants. We compare the different boosting algorithms
on a wide range of real and simulated datasets for various choices of loss
functions using trees as base learners. In addition, we introduce a novel
tuning parameter for Newton boosting. We find that Newton boosting performs
substantially better than the other boosting variants for classification, and
that the novel tuning parameter is important for predictive accuracy
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sigrist_F/0/1/0/all/0/1&quot;&gt;Fabio Sigrist&lt;/a&gt;</dc:creator>
</item></rdf:RDF>