<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04013"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04118"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00342"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02389"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03858"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03909"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03975"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04001"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04170"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.02000"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08829"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.11088"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.02303"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03845"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03873"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03876"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03877"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03878"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03907"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03929"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03931"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03933"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04010"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04020"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04056"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04073"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04106"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04119"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04183"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04188"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04193"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04222"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04239"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04252"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04270"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1501.02102"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1603.06202"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10388"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06531"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01330"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08598"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09046"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06270"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.07004"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03711"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.04013">
<title>Medusa: A Scalable Interconnect for Many-Port DNN Accelerators and Wide DRAM Controller Interfaces. (arXiv:1807.04013v1 [cs.AR])</title>
<link>http://arxiv.org/abs/1807.04013</link>
<description rdf:parseType="Literal">&lt;p&gt;To cope with the increasing demand and computational intensity of deep neural
networks (DNNs), industry and academia have turned to accelerator technologies.
In particular, FPGAs have been shown to provide a good balance between
performance and energy efficiency for accelerating DNNs. While significant
research has focused on how to build efficient layer processors, the
computational building blocks of DNN accelerators, relatively little attention
has been paid to the on-chip interconnects that sit between the layer
processors and the FPGA&apos;s DRAM controller.
&lt;/p&gt;
&lt;p&gt;We observe a disparity between DNN accelerator interfaces, which tend to
comprise many narrow ports, and FPGA DRAM controller interfaces, which tend to
be wide buses. This mismatch causes traditional interconnects to consume
significant FPGA resources. To address this problem, we designed Medusa: an
optimized FPGA memory interconnect which transposes data in the interconnect
fabric, tailoring the interconnect to the needs of DNN layer processors.
Compared to a traditional FPGA interconnect, our design can reduce LUT and FF
use by 4.7x and 6.0x, and improves frequency by 1.8x.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yongming Shen&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1&quot;&gt;Tianchu Ji&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferdman_M/0/1/0/all/0/1&quot;&gt;Michael Ferdman&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milder_P/0/1/0/all/0/1&quot;&gt;Peter Milder&lt;/a&gt; (1) ((1) Stony Brook University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04118">
<title>Emergence of Altruism Behavior for Multi Feeding Areas in Army Ant Social Evolutionary System. (arXiv:1807.04118v1 [cs.MA])</title>
<link>http://arxiv.org/abs/1807.04118</link>
<description rdf:parseType="Literal">&lt;p&gt;Army ants perform the altruism that an ant sacrifices its own well-being for
the benefit of another ants. Army ants build bridges using their own bodies
along the path from a food to the nest. We developed the army ant inspired
social evolutionary system which can perform the altruism. The system has 2
kinds of ant agents, `Major ant&apos; and `Minor ant&apos; and the ants communicate with
each other via pheromones. One ants can recognize them as the signals from the
other ants. The pheromones evaporate with the certain ratio and diffused into
the space of neighbors stochastically. If the optimal bridge is found, the path
through the bridge is the shortest route from foods to the nest. We define the
probability for an ant to leave a bridge at a low occupancy condition of ants
and propose the constructing method of the optimal route. In this paper, the
behaviors of ant under the environment with two or more feeding spots are
observed. Some experimental results show the behaviors of great interest with
respect to altruism of ants. The description in some computer simulation is
reported in this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uemoto_T/0/1/0/all/0/1&quot;&gt;Takuya Uemoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hara_A/0/1/0/all/0/1&quot;&gt;Akira Hara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00342">
<title>A Feedback Neural Network for Small Target Motion Detection in Cluttered Backgrounds. (arXiv:1805.00342v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00342</link>
<description rdf:parseType="Literal">&lt;p&gt;Small target motion detection is critical for insects to search for and track
mates or prey which always appear as small dim speckles in the visual field. A
class of specific neurons, called small target motion detectors (STMDs), has
been characterized by exquisite sensitivity for small target motion.
Understanding and analyzing visual pathway of STMD neurons are beneficial to
design artificial visual systems for small target motion detection. Feedback
loops have been widely identified in visual neural circuits and play an
important role in target detection. However, if there exists a feedback loop in
the STMD visual pathway or if a feedback loop could significantly improve the
detection performance of STMD neurons, is unclear. In this paper, we propose a
feedback neural network for small target motion detection against naturally
cluttered backgrounds. In order to form a feedback loop, model output is
temporally delayed and relayed to previous neural layer as feedback signal.
Extensive experiments showed that the significant improvement of the proposed
feedback neural network over the existing STMD-based models for small target
motion detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jigen Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1&quot;&gt;Shigang Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02389">
<title>Generative models on accelerated neuromorphic hardware. (arXiv:1807.02389v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1807.02389</link>
<description rdf:parseType="Literal">&lt;p&gt;The traditional von Neumann computer architecture faces serious obstacles,
both in terms of miniaturization and in terms of heat production, with
increasing performance. Artificial neural (neuromorphic) substrates represent
an alternative approach to tackle this challenge. A special subset of these
systems follow the principle of &quot;physical modeling&quot; as they directly use the
physical properties of the underlying substrate to realize computation with
analog components. While these systems are potentially faster and/or more
energy efficient than conventional computers, they require robust models that
can cope with their inherent limitations in terms of controllability and range
of parameters. A natural source of inspiration for robust models is
neuroscience as the brain faces similar challenges. It has been recently
suggested that sampling with the spiking dynamics of neurons is potentially
suitable both as a generative and a discriminative model for artificial neural
substrates. In this work we present the implementation of sampling with leaky
integrate-and-fire neurons on the BrainScaleS physical model system. We prove
the sampling property of the network and demonstrate its applicability to
high-dimensional datasets. The required stochasticity is provided by a spiking
random network on the same substrate. This allows the system to run in a
self-contained fashion without external stochastic input from the host
environment. The implementation provides a basis as a building block in
large-scale biologically relevant emulations, as a fast approximate sampler or
as a framework to realize on-chip learning on (future generations of)
accelerated spiking neuromorphic hardware. Our work contributes to the
development of robust computation on physical model systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kungl_A/0/1/0/all/0/1&quot;&gt;Akos F. Kungl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmitt_S/0/1/0/all/0/1&quot;&gt;Sebastian Schmitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klahn_J/0/1/0/all/0/1&quot;&gt;Johann Kl&amp;#xe4;hn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_P/0/1/0/all/0/1&quot;&gt;Paul M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumbach_A/0/1/0/all/0/1&quot;&gt;Andreas Baumbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dold_D/0/1/0/all/0/1&quot;&gt;Dominik Dold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kugele_A/0/1/0/all/0/1&quot;&gt;Alexander Kugele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurtler_N/0/1/0/all/0/1&quot;&gt;Nico G&amp;#xfc;rtler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_L/0/1/0/all/0/1&quot;&gt;Luziwei Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_E/0/1/0/all/0/1&quot;&gt;Eric M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koke_C/0/1/0/all/0/1&quot;&gt;Christoph Koke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleider_M/0/1/0/all/0/1&quot;&gt;Mitja Kleider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mauch_C/0/1/0/all/0/1&quot;&gt;Christian Mauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breitwieser_O/0/1/0/all/0/1&quot;&gt;Oliver Breitwieser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guttler_M/0/1/0/all/0/1&quot;&gt;Maurice G&amp;#xfc;ttler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Husmann_D/0/1/0/all/0/1&quot;&gt;Dan Husmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Husmann_K/0/1/0/all/0/1&quot;&gt;Kai Husmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilmberger_J/0/1/0/all/0/1&quot;&gt;Joscha Ilmberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartel_A/0/1/0/all/0/1&quot;&gt;Andreas Hartel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karasenko_V/0/1/0/all/0/1&quot;&gt;Vitali Karasenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grubl_A/0/1/0/all/0/1&quot;&gt;Andreas Gr&amp;#xfc;bl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schemmel_J/0/1/0/all/0/1&quot;&gt;Johannes Schemmel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_K/0/1/0/all/0/1&quot;&gt;Karlheinz Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrovici_M/0/1/0/all/0/1&quot;&gt;Mihai A. Petrovici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03858">
<title>Algorithmic Framework for Model-based Reinforcement Learning with Theoretical Guarantees. (arXiv:1807.03858v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03858</link>
<description rdf:parseType="Literal">&lt;p&gt;While model-based reinforcement learning has empirically been shown to
significantly reduce the sample complexity that hinders model-free RL, the
theoretical understanding of such methods has been rather limited. In this
paper, we introduce a novel algorithmic framework for designing and analyzing
model-based RL algorithms with theoretical guarantees, and a practical
algorithm Optimistic Lower Bounds Optimization (OLBO). In particular, we derive
a theoretical guarantee of monotone improvement for model-based RL with our
framework. We iteratively build a lower bound of the expected reward based on
the estimated dynamical model and sample trajectories, and maximize it jointly
over the policy and the model. Assuming the optimization in each iteration
succeeds, the expected reward is guaranteed to improve. The framework also
incorporates an optimism-driven perspective, and reveals the intrinsic measure
for the model prediction error. Preliminary simulations demonstrate that our
approach outperforms the standard baselines on continuous control benchmark
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huazhe Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuandong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengyu Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03909">
<title>Emotion Recognition from Speech based on Relevant Feature and Majority Voting. (arXiv:1807.03909v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1807.03909</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes an approach to detect emotion from human speech employing
majority voting technique over several machine learning techniques. The
contribution of this work is in two folds: firstly it selects those features of
speech which is most promising for classification and secondly it uses the
majority voting technique that selects the exact class of emotion. Here,
majority voting technique has been applied over Neural Network (NN), Decision
Tree (DT), Support Vector Machine (SVM) and K-Nearest Neighbor (KNN). Input
vector of NN, DT, SVM and KNN consists of various acoustic and prosodic
features like Pitch, Mel-Frequency Cepstral coefficients etc. From speech
signal many feature have been extracted and only promising features have been
selected. To consider a feature as promising, Fast Correlation based feature
selection (FCBF) and Fisher score algorithms have been used and only those
features are selected which are highly ranked by both of them. The proposed
approach has been tested on Berlin dataset of emotional speech [3] and
Electromagnetic Articulography (EMA) dataset [4]. The experimental result shows
that majority voting technique attains better accuracy over individual machine
learning techniques. The employment of the proposed approach can effectively
recognize the emotion of human beings in case of social robot, intelligent chat
client, call-center of a company etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1&quot;&gt;Md. Kamruzzaman Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_K/0/1/0/all/0/1&quot;&gt;Kazi Md. Rokibul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arifuzzaman_M/0/1/0/all/0/1&quot;&gt;Md. Arifuzzaman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03975">
<title>Testing Global Constraints. (arXiv:1807.03975v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.03975</link>
<description rdf:parseType="Literal">&lt;p&gt;Every Constraint Programming (CP) solver exposes a library of constraints for
solving combinatorial problems. In order to be useful, CP solvers need to be
bug-free. Therefore the testing of the solver is crucial to make developers and
users confident. We present a Java library allowing any JVM based solver to
test that the implementations of the individual constraints are correct. The
library can be used in a test suite executed in a continuous integration tool
or it can also be used to discover minimalist instances violating some
properties (arc-consistency, etc) in order to help the developer to identify
the origin of the problem using standard debuggers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Massart_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lie Massart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rombouts_V/0/1/0/all/0/1&quot;&gt;Valentin Rombouts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaus_P/0/1/0/all/0/1&quot;&gt;Pierre Schaus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04001">
<title>Learning Neural Models for End-to-End Clustering. (arXiv:1807.04001v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04001</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel end-to-end neural network architecture that, once trained,
directly outputs a probabilistic clustering of a batch of input examples in one
pass. It estimates a distribution over the number of clusters $k$, and for each
$1 \leq k \leq k_\mathrm{max}$, a distribution over the individual cluster
assignment for each data point. The network is trained in advance in a
supervised fashion on separate data to learn grouping by any perceptual
similarity criterion based on pairwise labels (same/different group). It can
then be applied to different data containing different groups. We demonstrate
promising performance on high-dimensional data like images (COIL-100) and
speech (TIMIT). We call this ``learning to cluster&apos;&apos; and show its conceptual
difference to deep metric learning, semi-supervise clustering and other related
approaches while having the advantage of performing learnable clustering fully
end-to-end.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_B/0/1/0/all/0/1&quot;&gt;Benjamin Bruno Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1&quot;&gt;Ismail Elezi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amirian_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Amirian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durr_O/0/1/0/all/0/1&quot;&gt;Oliver Durr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stadelmann_T/0/1/0/all/0/1&quot;&gt;Thilo Stadelmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04170">
<title>Decision method choice in a human posture recognition context. (arXiv:1807.04170v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.04170</link>
<description rdf:parseType="Literal">&lt;p&gt;Human posture recognition provides a dynamic field that has produced many
methods. Using fuzzy subsets based data fusion methods to aggregate the results
given by different types of recognition processes is a convenient way to
improve recognition methods. Nevertheless, choosing a defuzzification method to
imple-ment the decision is a crucial point of this approach. The goal of this
paper is to present an approach where the choice of the defuzzification method
is driven by the constraints of the final data user, which are expressed as
limitations on indica-tors like confidence or accuracy. A practical
experimentation illustrating this ap-proach is presented: from a depth camera
sensor, human posture is interpreted and the defuzzification method is selected
in accordance with the constraints of the final information consumer. The paper
illustrates the interest of the approach in a context of postures based human
robot communication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perrin_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Perrin&lt;/a&gt; (LISTIC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benoit_E/0/1/0/all/0/1&quot;&gt;Eric Benoit&lt;/a&gt; (LISTIC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coquin_D/0/1/0/all/0/1&quot;&gt;Didier Coquin&lt;/a&gt; (LISTIC)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.02000">
<title>AM-GAN: Improved Usage of Class-Labels in Generative Adversarial Nets. (arXiv:1703.02000v8 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.02000</link>
<description rdf:parseType="Literal">&lt;p&gt;Class labels have been empirically shown useful in improving the sample
quality of generative adversarial nets (GANs). In this paper, we mathematically
study the properties of the current variants of GANs that make use of class
label information. With class aware gradient and cross-entropy decomposition,
we reveal how class labels and associated losses influence GAN&apos;s training.
Based on that, we propose Activation Maximization Generative Adversarial
Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been
conducted to validate our analysis and evaluate the effectiveness of our
solution, where AM-GAN outperforms other strong baselines and achieves
state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we
demonstrate that, with the Inception ImageNet classifier, Inception Score
mainly tracks the diversity of the generator, and there is, however, no
reliable evidence that it can reflect the true sample quality. We thus propose
a new metric, called AM Score, to provide a more accurate estimation of the
sample quality. Our proposed model also outperforms the baseline methods in the
new metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhiming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Han Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_S/0/1/0/all/0/1&quot;&gt;Shu Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08829">
<title>Symbol Emergence in Cognitive Developmental Systems: a Survey. (arXiv:1801.08829v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.08829</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans use signs, e.g., sentences in a spoken language, for communication and
thought. Hence, symbol systems like language are crucial for our communication
with other agents and adaptation to our real-world environment. The symbol
systems we use in our human society adaptively and dynamically change over
time. In the context of artificial intelligence (AI) and cognitive systems, the
symbol grounding problem has been regarded as one of the central problems
related to {\it symbols}. However, the symbol grounding problem was originally
posed to connect symbolic AI and sensorimotor information and did not consider
many interdisciplinary phenomena in human communication and dynamic symbol
systems in our society, which semiotics considered. In this paper, we focus on
the symbol emergence problem, addressing not only cognitive dynamics but also
the dynamics of symbol systems in society, rather than the symbol grounding
problem. We first introduce the notion of a symbol in semiotics from the
humanities, to leave the very narrow idea of symbols in symbolic AI.
Furthermore, over the years, it became more and more clear that symbol
emergence has to be regarded as a multifaceted problem. Therefore, secondly, we
review the history of the symbol emergence problem in different fields,
including both biological and artificial systems, showing their mutual
relations. We summarize the discussion and provide an integrative viewpoint and
comprehensive overview of symbol emergence in cognitive systems. Additionally,
we describe the challenges facing the creation of cognitive systems that can be
part of symbol emergence systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1&quot;&gt;Tadahiro Taniguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ugur_E/0/1/0/all/0/1&quot;&gt;Emre Ugur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffmann_M/0/1/0/all/0/1&quot;&gt;Matej Hoffmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamone_L/0/1/0/all/0/1&quot;&gt;Lorenzo Jamone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagai_T/0/1/0/all/0/1&quot;&gt;Takayuki Nagai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosman_B/0/1/0/all/0/1&quot;&gt;Benjamin Rosman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsuka_T/0/1/0/all/0/1&quot;&gt;Toshihiko Matsuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwahashi_N/0/1/0/all/0/1&quot;&gt;Naoto Iwahashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oztop_E/0/1/0/all/0/1&quot;&gt;Erhan Oztop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piater_J/0/1/0/all/0/1&quot;&gt;Justus Piater&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worgotter_F/0/1/0/all/0/1&quot;&gt;Florentin W&amp;#xf6;rg&amp;#xf6;tter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.11088">
<title>Deep Reinforcement Learning in Ice Hockey for Context-Aware Player Evaluation. (arXiv:1805.11088v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.11088</link>
<description rdf:parseType="Literal">&lt;p&gt;A variety of machine learning models have been proposed to assess the
performance of players in professional sports. However, they have only a
limited ability to model how player performance depends on the game context.
This paper proposes a new approach to capturing game context: we apply Deep
Reinforcement Learning (DRL) to learn an action-value Q function from 3M
play-by-play events in the National Hockey League (NHL). The neural network
representation integrates both continuous context signals and game history,
using a possession-based LSTM. The learned Q-function is used to value players&apos;
actions under different game contexts. To assess a player&apos;s overall
performance, we introduce a novel Game Impact Metric (GIM) that aggregates the
values of the player&apos;s actions. Empirical Evaluation shows GIM is consistent
throughout a play season, and correlates highly with standard success measures
and future salary.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guiliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulte_O/0/1/0/all/0/1&quot;&gt;Oliver Schulte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.02303">
<title>A survey on policy search algorithms for learning robot controllers in a handful of trials. (arXiv:1807.02303v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1807.02303</link>
<description rdf:parseType="Literal">&lt;p&gt;Most policy search algorithms require thousands of training episodes to find
an effective policy, which is often infeasible with a physical robot. This
survey article focuses on the extreme other end of the spectrum: how can a
robot adapt with only a handful of trials (a dozen) and a few minutes? By
analogy with the word &quot;big-data&quot;, we refer to this challenge as &quot;micro-data
reinforcement learning&quot;. We show that a first strategy is to leverage prior
knowledge on the policy structure (e.g., dynamic movement primitives), on the
policy parameters (e.g., demonstrations), or on the dynamics (e.g.,
simulators). A second strategy is to create data-driven surrogate models of the
expected reward (e.g., Bayesian optimization) or the dynamical model (e.g.,
model-based policy search), so that the policy optimizer queries the model
instead of the real system. Overall, all successful micro-data algorithms
combine these two strategies by varying the kind of model and prior knowledge.
The current scientific challenges essentially revolve around scaling up to
complex robots (e.g., humanoids), designing generic priors, and optimizing the
computing time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatzilygeroudis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Chatzilygeroudis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vassiliades_V/0/1/0/all/0/1&quot;&gt;Vassilis Vassiliades&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stulp_F/0/1/0/all/0/1&quot;&gt;Freek Stulp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calinon_S/0/1/0/all/0/1&quot;&gt;Sylvain Calinon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Mouret&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03845">
<title>Model-based free-breathing cardiac MRI reconstruction using deep learned \&amp; STORM priors: MoDL-STORM. (arXiv:1807.03845v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03845</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a model-based reconstruction framework with deep learned (DL)
and smoothness regularization on manifolds (STORM) priors to recover free
breathing and ungated (FBU) cardiac MRI from highly undersampled measurements.
The DL priors enable us to exploit the local correlations, while the STORM
prior enables us to make use of the extensive non-local similarities that are
subject dependent. We introduce a novel model-based formulation that allows the
seamless integration of deep learning methods with available prior information,
which current deep learning algorithms are not capable of. The experimental
results demonstrate the preliminary potential of this work in accelerating FBU
cardiac MRI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1&quot;&gt;Sampurna Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_H/0/1/0/all/0/1&quot;&gt;Hemant K. Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1&quot;&gt;Sunrita Poddar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacob_M/0/1/0/all/0/1&quot;&gt;Mathews Jacob&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03873">
<title>Automatic Gradient Boosting. (arXiv:1807.03873v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03873</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic machine learning performs predictive modeling with high performing
machine learning tools without human interference. This is achieved by making
machine learning applications parameter-free, i.e. only a dataset is provided
while the complete model selection and model building process is handled
internally through (often meta) optimization. Projects like Auto-WEKA and
auto-sklearn aim to solve the Combined Algorithm Selection and Hyperparameter
optimization (CASH) problem resulting in huge configuration spaces. However,
for most real-world applications, the optimization over only a few different
key learning algorithms can not only be sufficient, but also potentially
beneficial. The latter becomes apparent when one considers that models have to
be validated, explained, deployed and maintained. Here, less complex model are
often preferred, for validation or efficiency reasons, or even a strict
requirement. Automatic gradient boosting simplifies this idea one step further,
using only gradient boosting as a single learning algorithm in combination with
model-based hyperparameter tuning, threshold optimization and encoding of
categorical features. We introduce this general framework as well as a concrete
implementation called autoxgboost. It is compared to current AutoML projects on
16 datasets and despite its simplicity is able to achieve comparable results on
about half of the datasets as well as performing best on two.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thomas_J/0/1/0/all/0/1&quot;&gt;Janek Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Coors_S/0/1/0/all/0/1&quot;&gt;Stefan Coors&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03876">
<title>Using deep learning for comprehensive, personalized forecasting of Alzheimer&apos;s Disease progression. (arXiv:1807.03876v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03876</link>
<description rdf:parseType="Literal">&lt;p&gt;A patient is more than one number, yet most approaches to machine learning
from electronic health data can only predict a single endpoint. Here, we
present an alternative -- using unsupervised deep learning to simulate detailed
patient trajectories. We use data comprising 18-month longitudinal trajectories
of 42 clinical variables from 1908 patients with Mild Cognitive Impairment
(MCI) or Alzheimer&apos;s Disease (AD) to train a model for personalized forecasting
of disease progression. Our model simulates the evolution of each sub-component
of cognitive exams, laboratory tests, and their associations with baseline
clinical characteristics, generating both predictions and their confidence
intervals. Even though it is not trained to predict changes in disease
severity, our unsupervised model predicts changes in total ADAS-Cog scores with
the same accuracy as specifically trained supervised models. We show how
simulations can be used to interpret our model and demonstrate how to create
synthetic control arm data for AD clinical trials. Our model&apos;s ability to
simultaneously predict dozens of characteristics of a patient at any point in
the future is a crucial step forward in computational precision medicine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fisher_C/0/1/0/all/0/1&quot;&gt;Charles K. Fisher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1&quot;&gt;Aaron M. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walsh_J/0/1/0/all/0/1&quot;&gt;Jonathan R. Walsh&lt;/a&gt;, the &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diseases_C/0/1/0/all/0/1&quot;&gt;Coalition Against Major Diseases&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03877">
<title>Deep Structured Generative Models. (arXiv:1807.03877v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03877</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models have shown promising results in generating realistic
images, but it is still non-trivial to generate images with complicated
structures. The main reason is that most of the current generative models fail
to explore the structures in the images including spatial layout and semantic
relations between objects. To address this issue, we propose a novel deep
structured generative model which boosts generative adversarial networks (GANs)
with the aid of structure information. In particular, the layout or structure
of the scene is encoded by a stochastic and-or graph (sAOG), in which the
terminal nodes represent single objects and edges represent relations between
objects. With the sAOG appropriately harnessed, our model can successfully
capture the intrinsic structure in the scenes and generate images of
complicated scenes accordingly. Furthermore, a detection network is introduced
to infer scene structures from a image. Experimental results demonstrate the
effectiveness of our proposed method on both modeling the intrinsic structures,
and generating realistic images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Haoyu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03878">
<title>DeepDiff: Deep-learning for predicting Differential gene expression from histone modifications. (arXiv:1807.03878v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03878</link>
<description rdf:parseType="Literal">&lt;p&gt;Computational methods that predict differential gene expression from histone
modification signals are highly desirable for understanding how histone
modifications control the functional heterogeneity of cells through influencing
differential gene regulation. Recent studies either failed to capture
combinatorial effects on differential prediction or primarily only focused on
cell type-specific analysis. In this paper, we develop a novel attention-based
deep learning architecture, DeepDiff, that provides a unified and end-to-end
solution to model and to interpret how dependencies among histone modifications
control the differential patterns of gene regulation. DeepDiff uses a hierarchy
of multiple Long short-term memory (LSTM) modules to encode the spatial
structure of input signals and to model how various histone modifications
cooperate automatically. We introduce and train two levels of attention jointly
with the target prediction, enabling DeepDiff to attend differentially to
relevant modifications and to locate important genome positions for each
modification. Additionally, DeepDiff introduces a novel deep-learning based
multi-task formulation to use the cell-type-specific gene expression
predictions as auxiliary tasks, encouraging richer feature embeddings in our
primary task of differential expression prediction. Using data from Roadmap
Epigenomics Project (REMC) for ten different pairs of cell types, we show that
DeepDiff significantly outperforms the state-of-the-art baselines for
differential gene expression prediction. The learned attention weights are
validated by observations from previous studies about how epigenetic mechanisms
connect to differential gene expression. Codes and results are available at
\url{deepchrome.org}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1&quot;&gt;Arshdeep Sekhon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Ritambhara Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yanjun Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03907">
<title>The Limit Points of (Optimistic) Gradient Descent in Min-Max Optimization. (arXiv:1807.03907v1 [math.OC])</title>
<link>http://arxiv.org/abs/1807.03907</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by applications in Optimization, Game Theory, and the training of
Generative Adversarial Networks, the convergence properties of first order
methods in min-max problems have received extensive study. It has been
recognized that they may cycle, and there is no good understanding of their
limit points when they do not. When they converge, do they converge to local
min-max solutions? We characterize the limit points of two basic first order
methods, namely Gradient Descent/Ascent (GDA) and Optimistic Gradient Descent
Ascent (OGDA). We show that both dynamics avoid unstable critical points for
almost all initializations. Moreover, for small step sizes and under mild
assumptions, the set of \{OGDA\}-stable critical points is a superset of
\{GDA\}-stable critical points, which is a superset of local min-max solutions
(strict in some cases). The connecting thread is that the behavior of these
dynamics can be studied from a dynamical systems perspective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Daskalakis_C/0/1/0/all/0/1&quot;&gt;Constantinos Daskalakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Panageas_I/0/1/0/all/0/1&quot;&gt;Ioannis Panageas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03929">
<title>Quantification under prior probability shift: the ratio estimator and its extensions. (arXiv:1807.03929v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.03929</link>
<description rdf:parseType="Literal">&lt;p&gt;The quantification problem consists of determining the prevalence of a given
label in a target population. However, one often has access to the labels in a
sample from the training population but not in the target population. A common
assumption in this situation is that of prior probability shift, that is, once
the labels are known, the distribution of the features is the same in the
training and target populations. In this paper, we derive a new lower bound for
the risk of the quantification problem under the prior shift assumption.
Complementing this lower bound, we present a new approximately minimax class of
estimators, ratio estimators, which generalize several previous proposals in
the literature. Using a weaker version of the prior shift assumption, which can
be tested, we show that ratio estimators can be used to build confidence
intervals for the quantification problem. We also extend the ratio estimator so
that it can: (i) incorporate labels from the target population, when they are
available and (ii) estimate how the prevalence of positive labels varies
according to a function of certain covariates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vaz_A/0/1/0/all/0/1&quot;&gt;Afonso Fernandes Vaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1&quot;&gt;Rafael Izbicki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stern_R/0/1/0/all/0/1&quot;&gt;Rafael Bassi Stern&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03931">
<title>A Hierarchical Bayesian Linear Regression Model with Local Features for Stochastic Dynamics Approximation. (arXiv:1807.03931v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03931</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the challenges in model-based control of stochastic dynamical systems
is that the state transition dynamics are involved, and it is not easy or
efficient to make good-quality predictions of the states. Moreover, there are
not many representational models for the majority of autonomous systems, as it
is not easy to build a compact model that captures the entire dynamical
subtleties and uncertainties. In this work, we present a hierarchical Bayesian
linear regression model with local features to learn the dynamics of a
micro-robotic system as well as two simpler examples, consisting of a
stochastic mass-spring damper and a stochastic double inverted pendulum on a
cart. The model is hierarchical since we assume non-stationary priors for the
model parameters. These non-stationary priors make the model more flexible by
imposing priors on the priors of the model. To solve the maximum likelihood
(ML) problem for this hierarchical model, we use the variational expectation
maximization (EM) algorithm, and enhance the procedure by introducing hidden
target variables. The algorithm yields parsimonious model structures, and
consistently provides fast and accurate predictions for all our examples
involving large training and test sets. This demonstrates the effectiveness of
the method in learning stochastic dynamics, which makes it suitable for future
use in a paradigm, such as model-based reinforcement learning, to compute
optimal control policies in real time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parsa_B/0/1/0/all/0/1&quot;&gt;Behnoosh Parsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajasekaran_K/0/1/0/all/0/1&quot;&gt;Keshav Rajasekaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_F/0/1/0/all/0/1&quot;&gt;Franziska Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Ashis G. Banerjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03933">
<title>Instance-based entropy fuzzy support vector machine for imbalanced data. (arXiv:1807.03933v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.03933</link>
<description rdf:parseType="Literal">&lt;p&gt;Imbalanced classification has been a major challenge for machine learning
because many standard classifiers mainly focus on balanced datasets and tend to
have biased results towards the majority class. We modify entropy fuzzy support
vector machine (EFSVM) and introduce instance-based entropy fuzzy support
vector machine (IEFSVM). Both EFSVM and IEFSVM use the entropy information of
k-nearest neighbors to determine the fuzzy membership value for each sample
which prioritizes the importance of each sample. IEFSVM considers the diversity
of entropy patterns for each sample when increasing the size of neighbors, k,
while EFSVM uses single entropy information of the fixed size of neighbors for
all samples. By varying k, we can reflect the component change of sample&apos;s
neighbors from near to far distance in the determination of fuzzy value
membership. Numerical experiments on 35 public and 12 real-world imbalanced
datasets are performed to validate IEFSVM and area under the receiver operating
characteristic curve (AUC) is used to compare its performance with other SVMs
and machine learning methods. IEFSVM shows a much higher AUC value for datasets
with high imbalance ratio, implying that IEFSVM is effective in dealing with
the class imbalance problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_P/0/1/0/all/0/1&quot;&gt;Poongjin Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minhyuk Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1&quot;&gt;Woojin Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04010">
<title>Causal discovery in the presence of missing data. (arXiv:1807.04010v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04010</link>
<description rdf:parseType="Literal">&lt;p&gt;Missing data are ubiquitous in many domains such as healthcare. Depending on
how they are missing, the (conditional) independence relations in the observed
data may be different from those for the complete data generated by the
underlying causal process and, as a consequence, simply applying existing
causal discovery methods to the observed data may lead to wrong conclusions. It
is then essential to extend existing causal discovery approaches to find true
underlying causal structure from such incomplete data. In this paper, we aim at
solving this problem for data that are missing with different mechanisms,
including missing completely at random (MCAR), missing at random (MAR), and
missing not at random (MNAR). With missingness mechanisms represented by
missingness Graph (m-Graph), we analyze conditions under which addition
correction is needed to derive conditional independence/dependence relations in
the complete data. Based on our analysis, we propose missing value PC (MVPC),
which combines additional corrections with traditional causal discovery
algorithm, in particular, PC. Our proposed MVPC is shown in theory to give
asymptotically correct results even using data that are MAR and MNAR.
Experiment results illustrate that the proposed algorithm can correct the
conditional independence for values MCAR, MAR and rather general cases of
values MNAR both with synthetic data as well as real-life healthcare
application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1&quot;&gt;Ruibo Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ackermann_P/0/1/0/all/0/1&quot;&gt;Paul Ackermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1&quot;&gt;Hedvig Kjellstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04020">
<title>Improved SVD-based Initialization for Nonnegative Matrix Factorization using Low-Rank Correction. (arXiv:1807.04020v1 [cs.NA])</title>
<link>http://arxiv.org/abs/1807.04020</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the iterative nature of most nonnegative matrix factorization
(\textsc{NMF}) algorithms, initialization is a key aspect as it significantly
influences both the convergence and the final solution obtained. Many
initialization schemes have been proposed for NMF, among which one of the most
popular class of methods are based on the singular value decomposition (SVD).
However, these SVD-based initializations do not satisfy a rather natural
condition, namely that the error should decrease as the rank of factorization
increases. In this paper, we propose a novel SVD-based \textsc{NMF}
initialization to specifically address this shortcoming by taking into account
the SVD factors that were discarded to obtain a nonnegative initialization.
This method, referred to as nonnegative SVD with low-rank correction
(NNSVD-LRC), allows us to significantly reduce the initial error at a
negligible additional computational cost using the low-rank structure of the
discarded SVD factors. NNSVD-LRC has two other advantages compared to previous
SVD-based initializations: (1) it provably generates sparse initial factors,
and (2) it is faster as it only requires to compute a truncated SVD of rank
$\lceil r/2 + 1 \rceil$ where $r$ is the factorization rank of the sought NMF
decomposition (as opposed to a rank-$r$ truncated SVD for other methods). We
show on several standard dense and sparse data sets that our new method
competes favorably with state-of-the-art SVD-based initializations for NMF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syed_A/0/1/0/all/0/1&quot;&gt;Atif Muhammad Syed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qazi_S/0/1/0/all/0/1&quot;&gt;Sameer Qazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gillis_N/0/1/0/all/0/1&quot;&gt;Nicolas Gillis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04056">
<title>Temporal Convolution Networks for Real-Time Abdominal Fetal Aorta Analysis with Ultrasound. (arXiv:1807.04056v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.04056</link>
<description rdf:parseType="Literal">&lt;p&gt;The automatic analysis of ultrasound sequences can substantially improve the
efficiency of clinical diagnosis. In this work we present our attempt to
automate the challenging task of measuring the vascular diameter of the fetal
abdominal aorta from ultrasound images. We propose a neural network
architecture consisting of three blocks: a convolutional layer for the
extraction of imaging features, a Convolution Gated Recurrent Unit (C-GRU) for
enforcing the temporal coherence across video frames and exploiting the
temporal redundancy of a signal, and a regularized loss function, called
\textit{CyclicLoss}, to impose our prior knowledge about the periodicity of the
observed signal. We present experimental evidence suggesting that the proposed
architecture can reach an accuracy substantially superior to previously
proposed methods, providing an average reduction of the mean squared error from
$0.31 mm^2$ (state-of-art) to $0.09 mm^2$, and a relative error reduction from
$8.1\%$ to $5.3\%$. The mean execution speed of the proposed approach of 289
frames per second makes it suitable for real time clinical use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savioli_N/0/1/0/all/0/1&quot;&gt;Nicolo&amp;#x27; Savioli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Visentin_S/0/1/0/all/0/1&quot;&gt;Silvia Visentin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosmi_E/0/1/0/all/0/1&quot;&gt;Erich Cosmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grisan_E/0/1/0/all/0/1&quot;&gt;Enrico Grisan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamata_P/0/1/0/all/0/1&quot;&gt;Pablo Lamata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montana_G/0/1/0/all/0/1&quot;&gt;Giovanni Montana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04073">
<title>A punishment voting algorithm based on super categories construction for acoustic scene classification. (arXiv:1807.04073v1 [cs.SD])</title>
<link>http://arxiv.org/abs/1807.04073</link>
<description rdf:parseType="Literal">&lt;p&gt;In acoustic scene classification researches, audio segment is usually split
into multiple samples. Majority voting is then utilized to ensemble the results
of the samples. In this paper, we propose a punishment voting algorithm based
on the super categories construction method for acoustic scene classification.
Specifically, we propose a DenseNet-like model as the base classifier. The base
classifier is trained by the CQT spectrograms generated from the raw audio
segments. Taking advantage of the results of the base classifier, we propose a
super categories construction method using the spectral clustering. Super
classifiers corresponding to the constructed super categories are further
trained. Finally, the super classifiers are utilized to enhance the majority
voting of the base classifier by punishment voting. Experiments show that the
punishment voting obviously improves the performances on both the DCASE2017
Development dataset and the LITIS Rouen dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Weiping Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_Z/0/1/0/all/0/1&quot;&gt;Zhenyao Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jiantao Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04106">
<title>VFunc: a Deep Generative Model for Functions. (arXiv:1807.04106v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04106</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a deep generative model for functions. Our model provides a
joint distribution p(f, z) over functions f and latent variables z which lets
us efficiently sample from the marginal p(f) and maximize a variational lower
bound on the entropy H(f). We can thus maximize objectives of the form
E_{f~p(f)}[R(f)] + c*H(f), where R(f) denotes, e.g., a data log-likelihood term
or an expected reward. Such objectives encompass Bayesian deep learning in
function space, rather than parameter space, and Bayesian deep RL with
representations of uncertainty that offer benefits over bootstrapping and
parameter noise. In this short paper we describe our model, situate it in the
context of prior work, and present proof-of-concept experiments for regression
and RL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachman_P/0/1/0/all/0/1&quot;&gt;Philip Bachman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1&quot;&gt;Riashat Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1&quot;&gt;Alessandro Sordoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1&quot;&gt;Zafarali Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04119">
<title>Exploiting statistical dependencies of time series with hierarchical correlation reconstruction. (arXiv:1807.04119v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04119</link>
<description rdf:parseType="Literal">&lt;p&gt;While we are usually focused on predicting future values of time series, it
is often valuable to additionally predict their entire probability
distributions, for example to evaluate risk or Monte Carlo simulations. On
example of time series of $\approx$ 30000 Dow Jones Industrial Averages, there
will be shown application of hierarchical correlation reconstruction for this
purpose: mean-square fitting polynomial as joint density for (current value,
context), where context is for example a few previous values. Then substituting
the currently observed context and normalizing density to 1, we get predicted
probability distribution for the current value. In contrast to standard machine
learning approaches like neural networks, optimal coefficients here can be
inexpensively directly calculated, are unique and independent, each has a
specific cumulant-like interpretation, and such approximation can approach
complete description of any joint distribution - providing a perfect tool to
quantitatively describe and exploit statistical dependencies in time series.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duda_J/0/1/0/all/0/1&quot;&gt;Jarek Duda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04183">
<title>Optimization over Continuous and Multi-dimensional Decisions with Observational Data. (arXiv:1807.04183v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.04183</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the optimization of an uncertain objective over continuous and
multi-dimensional decision spaces in problems in which we are only provided
with observational data. We propose a novel algorithmic framework that is
tractable, asymptotically consistent, and superior to comparable methods on
example problems. Our approach leverages predictive machine learning methods
and incorporates information on the uncertainty of the predicted outcomes for
the purpose of prescribing decisions. We demonstrate the efficacy of our method
on examples involving both synthetic and real data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bertsimas_D/0/1/0/all/0/1&quot;&gt;Dimitris Bertsimas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McCord_C/0/1/0/all/0/1&quot;&gt;Christopher McCord&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04188">
<title>VTA: An Open Hardware-Software Stack for Deep Learning. (arXiv:1807.04188v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04188</link>
<description rdf:parseType="Literal">&lt;p&gt;Hardware acceleration is an enabler for ubiquitous and efficient deep
learning. With hardware accelerators being introduced in datacenter and edge
devices, it is time to acknowledge that hardware specialization is central to
the deep learning system stack.
&lt;/p&gt;
&lt;p&gt;This technical report presents the Versatile Tensor Accelerator (VTA), an
open, generic, and customizable deep learning accelerator design. VTA is a
programmable accelerator that exposes a RISC-like programming abstraction to
describe operations at the tensor level. We designed VTA to expose the most
salient and common characteristics of mainstream deep learning accelerators,
such as tensor operations, DMA load/stores, and explicit compute/memory
arbitration.
&lt;/p&gt;
&lt;p&gt;VTA is more than a standalone accelerator design: it&apos;s an end-to-end solution
that includes drivers, a JIT runtime, and an optimizing compiler stack based on
TVM. The current release of VTA includes a behavioral hardware simulator, as
well as the infrastructure to deploy VTA on low-cost FPGA development boards
for fast prototyping.
&lt;/p&gt;
&lt;p&gt;By extending the TVM stack with a customizable, and open source deep learning
hardware accelerator design, we are exposing a transparent end-to-end deep
learning stack from the high-level deep learning framework, down to the actual
hardware design and implementation. This forms a truly end-to-end, from
software-to-hardware open source stack for deep learning systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1&quot;&gt;Thierry Moreau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Ziheng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceze_L/0/1/0/all/0/1&quot;&gt;Luis Ceze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1&quot;&gt;Carlos Guestrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1&quot;&gt;Arvind Krishnamurthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04193">
<title>Distributed Variational Representation Learning. (arXiv:1807.04193v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.04193</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of distributed representation learning is one in which multiple
sources of information $X_1,\ldots,X_K$ are processed separately so as to learn
as much information as possible about some ground truth $Y$. We investigate
this problem from information-theoretic grounds, through a generalization of
Tishby&apos;s centralized Information Bottleneck (IB) method to the distributed
setting. Specifically, $K$ encoders, $K \geq 2$, compress their observations
$X_1,\ldots,X_K$ separately in a manner such that, collectively, the produced
representations preserve as much information as possible about $Y$. We study
both discrete memoryless (DM) and memoryless vector Gaussian data models. For
the discrete model, we establish a single-letter characterization of the
optimal tradeoff between complexity (or rate) and relevance (or information)
for a class of memoryless sources (the observations $X_1,\ldots,X_K$ being
conditionally independent given $Y$). For the vector Gaussian model, we provide
an explicit characterization of the optimal complexity-relevance tradeoff.
Furthermore, we develop a variational bound on the complexity-relevance
tradeoff which generalizes the evidence lower bound (ELBO) to the distributed
setting. We also provide two algorithms that allow to compute this bound: i) a
Blahut-Arimoto type iterative algorithm which enables to compute optimal
complexity-relevance encoding mappings by iterating over a set of
self-consistent equations, and ii) a variational inference type algorithm in
which the encoding mappings are parametrized by neural networks and the bound
approximated by Markov sampling and optimized with stochastic gradient descent.
Numerical results on synthetic and real datasets are provided to support the
efficiency of the approaches and algorithms developed in this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aguerri_I/0/1/0/all/0/1&quot;&gt;Inaki Estella Aguerri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zaidi_A/0/1/0/all/0/1&quot;&gt;Abdellatif Zaidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04222">
<title>Modified Regularized Dual Averaging Method for Training Sparse Convolutional Neural Networks. (arXiv:1807.04222v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04222</link>
<description rdf:parseType="Literal">&lt;p&gt;We proposed a modified regularized dual averaging method for training sparse
deep convolutional neural networks. The regularized dual averaging method has
been proven to be effective in obtaining sparse solutions in convex
optimization problems, but not applied to deep learning fields before. We
analyzed the new version in convex conditions and prove the convergence of it.
The modified method can obtain more sparse solutions than traditional sparse
optimization methods such as proximal-SGD, while keeping almost the same
accuracy as stochastic gradient method with momentum on certain datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaodong Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Juncai He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jinchao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04239">
<title>Morse Code Datasets for Machine Learning. (arXiv:1807.04239v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04239</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an algorithm to generate synthetic datasets of tunable difficulty
on classification of Morse code symbols for supervised machine learning
problems, in particular, neural networks. The datasets are spatially
one-dimensional and have a small number of input features, leading to high
density of input information content. This makes them particularly challenging
when implementing network complexity reduction methods. We explore how network
performance is affected by deliberately adding various forms of noise and
expanding the feature set and dataset size. Finally, we establish several
metrics to indicate the difficulty of a dataset, and evaluate their merits. The
algorithm and datasets are open-source.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1&quot;&gt;Sourya Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chugg_K/0/1/0/all/0/1&quot;&gt;Keith M. Chugg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1&quot;&gt;Peter A. Beerel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04252">
<title>Last-Iterate Convergence: Zero-Sum Games and Constrained Min-Max Optimization. (arXiv:1807.04252v1 [math.OC])</title>
<link>http://arxiv.org/abs/1807.04252</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by applications in Game Theory, Optimization, and Generative
Adversarial Networks, recent work of Daskalakis et al. and Liang and Stokes has
established that a variant of the widely used Gradient Descent/Ascent
procedure, called &quot;Optimistic Gradient Descent/Ascent (OGDA)&quot;, exhibits
last-iterate convergence to saddle points in {\em unconstrained} convex-concave
min-max optimization problems. We show that the same holds true in the more
general problem of {\em constrained} min-max optimization under a variant of
the Multiplicative-Weights-Update method called &quot;Optimistic
Multiplicative-Weights Update (OMWU)&quot;. The generality of the constrained
problem, which in particular captures all Linear Programming, requires
fundamentally different techniques for analyzing the progress of OMWU towards
min-max solutions. We show that OMWU monotonically improves the
Kullback-Leibler divergence of the current iterate to the (appropriately
normalized) min-max solution until it enters a neighborhood of the solution.
Inside that neighborhood we show that OMWU becomes a contracting map converging
to the exact solution. We experiment with zero-sum games to measure how the
convergence rate scales with the dimension.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Daskalakis_C/0/1/0/all/0/1&quot;&gt;Constantinos Daskalakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Panageas_I/0/1/0/all/0/1&quot;&gt;Ioannis Panageas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04270">
<title>Fooling the classifier: Ligand antagonism and adversarial examples. (arXiv:1807.04270v1 [physics.bio-ph])</title>
<link>http://arxiv.org/abs/1807.04270</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning algorithms are sensitive to so-called adversarial
perturbations. This is reminiscent of cellular decision-making where antagonist
ligands may prevent correct signaling, like during the early immune response.
We draw a formal analogy between neural networks used in machine learning and
the general class of adaptive proofreading networks. We then apply simple
adversarial strategies from machine learning to models of ligand
discrimination. We show how kinetic proofreading leads to &quot;boundary tilting&quot;
and identify three types of perturbation (adversarial, non adversarial and
ambiguous). We then use a gradient-descent approach to compare different
adaptive proofreading models, and we reveal the existence of two qualitatively
different regimes characterized by the presence or absence of a critical point.
These regimes are reminiscent of the &quot;feature-to-prototype&quot; transition
identified in machine learning, corresponding to two strategies in ligand
antagonism (broad vs. specialized). Overall, our work connects evolved cellular
decision-making to classification in machine learning, showing that behaviours
close to the decision boundary can be understood through the same mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rademaker_T/0/1/0/all/0/1&quot;&gt;Thomas J. Rademaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bengio_E/0/1/0/all/0/1&quot;&gt;Emmanuel Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Francois_P/0/1/0/all/0/1&quot;&gt;Paul Fran&amp;#xe7;ois&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1501.02102">
<title>Equitability of Dependence Measure. (arXiv:1501.02102v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1501.02102</link>
<description rdf:parseType="Literal">&lt;p&gt;Measuring dependence between two random variables is very important, and
critical in many applied areas such as variable selection, brain network
analysis. However, we do not know what kind of functional relationship is
between two covariates, which requires the dependence measure to be equitable.
That is, it gives similar scores to equally noisy relationship of different
types. In fact, the dependence score is a continuous random variable taking
values in $[0,1]$, thus it is theoretically impossible to give similar scores.
In this paper, we introduce a new definition of equitability of a dependence
measure, i.e, power-equitable (weak-equitable) and show by simulation that HHG
and Copula Dependence Coefficient (CDC) are weak-equitable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hangjin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yiming Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1603.06202">
<title>Extracting Predictive Information from Heterogeneous Data Streams using Gaussian Processes. (arXiv:1603.06202v2 [q-fin.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1603.06202</link>
<description rdf:parseType="Literal">&lt;p&gt;Financial markets are notoriously complex environments, presenting vast
amounts of noisy, yet potentially informative data. We consider the problem of
forecasting financial time series from a wide range of information sources
using online Gaussian Processes with Automatic Relevance Determination (ARD)
kernels. We measure the performance gain, quantified in terms of Normalised
Root Mean Square Error (NRMSE), Median Absolute Deviation (MAD) and Pearson
correlation, from fusing each of four separate data domains: time series
technicals, sentiment analysis, options market data and broker recommendations.
We show evidence that ARD kernels produce meaningful feature rankings that help
retain salient inputs and reduce input dimensionality, providing a framework
for sifting through financial complexity. We measure the performance gain from
fusing each domain&apos;s heterogeneous data streams into a single probabilistic
model. In particular our findings highlight the critical value of options data
in mapping out the curvature of price space and inspire an intuitive, novel
direction for research in financial prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Ghoshal_S/0/1/0/all/0/1&quot;&gt;Sid Ghoshal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Roberts_S/0/1/0/all/0/1&quot;&gt;Stephen Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10388">
<title>Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram Completion. (arXiv:1711.10388v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10388</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed Tomography (CT) reconstruction is a fundamental component to a wide
variety of applications ranging from security, to healthcare. The classical
techniques require measuring projections, called sinograms, from a full
180$^\circ$ view of the object. This is impractical in a limited angle
scenario, when the viewing angle is less than 180$^\circ$, which can occur due
to different factors including restrictions on scanning time, limited
flexibility of scanner rotation, etc. The sinograms obtained as a result, cause
existing techniques to produce highly artifact-laden reconstructions. In this
paper, we propose to address this problem through implicit sinogram completion,
on a challenging real world dataset containing scans of common checked-in
luggage. We propose a system, consisting of 1D and 2D convolutional neural
networks, that operates on a limited angle sinogram to directly produce the
best estimate of a reconstruction. Next, we use the x-ray transform on this
reconstruction to obtain a &quot;completed&quot; sinogram, as if it came from a full
180$^\circ$ measurement. We feed this to standard analytical and iterative
reconstruction techniques to obtain the final reconstruction. We show with
extensive experimentation that this combined strategy outperforms many
competitive baselines. We also propose a measure of confidence for the
reconstruction that enables a practitioner to gauge the reliability of a
prediction made by our network. We show that this measure is a strong indicator
of quality as measured by the PSNR, while not requiring ground truth at test
time. Finally, using a segmentation experiment, we show that our reconstruction
preserves the 3D structure of objects effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1&quot;&gt;Rushil Anirudh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyojin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1&quot;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_K/0/1/0/all/0/1&quot;&gt;K. Aditya Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Champley_K/0/1/0/all/0/1&quot;&gt;Kyle Champley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bremer_T/0/1/0/all/0/1&quot;&gt;Timo Bremer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04374">
<title>Tempered Adversarial Networks. (arXiv:1802.04374v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04374</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks (GANs) have been shown to produce realistic
samples from high-dimensional distributions, but training them is considered
hard. A possible explanation for training instabilities is the inherent
imbalance between the networks: While the discriminator is trained directly on
both real and fake samples, the generator only has control over the fake
samples it produces since the real data distribution is fixed by the choice of
a given dataset. We propose a simple modification that gives the generator
control over the real samples which leads to a tempered learning process for
both generator and discriminator. The real data distribution passes through a
lens before being revealed to the discriminator, balancing the generator and
discriminator by gradually revealing more detailed features necessary to
produce high-quality results. The proposed module automatically adjusts the
learning process to the current strength of the networks, yet is generic and
easy to add to any GAN variant. In a number of experiments, we show that this
can improve quality, stability and/or convergence speed across a range of
different GAN architectures (DCGAN, LSGAN, WGAN-GP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sajjadi_M/0/1/0/all/0/1&quot;&gt;Mehdi S. M. Sajjadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parascandolo_G/0/1/0/all/0/1&quot;&gt;Giambattista Parascandolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mehrjou_A/0/1/0/all/0/1&quot;&gt;Arash Mehrjou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06531">
<title>Topology Estimation using Graphical Models in Multi-Phase Power Distribution Grids. (arXiv:1803.06531v2 [cs.SY] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06531</link>
<description rdf:parseType="Literal">&lt;p&gt;Distribution grid is the medium and low voltage part of a large power system.
Structurally, the majority of distribution networks operate radially, such that
energized lines form a collection of trees, i.e. forest, with a substation
being at the root of any tree. The operational topology/forest may change from
time to time, however tracking these changes, even though important for the
distribution grid operation and control, is hindered by limited real-time
monitoring. This paper develops a learning framework to reconstruct radial
operational structure of the distribution grid from synchronized voltage
measurements in the grid subject to the exogenous fluctuations in nodal power
consumption. To detect operational lines our learning algorithm uses
conditional independence tests for continuous random variables that is
applicable to a wide class of probability distributions of the nodal
consumption and Gaussian injections in particular. Moreover, our algorithm
applies to the practical case of unbalanced three-phase power flow. Algorithm
performance is validated on AC power flow simulations over IEEE distribution
grid test cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deka_D/0/1/0/all/0/1&quot;&gt;Deepjyoti Deka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chertkov_M/0/1/0/all/0/1&quot;&gt;Michael Chertkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Backhaus_S/0/1/0/all/0/1&quot;&gt;Scott Backhaus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01330">
<title>An Imprecise Probabilistic Estimator for the Transition Rate Matrix of a Continuous-Time Markov Chain. (arXiv:1804.01330v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01330</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of estimating the transition rate matrix of a
continuous-time Markov chain from a finite-duration realisation of this
process. We approach this problem in an imprecise probabilistic framework,
using a set of prior distributions on the unknown transition rate matrix. The
resulting estimator is a set of transition rate matrices that, for reasons of
conjugacy, is easy to find. To determine the hyperparameters for our set of
priors, we reconsider the problem in discrete time, where we can use the
well-known Imprecise Dirichlet Model. In particular, we show how the limit of
the resulting discrete-time estimators is a continuous-time estimator. It
corresponds to a specific choice of hyperparameters and has an exceptionally
simple closed-form expression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krak_T/0/1/0/all/0/1&quot;&gt;Thomas Krak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Erreygers_A/0/1/0/all/0/1&quot;&gt;Alexander Erreygers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bock_J/0/1/0/all/0/1&quot;&gt;Jasper De Bock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08598">
<title>Black-box Adversarial Attacks with Limited Queries and Information. (arXiv:1804.08598v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08598</link>
<description rdf:parseType="Literal">&lt;p&gt;Current neural network-based classifiers are susceptible to adversarial
examples even in the black-box setting, where the attacker only has query
access to the model. In practice, the threat model for real-world systems is
often more restrictive than the typical black-box model where the adversary can
observe the full output of the network on arbitrarily many chosen inputs. We
define three realistic threat models that more accurately characterize many
real-world classifiers: the query-limited setting, the partial-information
setting, and the label-only setting. We develop new attacks that fool
classifiers under these more restrictive threat models, where previous methods
would be impractical or ineffective. We demonstrate that our methods are
effective against an ImageNet classifier under our proposed threat models. We
also demonstrate a targeted black-box attack against a commercial classifier,
overcoming the challenges of limited query access, partial information, and
other practical issues to break the Google Cloud Vision API.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilyas_A/0/1/0/all/0/1&quot;&gt;Andrew Ilyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engstrom_L/0/1/0/all/0/1&quot;&gt;Logan Engstrom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athalye_A/0/1/0/all/0/1&quot;&gt;Anish Athalye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jessy Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09046">
<title>Developing a machine learning framework for estimating soil moisture with VNIR hyperspectral data. (arXiv:1804.09046v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.09046</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the potential of estimating the soil-moisture
content based on VNIR hyperspectral data combined with LWIR data. Measurements
from a multi-sensor field campaign represent the benchmark dataset which
contains measured hyperspectral, LWIR, and soil-moisture data conducted on
grassland site. We introduce a regression framework with three steps consisting
of feature selection, preprocessing, and well-chosen regression models. The
latter are mainly supervised machine learning models. An exception are the
self-organizing maps which combine unsupervised and supervised learning. We
analyze the impact of the distinct preprocessing methods on the regression
results. Of all regression models, the extremely randomized trees model without
preprocessing provides the best estimation performance. Our results reveal the
potential of the respective regression framework combined with the VNIR
hyperspectral data to estimate soil moisture measured under real-world
conditions. In conclusion, the results of this paper provide a basis for
further improvements in different research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_S/0/1/0/all/0/1&quot;&gt;Sina Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riese_F/0/1/0/all/0/1&quot;&gt;Felix M. Riese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stotzer_J/0/1/0/all/0/1&quot;&gt;Johanna St&amp;#xf6;tzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_P/0/1/0/all/0/1&quot;&gt;Philipp M. Maier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hinz_S/0/1/0/all/0/1&quot;&gt;Stefan Hinz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06270">
<title>Stable Prediction across Unknown Environments. (arXiv:1806.06270v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.06270</link>
<description rdf:parseType="Literal">&lt;p&gt;In many important machine learning applications, the training distribution
used to learn a probabilistic classifier differs from the testing distribution
on which the classifier will be used to make predictions. Traditional methods
correct the distribution shift by reweighting the training data with the ratio
of the density between test and training data. In many applications training
takes place without prior knowledge of the testing distribution on which the
algorithm will be applied in the future. Recently, methods have been proposed
to address the shift by learning causal structure, but those methods rely on
the diversity of multiple training data to a good performance, and have
complexity limitations in high dimensions. In this paper, we propose a novel
Deep Global Balancing Regression (DGBR) algorithm to jointly optimize a deep
auto-encoder model for feature selection and a global balancing model for
stable prediction across unknown environments. The global balancing model
constructs balancing weights that facilitate estimating of partial effects of
features (holding fixed all other features), a problem that is challenging in
high dimensions, and thus helps to identify stable, causal relationships
between features and outcomes. The deep auto-encoder model is designed to
reduce the dimensionality of the feature space, thus making global balancing
easier. We show, both theoretically and with empirical experiments, that our
algorithm can make stable predictions across unknown environments. Our
experiments on both synthetic and real world datasets demonstrate that our DGBR
algorithm outperforms the state-of-the-art methods for stable prediction across
unknown environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1&quot;&gt;Kun Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1&quot;&gt;Ruoxuan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1&quot;&gt;Peng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1&quot;&gt;Susan Athey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.07004">
<title>Maximally Invariant Data Perturbation as Explanation. (arXiv:1806.07004v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.07004</link>
<description rdf:parseType="Literal">&lt;p&gt;While several feature scoring methods are proposed to explain the output of
complex machine learning models, most of them lack formal mathematical
definitions. In this study, we propose a novel definition of the feature score
using the maximally invariant data perturbation, which is inspired from the
idea of adversarial example. In adversarial example, one seeks the smallest
data perturbation that changes the model&apos;s output. In our proposed approach, we
consider the opposite: we seek the maximally invariant data perturbation that
does not change the model&apos;s output. In this way, we can identify important
input features as the ones with small allowable data perturbations. To find the
maximally invariant data perturbation, we formulate the problem as linear
programming. The experiment on the image classification with VGG16 shows that
the proposed method could identify relevant parts of the images effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hara_S/0/1/0/all/0/1&quot;&gt;Satoshi Hara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ikeno_K/0/1/0/all/0/1&quot;&gt;Kouichi Ikeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soma_T/0/1/0/all/0/1&quot;&gt;Tasuku Soma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maehara_T/0/1/0/all/0/1&quot;&gt;Takanori Maehara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03711">
<title>Geometric Generalization Based Zero-Shot Learning Dataset Infinite World: Simple Yet Powerful. (arXiv:1807.03711v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03711</link>
<description rdf:parseType="Literal">&lt;p&gt;Raven&apos;s Progressive Matrices are one of the widely used tests in evaluating
the human test taker&apos;s fluid intelligence. Analogously, this paper introduces
geometric generalization based zero-shot learning tests to measure the rapid
learning ability and the internal consistency of deep generative models. Our
empirical research analysis on state-of-the-art generative models discern their
ability to generalize concepts across classes. In the process, we introduce
Infinite World, an evaluable, scalable, multi-modal, light-weight dataset and
Zero-Shot Intelligence Metric ZSI. The proposed tests condenses human-level
spatial and numerical reasoning tasks to its simplistic geometric forms. The
dataset is scalable to a theoretical limit of infinity, in numerical features
of the generated geometric figures, image size and in quantity. We
systematically analyze state-of-the-art model&apos;s internal consistency, identify
their bottlenecks and propose a pro-active optimization method for few-shot and
zero-shot learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chidambaram_R/0/1/0/all/0/1&quot;&gt;Rajesh Chidambaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1&quot;&gt;Michael Kampffmeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1&quot;&gt;Willie Neiswanger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lachmann_T/0/1/0/all/0/1&quot;&gt;Thomas Lachmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric Xing&lt;/a&gt;</dc:creator>
</item></rdf:RDF>