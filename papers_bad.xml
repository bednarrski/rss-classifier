<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03209"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03144"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03236"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03239"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03275"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03375"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.03500"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.08071"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11565"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.10459"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03050"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03063"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03065"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03101"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03127"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03133"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03151"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03170"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03203"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03212"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03284"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03300"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03319"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03335"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03337"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.04838"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1609.05057"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.09600"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.07926"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.04126"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.08816"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.06941"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06635"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00393"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.04510"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.00474"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.03209">
<title>Drift Theory in Continuous Search Spaces: Expected Hitting Time of the (1+1)-ES with 1/5 Success Rule. (arXiv:1802.03209v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.03209</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the use of the standard approach for proving runtime
bounds in discrete domains---often referred to as drift analysis---in the
context of optimization on a continuous domain. Using this framework we analyze
the (1+1) Evolution Strategy with one-fifth success rule on the sphere
function. To deal with potential functions that are not lower-bounded, we
formulate novel drift theorems. We then use the theorems to prove bounds on the
expected hitting time to reach a certain target fitness in finite dimension
$d$. The bounds are akin to linear convergence. We then study the dependency of
the different terms on $d$ proving a convergence rate dependency of
$\Theta(1/d)$. Our results constitute the first non-asymptotic analysis for the
algorithm considered as well as the first explicit application of drift
analysis to a randomized search heuristic with continuous domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akimoto_Y/0/1/0/all/0/1&quot;&gt;Youhei Akimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auger_A/0/1/0/all/0/1&quot;&gt;Anne Auger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glasmachers_T/0/1/0/all/0/1&quot;&gt;Tobias Glasmachers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03144">
<title>Neural Dynamic Programming for Musical Self Similarity. (arXiv:1802.03144v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.03144</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a neural sequence model designed specifically for symbolic music.
The model is based on a learned edit distance mechanism which generalises a
classic recursion from computer science, leading to a neural dynamic program.
Repeated motifs are detected by learning the transformations between them. We
represent the arising computational dependencies using a novel data structure,
the edit tree; this perspective suggests natural approximations which afford
the scaling up of our otherwise cubic time algorithm. We demonstrate our model
on real and synthetic data; in all cases it out-performs a strong stacked long
short-term memory benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walder_C/0/1/0/all/0/1&quot;&gt;Christian J. Walder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongwoo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03236">
<title>Learning Robust Options. (arXiv:1802.03236v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.03236</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust reinforcement learning aims to produce policies that have strong
guarantees even in the face of environments/transition models whose parameters
have strong uncertainty. Existing work uses value-based methods and the usual
primitive action setting. In this paper, we propose robust methods for learning
temporally abstract actions, in the framework of options. We present a Robust
Options Policy Iteration (ROPI) algorithm with convergence guarantees, which
learns options that are robust to model uncertainty. We utilize ROPI to learn
robust options with the Robust Options Deep Q Network (RO-DQN) that solves
multiple tasks and mitigates model misspecification due to model uncertainty.
We present experimental results which suggest that policy iteration with linear
features may have an inherent form of robustness when using coarse feature
representations. In addition, we present experimental results which demonstrate
that robustness helps policy iteration implemented on top of deep neural
networks to generalize over a much broader range of dynamics than non-robust
policy iteration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mankowitz_D/0/1/0/all/0/1&quot;&gt;Daniel J. Mankowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mann_T/0/1/0/all/0/1&quot;&gt;Timothy A. Mann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bacon_P/0/1/0/all/0/1&quot;&gt;Pierre-Luc Bacon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1&quot;&gt;Shie Mannor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03239">
<title>Using Discretization for Extending the Set of Predictive Features. (arXiv:1802.03239v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03239</link>
<description rdf:parseType="Literal">&lt;p&gt;To date, attribute discretization is typically performed by replacing the
original set of continuous features with a transposed set of discrete ones.
This paper provides support for a new idea that discretized features should
often be used in addition to existing features and as such, datasets should be
extended, and not replaced, by discretization. We also claim that
discretization algorithms should be developed with the explicit purpose of
enriching a non-discretized dataset with discretized values. We present such an
algorithm, D-MIAT, a supervised algorithm that discretizes data based on
Minority Interesting Attribute Thresholds. D-MIAT only generates new features
when strong indications exist for one of the target values needing to be
learned and thus is intended to be used in addition to the original data. We
present extensive empirical results demonstrating the success of using D-MIAT
on $ 28 $ benchmark datasets. We also demonstrate that $ 10 $ other
discretization algorithms can also be used to generate features that yield
improved performance when used in combination with the original non-discretized
data. Our results show that the best predictive performance is attained using a
combination of the original dataset with added features from a &quot;standard&quot;
supervised discretization algorithm and D-MIAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenfeld_A/0/1/0/all/0/1&quot;&gt;Avi Rosenfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Illuz_R/0/1/0/all/0/1&quot;&gt;Ron Illuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottesman_D/0/1/0/all/0/1&quot;&gt;Dovid Gottesman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Last_M/0/1/0/all/0/1&quot;&gt;Mark Last&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03275">
<title>Slice Sampling Particle Belief Propagation. (arXiv:1802.03275v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.03275</link>
<description rdf:parseType="Literal">&lt;p&gt;Inference in continuous label Markov random fields is a challenging task. We
use particle belief propagation (PBP) for solving the inference problem in
continuous label space. Sampling particles from the belief distribution is
typically done by using Metropolis-Hastings Markov chain Monte Carlo methods
which involves sampling from a proposal distribution. This proposal
distribution has to be carefully designed depending on the particular model and
input data to achieve fast convergence. We propose to avoid dependence on a
proposal distribution by introducing a slice sampling based PBP algorithm. The
proposed approach shows superior convergence performance on an image denoising
toy example. Our findings are validated on a challenging relational 2D feature
tracking application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueller_O/0/1/0/all/0/1&quot;&gt;Oliver Mueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Michael Ying Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1&quot;&gt;Bodo Rosenhahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03375">
<title>ATPboost: Learning Premise Selection in Binary Setting with ATP Feedback. (arXiv:1802.03375v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1802.03375</link>
<description rdf:parseType="Literal">&lt;p&gt;ATPboost is a system for solving sets of large-theory problems by
interleaving ATP runs with state-of-the-art machine learning of premise
selection from the proofs. Unlike many previous approaches that use multi-label
setting, the learning is implemented as binary classification that estimates
the pairwise-relevance of (theorem, premise) pairs. ATPboost uses for this the
XGBoost gradient boosting algorithm, which is fast and has state-of-the-art
performance on many tasks. Learning in the binary setting however requires
negative examples, which is nontrivial due to many alternative proofs. We
discuss and implement several solutions in the context of the ATP/ML feedback
loop, and show that ATPboost with such methods significantly outperforms the
k-nearest neighbors multilabel classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piotrowski_B/0/1/0/all/0/1&quot;&gt;Bartosz Piotrowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1&quot;&gt;Josef Urban&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.03500">
<title>A Savage-Like Axiomatization for Nonstandard Expected Utility. (arXiv:1701.03500v7 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1701.03500</link>
<description rdf:parseType="Literal">&lt;p&gt;Since Leonard Savage&apos;s epoch-making &quot;Foundations of Statistics&quot;, Subjective
Expected Utility Theory has been the presumptive model for decision-making.
Savage provided an act-based axiomatization of standard expected utility
theory. In this article, we provide a Savage-like axiomatization of nonstandard
expected utility theory. It corresponds to a weakening of Savage&apos;s 6th axiom.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molnar_G/0/1/0/all/0/1&quot;&gt;Grant Molnar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.08071">
<title>Autonomous Agents Modelling Other Agents: A Comprehensive Survey and Open Problems. (arXiv:1709.08071v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.08071</link>
<description rdf:parseType="Literal">&lt;p&gt;Much research in artificial intelligence is concerned with the development of
autonomous agents that can interact effectively with other agents. An important
aspect of such agents is the ability to reason about the behaviours of other
agents, by constructing models which make predictions about various properties
of interest (such as actions, goals, beliefs) of the modelled agents. A variety
of modelling approaches now exist which vary widely in their methodology and
underlying assumptions, catering to the needs of the different sub-communities
within which they were developed and reflecting the different practical uses
for which they are intended. The purpose of the present article is to provide a
comprehensive survey of the salient modelling methods which can be found in the
literature. The article concludes with a discussion of open problems which may
form the basis for fruitful future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1&quot;&gt;Stefano V. Albrecht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11565">
<title>Deep Neural Networks for Multiple Speaker Detection and Localization. (arXiv:1711.11565v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11565</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to use neural networks for simultaneous detection and localization
of multiple sound sources in human-robot interaction. In contrast to
conventional signal processing techniques, neural network-based sound source
localization methods require fewer strong assumptions about the environment.
Previous neural network-based methods have been focusing on localizing a single
sound source, which do not extend to multiple sources in terms of detection and
localization. In this paper, we thus propose a likelihood-based encoding of the
network output, which naturally allows the detection of an arbitrary number of
sources. In addition, we investigate the use of sub-band cross-correlation
information as features for better localization in sound mixtures, as well as
three different network architectures based on different motivations.
Experiments on real data recorded from a robot show that our proposed methods
significantly outperform the popular spatial spectrum-based approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Weipeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1&quot;&gt;Petr Motlicek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Odobez_J/0/1/0/all/0/1&quot;&gt;Jean-Marc Odobez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08875">
<title>Predicting Rich Drug-Drug Interactions via Biomedical Knowledge Graphs and Text Jointly Embedding. (arXiv:1712.08875v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08875</link>
<description rdf:parseType="Literal">&lt;p&gt;Minimizing adverse reactions caused by drug-drug interactions has always been
a momentous research topic in clinical pharmacology. Detecting all possible
interactions through clinical studies before a drug is released to the market
is a demanding task. The power of big data is opening up new approaches to
discover various drug-drug interactions. However, these discoveries contain a
huge amount of noise and provide knowledge bases far from complete and
trustworthy ones to be utilized. Most existing studies focus on predicting
binary drug-drug interactions between drug pairs but ignore other interactions.
In this paper, we propose a novel framework, called PRD, to predict drug-drug
interactions. The framework uses the graph embedding that can overcome data
incompleteness and sparsity issues to achieve multiple DDI label prediction.
First, a large-scale drug knowledge graph is generated from different sources.
Then, the knowledge graph is embedded with comprehensive biomedical text into a
common low dimensional space. Finally, the learned embeddings are used to
efficiently compute rich DDI information through a link prediction process. To
validate the effectiveness of the proposed framework, extensive experiments
were conducted on real-world datasets. The results demonstrate that our model
outperforms several state-of-the-art baseline methods in terms of capability
and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yihe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1&quot;&gt;Buyue Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1&quot;&gt;Guodong Long&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.10459">
<title>Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With Expert Demonstrations. (arXiv:1801.10459v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.10459</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretraining with expert demonstrations have been found useful in speeding up
the training process of deep reinforcement learning algorithms since less
online simulation data is required. Some people use supervised learning to
speed up the process of feature learning, others pretrain the policies by
imitating expert demonstrations. However, these methods are unstable and not
suitable for actor-critic reinforcement learning algorithms. Also, some
existing methods rely on the global optimum assumption, which is not true in
most scenarios. In this paper, we employ expert demonstrations in a
actor-critic reinforcement learning framework, and meanwhile ensure that the
performance is not affected by the fact that expert demonstrations are not
global optimal. We theoretically derive a method for computing policy gradients
and value estimators with only expert demonstrations. Our method is
theoretically plausible for actor-critic reinforcement learning algorithms that
pretrains both policy and value functions. We apply our method to two of the
typical actor-critic reinforcement learning algorithms, DDPG and ACER, and
demonstrate with experiments that our method not only outperforms the RL
algorithms without pretraining process, but also is more simulation efficient.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huimin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03050">
<title>Thompson Sampling for Dynamic Pricing. (arXiv:1802.03050v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03050</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we apply active learning algorithms for dynamic pricing in a
prominent e-commerce website. Dynamic pricing involves changing the price of
items on a regular basis, and uses the feedback from the pricing decisions to
update prices of the items. Most popular approaches to dynamic pricing use a
passive learning approach, where the algorithm uses historical data to learn
various parameters of the pricing problem, and uses the updated parameters to
generate a new set of prices. We show that one can use active learning
algorithms such as Thompson sampling to more efficiently learn the underlying
parameters in a pricing problem. We apply our algorithms to a real e-commerce
system and show that the algorithms indeed improve revenue compared to pricing
algorithms that use passive learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ganti_R/0/1/0/all/0/1&quot;&gt;Ravi Ganti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sustik_M/0/1/0/all/0/1&quot;&gt;Matyas Sustik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tran_Q/0/1/0/all/0/1&quot;&gt;Quoc Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Seaman_B/0/1/0/all/0/1&quot;&gt;Brian Seaman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03063">
<title>Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization. (arXiv:1802.03063v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03063</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel unsupervised clustering approach exploiting
the hidden information that is indirectly introduced through a pseudo
classification objective. Specifically, we randomly assign a pseudo
parent-class label to each observation which is then modified by applying the
domain specific transformation associated with the assigned label. Generated
pseudo observation-label pairs are subsequently used to train a neural network
with Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes
for each pseudo parent-class. Due to the unsupervised objective based on
Graph-based Activity Regularization (GAR) terms, softmax duplicates of each
parent-class are specialized as the hidden information captured through the
help of domain specific transformations is propagated during training.
Ultimately we obtain a k-means friendly latent representation. Furthermore, we
demonstrate how the chosen transformation type impacts performance and helps
propagate the latent information that is useful in revealing unknown clusters.
Our results show state-of-the-art performance for unsupervised clustering tasks
on MNIST, SVHN and USPS datasets, with the highest accuracies reported to date
in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilinc_O/0/1/0/all/0/1&quot;&gt;Ozsel Kilinc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uysal_I/0/1/0/all/0/1&quot;&gt;Ismail Uysal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03065">
<title>Generating Realistic Geology Conditioned on Physical Measurements with Generative Adversarial Networks. (arXiv:1802.03065v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03065</link>
<description rdf:parseType="Literal">&lt;p&gt;An important problem in geostatistics is to build models of the subsurface of
the Earth given physical measurements at sparse spatial locations. Typically,
this is done using spatial interpolation methods or by reproducing patterns
from a reference image. However, these algorithms fail to produce realistic
patterns and do not exhibit the wide range of uncertainty inherent in the
prediction of geology. In this paper, we show how semantic inpainting with
Generative Adversarial Networks can be used to generate varied realizations of
geology which honor physical measurements while matching the expected
geological patterns. In contrast to other algorithms, our method scales well
with the number of data points and mimics a distribution of patterns as opposed
to a single pattern or image. The generated conditional samples are state of
the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dupont_E/0/1/0/all/0/1&quot;&gt;Emilien Dupont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tuanfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tilke_P/0/1/0/all/0/1&quot;&gt;Peter Tilke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Lin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bailey_W/0/1/0/all/0/1&quot;&gt;William Bailey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03101">
<title>Convolutional Hashing for Automated Scene Matching. (arXiv:1802.03101v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.03101</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a powerful new loss function and training scheme for learning
binary hash functions. In particular, we demonstrate our method by creating for
the first time a neural network that outperforms state-of-the-art Haar wavelets
and color layout descriptors at the task of automated scene matching. By
accurately relating distance on the manifold of network outputs to distance in
Hamming space, we achieve a 100-fold reduction in nontrivial false positive
rate and significantly higher true positive rate. We expect our insights to
provide large wins for hashing models applied to other information retrieval
hashing tasks as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loncaric_M/0/1/0/all/0/1&quot;&gt;Martin Loncaric&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bowei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_R/0/1/0/all/0/1&quot;&gt;Ryan Weber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03127">
<title>Robust and Sparse Regression in GLM by Stochastic Optimization. (arXiv:1802.03127v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03127</link>
<description rdf:parseType="Literal">&lt;p&gt;The generalized linear model (GLM) plays a key role in regression analyses.
In high-dimensional data, the sparse GLM has been used but it is not robust
against outliers. Recently, the robust methods have been proposed for the
specific example of the sparse GLM. Among them, we focus on the robust and
sparse linear regression based on the $\gamma$-divergence. The estimator of the
$\gamma$-divergence has strong robustness under heavy contamination. In this
paper, we extend the robust and sparse linear regression based on the
$\gamma$-divergence to the robust and sparse GLM based on the
$\gamma$-divergence with a stochastic optimization approach in order to obtain
the estimate. We adopt the randomized stochastic projected gradient descent as
a stochastic optimization approach and extend the established convergence
property to the classical first-order necessary condition. By virtue of the
stochastic optimization approach, we can efficiently estimate parameters for
very large problems. Particularly, we show the linear regression, logistic
regression and Poisson regression with $L_1$ regularization in detail as
specific examples of robust and sparse GLM. In numerical experiments and real
data analysis, the proposed method outperformed comparative methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kawashima_T/0/1/0/all/0/1&quot;&gt;Takayuki Kawashima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fujisawa_H/0/1/0/all/0/1&quot;&gt;Hironori Fujisawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03133">
<title>Batch Kalman Normalization: Towards Training Deep Neural Networks with Micro-Batches. (arXiv:1802.03133v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.03133</link>
<description rdf:parseType="Literal">&lt;p&gt;As an indispensable component, Batch Normalization (BN) has successfully
improved the training of deep neural networks (DNNs) with mini-batches, by
normalizing the distribution of the internal representation for each hidden
layer. However, the effectiveness of BN would diminish with scenario of
micro-batch (e.g., less than 10 samples in a mini-batch), since the estimated
statistics in a mini-batch are not reliable with insufficient samples. In this
paper, we present a novel normalization method, called Batch Kalman
Normalization (BKN), for improving and accelerating the training of DNNs,
particularly under the context of micro-batches. Specifically, unlike the
existing solutions treating each hidden layer as an isolated system, BKN treats
all the layers in a network as a whole system, and estimates the statistics of
a certain layer by considering the distributions of all its preceding layers,
mimicking the merits of Kalman Filtering. BKN has two appealing properties.
First, it enables more stable training and faster convergence compared to
previous works. Second, training DNNs using BKN performs substantially better
than those using BN and its variants, especially when very small mini-batches
are presented. On the image classification benchmark of ImageNet, using BKN
powered networks we improve upon the best-published model-zoo results: reaching
74.0% top-1 val accuracy for InceptionV2. More importantly, using BKN achieves
the comparable accuracy with extremely smaller batch size, such as 64 times
smaller on CIFAR-10/100 and 8 times smaller on ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangrun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jiefeng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinjiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03151">
<title>Deep Private-Feature Extraction. (arXiv:1802.03151v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03151</link>
<description rdf:parseType="Literal">&lt;p&gt;We present and evaluate Deep Private-Feature Extractor (DPFE), a deep model
which is trained and evaluated based on information theoretic constraints.
Using the selective exchange of information between a user&apos;s device and a
service provider, DPFE enables the user to prevent certain sensitive
information from being shared with a service provider, while allowing them to
extract approved information using their model. We introduce and utilize the
log-rank privacy, a novel measure to assess the effectiveness of DPFE in
removing sensitive information and compare different models based on their
accuracy-privacy tradeoff. We then implement and evaluate the performance of
DPFE on smartphones to understand its complexity, resource demands, and
efficiency tradeoffs. Our results on benchmark image datasets demonstrate that
under moderate resource utilization, DPFE can achieve high accuracy for primary
tasks while preserving the privacy of sensitive features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osia_S/0/1/0/all/0/1&quot;&gt;Seyed Ali Osia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Taheri_A/0/1/0/all/0/1&quot;&gt;Ali Taheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shamsabadi_A/0/1/0/all/0/1&quot;&gt;Ali Shahin Shamsabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Katevas_K/0/1/0/all/0/1&quot;&gt;Kleomenis Katevas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haddadi_H/0/1/0/all/0/1&quot;&gt;Hamed Haddadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rabiee_H/0/1/0/all/0/1&quot;&gt;Hamid R. Rabiee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03170">
<title>Adversarial Metric Learning. (arXiv:1802.03170v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03170</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past decades, intensive efforts have been put to design various loss
functions and metric forms for metric learning problem. These improvements have
shown promising results when the test data is similar to the training data.
However, the trained models often fail to produce reliable distances on the
ambiguous test pairs due to the distribution bias between training set and test
set. To address this problem, the Adversarial Metric Learning (AML) is proposed
in this paper, which automatically generates adversarial pairs to remedy the
distribution bias and facilitate robust metric learning. Specifically, AML
consists of two adversarial stages, i.e. confusion and distinguishment. In
confusion stage, the ambiguous but critical adversarial data pairs are
adaptively generated to mislead the learned metric. In distinguishment stage, a
metric is exhaustively learned to try its best to distinguish both the
adversarial pairs and the original training pairs. Thanks to the challenges
posed by the confusion stage in such competing process, the AML model is able
to grasp plentiful difficult knowledge that has not been contained by the
original training pairs, so the discriminability of AML can be significantly
improved. The entire model is formulated into optimization framework, of which
the global convergence is theoretically proved. The experimental results on toy
data and practical datasets clearly demonstrate the superiority of AML to the
representative state-of-the-art metric learning methodologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03203">
<title>Curve Registered Coupled Low Rank Factorization. (arXiv:1802.03203v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03203</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an extension of the canonical polyadic (CP) tensor model where one
of the latent factors is allowed to vary through data slices in a constrained
way. The components of the latent factors, which we want to retrieve from data,
can vary from one slice to another up to a diffeomorphism. We suppose that the
diffeomorphisms are also unknown, thus merging curve registration and tensor
decomposition in one model, which we call registered CP. We present an
algorithm to retrieve both the latent factors and the diffeomorphism, which is
assumed to be in a parametrized form. At the end of the paper, we show
simulation results comparing registered CP with other models from the
literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cohen_J/0/1/0/all/0/1&quot;&gt;Jeremy Emile Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Farias_R/0/1/0/all/0/1&quot;&gt;Rodrigo Cabral Farias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rivet_B/0/1/0/all/0/1&quot;&gt;Bertrand Rivet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03212">
<title>Deep clustering of longitudinal data. (arXiv:1802.03212v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03212</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are a family of computational models that have led to a
dramatical improvement of the state of the art in several domains such as
image, voice or text analysis. These methods provide a framework to model
complex, non-linear interactions in large datasets, and are naturally suited to
the analysis of hierarchical data such as, for instance, longitudinal data with
the use of recurrent neural networks. In the other hand, cohort studies have
become a tool of importance in the research field of epidemiology. In such
studies, variables are measured repeatedly over time, to allow the practitioner
to study their temporal evolution as trajectories, and, as such, as
longitudinal data. This paper investigates the application of the advanced
modelling techniques provided by the deep learning framework in the analysis of
the longitudinal data provided by cohort studies. Methods: A method for
visualizing and clustering longitudinal dataset is proposed, and compared to
other widely used approaches to the problem on both real and simulated
datasets. Results: The proposed method is shown to be coherent with the
preexisting procedures on simple tasks, and to outperform them on more complex
tasks such as the partitioning of longitudinal datasets into non-spherical
clusters. Conclusion: Deep artificial neural networks can be used to visualize
longitudinal data in a low dimensional manifold that is much simpler to
interpret than traditional longitudinal plots are. Consequently, practitioners
should start considering the use of deep artificial neural networks for the
analysis of their longitudinal data in studies to come.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Falissard_L/0/1/0/all/0/1&quot;&gt;Louis Falissard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fagherazzi_G/0/1/0/all/0/1&quot;&gt;Guy Fagherazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Howard_N/0/1/0/all/0/1&quot;&gt;Newton Howard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Falissard_B/0/1/0/all/0/1&quot;&gt;Bruno Falissard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03284">
<title>Mini-Batch Stochastic ADMMs for Nonconvex Nonsmooth Optimization. (arXiv:1802.03284v1 [math.OC])</title>
<link>http://arxiv.org/abs/1802.03284</link>
<description rdf:parseType="Literal">&lt;p&gt;In the paper, we study the mini-batch stochastic ADMMs (alternating direction
method of multipliers) for the nonconvex nonsmooth optimization. We prove that,
given an appropriate mini-batch size, the mini-batch stochastic ADMM without
variance reduction (VR) technique is convergent and reaches the convergence
rate of $O(1/T)$ to obtain a stationary point of the nonconvex optimization,
where $T$ denotes the number of iterations. Moreover, we extend the mini-batch
stochastic gradient method to both the nonconvex SVRG-ADMM and SAGA-ADMM in our
initial paper \citep{huang2016stochastic}, and also prove that these mini-batch
stochastic ADMMs reach the convergence rate of $O(1/T)$ without the condition
on the mini-batch size. In particular, we provide a specific parameter
selection for step size $\eta$ of stochastic gradients and penalization
parameter $\rho$ of the augmented Lagrangian function. Finally, some
experimental results demonstrate the effectiveness of our algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Feihu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Songcan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03300">
<title>Bayesian inference for bivariate ranks. (arXiv:1802.03300v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03300</link>
<description rdf:parseType="Literal">&lt;p&gt;A recommender system based on ranks is proposed, where an expert&apos;s ranking of
a set of objects and a user&apos;s ranking of a subset of those objects are combined
to make a prediction of the user&apos;s ranking of all objects. The rankings are
assumed to be induced by latent continuous variables corresponding to the
grades assigned by the expert and the user to the objects. The dependence
between the expert and user grades is modelled by a copula in some parametric
family. Given a prior distribution on the copula parameter, the user&apos;s complete
ranking is predicted by the mode of the posterior predictive distribution of
the user&apos;s complete ranking conditional on the expert&apos;s complete and the user&apos;s
incomplete rankings. Various Markov chain Monte-Carlo algorithms are proposed
to approximate the predictive distribution or only its mode. The predictive
distribution can be obtained exactly for the Farlie-Gumbel-Morgenstern copula
family, providing a benchmark for the approximation accuracy of the algorithms.
The method is applied to the MovieLens 100k dataset with a Gaussian copula
modelling dependence between the expert&apos;s and user&apos;s grades.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guillotte_S/0/1/0/all/0/1&quot;&gt;Simon Guillotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perron_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Perron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Segers_J/0/1/0/all/0/1&quot;&gt;Johan Segers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03319">
<title>Predicting Audio Advertisement Quality. (arXiv:1802.03319v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03319</link>
<description rdf:parseType="Literal">&lt;p&gt;Online audio advertising is a particular form of advertising used abundantly
in online music streaming services. In these platforms, which tend to host tens
of thousands of unique audio advertisements (ads), providing high quality ads
ensures a better user experience and results in longer user engagement.
Therefore, the automatic assessment of these ads is an important step toward
audio ads ranking and better audio ads creation. In this paper we propose one
way to measure the quality of the audio ads using a proxy metric called Long
Click Rate (LCR), which is defined by the amount of time a user engages with
the follow-up display ad (that is shown while the audio ad is playing) divided
by the impressions. We later focus on predicting the audio ad quality using
only acoustic features such as harmony, rhythm, and timbre of the audio,
extracted from the raw waveform. We discuss how the characteristics of the
sound can be connected to concepts such as the clarity of the audio ad message,
its trustworthiness, etc. Finally, we propose a new deep learning model for
audio ad quality prediction, which outperforms the other discussed models
trained on hand-crafted features. To the best of our knowledge, this is the
first large-scale audio ad quality prediction study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ebrahimi_S/0/1/0/all/0/1&quot;&gt;Samaneh Ebrahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vahabi_H/0/1/0/all/0/1&quot;&gt;Hossein Vahabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Prockup_M/0/1/0/all/0/1&quot;&gt;Matthew Prockup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nieto_O/0/1/0/all/0/1&quot;&gt;Oriol Nieto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03335">
<title>Black-box Variational Inference for Stochastic Differential Equations. (arXiv:1802.03335v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1802.03335</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter inference for stochastic differential equations is challenging due
to the presence of a latent diffusion process. Working with an Euler-Maruyama
discretisation for the diffusion, we use variational inference to jointly learn
the parameters and the diffusion paths. We use a standard mean-field
variational approximation of the parameter posterior, and introduce a recurrent
neural network to approximate the posterior for the diffusion paths conditional
on the parameters. This neural network learns how to provide Gaussian state
transitions which bridge between observations in a very similar way to the
conditioned diffusion process. The resulting black-box inference method can be
applied to any SDE system with light tuning requirements. We illustrate the
method on a Lotka-Volterra system and an epidemic model, producing accurate
parameter estimates in a few hours.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ryder_T/0/1/0/all/0/1&quot;&gt;Thomas Ryder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Golightly_A/0/1/0/all/0/1&quot;&gt;Andrew Golightly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McGough_A/0/1/0/all/0/1&quot;&gt;A. Stephen McGough&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Prangle_D/0/1/0/all/0/1&quot;&gt;Dennis Prangle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03337">
<title>Large Scale Constrained Linear Regression Revisited: Faster Algorithms via Preconditioning. (arXiv:1802.03337v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1802.03337</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we revisit the large-scale constrained linear regression
problem and propose faster methods based on some recent developments in
sketching and optimization. Our algorithms combine (accelerated) mini-batch SGD
with a new method called two-step preconditioning to achieve an approximate
solution with a time complexity lower than that of the state-of-the-art
techniques for the low precision case. Our idea can also be extended to the
high precision case, which gives an alternative implementation to the Iterative
Hessian Sketch (IHS) method with significantly improved time complexity.
Experiments on benchmark and synthetic datasets suggest that our methods indeed
outperform existing ones considerably in both the low and high precision cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jinhui Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.04838">
<title>Optimization Methods for Large-Scale Machine Learning. (arXiv:1606.04838v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1606.04838</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a review and commentary on the past, present, and future
of numerical optimization algorithms in the context of machine learning
applications. Through case studies on text classification and the training of
deep neural networks, we discuss how optimization problems arise in machine
learning and what makes them challenging. A major theme of our study is that
large-scale machine learning represents a distinctive setting in which the
stochastic gradient (SG) method has traditionally played a central role while
conventional gradient-based nonlinear optimization techniques typically falter.
Based on this viewpoint, we present a comprehensive theory of a
straightforward, yet versatile SG algorithm, discuss its practical behavior,
and highlight opportunities for designing algorithms with improved performance.
This leads to a discussion about the next generation of optimization methods
for large-scale machine learning, including an investigation of two main
streams of research on techniques that diminish noise in the stochastic
directions and methods that make use of second-order derivative approximations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bottou_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;on Bottou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Curtis_F/0/1/0/all/0/1&quot;&gt;Frank E. Curtis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nocedal_J/0/1/0/all/0/1&quot;&gt;Jorge Nocedal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1609.05057">
<title>Unbiased Sparse Subspace Clustering By Selective Pursuit. (arXiv:1609.05057v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1609.05057</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse subspace clustering (SSC) is an elegant approach for unsupervised
segmentation if the data points of each cluster are located in linear
subspaces. This model applies, for instance, in motion segmentation if some
restrictions on the camera model hold. SSC requires that problems based on the
$l_1$-norm are solved to infer which points belong to the same subspace. If
these unknown subspaces are well-separated this algorithm is guaranteed to
succeed. The algorithm rests upon the assumption that points on the same
subspace are well spread. The question what happens if this condition is
violated has not yet been investigated. In this work, the effect of particular
distributions on the same subspace will be analyzed. It will be shown that SSC
fails to infer correct labels if points on the same subspace fall into more
than one cluster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ackermann_H/0/1/0/all/0/1&quot;&gt;Hanno Ackermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Michael Ying Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosenhahn_B/0/1/0/all/0/1&quot;&gt;Bodo Rosenhahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.09600">
<title>Super-resolution estimation of cyclic arrival rates. (arXiv:1610.09600v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1610.09600</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploiting the fact that most arrival processes exhibit cyclic behaviour, we
propose a simple procedure for estimating the intensity of a nonhomogeneous
Poisson process. The estimator is the super-resolution analogue to Shao 2010
and Shao &amp;amp; Lii 2011, which is a sum of $p$ sinusoids where $p$ and the
frequency, amplitude, and phase of each wave are not known and need to be
estimated. This results in an interpretable yet flexible specification that is
suitable for use in modelling as well as in high resolution simulations.
&lt;/p&gt;
&lt;p&gt;Our estimation procedure sits in between classic periodogram methods and
atomic/total variation norm thresholding. Through a novel use of window
functions in the point process domain, our approach attains super-resolution
without semidefinite programming. Under suitable conditions, finite sample
guarantees can be derived for our procedure. These resolve some open questions
and expand existing results in spectral estimation literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Ningyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Donald K.K. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Negahban_S/0/1/0/all/0/1&quot;&gt;Sahand Negahban&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.07926">
<title>Boosting hazard regression with time-varying covariates. (arXiv:1701.07926v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1701.07926</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider a left-truncated right-censored survival process whose evolution
depends on time-varying covariates. Given functional data samples from the
process, we propose a practical boosting procedure for estimating its
log-intensity function. Our method does not require any separability
assumptions like Cox proportional- or Aalen additive-hazards, thus it can
flexibly capture time-covariate interactions. The estimator is consistent if
the model is correctly specified; alternatively an oracle inequality can be
demonstrated for tree-based models. We use the procedure to shed new light on a
question from the operations literature concerning the effect of workload on
service rates in an emergency department.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Donald K.K. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Ningyuan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.04126">
<title>Gaussian-Dirichlet Posterior Dominance in Sequential Learning. (arXiv:1702.04126v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1702.04126</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of sequential learning from categorical observations
bounded in [0,1]. We establish an ordering between the Dirichlet posterior over
categorical outcomes and a Gaussian posterior under observations with N(0,1)
noise. We establish that, conditioned upon identical data with at least two
observations, the posterior mean of the categorical distribution will always
second-order stochastically dominate the posterior mean of the Gaussian
distribution. These results provide a useful tool for the analysis of
sequential learning under categorical outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Osband_I/0/1/0/all/0/1&quot;&gt;Ian Osband&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roy_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.08816">
<title>Uncertainty quantification in graph-based classification of high dimensional data. (arXiv:1703.08816v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1703.08816</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification of high dimensional data finds wide-ranging applications. In
many of these applications equipping the resulting classification with a
measure of uncertainty may be as important as the classification itself. In
this paper we introduce, develop algorithms for, and investigate the properties
of, a variety of Bayesian models for the task of binary classification; via the
posterior distribution on the classification labels, these methods
automatically give measures of uncertainty. The methods are all based around
the graph formulation of semi-supervised learning.
&lt;/p&gt;
&lt;p&gt;We provide a unified framework which brings together a variety of methods
which have been introduced in different communities within the mathematical
sciences. We study probit classification in the graph-based setting, generalize
the level-set method for Bayesian inverse problems to the classification
setting, and generalize the Ginzburg-Landau optimization-based classifier to a
Bayesian setting; we also show that the probit and level set approaches are
natural relaxations of the harmonic function approach introduced in [Zhu et al
2003].
&lt;/p&gt;
&lt;p&gt;We introduce efficient numerical methods, suited to large data-sets, for both
MCMC-based sampling as well as gradient-based MAP estimation. Through numerical
experiments we study classification accuracy and uncertainty quantification for
our models; these experiments showcase a suite of datasets commonly used to
evaluate graph-based semi-supervised learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertozzi_A/0/1/0/all/0/1&quot;&gt;Andrea L. Bertozzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiyang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stuart_A/0/1/0/all/0/1&quot;&gt;Andrew M. Stuart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zygalakis_K/0/1/0/all/0/1&quot;&gt;Konstantinos C. Zygalakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.06941">
<title>Concept Drift and Anomaly Detection in Graph Streams. (arXiv:1706.06941v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.06941</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph representations offer powerful and intuitive ways to describe data in a
multitude of application domains. Here, we consider stochastic processes
generating graphs and propose a methodology for detecting changes in
stationarity of such processes. The methodology is general and considers a
process generating attributed graphs with a variable number of vertices/edges,
without the need to assume one-to-one correspondence between vertices at
different time steps. The methodology acts by embedding every graph of the
stream into a vector domain, where a conventional multivariate change detection
procedure can be easily applied. We ground the soundness of our proposal by
proving several theoretical results. In addition, we provide a specific
implementation of the methodology and evaluate its effectiveness on several
detection problems involving attributed graphs representing biological
molecules and drawings. Experimental results are contrasted with respect to
suitable baseline methods, demonstrating the effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambon_D/0/1/0/all/0/1&quot;&gt;Daniele Zambon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1&quot;&gt;Cesare Alippi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Livi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Livi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06635">
<title>A Sinkhorn-Newton method for entropic optimal transport. (arXiv:1710.06635v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06635</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the entropic regularization of discretized optimal transport and
propose to solve its optimality conditions via a logarithmic Newton iteration.
We show a quadratic convergence rate and validate numerically that the method
compares favorably with the more commonly used Sinkhorn--Knopp algorithm for
small regularization strength. We further investigate numerically the
robustness of the proposed method with respect to parameters such as the mesh
size of the discretization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Brauer_C/0/1/0/all/0/1&quot;&gt;Christoph Brauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Clason_C/0/1/0/all/0/1&quot;&gt;Christian Clason&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lorenz_D/0/1/0/all/0/1&quot;&gt;Dirk Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wirth_B/0/1/0/all/0/1&quot;&gt;Benedikt Wirth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00393">
<title>Theoretical Analysis of Sparse Subspace Clustering with Missing Entries. (arXiv:1801.00393v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.00393</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse Subspace Clustering (SSC) is a popular unsupervised machine learning
method for clustering data lying close to an unknown union of low-dimensional
linear subspaces; a problem with numerous applications in pattern recognition
and computer vision. Even though the behavior of SSC for complete data is by
now well-understood, little is known about its theoretical properties when
applied to data with missing entries. In this paper we give theoretical
guarantees for SSC with incomplete data, and analytically establish that
projecting the zero-filled data onto the observation pattern of the point being
expressed leads to a substantial improvement in performance. The main insight
that stems from our analysis is that even though the projection induces
additional missing entries, this is counterbalanced by the fact that the
projected and zero-filled data are in effect incomplete points associated with
the union of the corresponding projected subspaces, with respect to which the
point being expressed is complete. The significance of this phenomenon
potentially extends to the entire class of self-expressive methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsakiris_M/0/1/0/all/0/1&quot;&gt;Manolis C. Tsakiris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1&quot;&gt;Rene Vidal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.04510">
<title>Brain EEG Time Series Selection: A Novel Graph-Based Approach for Classification. (arXiv:1801.04510v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.04510</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain Electroencephalography (EEG) classification is widely applied to
analyze cerebral diseases in recent years. Unfortunately, invalid/noisy EEGs
degrade the diagnosis performance and most previously developed methods ignore
the necessity of EEG selection for classification. To this end, this paper
proposes a novel maximum weight clique-based EEG selection approach, named
mwcEEGs, to map EEG selection to searching maximum similarity-weighted cliques
from an improved Fr\&apos;{e}chet distance-weighted undirected EEG graph
simultaneously considering edge weights and vertex weights. Our mwcEEGs
improves the classification performance by selecting intra-clique pairwise
similar and inter-clique discriminative EEGs with similarity threshold
$\delta$. Experimental results demonstrate the algorithm effectiveness compared
with the state-of-the-art time series selection algorithms on real-world EEG
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1&quot;&gt;Chenglong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pi_D/0/1/0/all/0/1&quot;&gt;Dechang Pi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Lin Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.00474">
<title>Bayesian Modeling via Goodness-of-fit. (arXiv:1802.00474v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1802.00474</link>
<description rdf:parseType="Literal">&lt;p&gt;The two key issues of modern Bayesian statistics are: (i) establishing
principled approach for distilling statistical prior that is consistent with
the given data from an initial believable scientific prior; and (ii)
development of a Bayes-frequentist consolidated data analysis workflow that is
more effective than either of the two separately. In this paper, we propose the
idea of &quot;Bayes via goodness of fit&quot; as a framework for exploring these
fundamental questions, in a way that is general enough to embrace almost all of
the familiar probability models. Several illustrative examples show the benefit
of this new point of view as a practical data analysis tool. Relationship with
other Bayesian cultures is also discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Subhadeep/0/1/0/all/0/1&quot;&gt;Subhadeep&lt;/a&gt; (Deep) &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mukhopadhyay/0/1/0/all/0/1&quot;&gt;Mukhopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fletcher_D/0/1/0/all/0/1&quot;&gt;Douglas Fletcher&lt;/a&gt;</dc:creator>
</item></rdf:RDF>