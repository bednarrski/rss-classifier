<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-14T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05117"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05132"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05174"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05192"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05383"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.01211"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06434"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04357"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05098"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05156"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05181"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05340"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05407"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07073"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03800"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04965"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05045"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05070"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05104"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05112"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05307"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05339"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05391"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05397"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.05419"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.06686"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.09930"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.08360"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01543"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.02711"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06034"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.09514"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10388"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.01587"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.08694"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.01751"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02840"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07329"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.05117">
<title>MT-Spike: A Multilayer Time-based Spiking Neuromorphic Architecture with Temporal Error Backpropagation. (arXiv:1803.05117v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.05117</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern deep learning enabled artificial neural networks, such as Deep Neural
Network (DNN) and Convolutional Neural Network (CNN), have achieved a series of
breaking records on a broad spectrum of recognition applications. However, the
enormous computation and storage requirements associated with such deep and
complex neural network models greatly challenge their implementations on
resource-limited platforms. Time-based spiking neural network has recently
emerged as a promising solution in Neuromorphic Computing System designs for
achieving remarkable computing and power efficiency within a single chip.
However, the relevant research activities have been narrowly concentrated on
the biological plausibility and theoretical learning approaches, causing
inefficient neural processing and impracticable multilayer extension thus
significantly limitations on speed and accuracy when handling the realistic
cognitive tasks. In this work, a practical multilayer time-based spiking
neuromorphic architecture, namely &quot;MT-Spike&quot;, is developed to fill this gap.
With the proposed practical time-coding scheme, average delay response model,
temporal error backpropagation algorithm, and heuristic loss function,
&quot;MT-Spike&quot; achieves more efficient neural processing through flexible neural
model size reduction while offering very competitive classification accuracy
for realistic recognition tasks. Simulation results well validated that the
algorithmic power of deep multi-layer learning can be seamlessly merged with
the efficiency of time-based spiking neuromorphic architecture, demonstrating
great potentials of &quot;MT-Spike&quot; in resource and power constrained embedded
platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zihao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fuhong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yier Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_G/0/1/0/all/0/1&quot;&gt;Gang Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wujie Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05132">
<title>Neuron inspired data encoding memristive multi-level memory cell. (arXiv:1803.05132v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1803.05132</link>
<description rdf:parseType="Literal">&lt;p&gt;Mapping neuro-inspired algorithms to sensor backplanes of on-chip hardware
require shifting the signal processing from digital to the analog domain,
demanding memory technologies beyond conventional CMOS binary storage units.
Using memristors for building analog data storage is one of the promising
approaches amongst emerging non-volatile memory technologies. Recently, a
memristive multi-level memory (MLM) cell for storing discrete analog values has
been developed in which memory system is implemented combining memristors in
voltage divider configuration. In given example, the memory cell of 3 sub-cells
with a memristor in each was programmed to store ternary bits which overall
achieved 10 and 27 discrete voltage levels. However, for further use of
proposed memory cell in analog signal processing circuits data encoder is
required to generate control voltages for programming memristors to store
discrete analog values. In this paper, we present the design and performance
analysis of data encoder that generates write pattern signals for 10 level
memristive memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irmanova_A/0/1/0/all/0/1&quot;&gt;Aidana Irmanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_A/0/1/0/all/0/1&quot;&gt;Alex Pappachen James&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05174">
<title>Multi-objective Analysis of MAP-Elites Performance. (arXiv:1803.05174v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.05174</link>
<description rdf:parseType="Literal">&lt;p&gt;In certain complex optimization tasks, it becomes necessary to use multiple
measures to characterize the performance of different algorithms. This paper
presents a method that combines ordinal effect sizes with Pareto dominance to
analyze such cases. Since the method is ordinal, it can also generalize across
different optimization tasks even when the performance measurements are
differently scaled. Through a case study, we show that this method can discover
and quantify relations that would be difficult to deduce using a conventional
measure-by-measure analysis. This case study applies the method to the
evolution of robot controller repertoires using the MAP-Elites algorithm. Here,
we analyze the search performance across a large set of parametrizations;
varying mutation size and operator type, as well as map resolution, across four
different robot morphologies. We show that the average magnitude of mutations
has a bigger effect on outcomes than their precise distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samuelsen_E/0/1/0/all/0/1&quot;&gt;Eivind Samuelsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glette_K/0/1/0/all/0/1&quot;&gt;Kyrre Glette&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05192">
<title>Spatio-temporal Deep De-aliasing for Prospective Assessment of Real-time Ventricular Volumes. (arXiv:1803.05192v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.05192</link>
<description rdf:parseType="Literal">&lt;p&gt;PURPOSE: Real-time assessment of ventricular volumes requires high
acceleration factors. Residual convolutional neural networks (CNN) have shown
potential for removing artifacts caused by data undersampling. In this study we
investigated the effect of different radial sampling patterns on the accuracy
of a CNN. We also acquired actual real-time undersampled radial data in
patients with congenital heart disease (CHD), and compare CNN reconstruction to
Compressed Sensing (CS).
&lt;/p&gt;
&lt;p&gt;METHODS: A 3D (2D plus time) CNN architecture was developed, and trained
using 2276 gold-standard paired 3D data sets, with 14x radial undersampling.
Four sampling schemes were tested, using 169 previously unseen 3D &apos;synthetic&apos;
test data sets. Actual real-time tiny Golden Angle (tGA) radial SSFP data was
acquired in 10 new patients (122 3D data sets), and reconstructed using the 3D
CNN as well as a CS algorithm; GRASP.
&lt;/p&gt;
&lt;p&gt;RESULTS: Sampling pattern was shown to be important for image quality, and
accurate visualisation of cardiac structures. For actual real-time data,
overall reconstruction time with CNN (including creation of aliased images) was
shown to be more than 5x faster than GRASP. Additionally, CNN image quality and
accuracy of biventricular volumes was observed to be superior to GRASP for the
same raw data.
&lt;/p&gt;
&lt;p&gt;CONCLUSION: This paper has demonstrated the potential for the use of a 3D CNN
for deep de-aliasing of real-time radial data, within the clinical setting.
Clinical measures of ventricular volumes using real-time data with CNN
reconstruction are not statistically significantly different from the
gold-standard, cardiac gated, BH techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1&quot;&gt;Andreas Hauptmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arridge_S/0/1/0/all/0/1&quot;&gt;Simon Arridge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucka_F/0/1/0/all/0/1&quot;&gt;Felix Lucka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muthurangu_V/0/1/0/all/0/1&quot;&gt;Vivek Muthurangu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steeden_J/0/1/0/all/0/1&quot;&gt;Jennifer A. Steeden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05383">
<title>Use of recurrent infomax to improve the memory capability of input-driven recurrent neural networks. (arXiv:1803.05383v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.05383</link>
<description rdf:parseType="Literal">&lt;p&gt;The inherent transient dynamics of recurrent neural networks (RNNs) have been
exploited as a computational resource in input-driven RNNs. However, the
information processing capability varies from RNN to RNN, depending on their
properties. Many authors have investigated the dynamics of RNNs and their
relevance to the information processing capability. In this study, we present a
detailed analysis of the information processing capability of an RNN optimized
by recurrent infomax (RI), which is an unsupervised learning scheme that
maximizes the mutual information of RNNs by adjusting the connection strengths
of the network. Thus, we observe that a delay-line structure emerges from the
RI and the network optimized by the RI possesses superior short-term memory,
which is the ability to store the temporal information of the input stream in
its transient dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwade_H/0/1/0/all/0/1&quot;&gt;Hisashi Iwade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakajima_K/0/1/0/all/0/1&quot;&gt;Kohei Nakajima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1&quot;&gt;Takuma Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aoyagi_T/0/1/0/all/0/1&quot;&gt;Toshio Aoyagi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.01211">
<title>Combating Reinforcement Learning&apos;s Sisyphean Curse with Intrinsic Fear. (arXiv:1611.01211v8 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1611.01211</link>
<description rdf:parseType="Literal">&lt;p&gt;Many practical environments contain catastrophic states that an optimal agent
would visit infrequently or never. Even on toy problems, Deep Reinforcement
Learning (DRL) agents tend to periodically revisit these states upon forgetting
their existence under a new policy. We introduce intrinsic fear (IF), a learned
reward shaping that guards DRL agents against periodic catastrophes. IF agents
possess a fear model trained to predict the probability of imminent
catastrophe. This score is then used to penalize the Q-learning objective. Our
theoretical analysis bounds the reduction in average return due to learning on
the perturbed objective. We also prove robustness to classification errors. As
a bonus, IF models tend to learn faster, owing to reward shaping. Experiments
demonstrate that intrinsic-fear DQNs solve otherwise pathological environments
and improve on several Atari games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1&quot;&gt;Kamyar Azizzadenesheli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Abhishek Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lihong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1&quot;&gt;Li Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06434">
<title>EffNet: An Efficient Structure for Convolutional Neural Networks. (arXiv:1801.06434v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06434</link>
<description rdf:parseType="Literal">&lt;p&gt;With the ever increasing application of Convolutional Neural Networks to
costumer products the need emerges for models which can efficiently run on
embedded, mobile hardware. Slimmer models have therefore become a hot research
topic with multiple different approaches which vary from binary networks to
revised convolution layers. We offer our contribution to the latter and propose
a novel convolution block which significantly reduces the computational burden
while surpassing the current state-of-the-art. Our model, dubbed EffNet, is
optimised for models which are slim to begin with and is created to tackle
issues in existing models such as MobileNet and ShuffleNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freeman_I/0/1/0/all/0/1&quot;&gt;Ido Freeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roese_Koerner_L/0/1/0/all/0/1&quot;&gt;Lutz Roese-Koerner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kummert_A/0/1/0/all/0/1&quot;&gt;Anton Kummert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04357">
<title>Learning the Base Distribution in Implicit Generative Models. (arXiv:1803.04357v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04357</link>
<description rdf:parseType="Literal">&lt;p&gt;Popular generative model learning methods such as Generative Adversarial
Networks (GANs), and Variational Autoencoders (VAE) enforce the latent
representation to follow simple distributions such as isotropic Gaussian. In
this paper, we argue that learning a complicated distribution over the latent
space of an auto-encoder enables more accurate modeling of complicated data
distributions. Based on this observation, we propose a two stage optimization
procedure which maximizes an approximate implicit density model. We
experimentally verify that our method outperforms GANs and VAEs on two image
datasets (MNIST, CELEB-A). We also show that our approach is amenable to
learning generative model for sequential data, by learning to generate speech
and music.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subakan_C/0/1/0/all/0/1&quot;&gt;Cem Subakan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1&quot;&gt;Oluwasanmi Koyejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1&quot;&gt;Paris Smaragdis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05098">
<title>Algorithmic Social Intervention. (arXiv:1803.05098v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.05098</link>
<description rdf:parseType="Literal">&lt;p&gt;Social and behavioral interventions are a critical tool for governments and
communities to tackle deep-rooted societal challenges such as homelessness,
disease, and poverty. However, real-world interventions are almost always
plagued by limited resources and limited data, which creates a computational
challenge: how can we use algorithmic techniques to enhance the targeting and
delivery of social and behavioral interventions? The goal of my thesis is to
provide a unified study of such questions, collectively considered under the
name &quot;algorithmic social intervention&quot;. This proposal introduces algorithmic
social intervention as a distinct area with characteristic technical
challenges, presents my published research in the context of these challenges,
and outlines open problems for future work. A common technical theme is
decision making under uncertainty: how can we find actions which will impact a
social system in desirable ways under limitations of knowledge and resources?
The primary application area for my work thus far is public health, e.g. HIV or
tuberculosis prevention. For instance, I have developed a series of algorithms
which optimize social network interventions for HIV prevention. Two of these
algorithms have been pilot-tested in collaboration with LA-area service
providers for homeless youth, with preliminary results showing substantial
improvement over status-quo approaches. My work also spans other topics in
infectious disease prevention and underlying algorithmic questions in robust
and risk-aware submodular optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilder_B/0/1/0/all/0/1&quot;&gt;Bryan Wilder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05156">
<title>The 2017 AIBIRDS Competition. (arXiv:1803.05156v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.05156</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an overview of the sixth AIBIRDS competition, held at the
26th International Joint Conference on Artificial Intelligence. This
competition tasked participants with developing an intelligent agent which can
play the physics-based puzzle game Angry Birds. This game uses a sophisticated
physics engine that requires agents to reason and predict the outcome of
actions with only limited environmental information. Agents entered into this
competition were required to solve a wide assortment of previously unseen
levels within a set time limit. The physical reasoning and planning required to
solve these levels are very similar to those of many real-world problems. This
year&apos;s competition featured some of the best agents developed so far and even
included several new AI techniques such as deep reinforcement learning. Within
this paper we describe the framework, rules, submitted agents and results for
this competition. We also provide some background information on related work
and other video game AI competitions, as well as discussing some potential
ideas for future AIBIRDS competitions and agent improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stephenson_M/0/1/0/all/0/1&quot;&gt;Matthew Stephenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renz_J/0/1/0/all/0/1&quot;&gt;Jochen Renz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05181">
<title>Can Autism be Catered with Artificial Intelligence-Assisted Intervention Technology? A Literature Review. (arXiv:1803.05181v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1803.05181</link>
<description rdf:parseType="Literal">&lt;p&gt;This article presents an extensive literature review of technology based
intervention methodologies for individuals facing Autism Spectrum Disorder
(ASD). Reviewed methodologies include: contemporary Computer Aided Systems
(CAS), Computer Vision Assisted Technologies (CVAT) and Virtual Reality (VR) or
Artificial Intelligence-Assisted interventions. The research over the past
decade has provided enough demonstrations that individuals of ASD have a strong
interest in technology based interventions and can connect with them for longer
durations without facing any trouble(s). Theses technology based interventions
are useful for individuals facing autism in clinical settings as well as at
home and classrooms.
&lt;/p&gt;
&lt;p&gt;Despite showing great promise, research in developing an advanced technology
based intervention that is clinically quantitative for ASD is minimal.
Moreover, the clinicians are generally not convinced about the potential of the
technology based interventions due to non-empirical nature of published
results. A major reason behind this non-acceptability is a vast majority of
studies on distinct intervention methodologies do not follow any specific
standard or research design. Consequently, the data produced by these studies
is minimally appealing to the clinical community.
&lt;/p&gt;
&lt;p&gt;This research domain has a vast social impact as per official statistics
given by the Autism Society of America, autism is the fastest growing
developmental disability in the United States (US). The estimated annual cost
in the US for diagnosis and treatment for ASD is 236-262 Billion US Dollars.
The cost of up-bringing an ASD individual is estimated to be 1.4 million USD
while statistics show 1% of the worlds&apos; total population is suffering from ASD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaliawala_M/0/1/0/all/0/1&quot;&gt;Muhammad Shoaib Jaliawala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1&quot;&gt;Rizwan Ahmed Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05340">
<title>Measurement-based adaptation protocol with quantum reinforcement learning. (arXiv:1803.05340v1 [quant-ph])</title>
<link>http://arxiv.org/abs/1803.05340</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning employs dynamical algorithms that mimic the human capacity
to learn, where the reinforcement learning ones are among the most similar to
humans in this respect. On the other hand, adaptability is an essential aspect
to perform any task efficiently in a changing environment, and it is
fundamental for many purposes, such as natural selection. Here, we propose an
algorithm based on successive measurements to adapt one quantum state to a
reference unknown state, in the sense of achieving maximum overlap. The
protocol naturally provides many identical copies of the reference state, such
that in each measurement iteration more information about it is obtained. In
our protocol, we consider a system composed of three parts, the &quot;environment&quot;
system, which provides the reference state copies; the register, which is an
auxiliary subsystem that interacts with the environment to acquire information
from it; and the agent, which corresponds to the quantum state that is adapted
by digital feedback with input corresponding to the outcome of the measurements
on the register. With this proposal we can achieve an average fidelity between
the environment and the agent of more than $90\% $ with less than $30$
iterations of the protocol. In addition, we extend the formalism to $ d
$-dimensional states, reaching an average fidelity of around $80\% $ in less
than $400$ iterations for $d=$ 11, for a variety of genuinely quantum as well
as semiclassical states. This work paves the way for the development of quantum
reinforcement learning protocols using quantum data, and the future deployment
of semi-autonomous quantum systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Albarran_Arriagada_F/0/1/0/all/0/1&quot;&gt;F. Albarr&amp;#xe1;n-Arriagada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Retamal_J/0/1/0/all/0/1&quot;&gt;J. C. Retamal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Solano_E/0/1/0/all/0/1&quot;&gt;E. Solano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Lamata_L/0/1/0/all/0/1&quot;&gt;L. Lamata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05407">
<title>Averaging Weights Leads to Wider Optima and Better Generalization. (arXiv:1803.05407v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.05407</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are typically trained by optimizing a loss function with
an SGD variant, in conjunction with a decaying learning rate, until
convergence. We show that simple averaging of multiple points along the
trajectory of SGD, with a cyclical or constant learning rate, leads to better
generalization than conventional training. We also show that this Stochastic
Weight Averaging (SWA) procedure finds much broader optima than SGD, and
approximates the recent Fast Geometric Ensembling (FGE) approach with a single
model. Using SWA we achieve notable improvement in test accuracy over
conventional SGD training on a range of state-of-the-art residual networks,
PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and
ImageNet. In short, SWA is extremely easy to implement, improves
generalization, and has almost no computational overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izmailov_P/0/1/0/all/0/1&quot;&gt;Pavel Izmailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Podoprikhin_D/0/1/0/all/0/1&quot;&gt;Dmitrii Podoprikhin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garipov_T/0/1/0/all/0/1&quot;&gt;Timur Garipov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1&quot;&gt;Dmitry Vetrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07073">
<title>Robust Maximization of Non-Submodular Objectives. (arXiv:1802.07073v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07073</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of maximizing a monotone set function subject to a
cardinality constraint $k$ in the setting where some number of elements $\tau$
is deleted from the returned set. The focus of this work is on the worst-case
adversarial setting. While there exist constant-factor guarantees when the
function is submodular, there are no guarantees for non-submodular objectives.
In this work, we present a new algorithm Oblivious-Greedy and prove the first
constant-factor approximation guarantees for a wider class of non-submodular
objectives. The obtained theoretical bounds are the first constant-factor
bounds that also hold in the linear regime, i.e. when the number of deletions
$\tau$ is linear in $k$. Our bounds depend on established parameters such as
the submodularity ratio and some novel ones such as the inverse curvature. We
bound these parameters for two important objectives including support selection
and variance reduction. Finally, we numerically demonstrate the robust
performance of Oblivious-Greedy for these two objectives on various datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bogunovic_I/0/1/0/all/0/1&quot;&gt;Ilija Bogunovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junyao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03800">
<title>ARMDN: Associative and Recurrent Mixture Density Networks for eRetail Demand Forecasting. (arXiv:1803.03800v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1803.03800</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate demand forecasts can help on-line retail organizations better plan
their supply-chain processes. The challenge, however, is the large number of
associative factors that result in large, non-stationary shifts in demand,
which traditional time series and regression approaches fail to model. In this
paper, we propose a Neural Network architecture called AR-MDN, that
simultaneously models associative factors, time-series trends and the variance
in the demand. We first identify several causal features and use a combination
of feature embeddings, MLP and LSTM to represent them. We then model the output
density as a learned mixture of Gaussian distributions. The AR-MDN can be
trained end-to-end without the need for additional supervision. We experiment
on a dataset of an year&apos;s worth of data over tens-of-thousands of products from
Flipkart. The proposed architecture yields a significant improvement in
forecasting accuracy when compared with existing alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Srayanta Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shankar_D/0/1/0/all/0/1&quot;&gt;Devashish Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Atin Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tathawadekar_N/0/1/0/all/0/1&quot;&gt;Nilam Tathawadekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kompalli_P/0/1/0/all/0/1&quot;&gt;Pramod Kompalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1&quot;&gt;Sunita Sarawagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhury_K/0/1/0/all/0/1&quot;&gt;Krishnendu Chaudhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04965">
<title>Coregionalised Locomotion Envelopes - A Qualitative Approach. (arXiv:1803.04965v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04965</link>
<description rdf:parseType="Literal">&lt;p&gt;&apos;Sharing of statistical strength&apos; is a phrase often employed in machine
learning and signal processing. In sensor networks, for example, missing
signals from certain sensors may be predicted by exploiting their correlation
with observed signals acquired from other sensors. For humans, our hands move
synchronously with our legs, and we can exploit these implicit correlations for
predicting new poses and for generating new natural-looking walking sequences.
We can also go much further and exploit this form of transfer learning, to
develop new control schemas for robust control of rehabilitation robots. In
this short paper we introduce coregionalised locomotion envelopes - a method
for multi-dimensional manifold regression, on human locomotion variates. Herein
we render a qualitative description of this method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhir_N/0/1/0/all/0/1&quot;&gt;Neil Dhir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dallali_H/0/1/0/all/0/1&quot;&gt;Houman Dallali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rastgaar_M/0/1/0/all/0/1&quot;&gt;Mo Rastgaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05045">
<title>Analysis of Nonautonomous Adversarial Systems. (arXiv:1803.05045v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.05045</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks are used to generate images but still their
convergence properties are not well understood. There have been a few studies
who intended to investigate the stability properties of GANs as a dynamical
system. This short writing can be seen in that direction. Among the proposed
methods for stabilizing training of GANs, {\ss}-GAN was the first who proposed
a complete annealing strategy to change high-level conditions of the GAN
objective. In this note, we show by a simple example how annealing strategy
works in GANs. The theoretical analysis is supported by simple simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mehrjou_A/0/1/0/all/0/1&quot;&gt;Arash Mehrjou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05070">
<title>A Multi-Modal Approach to Infer Image Affect. (arXiv:1803.05070v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.05070</link>
<description rdf:parseType="Literal">&lt;p&gt;The group affect or emotion in an image of people can be inferred by
extracting features about both the people in the picture and the overall makeup
of the scene. The state-of-the-art on this problem investigates a combination
of facial features, scene extraction and even audio tonality. This paper
combines three additional modalities, namely, human pose, text-based tagging
and CNN extracted features / predictions. To the best of our knowledge, this is
the first time all of the modalities were extracted using deep neural networks.
We evaluate the performance of our approach against baselines and identify
insights throughout this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundaresan_A/0/1/0/all/0/1&quot;&gt;Ashok Sundaresan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murugesan_S/0/1/0/all/0/1&quot;&gt;Sugumar Murugesan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_S/0/1/0/all/0/1&quot;&gt;Sean Davis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kappaganthu_K/0/1/0/all/0/1&quot;&gt;Karthik Kappaganthu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;ZhongYi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1&quot;&gt;Divya Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maunder_A/0/1/0/all/0/1&quot;&gt;Anurag Maunder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05104">
<title>Bucket Renormalization for Approximate Inference. (arXiv:1803.05104v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.05104</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic graphical models are a key tool in machine learning
applications. Computing the partition function, i.e., normalizing constant, is
a fundamental task of statistical inference but it is generally computationally
intractable, leading to extensive study of approximation methods. Iterative
variational methods are a popular and successful family of approaches. However,
even state of the art variational methods can return poor results or fail to
converge on difficult instances. In this paper, we instead consider computing
the partition function via sequential summation over variables. We develop
robust approximate algorithms by combining ideas from mini-bucket elimination
with tensor network and renormalization group methods from statistical physics.
The resulting &quot;convergence-free&quot; methods show good empirical performance on
both synthetic and real-world benchmark models, even for difficult instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ahn_S/0/1/0/all/0/1&quot;&gt;Sungsoo Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chertkov_M/0/1/0/all/0/1&quot;&gt;Michael Chertkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jinwoo Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05112">
<title>Uplift Modeling from Separate Labels. (arXiv:1803.05112v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.05112</link>
<description rdf:parseType="Literal">&lt;p&gt;Uplift modeling is aimed at estimating the incremental impact of an action on
an individual&apos;s behavior, which is useful in various application domains such
as targeted marketing (advertisement campaigns) and personalized medicine
(medical treatments). Conventional methods of uplift modeling require every
instance to be jointly equipped with two types of labels: the taken action and
its outcome. However, obtaining two labels for each instance at the same time
is difficult or expensive in many real-world problems. In this paper, we
propose a novel method of uplift modeling that is applicable to a more
practical setting where only one type of labels is available for each instance.
We demonstrate the effectiveness of the proposed method through experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yamane_I/0/1/0/all/0/1&quot;&gt;Ikko Yamane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yger_F/0/1/0/all/0/1&quot;&gt;Florian Yger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05307">
<title>Deep CNN based feature extractor for text-prompted speaker recognition. (arXiv:1803.05307v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1803.05307</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning is still not a very common tool in speaker verification field.
We study deep convolutional neural network performance in the text-prompted
speaker verification task. The prompted passphrase is segmented into word
states - i.e. digits -to test each digit utterance separately. We train a
single high-level feature extractor for all states and use cosine similarity
metric for scoring. The key feature of our network is the Max-Feature-Map
activation function, which acts as an embedded feature selector. By using
multitask learning scheme to train the high-level feature extractor we were
able to surpass the classic baseline systems in terms of quality and achieved
impressive results for such a novice approach, getting 2.85% EER on the RSR2015
evaluation set. Fusion of the proposed and the baseline systems improves this
result.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Novoselov_S/0/1/0/all/0/1&quot;&gt;Sergey Novoselov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kudashev_O/0/1/0/all/0/1&quot;&gt;Oleg Kudashev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schemelinin_V/0/1/0/all/0/1&quot;&gt;Vadim Schemelinin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kremnev_I/0/1/0/all/0/1&quot;&gt;Ivan Kremnev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lavrentyeva_G/0/1/0/all/0/1&quot;&gt;Galina Lavrentyeva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05339">
<title>Predicting Oral Disintegrating Tablet Formulations by Neural Network Techniques. (arXiv:1803.05339v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.05339</link>
<description rdf:parseType="Literal">&lt;p&gt;Oral Disintegrating Tablets (ODTs) is a novel dosage form that can be
dissolved on the tongue within 3min or less especially for geriatric and
pediatric patients. Current ODT formulation studies usually rely on the
personal experience of pharmaceutical experts and trial-and-error in the
laboratory, which is inefficient and time-consuming. The aim of current
research was to establish the prediction model of ODT formulations with direct
compression process by Artificial Neural Network (ANN) and Deep Neural Network
(DNN) techniques. 145 formulation data were extracted from Web of Science. All
data sets were divided into three parts: training set (105 data), validation
set (20) and testing set (20). ANN and DNN were compared for the prediction of
the disintegrating time. The accuracy of the ANN model has reached 85.60%,
80.00% and 75.00% on the training set, validation set and testing set
respectively, whereas that of the DNN model was 85.60%, 85.00% and 80.00%,
respectively. Compared with the ANN, DNN showed the better prediction for ODT
formulations. It is the first time that deep neural network with the improved
dataset selection algorithm is applied to formulation prediction on small data.
The proposed predictive approach could evaluate the critical parameters about
quality control of formulation, and guide research and process development. The
implementation of this prediction model could effectively reduce drug product
development timeline and material usage, and proactively facilitate the
development of a robust drug product.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Han_R/0/1/0/all/0/1&quot;&gt;Run Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yilong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoshan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ouyang_D/0/1/0/all/0/1&quot;&gt;Defang Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05391">
<title>On the Universal Approximation Property and Equivalence of Stochastic Computing-based Neural Networks and Binary Neural Networks. (arXiv:1803.05391v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.05391</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale deep neural networks are both memory intensive and
computation-intensive, thereby posing stringent requirements on the computing
platforms. Hardware accelerations of deep neural networks have been extensively
investigated in both industry and academia. Specific forms of binary neural
networks (BNNs) and stochastic computing based neural networks (SCNNs) are
particularly appealing to hardware implementations since they can be
implemented almost entirely with binary operations. Despite the obvious
advantages in hardware implementation, these approximate computing techniques
are questioned by researchers in terms of accuracy and universal applicability.
Also it is important to understand the relative pros and cons of SCNNs and BNNs
in theory and in actual hardware implementations. In order to address these
concerns, in this paper we prove that the &quot;ideal&quot; SCNNs and BNNs satisfy the
universal approximation property with probability 1 (due to the stochastic
behavior). The proof is conducted by first proving the property for SCNNs from
the strong law of large numbers, and then using SCNNs as a &quot;bridge&quot; to prove
for BNNs. Based on the universal approximation property, we further prove that
SCNNs and BNNs exhibit the same energy complexity. In other words, they have
the same asymptotic energy consumption with the growing of network size. We
also provide a detailed analysis of the pros and cons of SCNNs and BNNs for
hardware implementations and conclude that SCNNs are more suitable for
hardware.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiayu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1&quot;&gt;Bo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wujie Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xue Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05397">
<title>Redundancy Techniques for Straggler Mitigation in Distributed Optimization and Learning. (arXiv:1803.05397v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.05397</link>
<description rdf:parseType="Literal">&lt;p&gt;Performance of distributed optimization and learning systems is bottlenecked
by &quot;straggler&quot; nodes and slow communication links, which significantly delay
computation. We propose a distributed optimization framework where the dataset
is &quot;encoded&quot; to have an over-complete representation with built-in redundancy,
and the straggling nodes in the system are dynamically left out of the
computation at every iteration, whose loss is compensated by the embedded
redundancy. We show that oblivious application of several popular optimization
algorithms on encoded data, including gradient descent, L-BFGS, proximal
gradient under data parallelism, and coordinate descent under model
parallelism, converge to either approximate or exact solutions of the original
problem when stragglers are treated as erasures. These convergence results are
deterministic, i.e., they establish sample path convergence for arbitrary
sequences of delay patterns or distributions on the nodes, and are independent
of the tail behavior of the delay distribution. We demonstrate that equiangular
tight frames have desirable properties as encoding matrices, and propose
efficient mechanisms for encoding large-scale data. We implement the proposed
technique on Amazon EC2 clusters, and demonstrate its performance over several
learning problems, including matrix factorization, LASSO, ridge regression and
logistic regression, and compare the proposed method with uncoded,
asynchronous, and data replication strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karakus_C/0/1/0/all/0/1&quot;&gt;Can Karakus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Diggavi_S/0/1/0/all/0/1&quot;&gt;Suhas Diggavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wotao Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.05419">
<title>Generalised Structural CNNs (SCNNs) for time series data with arbitrary graph-toplogies. (arXiv:1803.05419v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.05419</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning methods, specifically convolutional neural networks (CNNs),
have seen a lot of success in the domain of image-based data, where the data
offers a clearly structured topology in the regular lattice of pixels. This
4-neighbourhood topological simplicity makes the application of convolutional
masks straightforward for time series data, such as video applications, but
many high-dimensional time series data are not organised in regular lattices,
and instead values may have adjacency relationships with non-trivial
topologies, such as small-world networks or trees. In our application case,
human kinematics, it is currently unclear how to generalise convolutional
kernels in a principled manner. Therefore we define and implement here a
framework for general graph-structured CNNs for time series analysis. Our
algorithm automatically builds convolutional layers using the specified
adjacency matrix of the data dimensions and convolutional masks that scale with
the hop distance. In the limit of a lattice-topology our method produces the
well-known image convolutional masks. We test our method first on synthetic
data of arbitrarily-connected graphs and human hand motion capture data, where
the hand is represented by a tree capturing the mechanical dependencies of the
joints. We are able to demonstrate, amongst other things, that inclusion of the
graph structure of the data dimensions improves model prediction significantly,
when compared against a benchmark CNN model with only time convolution layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Teh_T/0/1/0/all/0/1&quot;&gt;Thomas Teh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Auepanwiriyakul_C/0/1/0/all/0/1&quot;&gt;Chaiyawan Auepanwiriyakul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Harston_J/0/1/0/all/0/1&quot;&gt;John Alexander Harston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Faisal_A/0/1/0/all/0/1&quot;&gt;A. Aldo Faisal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.06686">
<title>Copula Index for Detecting Dependence and Monotonicity between Stochastic Signals. (arXiv:1703.06686v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1703.06686</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a nonparametric copula-based index for detecting the
strength and monotonicity structure of linear and nonlinear statistical
dependence between pairs of random variables or stochastic signals. Our index,
termed Copula Index for Detecting Dependence and Monotonicity (CIM), satisfies
several desirable properties of measures of association, including R\&apos;enyi&apos;s
properties, the data processing inequality (DPI), and consequently
self-equitability. Synthetic data simulations reveal that the statistical power
of CIM compares favorably to other state-of-the-art measures of association
that are proven to satisfy the DPI. Simulation results with real-world data
reveal the CIM&apos;s unique ability to detect the monotonicity structure among
stochastic signals to find interesting dependencies in large datasets.
Additionally, simulations show that the CIM shows favorable performance to
estimators of mutual information when discovering Markov network structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karra_K/0/1/0/all/0/1&quot;&gt;Kiran Karra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mili_L/0/1/0/all/0/1&quot;&gt;Lamine Mili&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.09930">
<title>Adaptive Gaussian process approximation for Bayesian inference with expensive likelihood functions. (arXiv:1703.09930v4 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1703.09930</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider Bayesian inference problems with computationally intensive
likelihood functions. We propose a Gaussian process (GP) based method to
approximate the joint distribution of the unknown parameters and the data. In
particular, we write the joint density approximately as a product of an
approximate posterior density and an exponentiated GP surrogate. We then
provide an adaptive algorithm to construct such an approximation, where an
active learning method is used to choose the design points. With numerical
examples, we illustrate that the proposed method has competitive performance
against existing approaches for Bayesian computation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongqiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinglai Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.08360">
<title>Efficient and principled score estimation with Nystr\&quot;om kernel exponential families. (arXiv:1705.08360v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.08360</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a fast method with statistical guarantees for learning an
exponential family density model where the natural parameter is in a
reproducing kernel Hilbert space, and may be infinite-dimensional. The model is
learned by fitting the derivative of the log density, the score, thus avoiding
the need to compute a normalization constant. Our approach improves the
computational efficiency of an earlier solution by using a low-rank,
Nystr\&quot;om-like solution. The new solution retains the consistency and
convergence rates of the full-rank solution (exactly in Fisher distance, and
nearly in other distances), with guarantees on the degree of cost and storage
reduction. We evaluate the method in experiments on density estimation and in
the construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an
existing score learning approach using a denoising autoencoder, our estimator
is empirically more data-efficient when estimating the score, runs faster, and
has fewer parameters (which can be tuned in a principled and interpretable
way), in addition to providing statistical guarantees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1&quot;&gt;Dougal J. Sutherland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Strathmann_H/0/1/0/all/0/1&quot;&gt;Heiko Strathmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arbel_M/0/1/0/all/0/1&quot;&gt;Michael Arbel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1&quot;&gt;Arthur Gretton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01543">
<title>Early stopping for kernel boosting algorithms: A general analysis with localized complexities. (arXiv:1707.01543v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01543</link>
<description rdf:parseType="Literal">&lt;p&gt;Early stopping of iterative algorithms is a widely-used form of
regularization in statistics, commonly used in conjunction with boosting and
related gradient-type algorithms. Although consistency results have been
established in some settings, such estimators are less well-understood than
their analogues based on penalized regularization. In this paper, for a
relatively broad class of loss functions and boosting algorithms (including
L2-boost, LogitBoost and AdaBoost, among others), we exhibit a direct
connection between the performance of a stopped iterate and the localized
Gaussian complexity of the associated function class. This connection allows us
to show that local fixed point analysis of Gaussian or Rademacher complexities,
now standard in the analysis of penalized estimators, can be used to derive
optimal stopping rules. We derive such stopping rules in detail for various
kernel classes, and illustrate the correspondence of our theory with practice
for Sobolev kernel classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yuting Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fanny Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wainwright_M/0/1/0/all/0/1&quot;&gt;Martin J. Wainwright&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.02711">
<title>Topology Reduction in Deep Convolutional Feature Extraction Networks. (arXiv:1707.02711v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.02711</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional neural networks (CNNs) used in practice employ potentially
hundreds of layers and $10$,$000$s of nodes. Such network sizes entail
significant computational complexity due to the large number of convolutions
that need to be carried out; in addition, a large number of parameters needs to
be learned and stored. Very deep and wide CNNs may therefore not be well suited
to applications operating under severe resource constraints as is the case,
e.g., in low-power embedded and mobile platforms. This paper aims at
understanding the impact of CNN topology, specifically depth and width, on the
network&apos;s feature extraction capabilities. We address this question for the
class of scattering networks that employ either Weyl-Heisenberg filters or
wavelets, the modulus non-linearity, and no pooling. The exponential feature
map energy decay results in Wiatowski et al., 2017, are generalized to
$\mathcal{O}(a^{-N})$, where an arbitrary decay factor $a&amp;gt;1$ can be realized
through suitable choice of the Weyl-Heisenberg prototype function or the mother
wavelet. We then show how networks of fixed (possibly small) depth $N$ can be
designed to guarantee that $((1-\varepsilon)\cdot 100)\%$ of the input signal&apos;s
energy are contained in the feature vector. Based on the notion of
operationally significant nodes, we characterize, partly rigorously and partly
heuristically, the topology-reducing effects of (effectively) band-limited
input signals, band-limited filters, and feature map symmetries. Finally, for
networks based on Weyl-Heisenberg filters, we determine the prototype function
bandwidth that minimizes---for fixed network depth $N$---the average number of
operationally significant nodes per layer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wiatowski_T/0/1/0/all/0/1&quot;&gt;Thomas Wiatowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grohs_P/0/1/0/all/0/1&quot;&gt;Philipp Grohs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bolcskei_H/0/1/0/all/0/1&quot;&gt;Helmut B&amp;#xf6;lcskei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06034">
<title>Stochastic Variance Reduction for Policy Gradient Estimation. (arXiv:1710.06034v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06034</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in policy gradient methods and deep learning have
demonstrated their applicability for complex reinforcement learning problems.
However, the variance of the performance gradient estimates obtained from the
simulation is often excessive, leading to poor sample efficiency. In this
paper, we apply the stochastic variance reduced gradient descent (SVRG) to
model-free policy gradient to significantly improve the sample-efficiency. The
SVRG estimation is incorporated into a trust-region Newton conjugate gradient
framework for the policy optimization. On several Mujoco tasks, our method
achieves significantly better performance compared to the state-of-the-art
model-free policy gradient methods in robotic continuous control such as trust
region policy optimization (TRPO)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianbing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jian Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.09514">
<title>Asymptotic Analysis via Stochastic Differential Equations of Gradient Descent Algorithms in Statistical and Computational Paradigms. (arXiv:1711.09514v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.09514</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates asymptotic behaviors of gradient descent algorithms
(particularly accelerated gradient descent and stochastic gradient descent) in
the context of stochastic optimization arose in statistics and machine learning
where objective functions are estimated from available data. We show that these
algorithms can be modeled by continuous-time ordinary or stochastic
differential equations, and their asymptotic dynamic evolutions and
distributions are governed by some linear ordinary or stochastic differential
equations, as the data size goes to infinity. We illustrate that our study can
provide a novel unified framework for a joint computational and statistical
asymptotic analysis on dynamic behaviors of these algorithms with the time (or
the number of iterations in the algorithms) and large sample behaviors of the
statistical decision rules (like estimators and classifiers) that the
algorithms are applied to compute, where the statistical decision rules are the
limits of the random sequences generated from these iterative algorithms as the
number of iterations goes to infinity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yazhen Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10388">
<title>Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram Completion. (arXiv:1711.10388v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10388</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed Tomography (CT) reconstruction is a fundamental component to a wide
variety of applications ranging from security, to healthcare. The classical
techniques require measuring projections, called sinograms, from a full
180$^\circ$ view of the object. This is impractical in a limited angle
scenario, when the viewing angle is less than 180$^\circ$, which can occur due
to different factors including restrictions on scanning time, limited
flexibility of scanner rotation, etc. The sinograms obtained as a result, cause
existing techniques to produce highly artifact-laden reconstructions. In this
paper, we propose to address this problem through implicit sinogram completion,
on a challenging real world dataset containing scans of common checked-in
luggage. We propose a system, consisting of 1D and 2D convolutional neural
networks, that operates on a limited angle sinogram to directly produce the
best estimate of a reconstruction. Next, we use the x-ray transform on this
reconstruction to obtain a &quot;completed&quot; sinogram, as if it came from a full
180$^\circ$ measurement. We feed this to standard analytical and iterative
reconstruction techniques to obtain the final reconstruction. We show with
extensive experimentation that this combined strategy outperforms many
competitive baselines. We also propose a measure of confidence for the
reconstruction that enables a practitioner to gauge the reliability of a
prediction made by our network. We show that this measure is a strong indicator
of quality as measured by the PSNR, while not requiring ground truth at test
time. Finally, using a segmentation experiment, we show that our reconstruction
preserves the 3D structure of objects effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1&quot;&gt;Rushil Anirudh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyojin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1&quot;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_K/0/1/0/all/0/1&quot;&gt;K. Aditya Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Champley_K/0/1/0/all/0/1&quot;&gt;Kyle Champley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bremer_T/0/1/0/all/0/1&quot;&gt;Timo Bremer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.01587">
<title>SpectralNet: Spectral Clustering using Deep Neural Networks. (arXiv:1801.01587v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.01587</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral clustering is a leading and popular technique in unsupervised data
analysis. Two of its major limitations are scalability and generalization of
the spectral embedding (i.e., out-of-sample-extension). In this paper we
introduce a deep learning approach to spectral clustering that overcomes the
above shortcomings. Our network, which we call SpectralNet, learns a map that
embeds input data points into the eigenspace of their associated graph
Laplacian matrix and subsequently clusters them. We train SpectralNet using a
procedure that involves constrained stochastic optimization. Stochastic
optimization allows it to scale to large datasets, while the constraints, which
are implemented using a special-purpose output layer, allow us to keep the
network output orthogonal. Moreover, the map learned by SpectralNet naturally
generalizes the spectral embedding to unseen data points. To further improve
the quality of the clustering, we replace the standard pairwise Gaussian
affinities with affinities leaned from unlabeled data using a Siamese network.
Additional improvement can be achieved by applying the network to code
representations produced, e.g., by standard autoencoders. Our end-to-end
learning procedure is fully unsupervised. In addition, we apply VC dimension
theory to derive a lower bound on the size of SpectralNet. State-of-the-art
clustering results are reported on the Reuters dataset. Our implementation is
publicly available at https://github.com/kstant0725/SpectralNet .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shaham_U/0/1/0/all/0/1&quot;&gt;Uri Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stanton_K/0/1/0/all/0/1&quot;&gt;Kelly Stanton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Henry Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nadler_B/0/1/0/all/0/1&quot;&gt;Boaz Nadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Basri_R/0/1/0/all/0/1&quot;&gt;Ronen Basri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kluger_Y/0/1/0/all/0/1&quot;&gt;Yuval Kluger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.08694">
<title>PDNet: Semantic Segmentation integrated with a Primal-Dual Network for Document binarization. (arXiv:1801.08694v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.08694</link>
<description rdf:parseType="Literal">&lt;p&gt;Binarization of digital documents is the task of classifying each pixel in an
image of the document as belonging to the background (parchment/paper) or
foreground (text/ink). Historical documents are often subject to degradations,
that make the task challenging. In the current work a deep neural network
architecture is proposed that combines a fully convolutional network with an
unrolled primal-dual network that can be trained end-to-end in order to achieve
state of the art binarization on four out of seven datasets. Document
binarization is formulated as a energy minimization problem. A fully
convolutional neural network is trained for semantic labeling of pixels to
provide class labeling cost associated with each pixel. This cost estimate is
refined along the edges to compensate for any over or under estimation of the
under represented fore-ground class using a primal-dual approach. We provide
necessary overview on proximal operator that facilitates theoretical
underpinning in order to train a primal-dual network using a gradient descent
algorithm. Numerical instabilities encountered due to the recurrent nature of
primal-dual approach are handled. We provide experimental results on document
binarization competition dataset along with network changes and hyperparameter
tuning required for stability and performance of the network. The network when
pre-trained on synthetic dataset performs better as per the competition
metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ayyalasomayajula_K/0/1/0/all/0/1&quot;&gt;Kalyan Ram Ayyalasomayajula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Malmberg_F/0/1/0/all/0/1&quot;&gt;Filip Malmberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brun_A/0/1/0/all/0/1&quot;&gt;Anders Brun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.01751">
<title>Near-Optimal Coresets of Kernel Density Estimates. (arXiv:1802.01751v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.01751</link>
<description rdf:parseType="Literal">&lt;p&gt;We construct near-optimal coresets for kernel density estimate for points in
$\mathbb{R^d}$ when the kernel is positive definite. Specifically we show a
polynomial time construction for a coreset of size $O(\sqrt{d\log
(1/\epsilon)}/\epsilon)$, and we show a near-matching lower bound of size
$\Omega(\sqrt{d}/\epsilon)$. The upper bound is a polynomial in $1/\epsilon$
improvement when $d \in [3,1/\epsilon^2)$ (for all kernels except the Gaussian
kernel which had a previous upper bound of $O((1/\epsilon) \log^d
(1/\epsilon))$) and the lower bound is the first known lower bound to depend on
$d$ for this problem. Moreover, the upper bound restriction that the kernel is
positive definite is significant in that it applies to a wide-variety of
kernels, specifically those most important for machine learning. This includes
kernels for information distances and the sinc kernel which can be negative.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1&quot;&gt;Jeff M. Phillips&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_W/0/1/0/all/0/1&quot;&gt;Wai Ming Tai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02840">
<title>Neural Network Renormalization Group. (arXiv:1802.02840v2 [cond-mat.stat-mech] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02840</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a variational renormalization group approach using deep generative
model composed of bijectors. The model can learn hierarchical transformations
between physical variables and renormalized collective variables. It can
directly generate statistically independent physical configurations by
iterative refinement at various length scales. The generative model has an
exact and tractable likelihood, which provides renormalized energy function of
the collective variables and supports unbiased rejection sampling of the
physical variables. To train the neural network, we employ probability density
distillation, in which the training loss is a variational upper bound of the
physical free energy. The approach could be useful for automatically
identifying collective variables and effective field theories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuo-Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07329">
<title>Bayesian Incremental Learning for Deep Neural Networks. (arXiv:1802.07329v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07329</link>
<description rdf:parseType="Literal">&lt;p&gt;In industrial machine learning pipelines, data often arrive in parts.
Particularly in the case of deep neural networks, it may be too expensive to
train the model from scratch each time, so one would rather use a previously
learned model and the new data to improve performance. However, deep neural
networks are prone to getting stuck in a suboptimal solution when trained on
only new data as compared to the full dataset. Our work focuses on a continuous
learning setup where the task is always the same and new parts of data arrive
sequentially. We apply a Bayesian approach to update the posterior
approximation with each new piece of data and find this method to outperform
the traditional approach in our experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kochurov_M/0/1/0/all/0/1&quot;&gt;Max Kochurov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Garipov_T/0/1/0/all/0/1&quot;&gt;Timur Garipov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Podoprikhin_D/0/1/0/all/0/1&quot;&gt;Dmitry Podoprikhin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Molchanov_D/0/1/0/all/0/1&quot;&gt;Dmitry Molchanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ashukha_A/0/1/0/all/0/1&quot;&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vetrov_D/0/1/0/all/0/1&quot;&gt;Dmitry Vetrov&lt;/a&gt;</dc:creator>
</item></rdf:RDF>