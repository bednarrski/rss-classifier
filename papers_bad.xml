<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-23T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07979"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.05300"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07868"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07984"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07991"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08058"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08060"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08173"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08204"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08229"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08237"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08265"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08430"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08447"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08484"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08709"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08716"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08725"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.00848"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.00146"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.07846"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01690"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.08024"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.00325"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09521"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03633"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07978"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07987"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08046"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08088"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08091"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08158"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08362"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08409"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08446"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08465"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08534"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.08596"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.07272"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.02041"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.02436"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.02757"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.03494"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.09430"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11423"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07420"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06818"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06118"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.04874"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06576"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.09772"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00728"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.09429"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07610"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.07979">
<title>Multi-criteria Evolution of Neural Network Topologies: Balancing Experience and Performance in Autonomous Systems. (arXiv:1807.07979v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.07979</link>
<description rdf:parseType="Literal">&lt;p&gt;Majority of Artificial Neural Network (ANN) implementations in autonomous
systems use a fixed/user-prescribed network topology, leading to sub-optimal
performance and low portability. The existing neuro-evolution of augmenting
topology or NEAT paradigm offers a powerful alternative by allowing the network
topology and the connection weights to be simultaneously optimized through an
evolutionary process. However, most NEAT implementations allow the
consideration of only a single objective. There also persists the question of
how to tractably introduce topological diversification that mitigates
overfitting to training scenarios. To address these gaps, this paper develops a
multi-objective neuro-evolution algorithm. While adopting the basic elements of
NEAT, important modifications are made to the selection, speciation, and
mutation processes. With the backdrop of small-robot path-planning
applications, an experience-gain criterion is derived to encapsulate the amount
of diverse local environment encountered by the system. This criterion
facilitates the evolution of genes that support exploration, thereby seeking to
generalize from a smaller set of mission scenarios than possible with
performance maximization alone. The effectiveness of the single-objective
(optimizing performance) and the multi-objective (optimizing performance and
experience-gain) neuro-evolution approaches are evaluated on two different
small-robot cases, with ANNs obtained by the multi-objective optimization
observed to provide superior performance in unseen scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chidambaran_S/0/1/0/all/0/1&quot;&gt;Sharat Chidambaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behjat_A/0/1/0/all/0/1&quot;&gt;Amir Behjat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1&quot;&gt;Souma Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.05300">
<title>Reverse Curriculum Generation for Reinforcement Learning. (arXiv:1707.05300v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1707.05300</link>
<description rdf:parseType="Literal">&lt;p&gt;Many relevant tasks require an agent to reach a certain state, or to
manipulate objects into a desired configuration. For example, we might want a
robot to align and assemble a gear onto an axle or insert and turn a key in a
lock. These goal-oriented tasks present a considerable challenge for
reinforcement learning, since their natural reward function is sparse and
prohibitive amounts of exploration are required to reach the goal and receive
some learning signal. Past approaches tackle these problems by exploiting
expert demonstrations or by manually designing a task-specific reward shaping
function to guide the learning agent. Instead, we propose a method to learn
these tasks without requiring any prior knowledge other than obtaining a single
state in which the task is achieved. The robot is trained in reverse, gradually
learning to reach the goal from a set of start states increasingly far from the
goal. Our method automatically generates a curriculum of start states that
adapts to the agent&apos;s performance, leading to efficient training on
goal-oriented tasks. We demonstrate our approach on difficult simulated
navigation and fine-grained manipulation problems, not solvable by
state-of-the-art reinforcement learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Florensa_C/0/1/0/all/0/1&quot;&gt;Carlos Florensa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1&quot;&gt;David Held&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wulfmeier_M/0/1/0/all/0/1&quot;&gt;Markus Wulfmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Michael Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07868">
<title>The Deep Kernelized Autoencoder. (arXiv:1807.07868v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.07868</link>
<description rdf:parseType="Literal">&lt;p&gt;Autoencoders learn data representations (codes) in such a way that the input
is reproduced at the output of the network. However, it is not always clear
what kind of properties of the input data need to be captured by the codes.
Kernel machines have experienced great success by operating via inner-products
in a theoretically well-defined reproducing kernel Hilbert space, hence
capturing topological properties of input data. In this paper, we enhance the
autoencoder&apos;s ability to learn effective data representations by aligning inner
products between codes with respect to a kernel matrix. By doing so, the
proposed kernelized autoencoder allows learning similarity-preserving
embeddings of input data, where the notion of similarity is explicitly
controlled by the user and encoded in a positive semi-definite kernel matrix.
Experiments are performed for evaluating both reconstruction and kernel
alignment performance in classification tasks and visualization of
high-dimensional data. Additionally, we show that our method is capable to
emulate kernel principal component analysis on a denoising task, obtaining
competitive results at a much lower computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kampffmeyer_M/0/1/0/all/0/1&quot;&gt;Michael Kampffmeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lokse_S/0/1/0/all/0/1&quot;&gt;Sigurd L&amp;#xf8;kse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bianchi_F/0/1/0/all/0/1&quot;&gt;Filippo M. Bianchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jenssen_R/0/1/0/all/0/1&quot;&gt;Robert Jenssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Livi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Livi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07984">
<title>Attention Models in Graphs: A Survey. (arXiv:1807.07984v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.07984</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph-structured data arise naturally in many different application domains.
By representing data as graphs, we can capture entities (i.e., nodes) as well
as their relationships (i.e., edges) with each other. Many useful insights can
be derived from graph-structured data as demonstrated by an ever-growing body
of work focused on graph mining. However, in the real-world, graphs can be both
large - with many complex patterns - and noisy which can pose a problem for
effective graph mining. An effective way to deal with this issue is to
incorporate &quot;attention&quot; into graph mining solutions. An attention mechanism
allows a method to focus on task-relevant parts of the graph, helping it to
make better decisions. In this work, we conduct a comprehensive and focused
survey of the literature on the emerging field of graph attention models. We
introduce three intuitive taxonomies to group existing work. These are based on
problem setting (type of input and output), the type of attention mechanism
used, and the task (e.g., graph classification, link prediction, etc.). We
motivate our taxonomies through detailed examples and use each to survey
competing approaches from a unique standpoint. Finally, we highlight several
challenges in the area and discuss promising directions for future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;John Boaz Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1&quot;&gt;Ryan A. Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sungchul Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nesreen K. Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_E/0/1/0/all/0/1&quot;&gt;Eunyee Koh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07991">
<title>Knowledge Integration for Disease Characterization: A Breast Cancer Example. (arXiv:1807.07991v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.07991</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid advancements in cancer research, the information that is
useful for characterizing disease, staging tumors, and creating treatment and
survivorship plans has been changing at a pace that creates challenges when
physicians try to remain current. One example involves increasing usage of
biomarkers when characterizing the pathologic prognostic stage of a breast
tumor. We present our semantic technology approach to support cancer
characterization and demonstrate it in our end-to-end prototype system that
collects the newest breast cancer staging criteria from authoritative oncology
manuals to construct an ontology for breast cancer. Using a tool we developed
that utilizes this ontology, physician-facing applications can be used to
quickly stage a new patient to support identifying risks, treatment options,
and monitoring plans based on authoritative and best practice guidelines.
Physicians can also re-stage existing patients or patient populations, allowing
them to find patients whose stage has changed in a given patient cohort. As new
guidelines emerge, using our proposed mechanism, which is grounded by semantic
technologies for ingesting new data from staging manuals, we have created an
enriched cancer staging ontology that integrates relevant data from several
sources with very little human intervention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seneviratne_O/0/1/0/all/0/1&quot;&gt;Oshani Seneviratne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashid_S/0/1/0/all/0/1&quot;&gt;Sabbir M. Rashid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chari_S/0/1/0/all/0/1&quot;&gt;Shruthi Chari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCusker_J/0/1/0/all/0/1&quot;&gt;James P. McCusker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennett_K/0/1/0/all/0/1&quot;&gt;Kristin P. Bennett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendler_J/0/1/0/all/0/1&quot;&gt;James A. Hendler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGuinness_D/0/1/0/all/0/1&quot;&gt;Deborah L. McGuinness&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08048">
<title>Baidu Apollo EM Motion Planner. (arXiv:1807.08048v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1807.08048</link>
<description rdf:parseType="Literal">&lt;p&gt;In this manuscript, we introduce a real-time motion planning system based on
the Baidu Apollo (open source) autonomous driving platform. The developed
system aims to address the industrial level-4 motion planning problem while
considering safety, comfort and scalability. The system covers multilane and
single-lane autonomous driving in a hierarchical manner: (1) The top layer of
the system is a multilane strategy that handles lane-change scenarios by
comparing lane-level trajectories computed in parallel. (2) Inside the
lane-level trajectory generator, it iteratively solves path and speed
optimization based on a Frenet frame. (3) For path and speed optimization, a
combination of dynamic programming and spline-based quadratic programming is
proposed to construct a scalable and easy-to-tune framework to handle traffic
rules, obstacle decisions and smoothness simultaneously. The planner is
scalable to both highway and lower-speed city driving scenarios. We also
demonstrate the algorithm through scenario illustrations and on-road test
results.
&lt;/p&gt;
&lt;p&gt;The system described in this manuscript has been deployed to dozens of Baidu
Apollo autonomous driving vehicles since Apollo v1.5 was announced in September
2017. As of May 16th, 2018, the system has been tested under 3,380 hours and
approximately 68,000 kilometers (42,253 miles) of closed-loop autonomous
driving under various urban scenarios.
&lt;/p&gt;
&lt;p&gt;The algorithm described in this manuscript is available at
https://github.com/ApolloAuto/apollo/tree/master/modules/planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Haoyang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Changchun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liangliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_L/0/1/0/all/0/1&quot;&gt;Li Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Weicheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jiangtao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongye Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1&quot;&gt;Qi Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08058">
<title>Learning Heuristics for Automated Reasoning through Deep Reinforcement Learning. (arXiv:1807.08058v1 [cs.LO])</title>
<link>http://arxiv.org/abs/1807.08058</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate how to learn efficient heuristics for automated reasoning
algorithms through deep reinforcement learning. We consider search algorithms
for quantified Boolean logics, that already can solve formulas of impressive
size - up to 100s of thousands of variables. The main challenge is to find a
representation which lends to making predictions in a scalable way. The
heuristics learned through our approach significantly improve over the
handwritten heuristics for several sets of formulas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lederman_G/0/1/0/all/0/1&quot;&gt;Gil Lederman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabe_M/0/1/0/all/0/1&quot;&gt;Markus N. Rabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1&quot;&gt;Sanjit A. Seshia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08060">
<title>Safe Option-Critic: Learning Safety in the Option-Critic Architecture. (arXiv:1807.08060v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.08060</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing hierarchical reinforcement learning algorithms that induce a notion
of safety is not only vital for safety-critical applications, but also, brings
better understanding of an artificially intelligent agent&apos;s decisions. While
learning end-to-end options automatically has been fully realized recently, we
propose a solution to learning safe options. We introduce the idea of
controllability of states based on the temporal difference errors in the
option-critic framework. We then derive the policy-gradient theorem with
controllability and propose a novel framework called safe option-critic. We
demonstrate the effectiveness of our approach in the four-rooms grid-world,
cartpole, and three games in the Arcade Learning Environment (ALE): MsPacman,
Amidar and Q*Bert. Learning of end-to-end options with the proposed notion of
safety achieves reduction in the variance of return and boosts the performance
in environments with intrinsic variability in the reward structure. More
importantly, the proposed algorithm outperforms the vanilla options in all the
environments and primitive actions in two out of three ALE games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Arushi Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khetarpal_K/0/1/0/all/0/1&quot;&gt;Khimya Khetarpal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08173">
<title>Modeling Taxi Drivers&apos; Behaviour for the Next Destination Prediction. (arXiv:1807.08173v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.08173</link>
<description rdf:parseType="Literal">&lt;p&gt;Taxi destination prediction is a very important task for optimizing the
efficiency of electronic dispatching systems, thus allowing relevant advantages
for both taxi companies and customers. In fact, during periods of high demand,
there should be a taxi whose current ride will end near a requested pick up
location from a new customer. If an electronic dispatcher is able to know in
advance where all taxi drivers will end their current ride, it will also be
able to better allocate its resources, identifying which taxi to assign to each
call. Moreover, automatic systems for the taxi mobility monitoring collect data
that, integrated with other information sources, can help in understanding
daytime human mobility routines. In this paper, we introduce a novel approach
for addressing the taxi destination prediction problem, based on Recurrent
Neural Networks (RNNs) applied to a regression setting. RNNs are trained based
on the individual drivers&apos; history and on geographical information (i.e.,
points of interest), using only the starting point of each ride (with no
knowledge about the whole trajectory). The proposed approach was tested on the
dataset of the ECML/PKDD Discovery Challenge 2015 - based on the city of Porto
- obtaining better results with respect to the competition winner, whilst using
less information, and on Manhattan and San Francisco datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_A/0/1/0/all/0/1&quot;&gt;Alberto Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barlacchi_G/0/1/0/all/0/1&quot;&gt;Gianni Barlacchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bianchini_M/0/1/0/all/0/1&quot;&gt;Monica Bianchini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1&quot;&gt;Bruno Lepri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08204">
<title>Towards Neural Theorem Proving at Scale. (arXiv:1807.08204v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.08204</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural models combining representation learning and reasoning in an
end-to-end trainable manner are receiving increasing interest. However, their
use is severely limited by their computational complexity, which renders them
unusable on real world datasets. We focus on the Neural Theorem Prover (NTP)
model proposed by Rockt{\&quot;{a}}schel and Riedel (2017), a continuous relaxation
of the Prolog backward chaining algorithm where unification between terms is
replaced by the similarity between their embedding representations. For
answering a given query, this model needs to consider all possible proof paths,
and then aggregate results - this quickly becomes infeasible even for small
Knowledge Bases (KBs). We observe that we can accurately approximate the
inference process in this model by considering only proof paths associated with
the highest proof scores. This enables inference and learning on previously
impracticable KBs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1&quot;&gt;Pasquale Minervini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosnjak_M/0/1/0/all/0/1&quot;&gt;Matko Bosnjak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1&quot;&gt;Tim Rockt&amp;#xe4;schel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1&quot;&gt;Sebastian Riedel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08229">
<title>Optimal Continuous State POMDP Planning with Semantic Observations: A Variational Approach. (arXiv:1807.08229v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.08229</link>
<description rdf:parseType="Literal">&lt;p&gt;This work develops novel strategies for optimal planning with semantic
observations using continuous state Partially Observable Markov Decision
Processes (CPOMDPs). Two major innovations are presented in relation to
Gaussian mixture (GM) CPOMDP policy approximation methods. While existing
methods have many theoretically nice properties, they are hampered by the
inability to efficiently represent and reason over hybrid continuous-discrete
probabilistic models. The first major innovation is the derivation of
closed-form variational Bayes GM approximations of Point-Based Value Iteration
Bellman policy backups, using softmax models of continuous-discrete semantic
observation probabilities. A key benefit of this approach is that dynamic
decision-making tasks can be performed with complex non-Gaussian uncertainties,
while also exploiting continuous dynamic state space models (thus avoiding
cumbersome and costly discretization). The second major innovation is a new
clustering-based technique for mixture condensation that scales well to very
large GM policy functions and belief functions. Simulation results for a target
search and interception task with semantic observations show that the GM
policies resulting from these innovations are more effective than those
produced by other state of the art GM and Monte Carlo based policy
approximations, but require significantly less modeling overhead and runtime
cost. Additional results demonstrate the robustness of this approach to model
errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burks_L/0/1/0/all/0/1&quot;&gt;Luke Burks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loefgren_I/0/1/0/all/0/1&quot;&gt;Ian Loefgren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nisar Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08237">
<title>Learning Deep Hidden Nonlinear Dynamics from Aggregate Data. (arXiv:1807.08237v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08237</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning nonlinear dynamics from diffusion data is a challenging problem
since the individuals observed may be different at different time points,
generally following an aggregate behaviour. Existing work cannot handle the
tasks well since they model such dynamics either directly on observations or
enforce the availability of complete longitudinal individual-level
trajectories. However, in most of the practical applications, these
requirements are unrealistic: the evolving dynamics may be too complex to be
modeled directly on observations, and individual-level trajectories may not be
available due to technical limitations, experimental costs and/or privacy
issues. To address these challenges, we formulate a model of diffusion dynamics
as the {\em hidden stochastic process} via the introduction of hidden variables
for flexibility, and learn the hidden dynamics directly on {\em aggregate
observations} without any requirement for individual-level trajectories. We
propose a dynamic generative model with Wasserstein distance for LEarninG dEep
hidden Nonlinear Dynamics (LEGEND) and prove its theoretical guarantees as
well. Experiments on a range of synthetic and real-world datasets illustrate
that LEGEND has very strong performance compared to state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yisen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingkai Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xingjun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1&quot;&gt;Sarah Monazam Erfani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1&quot;&gt;James Bailey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Le Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08265">
<title>Deep learning at the shallow end: Malware classification for non-domain experts. (arXiv:1807.08265v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1807.08265</link>
<description rdf:parseType="Literal">&lt;p&gt;Current malware detection and classification approaches generally rely on
time consuming and knowledge intensive processes to extract patterns
(signatures) and behaviors from malware, which are then used for
identification. Moreover, these signatures are often limited to local,
contiguous sequences within the data whilst ignoring their context in relation
to each other and throughout the malware file as a whole. We present a Deep
Learning based malware classification approach that requires no expert domain
knowledge and is based on a purely data driven approach for complex pattern and
feature identification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quan Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boydell_O/0/1/0/all/0/1&quot;&gt;Ois&amp;#xed;n Boydell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1&quot;&gt;Brian Mac Namee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scanlon_M/0/1/0/all/0/1&quot;&gt;Mark Scanlon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08430">
<title>Actor-Action Semantic Segmentation with Region Masks. (arXiv:1807.08430v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.08430</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the actor-action semantic segmentation problem, which
requires joint labeling of both actor and action categories in video frames.
One major challenge for this task is that when an actor performs an action,
different body parts of the actor provide different types of cues for the
action category and may receive inconsistent action labeling when they are
labeled independently. To address this issue, we propose an end-to-end
region-based actor-action segmentation approach which relies on region masks
from an instance segmentation algorithm. Our main novelty is to avoid labeling
pixels in a region mask independently - instead we assign a single action label
to these pixels to achieve consistent action labeling. When a pixel belongs to
multiple region masks, max pooling is applied to resolve labeling conflicts.
Our approach uses a two-stream network as the front-end (which learns features
capturing both appearance and motion information), and uses two region-based
segmentation networks as the back-end (which takes the fused features from the
two-stream network as the input and predicts actor-action labeling).
Experiments on the A2D dataset demonstrate that both the region-based
segmentation strategy and the fused features from the two-stream network
contribute to the performance improvements. The proposed approach outperforms
the state-of-the-art results by more than 8% in mean class accuracy, and more
than 5% in mean class IOU, which validates its effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_K/0/1/0/all/0/1&quot;&gt;Kang Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chunluan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhigang Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoy_M/0/1/0/all/0/1&quot;&gt;Michael Hoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dauwels_J/0/1/0/all/0/1&quot;&gt;Justin Dauwels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Junsong Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08447">
<title>LinkNBed: Multi-Graph Representation Learning with Entity Linkage. (arXiv:1807.08447v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08447</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs have emerged as an important model for studying complex
multi-relational data. This has given rise to the construction of numerous
large scale but incomplete knowledge graphs encoding information extracted from
various resources. An effective and scalable approach to jointly learn over
multiple graphs and eventually construct a unified graph is a crucial next step
for the success of knowledge-based inference for many downstream applications.
To this end, we propose LinkNBed, a deep relational learning framework that
learns entity and relationship representations across multiple graphs. We
identify entity linkage across graphs as a vital component to achieve our goal.
We design a novel objective that leverage entity linkage and build an efficient
multi-task training procedure. Experiments on link prediction and entity
linkage demonstrate substantial improvements over the state-of-the-art
relational learning approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_R/0/1/0/all/0/1&quot;&gt;Rakshit Trivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sisman_B/0/1/0/all/0/1&quot;&gt;Bunyamin Sisman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faloutsos_C/0/1/0/all/0/1&quot;&gt;Christos Faloutsos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xin Luna Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08484">
<title>AceKG: A Large-scale Knowledge Graph for Academic Data Mining. (arXiv:1807.08484v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.08484</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing knowledge graphs (KGs) in academic domains suffer from problems
of insufficient multi-relational information, name ambiguity and improper data
format for large-scale machine pro- cessing. In this paper, we present AceKG, a
new large-scale KG in academic domain. AceKG not only provides clean academic
information, but also offers a large-scale benchmark dataset for researchers to
conduct challenging data mining projects including link prediction, community
detection and scholar classification. Specifically, AceKG describes 3.13
billion triples of academic facts based on a consistent ontology, including
necessary properties of papers, authors, fields of study, venues and
institutes, as well as the relations among them. To enrich the proposed
knowledge graph, we also perform entity alignment with existing databases and
rule-based inference. Based on AceKG, we conduct experiments of three typical
academic data mining tasks and evaluate several state-of- the-art knowledge
embedding and network representation learning approaches on the benchmark
datasets built from AceKG. Finally, we discuss several promising research
directions that benefit from AceKG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yuchen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jialu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yuting Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ye Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinbing Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08709">
<title>The Vadalog System: Datalog-based Reasoning for Knowledge Graphs. (arXiv:1807.08709v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1807.08709</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past years, there has been a resurgence of Datalog-based systems in
the database community as well as in industry. In this context, it has been
recognized that to handle the complex knowl\-edge-based scenarios encountered
today, such as reasoning over large knowledge graphs, Datalog has to be
extended with features such as existential quantification. Yet, Datalog-based
reasoning in the presence of existential quantification is in general
undecidable. Many efforts have been made to define decidable fragments. Warded
Datalog+/- is a very promising one, as it captures PTIME complexity while
allowing ontological reasoning. Yet so far, no implementation of Warded
Datalog+/- was available. In this paper we present the Vadalog system, a
Datalog-based system for performing complex logic reasoning tasks, such as
those required in advanced knowledge graphs. The Vadalog system is Oxford&apos;s
contribution to the VADA research programme, a joint effort of the universities
of Oxford, Manchester and Edinburgh and around 20 industrial partners. As the
main contribution of this paper, we illustrate the first implementation of
Warded Datalog+/-, a high-performance Datalog+/- system utilizing an aggressive
termination control strategy. We also provide a comprehensive experimental
evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellomarini_L/0/1/0/all/0/1&quot;&gt;Luigi Bellomarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottlob_G/0/1/0/all/0/1&quot;&gt;Georg Gottlob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sallinger_E/0/1/0/all/0/1&quot;&gt;Emanuel Sallinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08716">
<title>NullaNet: Training Deep Neural Networks for Reduced-Memory-Access Inference. (arXiv:1807.08716v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08716</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have been successfully deployed in a wide variety of
applications including computer vision and speech recognition. However,
computational and storage complexity of these models has forced the majority of
computations to be performed on high-end computing platforms or on the cloud.
To cope with computational and storage complexity of these models, this paper
presents a training method that enables a radically different approach for
realization of deep neural networks through Boolean logic minimization. The
aforementioned realization completely removes the energy-hungry step of
accessing memory for obtaining model parameters, consumes about two orders of
magnitude fewer computing resources compared to realizations that use
floatingpoint operations, and has a substantially lower latency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazemi_M/0/1/0/all/0/1&quot;&gt;Mahdi Nazemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasandi_G/0/1/0/all/0/1&quot;&gt;Ghasem Pasandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1&quot;&gt;Massoud Pedram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08725">
<title>Scalable Tensor Completion with Nonconvex Regularization. (arXiv:1807.08725v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08725</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-rank tensor completion problem aims to recover a tensor from limited
observations, which has many real-world applications. Due to the easy
optimization, the convex overlapping nuclear norm has been popularly used for
tensor completion. However, it over-penalizes top singular values and lead to
biased estimations. In this paper, we propose to use the nonconvex regularizer,
which can less penalize large singular values, instead of the convex one for
tensor completion. However, as the new regularizer is nonconvex and overlapped
with each other, existing algorithms are either too slow or suffer from the
huge memory cost. To address these issues, we develop an efficient and scalable
algorithm, which is based on the proximal average (PA) algorithm, for
real-world problems. Compared with the direct usage of PA algorithm, the
proposed algorithm runs orders faster and needs orders less space. We further
speed up the proposed algorithm with the acceleration technique, and show the
convergence to critical points is still guaranteed. Experimental comparisons of
the proposed approach are made with various other tensor completion approaches.
Empirical results show that the proposed algorithm is very fast and can produce
much better recovery performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1&quot;&gt;Quanming Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.00848">
<title>Unsupervised Image-to-Image Translation Networks. (arXiv:1703.00848v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1703.00848</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised image-to-image translation aims at learning a joint distribution
of images in different domains by using images from the marginal distributions
in individual domains. Since there exists an infinite set of joint
distributions that can arrive the given marginal distributions, one could infer
nothing about the joint distribution from the marginal distributions without
additional assumptions. To address the problem, we make a shared-latent space
assumption and propose an unsupervised image-to-image translation framework
based on Coupled GANs. We compare the proposed framework with competing
approaches and present high quality image translation results on various
challenging unsupervised image translation tasks, including street scene image
translation, animal image translation, and face image translation. We also
apply the proposed framework to domain adaptation and achieve state-of-the-art
performance on benchmark datasets. Code and additional results are available in
https://github.com/mingyuliutw/unit .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming-Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1&quot;&gt;Thomas Breuel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1&quot;&gt;Jan Kautz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.00146">
<title>Large-Scale Low-Rank Matrix Learning with Nonconvex Regularizers. (arXiv:1708.00146v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1708.00146</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-rank modeling has many important applications in computer vision and
machine learning. While the matrix rank is often approximated by the convex
nuclear norm, the use of nonconvex low-rank regularizers has demonstrated
better empirical performance. However, the resulting optimization problem is
much more challenging. Recent state-of-the-art requires an expensive full SVD
in each iteration. In this paper, we show that for many commonly-used nonconvex
low-rank regularizers, a cutoff can be derived to automatically threshold the
singular values obtained from the proximal operator. This allows such operator
being efficiently approximated by power method. Based on it, we develop a
proximal gradient algorithm (and its accelerated variant) with inexact proximal
splitting and prove that a convergence rate of O(1/T) where T is the number of
iterations is guaranteed. Furthermore, we show the proposed algorithm can be
well parallelized, which achieves nearly linear speedup w.r.t the number of
threads. Extensive experiments are performed on matrix completion and robust
principal component analysis, which shows a significant speedup over the
state-of-the-art. Moreover, the matrix solution obtained is more accurate and
has a lower rank than that of the nuclear norm regularizer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1&quot;&gt;Quanming Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1&quot;&gt;James T.Kwok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Taifeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tie-Yan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.07846">
<title>Cross-Modality Synthesis from CT to PET using FCN and GAN Networks for Improved Automated Lesion Detection. (arXiv:1802.07846v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1802.07846</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we present a novel system for generation of virtual PET images
using CT scans. We combine a fully convolutional network (FCN) with a
conditional generative adversarial network (GAN) to generate simulated PET data
from given input CT data. The synthesized PET can be used for false-positive
reduction in lesion detection solutions. Clinically, such solutions may enable
lesion detection and drug treatment evaluation in a CT-only environment, thus
reducing the need for the more expensive and radioactive PET/CT scan. Our
dataset includes 60 PET/CT scans from Sheba Medical center. We used 23 scans
for training and 37 for testing. Different schemes to achieve the synthesized
output were qualitatively compared. Quantitative evaluation was conducted using
an existing lesion detection software, combining the synthesized PET as a false
positive reduction layer for the detection of malignant lesions in the liver.
Current results look promising showing a 28% reduction in the average false
positive per case from 2.9 to 2.1. The suggested solution is comprehensive and
can be expanded to additional body organs, and different modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Cohen_A/0/1/0/all/0/1&quot;&gt;Avi Ben-Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klang_E/0/1/0/all/0/1&quot;&gt;Eyal Klang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raskin_S/0/1/0/all/0/1&quot;&gt;Stephen P. Raskin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soffer_S/0/1/0/all/0/1&quot;&gt;Shelly Soffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Haim_S/0/1/0/all/0/1&quot;&gt;Simona Ben-Haim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konen_E/0/1/0/all/0/1&quot;&gt;Eli Konen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amitai_M/0/1/0/all/0/1&quot;&gt;Michal Marianne Amitai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenspan_H/0/1/0/all/0/1&quot;&gt;Hayit Greenspan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01690">
<title>Memory, Search and Sense: A Theory about Nesting and Abstraction. (arXiv:1803.01690v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01690</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes an automatic process for combining patterns and
features, to guide a search process and reason about it. It is based on the
functionality that a human brain might have, which is a highly distributed
network of simple neuronal components that can apply some level of matching and
cross-referencing over retrieved patterns. The process uses memory in a more
dynamic way and it can realise results using a shallow hierarchy, which is a
recognised brain-like construct. The paper gives one example of the process,
using computer chess as a case study. The second half of the paper then
presents a formal language for describing the global pattern sequences and
transitions. These pattern ensembles are created from the same techniques that
the search and prediction processes require and they define an outer framework
that a distributed setup can try to learn. They can also be created
automatically, resulting in further functionality for the generic cognitive
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greer_K/0/1/0/all/0/1&quot;&gt;Kieran Greer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.08024">
<title>Stacked Cross Attention for Image-Text Matching. (arXiv:1803.08024v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1803.08024</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of image-text matching. Inferring the
latent semantic alignment between objects or other salient stuff (e.g. snow,
sky, lawn) and the corresponding words in sentences allows to capture
fine-grained interplay between vision and language, and makes image-text
matching more interpretable. Prior work either simply aggregates the similarity
of all possible pairs of regions and words without attending differentially to
more and less important words or regions, or uses a multi-step attentional
process to capture limited number of semantic alignments which is less
interpretable. In this paper, we present Stacked Cross Attention to discover
the full latent alignments using both image regions and words in a sentence as
context and infer image-text similarity. Our approach achieves the
state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K,
our approach outperforms the current best methods by 22.1% relatively in text
retrieval from image query, and 18.2% relatively in image retrieval with text
query (based on Recall@1). On MS-COCO, our approach improves sentence retrieval
by 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1
using the 5K test set). Code has been made available at:
https://github.com/kuanghuei/SCAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kuang-Huei Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1&quot;&gt;Gang Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Houdong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaodong He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.00325">
<title>Aggregated Momentum: Stability Through Passive Damping. (arXiv:1804.00325v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.00325</link>
<description rdf:parseType="Literal">&lt;p&gt;Momentum is a simple and widely used trick which allows gradient-based
optimizers to pick up speed along low curvature directions. Its performance
depends crucially on a damping coefficient $\beta$. Large $\beta$ values can
potentially deliver much larger speedups, but are prone to oscillations and
instability; hence one typically resorts to small values such as 0.5 or 0.9. We
propose Aggregated Momentum (AggMo), a variant of momentum which combines
multiple velocity vectors with different $\beta$ parameters. AggMo is trivial
to implement, but significantly dampens oscillations, enabling it to remain
stable even for aggressive $\beta$ values such as 0.999. We reinterpret
Nesterov&apos;s accelerated gradient descent as a special case of AggMo and analyze
rates of convergence for quadratic objectives. Empirically, we find that AggMo
is a suitable drop-in replacement for other momentum methods, and frequently
delivers faster convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_J/0/1/0/all/0/1&quot;&gt;James Lucas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shengyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1&quot;&gt;Roger Grosse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09521">
<title>Fair Division Under Cardinality Constraints. (arXiv:1804.09521v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/1804.09521</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of fairly allocating indivisible goods, among agents,
under cardinality constraints and additive valuations. In this setting, we are
given a partition of the entire set of goods---i.e., the goods are
categorized---and a limit is specified on the number of goods that can be
allocated from each category to any agent. The objective here is to find a fair
allocation in which the subset of goods assigned to any agent satisfies the
given cardinality constraints. This problem naturally captures a number of
resource-allocation applications, and is a generalization of the well-studied
(unconstrained) fair division problem.
&lt;/p&gt;
&lt;p&gt;The two central notions of fairness, in the context of fair division of
indivisible goods, are envy freeness up to one good (EF1) and the (approximate)
maximin share guarantee (MMS). We show that the existence and algorithmic
guarantees established for these solution concepts in the unconstrained setting
can essentially be achieved under cardinality constraints. Specifically, we
develop efficient algorithms which compute EF1 and approximately MMS
allocations in the constrained setting.
&lt;/p&gt;
&lt;p&gt;Furthermore, focusing on the case wherein all the agents have the same
additive valuation, we establish that EF1 allocations exist and can be computed
efficiently even under matroid constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barman_S/0/1/0/all/0/1&quot;&gt;Siddharth Barman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1&quot;&gt;Arpita Biswas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03633">
<title>Interpretable Patient Mortality Prediction with Multi-value Rule Sets. (arXiv:1807.03633v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03633</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a Multi-vAlue Rule Set (MRS) model for in-hospital predicting
patient mortality. Compared to rule sets built from single-valued rules, MRS
adopts a more generalized form of association rules that allows multiple values
in a condition. Rules of this form are more concise than classical
single-valued rules in capturing and describing patterns in data. Our
formulation also pursues a higher efficiency of feature utilization, which
reduces possible cost in data collection and storage. We propose a Bayesian
framework for formulating a MRS model and propose an efficient inference method
for learning a maximum \emph{a posteriori}, incorporating theoretically
grounded bounds to iteratively reduce the search space and improve the search
efficiency. Experiments show that our model was able to achieve better
performance than baseline method including the current system used by the
hospital.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allareddy_V/0/1/0/all/0/1&quot;&gt;Veerajalandhar Allareddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rampa_S/0/1/0/all/0/1&quot;&gt;Sankeerth Rampa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allareddy_V/0/1/0/all/0/1&quot;&gt;Veerasathpurush Allareddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07978">
<title>Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors. (arXiv:1807.07978v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.07978</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a framework that unifies the existing work on black-box
adversarial example generation. We demonstrate that the current state of the
art in the field is optimal in a certain natural sense. Despite this
optimality, we show how to improve black-box attacks by bringing a new element
into the problem: ambient priors for the gradient. We identify two such priors,
and give an algorithm based on bandit optimization that allows for seamless
integration of these and other priors. Our framework leads to methods that are
two to three times more query-efficient and two to three times smaller failure
rate than the state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ilyas_A/0/1/0/all/0/1&quot;&gt;Andrew Ilyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Engstrom_L/0/1/0/all/0/1&quot;&gt;Logan Engstrom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Madry_A/0/1/0/all/0/1&quot;&gt;Aleksander Madry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07987">
<title>Deep Learning. (arXiv:1807.07987v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.07987</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) is a high dimensional data reduction technique for
constructing high-dimensional predictors in input-output models. DL is a form
of machine learning that uses hierarchical layers of latent features. In this
article, we review the state-of-the-art of deep learning from a modeling and
algorithmic perspective. We provide a list of successful areas of applications
in Artificial Intelligence (AI), Image Processing, Robotics and Automation.
Deep learning is predictive in its nature rather then inferential and can be
viewed as a black-box methodology for high-dimensional function estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Polson_N/0/1/0/all/0/1&quot;&gt;Nicholas G. Polson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sokolov_V/0/1/0/all/0/1&quot;&gt;Vadim O. Sokolov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08046">
<title>A Fast, Principled Working Set Algorithm for Exploiting Piecewise Linear Structure in Convex Problems. (arXiv:1807.08046v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.08046</link>
<description rdf:parseType="Literal">&lt;p&gt;By reducing optimization to a sequence of smaller subproblems, working set
algorithms achieve fast convergence times for many machine learning problems.
Despite such performance, working set implementations often resort to
heuristics to determine subproblem size, makeup, and stopping criteria. We
propose BlitzWS, a working set algorithm with useful theoretical guarantees.
Our theory relates subproblem size and stopping criteria to the amount of
progress during each iteration. This result motivates strategies for optimizing
algorithmic parameters and discarding irrelevant components as BlitzWS
progresses toward a solution. BlitzWS applies to many convex problems,
including training L1-regularized models and support vector machines. We
showcase this versatility with empirical comparisons, which demonstrate BlitzWS
is indeed a fast algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Johnson_T/0/1/0/all/0/1&quot;&gt;Tyler B. Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guestrin_C/0/1/0/all/0/1&quot;&gt;Carlos Guestrin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08088">
<title>Learning Optimal Resource Allocations in Wireless Systems. (arXiv:1807.08088v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08088</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers the design of optimal resource allocation policies in
wireless communication systems which are generically modeled as a functional
optimization problems with stochastic constraints. These optimization problems
have the structure of a learning problem in which the statistical loss appears
as a constraint motivating the development of learning methodologies to attempt
their solution. To handle stochastic constraints, training is undertaken in the
dual domain. It is shown that this can be done with small loss of optimality
when using near-universal learning parameterizations. In particular, since deep
neural networks (DNN) are near-universal their use is advocated and explored.
DNNs are trained here with a model-free primal-dual method that simultaneously
learns a DNN parametrization of the resource allocation policy and optimizes
the primal and dual variables. Numerical simulations demonstrate the strong
performance of the proposed approach on a number of common wireless resource
allocation problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisen_M/0/1/0/all/0/1&quot;&gt;Mark Eisen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Clark Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chamon_L/0/1/0/all/0/1&quot;&gt;Luiz F. O. Chamon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Daniel D. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Alejandro Ribeiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08091">
<title>Streaming Methods for Restricted Strongly Convex Functions with Applications to Prototype Selection. (arXiv:1807.08091v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08091</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we show that if the optimization function is
restricted-strongly-convex (RSC) and restricted-smooth (RSM) -- a rich subclass
of weakly submodular functions -- then a streaming algorithm with constant
factor approximation guarantee is possible. More generally, our results are
applicable to any monotone weakly submodular function with submodularity ratio
bounded from above. This (positive) result which provides a sufficient
condition for having a constant factor streaming guarantee for weakly
submodular functions may be of special interest given the recent negative
result (Elenberg et al., 2017) for the general class of weakly submodular
functions. We apply our streaming algorithms for creating compact synopsis of
large complex datasets, by selecting $m$ representative elements, by optimizing
a suitable RSC and RSM objective function. Above results hold even with
additional constraints such as learning non-negative weights, for
interpretability, for each selected element indicative of its importance. We
empirically evaluate our algorithms on two real datasets: MNIST- a handwritten
digits dataset and Letters- a UCI dataset containing the alphabet written in
different fonts and styles. We observe that our algorithms are orders of
magnitude faster than the state-of-the-art streaming algorithm for weakly
submodular functions and with our main algorithm still providing equally good
solutions in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurumoorthy_K/0/1/0/all/0/1&quot;&gt;Karthik S. Gurumoorthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhurandhar_A/0/1/0/all/0/1&quot;&gt;Amit Dhurandhar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08158">
<title>Linear density-based clustering with a discrete density model. (arXiv:1807.08158v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08158</link>
<description rdf:parseType="Literal">&lt;p&gt;Density-based clustering techniques are used in a wide range of data mining
applications. One of their most attractive features con- sists in not making
use of prior knowledge of the number of clusters that a dataset contains along
with their shape. In this paper we propose a new algorithm named Linear DBSCAN
(Lin-DBSCAN), a simple approach to clustering inspired by the density model
introduced with the well known algorithm DBSCAN. Designed to minimize the
computational cost of density based clustering on geospatial data, Lin-DBSCAN
features a linear time complexity that makes it suitable for real-time
applications on low-resource devices. Lin-DBSCAN uses a discrete version of the
density model of DBSCAN that takes ad- vantage of a grid-based scan and merge
approach. The name of the algorithm stems exactly from its main features
outlined above. The algorithm was tested with well known data sets.
Experimental results prove the efficiency and the validity of this approach
over DBSCAN in the context of spatial data clustering, enabling the use of a
density-based clustering technique on large datasets with low computational
cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pirrone_R/0/1/0/all/0/1&quot;&gt;Roberto Pirrone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cannella_V/0/1/0/all/0/1&quot;&gt;Vincenzo Cannella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monteleone_S/0/1/0/all/0/1&quot;&gt;Sergio Monteleone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giordano_G/0/1/0/all/0/1&quot;&gt;Gabriella Giordano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08362">
<title>An Intersectional Definition of Fairness. (arXiv:1807.08362v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08362</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a measure of fairness for algorithms and data with regard to
multiple protected attributes. Our proposed definition, differential fairness,
is informed by the framework of intersectionality, which analyzes how
interlocking systems of power and oppression affect individuals along
overlapping dimensions including race, gender, sexual orientation, class, and
disability. We show that our criterion behaves sensibly for any subset of the
protected attributes, and we illustrate links to differential privacy. A case
study on census data demonstrates the utility of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foulds_J/0/1/0/all/0/1&quot;&gt;James Foulds&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shimei Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08409">
<title>Subsampling MCMC - A review for the survey statistician. (arXiv:1807.08409v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1807.08409</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid development of computing power and efficient Markov Chain Monte
Carlo (MCMC) simulation algorithms have revolutionized Bayesian statistics,
making it a highly practical inference method in applied work. However, MCMC
algorithms tend to be computationally demanding, and are particularly slow for
large datasets. Data subsampling has recently been suggested as a way to make
MCMC methods scalable on massively large data, utilizing efficient sampling
schemes and estimators from the survey sampling literature. These developments
tend to be unknown by many survey statisticians who traditionally work with
non-Bayesian methods, and rarely use MCMC. Our article reviews Subsampling
MCMC, a so called pseudo-marginal MCMC approach to speeding up MCMC through
data subsampling. The review is written for a survey statistician without
previous knowledge of MCMC methods since our aim is to motivate survey sampling
experts to contribute to the growing Subsampling MCMC literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Quiroz_M/0/1/0/all/0/1&quot;&gt;Matias Quiroz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Villani_M/0/1/0/all/0/1&quot;&gt;Mattias Villani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohn_R/0/1/0/all/0/1&quot;&gt;Robert Kohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Minh-Ngoc Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dang_K/0/1/0/all/0/1&quot;&gt;Khue-Dung Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08446">
<title>Minimizing Sum of Non-Convex but Piecewise log-Lipschitz Functions using Coresets. (arXiv:1807.08446v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08446</link>
<description rdf:parseType="Literal">&lt;p&gt;We suggest a new optimization technique for minimizing the sum $\sum_{i=1}^n
f_i(x)$ of $n$ non-convex real functions that satisfy a property that we call
piecewise log-Lipschitz. This is by forging links between techniques in
computational geometry, combinatorics and convex optimization.
&lt;/p&gt;
&lt;p&gt;Example applications include the first constant-factor approximation
algorithms whose running-time is polynomial in $n$ for the following
fundamental problems:
&lt;/p&gt;
&lt;p&gt;(i) Constrained $\ell_z$ Linear Regression: Given $z&amp;gt;0$, $n$ vectors
$p_1,\cdots,p_n$ on the plane, and a vector $b\in\mathbb{R}^n$, compute a unit
vector $x$ and a permutation $\pi:[n]\to[n]$ that minimizes $\sum_{i=1}^n
|p_ix-b_{\pi(i)}|^z$.
&lt;/p&gt;
&lt;p&gt;(ii) Points-to-Lines alignment: Given $n$ lines $\ell_1,\cdots,\ell_n$ on the
plane, compute the matching $\pi:[n]\to[n]$ and alignment (rotation matrix $R$
and a translation vector $t$) that minimize the sum of Euclidean distances \[
\sum_{i=1}^n \mathrm{dist}(Rp_i-t,\ell_{\pi(i)})^z \] between each point to its
corresponding line.
&lt;/p&gt;
&lt;p&gt;These problems are open even if $z=1$ and the matching $\pi$ is given. In
this case, the running time of our algorithms reduces to $O(n)$ using core-sets
that support: streaming, dynamic, and distributed parallel computations (e.g.
on the cloud) in poly-logarithmic update time. Generalizations for handling
e.g. outliers or pseudo-distances such as $M$-estimators for these problems are
also provided.
&lt;/p&gt;
&lt;p&gt;Experimental results show that our provable algorithms improve existing
heuristics also in practice. A demonstration in the context of Augmented
Reality show how such algorithms may be used in real-time systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jubran_I/0/1/0/all/0/1&quot;&gt;Ibrahim Jubran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_D/0/1/0/all/0/1&quot;&gt;Dan Feldman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08465">
<title>Multimodal Social Media Analysis for Gang Violence Prevention. (arXiv:1807.08465v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08465</link>
<description rdf:parseType="Literal">&lt;p&gt;Gang violence is a severe issue in major cities across the U.S. and recent
studies [Patton et al. 2017] have found evidence of social media communications
that can be linked to such violence in communities with high rates of exposure
to gang activity. In this paper we partnered computer scientists with social
work researchers, who have domain expertise in gang violence, to analyze how
public tweets with images posted by youth who mention gang associations on
Twitter can be leveraged to automatically detect psychosocial factors and
conditions that could potentially assist social workers and violence outreach
workers in prevention and early intervention programs. To this end, we
developed a rigorous methodology for collecting and annotating tweets. We
gathered 1,851 tweets and accompanying annotations related to visual concepts
and the psychosocial codes: aggression, loss, and substance use. These codes
are relevant to social work interventions, as they represent possible pathways
to violence on social media. We compare various methods for classifying tweets
into these three classes, using only the text of the tweet, only the image of
the tweet, or both modalities as input to the classifier. In particular, we
analyze the usefulness of mid-level visual concepts and the role of different
modalities for this tweet classification task. Our experiments show that
individually, text information dominates classification performance of the loss
class, while image information dominates the aggression and substance use
classes. Our multimodal approach provides a very promising improvement (18%
relative in mean average precision) over the best single modality approach.
Finally, we also illustrate the complexity of understanding social media data
and elaborate on open challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blandfort_P/0/1/0/all/0/1&quot;&gt;Philipp Blandfort&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patton_D/0/1/0/all/0/1&quot;&gt;Desmond Patton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frey_W/0/1/0/all/0/1&quot;&gt;William R. Frey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1&quot;&gt;Svebor Karaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhargava_S/0/1/0/all/0/1&quot;&gt;Surabhi Bhargava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_F/0/1/0/all/0/1&quot;&gt;Fei-Tzin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varia_S/0/1/0/all/0/1&quot;&gt;Siddharth Varia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kedzie_C/0/1/0/all/0/1&quot;&gt;Chris Kedzie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaskell_M/0/1/0/all/0/1&quot;&gt;Michael B. Gaskell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schifanella_R/0/1/0/all/0/1&quot;&gt;Rossano Schifanella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1&quot;&gt;Kathleen McKeown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shih-Fu Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08534">
<title>Particle Filtering Methods for Stochastic Optimization with Application to Large-Scale Empirical Risk Minimization. (arXiv:1807.08534v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08534</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a recent interest in developing statistical filtering methods for
stochastic optimization (FSO) by leveraging a probabilistic perspective of the
incremental proximity methods (IPMs). The existent FSO methods are derived
based on the Kalman filter (KF) and extended KF (EKF). Different with classical
stochastic optimization methods such as the stochastic gradient descent (SGD)
and typical IPMs, such KF-type algorithms possess a desirable property, namely
they do not require pre-scheduling of the learning rate for convergence.
However, on the other side, they have inherent limitations inherited from the
nature of KF mechanisms. It is a consensus that the class of particle filters
(PFs) outperforms the KF and its variants remarkably for nonlinear and/or
non-Gaussian statistical filtering tasks. Hence, it is natural to ask if the
FSO methods can benefit from the PF theory to get around of the limitations of
the KF-type IPMs. We provide an affirmative answer to the aforementioned
question by developing three PF based SO (PFSO) algorithms. We also provide a
discussion of relationships among (1) PF methods designed for stochastic
dynamic filtering; (2) PF methods designed for static parameter estimation; and
(3) our PFSO algorithms. For performance evaluation, we apply the proposed
algorithms to solve a least-square fitting problem using simulated dataset, and
the empirical risk minimization (ERM) problem in binary classification using
real datasets. The experimental results demonstrate that our algorithms
outperform remarkably existent methods in terms of numerical stability,
convergence speed, classification error rate and flexibility in handling
different types of models and loss functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.08596">
<title>Recent Advances in Convolutional Neural Network Acceleration. (arXiv:1807.08596v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.08596</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, convolutional neural networks (CNNs) have shown great
performance in various fields such as image classification, pattern
recognition, and multi-media compression. Two of the feature properties, local
connectivity and weight sharing, can reduce the number of parameters and
increase processing speed during training and inference. However, as the
dimension of data becomes higher and the CNN architecture becomes more
complicated, the end-to-end approach or the combined manner of CNN is
computationally intensive, which becomes limitation to CNN&apos;s further
implementation. Therefore, it is necessary and urgent to implement CNN in a
faster way. In this paper, we first summarize the acceleration methods that
contribute to but not limited to CNN by reviewing a broad variety of research
papers. We propose a taxonomy in terms of three levels, i.e.~structure level,
algorithm level, and implementation level, for acceleration methods. We also
analyze the acceleration methods in terms of CNN architecture compression,
algorithm optimization, and hardware-based improvement. At last, we give a
discussion on different perspectives of these acceleration and optimization
methods within each level. The discussion shows that the methods in each level
still have large exploration space. By incorporating such a wide range of
disciplines, we expect to provide a comprehensive reference for researchers who
are interested in CNN acceleration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qianru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Meng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tinghuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhifei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bei Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.07272">
<title>Matrix Completion has No Spurious Local Minimum. (arXiv:1605.07272v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1605.07272</link>
<description rdf:parseType="Literal">&lt;p&gt;Matrix completion is a basic machine learning problem that has wide
applications, especially in collaborative filtering and recommender systems.
Simple non-convex optimization algorithms are popular and effective in
practice. Despite recent progress in proving various non-convex algorithms
converge from a good initial point, it remains unclear why random or arbitrary
initialization suffices in practice. We prove that the commonly used non-convex
objective function for \textit{positive semidefinite} matrix completion has no
spurious local minima --- all local minima must also be global. Therefore, many
popular optimization algorithms such as (stochastic) gradient descent can
provably solve positive semidefinite matrix completion with \textit{arbitrary}
initialization in polynomial time. The result can be generalized to the setting
when the observed entries contain noise. We believe that our main proof
strategy can be useful for understanding geometric properties of other
statistical problems involving partial or noisy observations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1&quot;&gt;Rong Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jason D. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengyu Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.02041">
<title>Does Distributionally Robust Supervised Learning Give Robust Classifiers?. (arXiv:1611.02041v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1611.02041</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributionally Robust Supervised Learning (DRSL) is necessary for building
reliable machine learning systems. When machine learning is deployed in the
real world, its performance can be significantly degraded because test data may
follow a different distribution from training data. DRSL with f-divergences
explicitly considers the worst-case distribution shift by minimizing the
adversarially reweighted training loss. In this paper, we analyze this DRSL,
focusing on the classification scenario. Since the DRSL is explicitly
formulated for a distribution shift scenario, we naturally expect it to give a
robust classifier that can aggressively handle shifted distributions. However,
surprisingly, we prove that the DRSL just ends up giving a classifier that
exactly fits the given training distribution, which is too pessimistic. This
pessimism comes from two sources: the particular losses used in classification
and the fact that the variety of distributions to which the DRSL tries to be
robust is too wide. Motivated by our analysis, we propose simple DRSL that
overcomes this pessimism and empirically demonstrate its effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Weihua Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Niu_G/0/1/0/all/0/1&quot;&gt;Gang Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sato_I/0/1/0/all/0/1&quot;&gt;Issei Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.02436">
<title>Nonlinear Information Bottleneck. (arXiv:1705.02436v6 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/1705.02436</link>
<description rdf:parseType="Literal">&lt;p&gt;Information bottleneck [IB] is a technique for extracting information in some
`input&apos; random variable that is relevant for predicting some different &apos;output&apos;
random variable. IB works by encoding the input in a compressed &apos;bottleneck
variable&apos; from which the output can then be accurately decoded. IB can be
difficult to compute in practice, and has been mainly developed for two limited
cases: (1) discrete random variables with small state spaces, and (2)
continuous random variables that are jointly Gaussian distributed (in which
case the encoding and decoding maps are linear). We propose a method to perform
IB in more general domains. Our approach can be applied to discrete or
continuous inputs and outputs, and allows for nonlinear encoding and decoding
maps. The method uses a novel upper bound on the IB objective, derived using a
non-parametric estimator of mutual information and a variational approximation.
We show how to implement the method using neural networks and gradient-based
optimization, and demonstrate its performance on the MNIST dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolchinsky_A/0/1/0/all/0/1&quot;&gt;Artemy Kolchinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tracey_B/0/1/0/all/0/1&quot;&gt;Brendan D. Tracey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolpert_D/0/1/0/all/0/1&quot;&gt;David H. Wolpert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.02757">
<title>Subdeterminant Maximization via Nonconvex Relaxations and Anti-concentration. (arXiv:1707.02757v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1707.02757</link>
<description rdf:parseType="Literal">&lt;p&gt;Several fundamental problems that arise in optimization and computer science
can be cast as follows: Given vectors $v_1,\ldots,v_m \in \mathbb{R}^d$ and a
constraint family ${\cal B}\subseteq 2^{[m]}$, find a set $S \in \cal{B}$ that
maximizes the squared volume of the simplex spanned by the vectors in $S$. A
motivating example is the data-summarization problem in machine learning where
one is given a collection of vectors that represent data such as documents or
images. The volume of a set of vectors is used as a measure of their diversity,
and partition or matroid constraints over $[m]$ are imposed in order to ensure
resource or fairness constraints. Recently, Nikolov and Singh presented a
convex program and showed how it can be used to estimate the value of the most
diverse set when ${\cal B}$ corresponds to a partition matroid. This result was
recently extended to regular matroids in works of Straszak and Vishnoi, and
Anari and Oveis Gharan. The question of whether these estimation algorithms can
be converted into the more useful approximation algorithms -- that also output
a set -- remained open.
&lt;/p&gt;
&lt;p&gt;The main contribution of this paper is to give the first approximation
algorithms for both partition and regular matroids. We present novel
formulations for the subdeterminant maximization problem for these matroids;
this reduces them to the problem of finding a point that maximizes the absolute
value of a nonconvex function over a Cartesian product of probability
simplices. The technical core of our results is a new anti-concentration
inequality for dependent random variables that allows us to relate the optimal
value of these nonconvex functions to their value at a random point. Unlike
prior work on the constrained subdeterminant maximization problem, our proofs
do not rely on real-stability or convexity and could be of independent interest
both in algorithms and complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimi_J/0/1/0/all/0/1&quot;&gt;Javad B. Ebrahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Straszak_D/0/1/0/all/0/1&quot;&gt;Damian Straszak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1&quot;&gt;Nisheeth K. Vishnoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.03494">
<title>Unsupervised robust nonparametric learning of hidden community properties. (arXiv:1707.03494v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1707.03494</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider learning of fundamental properties of communities in large noisy
networks, in the prototypical situation where the nodes or users are split into
two classes according to a binary property, e.g., according to their opinions
or preferences on a topic. For learning these properties, we propose a
nonparametric, unsupervised, and scalable graph scan procedure that is, in
addition, robust against a class of powerful adversaries. In our setup, one of
the communities can fall under the influence of a knowledgeable adversarial
leader, who knows the full network structure, has unlimited computational
resources and can completely foresee our planned actions on the network. We
prove strong consistency of our results in this setup with minimal assumptions.
In particular, the learning procedure estimates the baseline activity of normal
users asymptotically correctly with probability 1; the only assumption being
the existence of a single implicit community of asymptotically negligible
logarithmic size. We provide experiments on real and synthetic data to
illustrate the performance of our method, including examples with adversaries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Langovoy_M/0/1/0/all/0/1&quot;&gt;Mikhail A. Langovoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gotmare_A/0/1/0/all/0/1&quot;&gt;Akhilesh Gotmare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.09430">
<title>A Markov Chain Theory Approach to Characterizing the Minimax Optimality of Stochastic Gradient Descent (for Least Squares). (arXiv:1710.09430v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.09430</link>
<description rdf:parseType="Literal">&lt;p&gt;This work provides a simplified proof of the statistical minimax optimality
of (iterate averaged) stochastic gradient descent (SGD), for the special case
of least squares. This result is obtained by analyzing SGD as a stochastic
process and by sharply characterizing the stationary covariance matrix of this
process. The finite rate optimality characterization captures the constant
factors and addresses model mis-specification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jain_P/0/1/0/all/0/1&quot;&gt;Prateek Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kakade_S/0/1/0/all/0/1&quot;&gt;Sham M. Kakade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kidambi_R/0/1/0/all/0/1&quot;&gt;Rahul Kidambi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Netrapalli_P/0/1/0/all/0/1&quot;&gt;Praneeth Netrapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pillutla_V/0/1/0/all/0/1&quot;&gt;Venkata Krishna Pillutla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sidford_A/0/1/0/all/0/1&quot;&gt;Aaron Sidford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11423">
<title>On reducing the communication cost of the diffusion LMS algorithm. (arXiv:1711.11423v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11423</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of digital and mobile communications has recently made the world
more connected and networked, resulting in an unprecedented volume of data
flowing between sources, data centers, or processes. While these data may be
processed in a centralized manner, it is often more suitable to consider
distributed strategies such as diffusion as they are scalable and can handle
large amounts of data by distributing tasks over networked agents. Although it
is relatively simple to implement diffusion strategies over a cluster, it
appears to be challenging to deploy them in an ad-hoc network with limited
energy budget for communication. In this paper, we introduce a diffusion LMS
strategy that significantly reduces communication costs without compromising
the performance. Then, we analyze the proposed algorithm in the mean and
mean-square sense. Next, we conduct numerical experiments to confirm the
theoretical findings. Finally, we perform large scale simulations to test the
algorithm efficiency in a scenario where energy is limited.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Harrane_I/0/1/0/all/0/1&quot;&gt;Ibrahim El Khalil Harrane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Flamary_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Flamary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Richard_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Richard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07420">
<title>Finding Competitive Network Architectures Within a Day Using UCT. (arXiv:1712.07420v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07420</link>
<description rdf:parseType="Literal">&lt;p&gt;The design of neural network architectures for a new data set is a laborious
task which requires human deep learning expertise. In order to make deep
learning available for a broader audience, automated methods for finding a
neural network architecture are vital. Recently proposed methods can already
achieve human expert level performances. However, these methods have run times
of months or even years of GPU computing time, ignoring hardware constraints as
faced by many researchers and companies. We propose the use of Monte Carlo
planning in combination with two different UCT (upper confidence bound applied
to trees) derivations to search for network architectures. We adapt the UCT
algorithm to the needs of network architecture search by proposing two ways of
sharing information between different branches of the search tree. In an
empirical study we are able to demonstrate that this method is able to find
competitive networks for MNIST, SVHN and CIFAR-10 in just a single GPU day.
Extending the search time to five GPU days, we are able to outperform human
architectures and our competitors which consider the same types of layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wistuba_M/0/1/0/all/0/1&quot;&gt;Martin Wistuba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06818">
<title>Community Recovery in a Preferential Attachment Graph. (arXiv:1801.06818v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06818</link>
<description rdf:parseType="Literal">&lt;p&gt;A message passing algorithm is derived for recovering communities within a
graph generated by a variation of the Barab\&apos;{a}si-Albert preferential
attachment model. The estimator is assumed to know the arrival times, or order
of attachment, of the vertices. The derivation of the algorithm is based on
belief propagation under an independence assumption. Two precursors to the
message passing algorithm are analyzed: the first is a degree thresholding (DT)
algorithm and the second is an algorithm based on the arrival times of the
children (C) of a given vertex, where the children of a given vertex are the
vertices that attached to it. Comparison of the performance of the algorithms
shows it is beneficial to know the arrival times, not just the number, of the
children. The probability of correct classification of a vertex is
asymptotically determined by the fraction of vertices arriving before it. Two
extensions of Algorithm C are given: the first is based on joint likelihood of
the children of a fixed set of vertices; it can sometimes be used to seed the
message passing algorithm. The second is the message passing algorithm.
Simulation results are given.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hajek_B/0/1/0/all/0/1&quot;&gt;Bruce Hajek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sankagiri_S/0/1/0/all/0/1&quot;&gt;Suryanarayana Sankagiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06118">
<title>Gaussian Processes indexed on the symmetric group: prediction and learning. (arXiv:1803.06118v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06118</link>
<description rdf:parseType="Literal">&lt;p&gt;In the framework of the supervised learning of a real function defined on a
space X , the so called Kriging method stands on a real Gaussian field defined
on X. The Euclidean case is well known and has been widely studied. In this
paper, we explore the less classical case where X is the non commutative finite
group of permutations. In this setting, we propose and study an harmonic
analysis of the covariance operators that enables to consider Gaussian
processes models and forecasting issues. Our theory is motivated by statistical
ranking problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bachoc_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Bachoc&lt;/a&gt; (GdR MASCOT-NUM), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Broto_B/0/1/0/all/0/1&quot;&gt;Baptiste Broto&lt;/a&gt; (CEA), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gamboa_F/0/1/0/all/0/1&quot;&gt;Fabrice Gamboa&lt;/a&gt; (IMT), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loubes_J/0/1/0/all/0/1&quot;&gt;Jean-Michel Loubes&lt;/a&gt; (IMT)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.04874">
<title>GAN Q-learning. (arXiv:1805.04874v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.04874</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributional reinforcement learning (distributional RL) has seen empirical
success in complex Markov Decision Processes (MDPs) in the setting of nonlinear
function approximation. However, there are many different ways in which one can
leverage the distributional approach to reinforcement learning. In this paper,
we propose GAN Q-learning, a novel distributional RL method based on generative
adversarial networks (GANs) and analyze its performance in simple tabular
environments, as well as OpenAI Gym. We empirically show that our algorithm
leverages the flexibility and blackbox approach of deep learning models while
providing a viable alternative to traditional methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Doan_T/0/1/0/all/0/1&quot;&gt;Thang Doan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mazoure_B/0/1/0/all/0/1&quot;&gt;Bogdan Mazoure&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lyle_C/0/1/0/all/0/1&quot;&gt;Clare Lyle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06576">
<title>Mad Max: Affine Spline Insights into Deep Learning. (arXiv:1805.06576v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.06576</link>
<description rdf:parseType="Literal">&lt;p&gt;We build a rigorous bridge between deep networks (DNs) and approximation
theory via spline functions and operators. Our key result is that a large class
of DNs can be written as a composition of max-affine spline operators (MASOs),
which provide a powerful portal through which to view and analyze their inner
workings. For instance, conditioned on the input signal, the output of a MASO
DN can be written as a simple affine transformation of the input. This implies
that a DN constructs a set of signal-dependent, class-specific templates
against which the signal is compared via a simple inner product; we explore the
links to the classical theory of optimal classification via matched filters and
the effects of data memorization. Going further, we propose a simple penalty
term that can be added to the cost function of any DN learning algorithm to
force the templates to be orthogonal with each other; this leads to
significantly improved classification performance and reduced overfitting with
no change to the DN architecture. The spline partition of the input signal
space that is implicitly induced by a MASO directly links DNs to the theory of
vector quantization (VQ) and $K$-means clustering, which opens up new geometric
avenue to study how DNs organize signals in a hierarchical fashion. To validate
the utility of the VQ interpretation, we develop and validate a new distance
metric for signals and images that quantifies the difference between their VQ
encodings. (This paper is a significantly expanded version of Balestriero and
richard baraniuk [2018].)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balestriero_R/0/1/0/all/0/1&quot;&gt;Randall Balestriero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baraniuk_R/0/1/0/all/0/1&quot;&gt;Richard Baraniuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.09772">
<title>Auto-Detection of Safety Issues in Baby Products. (arXiv:1805.09772v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.09772</link>
<description rdf:parseType="Literal">&lt;p&gt;Every year, thousands of people receive consumer product related injuries.
Research indicates that online customer reviews can be processed to
autonomously identify product safety issues. Early identification of safety
issues can lead to earlier recalls, and thus fewer injuries and deaths. A
dataset of product reviews from Amazon.com was compiled, along with
\emph{SaferProducts.gov} complaints and recall descriptions from the Consumer
Product Safety Commission (CPSC) and European Commission Rapid Alert system. A
system was built to clean the collected text and to extract relevant features.
Dimensionality reduction was performed by computing feature relevance through a
Random Forest and discarding features with low information gain. Various
classifiers were analyzed, including Logistic Regression, SVMs,
Na{\&quot;i}ve-Bayes, Random Forests, and an Ensemble classifier. Experimentation
with various features and classifier combinations resulted in a logistic
regression model with 66\% precision in the top 50 reviews surfaced. This
classifier outperforms all benchmarks set by related literature and consumer
product safety professionals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bleaney_G/0/1/0/all/0/1&quot;&gt;Graham Bleaney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuzyk_M/0/1/0/all/0/1&quot;&gt;Matthew Kuzyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Man_J/0/1/0/all/0/1&quot;&gt;Julian Man&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayanloo_H/0/1/0/all/0/1&quot;&gt;Hossein Mayanloo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1&quot;&gt;H.R.Tizhoosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00728">
<title>Data-Free/Data-Sparse Softmax Parameter Estimation with Structured Class Geometries. (arXiv:1806.00728v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00728</link>
<description rdf:parseType="Literal">&lt;p&gt;This note considers softmax parameter estimation when little/no labeled
training data is available, but a priori information about the relative
geometry of class label log-odds boundaries is available. It is shown that
`data-free&apos; softmax model synthesis corresponds to solving a linear system of
parameter equations, wherein desired dominant class log-odds boundaries are
encoded via convex polytopes that decompose the input feature space. When
solvable, the linear equations yield closed-form softmax parameter solution
families using class boundary polytope specifications only. This allows softmax
parameter learning to be implemented without expensive brute force data
sampling and numerical optimization. The linear equations can also be adapted
to constrained maximum likelihood estimation in data-sparse settings. Since
solutions may also fail to exist for the linear parameter equations derived
from certain polytope specifications, it is thus also shown that there exist
probabilistic classification problems over m convexly separable classes for
which the log-odds boundaries cannot be learned using an m-class softmax model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nisar Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.09429">
<title>A Distributed Flexible Delay-tolerant Proximal Gradient Algorithm. (arXiv:1806.09429v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1806.09429</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop and analyze an asynchronous algorithm for distributed convex
optimization when the objective writes a sum of smooth functions, local to each
worker, and a non-smooth function. Unlike many existing methods, our
distributed algorithm is adjustable to various levels of communication cost,
delays, machines computational power, and functions smoothness. A unique
feature is that the stepsizes do not depend on communication delays nor number
of machines, which is highly desirable for scalability. We prove that the
algorithm converges linearly in the strongly convex case, and provide
guarantees of convergence for the non-strongly convex case. The obtained rates
are the same as the vanilla proximal gradient algorithm over some introduced
epoch sequence that subsumes the delays of the system. We provide numerical
results on large-scale machine learning problems to demonstrate the merits of
the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mishchenko_K/0/1/0/all/0/1&quot;&gt;Konstantin Mishchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Iutzeler_F/0/1/0/all/0/1&quot;&gt;Franck Iutzeler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Malick_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xf4;me Malick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07610">
<title>Unrolling Swiss Cheese: Metric repair on manifolds with holes. (arXiv:1807.07610v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.07610</link>
<description rdf:parseType="Literal">&lt;p&gt;For many machine learning tasks, the input data lie on a low-dimensional
manifold embedded in a high dimensional space and, because of this
high-dimensional structure, most algorithms are inefficient. The typical
solution is to reduce the dimension of the input data using standard dimension
reduction algorithms such as ISOMAP, LAPLACIAN EIGENMAPS or LLES. This
approach, however, does not always work in practice as these algorithms require
that we have somewhat ideal data. Unfortunately, most data sets either have
missing entries or unacceptably noisy values. That is, real data are far from
ideal and we cannot use these algorithms directly. In this paper, we focus on
the case when we have missing data. Some techniques, such as matrix completion,
can be used to fill in missing data but these methods do not capture the
non-linear structure of the manifold. Here, we present a new algorithm
MR-MISSING that extends these previous algorithms and can be used to compute
low dimensional representation on data sets with missing entries. We
demonstrate the effectiveness of our algorithm by running three different
experiments. We visually verify the effectiveness of our algorithm on synthetic
manifolds, we numerically compare our projections against those computed by
first filling in data using nlPCA and mDRUR on the MNIST data set, and we also
show that we can do classification on MNIST with missing data. We also provide
a theoretical guarantee for MR-MISSING under some simplifying assumptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1&quot;&gt;Anna C. Gilbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonthalia_R/0/1/0/all/0/1&quot;&gt;Rishi Sonthalia&lt;/a&gt;</dc:creator>
</item></rdf:RDF>