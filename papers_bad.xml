<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2017-12-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07019"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.06920"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01213"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01219"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01220"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06530"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06657"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06778"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06868"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06924"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07081"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.06781"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.08329"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.03017"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.07871"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06658"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06695"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06704"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06715"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06745"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07022"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07027"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07042"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07101"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07102"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07106"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07108"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.04366"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1703.10893"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.00070"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.02675"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.06424"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1712.07019">
<title>PSO-Optimized Hopfield Neural Network-Based Multipath Routing for Mobile Ad-hoc Networks. (arXiv:1712.07019v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1712.07019</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobile ad-hoc network (MANET) is a dynamic collection of mobile computers
without the need for any existing infrastructure. Nodes in a MANET act as hosts
and routers. Designing of robust routing algorithms for MANETs is a challenging
task. Disjoint multipath routing protocols address this problem and increase
the reliability, security and lifetime of network. However, selecting an
optimal multipath is an NP-complete problem. In this paper, Hopfield neural
network (HNN) which its parameters are optimized by particle swarm optimization
(PSO) algorithm is proposed as multipath routing algorithm. Link expiration
time (LET) between each two nodes is used as the link reliability estimation
metric. This approach can find either node-disjoint or link-disjoint paths in
single phase route discovery. Simulation results confirm that PSO-HNN routing
algorithm has better performance as compared to backup path set selection
algorithm (BPSA) in terms of the path set reliability and number of paths in
the set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheikhan_M/0/1/0/all/0/1&quot;&gt;Mansour Sheikhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemmati_E/0/1/0/all/0/1&quot;&gt;Ehsan Hemmati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.06920">
<title>Genetic Algorithm with Optimal Recombination for the Asymmetric Travelling Salesman Problem. (arXiv:1706.06920v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1706.06920</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new genetic algorithm with optimal recombination for the
asymmetric instances of travelling salesman problem. The algorithm incorporates
several new features that contribute to its effectiveness: (i) Optimal
recombination problem is solved within crossover operator. (ii) A new mutation
operator performs a random jump within 3-opt or 4-opt neighborhood. (iii)
Greedy constructive heuristic of W.Zhang and 3-opt local search heuristic are
used to generate the initial population. A computational experiment on TSPLIB
instances shows that the proposed algorithm yields competitive results to other
well-known memetic algorithms for asymmetric travelling salesman problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eremeev_A/0/1/0/all/0/1&quot;&gt;A.V. Eremeev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovalenko_Y/0/1/0/all/0/1&quot;&gt;Yu.V. Kovalenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01213">
<title>Data-Driven Sparse Structure Selection for Deep Neural Networks. (arXiv:1707.01213v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01213</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional neural networks have liberated its extraordinary power on
various tasks. However, it is still very challenging to deploy state-of-the-art
models into real-world applications due to their high computational complexity.
How can we design a compact and effective network without massive experiments
and expert knowledge? In this paper, we propose a simple and effective
framework to learn and prune deep models in an end-to-end manner. In our
framework, a new type of parameter -- scaling factor is first introduced to
scale the outputs of specific structures, such as neurons, groups or residual
blocks. Then we add sparsity regularizations on these factors, and solve this
optimization problem by a modified stochastic Accelerated Proximal Gradient
(APG) method. By forcing some of the factors to zero, we can safely remove the
corresponding structures, thus prune the unimportant parts of a CNN. Comparing
with other structure selection methods that may need thousands of trials or
iterative fine-tuning, our method is trained fully end-to-end in one training
pass without bells and whistles. We evaluate our method, Sparse Structure
Selection with several state-of-the-art CNNs, and demonstrate very promising
results with adaptive depth and width selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zehao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Naiyan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01219">
<title>Like What You Like: Knowledge Distill via Neuron Selectivity Transfer. (arXiv:1707.01219v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01219</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite deep neural networks have demonstrated extraordinary power in various
applications, their superior performances are at expense of high storage and
computational costs. Consequently, the acceleration and compression of neural
networks have attracted much attention recently. Knowledge Transfer (KT), which
aims at training a smaller student network by transferring knowledge from a
larger teacher model, is one of the popular solutions. In this paper, we
propose a novel knowledge transfer method by treating it as a distribution
matching problem. Particularly, we match the distributions of neuron
selectivity patterns between teacher and student networks. To achieve this
goal, we devise a new KT loss function by minimizing the Maximum Mean
Discrepancy (MMD) metric between these distributions. Combined with the
original loss function, our method can significantly improve the performance of
student networks. We validate the effectiveness of our method across several
datasets, and further combine it with other KT methods to explore the best
possible results. Last but not least, we fine-tune the model to other tasks
such as object detection. The results are also encouraging, which confirm the
transferability of the learned features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zehao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Naiyan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01220">
<title>DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer. (arXiv:1707.01220v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01220</link>
<description rdf:parseType="Literal">&lt;p&gt;We have witnessed rapid evolution of deep neural network architecture design
in the past years. These latest progresses greatly facilitate the developments
in various areas such as computer vision and natural language processing.
However, along with the extraordinary performance, these state-of-the-art
models also bring in expensive computational cost. Directly deploying these
models into applications with real-time requirement is still infeasible.
Recently, Hinton etal. have shown that the dark knowledge within a powerful
teacher model can significantly help the training of a smaller and faster
student network. These knowledge are vastly beneficial to improve the
generalization ability of the student model. Inspired by their work, we
introduce a new type of knowledge -- cross sample similarities for model
compression and acceleration. This knowledge can be naturally derived from deep
metric learning model. To transfer them, we bring the &quot;learning to rank&quot;
technique into deep metric learning formulation. We test our proposed DarkRank
method on various metric learning tasks including pedestrian re-identification,
image retrieval and image clustering. The results are quite encouraging. Our
method can improve over the baseline method by a large margin. Moreover, it is
fully compatible with other existing methods. When combined, the performance
can be further boosted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Naiyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoxiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06530">
<title>Dynamic Weight Alignment for Convolutional Neural Networks. (arXiv:1712.06530v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.06530</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a method of improving Convolutional Neural Networks
(CNN) by determining the optimal alignment of weights and inputs using dynamic
programming. Conventional CNNs convolve learnable shared weights, or filters,
across the input data. The filters use a linear matching of weights to inputs
using an inner product between the filter and a window of the input. However,
it is possible that there exists a more optimal alignment of weights. Thus, we
propose the use of Dynamic Time Warping (DTW) to dynamically align the weights
to optimized input elements. This dynamic alignment is useful for time series
recognition due to the complexities of temporal relations and temporal
distortions. We demonstrate the effectiveness of the proposed architecture on
the Unipen online handwritten digit and character datasets, the UCI Spoken
Arabic Digit dataset, and the UCI Activities of Daily Life dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1&quot;&gt;Brian Kenji Iwana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1&quot;&gt;Seiichi Uchida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06657">
<title>Towards the Augmented Pathologist: Challenges of Explainable-AI in Digital Pathology. (arXiv:1712.06657v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.06657</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital pathology is not only one of the most promising fields of diagnostic
medicine, but at the same time a hot topic for fundamental research. Digital
pathology is not just the transfer of histopathological slides into digital
representations. The combination of different data sources (images, patient
records, and *omics data) together with current advances in artificial
intelligence/machine learning enable to make novel information accessible and
quantifiable to a human expert, which is not yet available and not exploited in
current medical settings. The grand goal is to reach a level of usable
intelligence to understand the data in the context of an application task,
thereby making machine decisions transparent, interpretable and explainable.
The foundation of such an &quot;augmented pathologist&quot; needs an integrated approach:
While machine learning algorithms require many thousands of training examples,
a human expert is often confronted with only a few data points. Interestingly,
humans can learn from such few examples and are able to instantly interpret
complex patterns. Consequently, the grand goal is to combine the possibilities
of artificial intelligence with human intelligence and to find a well-suited
balance between them to enable what neither of them could do on their own. This
can raise the quality of education, diagnosis, prognosis and prediction of
cancer and other diseases. In this paper we describe some (incomplete) research
issues which we believe should be addressed in an integrated and concerted
effort for paving the way towards the augmented pathologist.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holzinger_A/0/1/0/all/0/1&quot;&gt;Andreas Holzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malle_B/0/1/0/all/0/1&quot;&gt;Bernd Malle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kieseberg_P/0/1/0/all/0/1&quot;&gt;Peter Kieseberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_P/0/1/0/all/0/1&quot;&gt;Peter M. Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_H/0/1/0/all/0/1&quot;&gt;Heimo M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reihs_R/0/1/0/all/0/1&quot;&gt;Robert Reihs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zatloukal_K/0/1/0/all/0/1&quot;&gt;Kurt Zatloukal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06778">
<title>Learning Representations from Road Network for End-to-End Urban Growth Simulation. (arXiv:1712.06778v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.06778</link>
<description rdf:parseType="Literal">&lt;p&gt;From our experiences in the past, we have seen that the growth of cities is
very much dependent on the transportation networks. In mega cities,
transportation networks determine to a significant extent as to where the
people will move and houses will be built. Hence, transportation network data
is crucial to an urban growth prediction system. Existing works have used
manually derived distance based features based on the road networks to build
models on urban growth. But due to the non-generic and laborious nature of the
manual feature engineering process, we can shift to End-to-End systems which do
not rely on manual feature engineering. In this paper, we propose a method to
integrate road network data to an existing Rule based End-to-End framework
without manual feature engineering. Our method employs recurrent neural
networks to represent road networks in a structured way such that it can be
plugged into the previously proposed End-to-End framework. The proposed
approach enhances the performance in terms of Figure of Merit, Producer&apos;s
accuracy, User&apos;s accuracy and Overall accuracy of the existing Rule based
End-to-End framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1&quot;&gt;Saptarshi Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Soumya K Ghosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06868">
<title>Heinrich Behmann&apos;s Contributions to Second-Order Quantifier Elimination from the View of Computational Logic. (arXiv:1712.06868v1 [cs.LO])</title>
<link>http://arxiv.org/abs/1712.06868</link>
<description rdf:parseType="Literal">&lt;p&gt;For relational monadic formulas (the L\&quot;owenheim class) second-order
quantifier elimination, which is closely related to computation of uniform
interpolants, projection and forgetting - operations that currently receive
much attention in knowledge processing - always succeeds. The decidability
proof for this class by Heinrich Behmann from 1922 explicitly proceeds by
elimination with equivalence preserving formula rewriting. Here we reconstruct
the results from Behmann&apos;s publication in detail and discuss related issues
that are relevant in the context of modern approaches to second-order
quantifier elimination in computational logic. In addition, an extensive
documentation of the letters and manuscripts in Behmann&apos;s bequest that concern
second-order quantifier elimination is given, including a commented register
and English abstracts of the German sources with focus on technical material.
In the late 1920s Behmann attempted to develop an elimination-based decision
method for formulas with predicates whose arity is larger than one. His
manuscripts and the correspondence with Wilhelm Ackermann show technical
aspects that are still of interest today and give insight into the genesis of
Ackermann&apos;s landmark paper &quot;Untersuchungen \&quot;uber das Eliminationsproblem der
mathematischen Logik&quot; from 1935, which laid the foundation of the two
prevailing modern approaches to second-order quantifier elimination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wernhard_C/0/1/0/all/0/1&quot;&gt;Christoph Wernhard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06924">
<title>Safe Policy Improvement with Baseline Bootstrapping. (arXiv:1712.06924v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.06924</link>
<description rdf:parseType="Literal">&lt;p&gt;A common goal in Reinforcement Learning is to derive a good strategy given a
limited batch of data. In this paper, we adopt the safe policy improvement
(SPI) approach: we compute a target policy guaranteed to perform at least as
well as a given baseline policy. Our SPI strategy, inspired by the
knows-what-it-knows paradigms, consists in bootstrapping the target policy with
the baseline policy when it does not know. We develop two computationally
efficient bootstrapping algorithms, a value-based and a policy-based, both
accompanied with theoretical SPI bounds. Three algorithm variants are proposed.
We empirically show the literature algorithms limits on a small stochastic
gridworld problem, and then demonstrate that our five algorithms not only
improve the worst case scenarios, but also the mean performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1&quot;&gt;Romain Laroche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trichelair_P/0/1/0/all/0/1&quot;&gt;Paul Trichelair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asri_L/0/1/0/all/0/1&quot;&gt;Layla El Asri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07081">
<title>Column Generation for Interaction Coverage in Combinatorial Software Testing. (arXiv:1712.07081v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1712.07081</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel column generation framework for combinatorial
software testing. In particular, it combines Mathematical Programming and
Constraint Programming in a hybrid decomposition to generate covering arrays.
The approach allows generating parameterized test cases with coverage
guarantees between parameter interactions of a given application. Compared to
exhaustive testing, combinatorial test case generation reduces the number of
tests to run significantly. Our column generation algorithm is generic and can
accommodate mixed coverage arrays over heterogeneous alphabets. The algorithm
is realized in practice as a cloud service and recognized as one of the five
winners of the company-wide cloud application challenge at Oracle. The service
is currently helping software developers from a range of different product
teams in their testing efforts while exposing declarative constraint models and
hybrid optimization techniques to a broader audience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadioglu_S/0/1/0/all/0/1&quot;&gt;Serdar Kadioglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.06781">
<title>Modular Deep Q Networks for Sim-to-real Transfer of Visuo-motor Policies. (arXiv:1610.06781v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/1610.06781</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep learning has had significant successes in computer vision thanks
to the abundance of visual data, collecting sufficiently large real-world
datasets for robot learning can be costly. To increase the practicality of
these techniques on real robots, we propose a modular deep reinforcement
learning method capable of transferring models trained in simulation to a
real-world robotic task. We introduce a bottleneck between perception and
control, enabling the networks to be trained independently, but then merged and
fine-tuned in an end-to-end manner to further improve hand-eye coordination. On
a canonical, planar visually-guided robot reaching task a fine-tuned accuracy
of 1.6 pixels is achieved, a significant improvement over naive transfer (17.5
pixels), showing the potential for more complicated and broader applications.
Our method provides a technique for more efficient learning and transfer of
visuo-motor policies for real robotic systems without relying entirely on large
real-world robot datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fangyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leitner_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Leitner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1&quot;&gt;Michael Milford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corke_P/0/1/0/all/0/1&quot;&gt;Peter Corke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.08329">
<title>The Boolean Solution Problem from the Perspective of Predicate Logic - Extended Version. (arXiv:1706.08329v3 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/1706.08329</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding solution values for unknowns in Boolean equations was a principal
reasoning mode in the Algebra of Logic of the 19th century. Schr\&quot;oder
investigated it as &quot;Aufl\&quot;osungsproblem&quot; (&quot;solution problem&quot;). It is closely
related to the modern notion of Boolean unification. Today it is commonly
presented in an algebraic setting, but seems potentially useful also in
knowledge representation based on predicate logic. We show that it can be
modeled on the basis of first-order logic extended by second-order
quantification. A wealth of classical results transfers, foundations for
algorithms unfold, and connections with second-order quantifier elimination and
Craig interpolation show up. Although for first-order inputs the set of
solutions is recursively enumerable, the development of constructive methods
remains a challenge. We identify some cases that allow constructions, most of
them based on Craig interpolation, and show a method to take vocabulary
restrictions on solution components into account.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wernhard_C/0/1/0/all/0/1&quot;&gt;Christoph Wernhard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.03017">
<title>Learning Visual Reasoning Without Strong Priors. (arXiv:1707.03017v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1707.03017</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving artificial visual reasoning - the ability to answer image-related
questions which require a multi-step, high-level process - is an important step
towards artificial general intelligence. This multi-modal task requires
learning a question-dependent, structured reasoning process over images from
language. Standard deep learning approaches tend to exploit biases in the data
rather than learn this underlying structure, while leading methods learn to
visually reason successfully but are hand-crafted for reasoning. We show that a
general-purpose, Conditional Batch Normalization approach achieves
state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4%
error rate. We outperform the next best end-to-end method (4.5%) and even
methods that use extra supervision (3.1%). We probe our model to shed light on
how it reasons, showing it has learned a question-dependent, multi-step
process. Previous work has operated under the assumption that visual reasoning
calls for a specialized architecture, but we show that a general architecture
with proper conditioning can learn to visually reason effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1&quot;&gt;Ethan Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1&quot;&gt;Harm de Vries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1&quot;&gt;Florian Strub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1&quot;&gt;Vincent Dumoulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.07871">
<title>FiLM: Visual Reasoning with a General Conditioning Layer. (arXiv:1709.07871v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1709.07871</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a general-purpose conditioning method for neural networks called
FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network
computation via a simple, feature-wise affine transformation based on
conditioning information. We show that FiLM layers are highly effective for
visual reasoning - answering image-related questions which require a
multi-step, high-level process - a task which has proven difficult for standard
deep learning methods that do not explicitly model reasoning. Specifically, we
show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error
for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are
robust to ablations and architectural modifications, and 4) generalize well to
challenging, new data from few examples or even zero-shot.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1&quot;&gt;Ethan Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1&quot;&gt;Florian Strub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1&quot;&gt;Harm de Vries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1&quot;&gt;Vincent Dumoulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06658">
<title>MEBoost: Mixing Estimators with Boosting for Imbalanced Data Classification. (arXiv:1712.06658v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1712.06658</link>
<description rdf:parseType="Literal">&lt;p&gt;Class imbalance problem has been a challenging research problem in the fields
of machine learning and data mining as most real life datasets are imbalanced.
Several existing machine learning algorithms try to maximize the accuracy
classification by correctly identifying majority class samples while ignoring
the minority class. However, the concept of the minority class instances
usually represents a higher interest than the majority class. Recently, several
cost sensitive methods, ensemble models and sampling techniques have been used
in literature in order to classify imbalance datasets. In this paper, we
propose MEBoost, a new boosting algorithm for imbalanced datasets. MEBoost
mixes two different weak learners with boosting to improve the performance on
imbalanced datasets. MEBoost is an alternative to the existing techniques such
as SMOTEBoost, RUSBoost, Adaboost, etc. The performance of MEBoost has been
evaluated on 12 benchmark imbalanced datasets with state of the art ensemble
methods like SMOTEBoost, RUSBoost, Easy Ensemble, EUSBoost, DataBoost.
Experimental results show significant improvement over the other methods and it
can be concluded that MEBoost is an effective and promising algorithm to deal
with imbalance datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rayhan_F/0/1/0/all/0/1&quot;&gt;Farshid Rayhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Sajid Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahbub_A/0/1/0/all/0/1&quot;&gt;Asif Mahbub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jani_M/0/1/0/all/0/1&quot;&gt;Md. Rafsan Jani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shatabda_S/0/1/0/all/0/1&quot;&gt;Swakkhar Shatabda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farid_D/0/1/0/all/0/1&quot;&gt;Dewan Md. Farid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_C/0/1/0/all/0/1&quot;&gt;Chowdhury Mofizur Rahman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06695">
<title>Accurate Inference for Adaptive Linear Models. (arXiv:1712.06695v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.06695</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimators computed from adaptively collected data do not behave like their
non-adaptive brethren. Rather, the sequential dependence of the collection
policy can lead to severe distributional biases that persist even in the
infinite data limit. We develop a general method decorrelation procedure --
W-decorrelation -- for transforming the bias of adaptive linear regression
estimators into variance. The method uses only coarse-grained information about
the data collection policy and does not need access to propensity scores or
exact knowledge of the policy. We bound the finite-sample bias and variance of
the W-estimator and develop asymptotically correct confidence intervals based
on a novel martingale central limit theorem. We then demonstrate the empirical
benefits of the generic W-decorrelation procedure in two different adaptive
data settings: the multi-armed bandits and autoregressive time series models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deshpande_Y/0/1/0/all/0/1&quot;&gt;Yash Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Taddy_M/0/1/0/all/0/1&quot;&gt;Matt Taddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06704">
<title>Multilingual Topic Models. (arXiv:1712.06704v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.06704</link>
<description rdf:parseType="Literal">&lt;p&gt;Scientific publications have evolved several features for mitigating
vocabulary mismatch when indexing, retrieving, and computing similarity between
articles. These mitigation strategies range from simply focusing on high-value
article sections, such as titles and abstracts, to assigning keywords, often
from controlled vocabularies, either manually or through automatic annotation.
Various document representation schemes possess different cost-benefit
tradeoffs. In this paper, we propose to model different representations of the
same article as translations of each other, all generated from a common latent
representation in a multilingual topic model. We start with a methodological
overview on latent variable models for parallel document representations that
could be used across many information science tasks. We then show how solving
the inference problem of mapping diverse representations into a shared topic
space allows us to evaluate representations based on how topically similar they
are to the original article. In addition, our proposed approach provides means
to discover where different concept vocabularies require improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krstovski_K/0/1/0/all/0/1&quot;&gt;Kriste Krstovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kurtz_M/0/1/0/all/0/1&quot;&gt;Michael J. Kurtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smith_D/0/1/0/all/0/1&quot;&gt;David A. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Accomazzi_A/0/1/0/all/0/1&quot;&gt;Alberto Accomazzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06715">
<title>Deformable Classifiers. (arXiv:1712.06715v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.06715</link>
<description rdf:parseType="Literal">&lt;p&gt;Geometric variations of objects, which do not modify the object class, pose a
major challenge for object recognition. These variations could be rigid as well
as non-rigid transformations. In this paper, we design a framework for training
deformable classifiers, where latent transformation variables are introduced,
and a transformation of the object image to a reference instantiation is
computed in terms of the classifier output, separately for each class. The
classifier outputs for each class, after transformation, are compared to yield
the final decision. As a by-product of the classification this yields a
transformation of the input object to a reference pose, which can be used for
downstream tasks such as the computation of object support. We apply a two-step
training mechanism for our framework, which alternates between optimizing over
the latent transformation variables and the classifier parameters to minimize
the loss function. We show that multilayer perceptrons, also known as deep
networks, are well suited for this approach and achieve state of the art
results on the rotated MNIST and the Google Earth dataset, and produce
competitive results on MNIST and CIFAR-10 when training on smaller subsets of
training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jiajun Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Amit_Y/0/1/0/all/0/1&quot;&gt;Yali Amit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06745">
<title>Efficient Algorithms for Searching the Minimum Information Partition in Integrated Information Theory. (arXiv:1712.06745v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1712.06745</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to integrate information in the brain is considered to be an
essential property for cognition and consciousness. Integrated Information
Theory (IIT) hypothesizes that the amount of integrated information ($\Phi$) in
the brain is related to the level of consciousness. IIT proposes that to
quantify information integration in a system as a whole, integrated information
should be measured across the partition of the system at which information loss
caused by partitioning is minimized, called the Minimum Information Partition
(MIP). The computational cost for exhaustively searching for the MIP grows
exponentially with system size, making it difficult to apply IIT to real neural
data. It has been previously shown that if a measure of $\Phi$ satisfies a
mathematical property, submodularity, the MIP can be found in a polynomial
order by an optimization algorithm. However, although the first version of
$\Phi$ is submodular, the later versions are not. In this study, we empirically
explore to what extent the algorithm can be applied to the non-submodular
measures of $\Phi$ by evaluating the accuracy of the algorithm in simulated
data and real neural data. We find that the algorithm identifies the MIP in a
nearly perfect manner even for the non-submodular measures. Our results show
that the algorithm allows us to measure $\Phi$ in large systems within a
practical amount of time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kitazono_J/0/1/0/all/0/1&quot;&gt;Jun Kitazono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kanai_R/0/1/0/all/0/1&quot;&gt;Ryota Kanai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Oizumi_M/0/1/0/all/0/1&quot;&gt;Masafumi Oizumi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07022">
<title>Automatic Renal Segmentation in DCE-MRI using Convolutional Neural Networks. (arXiv:1712.07022v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1712.07022</link>
<description rdf:parseType="Literal">&lt;p&gt;Kidney function evaluation using dynamic contrast-enhanced MRI (DCE-MRI)
images could help in diagnosis and treatment of kidney diseases of children.
Automatic segmentation of renal parenchyma is an important step in this
process. In this paper, we propose a time and memory efficient fully automated
segmentation method which achieves high segmentation accuracy with running time
in the order of seconds in both normal kidneys and kidneys with hydronephrosis.
The proposed method is based on a cascaded application of two 3D convolutional
neural networks that employs spatial and temporal information at the same time
in order to learn the tasks of localization and segmentation of kidneys,
respectively. Segmentation performance is evaluated on both normal and abnormal
kidneys with varying levels of hydronephrosis. We achieved a mean dice
coefficient of 91.4 and 83.6 for normal and abnormal kidneys of pediatric
patients, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haghighi_M/0/1/0/all/0/1&quot;&gt;Marzieh Haghighi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warfield_S/0/1/0/all/0/1&quot;&gt;Simon K. Warfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurugol_S/0/1/0/all/0/1&quot;&gt;Sila Kurugol&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07027">
<title>Snake: a Stochastic Proximal Gradient Algorithm for Regularized Problems over Large Graphs. (arXiv:1712.07027v1 [math.OC])</title>
<link>http://arxiv.org/abs/1712.07027</link>
<description rdf:parseType="Literal">&lt;p&gt;A regularized optimization problem over a large unstructured graph is
studied, where the regularization term is tied to the graph geometry. Typical
regularization examples include the total variation and the Laplacian
regularizations over the graph. When applying the proximal gradient algorithm
to solve this problem, there exist quite affordable methods to implement the
proximity operator (backward step) in the special case where the graph is a
simple path without loops. In this paper, an algorithm, referred to as &quot;Snake&quot;,
is proposed to solve such regularized problems over general graphs, by taking
benefit of these fast methods. The algorithm consists in properly selecting
random simple paths in the graph and performing the proximal gradient algorithm
over these simple paths. This algorithm is an instance of a new general
stochastic proximal gradient algorithm, whose convergence is proven.
Applications to trend filtering and graph inpainting are provided among others.
Numerical experiments are conducted over large graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Salim_A/0/1/0/all/0/1&quot;&gt;Adil Salim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bianchi_P/0/1/0/all/0/1&quot;&gt;Pascal Bianchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hachem_W/0/1/0/all/0/1&quot;&gt;Walid Hachem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07042">
<title>Pafnucy -- A deep neural network for structure-based drug discovery. (arXiv:1712.07042v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.07042</link>
<description rdf:parseType="Literal">&lt;p&gt;Virtual screening is one of the most successful approaches for augmenting the
drug discovery process. Currently, there is a notable shift towards machine
learning (ML) methodologies to aid this process. Deep learning has recently
gained considerable attention as it allows the model to &quot;learn&quot; to extract
features that are relevant for the task at hand. We have developed a new deep
neural network tailored to estimating the binding affinity of ligand-receptor
complexes. The complex is represented with a 3D grid, and the model utilizes a
3D convolution to produce a feature map of this representation, treating the
atoms of both proteins and ligands in the same manner. Our network was tested
on the CASF &quot;scoring power&quot; benchmark and Astex diverse set and outperformed
classical scoring functions. The model, together with usage instructions and
examples, is available as a git repository at
&lt;a href=&quot;http://gitlab.com/cheminfIBB/pafnucy&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stepniewska_Dziubinska_M/0/1/0/all/0/1&quot;&gt;Marta M. Stepniewska-Dziubinska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zielenkiewicz_P/0/1/0/all/0/1&quot;&gt;Piotr Zielenkiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siedlecki_P/0/1/0/all/0/1&quot;&gt;Pawel Siedlecki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07101">
<title>Improving End-to-End Speech Recognition with Policy Learning. (arXiv:1712.07101v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1712.07101</link>
<description rdf:parseType="Literal">&lt;p&gt;Connectionist temporal classification (CTC) is widely used for maximum
likelihood learning in end-to-end speech recognition models. However, there is
usually a disparity between the negative maximum likelihood and the performance
metric used in speech recognition, e.g., word error rate (WER). This results in
a mismatch between the objective function and metric during training. We show
that the above problem can be mitigated by jointly training with maximum
likelihood and policy gradient. In particular, with policy learning we are able
to directly optimize on the (otherwise non-differentiable) performance metric.
We show that joint training improves relative performance by 4% to 13% for our
end-to-end model as compared to the same model learned through maximum
likelihood. The model achieves 5.53% WER on Wall Street Journal dataset, and
5.42% and 14.70% on Librispeech test-clean and test-other set, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingbo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07102">
<title>On Data-Dependent Random Features for Improved Generalization in Supervised Learning. (arXiv:1712.07102v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.07102</link>
<description rdf:parseType="Literal">&lt;p&gt;The randomized-feature approach has been successfully employed in large-scale
kernel approximation and supervised learning. The distribution from which the
random features are drawn impacts the number of features required to
efficiently perform a learning task. Recently, it has been shown that employing
data-dependent randomization improves the performance in terms of the required
number of random features. In this paper, we are concerned with the
randomized-feature approach in supervised learning for good generalizability.
We propose the Energy-based Exploration of Random Features (EERF) algorithm
based on a data-dependent score function that explores the set of possible
features and exploits the promising regions. We prove that the proposed score
function with high probability recovers the spectrum of the best fit within the
model class. Our empirical results on several benchmark datasets further verify
that our method requires smaller number of random features to achieve a certain
generalization error compared to the state-of-the-art while introducing
negligible pre-processing overhead. EERF can be implemented in a few lines of
code and requires no additional tuning parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shahrampour_S/0/1/0/all/0/1&quot;&gt;Shahin Shahrampour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Beirami_A/0/1/0/all/0/1&quot;&gt;Ahmad Beirami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tarokh_V/0/1/0/all/0/1&quot;&gt;Vahid Tarokh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07106">
<title>Exploring High-Dimensional Structure via Axis-Aligned Decomposition of Linear Projections. (arXiv:1712.07106v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1712.07106</link>
<description rdf:parseType="Literal">&lt;p&gt;Two-dimensional embeddings remain the dominant approach to visualize high
dimensional data. The choice of embeddings ranges from highly non-linear ones,
which can capture complex relationships but are difficult to interpret
quantitatively, to axis-aligned projections, which are easy to interpret but
are limited to bivariate relationships. Linear project can be considered as a
compromise between complexity and interpretability, as they allow explicit axes
labels, yet provide significantly more degrees of freedom compared to
axis-aligned projections. Nevertheless, interpreting the axes directions, which
are linear combinations often with many non-trivial components, remains
difficult. To address this problem we introduce a structure aware decomposition
of (multiple) linear projections into sparse sets of axis aligned projections,
which jointly capture all information of the original linear ones. In
particular, we use tools from Dempster-Shafer theory to formally define how
relevant a given axis aligned project is to explain the neighborhood relations
displayed in some linear projection. Furthermore, we introduce a new approach
to discover a diverse set of high quality linear projections and show that in
practice the information of $k$ linear projections is often jointly encoded in
$\sim k$ axis aligned plots. We have integrated these ideas into an interactive
visualization system that allows users to jointly browse both linear
projections and their axis aligned representatives. Using a number of case
studies we show how the resulting plots lead to more intuitive visualizations
and new insight.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thiagarajan_J/0/1/0/all/0/1&quot;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shusen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramamurthy_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bremer_P/0/1/0/all/0/1&quot;&gt;Peer-Timo Bremer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07108">
<title>Improved Regularization Techniques for End-to-End Speech Recognition. (arXiv:1712.07108v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1712.07108</link>
<description rdf:parseType="Literal">&lt;p&gt;Regularization is important for end-to-end speech models, since the models
are highly flexible and easy to overfit. Data augmentation and dropout has been
important for improving end-to-end models in other domains. However, they are
relatively under explored for end-to-end speech models. Therefore, we
investigate the effectiveness of both methods for end-to-end trainable, deep
speech recognition models. We augment audio data through random perturbations
of tempo, pitch, volume, temporal alignment, and adding random noise.We further
investigate the effect of dropout when applied to the inputs of all layers of
the network. We show that the combination of data augmentation and dropout give
a relative performance improvement on both Wall Street Journal (WSJ) and
LibriSpeech dataset of over 20%. Our model performance is also competitive with
other end-to-end speech models on both datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingbo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1&quot;&gt;Richard Socher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.04366">
<title>Recursive nonlinear-system identification using latent variables. (arXiv:1606.04366v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1606.04366</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we develop a method for learning nonlinear systems with
multiple outputs and inputs. We begin by modelling the errors of a nominal
predictor of the system using a latent variable framework. Then using the
maximum likelihood principle we derive a criterion for learning the model. The
resulting optimization problem is tackled using a majorization-minimization
approach. Finally, we develop a convex majorization technique and show that it
enables a recursive identification method. The method learns parsimonious
predictive models and is tested on both synthetic and real nonlinear systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mattsson_P/0/1/0/all/0/1&quot;&gt;Per Mattsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zachariah_D/0/1/0/all/0/1&quot;&gt;Dave Zachariah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stoica_P/0/1/0/all/0/1&quot;&gt;Petre Stoica&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1703.10893">
<title>Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional Neural Networks. (arXiv:1703.10893v5 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/1703.10893</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech enhancement (SE) aims to reduce noise in speech signals. Most SE
techniques focus only on addressing audio information. In this work, inspired
by multimodal learning, which utilizes data from different modalities, and the
recent success of convolutional neural networks (CNNs) in SE, we propose an
audio-visual deep CNNs (AVDCNN) SE model, which incorporates audio and visual
streams into a unified network model. We also propose a multi-task learning
framework for reconstructing audio and visual signals at the output layer.
Precisely speaking, the proposed AVDCNN model is structured as an audio-visual
encoder-decoder network, in which audio and visual data are first processed
using individual CNNs, and then fused into a joint network to generate enhanced
speech (the primary task) and reconstructed images (the secondary task) at the
output layer. The model is trained in an end-to-end manner, and parameters are
jointly learned through back-propagation. We evaluate enhanced speech using
five instrumental criteria. Results show that the AVDCNN model yields a notably
superior performance compared with an audio-only CNN-based SE model and two
conventional SE approaches, confirming the effectiveness of integrating visual
information into the SE process. In addition, the AVDCNN model also outperforms
an existing audio-visual SE model, confirming its capability of effectively
combining audio and visual information in SE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jen-Cheng Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Syu-Siang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1&quot;&gt;Ying-Hui Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1&quot;&gt;Yu Tsao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hsiu-Wen Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hsin-Min Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.00070">
<title>Ranking Median Regression: Learning to Order through Local Consensus. (arXiv:1711.00070v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1711.00070</link>
<description rdf:parseType="Literal">&lt;p&gt;This article is devoted to the problem of predicting the value taken by a
random permutation $\Sigma$, describing the preferences of an individual over a
set of numbered items $\{1,\; \ldots,\; n\}$ say, based on the observation of
an input/explanatory r.v. $X$ e.g. characteristics of the individual), when
error is measured by the Kendall $\tau$ distance. In the probabilistic
formulation of the &apos;Learning to Order&apos; problem we propose, which extends the
framework for statistical Kemeny ranking aggregation developped in
\citet{CKS17}, this boils down to recovering conditional Kemeny medians of
$\Sigma$ given $X$ from i.i.d. training examples $(X_1, \Sigma_1),\; \ldots,\;
(X_N, \Sigma_N)$. For this reason, this statistical learning problem is
referred to as \textit{ranking median regression} here. Our contribution is
twofold. We first propose a probabilistic theory of ranking median regression:
the set of optimal elements is characterized, the performance of empirical risk
minimizers is investigated in this context and situations where fast learning
rates can be achieved are also exhibited. Next we introduce the concept of
local consensus/median, in order to derive efficient methods for ranking median
regression. The major advantage of this local learning approach lies in its
close connection with the widely studied Kemeny aggregation problem. From an
algorithmic perspective, this permits to build predictive rules for ranking
median regression by implementing efficient techniques for (approximate) Kemeny
median computations at a local level in a tractable manner. In particular,
versions of $k$-nearest neighbor and tree-based methods, tailored to ranking
median regression, are investigated. Accuracy of piecewise constant ranking
median regression rules is studied under a specific smoothness assumption for
$\Sigma$&apos;s conditional distribution given $X$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Clemencon_S/0/1/0/all/0/1&quot;&gt;Stephan Cl&amp;#xe9;men&amp;#xe7;on&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Korba_A/0/1/0/all/0/1&quot;&gt;Anna Korba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sibony_E/0/1/0/all/0/1&quot;&gt;Eric Sibony&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.02675">
<title>How consistent is my model with the data? Information-Theoretic Model Check. (arXiv:1712.02675v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.02675</link>
<description rdf:parseType="Literal">&lt;p&gt;The choice of model class is fundamental in statistical learning and system
identification, no matter whether the class is derived from physical principles
or is a generic black-box. We develop a method to evaluate the specified model
class by assessing its capability of reproducing data that is similar to the
observed data record. This model check is based on the information-theoretic
properties of models viewed as data generators and is applicable to e.g.
sequential data and nonlinear dynamical models. The method can be understood as
a specific two-sided posterior predictive test. We apply the
information-theoretic model check to both synthetic and real data and compare
it with a classical whiteness test.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Svensson_A/0/1/0/all/0/1&quot;&gt;Andreas Svensson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zachariah_D/0/1/0/all/0/1&quot;&gt;Dave Zachariah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schon_T/0/1/0/all/0/1&quot;&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.06424">
<title>Learning to Write Stylized Chinese Characters by Reading a Handful of Examples. (arXiv:1712.06424v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1712.06424</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically writing stylized Chinese characters is an attractive yet
challenging task due to its wide applicabilities. In this paper, we propose a
novel framework named Style-Aware Variational Auto-Encoder (SA-VAE) to flexibly
generate Chinese characters. Specifically, we propose to capture the different
characteristics of a Chinese character by disentangling the latent features
into content-related and style-related components. Considering of the complex
shapes and structures, we incorporate the structure information as prior
knowledge into our framework to guide the generation. Our framework shows a
powerful one-shot/low-shot generalization ability by inferring the style
component given a character with unseen style. To the best of our knowledge,
this is the first attempt to learn to write new-style Chinese characters by
observing only one or a few examples. Extensive experiments demonstrate its
effectiveness in generating different stylized Chinese characters by fusing the
feature vectors corresponding to different contents and styles, which is of
significant importance in real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Danyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tongzheng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongxun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;</dc:creator>
</item></rdf:RDF>