<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02476"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02508"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02620"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02628"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02702"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02816"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02827"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02393"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02422"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02512"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02573"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02657"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02717"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02747"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02759"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02772"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02792"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02808"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02851"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02884"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02929"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02952"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02969"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03022"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03048"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03065"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03115"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.03124"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07893"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04585"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.01508"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02549"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02605"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02668"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02686"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02693"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02704"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02756"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02799"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02948"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02998"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1607.03200"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.05575"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.09283"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.10225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.03078"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.02840"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.08310"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10769"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02305"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.05363"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10284"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.03834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.10110"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09319"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09514"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04383"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.10815"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.02476">
<title>Associative Compression Networks. (arXiv:1804.02476v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.02476</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Associative Compression Networks (ACNs), a new
framework for variational autoencoding with neural networks. The system differs
from existing variational autoencoders in that the prior distribution used to
model each code is conditioned on a similar code from the dataset. In
compression terms this equates to sequentially transmitting the data using an
ordering determined by proximity in latent space. As the prior need only
account for local, rather than global variations in the latent space, the
coding cost is greatly reduced, leading to rich, informative codes, even when
autoregressive decoders are used. Experimental results on MNIST, CIFAR-10,
ImageNet and CelebA show that ACNs can yield improved dataset compression
relative to order-agnostic generative models, with an upper bound of 73.9 nats
per image on binarized MNIST. They also demonstrate that ACNs learn high-level
features such as object class, writing style, pose and facial expression, which
can be used to cluster and classify the data, as well as to generate diverse
and convincing samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graves_A/0/1/0/all/0/1&quot;&gt;Alex Graves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1&quot;&gt;Jacob Menick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oord_A/0/1/0/all/0/1&quot;&gt;Aaron van den Oord&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02508">
<title>Evolution leads to a diversity of motion-detection neuronal circuits. (arXiv:1804.02508v1 [q-bio.PE])</title>
<link>http://arxiv.org/abs/1804.02508</link>
<description rdf:parseType="Literal">&lt;p&gt;A central goal of evolutionary biology is to explain the origins and
distribution of diversity across life. Beyond species or genetic diversity, we
also observe diversity in the circuits (genetic or otherwise) underlying
complex functional traits. However, while the theory behind the origins and
maintenance of genetic and species diversity has been studied for decades,
theory concerning the origin of diverse functional circuits is still in its
infancy. It is not how many different circuit structures can implement any
given function, which evolutionary factors lead to different circuits, and
whether the evolution of a particular circuit was due to adaptive or
non-adaptive processes. Here, we use digital experimental evolution to study
the diversity of neural circuits that encode motion detection in digital
(artificial) brains. We find that evolution leads to an enormous diversity of
potential neural architectures encoding motion detection circuits, even for
circuits encoding the exact same function. Evolved circuits vary in both
redundancy and complexity (as previously found in genetic circuits) suggesting
that similar evolutionary principles underlie circuit formation using any
substrate. We also show that a simple (designed) motion detection circuit that
is optimally-adapted gains in complexity when evolved further, and that
selection for mutational robustness led this gain in complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tehrani_Saleh_A/0/1/0/all/0/1&quot;&gt;Ali Tehrani-Saleh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+LaBar_T/0/1/0/all/0/1&quot;&gt;Thomas LaBar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Adami_C/0/1/0/all/0/1&quot;&gt;Christoph Adami&lt;/a&gt; (Michigan State University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02620">
<title>A Proposal of Interactive Growing Hierarchical SOM. (arXiv:1804.02620v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.02620</link>
<description rdf:parseType="Literal">&lt;p&gt;Self Organizing Map is trained using unsupervised learning to produce a
two-dimensional discretized representation of input space of the training
cases. Growing Hierarchical SOM is an architecture which grows both in a
hierarchical way representing the structure of data distribution and in a
horizontal way representation the size of each individual maps. The control
method of the growing degree of GHSOM by pruning off the redundant branch of
hierarchy in SOM is proposed in this paper. Moreover, the interface tool for
the proposed method called interactive GHSOM is developed. We discuss the
computation results of Iris data by using the developed tool.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamaguchi_T/0/1/0/all/0/1&quot;&gt;Takashi Yamaguchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02628">
<title>Clustering and Retrieval Method of Immunological Memory Cell in Clonal Selection Algorithm. (arXiv:1804.02628v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.02628</link>
<description rdf:parseType="Literal">&lt;p&gt;The clonal selection principle explains the basic features of an adaptive
immune response to a antigenic stimulus. It established the idea that only
those cells that recognize the antigens are selected to proliferate and
differentiate. This paper explains a computational implementation of the clonal
selection principle that explicitly takes into account the affinity maturation
of the immune response. Antibodies generated by the clonal selection algorithm
are clustered in some categories according to the affinity maturation, so that
immunological memory cells which respond to the specified pathogen are created.
Experimental results to classify the medical database of Coronary Heart Disease
databases are reported. For the dataset, our proposed method shows the 99.6\%
classification capability of training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamada_S/0/1/0/all/0/1&quot;&gt;Shin Kamada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02702">
<title>Ordinal Pooling Networks: For Preserving Information over Shrinking Feature Maps. (arXiv:1804.02702v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.02702</link>
<description rdf:parseType="Literal">&lt;p&gt;In the framework of convolutional neural networks that lie at the heart of
deep learning, downsampling is often performed with a max-pooling operation
that however completely discards the information from other activations in a
pooling region. To address this issue, a novel pooling scheme, Ordinal Pooling
Network (OPN), is introduced in this work. OPN rearranges all the elements of a
pooling region in a sequence and assigns different weights to all the elements
based upon their orders in the sequence, where the weights are learned via the
gradient-based optimisation. The results of our small-scale experiments on
image classification task on MNIST database demonstrate that this scheme leads
to a consistent improvement in the accuracy over max-pooling operation. This
improvement is expected to increase in the deep networks, where several layers
of pooling become necessary.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Ashwani Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02816">
<title>A Generation Method of Immunological Memory in Clonal Selection Algorithm by using Restricted Boltzmann Machines. (arXiv:1804.02816v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.02816</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a high technique of image processing is required to extract the
image features in real time. In our research, the tourist subject data are
collected from the Mobile Phone based Participatory Sensing (MPPS) system. Each
record consists of image files with GPS, geographic location name, user&apos;s
numerical evaluation, and comments written in natural language at sightseeing
spots where a user really visits. In our previous research, the famous
landmarks in sightseeing spot can be detected by Clonal Selection Algorithm
with Immunological Memory Cell (CSAIM). However, some landmarks was not
detected correctly by the previous method because they didn&apos;t have enough
amount of information for the feature extraction. In order to improve the
weakness, we propose the generation method of immunological memory by
Restricted Boltzmann Machines. To verify the effectiveness of the method, some
experiments for classification of the subjective data are executed by using
machine learning tools for Deep Learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamada_S/0/1/0/all/0/1&quot;&gt;Shin Kamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02827">
<title>Composing photomosaic images using clustering based evolutionary programming. (arXiv:1804.02827v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.02827</link>
<description rdf:parseType="Literal">&lt;p&gt;Photomosaic images are a type of images consisting of various tiny images. A
complete form can be seen clearly by viewing it from a long distance. Small
tiny images which replace blocks of the original image can be seen clearly by
viewing it from a short distance. In the past, many algorithms have been
proposed trying to automatically compose photomosaic images. Most of these
algorithms are designed with greedy algorithms to match the blocks with the
tiny images. To obtain a better visual sense and satisfy some commercial
requirements, a constraint that a tiny image should not be repeatedly used many
times is usually added. With the constraint, the photomosaic problem becomes a
combinatorial optimization problem. Evolutionary algorithms imitating the
process of natural selection are popular and powerful in combinatorial
optimization problems. However, little work has been done on applying
evolutionary algorithms to photomosaic problem. In this paper, we present an
algorithm called clustering based evolutionary programming to deal with the
problem. We give prior knowledge to the optimization algorithm which makes the
optimization process converges faster. In our experiment, the proposed
algorithm is compared with the state of the art algorithms and software. The
results indicate that our algorithm performs the best.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yaodong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jianfeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuen_S/0/1/0/all/0/1&quot;&gt;Shiu Yin Yuen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02393">
<title>Formal Ways for Measuring Relations between Concepts in Conceptual Spaces. (arXiv:1804.02393v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.02393</link>
<description rdf:parseType="Literal">&lt;p&gt;The highly influential framework of conceptual spaces provides a geometric
way of representing knowledge. Instances are represented by points in a
high-dimensional space and concepts are represented by regions in this space.
In this article, we extend our recent mathematical formalization of this
framework by providing quantitative mathematical definitions for measuring
relations between concepts: We develop formal ways for computing concept size,
subsethood, implication, similarity, and betweenness. This considerably
increases the representational capabilities of our formalization and makes it
the most thorough and comprehensive formalization of conceptual spaces
developed so far.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bechberger_L/0/1/0/all/0/1&quot;&gt;Lucas Bechberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhnberger_K/0/1/0/all/0/1&quot;&gt;Kai-Uwe K&amp;#xfc;hnberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02422">
<title>Predictive Process Monitoring Methods: Which One Suits Me Best?. (arXiv:1804.02422v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.02422</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive process monitoring has recently gained traction in academia and is
maturing also in companies. However, with the growing body of research, it
might be daunting for companies to navigate in this domain in order to find,
provided certain data, what can be predicted and what methods to use. The main
objective of this paper is developing a value-driven framework for classifying
existing work on predictive process monitoring. This objective is achieved by
systematically identifying, categorizing, and analyzing existing approaches for
predictive process monitoring. The review is then used to develop a
value-driven framework that can support organizations to navigate in the
predictive process monitoring field and help them to find value and exploit the
opportunities enabled by these analysis techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francescomarino_C/0/1/0/all/0/1&quot;&gt;Chiara Di Francescomarino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghidini_C/0/1/0/all/0/1&quot;&gt;Chiara Ghidini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Maria Maggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milani_F/0/1/0/all/0/1&quot;&gt;Fredrik Milani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02512">
<title>Efficient Reciprocal Collision Avoidance between Heterogeneous Agents Using CTMAT. (arXiv:1804.02512v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.02512</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel algorithm for reciprocal collision avoidance between
heterogeneous agents of different shapes and sizes. We present a novel CTMAT
representation based on medial axis transform to compute a tight fitting
bounding shape for each agent. Each CTMAT is represented using tuples, which
are composed of circular arcs and line segments. Based on the reciprocal
velocity obstacle formulation, we reduce the problem to solving a
low-dimensional linear programming between each pair of tuples belonging to
adjacent agents. We precompute the Minkowski Sums of tuples to accelerate the
runtime performance. Finally, we provide an efficient method to update the
orientation of each agent in a local manner. We have implemented the algorithm
and highlight its performance on benchmarks corresponding to road traffic
scenarios and different vehicles. The overall runtime performance is comparable
to prior multi-agent collision avoidance algorithms that use circular or
elliptical agents. Our approach is less conservative and results in fewer false
collisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuexin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02573">
<title>Hindsight is Only 50/50: Unsuitability of MDP based Approximate POMDP Solvers for Multi-resolution Information Gathering. (arXiv:1804.02573v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.02573</link>
<description rdf:parseType="Literal">&lt;p&gt;Partially Observable Markov Decision Processes (POMDPs) offer an elegant
framework to model sequential decision making in uncertain environments.
Solving POMDPs online is an active area of research and given the size of
real-world problems approximate solvers are used. Recently, a few approaches
have been suggested for solving POMDPs by using MDP solvers in conjunction with
imitation learning. MDP based POMDP solvers work well for some cases, while
catastrophically failing for others. The main failure point of such solvers is
the lack of motivation for MDP solvers to gain information, since under their
assumption the environment is either already known as much as it can be or the
uncertainty will disappear after the next step. However for solving POMDP
problems gaining information can lead to efficient solutions. In this paper we
derive a set of conditions where MDP based POMDP solvers are provably
sub-optimal. We then use the well-known tiger problem to demonstrate such
sub-optimality. We show that multi-resolution, budgeted information gathering
cannot be addressed using MDP based POMDP solvers. The contribution of the
paper helps identify the properties of a POMDP problem for which the use of MDP
based POMDP solvers is inappropriate, enabling better design choices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1&quot;&gt;Sankalp Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1&quot;&gt;Sanjiban Choudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1&quot;&gt;Sebastian Scherer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02657">
<title>Emotion Orientated Recommendation System for Hiroshima Tourist by Fuzzy Petri Net. (arXiv:1804.02657v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1804.02657</link>
<description rdf:parseType="Literal">&lt;p&gt;We developed an Android Smartophone application software for tourist
information system. Especially, the agent system recommends the sightseeing
spot and local hospitality corresponding to the current feelings. The system
such as concierge can estimate user&apos;s emotion and mood by Emotion Generating
Calculations and Mental State Transition Network. In this paper, the system
decides the next candidates for spots and foods by the reasoning of fuzzy Petri
Net in order to make more smooth communication between human and smartphone.
The system was developed for Hiroshima Tourist Information and described some
hospitality about the concierge system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichimura_T/0/1/0/all/0/1&quot;&gt;Takumi Ichimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tachibana_I/0/1/0/all/0/1&quot;&gt;Issei Tachibana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02717">
<title>DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills. (arXiv:1804.02717v1 [cs.GR])</title>
<link>http://arxiv.org/abs/1804.02717</link>
<description rdf:parseType="Literal">&lt;p&gt;A longstanding goal in character animation is to combine data-driven
specification of behavior with a system that can execute a similar behavior in
a physical simulation, thus enabling realistic responses to perturbations and
environmental variation. We show that well-known reinforcement learning (RL)
methods can be adapted to learn robust control policies capable of imitating a
broad range of example motion clips, while also learning complex recoveries,
adapting to changes in morphology, and accomplishing user-specified goals. Our
method handles keyframed motions, highly-dynamic actions such as
motion-captured flips and spins, and retargeted motions. By combining a
motion-imitation objective with a task objective, we can train characters that
react intelligently in interactive settings, e.g., by walking in a desired
direction or throwing a ball at a user-specified target. This approach thus
combines the convenience and motion quality of using motion clips to define the
desired style and appearance, with the flexibility and generality afforded by
RL methods and physics-based animation. We further explore a number of methods
for integrating multiple clips into the learning process to develop
multi-skilled agents capable of performing a rich repertoire of diverse skills.
We demonstrate results using multiple characters (human, Atlas robot, bipedal
dinosaur, dragon) and a large variety of skills, including locomotion,
acrobatics, and martial arts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xue Bin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panne_M/0/1/0/all/0/1&quot;&gt;Michiel van de Panne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02747">
<title>Fast Conditional Independence Test for Vector Variables with Large Sample Sizes. (arXiv:1804.02747v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.02747</link>
<description rdf:parseType="Literal">&lt;p&gt;We present and evaluate the Fast (conditional) Independence Test (FIT) -- a
nonparametric conditional independence test. The test is based on the idea that
when $P(X \mid Y, Z) = P(X \mid Y)$, $Z$ is not useful as a feature to predict
$X$, as long as $Y$ is also a regressor. On the contrary, if $P(X \mid Y, Z)
\neq P(X \mid Y)$, $Z$ might improve prediction results. FIT applies to
thousand-dimensional random variables with a hundred thousand samples in a
fraction of the time required by alternative methods. We provide an extensive
evaluation that compares FIT to six extant nonparametric independence tests.
The evaluation shows that FIT has low probability of making both Type I and
Type II errors compared to other tests, especially as the number of available
samples grows. Our implementation of FIT is publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chalupka_K/0/1/0/all/0/1&quot;&gt;Krzysztof Chalupka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perona_P/0/1/0/all/0/1&quot;&gt;Pietro Perona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eberhardt_F/0/1/0/all/0/1&quot;&gt;Frederick Eberhardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02759">
<title>Order Effects for Queries in Intelligent Systems. (arXiv:1804.02759v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.02759</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper examines common assumptions regarding the decision-making internal
environment for intelligent agents and investigates issues related to
processing of memory and belief states to help obtain better understanding of
the responses. In specific, we consider order effects and discuss both
classical and non-classical explanations for them. We also consider implicit
cognition and explore if certain inaccessible states may be best modeled as
quantum states. We propose that the hypothesis that quantum states are at the
basis of order effects be tested on large databases such as those related to
medical treatment and drug efficacy. A problem involving a maze network is
considered and comparisons made between classical and quantum decision
scenarios for it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kak_S/0/1/0/all/0/1&quot;&gt;Subhash Kak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02772">
<title>Active Mini-Batch Sampling using Repulsive Point Processes. (arXiv:1804.02772v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.02772</link>
<description rdf:parseType="Literal">&lt;p&gt;The convergence speed of stochastic gradient descent (SGD) can be improved by
actively selecting mini-batches. We explore sampling schemes where similar data
points are less likely to be selected in the same mini-batch. In particular, we
prove that such repulsive sampling schemes lowers the variance of the gradient
estimator. This generalizes recent work on using Determinantal Point Processes
(DPPs) for mini-batch diversification (Zhang et al., 2017) to the broader class
of repulsive point processes. We first show that the phenomenon of variance
reduction by diversified sampling generalizes in particular to non-stationary
point processes. We then show that other point processes may be computationally
much more efficient than DPPs. In particular, we propose and investigate
Poisson Disk sampling---frequently encountered in the computer graphics
community---for this task. We show empirically that our approach improves over
standard SGD both in terms of convergence speed as well as final model
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oztireli_C/0/1/0/all/0/1&quot;&gt;Cengiz &amp;#xd6;ztireli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salvi_G/0/1/0/all/0/1&quot;&gt;Giampiero Salvi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02792">
<title>Occluded Person Re-identification. (arXiv:1804.02792v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.02792</link>
<description rdf:parseType="Literal">&lt;p&gt;Person re-identification (re-id) suffers from a serious occlusion problem
when applied to crowded public places. In this paper, we propose to retrieve a
full-body person image by using a person image with occlusions. This differs
significantly from the conventional person re-id problem where it is assumed
that person images are detected without any occlusion. We thus call this new
problem the occluded person re-identitification. To address this new problem,
we propose a novel Attention Framework of Person Body (AFPB) based on deep
learning, consisting of 1) an Occlusion Simulator (OS) which automatically
generates artificial occlusions for full-body person images, and 2) multi-task
losses that force the neural network not only to discriminate a person&apos;s
identity but also to determine whether a sample is from the occluded data
distribution or the full-body data distribution. Experiments on a new occluded
person re-id dataset and three existing benchmarks modified to include
full-body person images and occluded person images show the superiority of the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Jianhuang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangcong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02808">
<title>Latent Space Policies for Hierarchical Reinforcement Learning. (arXiv:1804.02808v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.02808</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of learning hierarchical deep neural network policies
for reinforcement learning. Our aim is to design a hierarchical reinforcement
learning algorithm that can construct hierarchical representations in bottom-up
layerwise fashion. In contrast to methods that explicitly restrict or cripple
lower layers of a hierarchy to force them to use higher-level modulating
signals, each layer in our framework is trained to directly solve the task, but
acquires a range of diverse strategies via a maximum entropy reinforcement
learning objective. Each layer is also augmented with latent random variables,
which are sampled from a prior distribution during the training of that layer.
The maximum entropy objective causes these latent variables to be incorporated
into the layer&apos;s policy, and the higher level layer can directly control the
behavior of the lower layer through this latent space. Furthermore, by
constraining the mapping from latent variables to actions to be invertible,
higher layers retain full expressivity: neither the higher layers nor the lower
layers are constrained in their behavior. Our experimental evaluation
demonstrates that we can improve on the performance of single-layer policies on
standard benchmark tasks simply by adding additional layers, and that our
method can solve more complex sparse-reward tasks by learning higher-level
policies on top of high-entropy skills optimized for simple low-level
objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haarnoja_T/0/1/0/all/0/1&quot;&gt;Tuomas Haarnoja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartikainen_K/0/1/0/all/0/1&quot;&gt;Kristian Hartikainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02851">
<title>Whale swarm algorithm with iterative counter for multimodal function optimization. (arXiv:1804.02851v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.02851</link>
<description rdf:parseType="Literal">&lt;p&gt;Most real-world optimization problems often come with multiple global optima
or local optima. Therefore, increasing niching metaheuristic algorithms, which
devote to finding multiple optima in a single run, are developed to solve these
multimodal optimization problems. However, there are two difficulties urgently
to be solved for most existing niching metaheuristic algorithms: how to set the
optimal values of niching parameters for different optimization problems, and
how to jump out of the local optima efficiently. These two difficulties limited
their practicality largely. Based on Whale Swarm Algorithm (WSA) we proposed
previously, this paper presents a new multimodal optimizer named WSA with
Iterative Counter (WSA-IC) to address these two difficulties. In the one hand,
WSA-IC improves the iteration rule of the original WSA for multimodal
optimization, which removes the need of specifying different values of
attenuation coefficient for different problems to form multiple subpopulations,
without introducing any niching parameter. In the other hand, WSA-IC enables
the identification of extreme point during iterations relying on two new
parameters (i.e., stability threshold Ts and fitness threshold Tf), to jump out
of the located extreme point. The proposed WSA-IC is compared with several
state-of-the-art niching metaheuristic algorithms on CEC2015 niching benchmark
test functions and five additional classical multimodal functions with high
dimensions. The experimental results demonstrate that WSA-IC statistically
outperforms other niching metaheuristic algorithms on most test functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1&quot;&gt;Bing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Liang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuyan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02884">
<title>Policy Gradient With Value Function Approximation For Collective Multiagent Planning. (arXiv:1804.02884v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.02884</link>
<description rdf:parseType="Literal">&lt;p&gt;Decentralized (PO)MDPs provide an expressive framework for sequential
decision making in a multiagent system. Given their computational complexity,
recent research has focused on tractable yet practical subclasses of
Dec-POMDPs. We address such a subclass called CDEC-POMDP where the collective
behavior of a population of agents affects the joint-reward and environment
dynamics. Our main contribution is an actor-critic (AC) reinforcement learning
method for optimizing CDEC-POMDP policies. Vanilla AC has slow convergence for
larger problems. To address this, we show how a particular decomposition of the
approximate action-value function over agents leads to effective updates, and
also derive a new way to train the critic based on local reward signals.
Comparisons on a synthetic benchmark and a real-world taxi fleet optimization
problem show that our new AC approach provides better quality solutions than
previous best approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duc Thien Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Akshat Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_H/0/1/0/all/0/1&quot;&gt;Hoong Chuin Lau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02929">
<title>First Experiments with a Flexible Infrastructure for Normative Reasoning. (arXiv:1804.02929v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.02929</link>
<description rdf:parseType="Literal">&lt;p&gt;A flexible infrastructure for normative reasoning is outlined. A small-scale
demonstrator version of the envisioned system has been implemented in the proof
assistant Isabelle/HOL by utilising the first authors universal logical
reasoning approach based on shallow semantical embeddings in meta-logic HOL.
The need for such a flexible reasoning infrastructure is motivated and
illustrated with a contrary-to-duty example scenario selected from the General
Data Protection Regulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benzmuller_C/0/1/0/all/0/1&quot;&gt;Christoph Benzm&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parent_X/0/1/0/all/0/1&quot;&gt;Xavier Parent&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02952">
<title>A theory of consciousness: computation, algorithm, and neurobiological realization. (arXiv:1804.02952v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1804.02952</link>
<description rdf:parseType="Literal">&lt;p&gt;The most enigmatic aspect of consciousness is the fact that it is felt, as a
subjective sensation. This particular aspect is explained by the theory
proposed here. The theory encompasses both the computation that is presumably
involved and the way in which that computation may be realized in the brain&apos;s
neurobiology. It is assumed that the brain makes an internal estimate of an
individual&apos;s own evolutionary fitness, which can be shown to produce an
irreducible, distinct cause. Communicating components of the fitness estimate
(either for external or internal use) requires inverting them. Such inversion
can be performed by the thalamocortical feedback loop in the mammalian brain,
if that loop is operating in a switched, dual-stage mode. A first
(nonconscious) stage produces forward estimates, whereas the second (conscious)
stage inverts those estimates. It is argued that inversion produces
irreducible, distinct, and spatially localized causes, which are plausibly
sensed as the feeling of consciousness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hateren_J/0/1/0/all/0/1&quot;&gt;J. H. van Hateren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02969">
<title>A review of possible effects of cognitive biases on interpretation of rule-based machine learning models. (arXiv:1804.02969v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.02969</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates to what extent do cognitive biases affect human
understanding of interpretable machine learning models, in particular of rules
discovered from data. Twenty cognitive biases (illusions, effects) are covered,
as are possibly effective debiasing techniques that can be adopted by designers
of machine learning algorithms and software. While there seems no universal
approach for eliminating all the identified cognitive biases, it follows from
our analysis that the effect of most biases can be ameliorated by making
rule-based models more concise. Due to lack of previous research, our review
transfers general results obtained in cognitive psychology to the domain of
machine learning. It needs to be succeeded by empirical studies specifically
aimed at the machine learning domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kliegr_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Kliegr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Furnkranz_J/0/1/0/all/0/1&quot;&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bahnik_S/0/1/0/all/0/1&quot;&gt;&amp;#x160;t&amp;#x11b;p&amp;#xe1;n Bahn&amp;#xed;k&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03022">
<title>Learning at the Ends: From Hand to Tool Affordances in Humanoid Robots. (arXiv:1804.03022v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1804.03022</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the open challenges in designing robots that operate successfully in
the unpredictable human environment is how to make them able to predict what
actions they can perform on objects, and what their effects will be, i.e., the
ability to perceive object affordances. Since modeling all the possible world
interactions is unfeasible, learning from experience is required, posing the
challenge of collecting a large amount of experiences (i.e., training data).
Typically, a manipulative robot operates on external objects by using its own
hands (or similar end-effectors), but in some cases the use of tools may be
desirable, nevertheless, it is reasonable to assume that while a robot can
collect many sensorimotor experiences using its own hands, this cannot happen
for all possible human-made tools.
&lt;/p&gt;
&lt;p&gt;Therefore, in this paper we investigate the developmental transition from
hand to tool affordances: what sensorimotor skills that a robot has acquired
with its bare hands can be employed for tool use? By employing a visual and
motor imagination mechanism to represent different hand postures compactly, we
propose a probabilistic model to learn hand affordances, and we show how this
model can generalize to estimate the affordances of previously unseen tools,
ultimately supporting planning, decision-making and tool selection tasks in
humanoid robots. We present experimental results with the iCub humanoid robot,
and we publicly release the collected sensorimotor data in the form of a hand
posture affordances dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saponaro_G/0/1/0/all/0/1&quot;&gt;Giovanni Saponaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vicente_P/0/1/0/all/0/1&quot;&gt;Pedro Vicente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehban_A/0/1/0/all/0/1&quot;&gt;Atabak Dehban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamone_L/0/1/0/all/0/1&quot;&gt;Lorenzo Jamone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernardino_A/0/1/0/all/0/1&quot;&gt;Alexandre Bernardino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_Victor_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Santos-Victor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03048">
<title>Clustrophile 2: Guided Visual Clustering Analysis. (arXiv:1804.03048v1 [cs.HC])</title>
<link>http://arxiv.org/abs/1804.03048</link>
<description rdf:parseType="Literal">&lt;p&gt;Data clustering is a common unsupervised learning method frequently used in
exploratory data analysis. However, identifying relevant structures in
unlabeled, high-dimensional data is nontrivial, requiring iterative
experimentation with clustering parameters as well as data features and
instances. The space of possible clusterings for a typical dataset is vast, and
navigating in this vast space is also challenging. The absence of ground-truth
labels makes it impossible to define an optimal solution, thus requiring user
judgment to establish what can be considered a satisfiable clustering result.
Data scientists need adequate interactive tools to effectively explore and
navigate the large space of clusterings so as to improve the effectiveness of
exploratory clustering analysis. We introduce \textit{Clustrophile 2}, a new
interactive tool for guided clustering analysis. \textit{Clustrophile 2} guides
users in clustering-based exploratory analysis, adapts user feedback to improve
user guidance, facilitates the interpretation of clusters, and helps quickly
reason about differences between clusterings. To this end, \textit{Clustrophile
2} contributes a novel feature, the clustering tour, to help users choose
clustering parameters and assess the quality of different clustering results in
relation to current analysis goals and user expectations. We evaluate
\textit{Clustrophile 2} through a user study with 12 data scientists, who used
our tool to explore and interpret sub-cohorts in a dataset of Parkinson&apos;s
disease patients. Results suggest that \textit{Clustrophile 2} improves the
speed and effectiveness of exploratory clustering analysis for both experts and
non-experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavallo_M/0/1/0/all/0/1&quot;&gt;Marco Cavallo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1&quot;&gt;&amp;#xc7;a&amp;#x11f;atay Demiralp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03065">
<title>Faster Anomaly Detection via Matrix Sketching. (arXiv:1804.03065v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.03065</link>
<description rdf:parseType="Literal">&lt;p&gt;We present efficient streaming algorithms to compute two commonly used
anomaly measures: the rank-$k$ leverage scores (aka Mahalanobis distance) and
the rank-$k$ projection distance, in the row-streaming model. We show that
commonly used matrix sketching techniques such as the Frequent Directions
sketch and random projections can be used to approximate these measures. Our
main technical contribution is to prove matrix perturbation inequalities for
operators arising in the computation of these measures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalan_P/0/1/0/all/0/1&quot;&gt;Parikshit Gopalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharan_V/0/1/0/all/0/1&quot;&gt;Vatsal Sharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wieder_U/0/1/0/all/0/1&quot;&gt;Udi Wieder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03115">
<title>AMNet: Memorability Estimation with Attention. (arXiv:1804.03115v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.03115</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present the design and evaluation of an end-to-end
trainable, deep neural network with a visual attention mechanism for
memorability estimation in still images. We analyze the suitability of transfer
learning of deep models from image classification to the memorability task.
Further on we study the impact of the attention mechanism on the memorability
estimation and evaluate our network on the SUN Memorability and the LaMem
datasets. Our network outperforms the existing state of the art models on both
datasets in terms of the Spearman&apos;s rank correlation as well as the mean
squared error, closely matching human consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fajtl_J/0/1/0/all/0/1&quot;&gt;Jiri Fajtl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Argyriou_V/0/1/0/all/0/1&quot;&gt;Vasileios Argyriou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monekosso_D/0/1/0/all/0/1&quot;&gt;Dorothy Monekosso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Remagnino_P/0/1/0/all/0/1&quot;&gt;Paolo Remagnino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.03124">
<title>Leveraging Intra-User and Inter-User Representation Learning for Automated Hate Speech Detection. (arXiv:1804.03124v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.03124</link>
<description rdf:parseType="Literal">&lt;p&gt;Hate speech detection is a critical, yet challenging problem in Natural
Language Processing (NLP). Despite the existence of numerous studies dedicated
to the development of NLP hate speech detection approaches, the accuracy is
still poor. The central problem is that social media posts are short and noisy,
and most existing hate speech detection solutions take each post as an isolated
input instance, which is likely to yield high false positive and negative
rates. In this paper, we radically improve automated hate speech detection by
presenting a novel model that leverages intra-user and inter-user
representation learning for robust hate speech detection on Twitter. In
addition to the target Tweet, we collect and analyze the user&apos;s historical
posts to model intra-user Tweet representations. To suppress the noise in a
single Tweet, we also model the similar Tweets posted by all other users with
reinforced inter-user representation learning techniques. Experimentally, we
show that leveraging these two representations can significantly improve the
f-score of a strong bidirectional LSTM baseline model by 10.1%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1&quot;&gt;Jing Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ElSherief_M/0/1/0/all/0/1&quot;&gt;Mai ElSherief&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belding_E/0/1/0/all/0/1&quot;&gt;Elizabeth M. Belding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07893">
<title>A Deep Policy Inference Q-Network for Multi-Agent Systems. (arXiv:1712.07893v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07893</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DPIQN, a deep policy inference Q-network that targets multi-agent
systems composed of controllable agents, collaborators, and opponents that
interact with each other. We focus on one challenging issue in such
systems---modeling agents with varying strategies---and propose to employ
&quot;policy features&quot; learned from raw observations (e.g., raw images) of
collaborators and opponents by inferring their policies. DPIQN incorporates the
learned policy features as a hidden vector into its own deep Q-network (DQN),
such that it is able to predict better Q values for the controllable agents
than the state-of-the-art deep reinforcement learning models. We further
propose an enhanced version of DPIQN, called deep recurrent policy inference
Q-network (DRPIQN), for handling partial observability. Both DPIQN and DRPIQN
are trained by an adaptive training procedure, which adjusts the network&apos;s
attention to learn the policy features and its own Q-values at different phases
of the training process. We present a comprehensive analysis of DPIQN and
DRPIQN, and highlight their effectiveness and generalizability in various
multi-agent settings. Our models are evaluated in a classic soccer game
involving both competitive and collaborative scenarios. Experimental results
performed on 1 vs. 1 and 2 vs. 2 games show that DPIQN and DRPIQN demonstrate
superior performance to the baseline DQN and deep recurrent Q-network (DRQN)
models. We also explore scenarios in which collaborators or opponents
dynamically change their policies, and show that DPIQN and DRPIQN do lead to
better overall performance in terms of stability and mean scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1&quot;&gt;Zhang-Wei Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shih-Yang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shann_T/0/1/0/all/0/1&quot;&gt;Tzu-Yun Shann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yi-Hsiang Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chun-Yi Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04585">
<title>Categorizing Variants of Goodhart&apos;s Law. (arXiv:1803.04585v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04585</link>
<description rdf:parseType="Literal">&lt;p&gt;There are several distinct failure modes for overoptimization of systems on
the basis of metrics. This occurs when a metric which can be used to improve a
system is used to an extent that further optimization is ineffective or
harmful, and is sometimes termed Goodhart&apos;s Law. This class of failure is often
poorly understood, partly because terminology for discussing them is ambiguous,
and partly because discussion using this ambiguous terminology ignores
distinctions between different failure modes of this general type. This paper
expands on an earlier discussion by Garrabrant, which notes there are &quot;(at
least) four different mechanisms&quot; that relate to Goodhart&apos;s Law. This paper is
intended to explore these mechanisms further, and specify more clearly how they
occur. This discussion should be helpful in better understanding these types of
failures in economic regulation, in public policy, in machine learning, and in
Artificial Intelligence alignment. The importance of Goodhart effects depends
on the amount of power directed towards optimizing the proxy, and so the
increased optimization power offered by artificial intelligence makes it
especially critical for that field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manheim_D/0/1/0/all/0/1&quot;&gt;David Manheim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garrabrant_S/0/1/0/all/0/1&quot;&gt;Scott Garrabrant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.01508">
<title>The Tsetlin Machine - A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic. (arXiv:1804.01508v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1804.01508</link>
<description rdf:parseType="Literal">&lt;p&gt;Although simple individually, artificial neurons provide state-of-the-art
performance when interconnected in deep networks. Unknown to many, there exists
an arguably even simpler and more versatile learning mechanism, namely, the
Tsetlin Automaton. Merely by means of a single integer as memory, it learns the
optimal action in stochastic environments. In this paper, we introduce the
Tsetlin Machine, which solves complex pattern recognition problems with
easy-to-interpret propositional formulas, composed by a collective of Tsetlin
Automata. To eliminate the longstanding problem of vanishing signal-to-noise
ratio, the Tsetlin Machine orchestrates the automata using a novel game. Our
theoretical analysis establishes that the Nash equilibria of the game are
aligned with the propositional formulas that provide optimal pattern
recognition accuracy. This translates to learning without local optima, only
global ones. We argue that the Tsetlin Machine finds the propositional formula
that provides optimal accuracy, with probability arbitrarily close to unity. In
four distinct benchmarks, the Tsetlin Machine outperforms both Neural Networks,
SVMs, Random Forests, the Naive Bayes Classifier and Logistic Regression. It
further turns out that the accuracy advantage of the Tsetlin Machine increases
with lack of data. The Tsetlin Machine has a significant computational
performance advantage since both inputs, patterns, and outputs are expressed as
bits, while recognition of patterns relies on bit manipulation. The combination
of accuracy, interpretability, and computational simplicity makes the Tsetlin
Machine a promising tool for a wide range of domains, including safety-critical
medicine. Being the first of its kind, we believe the Tsetlin Machine will
kick-start completely new paths of research, with a potentially significant
impact on the AI field and the applications of AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1&quot;&gt;Ole-Christoffer Granmo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02549">
<title>A comparison of recent waveform generation and acoustic modeling methods for neural-network-based speech synthesis. (arXiv:1804.02549v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1804.02549</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in speech synthesis suggest that limitations such as the
lossy nature of the amplitude spectrum with minimum phase approximation and the
over-smoothing effect in acoustic modeling can be overcome by using advanced
machine learning approaches. In this paper, we build a framework in which we
can fairly compare new vocoding and acoustic modeling techniques with
conventional approaches by means of a large scale crowdsourced evaluation.
Results on acoustic models showed that generative adversarial networks and an
autoregressive (AR) model performed better than a normal recurrent network and
the AR model performed best. Evaluation on vocoders by using the same AR
acoustic model demonstrated that a Wavenet vocoder outperformed classical
source-filter-based vocoders. Particularly, generated speech waveforms from the
combination of AR acoustic model and Wavenet vocoder achieved a similar score
of speech quality to vocoded speech.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1&quot;&gt;Jaime Lorenzo-Trueba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Takaki_S/0/1/0/all/0/1&quot;&gt;Shinji Takaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Juvela_L/0/1/0/all/0/1&quot;&gt;Lauri Juvela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02605">
<title>Moving Beyond Sub-Gaussianity in High-Dimensional Statistics: Applications in Covariance Estimation and Linear Regression. (arXiv:1804.02605v1 [math.ST])</title>
<link>http://arxiv.org/abs/1804.02605</link>
<description rdf:parseType="Literal">&lt;p&gt;Concentration inequalities form an essential toolkit in the study of
high-dimensional statistical methods. Most of the relevant statistics
literature is based on the assumptions of sub-Gaussian/sub-exponential random
vectors. In this paper, we bring together various probability inequalities for
sums of independent random variables under much weaker exponential type
(sub-Weibull) tail assumptions. These results extract a part sub-Gaussian tail
behavior in finite samples, matching the asymptotics governed by the central
limit theorem, and are compactly represented in terms of a new Orlicz
quasi-norm - the Generalized Bernstein-Orlicz norm - that typifies such tail
behaviors.
&lt;/p&gt;
&lt;p&gt;We illustrate the usefulness of these inequalities through the analysis of
four fundamental problems in high-dimensional statistics. In the first two
problems, we study the rate of convergence of the sample covariance matrix in
terms of the maximum elementwise norm and the maximum k-sub-matrix operator
norm which are key quantities of interest in bootstrap procedures and
high-dimensional structured covariance matrix estimation. The third example
concerns the restricted eigenvalue condition, required in high dimensional
linear regression, which we verify for all sub-Weibull random vectors under
only marginal (not joint) tail assumptions on the covariates. To our knowledge,
this is the first unified result obtained in such generality. In the final
example, we consider the Lasso estimator for linear regression and establish
its rate of convergence under much weaker tail assumptions (on the errors as
well as the covariates) than those in the existing literature. The common
feature in all our results is that the convergence rates under most exponential
tails match the usual ones under sub-Gaussian assumptions. Finally, we also
establish a high-dimensional CLT and tail bounds for empirical processes for
sub-Weibulls.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kuchibhotla_A/0/1/0/all/0/1&quot;&gt;Arun Kumar Kuchibhotla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chakrabortty_A/0/1/0/all/0/1&quot;&gt;Abhishek Chakrabortty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02668">
<title>Accelerating Prototype-Based Drug Discovery using Conditional Diversity Networks. (arXiv:1804.02668v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.02668</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing a new drug is a lengthy and expensive process. As the space of
potential molecules is very large (10^23-10^60), a common technique during drug
discovery is to start from a molecule which already has some of the desired
properties. An interdisciplinary team of scientists generates hypothesis about
the required changes to the prototype. In this work, we develop an algorithmic
unsupervised-approach that automatically generates potential drug molecules
given a prototype drug. We show that the molecules generated by the system are
valid molecules and significantly different from the prototype drug. Out of the
compounds generated by the system, we identified 35 FDA-approved drugs. As an
example, our system generated Isoniazid - one of the main drugs for
Tuberculosis. The system is currently being deployed for use in collaboration
with pharmaceutical companies to further analyze the additional generated
molecules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harel_S/0/1/0/all/0/1&quot;&gt;Shahar Harel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1&quot;&gt;Kira Radinsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02686">
<title>Complex energy landscapes in spiked-tensor and simple glassy models: ruggedness, arrangements of local minima and phase transitions. (arXiv:1804.02686v1 [cond-mat.dis-nn])</title>
<link>http://arxiv.org/abs/1804.02686</link>
<description rdf:parseType="Literal">&lt;p&gt;We study rough high-dimensional landscapes in which an increasingly stronger
preference for a given configuration emerges. Such energy landscapes arise in
glass physics and inference. In particular we focus on random Gaussian
functions, and on the spiked-tensor model and generalizations. We thoroughly
analyze the statistical properties of the corresponding landscapes and
characterize the associated geometrical phase transitions. In order to perform
our study, we develop a framework based on the Kac-Rice method that allows to
compute the complexity of the landscape, i.e. the logarithm of the typical
number of stationary points and their Hessian. This approach generalizes the
one used to compute rigorously the annealed complexity of mean-field glass
models. We discuss its advantages with respect to previous frameworks, in
particular the thermodynamical replica method which is shown to lead to
partially incorrect predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ros_V/0/1/0/all/0/1&quot;&gt;Valentina Ros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Arous_G/0/1/0/all/0/1&quot;&gt;Gerard Ben Arous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Biroli_G/0/1/0/all/0/1&quot;&gt;Giulio Biroli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Cammarota_C/0/1/0/all/0/1&quot;&gt;Chiara Cammarota&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02693">
<title>Path to Stochastic Stability: Comparative Analysis of Stochastic Learning Dynamics in Games. (arXiv:1804.02693v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.02693</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic stability is a popular solution concept for stochastic learning
dynamics in games. However, a critical limitation of this solution concept is
its inability to distinguish between different learning rules that lead to the
same steady-state behavior. We address this limitation for the first time and
develop a framework for the comparative analysis of stochastic learning
dynamics with different update rules but same steady-state behavior. We present
the framework in the context of two learning dynamics: Log-Linear Learning
(LLL) and Metropolis Learning (ML). Although both of these dynamics have the
same stochastically stable states, LLL and ML correspond to different
behavioral models for decision making. Moreover, we demonstrate through an
example setup of sensor coverage game that for each of these dynamics, the
paths to stochastically stable states exhibit distinctive behaviors. Therefore,
we propose multiple criteria to analyze and quantify the differences in the
short and medium run behavior of stochastic learning dynamics. We derive and
compare upper bounds on the expected hitting time to the set of Nash equilibria
for both LLL and ML. For the medium to long-run behavior, we identify a set of
tools from the theory of perturbed Markov chains that result in a hierarchical
decomposition of the state space into collections of states called cycles. We
compare LLL and ML based on the proposed criteria and develop invaluable
insights into the comparative behavior of the two dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaleel_H/0/1/0/all/0/1&quot;&gt;Hassan Jaleel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamma_J/0/1/0/all/0/1&quot;&gt;Jeff S. Shamma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02704">
<title>Discovering Process Maps from Event Streams. (arXiv:1804.02704v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.02704</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated process discovery is a class of process mining methods that allow
analysts to extract business process models from event logs. Traditional
process discovery methods extract process models from a snapshot of an event
log stored in its entirety. In some scenarios, however, events keep coming with
a high arrival rate to the extent that it is impractical to store the entire
event log and to continuously re-discover a process model from scratch. Such
scenarios require online process discovery approaches. Given an event stream
produced by the execution of a business process, the goal of an online process
discovery method is to maintain a continuously updated model of the process
with a bounded amount of memory while at the same time achieving similar
accuracy as offline methods. However, existing online discovery approaches
require relatively large amounts of memory to achieve levels of accuracy
comparable to that of offline methods. Therefore, this paper proposes an
approach that addresses this limitation by mapping the problem of online
process discovery to that of cache memory management, and applying well-known
cache replacement policies to the problem of online process discovery. The
approach has been implemented in .NET, experimentally integrated with the Minit
process mining tool and comparatively evaluated against an existing baseline
using real-life datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leno_V/0/1/0/all/0/1&quot;&gt;Volodymyr Leno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armas_Cervantes_A/0/1/0/all/0/1&quot;&gt;Abel Armas-Cervantes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_M/0/1/0/all/0/1&quot;&gt;Marlon Dumas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_M/0/1/0/all/0/1&quot;&gt;Marcello La Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1&quot;&gt;Fabrizio M. Maggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02744">
<title>Unsupervised Learning of Mixture Models with a Uniform Background Component. (arXiv:1804.02744v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.02744</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian Mixture Models are one of the most studied and mature models in
unsupervised learning. However, outliers are often present in the data and
could influence the cluster estimation. In this paper, we study a new model
that assumes that data comes from a mixture of a number of Gaussians as well as
a uniform &quot;background&quot; component assumed to contain outliers and other
non-interesting observations. We develop a novel method based on robust loss
minimization that performs well in clustering such GMM with a uniform
background. We give theoretical guarantees for our clustering algorithm to
obtain best clustering results with high probability. Besides, we show that the
result of our algorithm does not depend on initialization or local optima, and
the parameter tuning is an easy task. By numeric simulations, we demonstrate
that our algorithm enjoys high accuracy and achieves the best clustering
results given a large enough sample size. Finally, experimental comparisons
with typical clustering methods on real datasets witness the potential of our
algorithm in real applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sida Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barbu_A/0/1/0/all/0/1&quot;&gt;Adrian Barbu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02756">
<title>Pointwise adaptation via stagewise aggregation of local estimates for multiclass classification. (arXiv:1804.02756v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.02756</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a problem of multiclass classification, where the training sample
$S_n = \{(X_i, Y_i)\}_{i=1}^n$ is generated from the model $\mathbb p(Y = m | X
= x) = \theta_m(x)$, $1 \leq m \leq M$, and $\theta_1(x), \dots, \theta_M(x)$
are unknown Lipschitz functions. Given a test point $X$, our goal is to
estimate $\theta_1(X), \dots, \theta_M(X)$. An approach based on nonparametric
smoothing uses a localization technique, i.e. the weight of observation $(X_i,
Y_i)$ depends on the distance between $X_i$ and $X$. However, local estimates
strongly depend on localizing scheme. In our solution we fix several schemes
$W_1, \dots, W_K$, compute corresponding local estimates
$\widetilde\theta^{(1)}, \dots, \widetilde\theta^{(K)}$ for each of them and
apply an aggregation procedure. We propose an algorithm, which constructs a
convex combination of the estimates $\widetilde\theta^{(1)}, \dots,
\widetilde\theta^{(K)}$ such that the aggregated estimate behaves approximately
as well as the best one from the collection $\widetilde\theta^{(1)}, \dots,
\widetilde\theta^{(K)}$. We also study theoretical properties of the procedure,
prove oracle results and establish rates of convergence under mild assumptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Puchkin_N/0/1/0/all/0/1&quot;&gt;Nikita Puchkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Spokoiny_V/0/1/0/all/0/1&quot;&gt;Vladimir Spokoiny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02799">
<title>Deep Learning of the Nonlinear Schr\&quot;odinger Equation in Fiber-Optic Communications. (arXiv:1804.02799v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1804.02799</link>
<description rdf:parseType="Literal">&lt;p&gt;An important problem in fiber-optic communications is to invert the nonlinear
Schr\&quot;odinger equation in real time to reverse the deterministic effects of the
channel. Interestingly, the popular split-step Fourier method (SSFM) leads to a
computation graph that is reminiscent of a deep neural network. This
observation allows one to leverage tools from machine learning to reduce
complexity. In particular, the main disadvantage of the SSFM is that its
complexity using M steps is at least M times larger than a linear equalizer.
This is because the linear SSFM operator is a dense matrix. In previous work,
truncation methods such as frequency sampling, wavelets, or least-squares have
been used to obtain &quot;cheaper&quot; operators that can be implemented using filters.
However, a large number of filter taps are typically required to limit
truncation errors. For example, Ip and Kahn showed that for a 10 Gbaud signal
and 2000 km optical link, a truncated SSFM with 25 steps would require 70-tap
filters in each step and 100 times more operations than linear equalization. We
find that, by jointly optimizing all filters with deep learning, the complexity
can be reduced significantly for similar accuracy. Using optimized 5-tap and
3-tap filters in an alternating fashion, one requires only around 2-6 times the
complexity of linear equalization, depending on the implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hager_C/0/1/0/all/0/1&quot;&gt;Christian H&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1&quot;&gt;Henry D. Pfister&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02948">
<title>Sample-Derived Disjunctive Rules for Secure Power System Operation. (arXiv:1804.02948v1 [cs.SY])</title>
<link>http://arxiv.org/abs/1804.02948</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning techniques have been used in the past using Monte Carlo
samples to construct predictors of the dynamic stability of power systems. In
this paper we move beyond the task of prediction and propose a comprehensive
approach to use predictors, such as Decision Trees (DT), within a standard
optimization framework for pre- and post-fault control purposes. In particular,
we present a generalizable method for embedding rules derived from DTs in an
operation decision-making model. We begin by pointing out the specific
challenges entailed when moving from a prediction to a control framework. We
proceed with introducing the solution strategy based on generalized disjunctive
programming (GDP) as well as a two-step search method for identifying optimal
hyper-parameters for balancing cost and control accuracy. We showcase how the
proposed approach constructs security proxies that cover multiple contingencies
while facing high-dimensional uncertainty with respect to operating conditions
with the use of a case study on the IEEE 39-bus system. The method is shown to
achieve efficient system control at a marginal increase in system price
compared to an oracle model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremer_J/0/1/0/all/0/1&quot;&gt;Jochen L. Cremer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konstantelos_I/0/1/0/all/0/1&quot;&gt;Ioannis Konstantelos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tindemans_S/0/1/0/all/0/1&quot;&gt;Simon H. Tindemans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strbac_G/0/1/0/all/0/1&quot;&gt;Goran Strbac&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02998">
<title>Anomaly Detection for Industrial Big Data. (arXiv:1804.02998v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.02998</link>
<description rdf:parseType="Literal">&lt;p&gt;As the Industrial Internet of Things (IIoT) grows, systems are increasingly
being monitored by arrays of sensors returning time-series data at
ever-increasing &apos;volume, velocity and variety&apos; (i.e. Industrial Big Data). An
obvious use for these data is real-time systems condition monitoring and
prognostic time to failure analysis (remaining useful life, RUL). (e.g. See
white papers by Senseye.io, and output of the NASA Prognostics Center of
Excellence (PCoE).) However, as noted by Agrawal and Choudhary &apos;Our ability to
collect &quot;big data&quot; has greatly surpassed our capability to analyze it,
underscoring the emergence of the fourth paradigm of science, which is
data-driven discovery.&apos; In order to fully utilize the potential of Industrial
Big Data we need data-driven techniques that operate at scales that process
models cannot. Here we present a prototype technique for data-driven anomaly
detection to operate at industrial scale. The method generalizes to application
with almost any multivariate dataset based on independent ordinations of
repeated (bootstrapped) partitions of the dataset and inspection of the joint
distribution of ordinal distances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caithness_N/0/1/0/all/0/1&quot;&gt;Neil Caithness&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallom_D/0/1/0/all/0/1&quot;&gt;David Wallom&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1607.03200">
<title>Qualitative Judgement of Research Impact: Domain Taxonomy as a Fundamental Framework for Judgement of the Quality of Research. (arXiv:1607.03200v2 [cs.DL] UPDATED)</title>
<link>http://arxiv.org/abs/1607.03200</link>
<description rdf:parseType="Literal">&lt;p&gt;The appeal of metric evaluation of research impact has attracted considerable
interest in recent times. Although the public at large and administrative
bodies are much interested in the idea, scientists and other researchers are
much more cautious, insisting that metrics are but an auxiliary instrument to
the qualitative peer-based judgement. The goal of this article is to propose
availing of such a well positioned construct as domain taxonomy as a tool for
directly assessing the scope and quality of research. We first show how
taxonomies can be used to analyse the scope and perspectives of a set of
research projects or papers. Then we proceed to define a research team or
researcher&apos;s rank by those nodes in the hierarchy that have been created or
significantly transformed by the results of the researcher. An experimental
test of the approach in the data analysis domain is described. Although the
concept of taxonomy seems rather simplistic to describe all the richness of a
research domain, its changes and use can be made transparent and subject to
open discussions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murtagh_F/0/1/0/all/0/1&quot;&gt;Fionn Murtagh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orlov_M/0/1/0/all/0/1&quot;&gt;Michael Orlov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirkin_B/0/1/0/all/0/1&quot;&gt;Boris Mirkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.05575">
<title>A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics. (arXiv:1702.05575v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1702.05575</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the Stochastic Gradient Langevin Dynamics (SGLD) algorithm for
non-convex optimization. The algorithm performs stochastic gradient descent,
where in each step it injects appropriately scaled Gaussian noise to the
update. We analyze the algorithm&apos;s hitting time to an arbitrary subset of the
parameter space. Two results follow from our general theory: First, we prove
that for empirical risk minimization, if the empirical risk is point-wise close
to the (smooth) population risk, then the algorithm achieves an approximate
local minimum of the population risk in polynomial time, escaping suboptimal
local minima that only exist in the empirical risk. Second, we show that SGLD
improves on one of the best known learnability results for learning linear
classifiers under the zero-one loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charikar_M/0/1/0/all/0/1&quot;&gt;Moses Charikar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.09283">
<title>GXNOR-Net: Training deep neural networks with ternary weights and activations without full-precision memory under a unified discretization framework. (arXiv:1705.09283v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1705.09283</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a pressing need to build an architecture that could subsume these
networks under a unified framework that achieves both higher performance and
less overhead. To this end, two fundamental issues are yet to be addressed. The
first one is how to implement the back propagation when neuronal activations
are discrete. The second one is how to remove the full-precision hidden weights
in the training phase to break the bottlenecks of memory/computation
consumption. To address the first issue, we present a multi-step neuronal
activation discretization method and a derivative approximation technique that
enable the implementing the back propagation algorithm on discrete DNNs. While
for the second issue, we propose a discrete state transition (DST) methodology
to constrain the weights in a discrete space without saving the hidden weights.
Through this way, we build a unified framework that subsumes the binary or
ternary networks as its special cases, and under which a heuristic algorithm is
provided at the website https://github.com/AcrossV/Gated-XNOR. More
particularly, we find that when both the weights and activations become ternary
values, the DNNs can be reduced to sparse binary networks, termed as gated XNOR
networks (GXNOR-Nets) since only the event of non-zero weight and non-zero
activation enables the control gate to start the XNOR logic operations in the
original binary networks. This promises the event-driven hardware design for
efficient mobile intelligence. We achieve advanced performance compared with
state-of-the-art algorithms. Furthermore, the computational sparsity and the
number of states in the discrete space can be flexibly modified to make it
suitable for various hardware platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1&quot;&gt;Lei Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_P/0/1/0/all/0/1&quot;&gt;Peng Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1&quot;&gt;Jing Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhenzhi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoqi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.10225">
<title>Bayesian stochastic blockmodeling. (arXiv:1705.10225v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.10225</link>
<description rdf:parseType="Literal">&lt;p&gt;This chapter provides a self-contained introduction to the use of Bayesian
inference to extract large-scale modular structures from network data, based on
the stochastic block model (SBM), as well as its degree-corrected and
overlapping generalizations. We focus on nonparametric formulations that allow
their inference in a manner that prevents overfitting, and enables model
selection. We discuss aspects of the choice of priors, in particular how to
avoid underfitting via increased Bayesian hierarchies, and we contrast the task
of sampling network partitions from the posterior distribution with finding the
single point estimate that maximizes it, while describing efficient algorithms
to perform either one. We also show how inferring the SBM can be used to
predict missing and spurious links, and shed light on the fundamental
limitations of the detectability of modular structures in networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peixoto_T/0/1/0/all/0/1&quot;&gt;Tiago P. Peixoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.03078">
<title>Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations. (arXiv:1706.03078v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.03078</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of deep convolutional architectures is often attributed in part
to their ability to learn multiscale and invariant representations of natural
signals. However, a precise study of these properties and how they affect
learning guarantees is still missing. In this paper, we consider deep
convolutional representations of signals; we study their invariance to
translations and to more general groups of transformations, their stability to
the action of diffeomorphisms, and their ability to preserve signal
information. This analysis is carried by introducing a multilayer kernel based
on convolutional kernel networks and by studying the geometry induced by the
kernel mapping. We then characterize the corresponding reproducing kernel
Hilbert space (RKHS), showing that it contains a large class of convolutional
neural networks with homogeneous activation functions. This analysis allows us
to separate data representation from learning, and to provide a canonical
measure of model complexity, the RKHS norm, which controls both stability and
generalization of any learned model. In addition to models in the constructed
RKHS, our stability analysis also applies to convolutional networks with
generic activations such as rectified linear units, and we discuss its
relationship with recent generalization bounds based on spectral norms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1&quot;&gt;Alberto Bietti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mairal_J/0/1/0/all/0/1&quot;&gt;Julien Mairal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.02840">
<title>A Brief Introduction to Machine Learning for Engineers. (arXiv:1709.02840v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.02840</link>
<description rdf:parseType="Literal">&lt;p&gt;This monograph aims at providing an introduction to key concepts, algorithms,
and theoretical results in machine learning. The treatment concentrates on
probabilistic models for supervised and unsupervised learning problems. It
introduces fundamental concepts and algorithms by building on first principles,
while also exposing the reader to more advanced topics with extensive pointers
to the literature, within a unified notation and mathematical framework. The
material is organized according to clearly defined categories, such as
discriminative and generative models, frequentist and Bayesian approaches,
exact and approximate inference, as well as directed and undirected models.
This monograph is meant as an entry point for researchers with a background in
probability and linear algebra.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1&quot;&gt;Osvaldo Simeone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.08310">
<title>AutoEncoder Inspired Unsupervised Feature Selection. (arXiv:1710.08310v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.08310</link>
<description rdf:parseType="Literal">&lt;p&gt;High-dimensional data in many areas such as computer vision and machine
learning tasks brings in computational and analytical difficulty. Feature
selection which selects a subset from observed features is a widely used
approach for improving performance and effectiveness of machine learning models
with high-dimensional data. In this paper, we propose a novel AutoEncoder
Feature Selector (AEFS) for unsupervised feature selection which combines
autoencoder regression and group lasso tasks. Compared to traditional feature
selection methods, AEFS can select the most important features by excavating
both linear and nonlinear information among features, which is more flexible
than the conventional self-representation method for unsupervised feature
selection with only linear assumptions. Experimental results on benchmark
dataset show that the proposed method is superior to the state-of-the-art
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kai Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10769">
<title>Communication-Avoiding Optimization Methods for Distributed Massive-Scale Sparse Inverse Covariance Estimation. (arXiv:1710.10769v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10769</link>
<description rdf:parseType="Literal">&lt;p&gt;Across a variety of scientific disciplines, sparse inverse covariance
estimation is a popular tool for capturing the underlying dependency
relationships in multivariate data. Unfortunately, most estimators are not
scalable enough to handle the sizes of modern high-dimensional data sets (often
on the order of terabytes), and assume Gaussian samples. To address these
deficiencies, we introduce HP-CONCORD, a highly scalable optimization method
for estimating a sparse inverse covariance matrix based on a regularized
pseudolikelihood framework, without assuming Gaussianity. Our parallel proximal
gradient method uses a novel communication-avoiding linear algebra algorithm
and runs across a multi-node cluster with up to 1k nodes (24k cores), achieving
parallel scalability on problems with up to ~819 billion parameters (1.28
million dimensions); even on a single node, HP-CONCORD demonstrates
scalability, outperforming a state-of-the-art method. We also use HP-CONCORD to
estimate the underlying dependency structure of the brain from fMRI data, and
use the result to identify functional regions automatically. The results show
good agreement with a clustering from the neuroscience literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Koanantakool_P/0/1/0/all/0/1&quot;&gt;Penporn Koanantakool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ali_A/0/1/0/all/0/1&quot;&gt;Alnur Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Azad_A/0/1/0/all/0/1&quot;&gt;Ariful Azad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Buluc_A/0/1/0/all/0/1&quot;&gt;Aydin Buluc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Morozov_D/0/1/0/all/0/1&quot;&gt;Dmitriy Morozov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oliker_L/0/1/0/all/0/1&quot;&gt;Leonid Oliker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yelick_K/0/1/0/all/0/1&quot;&gt;Katherine Yelick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Sang-Yun Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.02305">
<title>Sketching Linear Classifiers over Data Streams. (arXiv:1711.02305v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.02305</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new sub-linear space sketch---the Weight-Median Sketch---for
learning compressed linear classifiers over data streams while supporting the
efficient recovery of large-magnitude weights in the model. This enables
memory-limited execution of several statistical analyses over streams,
including online feature selection, streaming data explanation, relative
deltoid detection, and streaming estimation of pointwise mutual information.
Unlike related sketches that capture the most frequently-occurring features (or
items) in a data stream, the Weight-Median Sketch captures the features that
are most discriminative of one stream (or class) compared to another. The
Weight-Median Sketch adopts the core data structure used in the Count-Sketch,
but, instead of sketching counts, it captures sketched gradient updates to the
model parameters. We provide a theoretical analysis that establishes recovery
guarantees for batch and online learning, and demonstrate empirical
improvements in memory-accuracy trade-offs over alternative memory-budgeted
methods, including count-based sketches and feature hashing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_K/0/1/0/all/0/1&quot;&gt;Kai Sheng Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharan_V/0/1/0/all/0/1&quot;&gt;Vatsal Sharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailis_P/0/1/0/all/0/1&quot;&gt;Peter Bailis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1&quot;&gt;Gregory Valiant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.05363">
<title>Kernel Conditional Exponential Family. (arXiv:1711.05363v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.05363</link>
<description rdf:parseType="Literal">&lt;p&gt;A nonparametric family of conditional distributions is introduced, which
generalizes conditional exponential families using functional parameters in a
suitable RKHS. An algorithm is provided for learning the generalized natural
parameter, and consistency of the estimator is established in the well
specified case. In experiments, the new method generally outperforms a
competing approach with consistency guarantees, and is competitive with a deep
conditional density model on datasets that exhibit abrupt transitions and
heteroscedasticity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Arbel_M/0/1/0/all/0/1&quot;&gt;Michael Arbel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1&quot;&gt;Arthur Gretton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10284">
<title>Between-class Learning for Image Classification. (arXiv:1711.10284v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10284</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel learning method for image classification
called Between-Class learning (BC learning). We generate between-class images
by mixing two images belonging to different classes with a random ratio. We
then input the mixed image to the model and train the model to output the
mixing ratio. BC learning has the ability to impose constraints on the shape of
the feature distributions, and thus the generalization ability is improved. BC
learning is originally a method developed for sounds, which can be digitally
mixed. Mixing two image data does not appear to make sense; however, we argue
that because convolutional neural networks have an aspect of treating input
data as waveforms, what works on sounds must also work on images. First, we
propose a simple mixing method using internal divisions, which surprisingly
proves to significantly improve performance. Second, we propose a mixing method
that treats the images as waveforms, which leads to a further improvement in
performance. As a result, we achieved 19.4% and 2.26% top-1 errors on
ImageNet-1K and CIFAR-10, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tokozume_Y/0/1/0/all/0/1&quot;&gt;Yuji Tokozume&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1&quot;&gt;Yoshitaka Ushiku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.03834">
<title>Crime prediction through urban metrics and statistical learning. (arXiv:1712.03834v2 [physics.soc-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1712.03834</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the causes of crime is a longstanding issue in researcher&apos;s
agenda. While it is a hard task to extract causality from data, several linear
models have been proposed to predict crime through the existing correlations
between crime and urban metrics. However, because of non-Gaussian distributions
and multicollinearity in urban indicators, it is common to find controversial
conclusions about the influence of some urban indicators on crime. Machine
learning ensemble-based algorithms can handle well such problems. Here, we use
a random forest regressor to predict crime and quantify the influence of urban
indicators on homicides. Our approach can have up to 97% of accuracy on crime
prediction, and the importance of urban indicators is ranked and clustered in
groups of equal influence, which are robust under slightly changes in the data
sample analyzed. Our results determine the rank of importance of urban
indicators to predict crime, unveiling that unemployment and illiteracy are the
most important variables for describing homicides in Brazilian cities. We
further believe that our approach helps in producing more robust conclusions
regarding the effects of urban indicators on crime, having potential
applications for guiding public policies for crime control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Alves_L/0/1/0/all/0/1&quot;&gt;Luiz G A Alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ribeiro_H/0/1/0/all/0/1&quot;&gt;Haroldo V Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rodrigues_F/0/1/0/all/0/1&quot;&gt;Francisco A Rodrigues&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.10110">
<title>Beyond Keywords and Relevance: A Personalized Ad Retrieval Framework in E-Commerce Sponsored Search. (arXiv:1712.10110v4 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/1712.10110</link>
<description rdf:parseType="Literal">&lt;p&gt;On most sponsored search platforms, advertisers bid on some keywords for
their advertisements (ads). Given a search request, ad retrieval module
rewrites the query into bidding keywords, and uses these keywords as keys to
select Top N ads through inverted indexes. In this way, an ad will not be
retrieved even if queries are related when the advertiser does not bid on
corresponding keywords. Moreover, most ad retrieval approaches regard rewriting
and ad-selecting as two separated tasks, and focus on boosting relevance
between search queries and ads. Recently, in e-commerce sponsored search more
and more personalized information has been introduced, such as user profiles,
long-time and real-time clicks. Personalized information makes ad retrieval
able to employ more elements (e.g. real-time clicks) as search signals and
retrieval keys, however it makes ad retrieval more difficult to measure ads
retrieved through different signals. To address these problems, we propose a
novel ad retrieval framework beyond keywords and relevance in e-commerce
sponsored search. Firstly, we employ historical ad click data to initialize a
hierarchical network representing signals, keys and ads, in which personalized
information is introduced. Then we train a model on top of the hierarchical
network by learning the weights of edges. Finally we select the best edges
according to the model, boosting RPM/CTR. Experimental results on our
e-commerce platform demonstrate that our ad retrieval framework achieves good
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Su Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianshu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1&quot;&gt;Daorui Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kaipeng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09319">
<title>Less is more: sampling chemical space with active learning. (arXiv:1801.09319v2 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/1801.09319</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of accurate and transferable machine learning (ML) potentials
for predicting molecular energetics is a challenging task. The process of data
generation to train such ML potentials is a task neither well understood nor
researched in detail. In this work, we present a fully automated approach for
the generation of datasets with the intent of training universal ML potentials.
It is based on the concept of active learning (AL) via Query by Committee
(QBC), which uses the disagreement between an ensemble of ML potentials to
infer the reliability of the ensemble&apos;s prediction. QBC allows the presented AL
algorithm to automatically sample regions of chemical space where the ML
potential fails to accurately predict the potential energy. AL improves the
overall fitness of ANAKIN-ME (ANI) deep learning potentials in rigorous test
cases by mitigating human biases in deciding what new training data to use. AL
also reduces the training set size to a fraction of the data required when
using naive random sampling techniques. To provide validation of our AL
approach we develop the COMP6 benchmark (publicly available on GitHub), which
contains a diverse set of organic molecules. Through the AL process, it is
shown that the AL-based potentials perform as well as the ANI-1 potential on
COMP6 with only 10% of the data, and vastly outperforms ANI-1 with 25% the
amount of data. Finally, we show that our proposed AL technique develops a
universal ANI potential (ANI-1x) that provides accurate energy and force
predictions on the entire COMP6 benchmark. This universal ML potential achieves
a level of accuracy on par with the best ML potentials for single molecule or
materials, while remaining applicable to the general class of organic molecules
comprised of the elements CHNO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Smith_J/0/1/0/all/0/1&quot;&gt;Justin S. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nebgen_B/0/1/0/all/0/1&quot;&gt;Ben Nebgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lubbers_N/0/1/0/all/0/1&quot;&gt;Nicholas Lubbers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Isayev_O/0/1/0/all/0/1&quot;&gt;Olexandr Isayev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Roitberg_A/0/1/0/all/0/1&quot;&gt;Adrian E. Roitberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09514">
<title>Best Arm Identification for Contaminated Bandits. (arXiv:1802.09514v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09514</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies active learning in the context of robust statistics.
Specifically, we propose the Contaminated Best Arm Identification variant of
the multi-armed bandit problem, in which every arm pull has probability
$\varepsilon$ of generating a sample from an arbitrary \emph{contamination}
distribution instead of the \emph{true} underlying distribution. The goal is to
identify the best (or approximately best) true distribution with high
probability, with a secondary goal of providing guarantees on the quality of
that arm&apos;s underlying distribution. It is simple to see that in this
contamination model there are no consistent estimators for statistics (e.g.
median) of the underlying distribution, and that even with infinite samples,
statistics can be estimated only up to some unavoidable bias. We present tight,
non-asymptotic sample complexity bounds for estimating the first two robust
moments (median and median absolute deviation) with high probability. We then
show how to use this algorithmically for our problem by adapting Best Arm
Identification algorithms from the classical multi-armed bandit literature. We
give matching upper and lower bounds (up to a small logarithmic factor) on
these algorithms&apos; sample complexities. These results suggest an inherent
robustness of classical Best Arm Identification algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Altschuler_J/0/1/0/all/0/1&quot;&gt;Jason Altschuler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Brunel_V/0/1/0/all/0/1&quot;&gt;Victor-Emmanuel Brunel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Malek_A/0/1/0/all/0/1&quot;&gt;Alan Malek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04383">
<title>Delayed Impact of Fair Machine Learning. (arXiv:1803.04383v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.04383</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness in machine learning has predominantly been studied in static
classification settings without concern for how decisions change the underlying
population over time. Conventional wisdom suggests that fairness criteria
promote the long-term well-being of those groups they aim to protect.
&lt;/p&gt;
&lt;p&gt;We study how static fairness criteria interact with temporal indicators of
well-being, such as long-term improvement, stagnation, and decline in a
variable of interest. We demonstrate that even in a one-step feedback model,
common fairness criteria in general do not promote improvement over time, and
may in fact cause harm in cases where an unconstrained objective would not.
&lt;/p&gt;
&lt;p&gt;We completely characterize the delayed impact of three standard criteria,
contrasting the regimes in which these exhibit qualitatively different
behavior. In addition, we find that a natural form of measurement error
broadens the regime in which fairness criteria perform favorably.
&lt;/p&gt;
&lt;p&gt;Our results highlight the importance of measurement and temporal modeling in
the evaluation of fairness criteria, suggesting a range of new challenges and
trade-offs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lydia T. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dean_S/0/1/0/all/0/1&quot;&gt;Sarah Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolf_E/0/1/0/all/0/1&quot;&gt;Esther Rolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1&quot;&gt;Max Simchowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1&quot;&gt;Moritz Hardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.10815">
<title>Supervising Feature Influence. (arXiv:1803.10815v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.10815</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal influence measures for machine learnt classifiers shed light on the
reasons behind classification, and aid in identifying influential input
features and revealing their biases. However, such analyses involve evaluating
the classifier using datapoints that may be atypical of its training
distribution. Standard methods for training classifiers that minimize empirical
risk do not constrain the behavior of the classifier on such datapoints. As a
result, training to minimize empirical risk does not distinguish among
classifiers that agree on predictions in the training distribution but have
wildly different causal influences. We term this problem covariate shift in
causal testing and formally characterize conditions under which it arises. As a
solution to this problem, we propose a novel active learning algorithm that
constrains the influence measures of the trained model. We prove that any two
predictors whose errors are close on both the original training distribution
and the distribution of atypical points are guaranteed to have causal
influences that are also close. Further, we empirically demonstrate with
synthetic labelers that our algorithm trains models that (i) have similar
causal influences as the labeler&apos;s model, and (ii) generalize better to
out-of-distribution points while (iii) retaining their accuracy on
in-distribution points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_S/0/1/0/all/0/1&quot;&gt;Shayak Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mardziel_P/0/1/0/all/0/1&quot;&gt;Piotr Mardziel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1&quot;&gt;Anupam Datta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1&quot;&gt;Matthew Fredrikson&lt;/a&gt;</dc:creator>
</item></rdf:RDF>