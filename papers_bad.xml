<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-04-25T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09331"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09558"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09238"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09400"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09465"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09473"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09521"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09635"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.07204"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.05825"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.04759"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.04318"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06439"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08473"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09217"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09278"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09314"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09348"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09415"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09461"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09530"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09554"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09597"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09618"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09619"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09629"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09699"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1607.02675"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.08683"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10856"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07873"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1804.09331">
<title>Where are we now? A large benchmark study of recent symbolic regression methods. (arXiv:1804.09331v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1804.09331</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we provide a broad benchmarking of recent genetic programming
approaches to symbolic regression in the context of state of the art machine
learning approaches. We use a set of nearly 100 regression benchmark problems
culled from open source repositories across the web. We conduct a rigorous
benchmarking of four recent symbolic regression approaches as well as nine
machine learning approaches from scikit-learn. The results suggest that
symbolic regression performs strongly compared to state-of-the-art gradient
boosting algorithms, although in terms of running times is among the slowest of
the available methodologies. We discuss the results in detail and point to
future research directions that may allow symbolic regression to gain wider
adoption in the machine learning community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orzechowski_P/0/1/0/all/0/1&quot;&gt;Patryk Orzechowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cava_W/0/1/0/all/0/1&quot;&gt;William La Cava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1&quot;&gt;Jason H. Moore&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09558">
<title>A Visual Distance for WordNet. (arXiv:1804.09558v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.09558</link>
<description rdf:parseType="Literal">&lt;p&gt;Measuring the distance between concepts is an important field of study of
Natural Language Processing, as it can be used to improve tasks related to the
interpretation of those same concepts. WordNet, which includes a wide variety
of concepts associated with words (i.e., synsets), is often used as a source
for computing those distances. In this paper, we explore a distance for WordNet
synsets based on visual features, instead of lexical ones. For this purpose, we
extract the graphic features generated within a deep convolutional neural
networks trained with ImageNet and use those features to generate a
representative of each synset. Based on those representatives, we define a
distance measure of synsets, which complements the traditional lexical
distances. Finally, we propose some experiments to evaluate its performance and
compare it with the current state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Arnal_R/0/1/0/all/0/1&quot;&gt;Raquel P&amp;#xe9;rez-Arnal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilalta_A/0/1/0/all/0/1&quot;&gt;Armand Vilalta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Gasulla_D/0/1/0/all/0/1&quot;&gt;Dario Garcia-Gasulla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cortes_U/0/1/0/all/0/1&quot;&gt;Ulises Cort&amp;#xe9;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayguade_E/0/1/0/all/0/1&quot;&gt;Eduard Ayguad&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Labarta_J/0/1/0/all/0/1&quot;&gt;Jesus Labarta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09238">
<title>Semi-Supervised Learning with Declaratively Specified Entropy Constraints. (arXiv:1804.09238v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.09238</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a technique for declaratively specifying strategies for
semi-supervised learning (SSL). The proposed method can be used to specify
ensembles of semi-supervised learning, as well as agreement constraints and
entropic regularization constraints between these learners, and can be used to
model both well-known heuristics such as co-training and novel domain-specific
heuristics. In addition to representing individual SSL heuristics, we show that
multiple heuristics can also be automatically combined using Bayesian
optimization methods. We show consistent improvements on a suite of
well-studied SSL benchmarks, including a new state-of-the-art result on a
difficult relation extraction task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haitian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1&quot;&gt;William W. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1&quot;&gt;Lidong Bing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09400">
<title>3D Consistent &amp; Robust Segmentation of Cardiac Images by Deep Learning with Spatial Propagation. (arXiv:1804.09400v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1804.09400</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method based on deep learning to perform cardiac segmentation on
short axis MRI image stacks iteratively from the top slice (around the base) to
the bottom slice (around the apex). At each iteration, a novel variant of U-net
is applied to propagate the segmentation of a slice to the adjacent slice below
it. In other words, the prediction of a segmentation of a slice is dependent
upon the already existing segmentation of an adjacent slice. 3D-consistency is
hence explicitly enforced. The method is trained on a large database of 3078
cases from UK Biobank. It is then tested on 756 different cases from UK Biobank
and three other state-of-the-art cohorts (ACDC with 100 cases, Sunnybrook with
30 cases, RVSC with 16 cases). Results comparable or even better than the
state-of-the-art in terms of distance measures are achieved. They also
emphasize the assets of our method, namely enhanced spatial consistency
(currently neither considered nor achieved by the state-of-the-art), and the
generalization ability to unseen cases even from other databases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qiao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delingette_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; Delingette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duchateau_N/0/1/0/all/0/1&quot;&gt;Nicolas Duchateau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayache_N/0/1/0/all/0/1&quot;&gt;Nicholas Ayache&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09465">
<title>Intelligent Physiotherapy Through Procedural Content Generation. (arXiv:1804.09465v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.09465</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes an avenue for artificial and computational intelligence
techniques applied within games research to be deployed for purposes of
physical therapy. We provide an overview of prototypical research focussed on
the application of motion sensor input devices and virtual reality equipment
for rehabilitation of motor impairment an issue typical of patient&apos;s of
traumatic brain injuries. We highlight how advances in procedural content
generation and player modelling can stimulate development in this area by
improving quality of rehabilitation programmes and measuring patient
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esfahlani_S/0/1/0/all/0/1&quot;&gt;Shabnam Sadeghi Esfahlani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thompson_T/0/1/0/all/0/1&quot;&gt;Tommy Thompson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09473">
<title>Stratified Negation in Limit Datalog Programs. (arXiv:1804.09473v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1804.09473</link>
<description rdf:parseType="Literal">&lt;p&gt;There has recently been an increasing interest in declarative data analysis,
where analytic tasks are specified using a logical language, and their
implementation and optimisation are delegated to a general-purpose query
engine. Existing declarative languages for data analysis can be formalised as
variants of logic programming equipped with arithmetic function symbols and/or
aggregation, and are typically undecidable. In prior work, the language of
$\mathit{limit\ programs}$ was proposed, which is sufficiently powerful to
capture many analysis tasks and has decidable entailment problem. Rules in this
language, however, do not allow for negation. In this paper, we study an
extension of limit programs with stratified negation-as-failure. We show that
the additional expressive power makes reasoning computationally more demanding,
and provide tight data complexity bounds. We also identify a fragment with
tractable data complexity and sufficient expressivity to capture many relevant
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaminski_M/0/1/0/all/0/1&quot;&gt;Mark Kaminski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grau_B/0/1/0/all/0/1&quot;&gt;Bernardo Cuenca Grau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kostylev_E/0/1/0/all/0/1&quot;&gt;Egor V. Kostylev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motik_B/0/1/0/all/0/1&quot;&gt;Boris Motik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1&quot;&gt;Ian Horrocks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09521">
<title>Fair Division Under Cardinality Constraints. (arXiv:1804.09521v1 [cs.GT])</title>
<link>http://arxiv.org/abs/1804.09521</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of fairly allocating indivisible goods, among agents,
under cardinality constraints and additive valuations. In this setting, we are
given a partition of the entire set of goods---i.e., the goods are
categorized---and a limit is specified on the number of goods that can be
allocated from each category to any agent. The objective here is to find a fair
allocation in which the subset of goods assigned to any agent satisfies the
given cardinality constraints. This problem naturally captures a number of
resource-allocation applications, and is a generalization of the well-studied
(unconstrained) fair division problem.
&lt;/p&gt;
&lt;p&gt;The two central notions of fairness, in the context of fair division of
indivisible goods, are envy freeness up to one good (EF1) and the (approximate)
maximin share guarantee (MMS). We show that the existence and algorithmic
guarantees established for these solution concepts in the unconstrained setting
can essentially be achieved under cardinality constraints. Specifically, we
develop efficient algorithms which compute EF1 and approximately MMS
allocations in the constrained setting.
&lt;/p&gt;
&lt;p&gt;Furthermore, focusing on the case wherein all the agents have the same
additive valuation, we establish that EF1 locations exist even under matroid
constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barman_S/0/1/0/all/0/1&quot;&gt;Siddharth Barman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1&quot;&gt;Arpita Biswas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09635">
<title>A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. (arXiv:1804.09635v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.09635</link>
<description rdf:parseType="Literal">&lt;p&gt;Peer reviewing is a central component in the scientific publishing process.
We present the first public dataset of scientific peer reviews available for
research purposes (PeerRead v1) providing an opportunity to study this
important artifact. The dataset consists of 14.7K paper drafts and the
corresponding accept/reject decisions in top-tier venues including ACL, NIPS
and ICLR. The dataset also includes 10.7K textual peer reviews written by
experts for a subset of the papers. We describe the data collection process and
report interesting observed phenomena in the peer reviews. We also propose two
novel NLP tasks based on this dataset and provide simple baseline models. In
the first task, we show that simple models can predict whether a paper is
accepted with up to 21% error reduction compared to the majority baseline. In
the second task, we predict the numerical scores of review aspects and show
that simple models can outperform the mean baseline for aspects with high
variance such as &apos;originality&apos; and &apos;impact&apos;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Dongyeop Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ammar_W/0/1/0/all/0/1&quot;&gt;Waleed Ammar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalvi_B/0/1/0/all/0/1&quot;&gt;Bhavana Dalvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuylen_M/0/1/0/all/0/1&quot;&gt;Madeleine van Zuylen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohlmeier_S/0/1/0/all/0/1&quot;&gt;Sebastian Kohlmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1&quot;&gt;Eduard Hovy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1&quot;&gt;Roy Schwartz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.07204">
<title>Fast Exact k-Means, k-Medians and Bregman Divergence Clustering in 1D. (arXiv:1701.07204v4 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1701.07204</link>
<description rdf:parseType="Literal">&lt;p&gt;The $k$-Means clustering problem on $n$ points is NP-Hard for any dimension
$d\ge 2$, however, for the 1D case there exists exact polynomial time
algorithms. Previous literature reported an $O(kn^2)$ time dynamic programming
algorithm that uses $O(kn)$ space. It turns out that the problem has been
considered under a different name more than twenty years ago. We present all
the existing work that had been overlooked and compare the various solutions
theoretically. Moreover, we show how to reduce the space usage for some of
them, as well as generalize them to data structures that can quickly report an
optimal $k$-Means clustering for any $k$. Finally we also generalize all the
algorithms to work for the absolute distance and to work for any Bregman
Divergence. We complement our theoretical contributions by experiments that
compare the practical performance of the various algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gronlund_A/0/1/0/all/0/1&quot;&gt;Allan Gr&amp;#xf8;nlund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1&quot;&gt;Kasper Green Larsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathiasen_A/0/1/0/all/0/1&quot;&gt;Alexander Mathiasen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nielsen_J/0/1/0/all/0/1&quot;&gt;Jesper Sindahl Nielsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1&quot;&gt;Stefan Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Mingzhou Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.05825">
<title>Relational Marginal Problems: Theory and Estimation. (arXiv:1709.05825v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1709.05825</link>
<description rdf:parseType="Literal">&lt;p&gt;In the propositional setting, the marginal problem is to find a
(maximum-entropy) distribution that has some given marginals. We study this
problem in a relational setting and make the following contributions. First, we
compare two different notions of relational marginals. Second, we show a
duality between the resulting relational marginal problems and the maximum
likelihood estimation of the parameters of relational models, which generalizes
a well-known duality from the propositional setting. Third, by exploiting the
relational marginal formulation, we present a statistically sound method to
learn the parameters of relational models that will be applied in settings
where the number of constants differs between the training and test data.
Furthermore, based on a relational generalization of marginal polytopes, we
characterize cases where the standard estimators based on feature&apos;s number of
true groundings needs to be adjusted and we quantitatively characterize the
consequences of these adjustments. Fourth, we prove bounds on expected errors
of the estimated parameters, which allows us to lower-bound, among other
things, the effective sample size of relational training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuzelka_O/0/1/0/all/0/1&quot;&gt;Ondrej Kuzelka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1&quot;&gt;Jesse Davis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1&quot;&gt;Steven Schockaert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.04759">
<title>Bayesian Hypernetworks. (arXiv:1710.04759v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.04759</link>
<description rdf:parseType="Literal">&lt;p&gt;We study Bayesian hypernetworks: a framework for approximate Bayesian
inference in neural networks. A Bayesian hypernetwork $\h$ is a neural network
which learns to transform a simple noise distribution, $p(\vec\epsilon) =
\N(\vec 0,\mat I)$, to a distribution $q(\pp) := q(h(\vec\epsilon))$ over the
parameters $\pp$ of another neural network (the &quot;primary network&quot;)\@. We train
$q$ with variational inference, using an invertible $\h$ to enable efficient
estimation of the variational lower bound on the posterior $p(\pp | \D)$ via
sampling. In contrast to most methods for Bayesian deep learning, Bayesian
hypernets can represent a complex multimodal approximate posterior with
correlations between parameters, while enabling cheap iid sampling of~$q(\pp)$.
In practice, Bayesian hypernets can provide a better defense against
adversarial examples than dropout, and also exhibit competitive performance on
a suite of tasks which evaluate model uncertainty, including regularization,
active learning, and anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krueger_D/0/1/0/all/0/1&quot;&gt;David Krueger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chin-Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Islam_R/0/1/0/all/0/1&quot;&gt;Riashat Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Ryan Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lacoste_A/0/1/0/all/0/1&quot;&gt;Alexandre Lacoste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.04318">
<title>Cross-Modal Retrieval with Implicit Concept Association. (arXiv:1804.04318v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.04318</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional cross-modal retrieval assumes explicit association of concepts
across modalities, where there is no ambiguity in how the concepts are linked
to each other, e.g., when we do the image search with a query &quot;dogs&quot;, we expect
to see dog images. In this paper, we consider a different setting for
cross-modal retrieval where data from different modalities are implicitly
linked via concepts that must be inferred by high-level reasoning; we call this
setting implicit concept association. To foster future research in this
setting, we present a new dataset containing 47K pairs of animated GIFs and
sentences crawled from the web, in which the GIFs depict physical or emotional
reactions to the scenarios described in the text (called &quot;reaction GIFs&quot;). We
report on a user study showing that, despite the presence of implicit concept
association, humans are able to identify video-sentence pairs with matching
concepts, suggesting the feasibility of our task. Furthermore, we propose a
novel visual-semantic embedding network based on multiple instance learning.
Unlike traditional approaches, we compute multiple embeddings from each
modality, each representing different concepts, and measure their similarity by
considering all possible combinations of visual-semantic embeddings in the
framework of multiple instance learning. We evaluate our approach on two
video-sentence datasets with explicit and implicit concept association and
report competitive results compared to existing approaches on cross-modal
retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yale Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soleymani_M/0/1/0/all/0/1&quot;&gt;Mohammad Soleymani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06439">
<title>Personalized neural language models for real-world query auto completion. (arXiv:1804.06439v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06439</link>
<description rdf:parseType="Literal">&lt;p&gt;Query auto completion (QAC) systems are a standard part of search engines in
industry, helping users formulate their query. Such systems update their
suggestions after the user types each character, predicting the user&apos;s intent
using various signals - one of the most common being popularity. Recently, deep
learning approaches have been proposed for the QAC task, to specifically
address the main limitation of previous popularity-based methods: the inability
to predict unseen queries. In this work we improve previous methods based on
neural language modeling, with the goal of building an end-to-end system. We
particularly focus on using real-world data by integrating user information for
personalized suggestions when possible. We also make use of time information
and study how to increase diversity in the suggestions while studying the
impact on scalability. Our empirical results demonstrate a marked improvement
on two separate datasets over previous best methods in both accuracy and
scalability, making a step towards neural query auto-completion in production
search engines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorini_N/0/1/0/all/0/1&quot;&gt;Nicolas Fiorini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08473">
<title>Beyond Narrative Description: Generating Poetry from Images by Multi-Adversarial Training. (arXiv:1804.08473v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08473</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic generation of natural language from images has attracted extensive
attention. In this paper, we take one step further to investigate generation of
poetic language (with multiple lines) to an image for automatic poetry
creation. This task involves multiple challenges, including discovering poetic
clues from the image (e.g., hope from green), and generating poems to satisfy
both relevance to the image and poeticness in language level. To solve the
above challenges, we formulate the task of poem generation into two correlated
sub-tasks by multi-adversarial training via policy gradient, through which the
cross-modal relevance and poetic language style can be ensured. To extract
poetic clues from images, we propose to learn a deep coupled visual-poetic
embedding, in which the poetic representation from objects, sentiments and
scenes in an image can be jointly learned. Two discriminative networks are
further introduced to guide the poem generation, including a multi-modal
discriminator and a poem-style discriminator. To facilitate the research, we
have collected two poem datasets by human annotators with two distinct
properties: 1) the first human annotated image-to-poem pair dataset (with 8,292
pairs in total), and 2) to-date the largest public English poem corpus dataset
(with 92,265 different poems in total). Extensive experiments are conducted
with 8K images generated with our model, among which 1.5K image are randomly
picked for evaluation. Both objective and subjective evaluations show the
superior performances against the state-of-art methods for poem generation from
images. Turing test carried out with over 500 human subjects, among which 30
evaluators are poetry experts, demonstrates the effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jianlong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1&quot;&gt;Makoto P. Kato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1&quot;&gt;Masatoshi Yoshikawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09217">
<title>On Learning Sparsely Used Dictionaries from Incomplete Samples. (arXiv:1804.09217v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.09217</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing algorithms for dictionary learning assume that all entries of
the (high-dimensional) input data are fully observed. However, in several
practical applications (such as hyper-spectral imaging or blood glucose
monitoring), only an incomplete fraction of the data entries may be available.
For incomplete settings, no provably correct and polynomial-time algorithm has
been reported in the dictionary learning literature. In this paper, we provide
provable approaches for learning - from incomplete samples - a family of
dictionaries whose atoms have sufficiently &quot;spread-out&quot; mass. First, we propose
a descent-style iterative algorithm that linearly converges to the true
dictionary when provided a sufficiently coarse initial estimate. Second, we
propose an initialization algorithm that utilizes a small number of extra fully
observed samples to produce such a coarse initial estimate. Finally, we
theoretically analyze their performance and provide asymptotic statistical and
computational guarantees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thanh V. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soni_A/0/1/0/all/0/1&quot;&gt;Akshay Soni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hegde_C/0/1/0/all/0/1&quot;&gt;Chinmay Hegde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09278">
<title>Opening the black box of neural nets: case studies in stop/top discrimination. (arXiv:1804.09278v1 [hep-ph])</title>
<link>http://arxiv.org/abs/1804.09278</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce techniques for exploring the functionality of a neural network
and extracting simple, human-readable approximations to its performance. By
performing gradient ascent on the input space of the network, we are able to
produce large populations of artificial events which strongly excite a given
classifier. By studying the populations of these events, we then directly
produce what are essentially contour maps of the network&apos;s classification
function. Combined with a suite of tools for identifying the input dimensions
deemed most important by the network, we can utilize these maps to efficiently
interpret the dominant criteria by which the network makes its classification.
&lt;/p&gt;
&lt;p&gt;As a test case, we study networks trained to discriminate supersymmetric stop
production in the dilepton channel from Standard Model backgrounds. In the case
of a heavy stop decaying to a light neutralino, we find individual neurons with
large mutual information with $m_{T2}^{\ell\ell}$, a human-designed variable
for optimizing the analysis. The network selects events with significant
missing $p_T$ oriented azimuthally away from both leptons, efficiently
rejecting $t\overline{t}$ background. In the case of a light stop with
three-body decays to $Wb{\widetilde \chi}$ and little phase space, we find
neurons that smoothly interpolate between a similar top-rejection strategy and
an ISR-tagging strategy allowing for more missing momentum. We also find that a
neural network trained on a stealth stop parameter point learns novel angular
correlations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Roxlo_T/0/1/0/all/0/1&quot;&gt;Thomas Roxlo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Reece_M/0/1/0/all/0/1&quot;&gt;Matthew Reece&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09314">
<title>Deep Learning for Predicting Asset Returns. (arXiv:1804.09314v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.09314</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning searches for nonlinear factors for predicting asset returns.
Predictability is achieved via multiple layers of composite factors as opposed
to additive ones. Viewed in this way, asset pricing studies can be revisited
using multi-layer deep learners, such as rectified linear units (ReLU) or
long-short-term-memory (LSTM) for time-series effects. State-of-the-art
algorithms including stochastic gradient descent (SGD), TensorFlow and dropout
design provide imple- mentation and efficient factor exploration. To illustrate
our methodology, we revisit the equity market risk premium dataset of Welch and
Goyal (2008). We find the existence of nonlinear factors which explain
predictability of returns, in particular at the extremes of the characteristic
space. Finally, we conclude with directions for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_G/0/1/0/all/0/1&quot;&gt;Guanhao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jingyu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Polson_N/0/1/0/all/0/1&quot;&gt;Nicholas G. Polson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09348">
<title>Generalized Gaussian Kernel Adaptive Filtering. (arXiv:1804.09348v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.09348</link>
<description rdf:parseType="Literal">&lt;p&gt;The present paper proposes generalized Gaussian kernel adaptive filtering,
where the kernel parameters are adaptive and data-driven. The Gaussian kernel
is parametrized by a center vector and a symmetric positive definite (SPD)
precision matrix, which is regarded as a generalization of the scalar width
parameter. These parameters are adaptively updated on the basis of a proposed
least-square-type rule to minimize the estimation error. The main contribution
of this paper is to establish update rules for precision matrices on the SPD
manifold in order to keep their symmetric positive-definiteness. Different from
conventional kernel adaptive filters, the proposed regressor is a superposition
of Gaussian kernels with all different parameters, which makes such regressor
more flexible. The kernel adaptive filtering algorithm is established together
with a l1-regularized least squares to avoid overfitting and the increase of
dimensionality of the dictionary. Experimental results confirm the validity of
the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wada_T/0/1/0/all/0/1&quot;&gt;Tomoya Wada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukumori_K/0/1/0/all/0/1&quot;&gt;Kosuke Fukumori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1&quot;&gt;Toshihisa Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiori_S/0/1/0/all/0/1&quot;&gt;Simone Fiori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09415">
<title>A Note on Kernel Methods for Multiscale Systems with Critical Transitions. (arXiv:1804.09415v1 [nlin.PS])</title>
<link>http://arxiv.org/abs/1804.09415</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the maximum mean discrepancy (MMD) in the context of critical
transitions modelled by fast-slow stochastic dynamical systems. We establish a
new link between the dynamical theory of critical transitions with the
statistical aspects of the MMD. In particular, we show that a formal
approximation of the MMD near fast subsystem bifurcation points can be computed
to leading-order. In particular, this leading order approximation shows that
the MMD depends intricately on the fast-slow systems parameters and one can
only expect to extract warning signs under rather stringent conditions.
However, the MMD turns out to be an excellent binary classifier to detect the
change point induced by the critical transition. We cross-validate our results
by numerical simulations for a van der Pol-type model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/nlin/1/au:+Hamzi_B/0/1/0/all/0/1&quot;&gt;Boumediene Hamzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nlin/1/au:+Kuehn_C/0/1/0/all/0/1&quot;&gt;Christian Kuehn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nlin/1/au:+Mohamed_S/0/1/0/all/0/1&quot;&gt;Sameh Mohamed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09461">
<title>Structured Deep Neural Network Pruning by Varying Regularization Parameters. (arXiv:1804.09461v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.09461</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNN&apos;s) are restricted by their massive
computation and high storage. Parameter pruning is a promising approach for CNN
compression and acceleration, which aims at eliminating redundant model
parameters with tolerable performance loss. Despite its effectiveness, existing
regularization-based parameter pruning methods usually assign a fixed
regularization parameter to all weights, which neglects the fact that different
weights may have different importance to CNN. To solve this problem, we propose
a theoretically sound regularization-based pruning method to incrementally
assign different regularization parameters to different weights based on their
importance to the network. On AlexNet and VGG-16, our method can achieve 4x
theoretical speedup with similar accuracies compared with the baselines. For
ResNet-50, the proposed method also achieves 2x acceleration and only suffers
0.1% top-5 accuracy loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuehai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Roland Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09530">
<title>Strong Baselines for Neural Semi-supervised Learning under Domain Shift. (arXiv:1804.09530v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1804.09530</link>
<description rdf:parseType="Literal">&lt;p&gt;Novel neural models have been proposed in recent years for learning under
domain shift. Most models, however, only evaluate on a single task, on
proprietary datasets, or compare to weak baselines, which makes comparison of
models difficult. In this paper, we re-evaluate classic general-purpose
bootstrapping approaches in the context of neural networks under domain shifts
vs. recent neural approaches and propose a novel multi-task tri-training method
that reduces the time and space complexity of classic tri-training. Extensive
experiments on two benchmarks are negative: while our novel method establishes
a new state-of-the-art for sentiment analysis, it does not fare consistently
the best. More importantly, we arrive at the somewhat surprising conclusion
that classic tri-training, with some additions, outperforms the state of the
art. We conclude that classic approaches constitute an important and strong
baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1&quot;&gt;Sebastian Ruder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1&quot;&gt;Barbara Plank&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09554">
<title>Stochastic Conditional Gradient Methods: From Convex Minimization to Submodular Maximization. (arXiv:1804.09554v1 [math.OC])</title>
<link>http://arxiv.org/abs/1804.09554</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers stochastic optimization problems for a large class of
objective functions, including convex and continuous submodular. Stochastic
proximal gradient methods have been widely used to solve such problems;
however, their applicability remains limited when the problem dimension is
large and the projection onto a convex set is costly. Instead, stochastic
conditional gradient methods are proposed as an alternative solution relying on
(i) Approximating gradients via a simple averaging technique requiring a single
stochastic gradient evaluation per iteration; (ii) Solving a linear program to
compute the descent/ascent direction. The averaging technique reduces the noise
of gradient approximations as time progresses, and replacing projection step in
proximal methods by a linear program lowers the computational complexity of
each iteration. We show that under convexity and smoothness assumptions, our
proposed method converges to the optimal objective function value at a
sublinear rate of $O(1/t^{1/3})$. Further, for a monotone and continuous
DR-submodular function and subject to a general convex body constraint, we
prove that our proposed method achieves a $((1-1/e)OPT-\eps)$ guarantee with
$O(1/\eps^3)$ stochastic gradient computations. This guarantee matches the
known hardness results and closes the gap between deterministic and stochastic
continuous submodular maximization. Additionally, we obtain $((1/e)OPT -\eps)$
guarantee after using $O(1/\eps^3)$ stochastic gradients for the case that the
objective function is continuous DR-submodular but non-monotone and the
constraint set is down-closed. By using stochastic continuous optimization as
an interface, we provide the first $(1-1/e)$ tight approximation guarantee for
maximizing a monotone but stochastic submodular set function subject to a
matroid constraint and $(1/e)$ approximation guarantee for the non-monotone
case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mokhtari_A/0/1/0/all/0/1&quot;&gt;Aryan Mokhtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hassani_H/0/1/0/all/0/1&quot;&gt;Hamed Hassani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Karbasi_A/0/1/0/all/0/1&quot;&gt;Amin Karbasi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09597">
<title>On The Complexity of Sparse Label Propagation. (arXiv:1804.09597v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.09597</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the computational complexity of sparse label
propagation which has been proposed recently for processing network structured
data. Sparse label propagation amounts to a convex optimization problem and
might be considered as an extension of basis pursuit from sparse vectors to
network structured datasets. Using a standard first-order oracle model, we
characterize the number of iterations for sparse label propagation to achieve a
prescribed accuracy. In particular, we derive an upper bound on the number of
iterations required to achieve a certain accuracy and show that this upper
bound is sharp for datasets having a chain structure (e.g., time series).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jung_A/0/1/0/all/0/1&quot;&gt;Alexander Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09618">
<title>t-DCF: a Detection Cost Function for the Tandem Assessment of Spoofing Countermeasures and Automatic Speaker Verification. (arXiv:1804.09618v1 [eess.AS])</title>
<link>http://arxiv.org/abs/1804.09618</link>
<description rdf:parseType="Literal">&lt;p&gt;The ASVspoof challenge series was born to spearhead research in anti-spoofing
for automatic speaker verification (ASV). The two challenge editions in 2015
and 2017 involved the assessment of spoofing countermeasures (CMs) in isolation
from ASV using an equal error rate (EER) metric. While a strategic approach to
assessment at the time, it has certain shortcomings. First, the CM EER is not
necessarily a reliable predictor of performance when ASV and CMs are combined.
Second, the EER operating point is ill-suited to user authentication
applications, e.g. telephone banking, characterised by a high target user prior
but a low spoofing attack prior. We aim to migrate from CM- to ASV-centric
assessment with the aid of a new tandem detection cost function (t-DCF) metric.
It extends the conventional DCF used in ASV research to scenarios involving
spoofing attacks. The t-DCF metric has 6 parameters: (i) false alarm and miss
costs for both systems, and (ii) prior probabilities of target and spoof trials
(with an implied third, nontarget prior). The study is intended to serve as a
self-contained, tutorial-like presentation. We analyse with the t-DCF a
selection of top-performing CM submissions to the 2015 and 2017 editions of
ASVspoof, with a focus on the spoofing attack prior. Whereas there is little to
choose between countermeasure systems for lower priors, system rankings derived
with the EER and t-DCF show differences for higher priors. We observe some
ranking changes. Findings support the adoption of the DCF-based metric into the
roadmap for future ASVspoof challenges, and possibly for other biometric
anti-spoofing evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1&quot;&gt;Tomi Kinnunen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kong Aik Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Delgado_H/0/1/0/all/0/1&quot;&gt;Hector Delgado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Evans_N/0/1/0/all/0/1&quot;&gt;Nicholas Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Todisco_M/0/1/0/all/0/1&quot;&gt;Massimiliano Todisco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sahidullah_M/0/1/0/all/0/1&quot;&gt;Md Sahidullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamagishi_J/0/1/0/all/0/1&quot;&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reynolds_D/0/1/0/all/0/1&quot;&gt;Douglas A. Reynolds&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09619">
<title>Identifying and Alleviating Concept Drift in Streaming Tensor Decomposition. (arXiv:1804.09619v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1804.09619</link>
<description rdf:parseType="Literal">&lt;p&gt;Tensor decompositions are used in various data mining applications from
social network to medical applications and are extremely useful in discovering
latent structures or concepts in the data. Many real-world applications are
dynamic in nature and so are their data. To deal with this dynamic nature of
data, there exist a variety of online tensor decomposition algorithms. A
central assumption in all those algorithms is that the number of latent
concepts remains fixed throughout the en- tire stream. However, this need not
be the case. Every incoming batch in the stream may have a different number of
latent concepts, and the difference in latent concepts from one tensor batch to
another can provide insights into how our findings in a particular application
behave and deviate over time. In this paper, we define &quot;concept&quot; and &quot;concept
drift&quot; in the context of streaming tensor decomposition, as the manifestation
of the variability of latent concepts throughout the stream. Furthermore, we
introduce SeekAndDestroy, an algorithm that detects concept drift in streaming
tensor decomposition and is able to produce results robust to that drift. To
the best of our knowledge, this is the first work that investigates concept
drift in streaming tensor decomposition. We extensively evaluate SeekAndDestroy
on synthetic datasets, which exhibit a wide variety of realistic drift. Our
experiments demonstrate the effectiveness of SeekAndDestroy, both in the
detection of concept drift and in the alleviation of its effects, producing
results with similar quality to decomposing the entire tensor in one shot.
Additionally, in real datasets, SeekAndDestroy outperforms other streaming
baselines, while discovering novel useful components.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasricha_R/0/1/0/all/0/1&quot;&gt;Ravdeep Pasricha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gujral_E/0/1/0/all/0/1&quot;&gt;Ekta Gujral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1&quot;&gt;Evangelos E. Papalexakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09629">
<title>Convergence guarantees for a class of non-convex and non-smooth optimization problems. (arXiv:1804.09629v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.09629</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of finding critical points of functions that are
non-convex and non-smooth. Studying a fairly broad class of such problems, we
analyze the behavior of three gradient-based methods (gradient descent,
proximal update, and Frank-Wolfe update). For each of these methods, we
establish rates of convergence for general problems, and also prove faster
rates for continuous sub-analytic functions. We also show that our algorithms
can escape strict saddle points for a class of non-smooth functions, thereby
generalizing known results for smooth functions. Our analysis leads to a
simplification of the popular CCCP algorithm, used for optimizing functions
that can be written as a difference of two convex functions. Our simplified
algorithm retains all the convergence properties of CCCP, along with a
significantly lower cost per iteration. We illustrate our methods and theory
via applications to the problems of best subset selection, robust estimation,
mixture density estimation, and shape-from-shading reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khamaru_K/0/1/0/all/0/1&quot;&gt;Koulik Khamaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wainwright_M/0/1/0/all/0/1&quot;&gt;Martin J. Wainwright&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09699">
<title>Towards Fast Computation of Certified Robustness for ReLU Networks. (arXiv:1804.09699v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1804.09699</link>
<description rdf:parseType="Literal">&lt;p&gt;Verifying the robustness property of a general Rectified Linear Unit (ReLU)
network is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer
CAV17]. Although finding the exact minimum adversarial distortion is hard,
giving a certified lower bound of the minimum distortion is possible. Current
available methods of computing such a bound are either time-consuming or
delivering low quality bounds that are too loose to be useful. In this paper,
we exploit the special structure of ReLU networks and provide two
computationally efficient algorithms (Fast-Lin and Fast-Lip) that are able to
certify non-trivial lower bounds of minimum distortions, by bounding the ReLU
units with appropriate linear functions (Fast-Lin), or by bounding the local
Lipschitz constant (Fast-Lip). Experiments show that (1) our proposed methods
deliver bounds close to (the gap is 2-3X) exact minimum distortion found by
Reluplex in small MNIST networks while our algorithms are more than 10,000
times faster; (2) our methods deliver similar quality of bounds (the gap is
within 35% and usually around 10%; sometimes our bounds are even better) for
larger networks compared to the methods based on solving linear programming
problems but our algorithms are 33-14,000 times faster; (3) our method is
capable of solving large MNIST and CIFAR networks up to 7 layers with more than
10,000 neurons within tens of seconds on a single CPU core.
&lt;/p&gt;
&lt;p&gt;In addition, we show that, in fact, there is no polynomial time algorithm
that can approximately find the minimum $\ell_1$ adversarial distortion of a
ReLU network with a $0.99\ln n$ approximation ratio unless
$\mathsf{NP}$=$\mathsf{P}$, where $n$ is the number of neurons in the network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weng_T/0/1/0/all/0/1&quot;&gt;Tsui-Wei Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongge Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boning_D/0/1/0/all/0/1&quot;&gt;Duane Boning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhillon_I/0/1/0/all/0/1&quot;&gt;Inderjit S. Dhillon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Daniel_L/0/1/0/all/0/1&quot;&gt;Luca Daniel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1607.02675">
<title>Covariate Regularized Community Detection in Sparse Graphs. (arXiv:1607.02675v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1607.02675</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate community detection in networks in the presence
of node covariates. In many instances, covariates and networks individually
only give a partial view of the cluster structure. One needs to jointly infer
the full cluster structure by considering both. In statistics, an emerging body
of work has been focused on combining information from both the edges in the
network and the node covariates to infer community memberships. However, so far
the theoretical guarantees have been established in the dense regime, where the
network can lead to perfect clustering under a broad parameter regime, and
hence the role of covariates is often not clear. In this paper, we examine
sparse networks in conjunction with finite dimensional sub-gaussian mixtures as
covariates under moderate separation conditions. In this setting each
individual source can only cluster a non-vanishing fraction of nodes correctly.
We propose a simple optimization framework which provably improves clustering
accuracy when the two sources carry partial information about the cluster
memberships, and hence perform poorly on their own. Our optimization problem
can be solved using scalable convex optimization algorithms. Using a variety of
simulated and real data examples, we show that the proposed method outperforms
other existing methodology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bowei Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sarkar_P/0/1/0/all/0/1&quot;&gt;Purnamrita Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.08683">
<title>Matrix Completion and Related Problems via Strong Duality. (arXiv:1704.08683v5 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1704.08683</link>
<description rdf:parseType="Literal">&lt;p&gt;This work studies the strong duality of non-convex matrix factorization
problems: we show that under certain dual conditions, these problems and its
dual have the same optimum. This has been well understood for convex
optimization, but little was known for non-convex problems. We propose a novel
analytical framework and show that under certain dual conditions, the optimal
solution of the matrix factorization program is the same as its bi-dual and
thus the global optimality of the non-convex program can be achieved by solving
its bi-dual which is convex. These dual conditions are satisfied by a wide
class of matrix factorization problems, although matrix factorization problems
are hard to solve in full generality. This analytical framework may be of
independent interest to non-convex optimization more broadly.
&lt;/p&gt;
&lt;p&gt;We apply our framework to two prototypical matrix factorization problems:
matrix completion and robust Principal Component Analysis (PCA). These are
examples of efficiently recovering a hidden matrix given limited reliable
observations of it. Our framework shows that exact recoverability and strong
duality hold with nearly-optimal sample complexity guarantees for matrix
completion and robust PCA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balcan_M/0/1/0/all/0/1&quot;&gt;Maria-Florina Balcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingyu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1&quot;&gt;David P. Woodruff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10856">
<title>Semi-Supervised and Active Few-Shot Learning with Prototypical Networks. (arXiv:1711.10856v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10856</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of semi-supervised few-shot classification where a
classifier needs to adapt to new tasks using a few labeled examples and
(potentially many) unlabeled examples. We propose a clustering approach to the
problem. The features extracted with Prototypical Networks are clustered using
$K$-means with the few labeled examples guiding the clustering process. We note
that in many real-world applications the adaptation performance can be
significantly improved by requesting the few labels through user feedback. We
demonstrate good performance of the active adaptation strategy using image
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boney_R/0/1/0/all/0/1&quot;&gt;Rinu Boney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1&quot;&gt;Alexander Ilin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07873">
<title>Gaussian variational approximation for high-dimensional state space models. (arXiv:1801.07873v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07873</link>
<description rdf:parseType="Literal">&lt;p&gt;This article considers variational approximations of the posterior
distribution in a high-dimensional state space model. The variational
approximation is a multivariate Gaussian density, in which the variational
parameters to be optimized are a mean vector and a covariance matrix. The
number of parameters in the covariance matrix grows as the square of the number
of model parameters, so it is necessary to find simple yet effective
parametrizations of the covariance structure when the number of model
parameters is large. The joint posterior distribution over the high-dimensional
state vectors is approximated using a dynamic factor model, with Markovian
dependence in time and a factor covariance structure for the states. This gives
a reduced dimension description of the dependence structure for the states, as
well as a temporal conditional independence structure similar to that in the
true posterior. We illustrate our approach in two high-dimensional applications
which are challenging for Markov chain Monte Carlo sampling. The first is a
spatio-temporal model for the spread of the Eurasian Collared-Dove across North
America. The second is a multivariate stochastic volatility model for financial
returns via a Wishart process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Quiroz_M/0/1/0/all/0/1&quot;&gt;Matias Quiroz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nott_D/0/1/0/all/0/1&quot;&gt;David J. Nott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kohn_R/0/1/0/all/0/1&quot;&gt;Robert Kohn&lt;/a&gt;</dc:creator>
</item></rdf:RDF>