<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-01T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11379"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02301"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11103"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11204"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11298"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11338"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11391"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11555"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.01606"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06562"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.07752"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09911"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11146"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11189"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11202"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11212"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11222"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11244"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11248"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11258"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11311"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11326"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11345"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11429"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11494"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11518"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11536"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11544"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1011.0774"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06703"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.08172"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.04755"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08240"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.09819"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.00422"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.02525"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.02605"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06913"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.00667"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.06850"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11096"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1405.3319"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.02633"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11038"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1806.11379">
<title>Theory IIIb: Generalization in Deep Networks. (arXiv:1806.11379v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11379</link>
<description rdf:parseType="Literal">&lt;p&gt;A main puzzle of deep neural networks (DNNs) revolves around the apparent
absence of &quot;overfitting&quot;, defined in this paper as follows: the expected error
does not get worse when increasing the number of neurons or of iterations of
gradient descent. This is surprising because of the large capacity demonstrated
by DNNs to fit randomly labeled data and the absence of explicit
regularization. Recent results by Srebro et al. provide a satisfying solution
of the puzzle for linear networks used in binary classification. They prove
that minimization of loss functions such as the logistic, the cross-entropy and
the exp-loss yields asymptotic, &quot;slow&quot; convergence to the maximum margin
solution for linearly separable datasets, independently of the initial
conditions. Here we prove a similar result for nonlinear multilayer DNNs near
zero minima of the empirical loss. The result holds for exponential-type losses
but not for the square loss. In particular, we prove that the weight matrix at
each layer of a deep network converges to a minimum norm solution up to a scale
factor (in the separable case). Our analysis of the dynamical system
corresponding to gradient descent of a multilayer network suggests a simple
criterion for ranking the generalization performance of different zero
minimizers of the empirical loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poggio_T/0/1/0/all/0/1&quot;&gt;Tomaso Poggio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1&quot;&gt;Qianli Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miranda_B/0/1/0/all/0/1&quot;&gt;Brando Miranda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banburski_A/0/1/0/all/0/1&quot;&gt;Andrzej Banburski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1&quot;&gt;Xavier Boix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hidary_J/0/1/0/all/0/1&quot;&gt;Jack Hidary&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.02301">
<title>Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?. (arXiv:1711.02301v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.02301</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning has achieved many recent successes, but our
understanding of its strengths and limitations is hampered by the lack of rich
environments in which we can fully characterize optimal behavior, and
correspondingly diagnose individual actions against such a characterization.
Here we consider a family of combinatorial games, arising from work of Erdos,
Selfridge, and Spencer, and we propose their use as environments for evaluating
and comparing different approaches to reinforcement learning. These games have
a number of appealing features: they are challenging for current learning
approaches, but they form (i) a low-dimensional, simply parametrized
environment where (ii) there is a linear closed form solution for optimal
behavior from any state, and (iii) the difficulty of the game can be tuned by
changing environment parameters in an interpretable way. We use these
Erdos-Selfridge-Spencer games not only to compare different algorithms, but
test for generalization, make comparisons to supervised learning, analyse
multiagent play, and even develop a self play algorithm. Code can be found at:
https://github.com/rubai5/ESS_Game
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghu_M/0/1/0/all/0/1&quot;&gt;Maithra Raghu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irpan_A/0/1/0/all/0/1&quot;&gt;Alex Irpan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1&quot;&gt;Jacob Andreas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinberg_R/0/1/0/all/0/1&quot;&gt;Robert Kleinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc V. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1&quot;&gt;Jon Kleinberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11103">
<title>Comment on: Decomposition of structural learning about directed acyclic graphs [1]. (arXiv:1806.11103v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.11103</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an alternative proof concerning necessary and sufficient
conditions to split the problem of searching for d-separators and building the
skeleton of a DAG into small problems for every node of a separation tree T.
The proof is simpler than the original [1]. The same proof structure has been
used in [2] for learning the structure of multivariate regression chain graphs
(MVR CGs).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javidian_M/0/1/0/all/0/1&quot;&gt;Mohammad Ali Javidian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valtorta_M/0/1/0/all/0/1&quot;&gt;Marco Valtorta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11204">
<title>Polynomial-time probabilistic reasoning with partial observations via implicit learning in probability logics. (arXiv:1806.11204v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.11204</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard approaches to probabilistic reasoning require that one possesses an
explicit model of the distribution in question. But, the empirical learning of
models of probability distributions from partial observations is a problem for
which efficient algorithms are generally not known. In this work we consider
the use of bounded-degree fragments of the &quot;sum-of-squares&quot; logic as a
probability logic. Prior work has shown that we can decide refutability for
such fragments in polynomial-time. We propose to use such fragments to answer
queries about whether a given probability distribution satisfies a given system
of constraints and bounds on expected values. We show that in answering such
queries, such constraints and bounds can be implicitly learned from partial
observations in polynomial-time as well. It is known that this logic is capable
of deriving many bounds that are useful in probabilistic analysis. We show here
that it furthermore captures useful polynomial-time fragments of resolution.
Thus, these fragments are also quite expressive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juba_B/0/1/0/all/0/1&quot;&gt;Brendan Juba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11298">
<title>A General Multi-agent Epistemic Planner Based on Higher-order Belief Change. (arXiv:1806.11298v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.11298</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, multi-agent epistemic planning has received attention from
both dynamic logic and planning communities. Existing implementations of
multi-agent epistemic planning are based on compilation into classical planning
and suffer from various limitations, such as generating only linear plans,
restriction to public actions, and incapability to handle disjunctive beliefs.
In this paper, we propose a general representation language for multi-agent
epistemic planning where the initial KB and the goal, the preconditions and
effects of actions can be arbitrary multi-agent epistemic formulas, and the
solution is an action tree branching on sensing results. To support efficient
reasoning in the multi-agent KD45 logic, we make use of a normal form called
alternating cover disjunctive formulas (ACDFs). We propose basic revision and
update algorithms for ACDFs. We also handle static propositional common
knowledge, which we call constraints. Based on our reasoning, revision and
update algorithms, adapting the PrAO algorithm for contingent planning from the
literature, we implemented a multi-agent epistemic planner called MEPK. Our
experimental results show the viability of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1&quot;&gt;Biqing Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1&quot;&gt;Hai Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongmei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11338">
<title>Quantum aspects of high dimensional formal representation of conceptual spaces. (arXiv:1806.11338v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.11338</link>
<description rdf:parseType="Literal">&lt;p&gt;Human cognition is a complex process facilitated by the intricate
architecture of human brain. However, human cognition is often reduced to
quantum theory based events in principle because of their correlative
conjectures for the purpose of analysis for reciprocal understanding. In this
paper, we begin our analysis of human cognition via formal methods and proceed
towards quantum theories. Human cognition often violate classic probabilities
on which formal representation of conceptual spaces are built. Further,
geometric representation of conceptual spaces proposed by Gardenfors discusses
the underlying content but lacks a systematic approach (Gardenfors, 2000; Kitto
et. al, 2012). However, the aforementioned views are not contradictory but
different perspective with a gap towards sufficient understanding of human
cognitive process. A comprehensive and systematic approach to model a
relatively complex scenario can be addressed by vector space approach of
conceptual spaces as discussed in literature. In this research, we have
proposed an approach that uses both formal representation and Gardenfors
geometric approach. The proposed model of high dimensional formal
representation of conceptual space is mathematically analysed and inferred to
exhibit quantum aspects. Also, the proposed model achieves cognition, in
particular, consciousness. We have demonstrated this process of achieving
consciousness with a constructive learning scenario. We have also proposed an
algorithm for conceptual scaling of a real world scenario under different
quality dimensions to obtain a conceptual scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_I/0/1/0/all/0/1&quot;&gt;Ishwarya M S&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherukuri_A/0/1/0/all/0/1&quot;&gt;Aswani Kumar Cherukuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11391">
<title>On embeddings as alternative paradigm for relational learning. (arXiv:1806.11391v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1806.11391</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world domains can be expressed as graphs and, more generally, as
multi-relational knowledge graphs. Though reasoning and learning with knowledge
graphs has traditionally been addressed by symbolic approaches, recent methods
in (deep) representation learning has shown promising results for specialized
tasks such as knowledge base completion. These approaches abandon the
traditional symbolic paradigm by replacing symbols with vectors in Euclidean
space. With few exceptions, symbolic and distributional approaches are explored
in different communities and little is known about their respective strengths
and weaknesses. In this work, we compare representation learning and relational
learning on various relational classification and clustering tasks and analyse
the complexity of the rules used implicitly by these approaches. Preliminary
results reveal possible indicators that could help in choosing one approach
over the other for particular knowledge graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumancic_S/0/1/0/all/0/1&quot;&gt;Sebastijan Dumancic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Duran_A/0/1/0/all/0/1&quot;&gt;Alberto Garcia-Duran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11555">
<title>High-Performance Parallel Implementation of Genetic Algorithm on FPGA. (arXiv:1806.11555v1 [cs.DC])</title>
<link>http://arxiv.org/abs/1806.11555</link>
<description rdf:parseType="Literal">&lt;p&gt;Genetic Algorithms (GAs) are used to solve search and optimization problems
in which an optimal solution can be found using an iterative process with
probabilistic and non-deterministic transitions. However, depending on the
problem&apos;s nature, the time required to find a solution can be high in
sequential machines due to the computational complexity of genetic algorithms.
This work proposes a parallel implementation of a genetic algorithm on
field-programmable gate array (FPGA). Optimization of the system&apos;s processing
time is the main goal of this project. Results associated with the processing
time and area occupancy (on FPGA) for various population sizes are analyzed.
Studies concerning the accuracy of the GA response for the optimization of two
variables functions were also evaluated for the hardware implementation.
However, the high-performance implementation proposes in this paper is able to
work with more variable from some adjustments on hardware architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torquato_M/0/1/0/all/0/1&quot;&gt;Matheus F. Torquato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandes_M/0/1/0/all/0/1&quot;&gt;Marcelo A. C. Fernandes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.01606">
<title>Multimedia Semantic Integrity Assessment Using Joint Embedding Of Images And Text. (arXiv:1707.01606v4 [cs.MM] UPDATED)</title>
<link>http://arxiv.org/abs/1707.01606</link>
<description rdf:parseType="Literal">&lt;p&gt;Real world multimedia data is often composed of multiple modalities such as
an image or a video with associated text (e.g. captions, user comments, etc.)
and metadata. Such multimodal data packages are prone to manipulations, where a
subset of these modalities can be altered to misrepresent or repurpose data
packages, with possible malicious intent. It is, therefore, important to
develop methods to assess or verify the integrity of these multimedia packages.
Using computer vision and natural language processing methods to directly
compare the image (or video) and the associated caption to verify the integrity
of a media package is only possible for a limited set of objects and scenes. In
this paper, we present a novel deep learning-based approach for assessing the
semantic integrity of multimedia packages containing images and captions, using
a reference set of multimedia packages. We construct a joint embedding of
images and captions with deep multimodal representation learning on the
reference dataset in a framework that also provides image-caption consistency
scores (ICCSs). The integrity of query media packages is assessed as the
inlierness of the query ICCSs with respect to the reference dataset. We present
the MultimodAl Information Manipulation dataset (MAIM), a new dataset of media
packages from Flickr, which we make available to the research community. We use
both the newly created dataset as well as Flickr30K and MS COCO datasets to
quantitatively evaluate our proposed approach. The reference dataset does not
contain unmanipulated versions of tampered query packages. Our method is able
to achieve F1 scores of 0.75, 0.89 and 0.94 on MAIM, Flickr30K and MS COCO,
respectively, for detecting semantically incoherent media packages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1&quot;&gt;Ayush Jaiswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabir_E/0/1/0/all/0/1&quot;&gt;Ekraam Sabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AbdAlmageed_W/0/1/0/all/0/1&quot;&gt;Wael AbdAlmageed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1&quot;&gt;Premkumar Natarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06562">
<title>An Iterative Closest Points Approach to Neural Generative Models. (arXiv:1711.06562v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06562</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a simple way to learn a transformation that maps samples of one
distribution to the samples of another distribution. Our algorithm comprises an
iteration of 1) drawing samples from some simple distribution and transforming
them using a neural network, 2) determining pairwise correspondences between
the transformed samples and training data (or a minibatch), and 3) optimizing
the weights of the neural network being trained to minimize the distances
between the corresponding vectors. This can be considered as a variant of the
Iterative Closest Points (ICP) algorithm, common in geometric computer vision,
although ICP typically operates on sensor point clouds and linear transforms
instead of random sample sets and neural nonlinear transforms. We demonstrate
the algorithm on simple synthetic data and MNIST data. We furthermore
demonstrate that the algorithm is capable of handling distributions with both
continuous and discrete variables.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajamaki_J/0/1/0/all/0/1&quot;&gt;Joose Rajam&amp;#xe4;ki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamalainen_P/0/1/0/all/0/1&quot;&gt;Perttu H&amp;#xe4;m&amp;#xe4;l&amp;#xe4;inen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.07752">
<title>Towards an unanimous international regulatory body for responsible use of Artificial Intelligence [UIRB-AI]. (arXiv:1712.07752v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.07752</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI), is once again in the phase of drastic
advancements. Unarguably, the technology itself can revolutionize the way we
live our everyday life. But the exponential growth of technology poses a
daunting task for policy researchers and law makers in making amendments to the
existing norms. In addition, not everyone in the society is studying the
potential socio-economic intricacies and cultural drifts that AI can bring
about. It is prudence to reflect from our historical past to propel the
development of technology in the right direction. To benefit the society of the
present and future, I scientifically explore the societal impact of AI. While
there are many public and private partnerships working on similar aspects, here
I describe the necessity for an Unanimous International Regulatory Body for all
applications of AI (UIRB-AI). I also discuss the benefits and drawbacks of such
an organization. To combat any drawbacks in the formation of an UIRB-AI, both
idealistic and pragmatic perspectives are discussed alternatively. The paper
further advances the discussion by proposing novel policies on how such
organization should be structured and how it can bring about a win-win
situation for everyone in the society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chidambaram_R/0/1/0/all/0/1&quot;&gt;Rajesh Chidambaram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09911">
<title>Discovering Bayesian Market Views for Intelligent Asset Allocation. (arXiv:1802.09911v2 [q-fin.CP] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09911</link>
<description rdf:parseType="Literal">&lt;p&gt;Along with the advance of opinion mining techniques, public mood has been
found to be a key element for stock market prediction. However, how market
participants&apos; behavior is affected by public mood has been rarely discussed.
Consequently, there has been little progress in leveraging public mood for the
asset allocation problem, which is preferred in a trusted and interpretable
way. In order to address the issue of incorporating public mood analyzed from
social media, we propose to formalize public mood into market views, because
market views can be integrated into the modern portfolio theory. In our
framework, the optimal market views will maximize returns in each period with a
Bayesian asset allocation model. We train two neural models to generate the
market views, and benchmark the model performance on other popular asset
allocation strategies. Our experimental results suggest that the formalization
of market views significantly increases the profitability (5% to 10% annually)
of the simulated portfolio at a given risk level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Xing_F/0/1/0/all/0/1&quot;&gt;Frank Z. Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Cambria_E/0/1/0/all/0/1&quot;&gt;Erik Cambria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Malandri_L/0/1/0/all/0/1&quot;&gt;Lorenzo Malandri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Vercellis_C/0/1/0/all/0/1&quot;&gt;Carlo Vercellis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11146">
<title>Adversarial Reprogramming of Neural Networks. (arXiv:1806.11146v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11146</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are susceptible to adversarial attacks. In computer
vision, well-crafted perturbations to images can cause neural networks to make
mistakes such as identifying a panda as a gibbon or confusing a cat with a
computer. Previous adversarial examples have been designed to degrade
performance of models or cause machine learning models to produce specific
outputs chosen ahead of time by the attacker. We introduce adversarial attacks
that instead reprogram the target model to perform a task chosen by the
attacker---without the attacker needing to specify or compute the desired
output for each test-time input. This attack is accomplished by optimizing for
a single adversarial perturbation, of unrestricted magnitude, that can be added
to all test-time inputs to a machine learning model in order to cause the model
to perform a task chosen by the adversary when processing these inputs---even
if the model was not trained to do this task. These perturbations can be thus
considered a program for the new task. We demonstrate adversarial reprogramming
on six ImageNet classification models, repurposing these models to perform a
counting task, as well as two classification tasks: classification of MNIST and
CIFAR-10 examples presented within the input to the ImageNet model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1&quot;&gt;Gamaleldin F. Elsayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodfellow_I/0/1/0/all/0/1&quot;&gt;Ian Goodfellow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1&quot;&gt;Jascha Sohl-Dickstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11189">
<title>A hybrid deep learning approach for medical relation extraction. (arXiv:1806.11189v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11189</link>
<description rdf:parseType="Literal">&lt;p&gt;Mining relationships between treatment(s) and medical problem(s) is vital in
the biomedical domain. This helps in various applications, such as decision
support system, safety surveillance, and new treatment discovery. We propose a
deep learning approach that utilizes both word level and sentence-level
representations to extract the relationships between treatment and problem.
While deep learning techniques demand a large amount of data for training, we
make use of a rule-based system particularly for relationship classes with
fewer samples. Our final relations are derived by jointly combining the results
from deep learning and rule-based models. Our system achieved a promising
performance on the relationship classes of I2b2 2010 relation extraction task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chikka_V/0/1/0/all/0/1&quot;&gt;Veera Raghavendra Chikka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlapalem_K/0/1/0/all/0/1&quot;&gt;Kamalakar Karlapalem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11202">
<title>Quit When You Can: Efficient Evaluation of Ensembles with Ordering Optimization. (arXiv:1806.11202v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11202</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a classifier ensemble and a set of examples to be classified, many
examples may be confidently and accurately classified after only a subset of
the base models in the ensemble are evaluated. This can reduce both mean
latency and CPU while maintaining the high accuracy of the original ensemble.
To achieve such gains, we propose jointly optimizing a fixed evaluation order
of the base models and early-stopping thresholds. Our proposed objective is a
combinatorial optimization problem, but we provide a greedy algorithm that
achieves a 4-approximation of the optimal solution for certain cases. For those
cases, this is also the best achievable polynomial time approximation bound
unless $P = NP$. Experiments on benchmark and real-world problems show that the
proposed Quit When You Can (QWYC) algorithm can speed-up average evaluation
time by $2$x--$4$x, and is around $1.5$x faster than prior work. QWYC&apos;s joint
optimization of ordering and thresholds also performed better in experiments
than various fixed orderings, including gradient boosted trees&apos; ordering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Serena Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Maya Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Seungil You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11212">
<title>Proxy Fairness. (arXiv:1806.11212v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11212</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of improving fairness when one lacks access to a
dataset labeled with protected groups, making it difficult to take advantage of
strategies that can improve fairness but require protected group labels, either
at training or runtime. To address this, we investigate improving fairness
metrics for proxy groups, and test whether doing so results in improved
fairness for the true sensitive groups. Results on benchmark and real-world
datasets demonstrate that such a proxy fairness strategy can work well in
practice. However, we caution that the effectiveness likely depends on the
choice of fairness metric, as well as how aligned the proxy groups are with the
true protected groups in terms of the constrained model parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Maya Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotter_A/0/1/0/all/0/1&quot;&gt;Andrew Cotter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fard_M/0/1/0/all/0/1&quot;&gt;Mahdi Milani Fard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Serena Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11222">
<title>Tight Prediction Intervals Using Expanded Interval Minimization. (arXiv:1806.11222v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11222</link>
<description rdf:parseType="Literal">&lt;p&gt;Prediction intervals are a valuable way of quantifying uncertainty in
regression problems. Good prediction intervals should be both correct,
containing the actual value between the lower and upper bound at least a target
percentage of the time; and tight, having a small mean width of the bounds.
Many prior techniques for generating prediction intervals make assumptions on
the distribution of error, which causes them to work poorly for problems with
asymmetric distributions.
&lt;/p&gt;
&lt;p&gt;This paper presents Expanded Interval Minimization (EIM), a novel loss
function for generating prediction intervals using neural networks. This loss
function uses minibatch statistics to estimate the coverage and optimize the
width of the prediction intervals. It does not make the same assumptions on the
distributions of data and error as prior work. We compare to three published
techniques and show EIM produces on average 1.37x tighter prediction intervals
and in the worst case 1.06x tighter intervals across two large real-world
datasets and varying coverage levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1&quot;&gt;Dongqi Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ting_Y/0/1/0/all/0/1&quot;&gt;Ying Yin Ting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ansel_J/0/1/0/all/0/1&quot;&gt;Jason Ansel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11244">
<title>Learning Multi-Step Robotic Tasks from Observation. (arXiv:1806.11244v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11244</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to burdensome data requirements, learning from demonstration often falls
short of its promise to allow users to quickly and naturally program robots.
Demonstrations are inherently ambiguous and incomplete, making a correct
generalization to unseen situations difficult without a large number of
demonstrations in varying conditions. By contrast, humans are often able to
learn complex tasks from a single demonstration (typically observations without
action labels) by leveraging context learned over a lifetime. Inspired by this
capability, we aim to enable robots to perform one-shot learning of multi-step
tasks from observation by leveraging auxiliary video data as context. Our
primary contribution is a novel action localization algorithm that identifies
clips of activities in auxiliary videos that match the activities in a
user-segmented demonstration, providing additional examples of each. While this
auxiliary video data could be used in multiple ways for learning, we focus on
an inverse reinforcement learning setting. We empirically show that across
several tasks, robots can learn multi-step tasks more effectively from videos
with localized actions, compared to unsegmented videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goo_W/0/1/0/all/0/1&quot;&gt;Wonjoon Goo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1&quot;&gt;Scott Niekum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11248">
<title>XGBoost: Scalable GPU Accelerated Learning. (arXiv:1806.11248v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11248</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe the multi-GPU gradient boosting algorithm implemented in the
XGBoost library (https://github.com/dmlc/xgboost). Our algorithm allows fast,
scalable training on multi-GPU systems with all of the features of the XGBoost
library. We employ data compression techniques to minimise the usage of scarce
GPU memory while still allowing highly efficient implementation. Using our
algorithm we show that it is possible to process 115 million training instances
in under three minutes on a publicly available cloud computing instance. The
algorithm is implemented using end-to-end GPU parallelism, with prediction,
gradient calculation, feature quantisation, decision tree construction and
evaluation phases all computed on device.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_R/0/1/0/all/0/1&quot;&gt;Rory Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adinets_A/0/1/0/all/0/1&quot;&gt;Andrey Adinets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_T/0/1/0/all/0/1&quot;&gt;Thejaswi Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1&quot;&gt;Eibe Frank&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11258">
<title>Hierarchical Dirichlet Process-based Open Set Recognition. (arXiv:1806.11258v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11258</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we proposed a novel hierarchical dirichlet process-based
classification framework for open set recognition (HDP-OSR) where new
categories&apos; samples unseen in training appear during testing. Unlike the
existing methods which deal with this problem from the perspective of
discriminative model, we reconsider this problem from the perspective of
generative model. We model each known class data in training set as a group in
hierarchical dirichlet process (HDP) while the testing set as a whole is
treated in the same way, then co-clustering all the groups under the HDP
framework. Based on the properties of HDP, our HDP-OSR does not overly depend
on training samples and can achieve adaptive change as the data changes. More
precisely, HDP-OSR can automatically reserve space for unknown categories while
it can also discover new categories, meaning it naturally adapts to the open
set recognition scenario. Furthermore, treating the testing set as a whole
makes our framework take the correlations among the testing samples into
account whereas the existing methods obviously ignore this information.
Experimental results on a set of benchmark data sets indicate the validity of
our learning framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_C/0/1/0/all/0/1&quot;&gt;Chuanxing Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Songcan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11311">
<title>Guaranteed Deterministic Bounds on the Total Variation Distance between Univariate Mixtures. (arXiv:1806.11311v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11311</link>
<description rdf:parseType="Literal">&lt;p&gt;The total variation distance is a core statistical distance between
probability measures that satisfies the metric axioms, with value always
falling in $[0,1]$. This distance plays a fundamental role in machine learning
and signal processing: It is a member of the broader class of $f$-divergences,
and it is related to the probability of error in Bayesian hypothesis testing.
Since the total variation distance does not admit closed-form expressions for
statistical mixtures (like Gaussian mixture models), one often has to rely in
practice on costly numerical integrations or on fast Monte Carlo approximations
that however do not guarantee deterministic lower and upper bounds. In this
work, we consider two methods for bounding the total variation of univariate
mixture models: The first method is based on the information monotonicity
property of the total variation to design guaranteed nested deterministic lower
bounds. The second method relies on computing the geometric lower and upper
envelopes of weighted mixture components to derive deterministic bounds based
on density ratio. We demonstrate the tightness of our bounds in a series of
experiments on Gaussian, Gamma and Rayleigh mixture models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1&quot;&gt;Frank Nielsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1&quot;&gt;Ke Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11326">
<title>Unsupervised Detection and Explanation of Latent-class Contextual Anomalies. (arXiv:1806.11326v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.11326</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting and explaining anomalies is a challenging effort. This holds
especially true when data exhibits strong dependencies and single measurements
need to be assessed and analyzed in their respective context. In this work, we
consider scenarios where measurements are non-i.i.d, i.e. where samples are
dependent on corresponding discrete latent variables which are connected
through some given dependency structure, the contextual information. Our
contribution is twofold: (i) Building atop of support vector data description
(SVDD), we derive a method able to cope with latent-class dependency structure
that can still be optimized efficiently. We further show that our approach
neatly generalizes vanilla SVDD as well as k-means and conditional random
fields (CRF) and provide a corresponding probabilistic interpretation. (ii) In
unsupervised scenarios where it is not possible to quantify the accuracy of an
anomaly detector, having an human-interpretable solution is the key to success.
Based on deep Taylor decomposition and a reformulation of our trained anomaly
detector as a neural network, we are able to backpropagate predictions to
pixel-domain and thus identify features and regions of high relevance. We
demonstrate the usefulness of our novel approach on toy data with known
spatio-temporal structure and successfully validate on synthetic as well as
real world off-shore data from the oil industry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kauffmann_J/0/1/0/all/0/1&quot;&gt;Jacob Kauffmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Montavon_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;goire Montavon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lima_L/0/1/0/all/0/1&quot;&gt;Luiz Alberto Lima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nakajima_S/0/1/0/all/0/1&quot;&gt;Shinichi Nakajima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gornitz_N/0/1/0/all/0/1&quot;&gt;Nico G&amp;#xf6;rnitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11345">
<title>Measuring the quality of Synthetic data for use in competitions. (arXiv:1806.11345v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11345</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning has the potential to assist many communities in using the
large datasets that are becoming more and more available. Unfortunately, much
of that potential is not being realized because it would require sharing data
in a way that compromises privacy. In order to overcome this hurdle, several
methods have been proposed that generate synthetic data while preserving the
privacy of the real data. In this paper we consider a key characteristic that
synthetic data should have in order to be useful for machine learning
researchers - the relative performance of two algorithms (trained and tested)
on the synthetic dataset should be the same as their relative performance (when
trained and tested) on the original dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordon_J/0/1/0/all/0/1&quot;&gt;James Jordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jinsung Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11429">
<title>Certifying Global Optimality of Graph Cuts via Semidefinite Relaxation: A Performance Guarantee for Spectral Clustering. (arXiv:1806.11429v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.11429</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral clustering has become one of the most widely used clustering
techniques when the structure of the individual clusters is non-convex or
highly anisotropic. Yet, despite its immense popularity, there exists fairly
little theory about performance guarantees for spectral clustering. This issue
is partly due to the fact that spectral clustering typically involves two steps
which complicated its theoretical analysis: first, the eigenvectors of the
associated graph Laplacian are used to embed the dataset, and second, k-means
clustering algorithm is applied to the embedded dataset to get the labels. This
paper is devoted to the theoretical foundations of spectral clustering and
graph cuts. We consider a convex relaxation of graph cuts, namely ratio cuts
and normalized cuts, that makes the usual two-step approach of spectral
clustering obsolete and at the same time gives rise to a rigorous theoretical
analysis of graph cuts and spectral clustering. We derive deterministic bounds
for successful spectral clustering via a spectral proximity condition that
naturally depends on the algebraic connectivity of each cluster and the
inter-cluster connectivity. Moreover, we demonstrate by means of some popular
examples that our bounds can achieve near-optimality. Our findings are also
fundamental for the theoretical understanding of kernel k-means. Numerical
simulations confirm and complement our analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ling_S/0/1/0/all/0/1&quot;&gt;Shuyang Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Strohmer_T/0/1/0/all/0/1&quot;&gt;Thomas Strohmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11494">
<title>Comparing Graph Clusterings: Set partition measures vs. Graph-aware measures. (arXiv:1806.11494v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11494</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a family of graph partition similarity measures
that take the topology of the graph into account. These graph-aware measures
are alternatives to using set partition similarity measures that are not
specifically designed for graph partitions. The two types of measures,
graph-aware and set partition measures, are shown to have opposite behaviors
with respect to resolution issues and provide complementary information
necessary to assess that two graph partitions are similar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poulin_V/0/1/0/all/0/1&quot;&gt;Val&amp;#xe9;rie Poulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theberge_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Th&amp;#xe9;berge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11518">
<title>Sparse Three-parameter Restricted Indian Buffet Process for Understanding International Trade. (arXiv:1806.11518v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11518</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a Bayesian nonparametric latent feature model specially
suitable for exploratory analysis of high-dimensional count data. We perform a
non-negative doubly sparse matrix factorization that has two main advantages:
not only we are able to better approximate the row input distributions, but the
inferred topics are also easier to interpret. By combining the three-parameter
and restricted Indian buffet processes into a single prior, we increase the
model flexibility, allowing for a full spectrum of sparse solutions in the
latent space. We demonstrate the usefulness of our approach in the analysis of
countries&apos; economic structure. Compared to other approaches, empirical results
show our model&apos;s ability to give easy-to-interpret information and better
capture the underlying sparsity structure of data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pradier_M/0/1/0/all/0/1&quot;&gt;Melanie F. Pradier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stojkoski_V/0/1/0/all/0/1&quot;&gt;Viktor Stojkoski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Utkovski_Z/0/1/0/all/0/1&quot;&gt;Zoran Utkovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocarev_L/0/1/0/all/0/1&quot;&gt;Ljupco Kocarev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Cruz_F/0/1/0/all/0/1&quot;&gt;Fernando Perez-Cruz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11536">
<title>Quantized Decentralized Consensus Optimization. (arXiv:1806.11536v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1806.11536</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of decentralized consensus optimization, where the
sum of $n$ convex functions are minimized over $n$ distributed agents that form
a connected network. In particular, we consider the case that the communicated
local decision variables among nodes are quantized in order to alleviate the
communication bottleneck in distributed optimization. We propose the Quantized
Decentralized Gradient Descent (QDGD) algorithm, in which nodes update their
local decision variables by combining the quantized information received from
their neighbors with their local information. We prove that under standard
strong convexity and smoothness assumptions for the objective function, QDGD
achieves a vanishing mean solution error. To the best of our knowledge, this is
the first algorithm that achieves vanishing consensus error in the presence of
quantization noise. Moreover, we provide simulation results that show tight
agreement between our derived theoretical convergence rate and the experimental
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reisizadeh_A/0/1/0/all/0/1&quot;&gt;Amirhossein Reisizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mokhtari_A/0/1/0/all/0/1&quot;&gt;Aryan Mokhtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1&quot;&gt;Hamed Hassani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedarsani_R/0/1/0/all/0/1&quot;&gt;Ramtin Pedarsani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11544">
<title>Nonparametric learning from Bayesian models with randomized objective functions. (arXiv:1806.11544v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1806.11544</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian learning is built on an assumption that the model space contains a
true reflection of the data generating mechanism. This assumption is
problematic, particularly in complex data environments. Here we present a
Bayesian nonparametric approach to learning that makes use of statistical
models, but does not assume that the model is true. Our approach has provably
better properties than using a parametric model and admits a trivially
parallelizable Monte Carlo sampling scheme that affords massive scalability on
modern computer architectures. The model-based aspect of learning is
particularly attractive for regularizing nonparametric inference when the
sample size is small, and also for correcting approximate approaches such as
variational Bayes (VB). We demonstrate the approach on a number of examples
including VB classifiers and Bayesian random forests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lyddon_S/0/1/0/all/0/1&quot;&gt;S. P. Lyddon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Walker_S/0/1/0/all/0/1&quot;&gt;S. G. Walker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1&quot;&gt;C. C. Holmes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1011.0774">
<title>Leaders, Followers, and Community Detectio. (arXiv:1011.0774v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1011.0774</link>
<description rdf:parseType="Literal">&lt;p&gt;Communities in social networks or graphs are sets of well-connected,
overlapping vertices. The effectiveness of a community detection algorithm is
determined by accuracy in finding the ground-truth communities and ability to
scale with the size of the data. In this work, we provide three contributions.
First, we show that a popular measure of accuracy known as the F1 score, which
is between 0 and 1, with 1 being perfect detection, has an information lower
bound is 0.5. We provide a trivial algorithm that produces communities with an
F1 score of 0.5 for any graph! Somewhat surprisingly, we find that popular
algorithms such as modularity optimization, BigClam and CESNA have F1 scores
less than 0.5 for the popular IMDB graph. To rectify this, as the second
contribution we propose a generative model for community formation, the
sequential community graph, which is motivated by the formation of social
networks. Third, motivated by our generative model, we propose the
leader-follower algorithm (LFA). We prove that it recovers all communities for
sequential community graphs by establishing a structural result that sequential
community graphs are chordal. For a large number of popular social networks, it
recovers communities with a much higher F1 score than other popular algorithms.
For the IMDB graph, it obtains an F1 score of 0.81. We also propose a
modification to the LFA called the fast leader-follower algorithm (FLFA) which
in addition to being highly accurate, is also fast, with a scaling that is
almost linear in the network size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Parthasarathy_D/0/1/0/all/0/1&quot;&gt;Dhruv Parthasarathy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shah_D/0/1/0/all/0/1&quot;&gt;Devavrat Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zaman_T/0/1/0/all/0/1&quot;&gt;Tauhid Zaman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06703">
<title>Function Norms and Regularization in Deep Networks. (arXiv:1710.06703v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06703</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have become increasingly important due to their
excellent empirical performance on a wide range of problems. However,
regularization is generally achieved by indirect means, largely due to the
complex set of functions defined by a network and the difficulty in measuring
function complexity. There exists no method in the literature for additive
regularization based on a norm of the function, as is classically considered in
statistical learning theory. In this work, we propose sampling-based
approximations to weighted function norms as regularizers for deep neural
networks. We provide, to the best of our knowledge, the first proof in the
literature of the NP-hardness of computing function norms of DNNs, motivating
the necessity of an approximate approach. We then derive a generalization bound
for functions trained with weighted norms and prove that a natural stochastic
optimization strategy minimizes the bound. Finally, we empirically validate the
improved performance of the proposed regularization strategies for both convex
function sets as well as DNNs on real-world classification and image
segmentation tasks demonstrating improved performance over weight decay,
dropout, and batch normalization. Source code will be released at the time of
publication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triki_A/0/1/0/all/0/1&quot;&gt;Amal Rannen Triki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berman_M/0/1/0/all/0/1&quot;&gt;Maxim Berman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1&quot;&gt;Matthew B. Blaschko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.08172">
<title>Run-and-Inspect Method for Nonconvex Optimization and Global Optimality Bounds for R-Local Minimizers. (arXiv:1711.08172v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1711.08172</link>
<description rdf:parseType="Literal">&lt;p&gt;Many optimization algorithms converge to stationary points. When the
underlying problem is nonconvex, they may get trapped at local minimizers and
occasionally stagnate near saddle points. We propose the Run-and-Inspect
Method, which adds an &quot;inspect&quot; phase to existing algorithms that helps escape
from non-global stationary points. The inspection samples a set of points in a
radius $R$ around the current point. When a sample point yields a sufficient
decrease in the objective, we move there and resume an existing algorithm. If
no sufficient decrease is found, the current point is called an approximate
$R$-local minimizer. We show that an $R$-local minimizer is globally optimal,
up to a specific error depending on $R$, if the objective function can be
implicitly decomposed into a smooth convex function plus a restricted function
that is possibly nonconvex, nonsmooth. For high-dimensional problems, we
introduce blockwise inspections to overcome the curse of dimensionality while
still maintaining optimality bounds up to a factor equal to the number of
blocks. Our method performs well on a set of artificial and realistic nonconvex
problems by coupling with gradient descent, coordinate descent, EM, and
prox-linear algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuejiao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wotao Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.04755">
<title>Exponential convergence of testing error for stochastic gradient methods. (arXiv:1712.04755v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.04755</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider binary classification problems with positive definite kernels and
square loss, and study the convergence rates of stochastic gradient methods. We
show that while the excess testing loss (squared loss) converges slowly to zero
as the number of observations (and thus iterations) goes to infinity, the
testing error (classification error) converges exponentially fast if low-noise
conditions are assumed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pillaud_Vivien_L/0/1/0/all/0/1&quot;&gt;Loucas Pillaud-Vivien&lt;/a&gt; (SIERRA), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1&quot;&gt;Alessandro Rudi&lt;/a&gt; (SIERRA), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis Bach&lt;/a&gt; (SIERRA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08240">
<title>Estimating activity cycles with probabilistic methods II. The Mount Wilson Ca H&amp;K data. (arXiv:1712.08240v3 [astro-ph.SR] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08240</link>
<description rdf:parseType="Literal">&lt;p&gt;Debate over the existence of branches in the stellar activity-rotation
diagrams continues. Application of modern time series analysis tools to study
the mean cycle periods in chromospheric activity index is lacking. We develop
such models, based on Gaussian processes, for one-dimensional time series and
apply it to the extended Mount Wilson Ca H&amp;amp;K sample. Our main aim is to study
how the previously commonly used assumption of strict harmonicity of the
stellar cycles as well as handling of the linear trends affects the results. We
introduce three methods of different complexity, starting with the simple
Bayesian harmonic model and followed by Gaussian Process models with periodic
and quasi-periodic covariance functions. We confirm the existence of two
populations in the activity-period diagram. We find only one significant trend
in the inactive population, namely that the cycle periods get shorter with
increasing rotation. This is in contrast with earlier studies, that postulate
the existence of trends in both of the populations. In terms of rotation to
cycle period ratio, our data is consistent with only two activity branches such
that the active branch merges together with the transitional one. The retrieved
stellar cycles are uniformly distributed over the R&apos;HK activity index,
indicating that the operation of stellar large-scale dynamos carries smoothly
over the Vaughan-Preston gap. At around the solar activity index, however,
indications of a disruption in the cyclic dynamo action are seen. Our study
shows that stellar cycle estimates depend significantly on the model applied.
Such model-dependent aspects include the improper treatment of linear trends,
while the assumption of strict harmonicity can result in the appearance of
double cyclicities that seem more likely to be explained by the
quasi-periodicity of the cycles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Olspert_N/0/1/0/all/0/1&quot;&gt;N. Olspert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Lehtinen_J/0/1/0/all/0/1&quot;&gt;J. Lehtinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kapyla_M/0/1/0/all/0/1&quot;&gt;M. J. K&amp;#xe4;pyl&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Pelt_J/0/1/0/all/0/1&quot;&gt;J. Pelt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Grigorievskiy_A/0/1/0/all/0/1&quot;&gt;A. Grigorievskiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.09819">
<title>Transformation Autoregressive Networks. (arXiv:1801.09819v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.09819</link>
<description rdf:parseType="Literal">&lt;p&gt;The fundamental task of general density estimation $p(x)$ has been of keen
interest to machine learning. In this work, we attempt to systematically
characterize methods for density estimation. Broadly speaking, most of the
existing methods can be categorized into either using: \textit{a})
autoregressive models to estimate the conditional factors of the chain rule,
$p(x_{i}\, |\, x_{i-1}, \ldots)$; or \textit{b}) non-linear transformations of
variables of a simple base distribution. Based on the study of the
characteristics of these categories, we propose multiple novel methods for each
category. For example we proposed RNN based transformations to model
non-Markovian dependencies. Further, through a comprehensive study over both
real world and synthetic data, we show for that jointly leveraging
transformations of variables and autoregressive conditional models, results in
a considerable improvement in performance. We illustrate the use of our models
in outlier detection and image modeling. Finally we introduce a novel data
driven framework for learning a family of distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oliva_J/0/1/0/all/0/1&quot;&gt;Junier B. Oliva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Avinava Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zaheer_M/0/1/0/all/0/1&quot;&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poczos_B/0/1/0/all/0/1&quot;&gt;Barnab&amp;#xe1;s P&amp;#xf3;czos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.00422">
<title>Distributed regression modeling for selecting markers under data protection constraints. (arXiv:1803.00422v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1803.00422</link>
<description rdf:parseType="Literal">&lt;p&gt;Data protection constraints frequently require a distributed analysis of
data, i.e., individual-level data remains at many different sites, but analysis
nevertheless has to be performed jointly. The corresponding aggregated data is
often exchanged manually, requiring explicit permission before transfer, i.e.,
the number of data calls and the amount of data should be limited. Thus, only
simple aggregated summary statistics are typically transferred with just a
single call. This does not allow for more complex tasks such as variable
selection. As an alternative, we propose a multivariable regression approach
for identifying important markers by automatic variable selection based on
aggregated data from different locations in iterative calls. To minimize the
amount of transferred data and the number of calls, we also provide a heuristic
variant of the approach. When performing a global data standardization, the
proposed methods yields the same results as when pooling individual-level data.
In a simulation study, the information loss introduced by a local
standardization is seen to be minimal. In a typical scenario, the heuristic
decreases the number of data calls from more than 10 to 3, rendering manual
data releases feasible. To make our approach widely available for application,
we provide an implementation on top of the DataSHIELD framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zoller_D/0/1/0/all/0/1&quot;&gt;Daniela Z&amp;#xf6;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lenz_S/0/1/0/all/0/1&quot;&gt;Stefan Lenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Binder_H/0/1/0/all/0/1&quot;&gt;Harald Binder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.02525">
<title>Fast Robust Methods for Singular State-Space Models. (arXiv:1803.02525v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1803.02525</link>
<description rdf:parseType="Literal">&lt;p&gt;State-space models are used in a wide range of time series analysis
formulations. Kalman filtering and smoothing are work-horse algorithms in these
settings. While classic algorithms assume Gaussian errors to simplify
estimation, recent advances use a broader range of optimization formulations to
allow outlier-robust estimation, as well as constraints to capture prior
information.
&lt;/p&gt;
&lt;p&gt;Here we develop methods on state-space models where either innovations or
error covariances may be singular. These models frequently arise in navigation
(e.g. for `colored noise&apos; models or deterministic integrals) and are ubiquitous
in auto-correlated time series models such as ARMA. We reformulate all
state-space models (singular as well as nonsinguar) as constrained convex
optimization problems, and develop an efficient algorithm for this
reformulation. The convergence rate is {\it locally linear}, with constants
that do not depend on the conditioning of the problem.
&lt;/p&gt;
&lt;p&gt;Numerical comparisons show that the new approach outperforms competing
approaches for {\it nonsingular} models, including state of the art interior
point (IP) methods. IP methods converge at superlinear rates; we expect them to
dominate. However, the steep rate of the proposed approach (independent of
problem conditioning) combined with cheap iterations wins against IP in a
run-time comparison. We therefore suggest that the proposed approach be the
{\it default choice} for estimating state space models outside of the Gaussian
context, regardless of whether the error covariances are singular or not.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jonker_J/0/1/0/all/0/1&quot;&gt;Jonathan Jonker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Aravkin_A/0/1/0/all/0/1&quot;&gt;Aleksandr Y. Aravkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Burke_J/0/1/0/all/0/1&quot;&gt;James V. Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pillonetto_G/0/1/0/all/0/1&quot;&gt;Gianluigi Pillonetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Webster_S/0/1/0/all/0/1&quot;&gt;Sarah Webster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.02605">
<title>Moving Beyond Sub-Gaussianity in High-Dimensional Statistics: Applications in Covariance Estimation and Linear Regression. (arXiv:1804.02605v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1804.02605</link>
<description rdf:parseType="Literal">&lt;p&gt;Concentration inequalities form an essential toolkit in the study of
high-dimensional statistical methods. Most of the relevant statistics
literature is based on the assumptions of sub-Gaussian/sub-exponential random
vectors. In this paper, we bring together various probability inequalities for
sums of independent random variables under much weaker exponential type
(sub-Weibull) tail assumptions. These results extract a part sub-Gaussian tail
behavior in finite samples, matching the asymptotics governed by the central
limit theorem, and are compactly represented in terms of a new Orlicz
quasi-norm - the Generalized Bernstein-Orlicz norm - that typifies such tail
behaviors.
&lt;/p&gt;
&lt;p&gt;We illustrate the usefulness of these inequalities through the analysis of
four fundamental problems in high-dimensional statistics. In the first two
problems, we study the rate of convergence of the sample covariance matrix in
terms of the maximum elementwise norm and the maximum k-sub-matrix operator
norm which are key quantities of interest in bootstrap procedures and
high-dimensional structured covariance matrix estimation. The third example
concerns the restricted eigenvalue condition, required in high dimensional
linear regression, which we verify for all sub-Weibull random vectors under
only marginal (not joint) tail assumptions on the covariates. To our knowledge,
this is the first unified result obtained in such generality. In the final
example, we consider the Lasso estimator for linear regression and establish
its rate of convergence under much weaker tail assumptions (on the errors as
well as the covariates) than those in the existing literature. The common
feature in all our results is that the convergence rates under most exponential
tails match the usual ones under sub-Gaussian assumptions. Finally, we also
establish a high-dimensional CLT and tail bounds for empirical processes for
sub-Weibulls.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kuchibhotla_A/0/1/0/all/0/1&quot;&gt;Arun Kumar Kuchibhotla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chakrabortty_A/0/1/0/all/0/1&quot;&gt;Abhishek Chakrabortty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06913">
<title>Fast inference of deep neural networks in FPGAs for particle physics. (arXiv:1804.06913v3 [physics.ins-det] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06913</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent results at the Large Hadron Collider (LHC) have pointed to enhanced
physics capabilities through the improvement of the real-time event processing
techniques. Machine learning methods are ubiquitous and have proven to be very
powerful in LHC physics, and particle physics as a whole. However, exploration
of the use of such techniques in low-latency, low-power FPGA hardware has only
just begun. FPGA-based trigger and data acquisition (DAQ) systems have
extremely low, sub-microsecond latency requirements that are unique to particle
physics. We present a case study for neural network inference in FPGAs focusing
on a classifier for jet substructure which would enable, among many other
physics scenarios, searches for new dark sector particles and novel
measurements of the Higgs boson. While we focus on a specific example, the
lessons are far-reaching. We develop a package based on High-Level Synthesis
(HLS) called hls4ml to build machine learning models in FPGAs. The use of HLS
increases accessibility across a broad user community and allows for a drastic
decrease in firmware development time. We map out FPGA resource usage and
latency versus neural network hyperparameters to identify the problems in
particle physics that would benefit from performing neural network inference
with FPGAs. For our example jet substructure model, we fit well within the
available resources of modern FPGAs with a latency on the scale of 100 ns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Duarte_J/0/1/0/all/0/1&quot;&gt;Javier Duarte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Song Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Harris_P/0/1/0/all/0/1&quot;&gt;Philip Harris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jindariani_S/0/1/0/all/0/1&quot;&gt;Sergo Jindariani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kreinar_E/0/1/0/all/0/1&quot;&gt;Edward Kreinar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kreis_B/0/1/0/all/0/1&quot;&gt;Benjamin Kreis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ngadiuba_J/0/1/0/all/0/1&quot;&gt;Jennifer Ngadiuba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pierini_M/0/1/0/all/0/1&quot;&gt;Maurizio Pierini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rivera_R/0/1/0/all/0/1&quot;&gt;Ryan Rivera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Tran_N/0/1/0/all/0/1&quot;&gt;Nhan Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhenbin Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.00667">
<title>Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks. (arXiv:1806.00667v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.00667</link>
<description rdf:parseType="Literal">&lt;p&gt;We prove, under two sufficient conditions, that idealised models can have no
adversarial examples. We discuss which idealised models satisfy our conditions,
and show that idealised Bayesian neural networks (BNNs) satisfy these. We
continue by studying near-idealised BNNs using HMC inference, demonstrating the
theoretical ideas in practice. We experiment with HMC on synthetic data derived
from MNIST for which we know the ground-truth image density, showing that
near-perfect epistemic uncertainty correlates to density under image manifold,
and that adversarial images lie off the manifold in our setting. This suggests
why MC dropout, which can be seen as performing approximate inference, has been
observed to be an effective defence against adversarial examples in practice;
We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting
a new attack for dropout models and a new defence as well. Lastly, we
demonstrate the defence on a cats-vs-dogs image classification task with a
VGG13 variant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gal_Y/0/1/0/all/0/1&quot;&gt;Yarin Gal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smith_L/0/1/0/all/0/1&quot;&gt;Lewis Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.06850">
<title>Polynomial Regression As an Alternative to Neural Nets. (arXiv:1806.06850v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.06850</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the success of neural networks (NNs), there is still a concern among
many over their &quot;black box&quot; nature. Why do they work? Here we present a simple
analytic argument that NNs are in fact essentially polynomial regression
models. This view will have various implications for NNs, e.g. providing an
explanation for why convergence problems arise in NNs, and it gives rough
guidance on avoiding overfitting. In addition, we use this phenomenon to
predict and confirm a multicollinearity property of NNs not previously reported
in the literature. Most importantly, given this loose correspondence, one may
choose to routinely use polynomial models instead of NNs, thus avoiding some
major problems of the latter, such as having to set many tuning parameters and
dealing with convergence issues. We present a number of empirical results; in
each case, the accuracy of the polynomial approach matches or exceeds that of
NN approaches. A many-featured, open-source software package, polyreg, is
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khomtchouk_B/0/1/0/all/0/1&quot;&gt;Bohdan Khomtchouk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matloff_N/0/1/0/all/0/1&quot;&gt;Norman Matloff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohanty_P/0/1/0/all/0/1&quot;&gt;Pete Mohanty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11096">
<title>Recovering Trees with Convex Clustering. (arXiv:1806.11096v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.11096</link>
<description rdf:parseType="Literal">&lt;p&gt;Convex clustering refers, for given $\left\{x_1, \dots, x_n\right\} \subset
\mathbb{R}^p$, to the minimization of \begin{eqnarray*} u(\gamma) &amp;amp; = &amp;amp;
\underset{u_1, \dots, u_n }{\arg\min}\;\sum_{i=1}^{n}{\lVert x_i - u_i
\rVert^2} + \gamma \sum_{i,j=1}^{n}{w_{ij} \lVert u_i - u_j\rVert},\\
\end{eqnarray*} where $w_{ij} \geq 0$ is an affinity that quantifies the
similarity between $x_i$ and $x_j$. We prove that if the affinities $w_{ij}$
reflect a tree structure in the $\left\{x_1, \dots, x_n\right\}$, then the
convex clustering solution path reconstructs the tree exactly. The main
technical ingredient implies the following combinatorial byproduct: for every
set $\left\{x_1, \dots, x_n \right\} \subset \mathbb{R}^p$ of $n \geq 2$
distinct points, there exist at least $n/6$ points with the property that for
any of these points $x$ there is a unit vector $v \in \mathbb{R}^p$ such that,
when viewed from $x$, `most&apos; points lie in the direction $v$ \begin{eqnarray*}
\frac{1}{n-1}\sum_{i=1 \atop x_i \neq x}^{n}{ \left\langle \frac{x_i -
x}{\lVert x_i - x \rVert}, v \right\rangle} &amp;amp; \geq &amp;amp; \frac{1}{4}.
\end{eqnarray*}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chi_E/0/1/0/all/0/1&quot;&gt;Eric C. Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Steinerberger_S/0/1/0/all/0/1&quot;&gt;Stefan Steinerberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1405.3319">
<title>Fully Bayesian Logistic Regression with Hyper-Lasso Priors for High-dimensional Feature Selection. (arXiv:1405.3319v4 [stat.CO] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1405.3319</link>
<description rdf:parseType="Literal">&lt;p&gt;High-dimensional feature selection arises in many areas of modern science.
For example, in genomic research we want to find the genes that can be used to
separate tissues of different classes (e.g. cancer and normal) from tens of
thousands of genes that are active (expressed) in certain tissue cells. To this
end, we wish to fit regression and classification models with a large number of
features (also called variables, predictors). In the past decade, penalized
likelihood methods for fitting regression models based on hyper-LASSO
penalization have received increasing attention in the literature. However,
fully Bayesian methods that use Markov chain Monte Carlo (MCMC) are still in
lack of development in the literature. In this paper we introduce an MCMC
(fully Bayesian) method for learning severely multi-modal posteriors of
logistic regression models based on hyper-LASSO priors (non-convex penalties).
Our MCMC algorithm uses Hamiltonian Monte Carlo in a restricted Gibbs sampling
framework; we call our method Bayesian logistic regression with hyper-LASSO
(BLRHL) priors. We have used simulation studies and real data analysis to
demonstrate the superior performance of hyper-LASSO priors, and to investigate
the issues of choosing heaviness and scale of hyper-LASSO priors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Longhai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Weixin Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.02633">
<title>Recursive Neural Networks in Quark/Gluon Tagging. (arXiv:1711.02633v2 [hep-ph] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1711.02633</link>
<description rdf:parseType="Literal">&lt;p&gt;Since the machine learning techniques are improving rapidly, it has been
shown that the image recognition techniques in deep neural networks can be used
to detect jet substructure. And it turns out that deep neural networks can
match or outperform traditional approach of expert features. However, there are
disadvantages such as sparseness of jet images. Based on the natural tree-like
structure of jet sequential clustering, the recursive neural networks (RecNNs),
which embed jet clustering history recursively as in natural language
processing, have a better behavior when confronted with these problems. We thus
try to explore the performance of RecNNs in quark/gluon discrimination. The
results show that RecNNs work better than the baseline boosted decision tree
(BDT) by a few percent in gluon rejection rate. However, extra implementation
of particle flow identification only increases the performance slightly. We
also experimented on some relevant aspects which might influence the
performance of the networks. It shows that even taking only particle flow
identification as input feature without any extra information on momentum or
angular position is already giving a fairly good result, which indicates that
the most of the information for quark/gluon discrimination is already included
in the tree-structure itself. As a bonus, a rough up/down quark jets
discrimination is also explored.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Cheng_T/0/1/0/all/0/1&quot;&gt;Taoli Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11038">
<title>Neural Network Cognitive Engine for Autonomous and Distributed Underlay Dynamic Spectrum Access. (arXiv:1806.11038v1 [cs.NI] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1806.11038</link>
<description rdf:parseType="Literal">&lt;p&gt;An important challenge in underlay dynamic spectrum access (DSA) is how to
establish an interference limit for the primary network (PN) and how cognitive
radios (CRs) in the secondary network (SN) become aware of their created
interference on the PN, especially when there is no exchange of information
between the primary and the secondary networks. This challenge is addressed in
this paper by present- ing a fully autonomous and distributed underlay DSA
scheme where each CR operates based on predicting its transmission effect on
the PN. The scheme is based on a cognitive engine with an artificial neural
network that predicts, without exchanging information between the networks, the
adaptive modulation and coding configuration for the primary link nearest to a
transmitting CR. By managing the tradeoff between the effect of the SN on the
PN and the achievable throughput at the SN, the presented technique maintains
the change in the PN relative average throughput within a prescribed maximum
value, while also finding transmit settings for the CRs that result in
throughput as large as allowed by the PN interference limit. Moreover, the
proposed technique increases the CRs transmission opportunities compared to a
scheme that can only estimate the modulation scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_F/0/1/0/all/0/1&quot;&gt;Fatemeh Shah Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwasinski_A/0/1/0/all/0/1&quot;&gt;Andres Kwasinski&lt;/a&gt;</dc:creator>
</item></rdf:RDF>