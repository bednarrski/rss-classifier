<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-02-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02678"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02844"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02987"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02669"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02892"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1707.05101"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.01867"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06800"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.00345"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02534"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02840"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02852"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02856"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02885"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02907"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02920"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03001"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1610.01857"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1612.00374"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.02971"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.03266"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.01204"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1802.02678">
<title>Biological Mechanisms for Learning: A Computational Model of Olfactory Learning in the Manduca sexta Moth, with Applications to Neural Nets. (arXiv:1802.02678v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1802.02678</link>
<description rdf:parseType="Literal">&lt;p&gt;The insect olfactory system, which includes the antennal lobe (AL), mushroom
body (MB), and ancillary structures, is a relatively simple neural system
capable of learning. Its structural features, which are widespread in
biological neural systems, process olfactory stimuli through a cascade of
networks where large dimension shifts occur from stage to stage and where
sparsity and randomness play a critical role in coding. Learning is partly
enabled by a neuromodulatory reward mechanism of octopamine stimulation of the
AL, whose increased activity induces rewiring of the MB through Hebbian
plasticity. Enforced sparsity in the MB focuses Hebbian growth on neurons that
are the most important for the representation of the learned odor. Based upon
current biophysical knowledge, we have constructed an end-to-end computational
model of the Manduca sexta moth olfactory system which includes the interaction
of the AL and MB under octopamine stimulation. Our model is able to robustly
learn new odors, and our simulations of integrate-and-fire neurons match the
statistical features of in-vivo firing rate data. From a biological
perspective, the model provides a valuable tool for examining the role of
neuromodulators, like octopamine, in learning, and gives insight into critical
interactions between sparsity, Hebbian growth, and stimulation during learning.
Our simulations also inform predictions about structural details of the
olfactory system that are not currently well-characterized. From a machine
learning perspective, the model yields bio-inspired mechanisms that are
potentially useful in constructing neural nets for rapid learning from very few
samples. These mechanisms include high-noise layers, sparse layers as noise
filters, and a biologically-plausible optimization method to train the network
based on octopamine stimulation, sparse layers, and Hebbian growth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Delahunt_C/0/1/0/all/0/1&quot;&gt;Charles B. Delahunt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Riffell_J/0/1/0/all/0/1&quot;&gt;Jeffrey A. Riffell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kutz_J/0/1/0/all/0/1&quot;&gt;J. Nathan Kutz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02844">
<title>Using a reservoir computer to learn chaotic attractors, with applications to chaos synchronisation and cryptography. (arXiv:1802.02844v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.02844</link>
<description rdf:parseType="Literal">&lt;p&gt;Using the machine learning approach known as reservoir computing, it is
possible to train one dynamical system to emulate another. We show that such
trained reservoir computers reproduce the properties of the attractor of the
chaotic system sufficiently well to exhibit chaos synchronisation. That is, the
trained reservoir computer, weakly driven by the chaotic system, will
synchronise with the chaotic system. Conversely, the chaotic system, weakly
driven by a trained reservoir computer, will synchronise with the reservoir
computer. We illustrate this behaviour on the Mackey-Glass and Lorenz systems.
We then show that trained reservoir computers can be used to crack chaos based
cryptography and illustrate this on a chaos cryptosystem based on the
Mackey-Glass system. We conclude by discussing why reservoir computers are so
good at emulating chaotic systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antonik_P/0/1/0/all/0/1&quot;&gt;Piotr Antonik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulina_M/0/1/0/all/0/1&quot;&gt;Marvyn Gulina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pauwels_J/0/1/0/all/0/1&quot;&gt;Ja&amp;#xeb;l Pauwels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Massar_S/0/1/0/all/0/1&quot;&gt;Serge Massar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02987">
<title>A Generalization Method of Partitioned Activation Function for Complex Number. (arXiv:1802.02987v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1802.02987</link>
<description rdf:parseType="Literal">&lt;p&gt;A method to convert real number partitioned activation function into complex
number one is provided. The method has 4em variations; 1 has potential to get
holomorphic activation, 2 has potential to conserve complex angle, and the last
1 guarantees interaction between real and imaginary parts. The method has been
applied to LReLU and SELU as examples. The complex number activation function
is an building block of complex number ANN, which has potential to properly
deal with complex number problems. But the complex activation is not well
established yet. Therefore, we propose a way to extend the partitioned real
activation to complex number.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;HyeonSeok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Hyo Seon Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02669">
<title>PPFNet: Global Context Aware Local Features for Robust 3D Point Matching. (arXiv:1802.02669v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1802.02669</link>
<description rdf:parseType="Literal">&lt;p&gt;We present PPFNet - Point Pair Feature NETwork for deeply learning a globally
informed 3D local feature descriptor to find correspondences in unorganized
point clouds. PPFNet learns local descriptors on pure geometry and is highly
aware of the global context, an important cue in deep learning. Our 3D
representation is computed as a collection of point-pair-features combined with
the points and normals within a local vicinity. Our permutation invariant
network design is inspired by PointNet and sets PPFNet to be ordering-free. As
opposed to voxelization, our method is able to consume raw point clouds to
exploit the full sparsity. PPFNet uses a novel \textit{N-tuple} loss and
architecture injecting the global information naturally into the local
descriptor. It shows that context awareness also boosts the local feature
representation. Qualitative and quantitative evaluations of our network suggest
increased recall, improved robustness and invariance as well as a vital step in
the 3D descriptor extraction performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Haowen Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1&quot;&gt;Tolga Birdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1&quot;&gt;Slobodan Ilic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02892">
<title>Efficient Large-Scale Multi-Modal Classification. (arXiv:1802.02892v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1802.02892</link>
<description rdf:parseType="Literal">&lt;p&gt;While the incipient internet was largely text-based, the modern digital world
is becoming increasingly multi-modal. Here, we examine multi-modal
classification where one modality is discrete, e.g. text, and the other is
continuous, e.g. visual representations transferred from a convolutional neural
network. In particular, we focus on scenarios where we have to be able to
classify large quantities of data quickly. We investigate various methods for
performing multi-modal fusion and analyze their trade-offs in terms of
classification accuracy and computational efficiency. Our findings indicate
that the inclusion of continuous information improves performance over
text-only on a range of multi-modal classification tasks, even with simple
fusion methods. In addition, we experiment with discretizing the continuous
features in order to speed up and simplify the fusion process even further. Our
results show that fusion with discretized features outperforms text-only
classification, at a fraction of the computational cost of full multi-modal
fusion, with the additional benefit of improved interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1&quot;&gt;D. Kiela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grave_E/0/1/0/all/0/1&quot;&gt;E. Grave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1&quot;&gt;A. Joulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikolov_T/0/1/0/all/0/1&quot;&gt;T. Mikolov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1707.05101">
<title>On consistency of optimal pricing algorithms in repeated posted-price auctions with strategic buyer. (arXiv:1707.05101v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/1707.05101</link>
<description rdf:parseType="Literal">&lt;p&gt;We study revenue optimization learning algorithms for repeated posted-price
auctions where a seller interacts with a single strategic buyer that holds a
fixed private valuation for a good and seeks to maximize his cumulative
discounted surplus. For this setting, first, we propose a novel algorithm that
never decreases offered prices and has a tight strategic regret bound in
$\Theta(\log\log T)$ under some mild assumptions on the buyer surplus
discounting. This result closes the open research question on the existence of
a no-regret horizon-independent weakly consistent pricing. The proposed
algorithm is inspired by our observation that a double decrease of offered
prices in a weakly consistent algorithm is enough to cause a linear regret.
This motivates us to construct a novel transformation that maps a
right-consistent algorithm to a weakly consistent one that never decreases
offered prices.
&lt;/p&gt;
&lt;p&gt;Second, we outperform the previously known strategic regret upper bound of
the algorithm PRRFES, where the improvement is achieved by means of a finer
constant factor $C$ of the principal term $C\log\log T$ in this upper bound.
Finally, we generalize results on strategic regret previously known for
geometric discounting of the buyer&apos;s surplus to discounting of other types,
namely: the optimality of the pricing PRRFES to the case of geometrically
concave decreasing discounting; and linear lower bound on the strategic regret
of a wide range of horizon-independent weakly consistent algorithms to the case
of arbitrary discounts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drutsa_A/0/1/0/all/0/1&quot;&gt;Alexey Drutsa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.01867">
<title>An Information-Theoretic Optimality Principle for Deep Reinforcement Learning. (arXiv:1708.01867v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1708.01867</link>
<description rdf:parseType="Literal">&lt;p&gt;We methodologically address the problem of Q-value overestimation in deep
reinforcement learning to handle high-dimensional state spaces efficiently. By
adapting concepts from information theory, we introduce an intrinsic penalty
signal encouraging reduced Q-value estimates. The resultant algorithm
encompasses a wide range of learning outcomes containing deep Q-networks as a
special case. Different learning outcomes can be demonstrated by tuning a
Lagrange multiplier accordingly. We furthermore propose a novel scheduling
scheme for this Lagrange multiplier to ensure efficient and robust learning. In
experiments on Atari games, our algorithm outperforms other algorithms (e.g.
deep and double deep Q-networks) in terms of both game-play performance and
sample complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leibfried_F/0/1/0/all/0/1&quot;&gt;Felix Leibfried&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grau_Moya_J/0/1/0/all/0/1&quot;&gt;Jordi Grau-Moya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bou_Ammar_H/0/1/0/all/0/1&quot;&gt;Haitham Bou-Ammar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06800">
<title>Scalable Relaxations of Sparse Packing Constraints: Optimal Biocontrol in Predator-Prey Network. (arXiv:1711.06800v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06800</link>
<description rdf:parseType="Literal">&lt;p&gt;Cascades represent rapid changes in networks. A cascading phenomenon of
ecological and economic impact is the spread of invasive species in geographic
landscapes. The most promising management strategy is often biocontrol, which
entails introducing a natural predator able to control the invading population,
a setting that can be treated as two interacting cascades of predator and prey
populations. We formulate and study a nonlinear problem of optimal biocontrol:
optimally seeding the predator cascade over time to minimize the harmful prey
population. Recurring budgets, which typically face conservation organizations,
naturally leads to sparse constraints which make the problem amenable to
approximation algorithms. Available methods based on continuous relaxations
scale poorly, to remedy this we develop a novel and scalable randomized
algorithm based on a width relaxation, applicable to a broad class of
combinatorial optimization problems. We evaluate our contributions in the
context of biocontrol for the insect pest Hemlock Wolly Adelgid (HWA) in
eastern North America. Our algorithm outperforms competing methods in terms of
scalability and solution quality, and finds near optimal strategies for the
control of the HWA for fine-grained networks -- an important problem in
computational sustainability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bjorck_J/0/1/0/all/0/1&quot;&gt;Johan Bjorck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yiwei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaojian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yexiang Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitmore_M/0/1/0/all/0/1&quot;&gt;Mark C. Whitmore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1&quot;&gt;Carla Gomes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.00345">
<title>Users Constraints in Itemset Mining. (arXiv:1801.00345v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1801.00345</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering significant itemsets is one of the fundamental problems in data
mining. It has recently been shown that constraint programming is a flexible
way to tackle data mining tasks. With a constraint programming approach, we can
easily express and efficiently answer queries with users constraints on items.
However, in many practical cases it is possible that queries also express users
constraints on the dataset itself. For instance, asking for a particular
itemset in a particular part of the dataset. This paper presents a general
constraint programming model able to handle any kind of query on the items or
the dataset for itemset mining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bessiere_C/0/1/0/all/0/1&quot;&gt;Christian Bessiere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazaar_N/0/1/0/all/0/1&quot;&gt;Nadjib Lazaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lebbah_Y/0/1/0/all/0/1&quot;&gt;Yahia Lebbah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maamar_M/0/1/0/all/0/1&quot;&gt;Mehdi Maamar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02534">
<title>FixaTons: A collection of Human Fixations Datasets and Metrics for Scanpath Similarity. (arXiv:1802.02534v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02534</link>
<description rdf:parseType="Literal">&lt;p&gt;In the last three decades, human visual attention has been a topic of great
interest in various disciplines. In computer vision, many models have been
proposed to predict the distribution of human fixations on a visual input.
Recently, thanks to the creation of large collections of data, machine learning
algorithms have obtained state-of-the-art performance on the task of saliency
map estimation. On the other hand, computational models of scanpath are much
less studied. Works are often only descriptive or task specific. Computational
models of scanpath with general purpose are present in the literature, but are
then evaluated in tasks of saliency prediction, losing therefore information
about the dynamics and the behaviour. This is due to the fact that the scanpath
is harder to model because it must include the description of a dynamic. In
addition to the difficulty of the problem itself, two technical reasons have
limited the research. The first reason is the lack of robust and uniformly used
set of metrics to compare the similarity between scanpath. The second reason is
the lack of sufficiently large and varied scanpath datasets. In this report we
want to help in both directions. We present FixaTons, a large collection of
human scanpaths (and saliency maps). It comes along with a software library for
easy data usage, statistics calculation and measures for scanpaths (and
saliency maps) similarity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1&quot;&gt;Dario Zanca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serchi_V/0/1/0/all/0/1&quot;&gt;Valeria Serchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piu_P/0/1/0/all/0/1&quot;&gt;Pietro Piu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosini_F/0/1/0/all/0/1&quot;&gt;Francesca Rosini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rufa_A/0/1/0/all/0/1&quot;&gt;Alessandra Rufa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02840">
<title>Neural Network Renormalization Group. (arXiv:1802.02840v1 [cond-mat.stat-mech])</title>
<link>http://arxiv.org/abs/1802.02840</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a variational renormalization group approach using deep generative
model composed of bijectors. The model can learn hierarchical transformations
between physical variables and renormalized collective variables. It can
directly generate statistically independent physical configurations by
iterative refinement at various length scales. The generative model has an
exact and tractable likelihood, which provides renormalized energy function of
the collective variables and supports unbiased rejection sampling of the
physical variables. To train the neural network, we employ probability density
distillation, in which the training loss is a variational upper bound of the
physical free energy. The approach could be useful for automatically
identifying collective variables and effective field theories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuo-Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02852">
<title>mGPfusion: Predicting protein stability changes with Gaussian process kernel learning and data fusion. (arXiv:1802.02852v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.02852</link>
<description rdf:parseType="Literal">&lt;p&gt;Proteins are commonly used by biochemical industry for numerous processes.
Refining these proteins&apos; properties via mutations causes stability effects as
well. Accurate computational method to predict how mutations affect protein
stability are necessary to facilitate efficient protein design. However,
accuracy of predictive models is ultimately constrained by the limited
availability of experimental data. We have developed mGPfusion, a novel
Gaussian process (GP) method for predicting protein&apos;s stability changes upon
single and multiple mutations. This method complements the limited experimental
data with large amounts of molecular simulation data. We introduce a Bayesian
data fusion model that re-calibrates the experimental and in silico data
sources and then learns a predictive GP model from the combined data. Our
protein-specific model requires experimental data only regarding the protein of
interest, and performs well even with few experimental measurements. The
mGPfusion models proteins by contact maps and infers the stability effects
caused by mutations with a mixture of graph kernels. Our results show that
mGPfusion outperforms state-of-the-art methods in predicting protein stability
on a dataset of 15 different proteins and that incorporating molecular
simulation data improves the model learning and prediction accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jokinen_E/0/1/0/all/0/1&quot;&gt;Emmi Jokinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Heinonen_M/0/1/0/all/0/1&quot;&gt;Markus Heinonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lahdesmaki_H/0/1/0/all/0/1&quot;&gt;Harri L&amp;#xe4;hdesm&amp;#xe4;ki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02856">
<title>Multivariate Study of the Star Formation Rate in Galaxies: Bimodality Revisited. (arXiv:1802.02856v1 [astro-ph.CO])</title>
<link>http://arxiv.org/abs/1802.02856</link>
<description rdf:parseType="Literal">&lt;p&gt;Subjective classification of galaxies can mislead us in the quest of the
origin regarding formation and evolution of galaxies. Multivariate analyses are
the best tools used for such kind of purpose to better understand the
differences between various objects, in an objective manner. In the present
study an objective classification of 362~923 galaxies of the Value Added Galaxy
Catalogue (VAGC) is carried out with the help of three methods of multivariate
analysis. First, independent component analysis (ICA) is used to determine a
set of derived independent variables that are linear combinations of various
observed parameters (viz. ionized lines, Lick indices, photometric and
morphological parameters, star formation rates etc.) of the galaxies.
Subsequently, K-means cluster analysis (CA) is applied on the independent
components to find the optimum number of homogeneous groups. Finally, a
stepwise multiple regression is carried out on each group to predict and study
the star formation rate as a function of other independent observables. The
properties of the ten groups thus uncovered, are used to explain their
formation and evolution mechanisms. It is suggested that three groups are young
and metal poor, belonging to the blue sequence, three others are old and metal
rich (red sequence). The remaining four groups of intermediate ages cannot be
classified in this bimodal sequence: two belong to a pronounced mixture of
early and late type galaxies whereas the other two mostly contain old early
type galaxies. The above result is indicative of a continuous evolutionary
scenario of galaxies instead of two discrete modes, blue and red, so far
suggested by previous authors. Some of our groups occupy the transition region
with different quenching mechanisms. This establishes the elegance of a
multivariate analysis giving rise to a sophisticated refinement over subjective
inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Chattopadhyay_T/0/1/0/all/0/1&quot;&gt;Tanuka Chattopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Fraix_Burnet_D/0/1/0/all/0/1&quot;&gt;Didier Fraix-Burnet&lt;/a&gt; (IPAG), &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Mondal_S/0/1/0/all/0/1&quot;&gt;Saptarshi Mondal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02885">
<title>Online Decomposition of Compressive Streaming Data Using $n$-$\ell_1$ Cluster-Weighted Minimization. (arXiv:1802.02885v1 [cs.IT])</title>
<link>http://arxiv.org/abs/1802.02885</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a decomposition method for compressive streaming data in the
context of online compressive Robust Principle Component Analysis (RPCA). The
proposed decomposition solves an $n$-$\ell_1$ cluster-weighted minimization to
decompose a sequence of frames (or vectors), into sparse and low-rank
components, from compressive measurements. Our method processes a data vector
of the stream per time instance from a small number of measurements in contrast
to conventional batch RPCA, which needs to access full data. The $n$-$\ell_1$
cluster-weighted minimization leverages the sparse components along with their
correlations with multiple previously-recovered sparse vectors. Moreover, the
proposed minimization can exploit the structures of sparse components via
clustering and re-weighting iteratively. The method outperforms the existing
methods for both numerical data and actual video data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luong_H/0/1/0/all/0/1&quot;&gt;Huynh Van Luong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1&quot;&gt;Nikos Deligiannis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forchhammer_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf8;ren Forchhammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaup_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Kaup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02907">
<title>A Game-Theoretic Approach to Design Secure and Resilient Distributed Support Vector Machines. (arXiv:1802.02907v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.02907</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed Support Vector Machines (DSVM) have been developed to solve
large-scale classification problems in networked systems with a large number of
sensors and control units. However, the systems become more vulnerable as
detection and defense are increasingly difficult and expensive. This work aims
to develop secure and resilient DSVM algorithms under adversarial environments
in which an attacker can manipulate the training data to achieve his objective.
We establish a game-theoretic framework to capture the conflicting interests
between an adversary and a set of distributed data processing units. The Nash
equilibrium of the game allows predicting the outcome of learning algorithms in
adversarial environments, and enhancing the resilience of the machine learning
through dynamic distributed learning algorithms. We prove that the convergence
of the distributed algorithm is guaranteed without assumptions on the training
data or network topologies. Numerical experiments are conducted to corroborate
the results. We show that network topology plays an important role in the
security of DSVM. Networks with fewer nodes and higher average degrees are more
secure. Moreover, a balanced network is found to be less vulnerable to attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Quanyan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02920">
<title>State Compression of Markov Processes via Empirical Low-Rank Estimation. (arXiv:1802.02920v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.02920</link>
<description rdf:parseType="Literal">&lt;p&gt;Model reduction is a central problem in analyzing complex systems and
high-dimensional data. We study the state compression of finite-state Markov
process from its empirical trajectories. We adopt a low-rank model which is
motivated by the state aggregation of controlled systems. A spectral method is
proposed for estimating the frequency and transition matrices, estimating the
compressed state spaces, and recovering the state aggregation structure if
there is any. We provide upper bounds for the estimation and recovery errors
and matching minimax lower bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03001">
<title>Statistical Learnability of Generalized Additive Models based on Total Variation Regularization. (arXiv:1802.03001v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1802.03001</link>
<description rdf:parseType="Literal">&lt;p&gt;A generalized additive model (GAM, Hastie and Tibshirani (1987)) is a
nonparametric model by the sum of univariate functions with respect to each
explanatory variable, i.e., $f({\mathbf x}) = \sum f_j(x_j)$, where
$x_j\in\mathbb{R}$ is $j$-th component of a sample ${\mathbf x}\in
\mathbb{R}^p$. In this paper, we introduce the total variation (TV) of a
function as a measure of the complexity of functions in $L^1_{\rm
c}(\mathbb{R})$-space. Our analysis shows that a GAM based on TV-regularization
exhibits a Rademacher complexity of $O(\sqrt{\frac{\log p}{m}})$, which is
tight in terms of both $m$ and $p$ in the agnostic case of the classification
problem. In result, we obtain generalization error bounds for finite samples
according to work by Bartlett and Mandelson (2002).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Matsushima_S/0/1/0/all/0/1&quot;&gt;Shin Matsushima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1610.01857">
<title>Go With the Flow, on Jupiter and Snow. Coherence From Model-Free Video Data without Trajectories. (arXiv:1610.01857v3 [physics.data-an] UPDATED)</title>
<link>http://arxiv.org/abs/1610.01857</link>
<description rdf:parseType="Literal">&lt;p&gt;Viewing a data set such as the clouds of Jupiter, coherence is readily
apparent to human observers, especially the Great Red Spot, but also other
great storms and persistent structures. There are now many different
definitions and perspectives mathematically describing coherent structures, but
we will take an image processing perspective here. We describe an image
processing perspective inference of coherent sets from a fluidic system
directly from image data, without attempting to first model underlying flow
fields, related to a concept in image processing called motion tracking. In
contrast to standard spectral methods for image processing which are generally
related to a symmetric affinity matrix, leading to standard spectral graph
theory, we need a not symmetric affinity which arises naturally from the
underlying arrow of time. We develop an anisotropic, directed diffusion
operator corresponding to flow on a directed graph, from a directed affinity
matrix developed with coherence in mind, and corresponding spectral graph
theory from the graph Laplacian. Our methodology is not offered as more
accurate than other traditional methods of finding coherent sets, but rather
our approach works with alternative kinds of data sets, in the absence of
vector field. Our examples will include partitioning the weather and cloud
structures of Jupiter, and a local to Potsdam, N.Y. lake-effect snow event on
Earth, as well as the benchmark test double-gyre system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+AlMomani_A/0/1/0/all/0/1&quot;&gt;Abd AlRahman AlMomani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bollt_E/0/1/0/all/0/1&quot;&gt;Erik M. Bollt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1612.00374">
<title>Spatial Decompositions for Large Scale SVMs. (arXiv:1612.00374v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1612.00374</link>
<description rdf:parseType="Literal">&lt;p&gt;Although support vector machines (SVMs) are theoretically well understood,
their underlying optimization problem becomes very expensive, if, for example,
hundreds of thousands of samples and a non-linear kernel are considered.
Several approaches have been proposed in the past to address this serious
limitation. In this work we investigate a decomposition strategy that learns on
small, spatially defined data chunks. Our contributions are two fold: On the
theoretical side we establish an oracle inequality for the overall learning
method using the hinge loss, and show that the resulting rates match those
known for SVMs solving the complete optimization problem with Gaussian kernels.
On the practical side we compare our approach to learning SVMs on small,
randomly chosen chunks. Here it turns out that for comparable training times
our approach is significantly faster during testing and also reduces the test
error in most cases significantly. Furthermore, we show that our approach
easily scales up to 10 million training samples: including hyper-parameter
selection using cross validation, the entire training only takes a few hours on
a single machine. Finally, we report an experiment on 32 million training
samples. All experiments used liquidSVM (Steinwart and Thomann, 2017).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thomann_P/0/1/0/all/0/1&quot;&gt;Philipp Thomann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blaschzyk_I/0/1/0/all/0/1&quot;&gt;Ingrid Blaschzyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meister_M/0/1/0/all/0/1&quot;&gt;Mona Meister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Steinwart_I/0/1/0/all/0/1&quot;&gt;Ingo Steinwart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.02971">
<title>Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec. (arXiv:1710.02971v4 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/1710.02971</link>
<description rdf:parseType="Literal">&lt;p&gt;Since the invention of word2vec, the skip-gram model has significantly
advanced the research of network embedding, such as the recent emergence of the
DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of
the aforementioned models with negative sampling can be unified into the matrix
factorization framework with closed forms. Our analysis and proofs reveal that:
(1) DeepWalk empirically produces a low-rank transformation of a network&apos;s
normalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk
when the size of vertices&apos; context is set to one; (3) As an extension of LINE,
PTE can be viewed as the joint factorization of multiple networks&apos; Laplacians;
(4) node2vec is factorizing a matrix related to the stationary distribution and
transition probability tensor of a 2nd-order random walk. We further provide
the theoretical connections between skip-gram based network embedding
algorithms and the theory of graph Laplacian. Finally, we present the NetMF
method as well as its approximation algorithm for computing network embedding.
Our method offers significant improvements over DeepWalk and LINE for
conventional network mining tasks. This work lays the theoretical foundation
for skip-gram based network embedding methods, leading to a better
understanding of latent network representation learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jiezhong Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kuansan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jie Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.03266">
<title>$\alpha$-Variational Inference with Statistical Guarantees. (arXiv:1710.03266v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1710.03266</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a family of variational approximations to Bayesian posterior
distributions, called $\alpha$-VB, with provable statistical guarantees. The
standard variational approximation is a special case of $\alpha$-VB with
$\alpha=1$. When $\alpha \in(0,1]$, a novel class of variational inequalities
are developed for linking the Bayes risk under the variational approximation to
the objective function in the variational optimization problem, implying that
maximizing the evidence lower bound in variational inference has the effect of
minimizing the Bayes risk within the variational density family. Operating in a
frequentist setup, the variational inequalities imply that point estimates
constructed from the $\alpha$-VB procedure converge at an optimal rate to the
true parameter in a wide range of problems. We illustrate our general theory
with a number of examples, including the mean-field variational approximation
to (low)-high-dimensional Bayesian linear regression with spike and slab
priors, mixture of Gaussian models, latent Dirichlet allocation, and (mixture
of) Gaussian variational approximation in regular parametric models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pati_D/0/1/0/all/0/1&quot;&gt;Debdeep Pati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bhattacharya_A/0/1/0/all/0/1&quot;&gt;Anirban Bhattacharya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.01204">
<title>Metrics for Deep Generative Models. (arXiv:1711.01204v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.01204</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural samplers such as variational autoencoders (VAEs) or generative
adversarial networks (GANs) approximate distributions by transforming samples
from a simple random source---the latent space---to samples from a more complex
distribution represented by a dataset. While the manifold hypothesis implies
that the density induced by a dataset contains large regions of low density,
the training criterions of VAEs and GANs will make the latent space densely
covered. Consequently points that are separated by low-density regions in
observation space will be pushed together in latent space, making stationary
distances poor proxies for similarity. We transfer ideas from Riemannian
geometry to this setting, letting the distance between two points be the
shortest path on a Riemannian manifold induced by the transformation. The
method yields a principled distance measure, provides a tool for visual
inspection of deep generative models, and an alternative to linear
interpolation in latent space. In addition, it can be applied for robot
movement generalization using previously learned skills. The method is
evaluated on a synthetic dataset with known ground truth; on a simulated robot
arm dataset; on human motion capture data; and on a generative model of
handwritten digits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Nutan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klushyn_A/0/1/0/all/0/1&quot;&gt;Alexej Klushyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kurle_R/0/1/0/all/0/1&quot;&gt;Richard Kurle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xueyan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bayer_J/0/1/0/all/0/1&quot;&gt;Justin Bayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smagt_P/0/1/0/all/0/1&quot;&gt;Patrick van der Smagt&lt;/a&gt;</dc:creator>
</item></rdf:RDF>