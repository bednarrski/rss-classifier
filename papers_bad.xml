<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-26T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09760"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10038"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10275"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.06525"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.03916"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.08711"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09836"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09886"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09936"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09962"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09991"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10029"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10096"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10104"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10110"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10119"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1611.05497"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10519"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06588"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06137"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06445"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06760"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03341"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09244"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09761"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09813"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09821"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09834"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09865"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09901"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09967"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09979"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10025"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10165"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10166"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10173"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10211"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10225"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.10262"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1309.7857"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.05917"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.05932"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.05482"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06576"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.05017"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.07207"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.09386"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.09760">
<title>Method for Hybrid Precision Convolutional Neural Network Representation. (arXiv:1807.09760v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.09760</link>
<description rdf:parseType="Literal">&lt;p&gt;This invention addresses fixed-point representations of convolutional neural
networks (CNN) in integrated circuits. When quantizing a CNN for a practical
implementation there is a trade-off between the precision used for operations
between coefficients and data and the accuracy of the system. A homogenous
representation may not be sufficient to achieve the best level of performance
at a reasonable cost in implementation complexity or power consumption.
Parsimonious ways of representing data and coefficients are needed to improve
power efficiency and throughput while maintaining accuracy of a CNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Hami_M/0/1/0/all/0/1&quot;&gt;Mo&amp;#x27;taz Al-Hami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pietron_M/0/1/0/all/0/1&quot;&gt;Marcin Pietron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1&quot;&gt;Rishi Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_R/0/1/0/all/0/1&quot;&gt;Raul A. Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hijazi_S/0/1/0/all/0/1&quot;&gt;Samer L. Hijazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rowen_C/0/1/0/all/0/1&quot;&gt;Chris Rowen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10038">
<title>Level-Based Analysis of the Univariate Marginal Distribution Algorithm. (arXiv:1807.10038v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.10038</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimation of Distribution Algorithms (EDAs) are stochastic heuristics that
search for optimal solutions by learning and sampling from probabilistic
models. Despite their popularity in real-world applications, there is little
rigorous understanding of their performance. Even for the Univariate Marginal
Distribution Algorithm (UMDA) -- a simple population-based EDA assuming
independence between decision variables -- the optimisation time on the linear
problem OneMax was until recently undetermined. The incomplete theoretical
understanding of EDAs is mainly due to lack of appropriate analytical tools.
&lt;/p&gt;
&lt;p&gt;We show that the recently developed level-based theorem for non-elitist
populations combined with anti-concentration results yield upper bounds on the
expected optimisation time of the UMDA. This approach results in the bound
$\mathcal{O}(n\lambda\log \lambda+n^2)$ on two problems, LeadingOnes and
BinVal, for population sizes $\lambda&amp;gt;\mu=\Omega(\log n)$, where $\mu$ and
$\lambda$ are parameters of the algorithm. We also prove that the UMDA with
population sizes $\mu\in \mathcal{O}(\sqrt{n}) \cap \Omega(\log n)$ optimises
OneMax in expected time $\mathcal{O}(\lambda n)$, and for larger population
sizes $\mu=\Omega(\sqrt{n}\log n)$, in expected time
$\mathcal{O}(\lambda\sqrt{n})$. The facility and generality of our arguments
suggest that this is a promising approach to derive bounds on the expected
optimisation time of EDAs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_D/0/1/0/all/0/1&quot;&gt;Duc-Cuong Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehre_P/0/1/0/all/0/1&quot;&gt;Per Kristian Lehre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phan Trung Hai Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10275">
<title>A Many-Objective Evolutionary Algorithm Based on Decomposition and Local Dominance. (arXiv:1807.10275v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.10275</link>
<description rdf:parseType="Literal">&lt;p&gt;Many-objective evolutionary algorithms (MOEAs), especially the
decomposition-based MOEAs, have attracted wide attention in recent years.
Recent studies show that a well designed combination of the decomposition
method and the domination method can improve the performance ,i.e., convergence
and diversity, of a MOEA. In this paper, a novel way of combining the
decomposition method and the domination method is proposed. More precisely, a
set of weight vectors is employed to decompose a given many-objective
optimization problem(MaOP), and a hybrid method of the penalty-based boundary
intersection function and dominance is proposed to compare local solutions
within a subpopulation defined by a weight vector. A MOEA based on the hybrid
method is implemented and tested on problems chosen from two famous test
suites, i.e., DTLZ and WFG. The experimental results show that our algorithm is
very competitive in dealing with MaOPs. Subsequently, our algorithm is extended
to solve constraint MaOPs, and the constrained version of our algorithm also
shows good performance in terms of convergence and diversity. These reveals
that using dominance locally and combining it with the decomposition method can
effectively improve the performance of a MOEA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.06525">
<title>Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection. (arXiv:1708.06525v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1708.06525</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of cross-platform binary code similarity detection aims at
detecting whether two binary functions coming from different platforms are
similar or not. It has many security applications, including plagiarism
detection, malware detection, vulnerability search, etc. Existing approaches
rely on approximate graph matching algorithms, which are inevitably slow and
sometimes inaccurate, and hard to adapt to a new task. To address these issues,
in this work, we propose a novel neural network-based approach to compute the
embedding, i.e., a numeric vector, based on the control flow graph of each
binary function, then the similarity detection can be done efficiently by
measuring the distance between the embeddings for two functions. We implement a
prototype called Gemini. Our extensive evaluation shows that Gemini outperforms
the state-of-the-art approaches by large margins with respect to similarity
detection accuracy. Further, Gemini can speed up prior art&apos;s embedding
generation time by 3 to 4 orders of magnitude and reduce the required training
time from more than 1 week down to 30 minutes to 10 hours. Our real world case
studies demonstrate that Gemini can identify significantly more vulnerable
firmware images than the state-of-the-art, i.e., Genius. Our research showcases
a successful application of deep learning on computer security problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaojun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1&quot;&gt;Qian Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Heng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Le Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawn Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.03916">
<title>Detecting and Correcting for Label Shift with Black Box Predictors. (arXiv:1802.03916v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.03916</link>
<description rdf:parseType="Literal">&lt;p&gt;Faced with distribution shift between training and test set, we wish to
detect and quantify the shift, and to correct our classifiers without test set
labels. Motivated by medical diagnosis, where diseases (targets) cause symptoms
(observations), we focus on label shift, where the label marginal $p(y)$
changes but the conditional $p(x| y)$ does not. We propose Black Box Shift
Estimation (BBSE) to estimate the test distribution $p(y)$. BBSE exploits
arbitrary black box predictors to reduce dimensionality prior to shift
correction. While better predictors give tighter estimates, BBSE works even
when predictors are biased, inaccurate, or uncalibrated, so long as their
confusion matrices are invertible. We prove BBSE&apos;s consistency, bound its
error, and introduce a statistical test that uses BBSE to detect shift. We also
leverage BBSE to correct classifiers. Experiments demonstrate accurate
estimates and improved prediction, even on high-dimensional datasets of natural
images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1&quot;&gt;Alex Smola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.08711">
<title>All-Optical Machine Learning Using Diffractive Deep Neural Networks. (arXiv:1804.08711v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1804.08711</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an all-optical Diffractive Deep Neural Network (D2NN)
architecture that can learn to implement various functions after deep
learning-based design of passive diffractive layers that work collectively. We
experimentally demonstrated the success of this framework by creating
3D-printed D2NNs that learned to implement handwritten digit classification and
the function of an imaging lens at terahertz spectrum. With the existing
plethora of 3D-printing and other lithographic fabrication methods as well as
spatial-light-modulators, this all-optical deep learning framework can perform,
at the speed of light, various complex functions that computer-based neural
networks can implement, and will find applications in all-optical image
analysis, feature detection and object classification, also enabling new camera
designs and optical components that can learn to perform unique tasks using
D2NNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xing Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivenson_Y/0/1/0/all/0/1&quot;&gt;Yair Rivenson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yardimci_N/0/1/0/all/0/1&quot;&gt;Nezih T. Yardimci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veli_M/0/1/0/all/0/1&quot;&gt;Muhammed Veli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jarrahi_M/0/1/0/all/0/1&quot;&gt;Mona Jarrahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozcan_A/0/1/0/all/0/1&quot;&gt;Aydogan Ozcan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09836">
<title>Robustness to fundamental uncertainty in AGI alignment. (arXiv:1807.09836v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.09836</link>
<description rdf:parseType="Literal">&lt;p&gt;The AGI alignment problem has a bimodal distribution of outcomes with most
outcomes clustering around the poles of total success and existential,
catastrophic failure. Consequently, attempts to solve AGI alignment should, all
else equal, prefer false negatives (ignoring research programs that would have
been successful) to false positives (pursuing research programs that will
unexpectedly fail). Thus, we propose adopting a policy of responding to points
of metaphysical and practical uncertainty associated with the alignment problem
by limiting and choosing necessary assumptions to reduce the risk false
positives. Herein we explore in detail some of the relevant points of
uncertainty that AGI alignment research hinges on and consider how to reduce
false positives in response to them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worley_G/0/1/0/all/0/1&quot;&gt;G Gordon Worley III&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09886">
<title>Evaluating Creativity in Computational Co-Creative Systems. (arXiv:1807.09886v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.09886</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a framework for evaluating creativity in co-creative
systems: those that involve computer programs collaborating with human users on
creative tasks. We situate co-creative systems within a broader context of
computational creativity and explain the unique qualities of these systems. We
present four main questions that can guide evaluation in co-creative systems:
Who is evaluating the creativity, what is being evaluated, when does evaluation
occur and how the evaluation is performed. These questions provide a framework
for comparing how existing co-creative systems evaluate creativity, and we
apply them to examples of co-creative systems in art, humor, games and
robotics. We conclude that existing co-creative systems tend to focus on
evaluating the user experience. Adopting evaluation methods from autonomous
creative systems may lead to co-creative systems that are self-aware and
intentional.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karimi_P/0/1/0/all/0/1&quot;&gt;Pegah Karimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grace_K/0/1/0/all/0/1&quot;&gt;Kazjon Grace&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maher_M/0/1/0/all/0/1&quot;&gt;Mary Lou Maher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_N/0/1/0/all/0/1&quot;&gt;Nicholas Davis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09936">
<title>Multi-Agent Generative Adversarial Imitation Learning. (arXiv:1807.09936v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.09936</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation learning algorithms can be used to learn a policy from expert
demonstrations without access to a reward signal. However, most existing
approaches are not applicable in multi-agent settings due to the existence of
multiple (Nash) equilibria and non-stationary environments. We propose a new
framework for multi-agent imitation learning for general Markov games, where we
build upon a generalized notion of inverse reinforcement learning. We further
introduce a practical multi-agent actor-critic algorithm with good empirical
performance. Our method can be used to imitate complex behaviors in
high-dimensional environments with multiple cooperative or competing agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jiaming Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hongyu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09962">
<title>Learning to guide task and motion planning using score-space representation. (arXiv:1807.09962v1 [cs.RO])</title>
<link>http://arxiv.org/abs/1807.09962</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a learning algorithm that speeds up the search in
task and motion planning problems. Our algorithm proposes solutions to three
different challenges that arise in learning to improve planning efficiency:
what to predict, how to represent a planning problem instance, and how to
transfer knowledge from one problem instance to another. We propose a method
that predicts constraints on the search space based on a generic representation
of a planning problem instance, called score-space, where we represent a
problem instance in terms of the performance of a set of solutions attempted so
far. Using this representation, we transfer knowledge, in the form of
constraints, from previous problems based on the similarity in score space. We
design a sequential algorithm that efficiently predicts these constraints, and
evaluate it in three different challenging task and motion planning problems.
Results indicate that our approach performs orders of magnitudes faster than an
unguided planner
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Beomjoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie Pack Kaelbling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1&quot;&gt;Tomas Lozano-Perez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09991">
<title>Multi-modal Feedback for Affordance-driven Interactive Reinforcement Learning. (arXiv:1807.09991v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.09991</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive reinforcement learning (IRL) extends traditional reinforcement
learning (RL) by allowing an agent to interact with parent-like trainers during
a task. In this paper, we present an IRL approach using dynamic audio-visual
input in terms of vocal commands and hand gestures as feedback. Our
architecture integrates multi-modal information to provide robust commands from
multiple sensory cues along with a confidence value indicating the
trustworthiness of the feedback. The integration process also considers the
case in which the two modalities convey incongruent information. Additionally,
we modulate the influence of sensory-driven feedback in the IRL task using
goal-oriented knowledge in terms of contextual affordances. We implement a
neural network architecture to predict the effect of performed actions with
different objects to avoid failed-states, i.e., states from which it is not
possible to accomplish the task. In our experimental setup, we explore the
interplay of multimodal feedback and task-specific affordances in a robot
cleaning scenario. We compare the learning performance of the agent under four
different conditions: traditional RL, multi-modal IRL, and each of these two
setups with the use of contextual affordances. Our experiments show that the
best performance is obtained by using audio-visual feedback with
affordancemodulated IRL. The obtained results demonstrate the importance of
multi-modal sensory processing integrated with goal-oriented knowledge in IRL
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_F/0/1/0/all/0/1&quot;&gt;Francisco Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parisi_G/0/1/0/all/0/1&quot;&gt;German I. Parisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10029">
<title>LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks. (arXiv:1807.10029v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.10029</link>
<description rdf:parseType="Literal">&lt;p&gt;Although weight and activation quantization is an effective approach for Deep
Neural Network (DNN) compression and has a lot of potentials to increase
inference speed leveraging bit-operations, there is still a noticeable gap in
terms of prediction accuracy between the quantized model and the full-precision
model. To address this gap, we propose to jointly train a quantized,
bit-operation-compatible DNN and its associated quantizers, as opposed to using
fixed, handcrafted quantization schemes such as uniform or logarithmic
quantization. Our method for learning the quantizers applies to both network
weights and activations with arbitrary-bit precision, and our quantizers are
easy to train. The comprehensive experiments on CIFAR-10 and ImageNet datasets
show that our method works consistently well for various network structures
such as AlexNet, VGG-Net, GoogLeNet, ResNet, and DenseNet, surpassing previous
quantization methods in terms of accuracy by an appreciable margin. Code
available at https://github.com/Microsoft/LQ-Nets
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiaolong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1&quot;&gt;Dongqiangzi Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1&quot;&gt;Gang Hua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10096">
<title>Towards a Deep Unified Framework for Nuclear Reactor Perturbation Analysis. (arXiv:1807.10096v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.10096</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes the first step towards a novel unified framework for the
analysis of perturbations occurring in nuclear reactors in both Time and
Frequency domain. The identification of type and source of such perturbations
is fundamental for monitoring core reactors and guarantee safety even while
running at nominal conditions. A 3D Convolutional Neural Network (3D-CNN) was
employed to analyse perturbations happening in the frequency domain, such as
the alteration of an absorber of variable strength or propagating perturbation.
Recurrent neural networks (RNN), specifically Long Short-Term Memory (LSTM) was
used to study signal sequences related to perturbations induced in the time
domain, including the vibrations of fuel assemblies and the fluctuation of
thermalhydraulic parameters at the inlet of the reactor coolant loops.
512-dimensional representations were extracted from the 3D-CNN and LSTM
architectures, and used as input to a fused multi-sigmoid classification layer
to recognise the perturbation type. If the perturbation is frequency domain
related, a separate fully-connected layer utilises said representations to
regress the coordinates of its source. The results showed that perturbation
type can be recognised with high accuracy in both domains, and frequency domain
scenario sources can be localised with high precision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_F/0/1/0/all/0/1&quot;&gt;Fabio De Sousa Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caliva_F/0/1/0/all/0/1&quot;&gt;Francesco Caliva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chionis_D/0/1/0/all/0/1&quot;&gt;Dionysios Chionis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dokhane_A/0/1/0/all/0/1&quot;&gt;Abdelhamid Dokhane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mylonakis_A/0/1/0/all/0/1&quot;&gt;Antonios Mylonakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demaziere_C/0/1/0/all/0/1&quot;&gt;Christophe Demaziere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leontidis_G/0/1/0/all/0/1&quot;&gt;Georgios Leontidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollias_S/0/1/0/all/0/1&quot;&gt;Stefanos Kollias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10104">
<title>Term Set Expansion based on Multi-Context Term Embeddings: an End-to-end Workflow. (arXiv:1807.10104v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.10104</link>
<description rdf:parseType="Literal">&lt;p&gt;We present SetExpander, a corpus-based system for expanding a seed set of
terms into a more complete set of terms that belong to the same semantic class.
SetExpander implements an iterative end-to end workflow for term set expansion.
It enables users to easily select a seed set of terms, expand it, view the
expanded set, validate it, re-expand the validated set and store it, thus
simplifying the extraction of domain-specific fine-grained semantic classes.
SetExpander has been used for solving real-life use cases including integration
in an automated recruitment system and an issues and defects resolution system.
A video demo of SetExpander is available at
https://drive.google.com/open?id=1e545bB87Autsch36DjnJHmq3HWfSd1Rv (some images
were blurred for privacy reasons).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mamou_J/0/1/0/all/0/1&quot;&gt;Jonathan Mamou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pereg_O/0/1/0/all/0/1&quot;&gt;Oren Pereg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasserblat_M/0/1/0/all/0/1&quot;&gt;Moshe Wasserblat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1&quot;&gt;Ido Dagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1&quot;&gt;Yoav Goldberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eirew_A/0/1/0/all/0/1&quot;&gt;Alon Eirew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Green_Y/0/1/0/all/0/1&quot;&gt;Yael Green&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guskin_S/0/1/0/all/0/1&quot;&gt;Shira Guskin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izsak_P/0/1/0/all/0/1&quot;&gt;Peter Izsak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korat_D/0/1/0/all/0/1&quot;&gt;Daniel Korat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10110">
<title>ToriLLE: Learning Environment for Hand-to-Hand Combat. (arXiv:1807.10110v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.10110</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Toribash Learning Environment (ToriLLE), an interface with video
game Toribash for training machine learning agents. Toribash is a MuJoCo-like
environment of two humanoid character fighting each other hand-to-hand,
controlled by changing states of body joints. Competitive nature of Toribash
lends itself to two-agent experiments, and active player-base can be used for
human baselines. This white paper describes the environment with its pros, cons
and limitations as well experimentally show ToriLLE&apos;s applicability as a
learning environment by successfully training reinforcement learning agents
that improved over time. The code is available at
https://github.com/Miffyli/ToriLLE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanervisto_A/0/1/0/all/0/1&quot;&gt;Anssi Kanervisto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hautamaki_V/0/1/0/all/0/1&quot;&gt;Ville Hautam&amp;#xe4;ki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10119">
<title>A Unified Approximation Framework for Non-Linear Deep Neural Networks. (arXiv:1807.10119v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.10119</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have achieved significant success in a variety of
real world applications. However, tons of parameters in the networks restrict
the efficiency of neural networks due to the large model size and the intensive
computation. To address this issue, various compression and acceleration
techniques have been investigated, among which low-rank filters and sparse
filters are heavily studied. In this paper we propose a unified framework to
compress the convolutional neural networks by combining these two strategies,
while taking the nonlinear activation into consideration. The filer of a layer
is approximated by the sum of a sparse component and a low-rank component, both
of which are in favor of model compression. Especially, we constrain the sparse
component to be structured sparse which facilitates acceleration. The
performance of the network is retained by minimizing the reconstruction error
of the feature maps after activation of each layer, using the alternating
direction method of multipliers (ADMM). The experimental results show that our
proposed approach can compress VGG-16 and AlexNet by over 4X. In addition, 2.2X
and 1.1X speedup are achieved on VGG-16 and AlexNet, respectively, at a cost of
less increase on error rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ran Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1&quot;&gt;Fanhua Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenjian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Minsik Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bei Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10268">
<title>Premise selection with neural networks and distributed representation of features. (arXiv:1807.10268v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.10268</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the problem of selecting relevant premises for a proof of a given
statement. When stated as a binary classification task for pairs (conjecture,
axiom), it can be efficiently solved using artificial neural networks. The key
difference between our advance to solve this problem and previous approaches is
the use of just functional signatures of premises. To further improve the
performance of the model, we use dimensionality reduction technique, to replace
long and sparse signature vectors with their compact and dense embedded
versions. These are obtained by firstly defining the concept of a context for
each functor symbol, and then training a simple neural network to predict the
distribution of other functor symbols in the context of this functor. After
training the network, the output of its hidden layer is used to construct a
lower dimensional embedding of a functional signature (for each premise) with a
distributed representation of features. This allows us to use 512-dimensional
embeddings for conjecture-axiom pairs, containing enough information about the
original statements to reach the accuracy of 76.45% in premise selection task,
only with simple two-layer densely connected neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kucik_A/0/1/0/all/0/1&quot;&gt;Andrzej Stanis&amp;#x142;aw Kucik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korovin_K/0/1/0/all/0/1&quot;&gt;Konstantin Korovin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1611.05497">
<title>Explicablility as Minimizing Distance from Expected Behavior. (arXiv:1611.05497v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1611.05497</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to have effective human-AI collaboration, it is necessary to address
how the AI agent&apos;s behavior is being perceived by the humans-in-the-loop. When
the agent&apos;s task plans are generated without such considerations, they may
often demonstrate inexplicable behavior from the human&apos;s point of view. This
problem may arise due to the human&apos;s partial or inaccurate understanding of the
agent&apos;s planning model. This may have serious implications from increased
cognitive load to more serious concerns of safety around a physical agent. In
this paper, we address this issue by modeling plan explicability as a function
of the distance between a plan that agent makes and the plan that human expects
it to make. We learn a regression model for mapping the plan distances to
explicability scores of plans and develop an anytime search algorithm that can
use this model as a heuristic to come up with progressively explicable plans.
We evaluate the effectiveness of our approach in a simulated autonomous car
domain and a physical robot domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1&quot;&gt;Anagha Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1&quot;&gt;Yantian Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborti_T/0/1/0/all/0/1&quot;&gt;Tathagata Chakraborti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vadlamudi_S/0/1/0/all/0/1&quot;&gt;Satya Gautam Vadlamudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1&quot;&gt;Subbarao Kambhampati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10519">
<title>Exploiting Points and Lines in Regression Forests for RGB-D Camera Relocalization. (arXiv:1710.10519v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10519</link>
<description rdf:parseType="Literal">&lt;p&gt;Camera relocalization plays a vital role in many robotics and computer vision
tasks, such as global localization, recovery from tracking failure and loop
closure detection. Recent random forests based methods exploit randomly sampled
pixel comparison features to predict 3D world locations for 2D image locations
to guide the camera pose optimization. However, these image features are only
sampled randomly in the images, without considering the spatial structures or
geometric information, leading to large errors or failure cases with the
existence of poorly textured areas or in motion blur. Line segment features are
more robust in these environments. In this work, we propose to jointly exploit
points and lines within the framework of uncertainty driven regression forests.
The proposed approach is thoroughly evaluated on three publicly available
datasets against several strong state-of-the-art baselines in terms of several
different error metrics. Experimental results prove the efficacy of our method,
showing superior or on-par state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1&quot;&gt;Lili Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tung_F/0/1/0/all/0/1&quot;&gt;Frederick Tung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Little_J/0/1/0/all/0/1&quot;&gt;James J. Little&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valentin_J/0/1/0/all/0/1&quot;&gt;Julien Valentin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1&quot;&gt;Clarence W. de Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06588">
<title>Dependent landmark drift: robust point set registration with a Gaussian mixture model and a statistical shape model. (arXiv:1711.06588v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06588</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of point set registration is to find point-by-point correspondences
between point sets, each of which characterizes the shape of an object. Because
local preservation of object geometry is assumed, prevalent algorithms in the
area can often elegantly solve the problems without using geometric information
specific to the objects. This means that registration performance can be
further improved by using prior knowledge of object geometry. In this paper, we
propose a novel point set registration method using the Gaussian mixture model
with prior shape information encoded as a statistical shape model. Our
transformation model is defined as a combination of the similar transformation,
motion coherence, and the statistical shape model. Therefore, the proposed
method works effectively if the target point set includes outliers and missing
regions, or if it is rotated. The computational cost can be reduced to linear,
and therefore the method is scalable to large point sets. The effectiveness of
the method will be verified through comparisons with existing algorithms using
datasets concerning human body shapes, hands, and faces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirose_O/0/1/0/all/0/1&quot;&gt;Osamu Hirose&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06137">
<title>A Unified Framework for Planning in Adversarial and Cooperative Environments. (arXiv:1802.06137v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06137</link>
<description rdf:parseType="Literal">&lt;p&gt;Users of AI systems may rely upon them to produce plans for achieving desired
objectives. Such AI systems should be able to compute obfuscated plans whose
execution in adversarial situations protects privacy, as well as legible plans
which are easy for team members to understand in cooperative situations. We
develop a unified framework that addresses these dual problems by computing
plans with a desired level of comprehensibility from the point of view of a
partially informed observer. For adversarial settings, our approach produces
obfuscated plans with observations that are consistent with at least k goals
from a set of decoy goals. By slightly varying our framework, we present an
approach for goal legibility in cooperative settings which produces plans that
achieve a goal while being consistent with at most j goals from a set of
confounding goals. In addition, we show how the observability of the observer
can be controlled to either obfuscate or clarify the next actions in a plan
when the goal is known to the observer. We present theoretical results on the
complexity analysis of our problems. We demonstrate the execution of obfuscated
and legible plans in a cooking domain using a physical robot Fetch. We also
provide an empirical evaluation to show the feasibility and usefulness of our
approaches using IPC domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1&quot;&gt;Anagha Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1&quot;&gt;Siddharth Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1&quot;&gt;Subbarao Kambhampati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06445">
<title>Datalog: Bag Semantics via Set Semantics. (arXiv:1803.06445v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06445</link>
<description rdf:parseType="Literal">&lt;p&gt;Duplicates in data management are common and problematic. In this work, we
present a translation of Datalog under bag semantics into a well-behaved
extension of Datalog (the so-called warded Datalog+-) under set semantics. From
a theoretical point of view, this allows us to reason on bag semantics by
making use of the well-established theoretical foundations of set semantics.
From a practical point of view, this allows us to handle the bag semantics of
Datalog by powerful, existing query engines for the required extension of
Datalog. Moreover, this translation has the potential for further extensions --
above all to capture the bag semantics of the semantic web query language
SPARQL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1&quot;&gt;Leopoldo Bertossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottlob_G/0/1/0/all/0/1&quot;&gt;Georg Gottlob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pichler_R/0/1/0/all/0/1&quot;&gt;Reinhard Pichler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06760">
<title>Simulation-based Adversarial Test Generation for Autonomous Vehicles with Machine Learning Components. (arXiv:1804.06760v3 [cs.SY] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06760</link>
<description rdf:parseType="Literal">&lt;p&gt;Many organizations are developing autonomous driving systems, which are
expected to be deployed at a large scale in the near future. Despite this,
there is a lack of agreement on appropriate methods to test, debug, and certify
the performance of these systems. One of the main challenges is that many
autonomous driving systems have machine learning components, such as deep
neural networks, for which formal properties are difficult to characterize. We
present a testing framework that is compatible with test case generation and
automatic falsification methods, which are used to evaluate cyber-physical
systems. We demonstrate how the framework can be used to evaluate closed-loop
properties of an autonomous driving system model that includes the ML
components, all within a virtual environment. We demonstrate how to use test
case generation methods, such as covering arrays, as well as requirement
falsification methods to automatically identify problematic test scenarios. The
resulting framework can be used to increase the reliability of autonomous
driving systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuncali_C/0/1/0/all/0/1&quot;&gt;Cumhur Erkan Tuncali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fainekos_G/0/1/0/all/0/1&quot;&gt;Georgios Fainekos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ito_H/0/1/0/all/0/1&quot;&gt;Hisahiro Ito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapinski_J/0/1/0/all/0/1&quot;&gt;James Kapinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03341">
<title>Troubling Trends in Machine Learning Scholarship. (arXiv:1807.03341v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03341</link>
<description rdf:parseType="Literal">&lt;p&gt;Collectively, machine learning (ML) researchers are engaged in the creation
and dissemination of knowledge about data-driven algorithms. In a given paper,
researchers might aspire to any subset of the following goals, among others: to
theoretically characterize what is learnable, to obtain understanding through
empirically rigorous experiments, or to build a working system that has high
predictive accuracy. While determining which knowledge warrants inquiry may be
subjective, once the topic is fixed, papers are most valuable to the community
when they act in service of the reader, creating foundational knowledge and
communicating as clearly as possible.
&lt;/p&gt;
&lt;p&gt;Recent progress in machine learning comes despite frequent departures from
these ideals. In this paper, we focus on the following four patterns that
appear to us to be trending in ML scholarship: (i) failure to distinguish
between explanation and speculation; (ii) failure to identify the sources of
empirical gains, e.g., emphasizing unnecessary modifications to neural
architectures when gains actually stem from hyper-parameter tuning; (iii)
mathiness: the use of mathematics that obfuscates or impresses rather than
clarifies, e.g., by confusing technical and non-technical concepts; and (iv)
misuse of language, e.g., by choosing terms of art with colloquial connotations
or by overloading established technical terms.
&lt;/p&gt;
&lt;p&gt;While the causes behind these patterns are uncertain, possibilities include
the rapid expansion of the community, the consequent thinness of the reviewer
pool, and the often-misaligned incentives between scholarship and short-term
measures of success (e.g., bibliometrics, attention, and entrepreneurial
opportunity). While each pattern offers a corresponding remedy (don&apos;t do it),
we also discuss some speculative suggestions for how the community might combat
these trends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Steinhardt_J/0/1/0/all/0/1&quot;&gt;Jacob Steinhardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09244">
<title>Unsupervised Learning of Latent Physical Properties Using Perception-Prediction Networks. (arXiv:1807.09244v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.09244</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a framework for the completely unsupervised learning of latent
object properties from their interactions: the perception-prediction network
(PPN). Consisting of a perception module that extracts representations of
latent object properties and a prediction module that uses those extracted
properties to simulate system dynamics, the PPN can be trained in an end-to-end
fashion purely from samples of object dynamics. The representations of latent
object properties learned by PPNs not only are sufficient to accurately
simulate the dynamics of systems comprised of previously unseen objects, but
also can be translated directly into human-interpretable properties (e.g.,
mass, coefficient of restitution) in an entirely unsupervised manner.
Crucially, PPNs also generalize to novel scenarios: their gradient-based
training can be applied to many dynamical systems and their graph-based
structure functions over systems comprised of different numbers of objects. Our
results demonstrate the efficacy of graph-based neural architectures in
object-centric inference and prediction tasks, and our model has the potential
to discover relevant object properties in systems that are not yet well
understood.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;David Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_V/0/1/0/all/0/1&quot;&gt;Vinson Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09761">
<title>Stripe-Based Fragility Analysis of Concrete Bridge Classes Using Machine Learning Techniques. (arXiv:1807.09761v1 [cs.CE])</title>
<link>http://arxiv.org/abs/1807.09761</link>
<description rdf:parseType="Literal">&lt;p&gt;A framework for the generation of bridge-specific fragility utilizing the
capabilities of machine learning and stripe-based approach is presented in this
paper. The proposed methodology using random forests helps to generate or
update fragility curves for a new set of input parameters with less
computational effort and expensive re-simulation. The methodology does not
place any assumptions on the demand model of various components and helps to
identify the relative importance of each uncertain variable in their seismic
demand model. The methodology is demonstrated through the case studies of
multi-span concrete bridges in California. Geometric, material and structural
uncertainties are accounted for in the generation of bridge models and
fragility curves. It is also noted that the traditional lognormality assumption
on the demand model leads to unrealistic fragility estimates. Fragility results
obtained the proposed methodology curves can be deployed in risk assessment
platform such as HAZUS for regional loss estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mangalathu_S/0/1/0/all/0/1&quot;&gt;Sujith Mangalathu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1&quot;&gt;Jong-Su Jeon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09813">
<title>Binacox: automatic cut-points detection in high-dimensional Cox model, with applications to genetic data. (arXiv:1807.09813v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.09813</link>
<description rdf:parseType="Literal">&lt;p&gt;Determining significant prognostic biomarkers is of increasing importance in
many areas of medicine. In order to translate a continuous biomarker into a
clinical decision, it is often necessary to determine cut-points. There is so
far no standard method to help evaluate how many cut-points are optimal for a
given feature in a survival analysis setting. Moreover, most existing methods
are univariate, hence not well suited for high-dimensional frameworks. This
paper introduces a prognostic method called Binacox to deal with the problem of
detecting multiple cut-points per features in a multivariate setting where a
large number of continuous features are available. It is based on the Cox model
and combines one-hot encodings with the binarsity penalty. This penalty uses
total-variation regularization together with an extra linear constraint to
avoid collinearity between the one-hot encodings and enable feature selection.
A non-asymptotic oracle inequality is established. The statistical performance
of the method is then examined on an extensive Monte Carlo simulation study,
and finally illustrated on three publicly available genetic cancer datasets
with high-dimensional features. On this datasets, our proposed methodology
significantly outperforms the state-of-the-art survival models regarding risk
prediction in terms of C-index, with a computing time orders of magnitude
faster. In addition, it provides powerful interpretability by automatically
pinpointing significant cut-points on relevant features from a clinical point
of view.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bussy_S/0/1/0/all/0/1&quot;&gt;Simon Bussy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alaya_M/0/1/0/all/0/1&quot;&gt;Mokhtar Z. Alaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guilloux_A/0/1/0/all/0/1&quot;&gt;Agathe Guilloux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jannot_A/0/1/0/all/0/1&quot;&gt;Anne-Sophie Jannot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09821">
<title>Comparison of methods for early-readmission prediction in a high-dimensional heterogeneous covariates and time-to-event outcome framework. (arXiv:1807.09821v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.09821</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Choosing the most performing method in terms of outcome
prediction or variables selection is a recurring problem in prognosis studies,
leading to many publications on methods comparison. But some aspects have
received little attention. First, most comparison studies treat prediction
performance and variable selection aspects separately. Second, methods are
either compared within a binary outcome setting (based on an arbitrarily chosen
delay) or within a survival setting, but not both. In this paper, we propose a
comparison methodology to weight up those different settings both in terms of
prediction and variables selection, while incorporating advanced machine
learning strategies. Methods: Using a high-dimensional case study on a
sickle-cell disease (SCD) cohort, we compare 8 statistical methods. In the
binary outcome setting, we consider logistic regression (LR), support vector
machine (SVM), random forest (RF), gradient boosting (GB) and neural network
(NN); while on the survival analysis setting, we consider the Cox Proportional
Hazards (PH), the CURE and the C-mix models. We then compare performances of
all methods both in terms of risk prediction and variable selection, with a
focus on the use of Elastic-Net regularization technique. Results: Among all
assessed statistical methods assessed, the C-mix model yields the better
performances in both the two considered settings, as well as interesting
interpretation aspects. There is some consistency in selected covariates across
methods within a setting, but not much across the two settings. Conclusions: It
appears that learning withing the survival setting first, and then going back
to a binary prediction using the survival estimates significantly enhance
binary predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bussy_S/0/1/0/all/0/1&quot;&gt;Simon Bussy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Veil_R/0/1/0/all/0/1&quot;&gt;Rapha&amp;#xeb;l Veil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Looten_V/0/1/0/all/0/1&quot;&gt;Vincent Looten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Burgun_A/0/1/0/all/0/1&quot;&gt;Anita Burgun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gaiffas_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Ga&amp;#xef;ffas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guilloux_A/0/1/0/all/0/1&quot;&gt;Agathe Guilloux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ranque_B/0/1/0/all/0/1&quot;&gt;Brigitte Ranque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jannot_A/0/1/0/all/0/1&quot;&gt;Anne-Sophie Jannot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09834">
<title>Applying Domain Randomization to Synthetic Data for Object Category Detection. (arXiv:1807.09834v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.09834</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep learning-based object detection techniques have
revolutionized their applicability in several fields. However, since these
methods rely on unwieldy and large amounts of data, a common practice is to
download models pre-trained on standard datasets and fine-tune them for
specific application domains with a small set of domain relevant images. In
this work, we show that using synthetic datasets that are not necessarily
photo-realistic can be a better alternative to simply fine-tune pre-trained
networks. Specifically, our results show an impressive 25% improvement in the
mAP metric over a fine-tuning baseline when only about 200 labelled images are
available to train. Finally, an ablation study of our results is presented to
delineate the individual contribution of different components in the
randomization pipeline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borrego_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Borrego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehban_A/0/1/0/all/0/1&quot;&gt;Atabak Dehban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Figueiredo_R/0/1/0/all/0/1&quot;&gt;Rui Figueiredo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1&quot;&gt;Plinio Moreno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernardino_A/0/1/0/all/0/1&quot;&gt;Alexandre Bernardino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_Victor_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Santos-Victor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09865">
<title>Predicting Acute Kidney Injury at Hospital Re-entry Using High-dimensional Electronic Health Record Data. (arXiv:1807.09865v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.09865</link>
<description rdf:parseType="Literal">&lt;p&gt;Acute Kidney Injury (AKI), a sudden decline in kidney function, is associated
with increased mortality, morbidity, length of stay, and hospital cost. Since
AKI is sometimes preventable, there is great interest in prediction. Most
existing studies consider all patients and therefore restrict to features
available in the first hours of hospitalization. Here, the focus is instead on
rehospitalized patients, a cohort in which rich longitudinal features from
prior hospitalizations can be analyzed. Our objective is to provide a risk
score directly at hospital re-entry. Gradient boosting, penalized logistic
regression (with and without stability selection), and a recurrent neural
network are trained on two years of adult inpatient EHR data (3,387 attributes
for 34,505 patients who generated 90,013 training samples with 5,618 cases and
84,395 controls). Predictions are internally evaluated with 50 iterations of
5-fold grouped cross-validation with special emphasis on calibration, an
analysis of which is performed at the patient as well as hospitalization level.
Error is assessed with respect to diagnosis, ethnicity, age, gender, AKI
identification method, and hospital utilization. In an additional experiment,
the regularization penalty is severely increased to induce parsimony and
interpretability. Predictors identified for rehospitalized patients are also
reported with a special analysis of medications that might be modifiable risk
factors. Insights from this study might be used to construct a predictive tool
for AKI in rehospitalized patients. An accurate estimate of AKI risk at
hospital entry might serve as a prior for an admitting provider or another
predictive algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weisenthal_S/0/1/0/all/0/1&quot;&gt;Samuel J. Weisenthal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quill_C/0/1/0/all/0/1&quot;&gt;Caroline Quill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farooq_S/0/1/0/all/0/1&quot;&gt;Samir Farooq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_H/0/1/0/all/0/1&quot;&gt;Henry Kautz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zand_M/0/1/0/all/0/1&quot;&gt;Martin S. Zand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09901">
<title>Neural State Classification for Hybrid Systems. (arXiv:1807.09901v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.09901</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the State Classification Problem (SCP) for hybrid systems, and
present Neural State Classification (NSC) as an efficient solution technique.
SCP generalizes the model checking problem as it entails classifying each state
$s$ of a hybrid automaton as either positive or negative, depending on whether
or not $s$ satisfies a given time-bounded reachability specification. This is
an interesting problem in its own right, which NSC solves using
machine-learning techniques, Deep Neural Networks in particular. State
classifiers produced by NSC tend to be very efficient (run in constant time and
space), but may be subject to classification errors. To quantify and mitigate
such errors, our approach comprises: i) techniques for certifying, with
statistical guarantees, that an NSC classifier meets given accuracy levels; ii)
tuning techniques, including a novel technique based on adversarial sampling,
that can virtually eliminate false negatives (positive states classified as
negative), thereby making the classifier more conservative. We have applied NSC
to six nonlinear hybrid system benchmarks, achieving an accuracy of 99.25% to
99.98%, and a false-negative rate of 0.0033 to 0, which we further reduced to
0.0015 to 0 after tuning the classifier. We believe that this level of accuracy
is acceptable in many practical applications, and that these results
demonstrate the promise of the NSC approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_D/0/1/0/all/0/1&quot;&gt;Dung Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paoletti_N/0/1/0/all/0/1&quot;&gt;Nicola Paoletti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Timothy Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1&quot;&gt;Radu Grosu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smolka_S/0/1/0/all/0/1&quot;&gt;Scott A. Smolka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoller_S/0/1/0/all/0/1&quot;&gt;Scott D. Stoller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09967">
<title>A Collaborative Approach to Angel and Venture Capital Investment Recommendations. (arXiv:1807.09967v1 [q-fin.PM])</title>
<link>http://arxiv.org/abs/1807.09967</link>
<description rdf:parseType="Literal">&lt;p&gt;Matrix factorization was used to generate investment recommendations for
investors. An iterative conjugate gradient method was used to optimize the
regularized squared-error loss function. The number of latent factors, number
of iterations, and regularization values were explored. Overfitting can be
addressed by either early stopping or regularization parameter tuning. The
model achieved the highest average prediction accuracy of 13.3%. With a similar
model, the same dataset was used to generate investor recommendations for
companies undergoing fundraising, which achieved highest prediction accuracy of
11.1%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Wangperawong_A/0/1/0/all/0/1&quot;&gt;Artit Wangperawong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09979">
<title>Deriving Information Acquisition Criteria For Sequentially Inferring The Expected Value Of A Black-Box Function. (arXiv:1807.09979v1 [math.OC])</title>
<link>http://arxiv.org/abs/1807.09979</link>
<description rdf:parseType="Literal">&lt;p&gt;Acquiring information about noisy expensive black-box functions (computer
simulations or physical experiments) is a tremendously challenging problem.
Finite computational and financial resources restrict the application of
traditional methods for design of experiments. The problem is surmounted by
hurdles such as numerical errors and stochastic approximations errors, when the
quantity of interest (QoI) in a problem depends on an expensive black-box
function. Bayesian optimal design of experiments has been reasonably successful
in guiding the designer towards the QoI for problems of the above kind. This is
usually achieved by sequentially querying the function at designs selected by
an infill-sampling criterion compatible with utility theory. However, most
current methods are semantically designed to work only on optimizing or
inferring the black-box function itself. We aim to construct a heuristic which
can unequivocally deal with the above problems irrespective of the QoI. This
paper applies the above mentioned heuristic to infer a specific QoI, namely the
expectation (expected value) of the function. The Kullback Leibler (KL)
divergence is fairly conspicuous among techniques that used to quantify
information gain. In this paper, we derive an expression for the expected KL
divergence to sequentially infer our QoI. The analytical tractability provided
by the Karhunene Loeve expansion around the Gaussian process (GP)
representation of the black-box function allows circumvention around numerical
issues associated with sample averaging. The proposed methodology can be
extended to any QoI, with reasonable assumptions. The proposed method is
verified and validated on three synthetic functions with varying levels of
complexity and dimensionality. We demonstrate our methodology on a steel wire
manufacturing problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Pandita_P/0/1/0/all/0/1&quot;&gt;Piyush Pandita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bilionis_I/0/1/0/all/0/1&quot;&gt;Ilias Bilionis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Panchal_J/0/1/0/all/0/1&quot;&gt;Jitesh Panchal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10025">
<title>Towards Optimal Power Control via Ensembling Deep Neural Networks. (arXiv:1807.10025v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1807.10025</link>
<description rdf:parseType="Literal">&lt;p&gt;A deep neural network (DNN) based power control method is proposed, which
aims at solving the non-convex optimization problem of maximizing the sum rate
of a multi-user interference channel. Towards this end, we first present PCNet,
which is a multi-layer fully connected neural network that is specifically
designed for the power control problem. PCNet takes the channel coefficients as
input and outputs the transmit power of all users. A key challenge in training
a DNN for the power control problem is the lack of ground truth, i.e., the
optimal power allocation is unknown. To address this issue, PCNet leverages the
unsupervised learning strategy and directly maximizes the sum rate in the
training phase. Observing that a single PCNet does not globally outperform the
existing solutions, we further propose ePCNet, a network ensemble with multiple
PCNets trained independently. Simulation results show that for the standard
symmetric multi-user Gaussian interference channel, ePCNet can outperform all
state-of-the-art power control methods by 1.2%-4.6% under a variety of system
configurations. Furthermore, the performance improvement of ePCNet comes with a
reduced computational complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_F/0/1/0/all/0/1&quot;&gt;Fei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Cong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Feng Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10165">
<title>UNet++: A Nested U-Net Architecture for Medical Image Segmentation. (arXiv:1807.10165v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.10165</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present UNet++, a new, more powerful architecture for
medical image segmentation. Our architecture is essentially a deeply-supervised
encoder-decoder network where the encoder and decoder sub-networks are
connected through a series of nested, dense skip pathways. The re-designed skip
pathways aim at reducing the semantic gap between the feature maps of the
encoder and decoder sub-networks. We argue that the optimizer would deal with
an easier learning task when the feature maps from the decoder and encoder
networks are semantically similar. We have evaluated UNet++ in comparison with
U-Net and wide U-Net architectures across multiple medical image segmentation
tasks: nodule segmentation in the low-dose CT scans of chest, nuclei
segmentation in the microscopy images, liver segmentation in abdominal CT
scans, and polyp segmentation in colonoscopy videos. Our experiments
demonstrate that UNet++ with deep supervision achieves an average IoU gain of
3.9 and 3.4 points over U-Net and wide U-Net, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zongwei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddiquee_M/0/1/0/all/0/1&quot;&gt;Md Mahfuzur Rahman Siddiquee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tajbakhsh_N/0/1/0/all/0/1&quot;&gt;Nima Tajbakhsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jianming Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10166">
<title>Rademacher Generalization Bounds for Classifier Chains. (arXiv:1807.10166v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.10166</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a new framework to study the generalization
property of classifier chains trained over observations associated with
multiple and interdependent class labels. The results are based on large
deviation inequalities for Lipschitz functions of weakly dependent sequences
proposed by Rio in 2000. We believe that the resulting generalization error
bound brings many advantages and could be adapted to other frameworks that
consider interdependent outputs. First, it explicitly exhibits the dependencies
between class labels. Secondly, it provides insights of the effect of the order
of the chain on the algorithm generalization performances. Finally, the two
dependency coefficients that appear in the bound could also be used to design
new strategies to decide the order of the chain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simon_M/0/1/0/all/0/1&quot;&gt;Moura Simon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Massih_Reza_A/0/1/0/all/0/1&quot;&gt;Amini Massih-Reza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sana_L/0/1/0/all/0/1&quot;&gt;Louhichi Sana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marianne_C/0/1/0/all/0/1&quot;&gt;Clausel Marianne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10173">
<title>Differential Analysis of Directed Networks. (arXiv:1807.10173v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1807.10173</link>
<description rdf:parseType="Literal">&lt;p&gt;We developed a novel statistical method to identify structural differences
between net- works characterized by structural equation models. We propose to
reparameterize the model to separate the differential structures from common
structures, and then design an algorithm with calibration and construction
stages to identify these differential structures. The calibration stage serves
to obtain con- sistent prediction by building the l2 regular- ized regression
of each endogenous variables against pre-screened exogenous variables, cor-
recting for potential endogeneity issue. The construction stage consistently
selects and es- timates both common and differential effects by undertaking l1
regularized regression of each endogenous variable against the predicts of
other endogenous variables as well as its an- choring exogenous variables. Our
method al- lows easy parallel computation at each stage. Theoretical results
are obtained to establish non-asymptotic error bounds of predictions and
estimates at both stages, as well as the con- sistency of identified common and
differential effects. Our studies on synthetic data demon- strated that our
proposed method performed much better than independently constructing the
networks. A real data set is analyzed to illustrate the applicability of our
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ren_M/0/1/0/all/0/1&quot;&gt;Min Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dabao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10211">
<title>Robust Tracking via Weighted Online Extreme Learning Machine. (arXiv:1807.10211v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.10211</link>
<description rdf:parseType="Literal">&lt;p&gt;The tracking method based on the extreme learning machine (ELM) is efficient
and effective. ELM randomly generates input weights and biases in the hidden
layer, and then calculates and computes the output weights by reducing the
iterative solution to the problem of linear equations. Therefore, ELM offers
the satisfying classification performance and fast training time than other
discriminative models in tracking. However, the original ELM method often
suffers from the problem of the imbalanced classification distribution, which
is caused by few target objects, leading to under-fitting and more background
samples leading to over-fitting. Worse still, it reduces the robustness of
tracking under special conditions including occlusion, illumination, etc. To
address above problems, in this paper, we present a robust tracking algorithm.
First, we introduce the local weight matrix that is the dynamic creation from
the data distribution at the current frame in the original ELM so as to balance
between the empirical and structure risk, and fully learn the target object to
enhance the classification performance. Second, we improve it to the
incremental learning method ensuring tracking real-time and efficient. Finally,
the forgetting factor is used to strengthen the robustness for changing of the
classification distribution with time. Meanwhile, we propose a novel optimized
method to obtain the optimal sample as the target object, which avoids tracking
drift resulting from noisy samples. Therefore, our tracking method can fully
learn both of the target object and background information to enhance the
tracking performance, and it is evaluated in 20 challenge image sequences with
different attributes including illumination, occlusion, deformation, etc.,
which achieves better performance than several state-of-the-art methods in
terms of effectiveness and robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huibing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yonggong Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10225">
<title>Medical Image Synthesis for Data Augmentation and Anonymization using Generative Adversarial Networks. (arXiv:1807.10225v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1807.10225</link>
<description rdf:parseType="Literal">&lt;p&gt;Data diversity is critical to success when training deep learning models.
Medical imaging data sets are often imbalanced as pathologic findings are
generally rare, which introduces significant challenges when training deep
learning models. In this work, we propose a method to generate synthetic
abnormal MRI images with brain tumors by training a generative adversarial
network using two publicly available data sets of brain MRI. We demonstrate two
unique benefits that the synthetic images provide. First, we illustrate
improved performance on tumor segmentation by leveraging the synthetic images
as a form of data augmentation. Second, we demonstrate the value of generative
models as an anonymization tool, achieving comparable tumor segmentation
results when trained on the synthetic data versus when trained on real subject
data. Together, these results offer a potential solution to two of the largest
challenges facing machine learning in medical imaging, namely the small
incidence of pathological findings, and the restrictions around sharing of
patient data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1&quot;&gt;Hoo-Chang Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenholtz_N/0/1/0/all/0/1&quot;&gt;Neil A Tenenholtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogers_J/0/1/0/all/0/1&quot;&gt;Jameson K Rogers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarz_C/0/1/0/all/0/1&quot;&gt;Christopher G Schwarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senjem_M/0/1/0/all/0/1&quot;&gt;Matthew L Senjem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunter_J/0/1/0/all/0/1&quot;&gt;Jeffrey L Gunter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andriole_K/0/1/0/all/0/1&quot;&gt;Katherine Andriole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michalski_M/0/1/0/all/0/1&quot;&gt;Mark Michalski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.10262">
<title>Seeded Graph Matching via Large Neighborhood Statistics. (arXiv:1807.10262v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.10262</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a well known noisy model of the graph isomorphism problem. In this
model, the goal is to perfectly recover the vertex correspondence between two
edge-correlated Erd\H{o}s-R\&apos;{e}nyi random graphs, with an initial seed set of
correctly matched vertex pairs revealed as side information. For seeded
problems, our result provides a significant improvement over previously known
results. We show that it is possible to achieve the information-theoretic limit
of graph sparsity in time polynomial in the number of vertices $n$. Moreover,
we show the number of seeds needed for exact recovery in polynomial-time can be
as low as $n^{3\epsilon}$ in the sparse graph regime (with the average degree
smaller than $n^{\epsilon}$) and $\Omega(\log n)$ in the dense graph regime.
&lt;/p&gt;
&lt;p&gt;Our results also shed light on the unseeded problem. In particular, we give
sub-exponential time algorithms for sparse models and an $n^{O(\log n)}$
algorithm for dense models for some parameters, including some that are not
covered by recent results of Barak et al.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mossel_E/0/1/0/all/0/1&quot;&gt;Elchanan Mossel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaming Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1309.7857">
<title>Generalized system identification with stable spline kernels. (arXiv:1309.7857v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1309.7857</link>
<description rdf:parseType="Literal">&lt;p&gt;Regularized least-squares approaches have been successfully applied to linear
system identification. Recent approaches use quadratic penalty terms on the
unknown impulse response defined by stable spline kernels, which control model
space complexity by leveraging regularity and bounded-input bounded-output
stability. This paper extends linear system identification to a wide class of
nonsmooth stable spline estimators, where regularization functionals and data
misfits can be selected from a rich set of piecewise linear-quadratic (PLQ)
penalties. This class includes the 1-norm, Huber, and Vapnik, in addition to
the least-squares penalty.
&lt;/p&gt;
&lt;p&gt;By representing penalties through their conjugates, the modeler can specify
any piecewise linear-quadratic penalty for misfit and regularizer, as well as
inequality constraints on the response. The interior-point solver we implement
(IPsolve) is locally quadratically convergent, with $O(\min(m,n)^2(m+n))$
arithmetic operations per iteration, where $n$ the number of unknown impulse
response coefficients and $m$ the number of observed output measurements.
IPsolve is competitive with available alternatives for system identification.
This is shown by a comparison with TFOCS, libSVM, and the FISTA algorithm. The
code is open source (https://github.com/saravkin/IPsolve).
&lt;/p&gt;
&lt;p&gt;The impact of the approach for system identification is illustrated with
numerical experiments featuring robust formulations for contaminated data,
relaxation systems, nonnegativity and unimodality constraints on the impulse
response, and sparsity promoting regularization. Incorporating constraints
yields particularly significant improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aravkin_A/0/1/0/all/0/1&quot;&gt;Aleksandr Y. Aravkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Burke_J/0/1/0/all/0/1&quot;&gt;James V. Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pillonetto_G/0/1/0/all/0/1&quot;&gt;Gianluigi Pillonetto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.05917">
<title>Accelerating Kernel Classifiers Through Borders Mapping. (arXiv:1708.05917v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.05917</link>
<description rdf:parseType="Literal">&lt;p&gt;Support vector machines (SVM) and other kernel techniques represent a family
of powerful statistical classification methods with high accuracy and broad
applicability. Because they use all or a significant portion of the training
data, however, they can be slow, especially for large problems. Piecewise
linear classifiers are similarly versatile, yet have the additional advantages
of simplicity, ease of interpretation and, if the number of component linear
classifiers is not too large, speed. Here we show how a simple, piecewise
linear classifier can be trained from a kernel-based classifier in order to
improve the classification speed. The method works by finding the root of the
difference in conditional probabilities between pairs of opposite classes to
build up a representation of the decision boundary. When tested on 17 different
datasets, it succeeded in improving the classification speed of a SVM for 9 of
them by factors as high as 88 times or more. The method is best suited to
problems with continuum features data and smooth probability functions. Because
the component linear classifiers are built up individually from an existing
classifier, rather than through a simultaneous optimization procedure, the
classifier is also fast to train.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mills_P/0/1/0/all/0/1&quot;&gt;Peter Mills&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.05932">
<title>Fundamental Limits of Weak Recovery with Applications to Phase Retrieval. (arXiv:1708.05932v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.05932</link>
<description rdf:parseType="Literal">&lt;p&gt;In phase retrieval we want to recover an unknown signal $\boldsymbol
x\in\mathbb C^d$ from $n$ quadratic measurements of the form $y_i =
|\langle{\boldsymbol a}_i,{\boldsymbol x}\rangle|^2+w_i$ where $\boldsymbol
a_i\in \mathbb C^d$ are known sensing vectors and $w_i$ is measurement noise.
We ask the following weak recovery question: what is the minimum number of
measurements $n$ needed to produce an estimator $\hat{\boldsymbol
x}(\boldsymbol y)$ that is positively correlated with the signal $\boldsymbol
x$? We consider the case of Gaussian vectors $\boldsymbol a_i$. We prove that -
in the high-dimensional limit - a sharp phase transition takes place, and we
locate the threshold in the regime of vanishingly small noise. For $n\le
d-o(d)$ no estimator can do significantly better than random and achieve a
strictly positive correlation. For $n\ge d+o(d)$ a simple spectral estimator
achieves a positive correlation. Surprisingly, numerical simulations with the
same spectral estimator demonstrate promising performance with realistic
sensing matrices. Spectral methods are used to initialize non-convex
optimization algorithms in phase retrieval, and our approach can boost the
performance in this setting as well.
&lt;/p&gt;
&lt;p&gt;Our impossibility result is based on classical information-theory arguments.
The spectral algorithm computes the leading eigenvector of a weighted empirical
covariance matrix. We obtain a sharp characterization of the spectral
properties of this random matrix using tools from free probability and
generalizing a recent result by Lu and Li. Both the upper and lower bound
generalize beyond phase retrieval to measurements $y_i$ produced according to a
generalized linear model. As a byproduct of our analysis, we compare the
threshold of the proposed spectral method with that of a message passing
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mondelli_M/0/1/0/all/0/1&quot;&gt;Marco Mondelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Montanari_A/0/1/0/all/0/1&quot;&gt;Andrea Montanari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.05482">
<title>Binary Matrix Factorization via Dictionary Learning. (arXiv:1804.05482v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1804.05482</link>
<description rdf:parseType="Literal">&lt;p&gt;Matrix factorization is a key tool in data analysis; its applications include
recommender systems, correlation analysis, signal processing, among others.
Binary matrices are a particular case which has received significant attention
for over thirty years, especially within the field of data mining. Dictionary
learning refers to a family of methods for learning overcomplete basis (also
called frames) in order to efficiently encode samples of a given type; this
area, now also about twenty years old, was mostly developed within the signal
processing field. In this work we propose two binary matrix factorization
methods based on a binary adaptation of the dictionary learning paradigm to
binary matrices. The proposed algorithms focus on speed and scalability; they
work with binary factors combined with bit-wise operations and a few auxiliary
integer ones. Furthermore, the methods are readily applicable to online binary
matrix factorization. Another important issue in matrix factorization is the
choice of rank for the factors; we address this model selection problem with an
efficient method based on the Minimum Description Length principle. Our
preliminary results show that the proposed methods are effective at producing
interpretable factorizations of various data types of different nature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramirez_I/0/1/0/all/0/1&quot;&gt;Ignacio Ramirez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06576">
<title>Mad Max: Affine Spline Insights into Deep Learning. (arXiv:1805.06576v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.06576</link>
<description rdf:parseType="Literal">&lt;p&gt;We build a rigorous bridge between deep networks (DNs) and approximation
theory via spline functions and operators. Our key result is that a large class
of DNs can be written as a composition of max-affine spline operators (MASOs),
which provide a powerful portal through which to view and analyze their inner
workings. For instance, conditioned on the input signal, the output of a MASO
DN can be written as a simple affine transformation of the input. This implies
that a DN constructs a set of signal-dependent, class-specific templates
against which the signal is compared via a simple inner product; we explore the
links to the classical theory of optimal classification via matched filters and
the effects of data memorization. Going further, we propose a simple penalty
term that can be added to the cost function of any DN learning algorithm to
force the templates to be orthogonal with each other; this leads to
significantly improved classification performance and reduced overfitting with
no change to the DN architecture. The spline partition of the input signal
space that is implicitly induced by a MASO directly links DNs to the theory of
vector quantization (VQ) and $K$-means clustering, which opens up new geometric
avenue to study how DNs organize signals in a hierarchical fashion. To validate
the utility of the VQ interpretation, we develop and validate a new distance
metric for signals and images that quantifies the difference between their VQ
encodings. (This paper is a significantly expanded version of A Spline Theory
of Deep Learning from ICML 2018.)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balestriero_R/0/1/0/all/0/1&quot;&gt;Randall Balestriero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baraniuk_R/0/1/0/all/0/1&quot;&gt;Richard Baraniuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.05017">
<title>Brain-Computer Interface with Corrupted EEG Data: A Tensor Completion Approach. (arXiv:1806.05017v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/1806.05017</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the current issues in Brain-Computer Interface is how to deal with
noisy Electroencephalography measurements organized as multidimensional
datasets. On the other hand, recently, significant advances have been made in
multidimensional signal completion algorithms that exploit tensor decomposition
models to capture the intricate relationship among entries in a
multidimensional signal. We propose to use tensor completion applied to EEG
data for improving the classification performance in a motor imagery BCI system
with corrupted measurements. Noisy measurements are considered as unknowns that
are inferred from a tensor decomposition model. We evaluate the performance of
four recently proposed tensor completion algorithms plus a simple interpolation
strategy, first with random missing entries and then with missing samples
constrained to have a specific structure (random missing channels), which is a
more realistic assumption in BCI Applications. We measured the ability of these
algorithms to reconstruct the tensor from observed data. Then, we tested the
classification accuracy of imagined movement in a BCI experiment with missing
samples. We show that for random missing entries, all tensor completion
algorithms can recover missing samples increasing the classification
performance compared to a simple interpolation approach. For the random missing
channels case, we show that tensor completion algorithms help to reconstruct
missing channels, significantly improving the accuracy in the classification of
motor imagery, however, not at the same level as clean data. Tensor completion
algorithms are useful in real BCI applications. The proposed strategy could
allow using motor imagery BCI systems even when EEG data is highly affected by
missing channels and/or samples, avoiding the need of new acquisitions in the
calibration stage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sole_Casals_J/0/1/0/all/0/1&quot;&gt;Jordi Sole-Casals&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Caiafa_C/0/1/0/all/0/1&quot;&gt;Cesar F. Caiafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qibin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cichocki_A/0/1/0/all/0/1&quot;&gt;Adrzej Cichocki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.07207">
<title>A Projection Pursuit Forest Algorithm for Supervised Classification. (arXiv:1807.07207v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.07207</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new ensemble learning method for classification
problems called projection pursuit random forest (PPF). PPF uses the PPtree
algorithm introduced in Lee et al. (2013). In PPF, trees are constructed by
splitting on linear combinations of randomly chosen variables. Projection
pursuit is used to choose a projection of the variables that best separates the
classes. Utilizing linear combinations of variables to separate classes takes
the correlation between variables into account which allows PPF to outperform a
traditional random forest when separations between groups occurs in
combinations of variables.
&lt;/p&gt;
&lt;p&gt;The method presented here can be used in multi-class problems and is
implemented into an R (R Core Team, 2018) package, PPforest, which is available
on CRAN, with development versions at https://github.com/natydasilva/PPforest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Silva_N/0/1/0/all/0/1&quot;&gt;Natalia da Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cook_D/0/1/0/all/0/1&quot;&gt;Dianne Cook&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_E/0/1/0/all/0/1&quot;&gt;Eun-Kyung Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.09386">
<title>On the Randomized Complexity of Minimizing a Convex Quadratic Function. (arXiv:1807.09386v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.09386</link>
<description rdf:parseType="Literal">&lt;p&gt;Minimizing a convex, quadratic objective is a fundamental problem in machine
learning and optimization. In this work, we study prove
$\textit{information-theoretic}$, gradient query complexity lower bounds for
minimizing convex quadratic functions, which, unlike prior works, apply even
for $\textit{randomized}$ algorithms. Specifically, we construct a distribution
over quadratic functions that witnesses lower bounds which match those known
for deterministic algorithms, up to multiplicative constants. The distribution
which witnesses our lower bound is in fact quite benign: it is both closed
form, and derived from classical ensembles in random matrix theory. We believe
that our construction constitutes a plausible &quot;average case&quot; setting, and thus
provides compelling evidence that the worst case and average case complexity of
convex-quadratic optimization are essentially identical.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1&quot;&gt;Max Simchowitz&lt;/a&gt;</dc:creator>
</item></rdf:RDF>