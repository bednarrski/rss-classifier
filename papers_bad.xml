<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-07-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04505"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04640"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04723"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.06770"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.02567"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.05984"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04320"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04457"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04458"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04551"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04561"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04302"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04307"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04369"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04386"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04426"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04427"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04428"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04489"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04566"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04594"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04602"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04639"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04689"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04709"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04715"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04720"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04734"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04742"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1705.00105"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.07600"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08240"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.06300"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.08318"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.06320"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.06021"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1804.09046"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.00794"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.07574"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.12332"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.03467"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.04594"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00243"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.01085"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.03431"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.04222"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1806.11424"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1807.04505">
<title>Achieving Connectivity Between Wide Areas Through Self-Organising Robot Swarm Using Embodied Evolution. (arXiv:1807.04505v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1807.04505</link>
<description rdf:parseType="Literal">&lt;p&gt;Abruptions to the communication infrastructure happens occasionally, where
manual dedicated personnel will go out to fix the interruptions, restoring
communication abilities. However, sometimes this can be dangerous to the
personnel carrying out the task, which can be the case in war situations,
environmental disasters like earthquakes or toxic spills or in the occurrence
of fire. Therefore, human casualties can be minimised if autonomous robots are
deployed that can achieve the same outcome: to establish a communication link
between two previously distant but connected sites. In this paper we
investigate the deployment of mobile ad hoc robots which relay traffic between
them. In order to get the robots to locate themselves appropriately, we take
inspiration from self-organisation and emergence in artificial life, where a
common overall goal may be achieved if the correct local rules on the agents in
system are invoked. We integrate the aspect of connectivity between two sites
into the multirobot simulation platform known as JBotEvolver. The robot swarm
is composed of Thymio II robots. In addition, we compare three heuristics, of
which one uses neuroevolution (evolution of neural networks) to show how
self-organisation and embodied evolution can be used within the integration.
Our use of embodiment in robotic controllers shows promising results and
provide solid knowledge and guidelines for further investigations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansen_E/0/1/0/all/0/1&quot;&gt;Erik Aaron Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nichele_S/0/1/0/all/0/1&quot;&gt;Stefano Nichele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazidi_A/0/1/0/all/0/1&quot;&gt;Anis Yazidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haugerud_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe5;rek Haugerud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mofrad_A/0/1/0/all/0/1&quot;&gt;Asieh Abolpour Mofrad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alcocer_A/0/1/0/all/0/1&quot;&gt;Alex Alcocer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04640">
<title>Automatically Composing Representation Transformations as a Means for Generalization. (arXiv:1807.04640v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04640</link>
<description rdf:parseType="Literal">&lt;p&gt;How can we build a learner that can capture the essence of what makes a hard
problem more complex than a simple one, break the hard problem along
characteristic lines into smaller problems it knows how to solve, and
sequentially solve the smaller problems until the larger one is solved? To work
towards this goal, we focus on learning to generalize in a particular family of
problems that exhibit compositional and recursive structure: their solutions
can be found by composing in sequence a set of reusable partial solutions. Our
key idea is to recast the problem of generalization as a problem of learning
algorithmic procedures: we can formulate a solution to this family as a
sequential decision-making process over transformations between
representations. Our formulation enables the learner to learn the structure and
parameters of its own computation graph with sparse supervision, make analogies
between problems by transforming one problem representation to another, and
exploit modularity and reuse to scale to problems of varying complexity.
Experiments on solving a variety of multilingual arithmetic problems
demonstrate that our method discovers the hierarchical decomposition of a
problem into its subproblems, generalizes out of distribution to unseen problem
classes, and extrapolates to harder versions of the same problem, yielding a
10-fold reduction in sample complexity compared to a monolithic recurrent
neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1&quot;&gt;Michael B. Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1&quot;&gt;Thomas L. Griffiths&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04723">
<title>The Bottleneck Simulator: A Model-based Deep Reinforcement Learning Approach. (arXiv:1807.04723v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04723</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning has recently shown many impressive successes.
However, one major obstacle towards applying such methods to real-world
problems is their lack of data-efficiency. To this end, we propose the
Bottleneck Simulator: a model-based reinforcement learning method which
combines a learned, factorized transition model of the environment with rollout
simulations to learn an effective policy from few examples. The learned
transition model employs an abstract, discrete (bottleneck) state, which
increases sample efficiency by reducing the number of model parameters and by
exploiting structural properties of the environment. We provide a mathematical
analysis of the Bottleneck Simulator in terms of fixed points of the learned
policy, which reveals how performance is affected by four distinct sources of
error: an error related to the abstract space structure, an error related to
the transition model estimation variance, an error related to the transition
model estimation bias, and an error related to the transition model class bias.
Finally, we evaluate the Bottleneck Simulator on two natural language
processing tasks: a text adventure game and a real-world, complex dialogue
response selection task. On both tasks, the Bottleneck Simulator yields
excellent performance beating competing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serban_I/0/1/0/all/0/1&quot;&gt;Iulian Vlad Serban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1&quot;&gt;Chinnadhurai Sankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pieper_M/0/1/0/all/0/1&quot;&gt;Michael Pieper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1&quot;&gt;Joelle Pineau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.06770">
<title>SQG-Differential Evolution for difficult optimization problems under a tight function evaluation budget. (arXiv:1710.06770v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1710.06770</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of industrial engineering, it is important to integrate
efficient computational optimization methods in the product development
process. Some of the most challenging simulation-based engineering design
optimization problems are characterized by: a large number of design variables,
the absence of analytical gradients, highly non-linear objectives and a limited
function evaluation budget. Although a huge variety of different optimization
algorithms is available, the development and selection of efficient algorithms
for problems with these industrial relevant characteristics, remains a
challenge. In this communication, a hybrid variant of Differential Evolution
(DE) is introduced which combines aspects of Stochastic Quasi-Gradient (SQG)
methods within the framework of DE, in order to improve optimization efficiency
on problems with the previously mentioned characteristics. The performance of
the resulting derivative-free algorithm is compared with other state-of-the-art
DE variants on 25 commonly used benchmark functions, under tight function
evaluation budget constraints of 1000 evaluations. The experimental results
indicate that the new algorithm performs excellent on the &apos;difficult&apos; (high
dimensional, multi-modal, inseparable) test functions. The operations used in
the proposed mutation scheme, are computationally inexpensive, and can be
easily implemented in existing differential evolution variants or other
population-based optimization algorithms by a few lines of program code as an
non-invasive optional setting. Besides the applicability of the presented
algorithm by itself, the described concepts can serve as a useful and
interesting addition to the algorithmic operators in the frameworks of
heuristics and evolutionary optimization and computing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_R/0/1/0/all/0/1&quot;&gt;Ramses Sala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldanzini_N/0/1/0/all/0/1&quot;&gt;Niccolo Baldanzini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pierini_M/0/1/0/all/0/1&quot;&gt;Marco Pierini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.02567">
<title>Weighted Contrastive Divergence. (arXiv:1801.02567v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1801.02567</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning algorithms for energy based Boltzmann architectures that rely on
gradient descent are in general computationally prohibitive, typically due to
the exponential number of terms involved in computing the partition function.
In this way one has to resort to approximation schemes for the evaluation of
the gradient. This is the case of Restricted Boltzmann Machines (RBM) and its
learning algorithm Contrastive Divergence (CD). It is well-known that CD has a
number of shortcomings, and its approximation to the gradient has several
drawbacks. Overcoming these defects has been the basis of much research and new
algorithms have been devised, such as persistent CD. In this manuscript we
propose a new algorithm that we call Weighted CD (WCD), built from small
modifications of the negative phase in standard CD. However small these
modifications may be, experimental work reported in this paper suggest that WCD
provides a significant improvement over standard CD and persistent CD at a
small additional computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merino_E/0/1/0/all/0/1&quot;&gt;Enrique Romero Merino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castrillejo_F/0/1/0/all/0/1&quot;&gt;Ferran Mazzanti Castrillejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pin_J/0/1/0/all/0/1&quot;&gt;Jordi Delgado Pin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prats_D/0/1/0/all/0/1&quot;&gt;David Buchaca Prats&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.05984">
<title>Prediction of the Optimal Threshold Value in DF Relay Selection Schemes Based on Artificial Neural Networks. (arXiv:1801.05984v1 [eess.SP] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1801.05984</link>
<description rdf:parseType="Literal">&lt;p&gt;In wireless communications, the cooperative communication (CC) technology
promises performance gains compared to traditional Single-Input Single Output
(SISO) techniques. Therefore, the CC technique is one of the nominees for 5G
networks. In the Decode-and-Forward (DF) relaying scheme which is one of the CC
techniques, determination of the threshold value at the relay has a key role
for the system performance and power usage. In this paper, we propose
prediction of the optimal threshold values for the best relay selection scheme
in cooperative communications, based on Artificial Neural Networks (ANNs) for
the first time in literature. The average link qualities and number of relays
have been used as inputs in the prediction of optimal threshold values using
Artificial Neural Networks (ANNs): Multi-Layer Perceptron (MLP) and Radial
Basis Function (RBF) networks. The MLP network has better performance from the
RBF network on the prediction of optimal threshold value when the same number
of neurons is used at the hidden layer for both networks. Besides, the optimal
threshold values obtained using ANNs are verified by the optimal threshold
values obtained numerically using the closed form expression derived for the
system. The results show that the optimal threshold values obtained by ANNs on
the best relay selection scheme provide a minimum Bit-Error-Rate (BER) because
of the reduction of the probability that error propagation may occur. Also, for
the same BER performance goal, prediction of optimal threshold values provides
2dB less power usage, which is great gain in terms of green communicationBER
performance goal, prediction of optimal threshold values provides 2dB less
power usage, which is great gain in terms of green communication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kara_F/0/1/0/all/0/1&quot;&gt;Ferdi Kara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kaya_H/0/1/0/all/0/1&quot;&gt;Hakan Kaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Erkaymaz_O/0/1/0/all/0/1&quot;&gt;Okan Erkaymaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ozturk_E/0/1/0/all/0/1&quot;&gt;Ertan Ozturk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04320">
<title>Automated Vulnerability Detection in Source Code Using Deep Representation Learning. (arXiv:1807.04320v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04320</link>
<description rdf:parseType="Literal">&lt;p&gt;Increasing numbers of software vulnerabilities are discovered every year
whether they are reported publicly or discovered internally in proprietary
code. These vulnerabilities can pose serious risk of exploit and result in
system compromise, information leaks, or denial of service. We leveraged the
wealth of C and C++ open-source code available to develop a large-scale
function-level vulnerability detection system using machine learning. To
supplement existing labeled vulnerability datasets, we compiled a vast dataset
of millions of open-source functions and labeled it with carefully-selected
findings from three different static analyzers that indicate potential
exploits. Using these datasets, we developed a fast and scalable vulnerability
detection tool based on deep feature representation learning that directly
interprets lexed source code. We evaluated our tool on code from both real
software packages and the NIST SATE IV benchmark dataset. Our results
demonstrate that deep feature representation learning on source code is a
promising approach for automated software vulnerability detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_R/0/1/0/all/0/1&quot;&gt;Rebecca L. Russell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_L/0/1/0/all/0/1&quot;&gt;Louis Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamilton_L/0/1/0/all/0/1&quot;&gt;Lei H. Hamilton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazovich_T/0/1/0/all/0/1&quot;&gt;Tomo Lazovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harer_J/0/1/0/all/0/1&quot;&gt;Jacob A. Harer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozdemir_O/0/1/0/all/0/1&quot;&gt;Onur Ozdemir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellingwood_P/0/1/0/all/0/1&quot;&gt;Paul M. Ellingwood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McConley_M/0/1/0/all/0/1&quot;&gt;Marc W. McConley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04457">
<title>Query-Efficient Hard-label Black-box Attack:An Optimization-based Approach. (arXiv:1807.04457v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04457</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of attacking a machine learning model in the hard-label
black-box setting, where no model information is revealed except that the
attacker can make queries to probe the corresponding hard-label decisions. This
is a very challenging problem since the direct extension of state-of-the-art
white-box attacks (e.g., CW or PGD) to the hard-label black-box setting will
require minimizing a non-continuous step function, which is combinatorial and
cannot be solved by a gradient-based optimizer. The only current approach is
based on random walk on the boundary, which requires lots of queries and lacks
convergence guarantees. We propose a novel way to formulate the hard-label
black-box attack as a real-valued optimization problem which is usually
continuous and can be solved by any zeroth order optimization algorithm. For
example, using the Randomized Gradient-Free method, we are able to bound the
number of iterations needed for our algorithm to achieve stationary points. We
demonstrate that our proposed method outperforms the previous random walk
approach to attacking convolutional neural networks on MNIST, CIFAR, and
ImageNet datasets. More interestingly, we show that the proposed algorithm can
also be used to attack other discrete and non-continuous machine learning
models, such as Gradient Boosting Decision Trees (GBDT).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Minhao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thong Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04458">
<title>Monte Carlo Methods for the Game Kingdomino. (arXiv:1807.04458v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.04458</link>
<description rdf:parseType="Literal">&lt;p&gt;Kingdomino is introduced as an interesting game for studying game playing:
the game is multiplayer (4 independent players per game); it has a limited game
depth (13 moves per player); and it has limited but not insignificant
interaction among players.
&lt;/p&gt;
&lt;p&gt;Several strategies based on locally greedy players, Monte Carlo Evaluation
(MCE), and Monte Carlo Tree Search (MCTS) are presented with variants. We
examine a variation of UCT called progressive win bias and a playout policy
(Player-greedy) focused on selecting good moves for the player. A thorough
evaluation is done showing how the strategies perform and how to choose
parameters given specific time constraints. The evaluation shows that
surprisingly MCE is stronger than MCTS for a game like Kingdomino.
&lt;/p&gt;
&lt;p&gt;All experiments use a cloud-native design, with a game server in a Docker
container, and agents communicating using a REST-style JSON protocol. This
enables a multi-language approach to separating the game state, the strategy
implementations, and the coordination layer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gedda_M/0/1/0/all/0/1&quot;&gt;Magnus Gedda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lagerkvist_M/0/1/0/all/0/1&quot;&gt;Mikael Z. Lagerkvist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Butler_M/0/1/0/all/0/1&quot;&gt;Martin Butler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04551">
<title>A Constrained Randomized Shortest-Paths Framework for Optimal Exploration. (arXiv:1807.04551v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04551</link>
<description rdf:parseType="Literal">&lt;p&gt;The present work extends the randomized shortest-paths framework (RSP),
interpolating between shortest-path and random-walk routing in a network, in
three directions. First, it shows how to deal with equality constraints on a
subset of transition probabilities and develops a generic algorithm for solving
this constrained RSP problem using Lagrangian duality. Second, it derives a
surprisingly simple iterative procedure to compute the optimal, randomized,
routing policy generalizing the previously developed &quot;soft&quot; Bellman-Ford
algorithm. The resulting algorithm allows balancing exploitation and
exploration in an optimal way by interpolating between a pure random behavior
and the deterministic, optimal, policy (least-cost paths) while satisfying the
constraints. Finally, the two algorithms are applied to Markov decision
problems by considering the process as a constrained RSP on a bipartite
state-action graph. In this context, the derived &quot;soft&quot; value iteration
algorithm appears to be closely related to dynamic policy programming as well
as Kullback-Leibler and path integral control, and similar to a recently
introduced reinforcement learning exploration strategy. This shows that this
strategy is optimal in the RSP sense - it minimizes expected path cost subject
to relative entropy constraint. Simulation results on illustrative examples
show that the model behaves as expected.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lebichot_B/0/1/0/all/0/1&quot;&gt;Bertrand Lebichot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guex_G/0/1/0/all/0/1&quot;&gt;Guillaume Guex&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kivimaki_I/0/1/0/all/0/1&quot;&gt;Ilkka Kivim&amp;#xe4;ki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saerens_M/0/1/0/all/0/1&quot;&gt;Marco Saerens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04561">
<title>Situation Calculus for Synthesis of Manufacturing Controllers. (arXiv:1807.04561v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1807.04561</link>
<description rdf:parseType="Literal">&lt;p&gt;Manufacturing is transitioning from a mass production model to a
manufacturing as a service model in which manufacturing facilities &apos;bid&apos; to
produce products. To decide whether to bid for a complex, previously unseen
product, a manufacturing facility must be able to synthesize, &apos;on the fly&apos;, a
process plan controller that delegates abstract manufacturing tasks in the
supplied process recipe to the appropriate manufacturing resources, e.g., CNC
machines, robots etc. Previous work in applying AI behaviour composition to
synthesize process plan controllers has considered only finite state ad-hoc
representations. Here, we study the problem in the relational setting of the
Situation Calculus. By taking advantage of recent work on abstraction in the
Situation Calculus, process recipes and available resources are represented by
ConGolog programs over, respectively, an abstract and a concrete action theory.
This allows us to capture the problem in a formal, general framework, and show
decidability for the case of bounded action theories. We also provide
techniques for actually synthesizing the controller.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giacomo_G/0/1/0/all/0/1&quot;&gt;Giuseppe De Giacomo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Logan_B/0/1/0/all/0/1&quot;&gt;Brian Logan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felli_P/0/1/0/all/0/1&quot;&gt;Paolo Felli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patrizi_F/0/1/0/all/0/1&quot;&gt;Fabio Patrizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sardina_S/0/1/0/all/0/1&quot;&gt;Sebastian Sardina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04302">
<title>Structured Bayesian Gaussian process latent variable model: applications to data-driven dimensionality reduction and high-dimensional inversion. (arXiv:1807.04302v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.04302</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a methodology for nonlinear inverse problems using a variational
Bayesian approach where the unknown quantity is a spatial field. A structured
Bayesian Gaussian process latent variable model is used both to construct a
low-dimensional generative model of the sample-based stochastic prior as well
as a surrogate for the forward evaluation. Its Bayesian formulation captures
epistemic uncertainty introduced by the limited number of input and output
examples, automatically selects an appropriate dimensionality for the learned
latent representation of the data, and rigorously propagates the uncertainty of
the data-driven dimensionality reduction of the stochastic space through the
forward model surrogate. The structured Gaussian process model explicitly
leverages spatial information for an informative generative prior to improve
sample efficiency while achieving computational tractability through Kronecker
product decompositions of the relevant kernel matrices. Importantly, the
Bayesian inversion is carried out by solving a variational optimization
problem, replacing traditional computationally-expensive Monte Carlo sampling.
The methodology is demonstrated on an elliptic PDE and is shown to return
well-calibrated posteriors and is tractable with latent spaces with over 100
dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Atkinson_S/0/1/0/all/0/1&quot;&gt;Steven Atkinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zabaras_N/0/1/0/all/0/1&quot;&gt;Nicholas Zabaras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04307">
<title>Manifold regularization with GANs for semi-supervised learning. (arXiv:1807.04307v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04307</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks are powerful generative models that are able
to model the manifold of natural images. We leverage this property to perform
manifold regularization by approximating a variant of the Laplacian norm using
a Monte Carlo approximation that is easily computed with the GAN. When
incorporated into the semi-supervised feature-matching GAN we achieve
state-of-the-art results for GAN-based semi-supervised learning on CIFAR-10 and
SVHN benchmarks, with a method that is significantly easier to implement than
competing methods. We also find that manifold regularization improves the
quality of generated images, and is affected by the quality of the GAN used to
approximate the regularizer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lecouat_B/0/1/0/all/0/1&quot;&gt;Bruno Lecouat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1&quot;&gt;Chuan-Sheng Foo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenati_H/0/1/0/all/0/1&quot;&gt;Houssam Zenati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasekhar_V/0/1/0/all/0/1&quot;&gt;Vijay Chandrasekhar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04369">
<title>Differentially-Private &quot;Draw and Discard&quot; Machine Learning. (arXiv:1807.04369v1 [cs.CR])</title>
<link>http://arxiv.org/abs/1807.04369</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a novel framework for privacy-preserving
client-distributed machine learning. It is motivated by the desire to achieve
differential privacy guarantees in the local model of privacy in a way that
satisfies all systems constraints using asynchronous client-server
communication and provides attractive model learning properties. We call it
&quot;Draw and Discard&quot; because it relies on random sampling of models for load
distribution (scalability), which also provides additional server-side privacy
protections and improved model quality through averaging. We present the
mechanics of client and server components of &quot;Draw and Discard&quot; and demonstrate
how the framework can be applied to learning Generalized Linear models. We then
analyze the privacy guarantees provided by our approach against several types
of adversaries and showcase experimental results that provide evidence for the
framework&apos;s viability in practical deployments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pihur_V/0/1/0/all/0/1&quot;&gt;Vasyl Pihur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korolova_A/0/1/0/all/0/1&quot;&gt;Aleksandra Korolova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Frederick Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankuratripati_S/0/1/0/all/0/1&quot;&gt;Subhash Sankuratripati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yung_M/0/1/0/all/0/1&quot;&gt;Moti Yung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Dachuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_R/0/1/0/all/0/1&quot;&gt;Ruogu Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04386">
<title>Topic Diffusion Discovery based on Sparseness-constrained Non-negative Matrix Factorization. (arXiv:1807.04386v1 [cs.IR])</title>
<link>http://arxiv.org/abs/1807.04386</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to recent explosion of text data, researchers have been overwhelmed by
ever-increasing volume of articles produced by different research communities.
Various scholarly search websites, citation recommendation engines, and
research databases have been created to simplify the text search tasks.
However, it is still difficult for researchers to be able to identify potential
research topics without doing intensive reviews on a tremendous number of
articles published by journals, conferences, meetings, and workshops. In this
paper, we consider a novel topic diffusion discovery technique that
incorporates sparseness-constrained Non-negative Matrix Factorization with
generalized Jensen-Shannon divergence to help understand term-topic evolutions
and identify topic diffusions. Our experimental result shows that this approach
can extract more prominent topics from large article databases, visualize
relationships between terms of interest and abstract topics, and further help
researchers understand whether given terms/topics have been widely explored or
whether new topics are emerging from literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yihuang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Keng-Pei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_I/0/1/0/all/0/1&quot;&gt;I-Ling Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04426">
<title>A likelihood-ratio type test for stochastic block models with bounded degrees. (arXiv:1807.04426v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1807.04426</link>
<description rdf:parseType="Literal">&lt;p&gt;A fundamental problem in network data analysis is to test Erd\&quot;{o}s-R\&apos;{e}nyi
model $\mathcal{G}\left(n,\frac{a+b}{2n}\right)$ versus a bisection stochastic
block model $\mathcal{G}\left(n,\frac{a}{n},\frac{b}{n}\right)$, where $a,b&amp;gt;0$
are constants that represent the expected degrees of the graphs and $n$ denotes
the number of nodes. This problem serves as the foundation of many other
problems such as testing-based methods for determining the number of
communities (\cite{BS16,L16}) and community detection (\cite{MS16}). Existing
work has been focusing on growing-degree regime $a,b\to\infty$
(\cite{BS16,L16,MS16,BM17,B18,GL17a,GL17b}) while leaving the bounded-degree
regime untreated. In this paper, we propose a likelihood-ratio (LR) type
procedure based on regularization to test stochastic block models with bounded
degrees. We derive the limit distributions as power Poisson laws under both
null and alternative hypotheses, based on which the limit power of the test is
carefully analyzed. We also examine a Monte-Carlo method that partly resolves
the computational cost issue. The proposed procedures are examined by both
simulated and real-world data. The proof depends on a contiguity theory
developed by Janson \cite{J95}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mingao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shang_Z/0/1/0/all/0/1&quot;&gt;Zuofeng Shang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04427">
<title>Amplifying state dissimilarity leads to robust and interpretable clustering of scientific data. (arXiv:1807.04427v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.04427</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods that aim to automatically cluster data into physically
meaningful subsets typically require assumptions regarding the number, size, or
shape of the coherent subgroups. We present a new method, simultaneous Coherent
Structure Coloring (sCSC), which accomplishes the task of unsupervised
clustering without a priori guidance regarding the underlying structure of the
data. To illustrate the versatility of the method, we apply it to frontier
physics problems at vastly different temporal and spatial scales: in a
theoretical model of geophysical fluid dynamics, in laboratory measurements of
vortex ring formation and entrainment, and in atomistic simulation of the
Protein G system. The theoretical flow involves sparse sampling of
non-equilibrium dynamics, where this new technique can find and characterize
the structures that govern fluid transport using two orders of magnitude less
data than required by existing methods. Application of the method to empirical
measurements of vortex formation leads to the discovery of a well defined
region in which vortex ring entrainment occurs, with potential implications
ranging from flow control to cardiovascular diagnostics. Finally, the protein
folding example demonstrates a data-rich application governed by equilibrium
dynamics, where the technique in this manuscript automatically discovers the
hierarchy of distinct processes that govern protein folding and clusters
protein configurations accordingly. We anticipate straightforward translation
to many other fields where existing analysis tools, such as k-means and
traditional hierarchical clustering, require ad hoc assumptions on the data
structure or lack the interpretability of the present method. The method is
also potentially generalizable to fields where the underlying processes are
less accessible, such as genomics and neuroscience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Husic_B/0/1/0/all/0/1&quot;&gt;Brooke E. Husic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schlueter_Kuck_K/0/1/0/all/0/1&quot;&gt;Kristy L. Schlueter-Kuck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dabiri_J/0/1/0/all/0/1&quot;&gt;John O. Dabiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04428">
<title>Convergence Rate of Block-Coordinate Maximization Burer-Monteiro Method for Solving Large SDPs. (arXiv:1807.04428v1 [math.OC])</title>
<link>http://arxiv.org/abs/1807.04428</link>
<description rdf:parseType="Literal">&lt;p&gt;Semidefinite programming (SDP) with equality constraints arise in many
optimization and machine learning problems, such as Max-Cut, community
detection and robust PCA. Although SDPs can be solved to arbitrary precision in
polynomial time, generic convex solvers do not scale well with the dimension of
the problem. In order to address this issue, Burer and Monteiro
\cite{burer2003nonlinear} proposed to reduce the dimension of the problem by
appealing to a low-rank factorization, and solve the subsequent non-convex
problem instead. It is well-understood that the resulting non-convex problem
acts as a reliable surrogate to the original SDP, and can be efficiently solved
using the block-coordinate maximization method. Despite its simplicity,
remarkable success, and wide use in practice, the theoretical understanding of
the convergence of this method is limited. We prove that the block-coordinate
maximization algorithm applied to the non-convex Burer-Monteiro approach enjoys
a global sublinear rate without any assumptions on the problem, and a local
linear convergence rate despite no local maxima is locally strongly concave. We
illustrate our results through examples and numerical experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Erdogdu_M/0/1/0/all/0/1&quot;&gt;Murat A. Erdogdu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ozdaglar_A/0/1/0/all/0/1&quot;&gt;Asuman Ozdaglar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Parrilo_P/0/1/0/all/0/1&quot;&gt;Pablo A. Parrilo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Vanli_N/0/1/0/all/0/1&quot;&gt;Nuri Denizcan Vanli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04431">
<title>Statistical Inference with Local Optima. (arXiv:1807.04431v1 [math.ST])</title>
<link>http://arxiv.org/abs/1807.04431</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the statistical properties of an estimator derived by applying a
gradient ascent method with multiple initializations to a multi-modal
likelihood function. We derive the population quantity that is the target of
this estimator and study the properties of confidence intervals (CIs)
constructed from asymptotic normality and the bootstrap approach. In
particular, we analyze the coverage deficiency due to finite number of random
initializations. We also investigate the CIs by inverting the likelihood ratio
test, the score test, and the Wald test, and we show that the resulting CIs may
be very different. We provide a summary of the uncertainties that we need to
consider while making inference about the population. Note that we do not
provide a solution to the problem of multiple local maxima; instead, our goal
is to investigate the effect from local maxima on the behavior of our
estimator. In addition, we analyze the performance of the EM algorithm under
random initializations and derive the coverage of a CI with a finite number of
initializations. Finally, we extend our analysis to a nonparametric mode
hunting problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yen-Chi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04489">
<title>Fast yet Simple Natural-Gradient Descent for Variational Inference in Complex Models. (arXiv:1807.04489v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.04489</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian inference plays an important role in advancing machine learning, but
faces computational challenges when applied to complex models such as deep
neural networks. Variational inference circumvents these challenges by
formulating Bayesian inference as an optimization problem and solving it using
gradient-based optimization. In this paper, we argue in favor of
natural-gradient approaches which, unlike their gradient-based counterparts,
can improve convergence by exploiting the information geometry of the
solutions. We show how to derive fast yet simple natural-gradient updates by
using a duality associated with exponential-family distributions. An attractive
feature of these methods is that, by using natural-gradients, they are able to
extract accurate local approximations for individual model components. We
summarize recent results for Bayesian deep learning showing the superiority of
natural-gradient approaches over their gradient counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nielsen_D/0/1/0/all/0/1&quot;&gt;Didrik Nielsen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04566">
<title>Decentralized Clustering on Compressed Data without Prior Knowledge of the Number of Clusters. (arXiv:1807.04566v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.04566</link>
<description rdf:parseType="Literal">&lt;p&gt;In sensor networks, it is not always practical to set up a fusion center.
Therefore, there is need for fully decentralized clustering algorithms.
Decentralized clustering algorithms should minimize the amount of data
exchanged between sensors in order to reduce sensor energy consumption. In this
respect, we propose one centralized and one decentralized clustering algorithm
that work on compressed data without prior knowledge of the number of clusters.
In the standard K-means clustering algorithm, the number of clusters is
estimated by repeating the algorithm several times, which dramatically
increases the amount of exchanged data, while our algorithm can estimate this
number in one run.
&lt;/p&gt;
&lt;p&gt;The proposed clustering algorithms derive from a theoretical framework
establishing that, under asymptotic conditions, the cluster centroids are the
only fixed-point of a cost function we introduce. This cost function depends on
a weight function which we choose as the p-value of a Wald hypothesis test.
This p-value measures the plausibility that a given measurement vector belongs
to a given cluster. Experimental results show that our two algorithms are
competitive in terms of clustering performance with respect to K-means and
DB-Scan, while lowering by a factor at least $2$ the amount of data exchanged
between sensors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dupraz_E/0/1/0/all/0/1&quot;&gt;Elsa Dupraz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pastor_D/0/1/0/all/0/1&quot;&gt;Dominique Pastor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Socheleau_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois-Xavier Socheleau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04594">
<title>The Incremental Proximal Method: A Probabilistic Perspective. (arXiv:1807.04594v1 [stat.CO])</title>
<link>http://arxiv.org/abs/1807.04594</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we highlight a connection between the incremental proximal
method and stochastic filters. We begin by showing that the proximal operators
coincide, and hence can be realized with, Bayes updates. We give the explicit
form of the updates for the linear regression problem and show that there is a
one-to-one correspondence between the proximal operator of the least-squares
regression and the Bayes update when the prior and the likelihood are Gaussian.
We then carry out this observation to a general sequential setting: We consider
the incremental proximal method, which is an algorithm for large-scale
optimization, and show that, for a linear-quadratic cost function, it can
naturally be realized by the Kalman filter. We then discuss the implications of
this idea for nonlinear optimization problems where proximal operators are in
general not realizable. In such settings, we argue that the extended Kalman
filter can provide a systematic way for the derivation of practical procedures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Akyildiz_O/0/1/0/all/0/1&quot;&gt;&amp;#xd6;mer Deniz Akyildiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Elvira_V/0/1/0/all/0/1&quot;&gt;Victor Elvira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Miguez_J/0/1/0/all/0/1&quot;&gt;Joaquin Miguez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04602">
<title>Rule Induction Partitioning Estimator. (arXiv:1807.04602v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.04602</link>
<description rdf:parseType="Literal">&lt;p&gt;RIPE is a novel deterministic and easily understandable prediction algorithm
developed for continuous and discrete ordered data. It infers a model, from a
sample, to predict and to explain a real variable $Y$ given an input variable
$X \in \mathcal X$ (features). The algorithm extracts a sparse set of
hyperrectangles $\mathbf r \subset \mathcal X$, which can be thought of as
rules of the form If-Then. This set is then turned into a partition of the
features space $\mathcal X$ of which each cell is explained as a list of rules
with satisfied their If conditions. The process of RIPE is illustrated on
simulated datasets and its efficiency compared with that of other usual
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Margot_V/0/1/0/all/0/1&quot;&gt;Vincent Margot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baudry_J/0/1/0/all/0/1&quot;&gt;Jean-Patrick Baudry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guilloux_F/0/1/0/all/0/1&quot;&gt;Frederic Guilloux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wintenberger_O/0/1/0/all/0/1&quot;&gt;Olivier Wintenberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04639">
<title>Moving Objects Analytics: Survey on Future Location &amp; Trajectory Prediction Methods. (arXiv:1807.04639v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04639</link>
<description rdf:parseType="Literal">&lt;p&gt;The tremendous growth of positioning technologies and GPS enabled devices has
produced huge volumes of tracking data during the recent years. This source of
information constitutes a rich input for data analytics processes, either
offline (e.g. cluster analysis, hot motion discovery) or online (e.g.
short-term forecasting of forthcoming positions). This paper focuses on
predictive analytics for moving objects (could be pedestrians, cars, vessels,
planes, animals, etc.) and surveys the state-of-the-art in the context of
future location and trajectory prediction. We provide an extensive review of
over 50 works, also proposing a novel taxonomy of predictive algorithms over
moving objects. We also list the properties of several real datasets used in
the past for validation purposes of those works and, motivated by this, we
discuss challenges that arise in the transition from conventional to Big Data
applications.
&lt;/p&gt;
&lt;p&gt;CCS Concepts: Information systems &amp;gt; Spatial-temporal systems; Information
systems &amp;gt; Data analytics; Information systems &amp;gt; Data mining; Computing
methodologies &amp;gt; Machine learning Additional Key Words and Phrases: mobility
data, moving object trajectories, trajectory prediction, future location
prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgiou_H/0/1/0/all/0/1&quot;&gt;Harris Georgiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karagiorgou_S/0/1/0/all/0/1&quot;&gt;Sophia Karagiorgou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kontoulis_Y/0/1/0/all/0/1&quot;&gt;Yannis Kontoulis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelekis_N/0/1/0/all/0/1&quot;&gt;Nikos Pelekis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrou_P/0/1/0/all/0/1&quot;&gt;Petros Petrou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarlatti_D/0/1/0/all/0/1&quot;&gt;David Scarlatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theodoridis_Y/0/1/0/all/0/1&quot;&gt;Yannis Theodoridis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04689">
<title>Explorations in Homeomorphic Variational Auto-Encoding. (arXiv:1807.04689v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1807.04689</link>
<description rdf:parseType="Literal">&lt;p&gt;The manifold hypothesis states that many kinds of high-dimensional data are
concentrated near a low-dimensional manifold. If the topology of this data
manifold is non-trivial, a continuous encoder network cannot embed it in a
one-to-one manner without creating holes of low density in the latent space.
This is at odds with the Gaussian prior assumption typically made in
Variational Auto-Encoders (VAEs), because the density of a Gaussian
concentrates near a blob-like manifold.
&lt;/p&gt;
&lt;p&gt;In this paper we investigate the use of manifold-valued latent variables.
Specifically, we focus on the important case of continuously differentiable
symmetry groups (Lie groups), such as the group of 3D rotations
$\operatorname{SO}(3)$. We show how a VAE with $\operatorname{SO}(3)$-valued
latent variables can be constructed, by extending the reparameterization trick
to compact connected Lie groups. Our experiments show that choosing
manifold-valued latent variables that match the topology of the latent data
manifold, is crucial to preserve the topological structure and learn a
well-behaved latent space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Falorsi_L/0/1/0/all/0/1&quot;&gt;Luca Falorsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Haan_P/0/1/0/all/0/1&quot;&gt;Pim de Haan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Davidson_T/0/1/0/all/0/1&quot;&gt;Tim R. Davidson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cao_N/0/1/0/all/0/1&quot;&gt;Nicola De Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Weiler_M/0/1/0/all/0/1&quot;&gt;Maurice Weiler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Forre_P/0/1/0/all/0/1&quot;&gt;Patrick Forr&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cohen_T/0/1/0/all/0/1&quot;&gt;Taco S. Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04709">
<title>Inferring Multi-Dimensional Rates of Aging from Cross-Sectional Data. (arXiv:1807.04709v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04709</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling how individuals evolve over time is a fundamental problem in the
natural and social sciences. However, existing datasets are often
cross-sectional with each individual only observed at a single timepoint,
making inference of temporal dynamics hard. Motivated by the study of human
aging, we present a model that can learn temporal dynamics from cross-sectional
data. Our model represents each individual with a low-dimensional latent state
that consists of 1) a dynamic vector $rt$ that evolves linearly with time $t$,
where $r$ is an individual-specific &quot;rate of aging&quot; vector, and 2) a static
vector $b$ that captures time-independent variation. Observed features are a
non-linear function of $rt$ and $b$. We prove that constraining the mapping
between $rt$ and a subset of the observed features to be order-isomorphic
yields a model class that is identifiable if the distribution of
time-independent variation is known. Our model correctly recovers the latent
rate vector $r$ in realistic synthetic data. Applied to the UK Biobank human
health dataset, our model accurately reconstructs the observed data while
learning interpretable rates of aging $r$ that are positively associated with
diseases, mortality, and aging risk factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pierson_E/0/1/0/all/0/1&quot;&gt;Emma Pierson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1&quot;&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koller_D/0/1/0/all/0/1&quot;&gt;Daphne Koller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eriksson_N/0/1/0/all/0/1&quot;&gt;Nicholas Eriksson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04715">
<title>Orthogonal Matching Pursuit for Text Classification. (arXiv:1807.04715v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04715</link>
<description rdf:parseType="Literal">&lt;p&gt;In text classification, the problem of overfitting arises due to the high
dimensionality, making regularization essential. Although classic regularizers
provide sparsity, they fail to return highly accurate models. On the contrary,
state-of-the-art group-lasso regularizers provide better results at the expense
of low sparsity. In this paper, we apply a greedy variable selection algorithm,
called Orthogonal Matching Pursuit, for the text classification task. We also
extend standard group OMP by introducing overlapping group OMP to handle
overlapping groups of features. Empirical analysis verifies that both OMP and
overlapping GOMP constitute powerful regularizers, able to produce effective
and super-sparse models. Code and data are available at:
https://www.dropbox.com/sh/7w7hjns71ol0xrz/AAC\string_G0\string_0DlcGkq6tQb2zqAaca\string?dl\string=0 .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skianis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Skianis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tziortziotis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Tziortziotis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1&quot;&gt;Michalis Vazirgiannis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04720">
<title>The GAN Landscape: Losses, Architectures, Regularization, and Normalization. (arXiv:1807.04720v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04720</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) are a class of deep generative models
which aim to learn a target distribution in an unsupervised fashion. While they
were successfully applied to many problems, training a GAN is a notoriously
challenging task and requires a significant amount of hyperparameter tuning,
neural architecture engineering, and a non-trivial amount of &quot;tricks&quot;. The
success in many practical applications coupled with the lack of a measure to
quantify the failure modes of GANs resulted in a plethora of proposed losses,
regularization and normalization schemes, and neural architectures. In this
work we take a sober view of the current state of GANs from a practical
perspective. We reproduce the current state of the art and go beyond fairly
exploring the GAN landscape. We discuss common pitfalls and reproducibility
issues, open-source our code on Github, and provide pre-trained models on
TensorFlow Hub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurach_K/0/1/0/all/0/1&quot;&gt;Karol Kurach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1&quot;&gt;Mario Lucic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1&quot;&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michalski_M/0/1/0/all/0/1&quot;&gt;Marcin Michalski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gelly_S/0/1/0/all/0/1&quot;&gt;Sylvain Gelly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04734">
<title>Scalable Convolutional Dictionary Learning with Constrained Recurrent Sparse Auto-encoders. (arXiv:1807.04734v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04734</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a convolutional dictionary underlying a set of observed signals, can a
carefully designed auto-encoder recover the dictionary in the presence of
noise? We introduce an auto-encoder architecture, termed constrained recurrent
sparse auto-encoder (CRsAE), that answers this question in the affirmative.
Given an input signal and an approximate dictionary, the encoder finds a sparse
approximation using FISTA. The decoder reconstructs the signal by applying the
dictionary to the output of the encoder. The encoder and decoder in CRsAE
parallel the sparse-coding and dictionary update steps in optimization-based
alternating-minimization schemes for dictionary learning. As such, the
parameters of the encoder and decoder are not independent, a constraint which
we enforce for the first time. We derive the back-propagation algorithm for
CRsAE. CRsAE is a framework for blind source separation that, only knowing the
number of sources (dictionary elements), and assuming sparsely-many can
overlap, is able to separate them. We demonstrate its utility in the context of
spike sorting, a source separation problem in computational neuroscience. We
demonstrate the ability of CRsAE to recover the underlying dictionary and
characterize its sensitivity as a function of SNR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolooshams_B/0/1/0/all/0/1&quot;&gt;Bahareh Tolooshams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1&quot;&gt;Sourav Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_D/0/1/0/all/0/1&quot;&gt;Demba Ba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04742">
<title>Visual Reinforcement Learning with Imagined Goals. (arXiv:1807.04742v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1807.04742</link>
<description rdf:parseType="Literal">&lt;p&gt;For an autonomous agent to fulfill a wide range of user-specified goals at
test time, it must be able to learn broadly applicable and general-purpose
skill repertoires. Furthermore, to provide the requisite level of generality,
these skills must handle raw sensory input such as images. In this paper, we
propose an algorithm that acquires such general-purpose skills by combining
unsupervised representation learning and reinforcement learning of
goal-conditioned policies. Since the particular goals that might be required at
test-time are not known in advance, the agent performs a self-supervised
&quot;practice&quot; phase where it imagines goals and attempts to achieve them. We learn
a visual representation with three distinct purposes: sampling goals for
self-supervised practice, providing a structured transformation of raw sensory
inputs, and computing a reward signal for goal reaching. We also propose a
retroactive goal relabeling scheme to further improve the sample-efficiency of
our method. Our off-policy algorithm is efficient enough to learn policies that
operate on raw image observations and goals for a real-world robotic system,
and substantially outperforms prior techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1&quot;&gt;Ashvin Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1&quot;&gt;Vitchyr Pong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalal_M/0/1/0/all/0/1&quot;&gt;Murtaza Dalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahl_S/0/1/0/all/0/1&quot;&gt;Shikhar Bahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Steven Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1705.00105">
<title>Representation Learning and Pairwise Ranking for Implicit Feedback in Recommendation Systems. (arXiv:1705.00105v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1705.00105</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel ranking framework for collaborative
filtering with the overall aim of learning user preferences over items by
minimizing a pairwise ranking loss. We show the minimization problem involves
dependent random variables and provide a theoretical analysis by proving the
consistency of the empirical risk minimization in the worst case where all
users choose a minimal number of positive and negative items. We further derive
a Neural-Network model that jointly learns a new representation of users and
items in an embedded space as well as the preference relation of users over the
pairs of items. The learning objective is based on three scenarios of ranking
losses that control the ability of the model to maintain the ordering over the
items induced from the users&apos; preferences, as well as, the capacity of the
dot-product defined in the learned embedded space to produce the ordering. The
proposed model is by nature suitable for implicit feedback and involves the
estimation of only very few parameters. Through extensive experiments on
several real-world benchmarks on implicit data, we show the interest of
learning the preference and the embedding simultaneously when compared to
learning those separately. We also demonstrate that our approach is very
competitive with the best state-of-the-art collaborative filtering techniques
proposed for implicit feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sidana_S/0/1/0/all/0/1&quot;&gt;Sumit Sidana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trofimov_M/0/1/0/all/0/1&quot;&gt;Mikhail Trofimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Horodnitskii_O/0/1/0/all/0/1&quot;&gt;Oleg Horodnitskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laclau_C/0/1/0/all/0/1&quot;&gt;Charlotte Laclau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maximov_Y/0/1/0/all/0/1&quot;&gt;Yury Maximov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Amini_M/0/1/0/all/0/1&quot;&gt;Massih-Reza Amini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.07600">
<title>Belief Propagation Min-Sum Algorithm for Generalized Min-Cost Network Flow. (arXiv:1710.07600v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1710.07600</link>
<description rdf:parseType="Literal">&lt;p&gt;Belief Propagation algorithms are instruments used broadly to solve graphical
model optimization and statistical inference problems. In the general case of a
loopy Graphical Model, Belief Propagation is a heuristic which is quite
successful in practice, even though its empirical success, typically, lacks
theoretical guarantees. This paper extends the short list of special cases
where correctness and/or convergence of a Belief Propagation algorithm is
proven. We generalize formulation of Min-Sum Network Flow problem by relaxing
the flow conservation (balance) constraints and then proving that the Belief
Propagation algorithm converges to the exact result.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Riazanov_A/0/1/0/all/0/1&quot;&gt;Andrii Riazanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maximov_Y/0/1/0/all/0/1&quot;&gt;Yury Maximov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chertkov_M/0/1/0/all/0/1&quot;&gt;Michael Chertkov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08240">
<title>Estimating activity cycles with probabilistic methods II. The Mount Wilson Ca H&amp;K data. (arXiv:1712.08240v4 [astro-ph.SR] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08240</link>
<description rdf:parseType="Literal">&lt;p&gt;Debate over the existence of branches in the stellar activity-rotation
diagrams continues. Application of modern time series analysis tools to study
the mean cycle periods in chromospheric activity index is lacking. We develop
such models, based on Gaussian processes, for one-dimensional time series and
apply it to the extended Mount Wilson Ca H&amp;amp;K sample. Our main aim is to study
how the previously commonly used assumption of strict harmonicity of the
stellar cycles as well as handling of the linear trends affects the results. We
introduce three methods of different complexity, starting with the simple
Bayesian harmonic model and followed by Gaussian Process models with periodic
and quasi-periodic covariance functions. We confirm the existence of two
populations in the activity-period diagram. We find only one significant trend
in the inactive population, namely that the cycle periods get shorter with
increasing rotation. This is in contrast with earlier studies, that postulate
the existence of trends in both of the populations. In terms of rotation to
cycle period ratio, our data is consistent with only two activity branches such
that the active branch merges together with the transitional one. The retrieved
stellar cycles are uniformly distributed over the R&apos;HK activity index,
indicating that the operation of stellar large-scale dynamos carries smoothly
over the Vaughan-Preston gap. At around the solar activity index, however,
indications of a disruption in the cyclic dynamo action are seen. Our study
shows that stellar cycle estimates depend significantly on the model applied.
Such model-dependent aspects include the improper treatment of linear trends,
while the assumption of strict harmonicity can result in the appearance of
double cyclicities that seem more likely to be explained by the
quasi-periodicity of the cycles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Olspert_N/0/1/0/all/0/1&quot;&gt;N. Olspert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Lehtinen_J/0/1/0/all/0/1&quot;&gt;J. Lehtinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kapyla_M/0/1/0/all/0/1&quot;&gt;M. J. K&amp;#xe4;pyl&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Pelt_J/0/1/0/all/0/1&quot;&gt;J. Pelt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Grigorievskiy_A/0/1/0/all/0/1&quot;&gt;A. Grigorievskiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.06300">
<title>Exact and Robust Conformal Inference Methods for Predictive Machine Learning With Dependent Data. (arXiv:1802.06300v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.06300</link>
<description rdf:parseType="Literal">&lt;p&gt;We extend conformal inference to general settings that allow for time series
data. Our proposal is developed as a randomization method and accounts for
potential serial dependence by including block structures in the permutation
scheme. As a result, the proposed method retains the exact, model-free validity
when the data are i.i.d. or more generally exchangeable, similar to usual
conformal inference methods. When exchangeability fails, as is the case for
common time series data, the proposed approach is approximately valid under
weak assumptions on the conformity score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chernozhukov_V/0/1/0/all/0/1&quot;&gt;Victor Chernozhukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wuthrich_K/0/1/0/all/0/1&quot;&gt;Kaspar Wuthrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinchu Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.08318">
<title>Proportional Volume Sampling and Approximation Algorithms for A-Optimal Design. (arXiv:1802.08318v4 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1802.08318</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the optimal design problems where the goal is to choose a set of
linear measurements to obtain the most accurate estimate of an unknown vector
in d dimensions. We study the A-optimal design variant where the objective is
to minimize the average variance of the error in the maximum likelihood
estimate of the vector being measured. The problem also finds applications in
sensor placement in wireless networks, sparse least squares regression, feature
selection for k-means clustering, and matrix approximation. In this paper, we
introduce proportional volume sampling to obtain improved approximation
algorithms for A-optimal design. Our main result is to obtain improved
approximation algorithms for the A-optimal design problem by introducing the
proportional volume sampling algorithm. Our results nearly optimal bounds in
the asymptotic regime when the number of measurements done, k, is significantly
more than the dimension d. We also give first approximation algorithms when k
is small including when k=d. The proportional volume-sampling algorithm also
gives approximation algorithms for other optimal design objectives such as
D-optimal design and generalized ratio objective matching or improving previous
best known results. Interestingly, we show that a similar guarantee cannot be
obtained for the E-optimal design problem. We also show that the A-optimal
design problem is \NP-hard to approximate within a fixed constant when k=d.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolov_A/0/1/0/all/0/1&quot;&gt;Aleksandar Nikolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Mohit Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tantipongpipat_U/0/1/0/all/0/1&quot;&gt;Uthaipon Tao Tantipongpipat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.06320">
<title>Synchronisation of Partial Multi-Matchings via Non-negative Factorisations. (arXiv:1803.06320v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1803.06320</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we study permutation synchronisation for the challenging case of
partial permutations, which plays an important role for the problem of matching
multiple objects (e.g. images or shapes). The term synchronisation refers to
the property that the set of pairwise matchings is cycle-consistent, i.e. in
the full matching case all compositions of pairwise matchings over cycles must
be equal to the identity. Motivated by clustering and matrix factorisation
perspectives of cycle-consistency, we derive an algorithm to tackle the
permutation synchronisation problem based on non-negative factorisations. In
order to deal with the inherent non-convexity of the permutation
synchronisation problem, we use an initialisation procedure based on a novel
rotation scheme applied to the solution of the spectral relaxation. Moreover,
this rotation scheme facilitates a convenient Euclidean projection to obtain a
binary solution after solving our relaxed problem. In contrast to
state-of-the-art methods, our approach is guaranteed to produce
cycle-consistent results. We experimentally demonstrate the efficacy of our
method and show that it achieves better results compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1&quot;&gt;Florian Bernard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thunberg_J/0/1/0/all/0/1&quot;&gt;Johan Thunberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncalves_J/0/1/0/all/0/1&quot;&gt;Jorge Goncalves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.06021">
<title>The return of $\epsilon$-greedy: sublinear regret for model-free linear quadratic control. (arXiv:1804.06021v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1804.06021</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-free approaches for reinforcement learning (RL) and continuous control
find policies based only on past states and rewards, without fitting a model of
the system dynamics. They are appealing as they are general purpose and easy to
implement; however, they also come with fewer theoretical guarantees than
model-based RL. In this work, we present a new model-free algorithm for
controlling linear quadratic (LQ) systems, and show that its regret scales as
$O(T^{3/4})$. The algorithm is based on a reduction of control of Markov
decision processes to an expert prediction problem. In practice, it corresponds
to a variant of policy iteration with $\epsilon$-greedy exploration, where the
policy is greedy with respect to the average of all previous value functions.
This is the first model-free algorithm for adaptive control of LQ systems that
provably achieves sublinear regret and has a polynomial computation cost.
Empirically, our algorithm dramatically outperforms standard policy iteration,
but performs worse than a model-based approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasi_Yadkori_Y/0/1/0/all/0/1&quot;&gt;Yasin Abbasi-Yadkori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazic_N/0/1/0/all/0/1&quot;&gt;Nevena Lazic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1&quot;&gt;Csaba Szepesvari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1804.09046">
<title>Developing a machine learning framework for estimating soil moisture with VNIR hyperspectral data. (arXiv:1804.09046v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1804.09046</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the potential of estimating the soil-moisture
content based on VNIR hyperspectral data combined with LWIR data. Measurements
from a multi-sensor field campaign represent the benchmark dataset which
contains measured hyperspectral, LWIR, and soil-moisture data conducted on
grassland site. We introduce a regression framework with three steps consisting
of feature selection, preprocessing, and well-chosen regression models. The
latter are mainly supervised machine learning models. An exception are the
self-organizing maps which combine unsupervised and supervised learning. We
analyze the impact of the distinct preprocessing methods on the regression
results. Of all regression models, the extremely randomized trees model without
preprocessing provides the best estimation performance. Our results reveal the
potential of the respective regression framework combined with the VNIR
hyperspectral data to estimate soil moisture measured under real-world
conditions. In conclusion, the results of this paper provide a basis for
further improvements in different research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_S/0/1/0/all/0/1&quot;&gt;Sina Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riese_F/0/1/0/all/0/1&quot;&gt;Felix M. Riese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stotzer_J/0/1/0/all/0/1&quot;&gt;Johanna St&amp;#xf6;tzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_P/0/1/0/all/0/1&quot;&gt;Philipp M. Maier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hinz_S/0/1/0/all/0/1&quot;&gt;Stefan Hinz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.00794">
<title>ECG Heartbeat Classification: A Deep Transferable Representation. (arXiv:1805.00794v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/1805.00794</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrocardiogram (ECG) can be reliably used as a measure to monitor the
functionality of the cardiovascular system. Recently, there has been a great
attention towards accurate categorization of heartbeats. While there are many
commonalities between different ECG conditions, the focus of most studies has
been classifying a set of conditions on a dataset annotated for that task
rather than learning and employing a transferable knowledge between different
tasks. In this paper, we propose a method based on deep convolutional neural
networks for the classification of heartbeats which is able to accurately
classify five different arrhythmias in accordance with the AAMI EC57 standard.
Furthermore, we suggest a method for transferring the knowledge acquired on
this task to the myocardial infarction (MI) classification task. We evaluated
the proposed method on PhysionNet&apos;s MIT-BIH and PTB Diagnostics datasets.
According to the results, the suggested method is able to make predictions with
the average accuracies of 93.4% and 95.9% on arrhythmia classification and MI
classification, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kachuee_M/0/1/0/all/0/1&quot;&gt;Mohammad Kachuee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazeli_S/0/1/0/all/0/1&quot;&gt;Shayan Fazeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarrafzadeh_M/0/1/0/all/0/1&quot;&gt;Majid Sarrafzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.07574">
<title>Chief complaint classification with recurrent neural networks. (arXiv:1805.07574v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.07574</link>
<description rdf:parseType="Literal">&lt;p&gt;Syndromic surveillance detects and monitors individual and population health
indicators through sources such as emergency department records. Automated
classification of these records can improve outbreak detection speed and
diagnosis accuracy. Current syndromic systems rely on hand-coded keyword-based
methods to parse written fields and may benefit from the use of modern
supervised-learning classifier models. In this paper we implement two recurrent
neural network models based on long short-term memory (LSTM) and gated
recurrent unit (GRU) cells and compare them to two traditional bag-of-words
classifiers: multinomial naive Bayes (MNB) and a support vector machine (SVM).
The MNB classifier is one of only two machine learning algorithms currently
being used for syndromic surveillance. All four models are trained to predict
diagnostic code groups as defined by Clinical Classification Software, first to
predict from discharge diagnosis, then from chief complaint fields. The
classifiers are trained on 3.6 million de-identified emergency department
records from a single United States jurisdiction. We compare performance of
these models primarily using the F1 score. Using discharge diagnoses, the LSTM
classifier performs best, though all models exhibit an F1 score above 96.00.
The GRU performs best on chief complaints (F1=47.38), and MNB with bigrams
performs worst (F1=39.40). Certain syndrome types are easier to detect than
others. For examples, chief complaints using the GRU model predicts
alcohol-related disorders well (F1=78.91) but predicts influenza poorly
(F1=14.80). In all instances, the RNN models outperformed the bag-of-word
classifiers, suggesting deep learning models could substantially improve the
automatic classification of unstructured text for syndromic surveillance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Scott H Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Levin_D/0/1/0/all/0/1&quot;&gt;Drew Levin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Finley_P/0/1/0/all/0/1&quot;&gt;Pat Finley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Heilig_C/0/1/0/all/0/1&quot;&gt;Charles M Heilig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.12332">
<title>On representation power of neural network-based graph embedding and beyond. (arXiv:1805.12332v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1805.12332</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the representation power of siamese-style similarity functions
used in neural network-based graph embedding. The inner product similarity
(IPS) with feature vectors computed via neural networks is commonly used for
representing the strength of association between two nodes. However, only a
little work has been done on the representation capability of IPS. A very
recent work shed light on the nature of IPS and reveals that IPS has the
capability of approximating any positive definite (PD) similarities. However, a
simple example demonstrates the fundamental limitation of IPS to approximate
non-PD similarities. We then propose a novel model named Shifted IPS (SIPS)
that approximates any Conditionally PD (CPD) similarities arbitrary well. CPD
is a generalization of PD with many examples such as negative Poincar\&apos;e
distance and negative Wasserstein distance, thus SIPS has a potential impact to
significantly improve the applicability of graph embedding without taking great
care in configuring the similarity function. Our numerical experiments
demonstrate the SIPS&apos;s superiority over IPS. In theory, we further extend SIPS
beyond CPD by considering the inner product in Minkowski space so that it
approximates more general similarities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Okuno_A/0/1/0/all/0/1&quot;&gt;Akifumi Okuno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shimodaira_H/0/1/0/all/0/1&quot;&gt;Hidetoshi Shimodaira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.03467">
<title>Orthogonal Random Forest for Heterogeneous Treatment Effect Estimation. (arXiv:1806.03467v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1806.03467</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of estimating heterogeneous treatment effects from
observational data, where the treatment policy on the collected data was
determined by potentially many confounding observable variables. We propose
orthogonal random forest, an algorithm that combines orthogonalization, a
technique that effectively removes the confounding effect in two-stage
estimation, with generalized random forests [Athey et al., 2017], a flexible
method for estimating treatment effect heterogeneity. We prove a consistency
rate result of our estimator in the partially linear regression model, and en
route we provide a consistency analysis for a general framework of performing
generalized method of moments (GMM) estimation. We also provide a comprehensive
empirical evaluation of our algorithms, and show that they consistently
outperform baseline approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oprescu_M/0/1/0/all/0/1&quot;&gt;Miruna Oprescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Steven Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.04594">
<title>Exponential Weights on the Hypercube in Polynomial Time. (arXiv:1806.04594v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1806.04594</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a general online linear optimization problem(OLO). At each round, a
subset of objects from a fixed universe of $n$ objects is chosen, and a linear
cost associated with the chosen subset is incurred. We use \textit{regret} as a
measure of performance of our algorithms. Regret is the difference between the
total cost incurred over all iterations and the cost of the best fixed subset
in hindsight. We consider \textit{Full Information}, \textit{Semi-Bandit} and
\textit{Bandit} feedback for this problem. Using characteristic vectors of the
subsets, this problem reduces to OLO on the $\{0,1\}^n$ hypercube. The Exp2
algorithm and its bandit variants are commonly used strategies for this
problem. It was previously unknown if it is possible to run Exp2 on the
hypercube in polynomial time.
&lt;/p&gt;
&lt;p&gt;In this paper, we present a polynomial time algorithm called \textit{PolyExp}
for OLO on the hypercube. We show that our algorithm is equivalent to both Exp2
on $\{0,1\}^n$ as well as Online Mirror Descent(OMD) with Entropic
regularization on $[0,1]^n$ and Bernoulli Sampling. Under $L_\infty$
adversarial losses, in the Full Information case and Semi-Bandit case,
analyzing Exp2 directly, gives an expected regret bound of
$O(n^{3/2}\sqrt{T})$, whereas PolyExp yields a regret of $O(n\sqrt{T})$. In the
Bandit case, analyzing Exp2 directly, gives an expected regret bound of
$O(n^{2}\sqrt{T})$, whereas PolyExp yields a regret of $O(n^{3/2}\sqrt{T})$.
This implies an improvement on Exp2&apos;s regret bound for these settings because
of the equivalence. Moreover, PolyExp is minimax optimal in all the three
settings as its regret bounds match the $L_\infty$ lowerbounds in
\cite{audibert2011minimax}. Finally, we show how to use PolyExp on the
$\{-1,+1\}^n$ hypercube, solving an open problem in \cite{bubeck2012towards}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Putta_S/0/1/0/all/0/1&quot;&gt;Sudeep Raja Putta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00243">
<title>chemmodlab: A Cheminformatics Modeling Laboratory for Fitting and Assessing Machine Learning Models. (arXiv:1807.00243v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00243</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of chemmodlab is to streamline the fitting and assessment pipeline
for many machine learning models in R, making it easy for researchers to
compare the utility of new models. While focused on implementing methods for
model fitting and assessment that have been accepted by experts in the
cheminformatics field, all of the methods in chemmodlab have broad utility for
the machine learning community. chemmodlab contains several assessment
utilities including a plotting function that constructs accumulation curves and
a function that computes many performance measures. The most novel feature of
chemmodlab is the ease with which statistically significant performance
differences for many machine learning models is presented by means of the
multiple comparisons similarity plot. Differences are assessed using repeated
k-fold cross validation where blocking increases precision and multiplicity
adjustments are applied.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hughes_Oliver_J/0/1/0/all/0/1&quot;&gt;Jeremy R. Ash Jacqueline M. Hughes-Oliver&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.01085">
<title>One-Class Kernel Spectral Regression for Outlier Detection. (arXiv:1807.01085v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.01085</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper introduces a new efficient nonlinear one-class classifier
formulated as the Rayleigh quotient criterion. The method, operating in a
reproducing kernel Hilbert subspace, minimises the scatter of target
distribution along an optimal projection direction while at the same time
keeping projections of positive observations as distant as possible from the
mean of the negative class. We provide a graph embedding view of the problem
which can then be solved efficiently using the spectral regression approach. In
this sense, unlike previous similar methods which often require costly
eigen-computations of dense matrices, the proposed approach casts the problem
under consideration into a regression framework which avoids
eigen-decomposition computations. In particular, it is shown that the dominant
complexity of the proposed method is the complexity of computing the kernel
matrix. Additional appealing characteristics of the proposed one-class
classifier are: 1-the ability to be trained in an incremental fashion (allowing
for application in streaming data scenarios while also reducing computational
complexity in a non-streaming operation mode); 2-being unsupervised while also
providing the functionality for refining the solution using negative training
examples, in case available; And last but not least 3-the deployment of the
kernel trick allowing for nonlinearly mapping the data into a high-dimensional
feature space. Extensive experiments conducted on several datasets verify the
merits of the proposed approach in comparison with some other alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arashloo_S/0/1/0/all/0/1&quot;&gt;Shervin Rahimzadeh Arashloo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1&quot;&gt;Josef Kittler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03431">
<title>A New Variational Model for Binary Classification in the Supervised Learning Context. (arXiv:1807.03431v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03431</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine the supervised learning problem in its continuous setting and give
a general optimality condition through techniques of functional analysis and
the calculus of variations. This enables us to solve the optimality condition
for the desired function u numerically and make several comparisons with other
widely utilized supervised learning models. We employ the accuracy and area
under the receiver operating characteristic curve as metrics of the
performance. Finally, 3 analyses are conducted based on these two mentioned
metrics where we compare the models and make conclusions to determine whether
or not our method is competitive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pacheco_C/0/1/0/all/0/1&quot;&gt;Carlos David Brito Pacheco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loeza_C/0/1/0/all/0/1&quot;&gt;Carlos Francisco Brito Loeza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.04222">
<title>Modified Regularized Dual Averaging Method for Training Sparse Convolutional Neural Networks. (arXiv:1807.04222v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1807.04222</link>
<description rdf:parseType="Literal">&lt;p&gt;We proposed a modified regularized dual averaging method for training sparse
deep convolutional neural networks. The regularized dual averaging method has
been proven to be effective in obtaining sparse solutions in convex
optimization problems, but not applied to deep learning fields before. We
analyzed the new version in convex conditions and prove the convergence of it.
The modified method can obtain more sparse solutions than traditional sparse
optimization methods such as proximal-SGD, while keeping almost the same
accuracy as stochastic gradient method with momentum on certain datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaodong Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Juncai He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jinchao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1806.11424">
<title>Understanding Fashionability: What drives sales of a style?. (arXiv:1806.11424v1 [cs.IR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1806.11424</link>
<description rdf:parseType="Literal">&lt;p&gt;We use customer demand data for fashion articles on Myntra, and derive a
fashionability or style quotient, which represents customer demand for the
stylistic content of a fashion article, decoupled with its commercials (price,
offers, etc.). We demonstrate learning for assortment planning in fashion that
would aim to keep a healthy mix of breadth and depth across various styles, and
we show the relationship between a customer&apos;s perception of a style vs a
merchandiser&apos;s catalogue of styles. We also backtest our method to calculate
prediction errors in our style quotient and customer demand, and discuss
various implications and findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Aniket Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_Y/0/1/0/all/0/1&quot;&gt;Yadunath Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Pawan Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1&quot;&gt;Aruna Rajan&lt;/a&gt;</dc:creator>
</item></rdf:RDF>