<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-01-23T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07650"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07668"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07357"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07384"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07411"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07440"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07653"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07654"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07674"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06588"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07292"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07318"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07389"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07426"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07615"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07644"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07691"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1511.02476"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.02883"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.04135"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.04969"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1801.07650">
<title>Dynamic Optimization of Neural Network Structures Using Probabilistic Modeling. (arXiv:1801.07650v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.07650</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are powerful machine learning models and have
succeeded in various artificial intelligence tasks. Although various
architectures and modules for the DNNs have been proposed, selecting and
designing the appropriate network structure for a target problem is a
challenging task. In this paper, we propose a method to simultaneously optimize
the network structure and weight parameters during neural network training. We
consider a probability distribution that generates network structures, and
optimize the parameters of the distribution instead of directly optimizing the
network structure. The proposed method can apply to the various network
structure optimization problems under the same framework. We apply the proposed
method to several structure optimization problems such as selection of layers,
selection of unit types, and selection of connections using the MNIST,
CIFAR-10, and CIFAR-100 datasets. The experimental results show that the
proposed method can find the appropriate and competitive network structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirakawa_S/0/1/0/all/0/1&quot;&gt;Shinichi Shirakawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwata_Y/0/1/0/all/0/1&quot;&gt;Yasushi Iwata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akimoto_Y/0/1/0/all/0/1&quot;&gt;Youhei Akimoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07668">
<title>Pruning Techniques for Mixed Ensembles of Genetic Programming Models. (arXiv:1801.07668v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1801.07668</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective of this paper is to define an effective strategy for building
an ensemble of Genetic Programming (GP) models. Ensemble methods are widely
used in machine learning due to their features: they average out biases, they
reduce the variance and they usually generalize better than single models.
Despite these advantages, building ensemble of GP models is not a
well-developed topic in the evolutionary computation community. To fill this
gap, we propose a strategy that blends individuals produced by standard
syntax-based GP and individuals produced by geometric semantic genetic
programming, one of the newest semantics-based method developed in GP. In fact,
recent literature showed that combining syntax and semantics could improve the
generalization ability of a GP model. Additionally, to improve the diversity of
the GP models used to build up the ensemble, we propose different pruning
criteria that are based on correlation and entropy, a commonly used measure in
information theory. Experimental results,obtained over different complex
problems, suggest that the pruning criteria based on correlation and entropy
could be effective in improving the generalization ability of the ensemble
model and in reducing the computational burden required to build it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castelli_M/0/1/0/all/0/1&quot;&gt;Mauro Castelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncalves_I/0/1/0/all/0/1&quot;&gt;Ivo Gon&amp;#xe7;alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manzoni_L/0/1/0/all/0/1&quot;&gt;Luca Manzoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanneschi_L/0/1/0/all/0/1&quot;&gt;Leonardo Vanneschi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07357">
<title>CHALET: Cornell House Agent Learning Environment. (arXiv:1801.07357v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.07357</link>
<description rdf:parseType="Literal">&lt;p&gt;We present CHALET, a 3D house simulator with support for navigation and
manipulation. CHALET includes 58 rooms and 10 house configuration, and allows
to easily create new house and room layouts. CHALET supports a range of common
household activities, including moving objects, toggling appliances, and
placing objects inside closeable containers. The environment and actions
available are designed to create a challenging domain to train and evaluate
autonomous agents, including for tasks that combine language, vision, and
planning in a dynamic environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1&quot;&gt;Claudia Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1&quot;&gt;Dipendra Misra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennnett_A/0/1/0/all/0/1&quot;&gt;Andrew Bennnett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walsman_A/0/1/0/all/0/1&quot;&gt;Aaron Walsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1&quot;&gt;Yoav Artzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07384">
<title>Hybrid Gradient Boosting Trees and NeuralNetworks for Forecasting Operating Room Data. (arXiv:1801.07384v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.07384</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series data constitutes a distinct and growing problem in machine
learning. As the corpus of time series data grows larger, deep models that
simultaneously learn features and classify with these features can be
intractable or suboptimal. In this paper, we present feature learning via long
short term memory (LSTM) networks and prediction via gradient boosting trees
(XGB). Focusing on the consequential setting of electronic health record data,
we predict the occurrence of hypoxemia five minutes into the future based on
past features. We make two observations: 1) long short term memory networks are
effective at capturing long term dependencies based on a single feature and 2)
gradient boosting trees are capable of tractably combining a large number of
features including static features like height and weight. With these
observations in mind, we generate features by performing &quot;supervised&quot;
representation learning with LSTM networks. Augmenting the original XGB model
with these features gives significantly better performance than either
individual method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hugh Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lundberg_S/0/1/0/all/0/1&quot;&gt;Scott Lundberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Su-In Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07411">
<title>Comparison Training for Computer Chinese Chess. (arXiv:1801.07411v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.07411</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes the application of comparison training (CT) for
automatic feature weight tuning, with the final objective of improving the
evaluation functions used in Chinese chess programs. First, we propose an
n-tuple network to extract features, since n-tuple networks require very little
expert knowledge through its large numbers of features, while simulta-neously
allowing easy access. Second, we propose a novel evalua-tion method that
incorporates tapered eval into CT. Experiments show that with the same features
and the same Chinese chess program, the automatically tuned comparison training
feature weights achieved a win rate of 86.58% against the weights that were
hand-tuned. The above trained version was then improved by adding additional
features, most importantly n-tuple features. This improved version achieved a
win rate of 81.65% against the trained version without additional features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseng_W/0/1/0/all/0/1&quot;&gt;Wen-Jie Tseng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jr-Chang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_I/0/1/0/all/0/1&quot;&gt;I-Chen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1&quot;&gt;Tinghan Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07440">
<title>Curiosity-driven reinforcement learning with homeostatic regulation. (arXiv:1801.07440v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1801.07440</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a curiosity reward based on information theory principles and
consistent with the animal instinct to maintain certain critical parameters
within a bounded range. Our experimental validation shows the added value of
the additional homeostatic drive to enhance the overall information gain of a
reinforcement learning agent interacting with a complex environment using
continuous actions. Our method builds upon two ideas: i) To take advantage of a
new Bellman-like equation of information gain and ii) to simplify the
computation of the local rewards by avoiding the approximation of complex
distributions over continuous states and actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abril_I/0/1/0/all/0/1&quot;&gt;Ildefons Magrans de Abril&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanai_R/0/1/0/all/0/1&quot;&gt;Ryota Kanai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07653">
<title>CaosDB - Research Data Management for Complex, Changing, and Automated Research Workflows. (arXiv:1801.07653v1 [cs.DB])</title>
<link>http://arxiv.org/abs/1801.07653</link>
<description rdf:parseType="Literal">&lt;p&gt;Here we present CaosDB, a Research Data Management System (RDMS) designed to
ensure seamless integration of inhomogeneous data sources and repositories of
legacy data. Its primary purpose is the management of data from biomedical
sciences, both from simulations and experiments during the complete research
data lifecycle. An RDMS for this domain faces particular challenges: Research
data arise in huge amounts, from a wide variety of sources, and traverse a
highly branched path of further processing. To be accepted by its users, an
RDMS must be built around workflows of the scientists and practices and thus
support changes in workflow and data structure. Nevertheless it should
encourage and support the development and observation of standards and
furthermore facilitate the automation of data acquisition and processing with
specialized software. The storage data model of an RDMS must reflect these
complexities with appropriate semantics and ontologies while offering simple
methods for finding, retrieving, and understanding relevant data. We show how
CaosDB responds to these challenges and give an overview of the CaosDB Server,
its data model and its easy-to-learn CaosDB Query Language. We briefly discuss
the status of the implementation, how we currently use CaosDB, and how we plan
to use and extend it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fitschen_T/0/1/0/all/0/1&quot;&gt;Timm Fitschen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlemmer_A/0/1/0/all/0/1&quot;&gt;Alexander Schlemmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hornung_D/0/1/0/all/0/1&quot;&gt;Daniel Hornung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worden_H/0/1/0/all/0/1&quot;&gt;Henrik tom W&amp;#xf6;rden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parlitz_U/0/1/0/all/0/1&quot;&gt;Ulrich Parlitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luther_S/0/1/0/all/0/1&quot;&gt;Stefan Luther&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07654">
<title>Expectation Learning for Adaptive Crossmodal Stimuli Association. (arXiv:1801.07654v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.07654</link>
<description rdf:parseType="Literal">&lt;p&gt;The human brain is able to learn, generalize, and predict crossmodal stimuli.
Learning by expectation fine-tunes crossmodal processing at different levels,
thus enhancing our power of generalization and adaptation in highly dynamic
environments. In this paper, we propose a deep neural architecture trained by
using expectation learning accounting for unsupervised learning tasks. Our
learning model exhibits a self-adaptable behavior, setting the first steps
towards the development of deep learning architectures for crossmodal stimuli
association.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barros_P/0/1/0/all/0/1&quot;&gt;Pablo Barros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parisi_G/0/1/0/all/0/1&quot;&gt;German I. Parisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Di Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07674">
<title>A Classification Refinement Strategy for Semantic Segmentation. (arXiv:1801.07674v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1801.07674</link>
<description rdf:parseType="Literal">&lt;p&gt;Based on the observation that semantic segmentation errors are partially
predictable, we propose a compact formulation using confusion statistics of the
trained classifier to refine (re-estimate) the initial pixel label hypotheses.
The proposed strategy is contingent upon computing the classifier confusion
probabilities for a given dataset and estimating a relevant prior on the object
classes present in the image to be classified. We provide a procedure to
robustly estimate the confusion probabilities and explore multiple prior
definitions. Experiments are shown comparing performances on multiple
challenging datasets using different priors to improve a state-of-the-art
semantic segmentation classifier. This study demonstrates the potential to
significantly improve semantic labeling and motivates future work for reliable
label prior estimation from images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1&quot;&gt;James W. Davis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menart_C/0/1/0/all/0/1&quot;&gt;Christopher Menart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbar_M/0/1/0/all/0/1&quot;&gt;Muhammad Akbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilin_R/0/1/0/all/0/1&quot;&gt;Roman Ilin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06588">
<title>Dependent landmark drift: robust point set registration based on the Gaussian mixture model with a statistical shape model. (arXiv:1711.06588v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06588</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of point set registration is to find point-by-point correspondences
between point sets, each of which characterizes the shape of an object. Because
local preservation of object geometry is assumed, prevalent algorithms in the
area can often elegantly solve the problems without using geometric information
specific to the objects. This means that registration performance can be
further improved by using prior knowledge of object geometry. In this paper, we
propose a novel point set registration method using the Gaussian mixture model
with prior shape information encoded as a statistical shape model. Our
transformation model is defined as a combination of the similar transformation,
motion coherence, and the statistical shape model. Therefore, the proposed
method works effectively if the target point set includes outliers and missing
regions, or if it is rotated. The computational cost can be reduced to linear,
and therefore the method is scalable to large point sets. The effectiveness of
the method will be verified through comparisons with existing algorithms using
datasets concerning human body shapes, hands, and faces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirose_O/0/1/0/all/0/1&quot;&gt;Osamu Hirose&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07292">
<title>Convergence of Value Aggregation for Imitation Learning. (arXiv:1801.07292v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.07292</link>
<description rdf:parseType="Literal">&lt;p&gt;Value aggregation is a general framework for solving imitation learning
problems. Based on the idea of data aggregation, it generates a policy sequence
by iteratively interleaving policy optimization and evaluation in an online
learning setting. While the existence of a good policy in the policy sequence
can be guaranteed non-asymptotically, little is known about the convergence of
the sequence or the performance of the last policy. In this paper, we debunk
the common belief that value aggregation always produces a convergent policy
sequence with improving performance. Moreover, we identify a critical stability
condition for convergence and provide a tight non-asymptotic bound on the
performance of the last policy. These new theoretical insights let us stabilize
problems with regularization, which removes the inconvenient process of
identifying the best policy in the policy sequence in stochastic problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Ching-An Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1&quot;&gt;Byron Boots&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07318">
<title>Predictor Variable Prioritization in Nonlinear Models: A Genetic Association Case Study. (arXiv:1801.07318v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1801.07318</link>
<description rdf:parseType="Literal">&lt;p&gt;The central aim in this paper is to address variable selection questions in
nonlinear and nonparametric regression. Motivated within the context of
statistical genetics, where nonlinear interactions are of particular interest,
we introduce a novel and interpretable way to summarize the relative importance
of predictor variables. Methodologically, we develop the &quot;RelATive cEntrality&quot;
(RATE) measure to prioritize candidate predictors that are not just marginally
important, but whose associations also stem from significant covarying
relationships with other variables in the data. We focus on illustrating RATE
through Bayesian Gaussian process regression; although, the methodological
innovations apply to other and more general methods. It is known that nonlinear
models often exhibit greater predictive accuracy than linear models,
particularly for outcomes generated by complex architectures. With detailed
simulations and a botanical QTL mapping study, we show that applying RATE
enables an explanation for this improved performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Crawford_L/0/1/0/all/0/1&quot;&gt;Lorin Crawford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Flaxman_S/0/1/0/all/0/1&quot;&gt;Seth R. Flaxman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Runcie_D/0/1/0/all/0/1&quot;&gt;Daniel E. Runcie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+West_M/0/1/0/all/0/1&quot;&gt;Mike West&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07389">
<title>On the complexity of convex inertial proximal algorithms. (arXiv:1801.07389v1 [math.OC])</title>
<link>http://arxiv.org/abs/1801.07389</link>
<description rdf:parseType="Literal">&lt;p&gt;The inertial proximal gradient algorithm is efficient for the composite
optimization problem. Recently, the convergence of a special inertial proximal
gradient algorithm under strong convexity has been also studied. In this paper,
we present more novel convergence complexity results, especially on the
convergence rates of the function values. The non-ergodic O(1/k) rate is proved
for inertial proximal gradient algorithm with constant stepzise when the
objective function is coercive. When the objective function fails to promise
coercivity, we prove the sublinear rate with diminishing inertial parameters.
When the function satisfies some condition (which is much weaker than the
strong convexity), the linear convergence is proved with much larger and
general stepsize than previous literature. We also extend our results to the
multi-block version and present the computational complexity. Both cyclic and
stochastic index selection strategies are considered.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yin_P/0/1/0/all/0/1&quot;&gt;Penghang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lizhi Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07426">
<title>Generalized two-dimensional linear discriminant analysis with regularization. (arXiv:1801.07426v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.07426</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances show that two-dimensional linear discriminant analysis
(2DLDA) is a successful matrix based dimensionality reduction method. However,
2DLDA may encounter the singularity issue theoretically and the sensitivity to
outliers. In this paper, a generalized Lp-norm 2DLDA framework with
regularization for an arbitrary $p&amp;gt;0$ is proposed, named G2DLDA. There are
mainly two contributions of G2DLDA: one is G2DLDA model uses an arbitrary
Lp-norm to measure the between-class and within-class scatter, and hence a
proper $p$ can be selected to achieve the robustness. The other one is that by
introducing an extra regularization term, G2DLDA achieves better generalization
performance, and solves the singularity problem. In addition, G2DLDA can be
solved through a series of convex problems with equality constraint, and it has
closed solution for each single problem. Its convergence can be guaranteed
theoretically when $1\leq p\leq2$. Preliminary experimental results on three
contaminated human face databases show the effectiveness of the proposed
G2DLDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chun-Na Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yuan-Hai Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei-Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1&quot;&gt;Nai-Yang Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07615">
<title>Fast Point Spread Function Modeling with Deep Learning. (arXiv:1801.07615v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/1801.07615</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling the Point Spread Function (PSF) of wide-field surveys is vital for
many astrophysical applications and cosmological probes including weak
gravitational lensing. The PSF smears the image of any recorded object and
therefore needs to be taken into account when inferring properties of galaxies
from astronomical images. In the case of cosmic shear, the PSF is one of the
dominant sources of systematic errors and must be treated carefully to avoid
biases in cosmological parameters. Recently, forward modeling approaches to
calibrate shear measurements within the Monte-Carlo Control Loops ($MCCL$)
framework have been developed. These methods typically require simulating a
large amount of wide-field images, thus, the simulations need to be very fast
yet have realistic properties in key features such as the PSF pattern. Hence,
such forward modeling approaches require a very flexible PSF model, which is
quick to evaluate and whose parameters can be estimated reliably from survey
data. We present a PSF model that meets these requirements based on a fast
deep-learning method to estimate its free parameters. We demonstrate our
approach on publicly available SDSS data. We extract the most important
features of the SDSS sample via principal component analysis. Next, we
construct our model based on perturbations of a fixed base profile, ensuring
that it captures these features. We then train a Convolutional Neural Network
to estimate the free parameters of the model from noisy images of the PSF. This
allows us to render a model image of each star, which we compare to the SDSS
stars to evaluate the performance of our method. We find that our approach is
able to accurately reproduce the SDSS PSF at the pixel level, which, due to the
speed of both the model evaluation and the parameter estimation, offers good
prospects for incorporating our method into the $MCCL$ framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Herbel_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Herbel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kacprzak_T/0/1/0/all/0/1&quot;&gt;Tomasz Kacprzak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Amara_A/0/1/0/all/0/1&quot;&gt;Adam Amara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Refregier_A/0/1/0/all/0/1&quot;&gt;Alexandre Refregier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Lucchi_A/0/1/0/all/0/1&quot;&gt;Aurelien Lucchi&lt;/a&gt; (ETH Zurich)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07644">
<title>Non-parametric sparse additive auto-regressive network models. (arXiv:1801.07644v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1801.07644</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider a multi-variate time series $(X_t)_{t=0}^{T}$ where $X_t \in
\mathbb{R}^d$ which may represent spike train responses for multiple neurons in
a brain, crime event data across multiple regions, and many others. An
important challenge associated with these time series models is to estimate an
influence network between the $d$ variables, especially when the number of
variables $d$ is large meaning we are in the high-dimensional setting. Prior
work has focused on parametric vector auto-regressive models. However,
parametric approaches are somewhat restrictive in practice. In this paper, we
use the non-parametric sparse additive model (SpAM) framework to address this
challenge. Using a combination of $\beta$ and $\phi$-mixing properties of
Markov chains and empirical process techniques for reproducing kernel Hilbert
spaces (RKHSs), we provide upper bounds on mean-squared error in terms of the
sparsity $s$, logarithm of the dimension $\log d$, number of time points $T$,
and the smoothness of the RKHSs. Our rates are sharp up to logarithm factors in
many cases. We also provide numerical experiments that support our theoretical
results and display potential advantages of using our non-parametric SpAM
framework for a Chicago crime dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raskutti_G/0/1/0/all/0/1&quot;&gt;Garvesh Raskutti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07691">
<title>Drug Selection via Joint Push and Learning to Rank. (arXiv:1801.07691v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1801.07691</link>
<description rdf:parseType="Literal">&lt;p&gt;Selecting the right drugs for the right patients is a primary goal of
precision medicine. In this manuscript, we consider the problem of cancer drug
selection in a learning-to-rank framework. We have formulated the cancer drug
selection problem as to accurately predicting 1). the ranking positions of
sensitive drugs and 2). the ranking orders among sensitive drugs in cancer cell
lines based on their responses to cancer drugs. We have developed a new
learning-to-rank method, denoted as pLETORg , that predicts drug ranking
structures in each cell line via using drug latent vectors and cell line latent
vectors. The pLETORg method learns such latent vectors through explicitly
enforcing that, in the drug ranking list of each cell line, the sensitive drugs
are pushed above insensitive drugs, and meanwhile the ranking orders among
sensitive drugs are correct. Genomics information on cell lines is leveraged in
learning the latent vectors. Our experimental results on a benchmark cell
line-drug response dataset demonstrate that the new pLETORg significantly
outperforms the state-of-the-art method in prioritizing new sensitive drugs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yicheng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Junfeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lijun Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1&quot;&gt;Xia Ning&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1511.02476">
<title>Statistical physics of inference: Thresholds and algorithms. (arXiv:1511.02476v5 [cond-mat.stat-mech] UPDATED)</title>
<link>http://arxiv.org/abs/1511.02476</link>
<description rdf:parseType="Literal">&lt;p&gt;Many questions of fundamental interest in todays science can be formulated as
inference problems: Some partial, or noisy, observations are performed over a
set of variables and the goal is to recover, or infer, the values of the
variables based on the indirect information contained in the measurements. For
such problems, the central scientific questions are: Under what conditions is
the information contained in the measurements sufficient for a satisfactory
inference to be possible? What are the most efficient algorithms for this task?
A growing body of work has shown that often we can understand and locate these
fundamental barriers by thinking of them as phase transitions in the sense of
statistical physics. Moreover, it turned out that we can use the gained
physical insight to develop new promising algorithms. Connection between
inference and statistical physics is currently witnessing an impressive
renaissance and we review here the current state-of-the-art, with a pedagogical
focus on the Ising model which formulated as an inference problem we call the
planted spin glass. In terms of applications we review two classes of problems:
(i) inference of clusters on graphs and networks, with community detection as a
special case and (ii) estimating a signal from its noisy linear measurements,
with compressed sensing as a case of sparse estimation. Our goal is to provide
a pedagogical review for researchers in physics and other fields interested in
this fascinating topic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zdeborova_L/0/1/0/all/0/1&quot;&gt;Lenka Zdeborov&amp;#xe1;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Krzakala_F/0/1/0/all/0/1&quot;&gt;Florent Krzakala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.02883">
<title>Maximum Volume Inscribed Ellipsoid: A New Simplex-Structured Matrix Factorization Framework via Facet Enumeration and Convex Optimization. (arXiv:1708.02883v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1708.02883</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider a structured matrix factorization model where one factor is
restricted to have its columns lying in the unit simplex. This
simplex-structured matrix factorization (SSMF) model and the associated
factorization techniques have spurred much interest in research topics over
different areas, such as hyperspectral unmixing in remote sensing, topic
discovery in machine learning, to name a few. In this paper we develop a new
theoretical SSMF framework whose idea is to study a maximum volume ellipsoid
inscribed in the convex hull of the data points. This maximum volume inscribed
ellipsoid (MVIE) idea has not been attempted in prior literature, and we show a
sufficient condition under which the MVIE framework guarantees exact recovery
of the factors. The sufficient recovery condition we show for MVIE is much more
relaxed than that of separable non-negative matrix factorization (or pure-pixel
search); coincidentally it is also identical to that of minimum volume
enclosing simplex, which is known to be a powerful SSMF framework for
non-separable problem instances. We also show that MVIE can be practically
implemented by performing facet enumeration and then by solving a convex
optimization problem. The potential of the MVIE framework is illustrated by
numerical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chia-Hsiang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruiyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wing-Kin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chi_C/0/1/0/all/0/1&quot;&gt;Chong-Yung Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.04135">
<title>Weighted Orthogonal Components Regression Analysis. (arXiv:1709.04135v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1709.04135</link>
<description rdf:parseType="Literal">&lt;p&gt;In the multiple linear regression setting, we propose a general framework,
termed weighted orthogonal components regression (WOCR), which encompasses many
known methods as special cases, including ridge regression and principal
components regression. WOCR makes use of the monotonicity inherent in
orthogonal components to parameterize the weight function. The formulation
allows for efficient determination of tuning parameters and hence is
computationally advantageous. Moreover, WOCR offers insights for deriving new
better variants. Specifically, we advocate weighting components based on their
correlations with the response, which leads to enhanced predictive performance.
Both simulated studies and real data examples are provided to assess and
illustrate the advantages of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Su_X/0/1/0/all/0/1&quot;&gt;Xiaogang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wonkye_Y/0/1/0/all/0/1&quot;&gt;Yaa Wonkye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiangrong Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.04969">
<title>Straggler Mitigation in Distributed Optimization Through Data Encoding. (arXiv:1711.04969v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.04969</link>
<description rdf:parseType="Literal">&lt;p&gt;Slow running or straggler tasks can significantly reduce computation speed in
distributed computation. Recently, coding-theory-inspired approaches have been
applied to mitigate the effect of straggling, through embedding redundancy in
certain linear computational steps of the optimization algorithm, thus
completing the computation without waiting for the stragglers. In this paper,
we propose an alternate approach where we embed the redundancy directly in the
data itself, and allow the computation to proceed completely oblivious to
encoding. We propose several encoding schemes, and demonstrate that popular
batch algorithms, such as gradient descent and L-BFGS, applied in a
coding-oblivious manner, deterministically achieve sample path linear
convergence to an approximate solution of the original problem, using an
arbitrarily varying subset of the nodes at each iteration. Moreover, this
approximation can be controlled by the amount of redundancy and the number of
nodes used in each iteration. We provide experimental results demonstrating the
advantage of the approach over uncoded and data replication strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Karakus_C/0/1/0/all/0/1&quot;&gt;Can Karakus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Diggavi_S/0/1/0/all/0/1&quot;&gt;Suhas Diggavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wotao Yin&lt;/a&gt;</dc:creator>
</item></rdf:RDF>