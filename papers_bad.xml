<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-03-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03692"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03737"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03744"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04239"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04357"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.00268"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.03082"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.06007"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.04899"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03639"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03735"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04119"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1506.08544"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1704.02906"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1708.04806"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1710.10824"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.06363"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.10938"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.08875"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09464"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.09904"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01316"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03666"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03669"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03677"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03756"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03759"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03769"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03800"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03877"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03910"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03916"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03919"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.03934"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04015"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04051"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04189"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04196"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04204"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04232"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04303"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04304"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.04371"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1404.1425"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1412.4869"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1508.00641"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1605.09658"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1606.01473"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.05936"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1701.06655"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1702.08536"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.03922"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1706.09773"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.00440"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.01674"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1711.11279"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.00181"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1712.01856"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1801.07047"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1802.02550"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01500"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1803.03692">
<title>On the information in spike timing: neural codes derived from polychronous groups. (arXiv:1803.03692v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/1803.03692</link>
<description rdf:parseType="Literal">&lt;p&gt;There is growing evidence regarding the importance of spike timing in neural
information processing, with even a small number of spikes carrying
information, but computational models lag significantly behind those for rate
coding. Experimental evidence on neuronal behavior is consistent with the
dynamical and state dependent behavior provided by recurrent connections. This
motivates the minimalistic abstraction investigated in this paper, aimed at
providing insight into information encoding in spike timing via recurrent
connections. We employ information-theoretic techniques for a simple reservoir
model which encodes input spatiotemporal patterns into a sparse neural code,
translating the polychronous groups introduced by Izhikevich into codewords on
which we can perform standard vector operations. We show that the distance
properties of the code are similar to those for (optimal) random codes. In
particular, the code meets benchmarks associated with both linear
classification and capacity, with the latter scaling exponentially with
reservoir size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Marzi_Z/0/1/0/all/0/1&quot;&gt;Zhinus Marzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hespanha_J/0/1/0/all/0/1&quot;&gt;Joao Hespanha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Madhow_U/0/1/0/all/0/1&quot;&gt;Upamanyu Madhow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03737">
<title>Enhancing Evolutionary Optimization in Uncertain Environments by Allocating Evaluations via Multi-armed Bandit Algorithms. (arXiv:1803.03737v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.03737</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimization problems with uncertain fitness functions are common in the real
world, and present unique challenges for evolutionary optimization approaches.
Existing issues include excessively expensive evaluation, lack of solution
reliability, and incapability in maintaining high overall fitness during
optimization. Using conversion rate optimization as an example, this paper
proposes a series of new techniques for addressing these issues. The main
innovation is to augment evolutionary algorithms by allocating evaluation
budget through multi-armed bandit algorithms. Experimental results demonstrate
that multi-armed bandit algorithms can be used to allocate evaluations
efficiently, select the winning solution reliably and increase overall fitness
during exploration. The proposed methods can be generalized to any optimization
problems with noisy fitness functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03744">
<title>Enhanced Optimization with Composite Objectives and Novelty Selection. (arXiv:1803.03744v1 [cs.NE])</title>
<link>http://arxiv.org/abs/1803.03744</link>
<description rdf:parseType="Literal">&lt;p&gt;An important benefit of multi-objective search is that it maintains a diverse
population of candidates, which helps in deceptive problems in particular. Not
all diversity is useful, however: candidates that optimize only one objective
while ignoring others are rarely helpful. This paper proposes a solution: The
original objectives are replaced by their linear combinations, thus focusing
the search on the most useful tradeoffs between objectives. To compensate for
the loss of diversity, this transformation is accompanied by a selection
mechanism that favors novelty. In the highly deceptive problem of discovering
minimal sorting networks, this approach finds better solutions, and finds them
faster and more consistently than standard methods. It is therefore a promising
approach to solving deceptive problems through multi-objective optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahrzad_H/0/1/0/all/0/1&quot;&gt;Hormoz Shahrzad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fink_D/0/1/0/all/0/1&quot;&gt;Daniel Fink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04239">
<title>FeTa: A DCA Pruning Algorithm with Generalization Error Guarantees. (arXiv:1803.04239v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04239</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent DNN pruning algorithms have succeeded in reducing the number of
parameters in fully connected layers, often with little or no drop in
classification accuracy. However, most of the existing pruning schemes either
have to be applied during training or require a costly retraining procedure
after pruning to regain classification accuracy. We start by proposing a cheap
pruning algorithm for fully connected DNN layers based on difference of convex
functions (DC) optimisation, that requires little or no retraining. We then
provide a theoretical analysis for the growth in the Generalization Error (GE)
of a DNN for the case of bounded perturbations to the hidden layers, of which
weight pruning is a special case. Our pruning method is orders of magnitude
faster than competing approaches, while our theoretical analysis sheds light to
previously observed problems in DNN pruning. Experiments on commnon feedforward
neural networks validate our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pitas_K/0/1/0/all/0/1&quot;&gt;Konstantinos Pitas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_M/0/1/0/all/0/1&quot;&gt;Mike Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vandergheynst_P/0/1/0/all/0/1&quot;&gt;Pierre Vandergheynst&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04357">
<title>Learning the Base Distribution in Implicit Generative Models. (arXiv:1803.04357v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04357</link>
<description rdf:parseType="Literal">&lt;p&gt;Popular generative model learning methods such as Generative Adversarial
Networks (GANs), and Variational Autoencoders (VAE) enforce the latent
representation to follow simple distributions such as isotropic Gaussian. In
this paper, we argue that learning a complicated distribution over the latent
space of an auto-encoder enables more accurate modeling of complicated data
distributions. Based on this observation, we propose a two stage optimization
procedure which maximizes an approximate implicit density model. We
experimentally verify that our method outperforms GANs and VAEs on two image
datasets (MNIST, CELEB-A). We also show that our approach is amenable to
learning generative model for sequential data, by learning to generate speech
and music.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subakan_C/0/1/0/all/0/1&quot;&gt;Cem Subakan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kojeyo_O/0/1/0/all/0/1&quot;&gt;Oluwasanmi Kojeyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1&quot;&gt;Paris Smaragdis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.00268">
<title>Algorithmically probable mutations reproduce aspects of evolution such as convergence rate, genetic memory, modularity, diversity explosions, and mass extinction. (arXiv:1709.00268v7 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1709.00268</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural selection explains how life has evolved over millions of years from
more primitive forms. The speed at which this happens, however, has sometimes
defied explanations based on random (uniformly distributed) mutations. Here we
investigate the application of algorithmic mutations (no recombination) to
binary matrices drawn from numerical approximations to algorithmic probability
in order to compare evolutionary convergence rates against the null hypothesis
(uniformly distributed mutations). Results both on synthetic and a small
biological examples lead to an accelerated rate of convergence when using the
algorithmic probability. We also show that algorithmically evolved modularity
provides an advantage that produces a genetic memory. We demonstrate that
regular structures are preserved and carried on when they first occur and can
lead to an accelerated production of diversity and extinction, possibly
explaining naturally occurring phenomena such as diversity explosions (e.g. the
Cambrian) and massive extinctions (e.g. the End Triassic) whose causes have
eluded researchers and are a cause for debate. The approach introduced here
appears to be a better approximation to biological evolution than models based
exclusively upon random uniform mutations, and it also approaches better a
formal version of open-ended evolution based on previous results. The results
validate the motivations and results of Chaitin&apos;s Metabiology programme and
previous suggestions that computation may be an equally important driver of
evolution together, and even before, the action and result of natural
selection. We also show that inducing the method on problems of optimization,
such as genetic algorithms, has the potential to accelerate convergence of
artificial evolutionary algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Orozco_S/0/1/0/all/0/1&quot;&gt;Santiago Hern&amp;#xe1;ndez-Orozco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiani_N/0/1/0/all/0/1&quot;&gt;Narsis A. Kiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenil_H/0/1/0/all/0/1&quot;&gt;Hector Zenil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.03082">
<title>A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data. (arXiv:1709.03082v7 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1709.03082</link>
<description rdf:parseType="Literal">&lt;p&gt;Gated Recurrent Unit (GRU) is a recently-developed variation of the long
short-term memory (LSTM) unit, both of which are types of recurrent neural
network (RNN). Through empirical evidence, both models have been proven to be
effective in a wide variety of machine learning tasks such as natural language
processing (Wen et al., 2015), speech recognition (Chorowski et al., 2015), and
text classification (Yang et al., 2016). Conventionally, like most neural
networks, both of the aforementioned RNN variants employ the Softmax function
as its final output layer for its prediction, and the cross-entropy function
for computing its loss. In this paper, we present an amendment to this norm by
introducing linear support vector machine (SVM) as the replacement for Softmax
in the final output layer of a GRU model. Furthermore, the cross-entropy
function shall be replaced with a margin-based function. While there have been
similar studies (Alalshekmubarak &amp;amp; Smith, 2013; Tang, 2013), this proposal is
primarily intended for binary classification on intrusion detection using the
2013 network traffic data from the honeypot systems of Kyoto University.
Results show that the GRU-SVM model performs relatively higher than the
conventional GRU-Softmax model. The proposed model reached a training accuracy
of ~81.54% and a testing accuracy of ~84.15%, while the latter was able to
reach a training accuracy of ~63.07% and a testing accuracy of ~70.75%. In
addition, the juxtaposition of these two final output layers indicate that the
SVM would outperform Softmax in prediction time - a theoretical implication
which was supported by the actual training and testing time in the study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarap_A/0/1/0/all/0/1&quot;&gt;Abien Fred Agarap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.06007">
<title>Layered TPOT: Speeding up Tree-based Pipeline Optimization. (arXiv:1801.06007v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/1801.06007</link>
<description rdf:parseType="Literal">&lt;p&gt;With the demand for machine learning increasing, so does the demand for tools
which make it easier to use. Automated machine learning (AutoML) tools have
been developed to address this need, such as the Tree-Based Pipeline
Optimization Tool (TPOT) which uses genetic programming to build optimal
pipelines. We introduce Layered TPOT, a modification to TPOT which aims to
create pipelines equally good as the original, but in significantly less time.
This approach evaluates candidate pipelines on increasingly large subsets of
the data according to their fitness, using a modified evolutionary algorithm to
allow for separate competition between pipelines trained on different sample
sizes. Empirical evaluation shows that, on sufficiently large datasets, Layered
TPOT indeed finds better models faster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gijsbers_P/0/1/0/all/0/1&quot;&gt;Pieter Gijsbers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1&quot;&gt;Joaquin Vanschoren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olson_R/0/1/0/all/0/1&quot;&gt;Randal S. Olson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.04899">
<title>Field-Programmable Deep Neural Network (DNN) Learning and Inference accelerator: a concept. (arXiv:1802.04899v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.04899</link>
<description rdf:parseType="Literal">&lt;p&gt;An accelerator is a specialized integrated circuit designed to perform
specific computations faster than if those were performed by CPU or GPU. A
Field-Programmable DNN learning and inference accelerator (FProg-DNN) using
hybrid systolic and non-systolic techniques, distributed information-control
and deep pipelined structure is proposed and its microarchitecture and
operation presented here. Reconfigurability attends diverse DNN designs and
allows for different number of workers to be assigned to different layers as a
function of the relative difference in computational load among layers. The
computational delay per layer is made roughly the same along pipelined
accelerator structure. VGG-16 and recently proposed Inception Modules are used
for showing the flexibility of the FProg-DNN reconfigurability. Special
structures were also added for a combination of convolution layer, map
coincidence and feedback for state of the art learning with small set of
examples, which is the focus of a companion paper by the author (Franca-Neto,
2018). The accelerator described is able to reconfigure from (1) allocating all
a DNN computations to a single worker in one extreme of sub-optimal performance
to (2) optimally allocating workers per layer according to computational load
in each DNN layer to be realized. Due the pipelined architecture, more than 50x
speedup is achieved relative to GPUs or TPUs. This speed-up is consequence of
hiding the delay in transporting activation outputs from one layer to the next
in a DNN behind the computations in the receiving layer. This FProg-DNN concept
has been simulated and validated at behavioral-functional level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franca_Neto_L/0/1/0/all/0/1&quot;&gt;Luiz M Franca-Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03639">
<title>A New Model for Evaluating Range-Based Anomaly Detection Algorithms. (arXiv:1803.03639v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.03639</link>
<description rdf:parseType="Literal">&lt;p&gt;Classical anomaly detection (AD) is principally concerned with point-based
anomalies, anomalies that occur at a single point in time. While point-based
anomalies are useful, many real-world anomalies are range-based, meaning they
occur over a period of time. Therefore, applying classical point-based accuracy
measures to range-based AD systems can be misleading. In this paper, we present
a new mathematical model that more accurately gauges the classification
correctness of AD systems for range-based anomalies. Unlike prior work, our
mathematical definitions are a superset of the classical AD definitions,
enabling our system to also subsume point-based anomalies. Moreover, our system
is broadly generalizable and provides a number of specialization functions that
can control the application&apos;s bias along a multi-dimensional axis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tatbul_N/0/1/0/all/0/1&quot;&gt;Nesime Tatbul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1&quot;&gt;Tae Jun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zdonik_S/0/1/0/all/0/1&quot;&gt;Stan Zdonik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottschlich_J/0/1/0/all/0/1&quot;&gt;Justin Gottschlich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03735">
<title>Attention-based Graph Neural Network for Semi-supervised Learning. (arXiv:1803.03735v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03735</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently popularized graph neural networks achieve the state-of-the-art
accuracy on a number of standard benchmark datasets for graph-based
semi-supervised learning, improving significantly over existing approaches.
These architectures alternate between a propagation layer that aggregates the
hidden states of the local neighborhood and a fully-connected layer. Perhaps
surprisingly, we show that a linear model, that removes all the intermediate
fully-connected layers, is still able to achieve a performance comparable to
the state-of-the-art models. This significantly reduces the number of
parameters, which is critical for semi-supervised learning where number of
labeled examples are small. This in turn allows a room for designing more
innovative propagation layers. Based on this insight, we propose a novel graph
neural network that removes all the intermediate fully-connected layers, and
replaces the propagation layers with attention mechanisms that respect the
structure of the graph. The attention mechanism allows us to learn a dynamic
and adaptive local summary of the neighborhood to achieve more accurate
predictions. In a number of experiments on benchmark citation networks
datasets, we demonstrate that our approach outperforms competing methods. By
examining the attention weights among neighbors, we show that our model
provides some interesting insights on how neighbors influence each other.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thekumparampil_K/0/1/0/all/0/1&quot;&gt;Kiran K. Thekumparampil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Sewoong Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li-Jia Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04119">
<title>A Deep Learning Based Behavioral Approach to Indoor Autonomous Navigation. (arXiv:1803.04119v1 [cs.AI])</title>
<link>http://arxiv.org/abs/1803.04119</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a semantically rich graph representation for indoor robotic
navigation. Our graph representation encodes: semantic locations such as
offices or corridors as nodes, and navigational behaviors such as enter office
or cross a corridor as edges. In particular, our navigational behaviors operate
directly from visual inputs to produce motor controls and are implemented with
deep learning architectures. This enables the robot to avoid explicit
computation of its precise location or the geometry of the environment, and
enables navigation at a higher level of semantic abstraction. We evaluate the
effectiveness of our representation by simulating navigation tasks in a large
number of virtual environments. Our results show that using a simple sets of
perceptual and navigational behaviors, the proposed approach can successfully
guide the way of the robot as it completes navigational missions such as going
to a specific office. Furthermore, our implementation shows to be effective to
control the selection and switching of behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sepulveda_G/0/1/0/all/0/1&quot;&gt;Gabriel Sepulveda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1&quot;&gt;Juan Carlos Niebles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1&quot;&gt;Alvaro Soto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1506.08544">
<title>Exact and approximate inference in graphical models: variable elimination and beyond. (arXiv:1506.08544v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1506.08544</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic graphical models offer a powerful framework to account for the
dependence structure between variables, which is represented as a graph.
However, the dependence between variables may render inference tasks
intractable. In this paper we review techniques exploiting the graph structure
for exact inference, borrowed from optimisation and computer science. They are
built on the principle of variable elimination whose complexity is dictated in
an intricate way by the order in which variables are eliminated. The so-called
treewidth of the graph characterises this algorithmic complexity: low-treewidth
graphs can be processed efficiently. The first message that we illustrate is
therefore the idea that for inference in graphical model, the number of
variables is not the limiting factor, and it is worth checking for the
treewidth before turning to approximate methods. We show how algorithms
providing an upper bound of the treewidth can be exploited to derive a &apos;good&apos;
elimination order enabling to perform exact inference. The second message is
that when the treewidth is too large, algorithms for approximate inference
linked to the principle of variable elimination, such as loopy belief
propagation and variational approaches, can lead to accurate results while
being much less time consuming than Monte-Carlo approaches. We illustrate the
techniques reviewed in this article on benchmarks of inference problems in
genetic linkage analysis and computer vision, as well as on hidden variables
restoration in coupled Hidden Markov Models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Peyrard_N/0/1/0/all/0/1&quot;&gt;Nathalie Peyrard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cros_M/0/1/0/all/0/1&quot;&gt;Marie-Jos&amp;#xe9;e Cros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Givry_S/0/1/0/all/0/1&quot;&gt;Simon de Givry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Franc_A/0/1/0/all/0/1&quot;&gt;Alain Franc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Robin_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Robin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sabbadin_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;gis Sabbadin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schiex_T/0/1/0/all/0/1&quot;&gt;Thomas Schiex&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vignes_M/0/1/0/all/0/1&quot;&gt;Matthieu Vignes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1704.02906">
<title>Multi-Agent Diverse Generative Adversarial Networks. (arXiv:1704.02906v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1704.02906</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an intuitive generalization to the Generative Adversarial Networks
(GANs) and its conditional variants to address the well known mode collapse
problem. Firstly, we propose a multi-agent GAN architecture incorporating
multiple generators and one discriminator. Secondly, to enforce different
generators to capture diverse high probability modes, we modify discriminator&apos;s
objective function where along with finding the real and fake samples, the
discriminator has to identify the generator that generated the fake sample.
Intuitively, to succeed in this task, the discriminator must learn to push
different generators towards different identifiable modes. Our framework
(MAD-GAN) is generalizable in the sense that it can be easily combined with
other existing variants of GANs to produce diverse samples. We perform
extensive experiments on synthetic and real datasets and compare MAD-GAN with
different variants of GAN. We show high quality diverse sample generations for
the challenging tasks such as image-to-image translation (known to learn delta
distribution) and face generation. In addition, we show that MAD-GAN is able to
disentangle different modalities even when trained using highly challenging
multi-view dataset (mixture of forests, icebergs, bedrooms etc). In the end, we
also show its efficacy for the unsupervised feature representation task. In the
appendix we introduce a similarity based competing objective which encourages
the different generators to generate varied samples judged by a user defined
similarity metric. We show extensive evaluations on a 1-D setting of mixture of
gaussians for non parametric density estimation. The theoretical proofs back
the efficacy of the framework and explains why various generators are pushed
towards distinct clusters of modes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Arnab Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulharia_V/0/1/0/all/0/1&quot;&gt;Viveka Kulharia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1&quot;&gt;Vinay Namboodiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip H. S. Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1&quot;&gt;Puneet K. Dokania&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1708.04806">
<title>New Ideas for Brain Modelling 4. (arXiv:1708.04806v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1708.04806</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper continues the research that considers a new cognitive model based
strongly on the human brain. In particular, it considers the neural binding
structure of an earlier paper. It also describes some new methods in the areas
of image processing and behaviour simulation. The work is all based on earlier
research by the author and the new additions are intended to fit in with the
overall design. For image processing, a grid-like structure is used with &apos;full
linking&apos;. Each cell in the classifier grid stores a list of all other cells it
gets associated with and this is used as the learned image that new input is
compared to. For the behaviour metric, a new prediction equation is suggested,
as part of a simulation, that uses feedback and history to dynamically
determine its course of action. While the new methods are from widely different
topics, both can be compared with the binary-analog type of interface that is
the main focus of the paper. It is suggested that the simplest of linking
between a tree and ensemble can explain neural binding and variable signal
strengths.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greer_K/0/1/0/all/0/1&quot;&gt;Kieran Greer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1710.10824">
<title>Rough extreme learning machine: a new classification method based on uncertainty measure. (arXiv:1710.10824v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1710.10824</link>
<description rdf:parseType="Literal">&lt;p&gt;Extreme learning machine (ELM) is a new single hidden layer feedback neural
network. The weights of the input layer and the biases of neurons in hidden
layer are randomly generated, the weights of the output layer can be
analytically determined. ELM has been achieved good results for a large number
of classification tasks. In this paper, a new extreme learning machine called
rough extreme learning machine (RELM) was proposed. RELM uses rough set to
divide data into upper approximation set and lower approximation set, and the
two approximation sets are utilized to train upper approximation neurons and
lower approximation neurons. In addition, an attribute reduction is executed in
this algorithm to remove redundant attributes. The experimental results showed,
comparing with the comparison algorithms, RELM can get a better accuracy and
repeatability in most cases, RELM can not only maintain the advantages of fast
speed, but also effectively cope with the classification task for
high-dimensional data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Lin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuliang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shenglan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.06363">
<title>3D Reconstruction of Incomplete Archaeological Objects Using a Generative Adversarial Network. (arXiv:1711.06363v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1711.06363</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a data-driven approach to aid the repairing and conservation of
archaeological objects: ORGAN, an object reconstruction generative adversarial
network (GAN). By using an encoder-decoder 3D deep neural network on a GAN
architecture, and combining two loss objectives: a completion loss and an
Improved Wasserstein GAN loss, we can train a network to effectively predict
the missing geometry of damaged objects. As archaeological objects can greatly
differ between them, the network is conditioned on a variable, which can be a
culture, a region or any metadata of the object. In our results, we show that
our method can recover most of the information from damaged objects, even in
cases where more than half of the voxels are missing, without producing many
errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hermoza_R/0/1/0/all/0/1&quot;&gt;Renato Hermoza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sipiran_I/0/1/0/all/0/1&quot;&gt;Ivan Sipiran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.10938">
<title>Extreme Dimension Reduction for Handling Covariate Shift. (arXiv:1711.10938v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1711.10938</link>
<description rdf:parseType="Literal">&lt;p&gt;In the covariate shift learning scenario, the training and test covariate
distributions differ, so that a predictor&apos;s average loss over the training and
test distributions also differ. In this work, we explore the potential of
extreme dimension reduction, i.e. to very low dimensions, in improving the
performance of importance weighting methods for handling covariate shift, which
fail in high dimensions due to potentially high train/test covariate divergence
and the inability to accurately estimate the requisite density ratios. We first
formulate and solve a problem optimizing over linear subspaces a combination of
their predictive utility and train/test divergence within. Applying it to
simulated and real data, we show extreme dimension reduction helps sometimes
but not always, due to a bias introduced by dimension reduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fulton Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.08875">
<title>Predicting Rich Drug-Drug Interactions via Biomedical Knowledge Graphs and Text Jointly Embedding. (arXiv:1712.08875v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1712.08875</link>
<description rdf:parseType="Literal">&lt;p&gt;Minimizing adverse reactions caused by drug-drug interactions has always been
a momentous research topic in clinical pharmacology. Detecting all possible
interactions through clinical studies before a drug is released to the market
is a demanding task. The power of big data is opening up new approaches to
discover various drug-drug interactions. However, these discoveries contain a
huge amount of noise and provide knowledge bases far from complete and
trustworthy ones to be utilized. Most existing studies focus on predicting
binary drug-drug interactions between drug pairs but ignore other interactions.
In this paper, we propose a novel framework, called PRD, to predict drug-drug
interactions. The framework uses the graph embedding that can overcome data
incompleteness and sparsity issues to achieve multiple DDI label prediction.
First, a large-scale drug knowledge graph is generated from different sources.
Then, the knowledge graph is embedded with comprehensive biomedical text into a
common low dimensional space. Finally, the learned embeddings are used to
efficiently compute rich DDI information through a link prediction process. To
validate the effectiveness of the proposed framework, extensive experiments
were conducted on real-world datasets. The results demonstrate that our model
outperforms several state-of-the-art baseline methods in terms of capability
and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09464">
<title>Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research. (arXiv:1802.09464v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09464</link>
<description rdf:parseType="Literal">&lt;p&gt;The purpose of this technical report is two-fold. First of all, it introduces
a suite of challenging continuous control tasks (integrated with OpenAI Gym)
based on currently existing robotics hardware. The tasks include pushing,
sliding and pick &amp;amp; place with a Fetch robotic arm as well as in-hand object
manipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards
and follow a Multi-Goal Reinforcement Learning (RL) framework in which an agent
is told what to do using an additional input.
&lt;/p&gt;
&lt;p&gt;The second part of the paper presents a set of concrete research ideas for
improving RL algorithms, most of which are related to Multi-Goal RL and
Hindsight Experience Replay.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plappert_M/0/1/0/all/0/1&quot;&gt;Matthias Plappert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrychowicz_M/0/1/0/all/0/1&quot;&gt;Marcin Andrychowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Alex Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGrew_B/0/1/0/all/0/1&quot;&gt;Bob McGrew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baker_B/0/1/0/all/0/1&quot;&gt;Bowen Baker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Powell_G/0/1/0/all/0/1&quot;&gt;Glenn Powell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jonas Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tobin_J/0/1/0/all/0/1&quot;&gt;Josh Tobin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chociej_M/0/1/0/all/0/1&quot;&gt;Maciek Chociej&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welinder_P/0/1/0/all/0/1&quot;&gt;Peter Welinder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vikash Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaremba_W/0/1/0/all/0/1&quot;&gt;Wojciech Zaremba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.09904">
<title>Ab initio Algorithmic Causal Deconvolution of Intertwined Programs and Networks by Generative Mechanism. (arXiv:1802.09904v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1802.09904</link>
<description rdf:parseType="Literal">&lt;p&gt;To extract and learn representations leading to generative mechanisms from
data, especially without making arbitrary decisions and biased assumptions, is
a central challenge in most areas of scientific research particularly in
connection to current major limitations of influential topics and methods of
machine and deep learning as they have often lost sight of the model component.
Complex data is usually produced by interacting sources with different
mechanisms. Here we introduce a parameter-free model-based approach, based upon
the seminal concept of Algorithmic Probability, that decomposes an observation
and signal into its most likely algorithmic generative mechanisms. Our methods
use a causal calculus to infer model representations. We demonstrate the method
ability to distinguish interacting mechanisms and deconvolve them, regardless
of whether the objects produce strings, space-time evolution diagrams, images
or networks. We numerically test and evaluate our method and find that it can
disentangle observations from discrete dynamic systems, random and complex
networks. We think that these causal inference techniques can contribute as key
pieces of information for estimations of probability distributions
complementing other more statistical-oriented techniques that otherwise lack
model inference capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenil_H/0/1/0/all/0/1&quot;&gt;Hector Zenil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiani_N/0/1/0/all/0/1&quot;&gt;Narsis A. Kiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1&quot;&gt;Jesper Tegn&amp;#xe9;r&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01316">
<title>On Cognitive Preferences and the Interpretability of Rule-based Models. (arXiv:1803.01316v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01316</link>
<description rdf:parseType="Literal">&lt;p&gt;It is conventional wisdom in machine learning and data mining that logical
models such as rule sets are more interpretable than other models, and that
among such rule-based models, simpler models are more interpretable than more
complex ones. In this position paper, we question this latter assumption, and
recapitulate evidence for and against this postulate. We also report the
results of an evaluation in a crowd-sourcing study, which does not reveal a
strong preference for simple rules, whereas we can observe a weak preference
for longer rules in some domains. We then continue to review criteria for
interpretability from the psychological literature, evaluate some of them, and
briefly discuss their potential use in machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1&quot;&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kliegr_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Kliegr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1&quot;&gt;Heiko Paulheim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03666">
<title>Standing Wave Decomposition Gaussian Process. (arXiv:1803.03666v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03666</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a Standing Wave Decomposition (SWD) approximation to Gaussian
Process regression (GP). GP involves a costly matrix inversion operation, which
limits applicability to large data analysis. For an input space that can be
approximated by a grid and when correlations among data are short-ranged, the
kernel matrix inversion can be replaced by analytic diagonalization using the
SWD. We show that this approach applies to uni- and multi-dimensional input
data, extends to include longer-range correlations, and the grid can be in a
latent space and used as inducing points. Through simulations, we show that our
approximate method outperforms existing methods in predictive accuracy per unit
time in the regime where data are plentiful. Our SWD-GP is recommended for
regression analyses where there is a relatively large amount of data and/or
there are constraints on computation time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chi-Ken Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Scott Cheng-Hsin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shafto_P/0/1/0/all/0/1&quot;&gt;Patrick Shafto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03669">
<title>Provably robust estimation of modulo 1 samples of a smooth function with applications to phase unwrapping. (arXiv:1803.03669v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03669</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider an unknown smooth function $f: [0,1]^d \rightarrow \mathbb{R}$, and
say we are given $n$ noisy mod 1 samples of $f$, i.e., $y_i = (f(x_i) +
\eta_i)\mod 1$, for $x_i \in [0,1]^d$, where $\eta_i$ denotes the noise. Given
the samples $(x_i,y_i)_{i=1}^{n}$, our goal is to recover smooth, robust
estimates of the clean samples $f(x_i) \bmod 1$. We formulate a natural
approach for solving this problem, which works with angular embeddings of the
noisy mod 1 samples over the unit circle, inspired by the angular
synchronization framework. This amounts to solving a smoothness regularized
least-squares problem -- a quadratically constrained quadratic program (QCQP)
-- where the variables are constrained to lie on the unit circle. Our approach
is based on solving its relaxation, which is a trust-region sub-problem and
hence solvable efficiently. We provide theoretical guarantees demonstrating its
robustness to noise for adversarial, and random Gaussian and Bernoulli noise
models. To the best of our knowledge, these are the first such theoretical
results for this problem. We demonstrate the robustness and efficiency of our
approach via extensive numerical simulations on synthetic data, along with a
simple least-squares solution for the unwrapping stage, that recovers the
original samples of $f$ (up to a global shift). It is shown to perform well at
high levels of noise, when taking as input the denoised modulo $1$ samples.
&lt;/p&gt;
&lt;p&gt;Finally, we also consider two other approaches for denoising the modulo 1
samples that leverage tools from Riemannian optimization on manifolds,
including a Burer-Monteiro approach for a semidefinite programming relaxation
of our formulation. For the two-dimensional version of the problem, which has
applications in radar interferometry, we are able to solve instances of
real-world data with a million sample points in under 10 seconds, on a personal
laptop.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cucuringu_M/0/1/0/all/0/1&quot;&gt;Mihai Cucuringu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tyagi_H/0/1/0/all/0/1&quot;&gt;Hemant Tyagi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03677">
<title>Nonparametric Risk Assessment and Density Estimation for Persistence Landscapes. (arXiv:1803.03677v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03677</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents approximate confidence intervals for each function of
parameters in a Banach space based on a bootstrap algorithm. We apply kernel
density approach to estimate the persistence landscape. In addition, we
evaluate the quality distribution function estimator of random variables using
integrated mean square error (IMSE). The results of simulation studies show a
significant improvement achieved by our approach compared to the standard
version of confidence intervals algorithm. In the next step, we provide several
algorithms to solve our model. Finally, real data analysis shows that the
accuracy of our method compared to that of previous works for computing the
confidence interval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pakniat_S/0/1/0/all/0/1&quot;&gt;Soroush Pakniat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eskandari_F/0/1/0/all/0/1&quot;&gt;Farzad Eskandari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03756">
<title>Influence of the Event Rate on Discrimination Abilities of Bankruptcy Prediction Models. (arXiv:1803.03756v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03756</link>
<description rdf:parseType="Literal">&lt;p&gt;In bankruptcy prediction, the proportion of events is very low, which is
often oversampled to eliminate this bias. In this paper, we study the influence
of the event rate on discrimination abilities of bankruptcy prediction models.
First the statistical association and significance of public records and
firmographics indicators with the bankruptcy were explored. Then the event rate
was oversampled from 0.12% to 10%, 20%, 30%, 40%, and 50%, respectively. Seven
models were developed, including Logistic Regression, Decision Tree, Random
Forest, Gradient Boosting, Support Vector Machine, Bayesian Network, and Neural
Network. Under different event rates, models were comprehensively evaluated and
compared based on Kolmogorov-Smirnov Statistic, accuracy, F1 score, Type I
error, Type II error, and ROC curve on the hold-out dataset with their best
probability cut-offs. Results show that Bayesian Network is the most
insensitive to the event rate, while Support Vector Machine is the most
sensitive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lili Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Priestley_J/0/1/0/all/0/1&quot;&gt;Jennifer Priestley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ni_X/0/1/0/all/0/1&quot;&gt;Xuelei Ni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03759">
<title>Speech Recognition: Keyword Spotting Through Image Recognition. (arXiv:1803.03759v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03759</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of identifying voice commands has always been a challenge due to
the presence of noise and variability in speed, pitch, etc. We will compare the
efficacies of several neural network architectures for the speech recognition
problem. In particular, we will build a model to determine whether a one second
audio clip contains a particular word (out of a set of 10), an unknown word, or
silence. The models to be implemented are a CNN recommended by the Tensorflow
Speech Recognition tutorial, a low-latency CNN, and an adversarially trained
CNN. The result is a demonstration of how to convert a problem in audio
recognition to the better-studied domain of image classification, where the
powerful techniques of convolutional neural networks are fully developed.
Additionally, we demonstrate the applicability of the technique of Virtual
Adversarial Training (VAT) to this problem domain, functioning as a powerful
regularizer with promising potential future applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gouda_S/0/1/0/all/0/1&quot;&gt;Sanjay Krishna Gouda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kanetkar_S/0/1/0/all/0/1&quot;&gt;Salil Kanetkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Harrison_D/0/1/0/all/0/1&quot;&gt;David Harrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Warmuth_M/0/1/0/all/0/1&quot;&gt;Manfred K Warmuth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03769">
<title>A Minimax Surrogate Loss Approach to Conditional Difference Estimation. (arXiv:1803.03769v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03769</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new machine learning approach to estimate personalized treatment
effects in the classical potential outcomes framework with binary outcomes. To
overcome the problem that both treatment and control outcomes for the same unit
are required for supervised learning, we propose surrogate loss functions that
incorporate both treatment and control data. The new surrogates yield tighter
bounds than the sum of losses for treatment and control groups. A specific
choice of loss function, namely a type of hinge loss, yields a minimax support
vector machine formulation. The resulting optimization problem requires the
solution to only a single convex optimization problem, incorporating both
treatment and control units, and it enables the kernel trick to be used to
handle nonlinear (also non-parametric) estimation. Statistical learning bounds
are also presented for the framework, and experimental results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goh_S/0/1/0/all/0/1&quot;&gt;Siong Thye Goh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rudin_C/0/1/0/all/0/1&quot;&gt;Cynthia Rudin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03800">
<title>ARMDN: Associative and Recurrent Mixture Density Networks for eRetail Demand Forecasting. (arXiv:1803.03800v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.03800</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate demand forecasts can help on-line retail organizations better plan
their supply-chain processes. The challenge, however, is the large number of
associative factors that result in large, non-stationary shifts in demand,
which traditional time series and regression approaches fail to model. In this
paper, we propose a Neural Network architecture called AR-MDN, that
simultaneously models associative factors, time-series trends and the variance
in the demand. We first identify several causal features and use a combination
of feature embeddings, MLP and LSTM to represent them. We then model the output
density as a learned mixture of Gaussian distributions. The AR-MDN can be
trained end-to-end without the need for additional supervision. We experiment
on a dataset of an year&apos;s worth of data over tens-of-thousands of products from
Flipkart. The proposed architecture yields a significant improvement in
forecasting accuracy when compared with existing alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Srayanta Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shankar_D/0/1/0/all/0/1&quot;&gt;Devashish Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Atin Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tathawadekar_N/0/1/0/all/0/1&quot;&gt;Nilam Tathawadekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kompalli_P/0/1/0/all/0/1&quot;&gt;Pramod Kompalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1&quot;&gt;Sunita Sarawagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhury_K/0/1/0/all/0/1&quot;&gt;Krishnendu Chaudhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03877">
<title>On dynamic ensemble selection and data preprocessing for multi-class imbalance learning. (arXiv:1803.03877v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03877</link>
<description rdf:parseType="Literal">&lt;p&gt;Class-imbalance refers to classification problems in which many more
instances are available for certain classes than for others. Such imbalanced
datasets require special attention because traditional classifiers generally
favor the majority class which has a large number of instances. Ensemble of
classifiers have been reported to yield promising results. However, the
majority of ensemble methods applied too imbalanced learning are static ones.
Moreover, they only deal with binary imbalanced problems. Hence, this paper
presents an empirical analysis of dynamic selection techniques and data
preprocessing methods for dealing with multi-class imbalanced problems. We
considered five variations of preprocessing methods and four dynamic selection
methods. Our experiments conducted on 26 multi-class imbalanced problems show
that the dynamic ensemble improves the F-measure and the G-mean as compared to
the static ensemble. Moreover, data preprocessing plays an important role in
such cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cruz_R/0/1/0/all/0/1&quot;&gt;Rafael M. O. Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sabourin_R/0/1/0/all/0/1&quot;&gt;Robert Sabourin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cavalcanti_G/0/1/0/all/0/1&quot;&gt;George D. C. Cavalcanti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03910">
<title>A pathway-based kernel boosting method for sample classification using genomic data. (arXiv:1803.03910v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03910</link>
<description rdf:parseType="Literal">&lt;p&gt;The analysis of cancer genomic data has long suffered &quot;the curse of
dimensionality&quot;. Sample sizes for most cancer genomic studies are a few
hundreds at most while there are tens of thousands of genomic features studied.
Various methods have been proposed to leverage prior biological knowledge, such
as pathways, to more effectively analyze cancer genomic data. Most of the
methods focus on testing marginal significance of the associations between
pathways and clinical phenotypes. They can identify relevant pathways, but do
not involve predictive modeling. In this article, we propose a Pathway-based
Kernel Boosting (PKB) method for integrating gene pathway information for
sample classification, where we use kernel functions calculated from each
pathway as base learners and learn the weights through iterative optimization
of the classification loss function. We apply PKB and several competing methods
to three cancer studies with pathological and clinical information, including
tumor grade, stage, tumor sites, and metastasis status. Our results show that
PKB outperforms other methods, and identifies pathways relevant to the outcome
variables.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeng_L/0/1/0/all/0/1&quot;&gt;Li Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhaolong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hongyu Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03916">
<title>Deep reinforcement learning for time series: playing idealized trading games. (arXiv:1803.03916v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.03916</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Q-learning is investigated as an end-to-end solution to estimate the
optimal strategies for acting on time series input. Experiments are conducted
on two idealized trading games. 1) Univariate: the only input is a wave-like
price time series, and 2) Bivariate: the input includes a random stepwise price
time series and a noisy signal time series, which is positively correlated with
future price changes. The Univariate game tests whether the agent can capture
the underlying dynamics, and the Bivariate game tests whether the agent can
utilize the hidden relation among the inputs. Stacked Gated Recurrent Unit
(GRU), Long Short-Term Memory (LSTM) units, Convolutional Neural Network (CNN),
and multi-layer perceptron (MLP) are used to model Q values. For both games,
all agents successfully find a profitable strategy. The GRU-based agents show
best overall performance in the Univariate game, while the MLP-based agents
outperform others in the Bivariate game.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiang Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03919">
<title>Detecting Nonlinear Causality in Multivariate Time Series with Sparse Additive Models. (arXiv:1803.03919v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03919</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a nonparametric method for detecting nonlinear causal relationship
within a set of multidimensional discrete time series, by using sparse additive
models (SpAMs). We show that, when the input to the SpAM is a $\beta$-mixing
time series, the model can be fitted by first approximating each unknown
function with a linear combination of a set of B-spline bases, and then solving
a group-lasso-type optimization problem with nonconvex regularization.
Theoretically, we characterize the oracle statistical properties of the
proposed sparse estimator in function estimation and model selection.
Numerically, we propose an efficient pathwise iterative shrinkage thresholding
algorithm (PISTA), which tames the nonconvexity and guarantees linear
convergence towards the desired sparse estimator with high probability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yingxiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_A/0/1/0/all/0/1&quot;&gt;Adams Wei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tuo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.03934">
<title>Empirical bounds for functions with weak interactions. (arXiv:1803.03934v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.03934</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide sharp empirical estimates of expectation, variance and normal
approximation for a class of statistics whose variation in any argument does
not change too much when another argument is modified. Examples of such weak
interactions are furnished by U- and V-statistics, Lipschitz L-statistics and
various error functionals of L2-regularized algorithms and Gibbs algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maurer_A/0/1/0/all/0/1&quot;&gt;Andreas Maurer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pontil_M/0/1/0/all/0/1&quot;&gt;Massimiliano Pontil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04015">
<title>Multi-objective Contextual Bandit Problem with Similarity Information. (arXiv:1803.04015v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04015</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose the multi-objective contextual bandit problem with
similarity information. This problem extends the classical contextual bandit
problem with similarity information by introducing multiple and possibly
conflicting objectives. Since the best arm in each objective can be different
given the context, learning the best arm based on a single objective can
jeopardize the rewards obtained from the other objectives. In order to evaluate
the performance of the learner in this setup, we use a performance metric
called the contextual Pareto regret. Essentially, the contextual Pareto regret
is the sum of the distances of the arms chosen by the learner to the context
dependent Pareto front. For this problem, we develop a new online learning
algorithm called Pareto Contextual Zooming (PCZ), which exploits the idea of
contextual zooming to learn the arms that are close to the Pareto front for
each observed context by adaptively partitioning the joint context-arm set
according to the observed rewards and locations of the context-arm pairs
selected in the past. Then, we prove that PCZ achieves $\tilde O
(T^{(1+d_p)/(2+d_p)})$ Pareto regret where $d_p$ is the Pareto zooming
dimension that depends on the size of the set of near-optimal context-arm
pairs. Moreover, we show that this regret bound is nearly optimal by providing
an almost matching $\Omega (T^{(1+d_p)/(2+d_p)})$ lower bound.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Turgay_E/0/1/0/all/0/1&quot;&gt;Eralp Tur&amp;#x11f;ay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Oner_D/0/1/0/all/0/1&quot;&gt;Doruk &amp;#xd6;ner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tekin_C/0/1/0/all/0/1&quot;&gt;Cem Tekin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04051">
<title>Representation Learning over Dynamic Graphs. (arXiv:1803.04051v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1803.04051</link>
<description rdf:parseType="Literal">&lt;p&gt;How can we effectively encode evolving information over dynamic graphs into
low-dimensional representations? In this paper, we propose DyRep, an inductive
deep representation learning framework that learns a set of functions to
efficiently produce low-dimensional node embeddings that evolves over time. The
learned embeddings drive the dynamics of two key processes namely,
communication and association between nodes in dynamic graphs. These processes
exhibit complex nonlinear dynamics that evolve at different time scales and
subsequently contribute to the update of node embeddings. We employ a
time-scale dependent multivariate point process model to capture these
dynamics. We devise an efficient unsupervised learning procedure and
demonstrate that our approach significantly outperforms representative
baselines on two real-world datasets for the problem of dynamic link prediction
and event time prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_R/0/1/0/all/0/1&quot;&gt;Rakshit Trivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farajtbar_M/0/1/0/all/0/1&quot;&gt;Mehrdad Farajtbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswal_P/0/1/0/all/0/1&quot;&gt;Prasenjeet Biswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04189">
<title>Noise2Noise: Learning Image Restoration without Clean Data. (arXiv:1803.04189v1 [cs.CV])</title>
<link>http://arxiv.org/abs/1803.04189</link>
<description rdf:parseType="Literal">&lt;p&gt;We apply basic statistical reasoning to signal reconstruction by machine
learning -- learning to map corrupted observations to clean signals -- with a
simple and powerful conclusion: under certain common circumstances, it is
possible to learn to restore signals without ever observing clean ones, at
performance close or equal to training using clean exemplars. We show
applications in photographic noise removal, denoising of synthetic Monte Carlo
images, and reconstruction of MRI scans from undersampled inputs, all based on
only observing corrupted data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1&quot;&gt;Jaakko Lehtinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munkberg_J/0/1/0/all/0/1&quot;&gt;Jacob Munkberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasselgren_J/0/1/0/all/0/1&quot;&gt;Jon Hasselgren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1&quot;&gt;Samuli Laine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1&quot;&gt;Tero Karras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1&quot;&gt;Miika Aittala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1&quot;&gt;Timo Aila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04196">
<title>Multi-kernel Regression For Graph Signal Processing. (arXiv:1803.04196v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04196</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a multi-kernel based regression method for graph signal processing
where the target signal is assumed to be smooth over a graph. In multi-kernel
regression, an effective kernel function is expressed as a linear combination
of many basis kernel functions. We estimate the linear weights to learn the
effective kernel function by appropriate regularization based on graph
smoothness. We show that the resulting optimization problem is shown to be
convex and pro- pose an accelerated projected gradient descent based solution.
Simulation results using real-world graph signals show efficiency of the
multi-kernel based approach over a standard kernel based approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Venkitaraman_A/0/1/0/all/0/1&quot;&gt;Arun Venkitaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chatterjee_S/0/1/0/all/0/1&quot;&gt;Saikat Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Handel_P/0/1/0/all/0/1&quot;&gt;Peter H&amp;#xe4;ndel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04204">
<title>Semiparametric Contextual Bandits. (arXiv:1803.04204v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04204</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies semiparametric contextual bandits, a generalization of the
linear stochastic bandit problem where the reward for an action is modeled as a
linear function of known action features confounded by an non-linear
action-independent term. We design new algorithms that achieve
$\tilde{O}(d\sqrt{T})$ regret over $T$ rounds, when the linear function is
$d$-dimensional, which matches the best known bounds for the simpler
unconfounded case and improves on a recent result of Greenewald et al. (2017).
Via an empirical evaluation, we show that our algorithms outperform prior
approaches when there are non-linear confounding effects on the rewards.
Technically, our algorithms use a new reward estimator inspired by
doubly-robust approaches and our proofs require new concentration inequalities
for self-normalized martingales.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krishnamurthy_A/0/1/0/all/0/1&quot;&gt;Akshay Krishnamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Steven Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04232">
<title>Variational Inference for Gaussian Process with Panel Count Data. (arXiv:1803.04232v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04232</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the first framework for Gaussian-process-modulated Poisson
processes when the temporal data appear in the form of panel counts. Panel
count data frequently arise when experimental subjects are observed only at
discrete time points and only the numbers of occurrences of the events between
subsequent observation times are available. The exact occurrence timestamps of
the events are unknown. The method of conducting the efficient variational
inference is presented, based on the assumption of a Gaussian-process-modulated
intensity function. We derive a tractable lower bound to alleviate the problems
of the intractable evidence lower bound inherent in the variational inference
framework. Our algorithm outperforms classical methods on both synthetic and
three real panel count sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hongyi Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Young Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sato_I/0/1/0/all/0/1&quot;&gt;Issei Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04303">
<title>Learning unknown ODE models with Gaussian processes. (arXiv:1803.04303v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04303</link>
<description rdf:parseType="Literal">&lt;p&gt;In conventional ODE modelling coefficients of an equation driving the system
state forward in time are estimated. However, for many complex systems it is
practically impossible to determine the equations or interactions governing the
underlying dynamics. In these settings, parametric ODE model cannot be
formulated. Here, we overcome this issue by introducing a novel paradigm of
nonparametric ODE modelling that can learn the underlying dynamics of arbitrary
continuous-time systems without prior knowledge. We propose to learn
non-linear, unknown differential functions from state observations using
Gaussian process vector fields within the exact ODE formalism. We demonstrate
the model&apos;s capabilities to infer dynamics from sparse data and to simulate the
system forward into future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Heinonen_M/0/1/0/all/0/1&quot;&gt;Markus Heinonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yildiz_C/0/1/0/all/0/1&quot;&gt;Cagatay Yildiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mannerstrom_H/0/1/0/all/0/1&quot;&gt;Henrik Mannerstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Intosalmi_J/0/1/0/all/0/1&quot;&gt;Jukka Intosalmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lahdesmaki_H/0/1/0/all/0/1&quot;&gt;Harri L&amp;#xe4;hdesm&amp;#xe4;ki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04304">
<title>Representation Learning and Recovery in the ReLU Model. (arXiv:1803.04304v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04304</link>
<description rdf:parseType="Literal">&lt;p&gt;Rectified linear units, or ReLUs, have become the preferred activation
function for artificial neural networks. In this paper we consider two basic
learning problems assuming that the underlying data follow a generative model
based on a ReLU-network -- a neural network with ReLU activations. As a
primarily theoretical study, we limit ourselves to a single-layer network. The
first problem we study corresponds to dictionary-learning in the presence of
nonlinearity (modeled by the ReLU functions). Given a set of observation
vectors $\mathbf{y}^i \in \mathbb{R}^d, i =1, 2, \dots , n$, we aim to recover
$d\times k$ matrix $A$ and the latent vectors $\{\mathbf{c}^i\} \subset
\mathbb{R}^k$ under the model $\mathbf{y}^i = \mathrm{ReLU}(A\mathbf{c}^i
+\mathbf{b})$, where $\mathbf{b}\in \mathbb{R}^d$ is a random bias. We show
that it is possible to recover the column space of $A$ within an error of
$O(d)$ (in Frobenius norm) under certain conditions on the probability
distribution of $\mathbf{b}$.
&lt;/p&gt;
&lt;p&gt;The second problem we consider is that of robust recovery of the signal in
the presence of outliers, i.e., large but sparse noise. In this setting we are
interested in recovering the latent vector $\mathbf{c}$ from its noisy
nonlinear sketches of the form $\mathbf{v} = \mathrm{ReLU}(A\mathbf{c}) +
\mathbf{e}+\mathbf{w}$, where $\mathbf{e} \in \mathbb{R}^d$ denotes the
outliers with sparsity $s$ and $\mathbf{w} \in \mathbb{R}^d$ denote the dense
but small noise. This line of work has recently been studied (Soltanolkotabi,
2017) without the presence of outliers. For this problem, we show that a
generalized LASSO algorithm is able to recover the signal $\mathbf{c} \in
\mathbb{R}^k$ within an $\ell_2$ error of $O(\sqrt{\frac{(k+s)\log d}{d}})$
when $A$ is a random Gaussian matrix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mazumdar_A/0/1/0/all/0/1&quot;&gt;Arya Mazumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rawat_A/0/1/0/all/0/1&quot;&gt;Ankit Singh Rawat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.04371">
<title>Optimal Rates of Sketched-regularized Algorithms for Least-Squares Regression over Hilbert Spaces. (arXiv:1803.04371v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1803.04371</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate regularized algorithms combining with projection for
least-squares regression problem over a Hilbert space, covering nonparametric
regression over a reproducing kernel Hilbert space. We prove convergence
results with respect to variants of norms, under a capacity assumption on the
hypothesis space and a regularity condition on the target function. As a
result, we obtain optimal rates for regularized algorithms with randomized
sketches, provided that the sketch dimension is proportional to the effective
dimension up to a logarithmic factor. As a byproduct, we obtain similar results
for Nystr\&quot;{o}m regularized algorithms. Our results are the first ones with
optimal, distribution-dependent rates that do not have any saturation effect
for sketched/Nystr\&quot;{o}m regularized algorithms, considering both the
attainable and non-attainable cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junhong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1404.1425">
<title>Density Estimation via Discrepancy Based Adaptive Sequential Partition. (arXiv:1404.1425v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1404.1425</link>
<description rdf:parseType="Literal">&lt;p&gt;Given $iid$ observations from an unknown absolute continuous distribution
defined on some domain $\Omega$, we propose a nonparametric method to learn a
piecewise constant function to approximate the underlying probability density
function. Our density estimate is a piecewise constant function defined on a
binary partition of $\Omega$. The key ingredient of the algorithm is to use
discrepancy, a concept originates from Quasi Monte Carlo analysis, to control
the partition process. The resulting algorithm is simple, efficient, and has a
provable convergence rate. We empirically demonstrate its efficiency as a
density estimation method. We present its applications on a wide range of
tasks, including finding good initializations for k-means.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dangna Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wong_W/0/1/0/all/0/1&quot;&gt;Wing Hung Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1412.4869">
<title>Expectation propagation as a way of life: A framework for Bayesian inference on partitioned data. (arXiv:1412.4869v3 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1412.4869</link>
<description rdf:parseType="Literal">&lt;p&gt;A common approach for Bayesian computation with big data is to partition the
data into smaller pieces, perform local inference for each piece separately,
and finally combine the results to obtain an approximation to the global
posterior. Looking at this from the bottom up, one can perform separate
analyses on individual sources of data and then combine these in a larger
Bayesian model. In either case, the idea of distributed modeling and inference
has both conceptual and computational appeal, but from the Bayesian perspective
there is no general way of handling the prior distribution: if the prior is
included in each separate inference, it will be multiply-counted when the
inferences are combined; but if the prior is itself divided into pieces, it may
not provide enough regularization for each separate computation, thus
eliminating one of the key advantages of Bayesian methods. To resolve this
dilemma, we propose expectation propagation (EP) as a general prototype for
distributed Bayesian inference. The central idea is to factor the likelihood
according to the data partitions, and to iteratively combine each factor with
an approximate model of the prior and all other parts of the data, thus
producing an overall approximation to the global posterior at convergence. In
this paper, we give an introduction to EP and an overview of some recent
developments of the method, with particular emphasis on its use in combining
inferences from partitioned data. In addition to distributed modeling of large
datasets, our unified treatment also includes hierarchical modeling of data
with a naturally partitioned structure. The paper describes a general
algorithmic framework, rather than a specific algorithm, and presents an
example implementation for it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vehtari_A/0/1/0/all/0/1&quot;&gt;Aki Vehtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gelman_A/0/1/0/all/0/1&quot;&gt;Andrew Gelman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sivula_T/0/1/0/all/0/1&quot;&gt;Tuomas Sivula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jylanki_P/0/1/0/all/0/1&quot;&gt;Pasi Jyl&amp;#xe4;nki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tran_D/0/1/0/all/0/1&quot;&gt;Dustin Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sahai_S/0/1/0/all/0/1&quot;&gt;Swupnil Sahai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blomstedt_P/0/1/0/all/0/1&quot;&gt;Paul Blomstedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cunningham_J/0/1/0/all/0/1&quot;&gt;John P. Cunningham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schiminovich_D/0/1/0/all/0/1&quot;&gt;David Schiminovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Robert_C/0/1/0/all/0/1&quot;&gt;Christian Robert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1508.00641">
<title>Episodic Multi-armed Bandits. (arXiv:1508.00641v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1508.00641</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new class of reinforcement learning methods referred to as
{\em episodic multi-armed bandits} (eMAB). In eMAB the learner proceeds in {\em
episodes}, each composed of several {\em steps}, in which it chooses an action
and observes a feedback signal. Moreover, in each step, it can take a special
action, called the $stop$ action, that ends the current episode. After the
$stop$ action is taken, the learner collects a terminal reward, and observes
the costs and terminal rewards associated with each step of the episode. The
goal of the learner is to maximize its cumulative gain (i.e., the terminal
reward minus costs) over all episodes by learning to choose the best sequence
of actions based on the feedback. First, we define an {\em oracle} benchmark,
which sequentially selects the actions that maximize the expected immediate
gain. Then, we propose our online learning algorithm, named {\em FeedBack
Adaptive Learning} (FeedBAL), and prove that its regret with respect to the
benchmark is bounded with high probability and increases logarithmically in
expectation. Moreover, the regret only has polynomial dependence on the number
of steps, actions and states. eMAB can be used to model applications that
involve humans in the loop, ranging from personalized medical screening to
personalized web-based education, where sequences of actions are taken in each
episode, and optimal behavior requires adapting the chosen actions based on the
feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tekin_C/0/1/0/all/0/1&quot;&gt;Cem Tekin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1605.09658">
<title>Continuation of Nesterov&apos;s Smoothing for Regression with Structured Sparsity in High-Dimensional Neuroimaging. (arXiv:1605.09658v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1605.09658</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive models can be used on high-dimensional brain images for diagnosis
of a clinical condition. Spatial regularization through structured sparsity
offers new perspectives in this context and reduces the risk of overfitting the
model while providing interpretable neuroimaging signatures by forcing the
solution to adhere to domain-specific constraints. Total Variation (TV)
enforces spatial smoothness of the solution while segmenting predictive regions
from the background. We consider the problem of minimizing the sum of a smooth
convex loss, a non-smooth convex penalty (whose proximal operator is known) and
a wide range of possible complex, non-smooth convex structured penalties such
as TV or overlapping group Lasso. Existing solvers are either limited in the
functions they can minimize or in their practical capacity to scale to
high-dimensional imaging data. Nesterov&apos;s smoothing technique can be used to
minimize a large number of non-smooth convex structured penalties but
reasonable precision requires a small smoothing parameter, which slows down the
convergence speed. To benefit from the versatility of Nesterov&apos;s smoothing
technique, we propose a first order continuation algorithm, CONESTA, which
automatically generates a sequence of decreasing smoothing parameters. The
generated sequence maintains the optimal convergence speed towards any globally
desired precision. Our main contributions are: To propose an expression of the
duality gap to probe the current distance to the global optimum in order to
adapt the smoothing parameter and the convergence speed. We provide a
convergence rate, which is an improvement over classical proximal gradient
smoothing methods. We demonstrate on both simulated and high-dimensional
structural neuroimaging data that CONESTA significantly outperforms many
state-of-the-art solvers in regard to convergence speed and precision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hadj_Selem_F/0/1/0/all/0/1&quot;&gt;Fouad Hadj-Selem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lofstedt_T/0/1/0/all/0/1&quot;&gt;Tommy Lofstedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dohmatob_E/0/1/0/all/0/1&quot;&gt;Elvis Dohmatob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frouin_V/0/1/0/all/0/1&quot;&gt;Vincent Frouin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dubois_M/0/1/0/all/0/1&quot;&gt;Mathieu Dubois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guillemot_V/0/1/0/all/0/1&quot;&gt;Vincent Guillemot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duchesnay_E/0/1/0/all/0/1&quot;&gt;Edouard Duchesnay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1606.01473">
<title>Confidence Intervals for Algorithmic Leveraging in Linear Regression. (arXiv:1606.01473v4 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/1606.01473</link>
<description rdf:parseType="Literal">&lt;p&gt;The age of big data has produced data sets that are computationally expensive
to analyze and store. Algorithmic leveraging proposes that we sample
observations from the original data set to generate a representative data set
and then perform analysis on the representative data set. In this paper, we
present efficient algorithms for constructing finite sample confidence
intervals for each algorithmic leveraging estimated regression coefficient,
with asymptotic coverage guarantees. In simulations, we confirm empirically
that the confidence intervals have the desired coverage probabilities, while
bootstrap confidence intervals may not.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gao_K/0/1/0/all/0/1&quot;&gt;Katelyn Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.05936">
<title>The biglasso Package: A Memory- and Computation-Efficient Solver for Lasso Model Fitting with Big Data in R. (arXiv:1701.05936v2 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/1701.05936</link>
<description rdf:parseType="Literal">&lt;p&gt;Penalized regression models such as the lasso have been extensively applied
to analyzing high-dimensional data sets. However, due to memory limitations,
existing R packages like glmnet and ncvreg are not capable of fitting
lasso-type models for ultrahigh-dimensional, multi-gigabyte data sets that are
increasingly seen in many areas such as genetics, genomics, biomedical imaging,
and high-frequency finance. In this research, we implement an R package called
biglasso that tackles this challenge. biglasso utilizes memory-mapped files to
store the massive data on the disk, only reading data into memory when
necessary during model fitting, and is thus able to handle out-of-core
computation seamlessly. Moreover, it&apos;s equipped with newly proposed, more
efficient feature screening rules, which substantially accelerate the
computation. Benchmarking experiments show that our biglasso package, as
compared to existing popular ones like glmnet, is much more memory- and
computation-efficient. We further analyze a 31 GB real data set on a laptop
with only 16 GB RAM to demonstrate the out-of-core computation capability of
biglasso in analyzing massive data sets that cannot be accommodated by existing
R packages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yaohui Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Breheny_P/0/1/0/all/0/1&quot;&gt;Patrick Breheny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1701.06655">
<title>Patchwork Kriging for Large-scale Gaussian Process Regression. (arXiv:1701.06655v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1701.06655</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new approach for Gaussian process (GP) regression for
large datasets. The approach involves partitioning the regression input domain
into multiple local regions with a different local GP model fitted in each
region. Unlike existing local partitioned GP approaches, we introduce a
technique for patching together the local GP models nearly seamlessly to ensure
that the local GP models for two neighboring regions produce nearly the same
response prediction and prediction error variance on the boundary between the
two regions. This effectively solves the well-known discontinuity problem that
degrades the boundary accuracy of existing local partitioned GP methods. Our
main innovation is to represent the continuity conditions as additional
pseudo-observations that the differences between neighboring GP responses are
identically zero at an appropriately chosen set of boundary input locations. To
predict the response at any input location, we simply augment the actual
response observations with the pseudo-observations and apply standard GP
prediction methods to the augmented data. In contrast to heuristic continuity
adjustments, this has an advantage of working within a formal GP framework, so
that the GP-based predictive uncertainty quantification remains valid. Our
approach also inherits a sparse block-like structure for the sample covariance
matrix, which results in computationally efficient closed-form expressions for
the predictive mean and variance. In addition, we provide a new spatial
partitioning scheme based on a recursive space partitioning along local
principal component directions, which makes the proposed approach applicable
for regression domains having more than two dimensions. Using three spatial
datasets and three higher dimensional datasets, we investigate the numerical
performance of the approach and compare it to several state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chiwoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apley_D/0/1/0/all/0/1&quot;&gt;Daniel Apley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1702.08536">
<title>Fast Threshold Tests for Detecting Discrimination. (arXiv:1702.08536v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1702.08536</link>
<description rdf:parseType="Literal">&lt;p&gt;Threshold tests have recently been proposed as a useful method for detecting
bias in lending, hiring, and policing decisions. For example, in the case of
credit extensions, these tests aim to estimate the bar for granting loans to
white and minority applicants, with a higher inferred threshold for minorities
indicative of discrimination. This technique, however, requires fitting a
complex Bayesian latent variable model for which inference is often
computationally challenging. Here we develop a method for fitting threshold
tests that is two orders of magnitude faster than the existing approach,
reducing computation from hours to minutes. To achieve these performance gains,
we introduce and analyze a flexible family of probability distributions on the
interval [0, 1] -- which we call discriminant distributions -- that is
computationally efficient to work with. We demonstrate our technique by
analyzing 2.7 million police stops of pedestrians in New York City.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pierson_E/0/1/0/all/0/1&quot;&gt;Emma Pierson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Corbett_Davies_S/0/1/0/all/0/1&quot;&gt;Sam Corbett-Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Goel_S/0/1/0/all/0/1&quot;&gt;Sharad Goel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.03922">
<title>Analyzing the Robustness of Nearest Neighbors to Adversarial Examples. (arXiv:1706.03922v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1706.03922</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by safety-critical applications, test-time attacks on classifiers
via adversarial examples has recently received a great deal of attention.
However, there is a general lack of understanding on why adversarial examples
arise; whether they originate due to inherent properties of data or due to lack
of training samples remains ill-understood. In this work, we introduce a
theoretical framework analogous to bias-variance theory for understanding these
effects.
&lt;/p&gt;
&lt;p&gt;We use our framework to analyze the robustness of a canonical non-parametric
classifier - the k-nearest neighbors. Our analysis shows that its robustness
properties depend critically on the value of k - the classifier may be
inherently non-robust for small k, but its robustness approaches that of the
Bayes Optimal classifier for fast-growing k. We propose a novel modified
1-nearest neighbor classifier, and guarantee its robustness in the large sample
limit. Our experiments suggest that this classifier may have good robustness
properties even for reasonable data set sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jha_S/0/1/0/all/0/1&quot;&gt;Somesh Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chaudhuri_K/0/1/0/all/0/1&quot;&gt;Kamalika Chaudhuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.09773">
<title>Interpretability via Model Extraction. (arXiv:1706.09773v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1706.09773</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to interpret machine learning models has become increasingly
important now that machine learning is used to inform consequential decisions.
We propose an approach called model extraction for interpreting complex,
blackbox models. Our approach approximates the complex model using a much more
interpretable model; as long as the approximation quality is good, then
statistical properties of the complex model are reflected in the interpretable
model. We show how model extraction can be used to understand and debug random
forests and neural nets trained on several datasets from the UCI Machine
Learning Repository, as well as control policies learned for several classical
reinforcement learning problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1&quot;&gt;Osbert Bastani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Carolyn Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastani_H/0/1/0/all/0/1&quot;&gt;Hamsa Bastani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.00440">
<title>PassGAN: A Deep Learning Approach for Password Guessing. (arXiv:1709.00440v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1709.00440</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art password guessing tools, such as HashCat and John the
Ripper, enable users to check billions of passwords per second against password
hashes. In addition to performing straightforward dictionary attacks, these
tools can expand password dictionaries using password generation rules, such as
concatenation of words (e.g., &quot;password123456&quot;) and leet speak (e.g.,
&quot;password&quot; becomes &quot;p4s5w0rd&quot;). Although these rules work well in practice,
expanding them to model further passwords is a laborious task that requires
specialized expertise. To address this issue, in this paper we introduce
PassGAN, a novel approach that replaces human-generated password rules with
theory-grounded machine learning algorithms. Instead of relying on manual
password analysis, PassGAN uses a Generative Adversarial Network (GAN) to
autonomously learn the distribution of real passwords from actual password
leaks, and to generate high-quality password guesses. Our experiments show that
this approach is very promising. When we evaluated PassGAN on two large
password datasets, we were able to surpass rule-based and state-of-the-art
machine learning password guessing tools. However, in contrast with the other
tools, PassGAN achieved this result without any a-priori knowledge on passwords
or common password structures. Additionally, when we combined the output of
PassGAN with the output of HashCat, we were able to match 51%-73% more
passwords than with HashCat alone. This is remarkable, because it shows that
PassGAN can autonomously extract a considerable number of password properties
that current state-of-the art rules do not encode.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitaj_B/0/1/0/all/0/1&quot;&gt;Briland Hitaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasti_P/0/1/0/all/0/1&quot;&gt;Paolo Gasti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ateniese_G/0/1/0/all/0/1&quot;&gt;Giuseppe Ateniese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Cruz_F/0/1/0/all/0/1&quot;&gt;Fernando Perez-Cruz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.01674">
<title>Probabilistic Rule Realization and Selection. (arXiv:1709.01674v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1709.01674</link>
<description rdf:parseType="Literal">&lt;p&gt;Abstraction and realization are bilateral processes that are key in deriving
intelligence and creativity. In many domains, the two processes are approached
through rules: high-level principles that reveal invariances within similar yet
diverse examples. Under a probabilistic setting for discrete input spaces, we
focus on the rule realization problem which generates input sample
distributions that follow the given rules. More ambitiously, we go beyond a
mechanical realization that takes whatever is given, but instead ask for
proactively selecting reasonable rules to realize. This goal is demanding in
practice, since the initial rule set may not always be consistent and thus
intelligent compromises are needed. We formulate both rule realization and
selection as two strongly connected components within a single and symmetric
bi-convex problem, and derive an efficient algorithm that works at large scale.
Taking music compositional rules as the main example throughout the paper, we
demonstrate our model&apos;s efficiency in not only music realization (composition)
but also music interpretation and understanding (analysis).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haizi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1&quot;&gt;Lav R. Varshney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1711.11279">
<title>TCAV: Relative concept importance testing with Linear Concept Activation Vectors. (arXiv:1711.11279v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1711.11279</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks commonly offer high utility but remain difficult to
interpret. Developing methods to explain their decisions is challenging due to
their large size, complex structure, and inscrutable internal representations.
This work argues that the language of explanations should be expanded from that
of input features (e.g., assigning importance weightings to pixels) to include
that of higher-level, human-friendly concepts. For example, an understandable
explanation of why an image classifier outputs the label &quot;zebra&quot; would ideally
relate to concepts such as &quot;stripes&quot; rather than a set of particular pixel
values. This paper introduces the &quot;concept activation vector&quot; (CAV) which
allows quantitative analysis of a concept&apos;s relative importance to
classification, with a user-provided set of input data examples defining the
concept. CAVs may be easily used by non-experts, who need only provide
examples, and with CAVs the high-dimensional structure of neural networks turns
into an aid to interpretation, rather than an obstacle. Using the domain of
image classification as a testing ground, we describe how CAVs may be used to
test hypotheses about classifiers and also generate insights into the
deficiencies and correlations in training data. CAVs also provide us a directed
approach to choose the combinations of neurons to visualize with the DeepDream
technique, which traditionally has chosen neurons or linear combinations of
neurons at random to visualize.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Been Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wattenberg_M/0/1/0/all/0/1&quot;&gt;Martin Wattenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gilmer_J/0/1/0/all/0/1&quot;&gt;Justin Gilmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cai_C/0/1/0/all/0/1&quot;&gt;Carrie Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wexler_J/0/1/0/all/0/1&quot;&gt;James Wexler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Viegas_F/0/1/0/all/0/1&quot;&gt;Fernanda Viegas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sayres_R/0/1/0/all/0/1&quot;&gt;Rory Sayres&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.00181">
<title>Personalized Gaussian Processes for Future Prediction of Alzheimer&apos;s Disease Progression. (arXiv:1712.00181v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1712.00181</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce the use of a personalized Gaussian Process model
(pGP) to predict the key metrics of Alzheimer&apos;s Disease progression (MMSE,
ADAS-Cog13, CDRSB and CS) based on each patient&apos;s previous visits. We start by
learning a population-level model using multi-modal data from previously seen
patients using the base Gaussian Process (GP) regression. Then, this model is
adapted sequentially over time to a new patient using domain adaptive GPs to
form the patient&apos;s pGP. We show that this new approach, together with an
auto-regressive formulation, leads to significant improvements in forecasting
future clinical status and cognitive scores for target patients when compared
to modeling the population with traditional GPs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peterson_K/0/1/0/all/0/1&quot;&gt;Kelly Peterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudovic_O/0/1/0/all/0/1&quot;&gt;Ognjen Rudovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerrero_R/0/1/0/all/0/1&quot;&gt;Ricardo Guerrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picard_R/0/1/0/all/0/1&quot;&gt;Rosalind W. Picard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1712.01856">
<title>Optimizing Human Learning. (arXiv:1712.01856v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1712.01856</link>
<description rdf:parseType="Literal">&lt;p&gt;Spaced repetition is a technique for efficient memorization which uses
repeated, spaced review of content to improve long-term retention. Can we find
the optimal reviewing schedule to maximize the benefits of spaced repetition?
In this paper, we introduce a novel, flexible representation of spaced
repetition using the framework of marked temporal point processes and then
address the above question as an optimal control problem for stochastic
differential equations with jumps. For two well-known human memory models, we
show that the optimal reviewing schedule is given by the recall probability of
the content to be learned. As a result, we can then develop a simple, scalable
online algorithm, Memorize, to sample the optimal reviewing times. Experiments
on both synthetic and real data gathered from Duolingo, a popular
language-learning online platform, show that our algorithm may be able to help
learners memorize more effectively than alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tabibian_B/0/1/0/all/0/1&quot;&gt;Behzad Tabibian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Upadhyay_U/0/1/0/all/0/1&quot;&gt;Utkarsh Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+De_A/0/1/0/all/0/1&quot;&gt;Abir De&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zarezade_A/0/1/0/all/0/1&quot;&gt;Ali Zarezade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schoelkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Schoelkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gomez_Rodriguez_M/0/1/0/all/0/1&quot;&gt;Manuel Gomez-Rodriguez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1801.07047">
<title>News-based forecasts of macroeconomic indicators: A semantic path model for interpretable predictions. (arXiv:1801.07047v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1801.07047</link>
<description rdf:parseType="Literal">&lt;p&gt;The macroeconomic climate influences operations with regard to, e.g., raw
material prices, financing, supply chain utilization and demand quotas. In
order to adapt to the economic environment, decision-makers across the public
and private sectors require accurate forecasts of the economic outlook.
Existing predictive frameworks base their forecasts primarily on time series
analysis, as well as the judgments of experts. As a consequence, current
approaches are often biased and prone to error. In order to reduce forecast
errors, this paper presents an innovative methodology that extends lag
variables with unstructured data in the form of financial news: (1) we apply a
variety of models from machine learning to word counts as a high-dimensional
input. However, this approach suffers from low interpretability and
overfitting, motivating the following remedies. (2) We follow the intuition
that the economic climate is driven by general sentiments and suggest a
projection of words onto latent semantic structures as a means of feature
engineering. (3) We propose a semantic path model, together with estimation
technique based on regularization, in order to yield full interpretability of
the forecasts. We demonstrate the predictive performance of our approach by
utilizing 80,813 ad hoc announcements in order to make long-term forecasts of
up to 24 months ahead regarding key macroeconomic indicators. Back-testing
reveals a considerable reduction in forecast errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feuerriegel_S/0/1/0/all/0/1&quot;&gt;Stefan Feuerriegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gordon_J/0/1/0/all/0/1&quot;&gt;Julius Gordon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1802.02550">
<title>Semi-Amortized Variational Autoencoders. (arXiv:1802.02550v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1802.02550</link>
<description rdf:parseType="Literal">&lt;p&gt;Amortized variational inference (AVI) replaces instance-specific local
inference with a global inference network. While AVI has enabled efficient
training of deep generative models such as variational autoencoders (VAE),
recent empirical work suggests that inference networks can produce suboptimal
variational parameters. We propose a hybrid approach, to use AVI to initialize
the variational parameters and run stochastic variational inference (SVI) to
refine them. Crucially, the local SVI procedure is itself differentiable, so
the inference network and generative model can be trained end-to-end with
gradient-based optimization. This semi-amortized approach enables the use of
rich generative models without experiencing the posterior-collapse phenomenon
common in training VAEs for problems like text generation. Experiments show
this approach outperforms strong autoregressive and variational baselines on
standard text and image datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wiseman_S/0/1/0/all/0/1&quot;&gt;Sam Wiseman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Miller_A/0/1/0/all/0/1&quot;&gt;Andrew C. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sontag_D/0/1/0/all/0/1&quot;&gt;David Sontag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rush_A/0/1/0/all/0/1&quot;&gt;Alexander M. Rush&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01500">
<title>Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks. (arXiv:1803.01500v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01500</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an approach to address two issues that commonly occur during
training of unsupervised GANs. First, since GANs use only a continuous latent
distribution to embed multiple classes or clusters of data, they often do not
correctly handle the structural discontinuity between disparate classes in a
latent space. Second, discriminators of GANs easily forget about past generated
samples by generators, incurring instability during adversarial training. We
argue that these two infamous problems of unsupervised GAN training can be
largely alleviated by a learnable memory network to which both generators and
discriminators can access. Generators can effectively learn representation of
training samples to understand underlying cluster distributions of data, which
ease the structure discontinuity problem. At the same time, discriminators can
better memorize clusters of previously generated samples, which mitigate the
forgetting problem. We propose a novel end-to-end GAN model named memoryGAN,
which involves a memory network that is unsupervisedly trainable and integrable
to many existing GAN models. With evaluations on multiple datasets such as
Fashion-MNIST, CelebA, CIFAR10, and Chairs, we show that our model is
probabilistically interpretable, and generates realistic image samples of high
visual fidelity. The memoryGAN also achieves the state-of-the-art inception
scores over unsupervised GAN models on the CIFAR10 dataset, without any
optimization tricks and weaker divergences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Youngjin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minjung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gunhee Kim&lt;/a&gt;</dc:creator>
</item></rdf:RDF>