<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>Boring papers from arXiv</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">The Bad Papers on ML, AI and Statistics</description>
<dc:language>en-us</dc:language>
<dc:date>2018-08-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Statistics -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06928"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06686"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06740"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.10461"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.05249"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06603"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06638"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06640"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06651"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06664"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06670"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06684"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06733"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06774"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06797"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06846"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06865"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06910"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06918"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06942"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06992"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1808.06996"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1510.02833"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1709.02538"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1803.01576"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1805.06350"/>
<rdf:li rdf:resource="http://arxiv.org/abs/1807.00939"/></rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image><item rdf:about="http://arxiv.org/abs/1808.06928">
<title>The Distribution of Reversible Functions is Normal. (arXiv:1808.06928v1 [cs.ET])</title>
<link>http://arxiv.org/abs/1808.06928</link>
<description rdf:parseType="Literal">&lt;p&gt;The distribution of reversible programs tends to a limit as their size
increases. For problems with a Hamming distance fitness function the limiting
distribution is binomial with an exponentially small chance (but non~zero)
chance of perfect solution. Sufficiently good reversible circuits are more
common. Expected RMS error is also calculated. Random unitary matrices may
suggest possible extension to quantum computing. Using the genetic programming
(GP) benchmark, the six multiplexor, circuits of Toffoli gates are shown to
give a fitness landscape amenable to evolutionary search. Minimal CCNOT
solutions to the six multiplexer are found but larger circuits are more
evolvable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langdon_W/0/1/0/all/0/1&quot;&gt;W. B. Langdon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06686">
<title>Deep Multimodal Image-Repurposing Detection. (arXiv:1808.06686v1 [cs.MM])</title>
<link>http://arxiv.org/abs/1808.06686</link>
<description rdf:parseType="Literal">&lt;p&gt;Nefarious actors on social media and other platforms often spread rumors and
falsehoods through images whose metadata (e.g., captions) have been modified to
provide visual substantiation of the rumor/falsehood. This type of modification
is referred to as image repurposing, in which often an unmanipulated image is
published along with incorrect or manipulated metadata to serve the actor&apos;s
ulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)
dataset, a substantially challenging dataset over that which has been
previously available to support research into image repurposing detection. The
new dataset includes location, person, and organization manipulations on
real-world data sourced from Flickr. We also present a novel, end-to-end, deep
multimodal learning model for assessing the integrity of an image by combining
information extracted from the image with related information from a knowledge
base. The proposed method is compared against state-of-the-art techniques on
existing datasets as well as MEIR, where it outperforms existing methods across
the board, with AUC improvement up to 0.23.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabir_E/0/1/0/all/0/1&quot;&gt;Ekraam Sabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AbdAlmageed_W/0/1/0/all/0/1&quot;&gt;Wael AbdAlmageed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1&quot;&gt;Prem Natarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06740">
<title>Interactive Semantic Parsing for If-Then Recipes via Hierarchical Reinforcement Learning. (arXiv:1808.06740v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.06740</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a text description, most existing semantic parsers synthesize a program
in one shot. However, in reality, the description can be ambiguous or
incomplete, solely based on which it is quite challenging to produce a correct
program. In this paper, we investigate interactive semantic parsing for If-Then
recipes where an agent can interact with users to resolve ambiguities. We
develop a hierarchical reinforcement learning (HRL) based agent that can
improve the parsing performance with minimal questions to users. Results under
both simulation and human evaluation show that our agent substantially
outperforms non-interactive semantic parsers and rule-based agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Ziyu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiujun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1&quot;&gt;Brian Sadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Huan Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.10461">
<title>From Knowledge Graph Embedding to Ontology Embedding? An Analysis of the Compatibility between Vector Space Representations and Rules. (arXiv:1805.10461v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1805.10461</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed the successful application of low-dimensional
vector space representations of knowledge graphs to predict missing facts or
find erroneous ones. However, it is not yet well-understood to what extent
ontological knowledge, e.g. given as a set of (existential) rules, can be
embedded in a principled way. To address this shortcoming, in this paper we
introduce a general framework based on a view of relations as regions, which
allows us to study the compatibility between ontological knowledge and
different types of vector space embeddings. Our technical contribution is
two-fold. First, we show that some of the most popular existing embedding
methods are not capable of modelling even very simple types of rules, which in
particular also means that they are not able to learn the type of dependencies
captured by such rules. Second, we study a model in which relations are
modelled as convex regions. We show particular that ontologies which are
expressed using so-called quasi-chained existential rules can be exactly
represented using convex regions, such that any set of facts which is induced
using that vector space embedding is logically consistent and deductively
closed with respect to the input ontology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_Basulto_V/0/1/0/all/0/1&quot;&gt;V&amp;#xed;ctor Guti&amp;#xe9;rrez-Basulto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1&quot;&gt;Steven Schockaert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.05249">
<title>LSTM-Based Goal Recognition in Latent Space. (arXiv:1808.05249v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1808.05249</link>
<description rdf:parseType="Literal">&lt;p&gt;Approaches to goal recognition have progressively relaxed the requirements
about the amount of domain knowledge and available observations, yielding
accurate and efficient algorithms capable of recognizing goals. However, to
recognize goals in raw data, recent approaches require either human engineered
domain knowledge, or samples of behavior that account for almost all actions
being observed to infer possible goals. This is clearly too strong a
requirement for real-world applications of goal recognition, and we develop an
approach that leverages advances in recurrent neural networks to perform goal
recognition as a classification task, using encoded plan traces for training.
We empirically evaluate our approach against the state-of-the-art in goal
recognition with image-based domains, and discuss under which conditions our
approach is superior to previous ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amado_L/0/1/0/all/0/1&quot;&gt;Leonardo Amado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aires_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Paulo Aires&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pereira_R/0/1/0/all/0/1&quot;&gt;Ramon Fraga Pereira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magnaguagno_M/0/1/0/all/0/1&quot;&gt;Maur&amp;#xed;cio C. Magnaguagno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granada_R/0/1/0/all/0/1&quot;&gt;Roger Granada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meneguzzi_F/0/1/0/all/0/1&quot;&gt;Felipe Meneguzzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06603">
<title>Network-based Biased Tree Ensembles (NetBiTE) for Drug Sensitivity Prediction and Drug Sensitivity Biomarker Identification in Cancer. (arXiv:1808.06603v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/1808.06603</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the Network-based Biased Tree Ensembles (NetBiTE) method for drug
sensitivity prediction and drug sensitivity biomarker identification in cancer
using a combination of prior knowledge and gene expression data. Our devised
method consists of a biased tree ensemble that is built according to a
probabilistic bias weight distribution. The bias weight distribution is
obtained from the assignment of high weights to the drug targets and
propagating the assigned weights over a protein-protein interaction network
such as STRING. The propagation of weights, defines neighborhoods of influence
around the drug targets and as such simulates the spread of perturbations
within the cell, following drug administration. Using a synthetic dataset, we
showcase how application of biased tree ensembles (BiTE) results in significant
accuracy gains at a much lower computational cost compared to the unbiased
random forests (RF) algorithm. We then apply NetBiTE to the Genomics of Drug
Sensitivity in Cancer (GDSC) dataset and demonstrate that NetBiTE outperforms
RF in predicting IC50 drug sensitivity, only for drugs that target membrane
receptor pathways (MRPs): RTK, EGFR and IGFR signaling pathways. We propose
based on the NetBiTE results, that for drugs that inhibit MRPs, the expression
of target genes prior to drug administration is a biomarker for IC50 drug
sensitivity following drug administration. We further verify and reinforce this
proposition through control studies on, PI3K/MTOR signaling pathway inhibitors,
a drug category that does not target MRPs, and through assignment of dummy
targets to MRP inhibiting drugs and investigating the variation in NetBiTE
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Oskooei_A/0/1/0/all/0/1&quot;&gt;Ali Oskooei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Manica_M/0/1/0/all/0/1&quot;&gt;Matteo Manica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mathis_R/0/1/0/all/0/1&quot;&gt;Roland Mathis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Martinez_M/0/1/0/all/0/1&quot;&gt;Maria Rodriguez Martinez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06638">
<title>Supervised Kernel PCA For Longitudinal Data. (arXiv:1808.06638v1 [stat.ME])</title>
<link>http://arxiv.org/abs/1808.06638</link>
<description rdf:parseType="Literal">&lt;p&gt;In statistical learning, high covariate dimensionality poses challenges for
robust prediction and inference. To address this challenge, supervised
dimension reduction is often performed, where dependence on the outcome is
maximized for a selected covariate subspace with smaller dimensionality.
Prevalent dimension reduction techniques assume data are $i.i.d.$, which is not
appropriate for longitudinal data comprising multiple subjects with repeated
measurements over time. In this paper, we derive a decomposition of the
Hilbert-Schmidt Independence Criterion as a supervised loss function for
longitudinal data, enabling dimension reduction between and within clusters
separately, and propose a dimensionality-reduction technique, $sklPCA$, that
performs this decomposed dimension reduction. We also show that this technique
yields superior model accuracy compared to the model it extends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Staples_P/0/1/0/all/0/1&quot;&gt;Patrick Staples&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ouyang_M/0/1/0/all/0/1&quot;&gt;Min Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dougherty_R/0/1/0/all/0/1&quot;&gt;Robert F. Dougherty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ryslik_G/0/1/0/all/0/1&quot;&gt;Gregory A. Ryslik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dagum_P/0/1/0/all/0/1&quot;&gt;Paul Dagum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06640">
<title>Adversarial Removal of Demographic Attributes from Text Data. (arXiv:1808.06640v1 [cs.CL])</title>
<link>http://arxiv.org/abs/1808.06640</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in Representation Learning and Adversarial Training seem to
succeed in removing unwanted features from the learned representation. We show
that demographic information of authors is encoded in -- and can be recovered
from -- the intermediate representations learned by text-based neural
classifiers. The implication is that decisions of classifiers trained on
textual data are not agnostic to -- and likely condition on -- demographic
attributes. When attempting to remove such demographic information using
adversarial training, we find that while the adversarial component achieves
chance-level development-set accuracy during training, a post-hoc classifier,
trained on the encoded sentences from the first part, still manages to reach
substantially higher classification accuracies on the same data. This behavior
is consistent across several tasks, demographic properties and datasets. We
explore several techniques to improve the effectiveness of the adversarial
component. Our main conclusion is a cautionary one: do not rely on the
adversarial training to achieve invariant representation to sensitive features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1&quot;&gt;Yanai Elazar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1&quot;&gt;Yoav Goldberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06651">
<title>Privacy Amplification by Iteration. (arXiv:1808.06651v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06651</link>
<description rdf:parseType="Literal">&lt;p&gt;Many commonly used learning algorithms work by iteratively updating an
intermediate solution using one or a few data points in each iteration.
Analysis of differential privacy for such algorithms often involves ensuring
privacy of each step and then reasoning about the cumulative privacy cost of
the algorithm. This is enabled by composition theorems for differential privacy
that allow releasing of all the intermediate results. In this work, we
demonstrate that for contractive iterations, not releasing the intermediate
results strongly amplifies the privacy guarantees. We describe several
applications of this new analysis technique to solving convex optimization
problems via noisy stochastic gradient descent. For example, we demonstrate
that a relatively small number of non-private data points from the same
distribution can be used to close the gap between private and non-private
convex optimization. In addition, we demonstrate that we can achieve guarantees
similar to those obtainable using the privacy-amplification-by-sampling
technique in several natural settings where that technique cannot be applied.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1&quot;&gt;Vitaly Feldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mironov_I/0/1/0/all/0/1&quot;&gt;Ilya Mironov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1&quot;&gt;Kunal Talwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakurta_A/0/1/0/all/0/1&quot;&gt;Abhradeep Thakurta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06664">
<title>Out-of-Distribution Detection using Multiple Semantic Label Representations. (arXiv:1808.06664v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.06664</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks are powerful models that attained remarkable results on
a variety of tasks. These models are shown to be extremely efficient when
training and test data are drawn from the same distribution. However, it is not
clear how a network will act when it is fed with an out-of-distribution
example. In this work, we consider the problem of out-of-distribution detection
in neural networks. We propose to use multiple semantic dense representations
instead of sparse representation as the target label. Specifically, we propose
to use several word representations obtained from different corpora or
architectures as target labels. We evaluated the proposed model on computer
vision, and speech commands detection tasks and compared it to previous
methods. Results suggest that our method compares favorably with previous work.
Besides, we present the efficiency of our approach for detecting wrongly
classified and adversarial examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shalev_G/0/1/0/all/0/1&quot;&gt;Gabi Shalev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Adi_Y/0/1/0/all/0/1&quot;&gt;Yossi Adi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Keshet_J/0/1/0/all/0/1&quot;&gt;Joseph Keshet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06670">
<title>Learning deep representations by mutual information estimation and maximization. (arXiv:1808.06670v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.06670</link>
<description rdf:parseType="Literal">&lt;p&gt;Many popular representation-learning algorithms use training objectives
defined on the observed data space, which we call pixel-level. This may be
detrimental when only a small fraction of the bits of signal actually matter at
a semantic level. We hypothesize that representations should be learned and
evaluated more directly in terms of their information content and statistical
or structural constraints. To address the first quality, we consider learning
unsupervised representations by maximizing mutual information between part or
all of the input and a high-level feature vector. To address the second, we
control characteristics of the representation by matching to a prior
adversarially. Our method, which we call Deep INFOMAX (DIM), can be used to
learn representations with desired characteristics and which empirically
outperform a number of popular unsupervised learning methods on classification
tasks. DIM opens new avenues for unsupervised learn-ing of representations and
is an important step towards flexible formulations of representation learning
objectives catered towards specific end-goals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hjelm_R/0/1/0/all/0/1&quot;&gt;R Devon Hjelm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fedorov_A/0/1/0/all/0/1&quot;&gt;Alex Fedorov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lavoie_Marchildon_S/0/1/0/all/0/1&quot;&gt;Samuel Lavoie-Marchildon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grewal_K/0/1/0/all/0/1&quot;&gt;Karan Grewal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trischler_A/0/1/0/all/0/1&quot;&gt;Adam Trischler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06684">
<title>Use Of Vapnik-Chervonenkis Dimension in Model Selection. (arXiv:1808.06684v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.06684</link>
<description rdf:parseType="Literal">&lt;p&gt;In this dissertation, I derive a new method to estimate the
Vapnik-Chervonenkis Dimension (VCD) for the class of linear functions. This
method is inspired by the technique developed by Vapnik et al. Vapnik et al.
(1994). My contribution rests on the approximation of the expected maximum
difference between two empirical Losses (EMDBTEL). In fact, I use a
cross-validated form of the error to compute the EMDBTEL, and I make the bound
on the EMDBTEL tighter by minimizing a constant in of its right upper bound. I
also derive two bounds for the true unknown risk using the additive (ERM1) and
the multiplicative (ERM2) Chernoff bounds. These bounds depend on the estimated
VCD and the empirical risk. These bounds can be used to perform model selection
and to declare with high probability, the chosen model will perform better
without making strong assumptions about the data generating process (DG).
&lt;/p&gt;
&lt;p&gt;I measure the accuracy of my technique on simulated datasets and also on
three real datasets. The model selection provided by VCD was always as good as
if not better than the other methods under reasonable conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mpoudeu_M/0/1/0/all/0/1&quot;&gt;Merlin Mpoudeu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06733">
<title>Wrapped Loss Function for Regularizing Nonconforming Residual Distributions. (arXiv:1808.06733v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06733</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-output is essential in machine learning that it might suffer from
nonconforming residual distributions, i.e., the multi-output residual
distributions are not conforming to the expected distribution. In this paper we
propose &quot;Wrapped Loss Function&quot; to wrap the original loss function to alleviate
the problem. This wrapped loss function acts just like original loss function
that its gradient can be used for backpropagation optimization. Empirical
evaluations show wrapped loss function has advanced properties of faster
convergence, better accuracy and improving imbalanced data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chun Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming Chuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Meng Chang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06774">
<title>Multi-task multiple kernel machines for personalized pain recognition from functional near-infrared spectroscopy brain signals. (arXiv:1808.06774v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06774</link>
<description rdf:parseType="Literal">&lt;p&gt;Currently there is no validated objective measure of pain. Recent
neuroimaging studies have explored the feasibility of using functional
near-infrared spectroscopy (fNIRS) to measure alterations in brain function in
evoked and ongoing pain. In this study, we applied multi-task machine learning
methods to derive a practical algorithm for pain detection derived from fNIRS
signals in healthy volunteers exposed to a painful stimulus. Especially, we
employed multi-task multiple kernel learning to account for the inter-subject
variability in pain response. Our results support the use of fNIRS and machine
learning techniques in developing objective pain detection, and also highlight
the importance of adopting personalized analysis in the process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_Martinez_D/0/1/0/all/0/1&quot;&gt;Daniel Lopez-Martinez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Ke Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steele_S/0/1/0/all/0/1&quot;&gt;Sarah C. Steele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Arielle J. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borsook_D/0/1/0/all/0/1&quot;&gt;David Borsook&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picard_R/0/1/0/all/0/1&quot;&gt;Rosalind Picard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06797">
<title>zoNNscan : a boundary-entropy index for zone inspection of neural models. (arXiv:1808.06797v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06797</link>
<description rdf:parseType="Literal">&lt;p&gt;The training of deep neural network classifiers results in decision
boundaries which geometry is still not well understood. This is in direct
relation with classification problems such as so called adversarial examples.
We introduce zoNNscan, an index that is intended to inform on the boundary
uncertainty (in terms of the presence of other classes) around one given input
datapoint. It is based on confidence entropy, and is implemented through
sampling in the multidimensional ball surrounding that input. We detail the
zoNNscan index, give an algorithm for approximating it, and finally illustrate
its benefits on four applications, including two important problems for the
adoption of deep networks in critical systems: adversarial examples and corner
case inputs. We highlight that zoNNscan exhibits significantly higher values
than for standard inputs in those two problem classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaouen_A/0/1/0/all/0/1&quot;&gt;Adel Jaouen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merrer_E/0/1/0/all/0/1&quot;&gt;Erwan Le Merrer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06846">
<title>Search for Common Minima in Joint Optimization of Multiple Cost Functions. (arXiv:1808.06846v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/1808.06846</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel optimization method, named the Combined Optimization
Method (COM), for the joint optimization of two or more cost functions. Unlike
the conventional joint optimization schemes, which try to find minima in a
weighted sum of cost functions, the COM explores search space for common minima
shared by all the cost functions. Given a set of multiple cost functions that
have qualitatively different distributions of local minima with each other, the
proposed method finds the common minima with a high success rate without the
help of any metaheuristics. As a demonstration, we apply the COM to the crystal
structure prediction in materials science. By introducing the concept of data
assimilation, i.e., adopting the theoretical potential energy of the crystal
and the crystallinity, which characterizes the agreement with the theoretical
and experimental X-ray diffraction patterns, as cost functions, we show that
the correct crystal structures of Si diamond, low quartz, and low cristobalite
can be predicted with significantly higher success rates than the previous
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Adachi_D/0/1/0/all/0/1&quot;&gt;Daiki Adachi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Tsujimoto_N/0/1/0/all/0/1&quot;&gt;Naoto Tsujimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Akashi_R/0/1/0/all/0/1&quot;&gt;Ryosuke Akashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Todo_S/0/1/0/all/0/1&quot;&gt;Synge Todo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Tsuneyuki_S/0/1/0/all/0/1&quot;&gt;Shinji Tsuneyuki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06865">
<title>Machine Learning for Spatiotemporal Sequence Forecasting: A Survey. (arXiv:1808.06865v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06865</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatiotemporal systems are common in the real-world. Forecasting the
multi-step future of these spatiotemporal systems based on the past
observations, or, Spatiotemporal Sequence Forecasting (STSF), is a significant
and challenging problem. Although lots of real-world problems can be viewed as
STSF and many research works have proposed machine learning based methods for
them, no existing work has summarized and compared these methods from a unified
perspective. This survey aims to provide a systematic review of machine
learning for STSF. In this survey, we define the STSF problem and classify it
into three subcategories: Trajectory Forecasting of Moving Point Cloud
(TF-MPC), STSF on Regular Grid (STSF-RG) and STSF on Irregular Grid (STSF-IG).
We then introduce the two major challenges of STSF: 1) how to learn a model for
multi-step forecasting and 2) how to adequately model the spatial and temporal
structures. After that, we review the existing works for solving these
challenges, including the general learning strategies for multi-step
forecasting, the classical machine learning based methods for STSF, and the
deep learning based methods for STSF. We also compare these methods and point
out some potential research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xingjian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1&quot;&gt;Dit-Yan Yeung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06910">
<title>Scalable Population Synthesis with Deep Generative Modeling. (arXiv:1808.06910v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.06910</link>
<description rdf:parseType="Literal">&lt;p&gt;Population synthesis is concerned with the generation of synthetic yet
realistic representations of populations. It is a fundamental problem in the
modeling of transport where the synthetic populations of micro agents represent
a key input to most agent-based models. In this paper, a new methodological
framework for how to grow pools of micro agents is presented. This is
accomplished by adopting a deep generative modeling approach from machine
learning based on a Variational Autoencoder (VAE) framework. Compared to the
previous population synthesis approaches based on Iterative Proportional
Fitting (IPF), Markov Chain Monte Carlo (MCMC) sampling or traditional
generative models, the proposed method allows unparalleled scalability with
respect to the number and types of attributes. In contrast to the approaches
that rely on approximating the joint distribution in the observed data space,
VAE learns its compressed latent representation. The advantage of the
compressed representation is that it avoids the problem of the generated
samples being trapped in local minima when the number of attributes becomes
large. The problem is illustrated using the Danish National Travel Survey data,
where the Gibbs sampler fails to generate a population with 21 attributes
(corresponding to the 121-dimensional joint distribution). At the same time,
VAE shows acceptable performance when 47 attributes (corresponding to the
357-dimensional joint distribution) are used. Moreover, VAE allows for growing
agents that are virtually different from those in the original data but have
similar statistical properties and correlation structure. The presented
approach will help modelers to generate better and richer populations with a
high level of detail, including smaller zones, personal details and travel
preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Borysov_S/0/1/0/all/0/1&quot;&gt;Stanislav S. Borysov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rich_J/0/1/0/all/0/1&quot;&gt;Jeppe Rich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pereira_F/0/1/0/all/0/1&quot;&gt;Francisco C. Pereira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06918">
<title>On a New Improvement-Based Acquisition Function for Bayesian Optimization. (arXiv:1808.06918v1 [stat.ML])</title>
<link>http://arxiv.org/abs/1808.06918</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization (BO) is a popular algorithm for solving challenging
optimization tasks. It is designed for problems where the objective function is
expensive to evaluate, perhaps not available in exact form, without gradient
information and possibly returning noisy values. Different versions of the
algorithm vary in the choice of the acquisition function, which recommends the
point to query the objective at next. Initially, researchers focused on
improvement-based acquisitions, while recently the attention has shifted to
more computationally expensive information-theoretical measures. In this paper
we present two major contributions to the literature. First, we propose a new
improvement-based acquisition function that recommends query points where the
improvement is expected to be high with high confidence. The proposed algorithm
is evaluated on a large set of benchmark functions from the global optimization
literature, where it turns out to perform at least as well as current
state-of-the-art acquisition functions, and often better. This suggests that it
is a powerful default choice for BO. The novel policy is then compared to
widely used global optimization solvers in order to confirm that BO methods
reduce the computational costs of the optimization by keeping the number of
function evaluations small. The second main contribution represents an
application to precision medicine, where the interest lies in the estimation of
parameters of a partial differential equations model of the human pulmonary
blood circulation system. Once inferred, these parameters can help clinicians
in diagnosing a patient with pulmonary hypertension without going through the
standard invasive procedure of right heart catheterization, which can lead to
side effects and complications (e.g. severe pain, internal bleeding,
thrombosis).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Noe_U/0/1/0/all/0/1&quot;&gt;Umberto No&amp;#xe8;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Husmeier_D/0/1/0/all/0/1&quot;&gt;Dirk Husmeier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06942">
<title>PACO: Signal Restoration via PAtch COnsensus. (arXiv:1808.06942v1 [eess.SP])</title>
<link>http://arxiv.org/abs/1808.06942</link>
<description rdf:parseType="Literal">&lt;p&gt;Many signal processing algorithms operate by breaking the target signal into
possibly overlapping segments (typically called windows or patches), processing
them separately, and then stitching them back into place to produce a unified
output. In most cases where pach overlapping occurs, the final value of those
samples that are estimated by more than one patch is resolved by averaging
those estimates; this includes many recent image processing algorithms. In
other cases, typically frequency-based restoration methods, the average is
implicitly weighted by some window function such as Hanning, Blackman, etc.
which is applied prior to the Fourier/DCT transform in order to avoid Gibbs
oscillations in the processed patches. Such averaging may incidentally help in
covering up artifacts in the restoration process, but more often will simply
degrade the overall result, posing an upper limit to the size of the patches
that can be used. In order to avoid such drawbacks, we propose a new
methodology where the different estimates of any given sample are forced to be
identical. We show that, together, these consensus constraints constitute a
non-empty convex feasible set, provide a general formulation of the resulting
constrained optimization problem which can be applied to a wide variety of
signal restoration tasks, and propose an efficient algorithm for finding the
corresponding solutions. Finally, we describe in detail the application of the
proposed methodology to three different signal processing problems, in some
cases surpassing the state of the art by a significant margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Paulino_I/0/1/0/all/0/1&quot;&gt;Ignacio Francisco Ram&amp;#xed;rez Paulino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06992">
<title>Optimizing the Union of Intersections LASSO ($UoI_{LASSO}$) and Vector Autoregressive ($UoI_{VAR}$) Algorithms for Improved Statistical Estimation at Scale. (arXiv:1808.06992v1 [cs.LG])</title>
<link>http://arxiv.org/abs/1808.06992</link>
<description rdf:parseType="Literal">&lt;p&gt;The analysis of scientific data of increasing size and complexity requires
statistical machine learning methods that are both interpretable and
predictive. Union of Intersections (UoI), a recently developed framework, is a
two-step approach that separates model selection and model estimation. A linear
regression algorithm based on UoI, $UoI_{LASSO}$, simultaneously achieves low
false positives and low false negative feature selection as well as low bias
and low variance estimates. Together, these qualities make the results both
predictive and interpretable. In this paper, we optimize the $UoI_{LASSO}$
algorithm for single-node execution on NERSC&apos;s Cori Knights Landing, a Xeon Phi
based supercomputer. We then scale $UoI_{LASSO}$ to execute on cores ranging
from 68-278,528 cores on a range of dataset sizes demonstrating the weak and
strong scaling of the implementation. We also implement a variant of
$UoI_{LASSO}$, $UoI_{VAR}$ for vector autoregressive models, to analyze high
dimensional time-series data. We perform single node optimization and
multi-node scaling experiments for $UoI_{VAR}$ to demonstrate the effectiveness
of the algorithm for weak and strong scaling. Our implementations enable to use
estimate the largest VAR model (1000 nodes) we are aware of, and apply it to
large neurophysiology data 192 nodes).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_M/0/1/0/all/0/1&quot;&gt;Mahesh Balasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_T/0/1/0/all/0/1&quot;&gt;Trevor Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cook_B/0/1/0/all/0/1&quot;&gt;Brandon Cook&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_S/0/1/0/all/0/1&quot;&gt;Sharmodeep Bhattacharyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhat/0/1/0/all/0/1&quot;&gt;Prabhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1&quot;&gt;Aviral Shrivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1&quot;&gt;Kristofer Bouchard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1808.06996">
<title>Curse of Heterogeneity: Computational Barriers in Sparse Mixture Models and Phase Retrieval. (arXiv:1808.06996v1 [math.ST])</title>
<link>http://arxiv.org/abs/1808.06996</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the fundamental tradeoffs between statistical accuracy and
computational tractability in the analysis of high dimensional heterogeneous
data. As examples, we study sparse Gaussian mixture model, mixture of sparse
linear regressions, and sparse phase retrieval model. For these models, we
exploit an oracle-based computational model to establish conjecture-free
computationally feasible minimax lower bounds, which quantify the minimum
signal strength required for the existence of any algorithm that is both
computationally tractable and statistically accurate. Our analysis shows that
there exist significant gaps between computationally feasible minimax risks and
classical ones. These gaps quantify the statistical price we must pay to
achieve computational tractability in the presence of data heterogeneity. Our
results cover the problems of detection, estimation, support recovery, and
clustering, and moreover, resolve several conjectures of Azizyan et al. (2013,
2015); Verzelen and Arias-Castro (2017); Cai et al. (2016). Interestingly, our
results reveal a new but counter-intuitive phenomenon in heterogeneous data
analysis that more data might lead to less computation complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jianqing Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1510.02833">
<title>On the Definiteness of Earth Mover&apos;s Distance and Its Relation to Set Intersection. (arXiv:1510.02833v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1510.02833</link>
<description rdf:parseType="Literal">&lt;p&gt;Positive definite kernels are an important tool in machine learning that
enable efficient solutions to otherwise difficult or intractable problems by
implicitly linearizing the problem geometry. In this paper we develop a
set-theoretic interpretation of the Earth Mover&apos;s Distance (EMD) and propose
Earth Mover&apos;s Intersection (EMI), a positive definite analog to EMD for sets of
different sizes. We provide conditions under which EMD or certain
approximations to EMD are negative definite. We also present a
positive-definite-preserving transformation that can be applied to any kernel
and can also be used to derive positive definite EMD-based kernels and show
that the Jaccard index is simply the result of this transformation. Finally, we
evaluate kernels based on EMI and the proposed transformation versus EMD in
various computer vision tasks and show that EMD is generally inferior even with
indefinite kernel techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_A/0/1/0/all/0/1&quot;&gt;Andrew Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duncan_C/0/1/0/all/0/1&quot;&gt;Christian A. Duncan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanno_J/0/1/0/all/0/1&quot;&gt;Jinko Kanno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Selmic_R/0/1/0/all/0/1&quot;&gt;Rastko R. Selmic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1709.02538">
<title>DeepFense: Online Accelerated Defense Against Adversarial Deep Learning. (arXiv:1709.02538v4 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/1709.02538</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in adversarial Deep Learning (DL) have opened up a largely
unexplored surface for malicious attacks jeopardizing the integrity of
autonomous DL systems. With the wide-spread usage of DL in critical and
time-sensitive applications, including unmanned vehicles, drones, and video
surveillance systems, online detection of malicious inputs is of utmost
importance. We propose DeepFense, the first end-to-end automated framework that
simultaneously enables efficient and safe execution of DL models. DeepFense
formalizes the goal of thwarting adversarial attacks as an optimization problem
that minimizes the rarely observed regions in the latent feature space spanned
by a DL network. To solve the aforementioned minimization problem, a set of
complementary but disjoint modular redundancies are trained to validate the
legitimacy of the input samples in parallel with the victim DL model. DeepFense
leverages hardware/software/algorithm co-design and customized acceleration to
achieve just-in-time performance in resource-constrained settings. The proposed
countermeasure is unsupervised, meaning that no adversarial sample is leveraged
to train modular redundancies. We further provide an accompanying API to reduce
the non-recurring engineering cost and ensure automated adaptation to various
platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders
of magnitude performance improvement while enabling online adversarial sample
detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rouhani_B/0/1/0/all/0/1&quot;&gt;Bita Darvish Rouhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samragh_M/0/1/0/all/0/1&quot;&gt;Mohammad Samragh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javaheripi_M/0/1/0/all/0/1&quot;&gt;Mojan Javaheripi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1&quot;&gt;Tara Javidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1&quot;&gt;Farinaz Koushanfar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1803.01576">
<title>Asymptotic Equivalence of Fixed-size and Varying-size Determinantal Point Processes. (arXiv:1803.01576v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1803.01576</link>
<description rdf:parseType="Literal">&lt;p&gt;Determinantal Point Processes (DPPs) are popular models for point processes
with repulsion. They appear in numerous contexts, from physics to graph theory,
and display appealing theoretical properties. On the more practical side of
things, since DPPs tend to select sets of points that are some distance apart
(repulsion), they have been advocated as a way of producing random subsets with
high diversity. DPPs come in two variants: fixed-size and varying-size. A
sample from a varying-size DPP is a subset of random cardinality, while in
fixed-size &quot;$k$-DPPs&quot; the cardinality is fixed. The latter makes more sense in
many applications, but unfortunately their computational properties are less
attractive, since, among other things, inclusion probabilities are harder to
compute. In this work we show that as the size of the ground set grows,
$k$-DPPs and DPPs become equivalent, meaning that their inclusion probabilities
converge. As a by-product, we obtain saddlepoint formulas for inclusion
probabilities in $k$-DPPs. These turn out to be extremely accurate, and suffer
less from numerical difficulties than exact methods do. Our results also
suggest that $k$-DPPs and DPPs also have equivalent maximum likelihood
estimators. Finally, we obtain results on asymptotic approximations of
elementary symmetric polynomials which may be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Barthelme_S/0/1/0/all/0/1&quot;&gt;Simon Barthelm&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Amblard_P/0/1/0/all/0/1&quot;&gt;Pierre-Olivier Amblard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tremblay_N/0/1/0/all/0/1&quot;&gt;Nicolas Tremblay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1805.06350">
<title>Approximating the Void: Learning Stochastic Channel Models from Observation with Variational Generative Adversarial Networks. (arXiv:1805.06350v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1805.06350</link>
<description rdf:parseType="Literal">&lt;p&gt;Channel modeling is a critical topic when considering designing, learning, or
evaluating the performance of any communications system. Most prior work in
designing or learning new modulation schemes has focused on using highly
simplified analytic channel models such as additive white Gaussian noise
(AWGN), Rayleigh fading channels or similar. Recently, we proposed the usage of
a generative adversarial networks (GANs) to jointly approximate a wireless
channel response model (e.g. from real black box measurements) and optimize for
an efficient modulation scheme over it using machine learning. This approach
worked to some degree, but was unable to produce accurate probability
distribution functions (PDFs) representing the stochastic channel response. In
this paper, we focus specifically on the problem of accurately learning a
channel PDF using a variational GAN, introducing an architecture and loss
function which can accurately capture stochastic behavior. We illustrate where
our prior method failed and share results capturing the performance of such as
system over a range of realistic channel distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OShea_T/0/1/0/all/0/1&quot;&gt;Timothy J. O&amp;#x27;Shea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1&quot;&gt;Tamoghna Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+West_N/0/1/0/all/0/1&quot;&gt;Nathan West&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.00939">
<title>Mining Illegal Insider Trading of Stocks: A Proactive Approach. (arXiv:1807.00939v2 [q-fin.ST] UPDATED)</title>
<link>http://arxiv.org/abs/1807.00939</link>
<description rdf:parseType="Literal">&lt;p&gt;Illegal insider trading of stocks is based on releasing non-public
information (e.g., new product launch, quarterly financial report, acquisition
or merger plan) before the information is made public. Detecting illegal
insider trading is difficult due to the complex, nonlinear, and non-stationary
nature of the stock market. In this work, we present an approach that detects
and predicts illegal insider trading proactively from large heterogeneous
sources of structured and unstructured data using a deep-learning based
approach combined with discrete signal processing on the time series data. In
addition, we use a tree-based approach that visualizes events and actions to
aid analysts in their understanding of large amounts of unstructured data.
Using existing data, we have discovered that our approach has a good success
rate in detecting illegal insider trading patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Islam_S/0/1/0/all/0/1&quot;&gt;Sheikh Rabiul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Ghafoor_S/0/1/0/all/0/1&quot;&gt;Sheikh Khaled Ghafoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Eberle_W/0/1/0/all/0/1&quot;&gt;William Eberle&lt;/a&gt;</dc:creator>
</item></rdf:RDF>